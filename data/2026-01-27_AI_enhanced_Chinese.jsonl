{"id": "2601.17027", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17027", "abs": "https://arxiv.org/abs/2601.17027", "authors": ["Honglin Lin", "Chonghan Qin", "Zheng Liu", "Qizhi Pei", "Yu Li", "Zhanping Zhong", "Xin Gao", "Yanfeng Wang", "Conghui He", "Lijun Wu"], "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility", "comment": null, "summary": "While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit \"understand - plan - code\" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86ImgCoder\u903b\u8f91\u9a71\u52a8\u6846\u67b6\u548cSciGenBench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u9ad8\u8d28\u91cf\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u5bf9\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u6f5c\u529b\u3002", "motivation": "\u867d\u7136\u5408\u6210\u6570\u636e\u5728\u6587\u672c\u9886\u57df\u7684\u79d1\u5b66\u63a8\u7406\u4e2d\u5df2\u88ab\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u591a\u6a21\u6001\u63a8\u7406\u4ecd\u53d7\u9650\u4e8e\u5408\u6210\u79d1\u5b66\u4e25\u8c28\u56fe\u50cf\u7684\u56f0\u96be\u3002\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u901a\u5e38\u4ea7\u751f\u89c6\u89c9\u4e0a\u5408\u7406\u4f46\u79d1\u5b66\u4e0a\u4e0d\u6b63\u786e\u7684\u8f93\u51fa\uff0c\u5bfc\u81f4\u89c6\u89c9-\u903b\u8f91\u5206\u6b67\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0b\u6e38\u63a8\u7406\u4e2d\u7684\u4ef7\u503c\u3002", "method": "1. \u7cfb\u7edf\u7814\u7a76\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u7684\u751f\u6210\u8303\u5f0f\u3001\u8bc4\u4f30\u548c\u4e0b\u6e38\u5e94\u7528\uff1b2. \u5206\u6790\u57fa\u4e8e\u50cf\u7d20\u7684\u76f4\u63a5\u751f\u6210\u548c\u7a0b\u5e8f\u5316\u5408\u6210\uff1b3. \u63d0\u51faImgCoder\u903b\u8f91\u9a71\u52a8\u6846\u67b6\uff0c\u91c7\u7528\"\u7406\u89e3-\u89c4\u5212-\u7f16\u7801\"\u5de5\u4f5c\u6d41\u7a0b\u63d0\u9ad8\u7ed3\u6784\u7cbe\u5ea6\uff1b4. \u5f15\u5165SciGenBench\u8bc4\u4f30\u57fa\u51c6\uff0c\u57fa\u4e8e\u4fe1\u606f\u6548\u7528\u548c\u903b\u8f91\u6709\u6548\u6027\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u57fa\u4e8e\u50cf\u7d20\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u7a81\u51fa\u4e86\u8868\u8fbe\u529b\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002\u5728\u4e25\u683c\u9a8c\u8bc1\u7684\u5408\u6210\u79d1\u5b66\u56fe\u50cf\u4e0a\u5fae\u8c03\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u63a8\u7406\u589e\u76ca\uff0c\u663e\u793a\u51fa\u7c7b\u4f3c\u4e8e\u6587\u672c\u9886\u57df\u7684\u6f5c\u5728\u6269\u5c55\u8d8b\u52bf\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u662f\u89e3\u9501\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u53ef\u884c\u8def\u5f84\uff0c\u9a8c\u8bc1\u4e86\u5408\u6210\u79d1\u5b66\u56fe\u50cf\u5728\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.17031", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17031", "abs": "https://arxiv.org/abs/2601.17031", "authors": ["Yunhao Xu", "Fuquan Zong", "Yexuan Xing", "Chulong Zhang", "Guang Yang", "Shilong Yang", "Xiaokun Liang", "Juan Yu"], "title": "Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection", "comment": null, "summary": "The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53cc\u589e\u5f3a\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u6d41\u5f62\u6269\u5c55\u548c\u8bed\u4e49\u5bf9\u8c61\u6ce8\u5165\uff0c\u901a\u8fc7\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5efa\u6a21\u8fde\u7eed\u901f\u5ea6\u573a\uff0c\u5728\u7ebf\u6027\u6df7\u5408\u53d8\u5f62\u573a\u4e2d\u751f\u6210\u89e3\u5256\u5b66\u5408\u7406\u7684\u53d8\u5f02\uff0c\u5e76\u901a\u8fc7Sim2Real\u75c5\u7076\u6ce8\u5165\u6a21\u5757\u5c06\u75c5\u7076\u7eb9\u7406\u79fb\u690d\u5230\u5065\u5eb7\u80cc\u666f\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u6709\u9650\u6807\u6ce8\u4e0b\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u8d8a\u6765\u8d8a\u53d6\u51b3\u4e8e\u6570\u636e\u5229\u7528\u6548\u7387\u800c\u975e\u539f\u59cb\u6570\u636e\u91cf\u3002\u5bf9\u4e8e\u8111\u819c\u7624\u7b49\u590d\u6742\u75c5\u7406\u7684\u51c6\u786e\u5206\u5272\uff0c\u9700\u8981\u6a21\u578b\u5145\u5206\u5229\u7528\u6709\u9650\u9ad8\u8d28\u91cf\u6807\u6ce8\u4e2d\u7684\u6f5c\u5728\u4fe1\u606f\u3002\u4e3a\u4e86\u6700\u5927\u5316\u73b0\u6709\u6570\u636e\u96c6\u7684\u4ef7\u503c\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cc\u589e\u5f3a\u6846\u67b6\uff1a1) \u7a7a\u95f4\u6d41\u5f62\u6269\u5c55\uff1a\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5efa\u6a21\u8fde\u7eed\u901f\u5ea6\u573a\uff0c\u5728\u7ebf\u6027\u6df7\u5408\u7684\u53d8\u5f62\u573a\u4e2d\u8fdb\u884c\u63d2\u503c\uff0c\u4ece\u5c11\u91cf\u951a\u70b9\u751f\u6210\u89e3\u5256\u5b66\u5408\u7406\u7684\u7ed3\u6784\u53d8\u5f02\uff1b2) Sim2Real\u75c5\u7076\u6ce8\u5165\uff1a\u6784\u5efa\u9ad8\u4fdd\u771f\u6a21\u62df\u57df\uff0c\u5c06\u75c5\u7076\u7eb9\u7406\u79fb\u690d\u5230\u5065\u5eb7\u89e3\u5256\u80cc\u666f\u4e2d\uff0c\u5f25\u5408\u5408\u6210\u589e\u5f3a\u4e0e\u771f\u5b9e\u75c5\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86nnU-Net\u548cU-Mamba\u7b49\u5148\u8fdb\u6a21\u578b\u7684\u6570\u636e\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\u7684\u9ad8\u6027\u80fd\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u7a7a\u95f4\u6d41\u5f62\u6269\u5c55\u548c\u8bed\u4e49\u5bf9\u8c61\u6ce8\u5165\u7684\u53cc\u91cd\u589e\u5f3a\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u6709\u9650\u6807\u6ce8\u6570\u636e\uff0c\u751f\u6210\u89e3\u5256\u5b66\u5408\u7406\u7684\u53d8\u5f02\u548c\u771f\u5b9e\u611f\u7684\u75c5\u7406\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6570\u636e\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.17032", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17032", "abs": "https://arxiv.org/abs/2601.17032", "authors": ["Wilkie Delgado-Font", "Miriela Escobedo-Nicot", "Manuel Gonz\u00e1lez-Hidalgo", "Silena Herold-Garcia", "Antoni Jaume-i-Cap\u00f3", "Arnau Mir"], "title": "Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images", "comment": null, "summary": "Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5916\u5468\u8840\u6d82\u7247\u56fe\u50cf\u5206\u6790\u7684\u81ea\u52a8\u5316\u7ea2\u7ec6\u80de\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u8bca\u65ad", "motivation": "\u4f20\u7edf\u663e\u5fae\u955c\u89c2\u5bdf\u7ea2\u7ec6\u80de\u53d8\u5f62\u8017\u65f6\u4e14\u4e3b\u89c2\u8bef\u5dee\u9ad8\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7684\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027", "method": "\u4f7f\u7528Chan-Vese\u4e3b\u52a8\u8f6e\u5ed3\u6a21\u578b\u5206\u5272\u56fe\u50cf\u4e2d\u7684\u7ea2\u7ec6\u80de\uff0c\u7136\u540e\u57fa\u4e8e\u5706\u5f62\u5f62\u72b6\u56e0\u5b50(CSF)\u548c\u692d\u5706\u5f62\u5f62\u72b6\u56e0\u5b50(ESF)\u5bf9\u7ea2\u7ec6\u80de\u8fdb\u884c\u5206\u7c7b\uff08\u6b63\u5e38\u3001\u7ec6\u957f\u6216\u5176\u4ed6\u53d8\u5f62\uff09\uff0c\u5e76\u5bf9\u7c07\u4e2d\u90e8\u5206\u906e\u6321\u7684\u7ec6\u80de\u8fdb\u884c\u692d\u5706\u8c03\u6574", "result": "\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0cF-measure\u503c\u8fbe\u52300.97\uff08\u6b63\u5e38\u7ec6\u80de\uff09\u548c0.95\uff08\u7ec6\u957f\u7ec6\u80de\uff09\uff0c\u591a\u9879\u591a\u7c7b\u6027\u80fd\u6307\u6807\u8868\u73b0\u4f18\u5f02", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u7528\u4e8e\u9570\u72b6\u7ec6\u80de\u8d2b\u8840\u7684\u4e34\u5e8a\u6cbb\u7597\u548c\u8bca\u65ad\u652f\u6301\uff0c\u80fd\u591f\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027"}}
{"id": "2601.17037", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17037", "abs": "https://arxiv.org/abs/2601.17037", "authors": ["Aahana Basappa", "Pranay Goel", "Anusri Karra", "Anish Karra", "Asa Gilmore", "Kevin Zhu"], "title": "AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs", "comment": "Comments: 13 pages, 4 figures. Presented at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: NeurIPS 2025 VLM4RWD. Authors Aahana Basappa and Pranay Goel contributed equally to this work. Code: https://github.com/AahanaB24/AMVICC, Data: https://doi.org/10.5281/zenodo.17646068", "summary": "We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \\textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86AMVICC\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5bf9\u8c61\u65b9\u5411\u3001\u6570\u91cf\u3001\u7a7a\u95f4\u5173\u7cfb\u7b49\u57fa\u672c\u89c6\u89c9\u6982\u5ff5\u4e0a\u5b58\u5728\u5171\u540c\u548c\u7279\u5b9a\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u57fa\u672c\u89c6\u89c9\u6982\u5ff5\uff08\u5982\u5bf9\u8c61\u65b9\u5411\u3001\u6570\u91cf\u3001\u7a7a\u95f4\u5173\u7cfb\uff09\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5c06MMVP\u57fa\u51c6\u95ee\u9898\u8f6c\u5316\u4e3a\u663e\u5f0f\u548c\u9690\u5f0f\u63d0\u793a\uff0c\u521b\u5efa\u4e86AMVICC\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57289\u4e2a\u89c6\u89c9\u63a8\u7406\u7c7b\u522b\u4e2d\u6d4b\u8bd5\u4e8611\u4e2aMLLM\u548c3\u4e2aIGM\uff0c\u7cfb\u7edf\u5206\u6790\u8de8\u6a21\u6001\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5931\u8d25\u6a21\u5f0f\u5728\u6a21\u578b\u548c\u6a21\u6001\u95f4\u65e2\u6709\u5171\u4eab\u4e5f\u6709\u7279\u5f02\u6027\uff1bIGM\u5728\u5904\u7406\u663e\u5f0f\u63d0\u793a\u65f6\u5c24\u5176\u96be\u4ee5\u64cd\u63a7\u7279\u5b9a\u89c6\u89c9\u7ec4\u4ef6\uff0c\u8868\u660e\u5bf9\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c5e\u6027\u7684\u63a7\u5236\u80fd\u529b\u8f83\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u8de8\u6a21\u6001\u5bf9\u9f50\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u63a2\u7a76\u751f\u6210\u548c\u89e3\u91ca\u5931\u8d25\u662f\u5426\u6e90\u4e8e\u5171\u4eab\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u6307\u5bfc\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u5efa\u6a21\u7684\u6539\u8fdb\u3002"}}
{"id": "2601.17000", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17000", "abs": "https://arxiv.org/abs/2601.17000", "authors": ["Jie Gao", "Shasha Li", "Jianhua Zhang", "Shan Li", "Tingting Wang"], "title": "Investigating Self-regulated Learning Sequences within a Generative AI-based Intelligent Tutoring System", "comment": null, "summary": "There has been a growing trend in employing generative artificial intelligence (GenAI) techniques to support learning. Moreover, scholars have reached a consensus on the critical role of self-regulated learning (SRL) in ensuring learning effectiveness within GenAI-assisted learning environments, making it essential to capture students' dynamic SRL patterns. In this study, we extracted students' interaction patterns with GenAI from trace data as they completed a problem-solving task within a GenAI-assisted intelligent tutoring system. Students' purpose of using GenAI was also analyzed from the perspective of information processing, i.e., information acquisition and information transformation. Using sequential and clustering analysis, this study classified participants into two groups based on their SRL sequences. These two groups differed in the frequency and temporal characteristics of GenAI use. In addition, most students used GenAI for information acquisition rather than information transformation, while the correlation between the purpose of using GenAI and learning performance was not statistically significant. Our findings inform both pedagogical design and the development of GenAI-assisted learning environments.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5b66\u751f\u5728GenAI\u8f85\u52a9\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u6570\u636e\uff0c\u8bc6\u522b\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u81ea\u6211\u8c03\u8282\u5b66\u4e60\u6a21\u5f0f\uff0c\u53d1\u73b0\u5b66\u751f\u4e3b\u8981\u4f7f\u7528GenAI\u8fdb\u884c\u4fe1\u606f\u83b7\u53d6\u800c\u975e\u4fe1\u606f\u8f6c\u5316\uff0c\u4f46\u4f7f\u7528\u76ee\u7684\u4e0e\u5b66\u4e60\u8868\u73b0\u65e0\u663e\u8457\u76f8\u5173\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u5b66\u8005\u4eec\u8ba4\u8bc6\u5230\u81ea\u6211\u8c03\u8282\u5b66\u4e60\u5728GenAI\u8f85\u52a9\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u56e0\u6b64\u9700\u8981\u6355\u6349\u5b66\u751f\u52a8\u6001\u7684SRL\u6a21\u5f0f\u4ee5\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "method": "\u4ece\u5b66\u751f\u5728GenAI\u8f85\u52a9\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u5b8c\u6210\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\u7684\u8ffd\u8e2a\u6570\u636e\u4e2d\u63d0\u53d6\u4ea4\u4e92\u6a21\u5f0f\uff0c\u4ece\u4fe1\u606f\u5904\u7406\u89d2\u5ea6\uff08\u4fe1\u606f\u83b7\u53d6\u548c\u4fe1\u606f\u8f6c\u5316\uff09\u5206\u6790\u4f7f\u7528\u76ee\u7684\uff0c\u91c7\u7528\u5e8f\u5217\u5206\u6790\u548c\u805a\u7c7b\u5206\u6790\u65b9\u6cd5\u5bf9\u5b66\u751f\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5c06\u53c2\u4e0e\u8005\u5206\u4e3a\u4e24\u7ec4\u4e0d\u540c\u7684SRL\u5e8f\u5217\u6a21\u5f0f\uff0c\u8fd9\u4e24\u7ec4\u5728GenAI\u4f7f\u7528\u7684\u9891\u7387\u548c\u65f6\u95f4\u7279\u5f81\u4e0a\u5b58\u5728\u5dee\u5f02\uff1b\u5927\u591a\u6570\u5b66\u751f\u4f7f\u7528GenAI\u8fdb\u884c\u4fe1\u606f\u83b7\u53d6\u800c\u975e\u4fe1\u606f\u8f6c\u5316\uff1bGenAI\u4f7f\u7528\u76ee\u7684\u4e0e\u5b66\u4e60\u8868\u73b0\u4e4b\u95f4\u65e0\u7edf\u8ba1\u5b66\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aGenAI\u8f85\u52a9\u5b66\u4e60\u73af\u5883\u7684\u6559\u5b66\u8bbe\u8ba1\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\uff0c\u5f3a\u8c03\u4e86\u7406\u89e3\u5b66\u751fSRL\u6a21\u5f0f\u7684\u91cd\u8981\u6027\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22GenAI\u4f7f\u7528\u76ee\u7684\u4e0e\u5b66\u4e60\u6548\u679c\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2601.17009", "categories": ["cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.17009", "abs": "https://arxiv.org/abs/2601.17009", "authors": ["Yanhua Zhao"], "title": "Online parameter estimation for the Crazyflie quadcopter through an EM algorithm", "comment": "20 pages, 37 figures", "summary": "Drones are becoming more and more popular nowadays. They are small in size, low in cost, and reliable in operation. They contain a variety of sensors and can perform a variety of flight tasks, reaching places that are difficult or inaccessible for humans. Earthquakes damage a lot of infrastructure, making it impossible for rescuers to reach some areas. But drones can help. Many amateur and professional photographers like to use drones for aerial photography. Drones play a non-negligible role in agriculture and transportation too. Drones can be used to spray pesticides, and they can also transport supplies. A quadcopter is a four-rotor drone and has been studied in this paper. In this paper, random noise is added to the quadcopter system and its effects on the drone system are studied. An extended Kalman filter has been used to estimate the state based on noisy observations from the sensor. Based on a SDE system, a linear quadratic Gaussian controller has been implemented. The expectation maximization algorithm has been applied for parameter estimation of the quadcopter. The results of offline parameter estimation and online parameter estimation are presented. The results show that the online parameter estimation has a slightly larger range of convergence values than the offline parameter estimation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u6dfb\u52a0\u968f\u673a\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u57fa\u4e8eSDE\u7cfb\u7edf\u5b9e\u73b0\u7ebf\u6027\u4e8c\u6b21\u9ad8\u65af\u63a7\u5236\u5668\uff0c\u5e76\u5e94\u7528\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u65e0\u4eba\u673a\u56e0\u5176\u4f53\u79ef\u5c0f\u3001\u6210\u672c\u4f4e\u3001\u53ef\u9760\u6027\u9ad8\u800c\u65e5\u76ca\u666e\u53ca\uff0c\u5728\u5730\u9707\u6551\u63f4\u3001\u822a\u62cd\u3001\u519c\u4e1a\u548c\u8fd0\u8f93\u7b49\u9886\u57df\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002\u5730\u9707\u4f1a\u7834\u574f\u57fa\u7840\u8bbe\u65bd\uff0c\u4f7f\u6551\u63f4\u4eba\u5458\u96be\u4ee5\u5230\u8fbe\u67d0\u4e9b\u533a\u57df\uff0c\u800c\u65e0\u4eba\u673a\u53ef\u4ee5\u63d0\u4f9b\u5e2e\u52a9\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u968f\u673a\u566a\u58f0\u5bf9\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "method": "1. \u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u6dfb\u52a0\u968f\u673a\u566a\u58f0\uff1b2. \u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u57fa\u4e8e\u4f20\u611f\u5668\u566a\u58f0\u89c2\u6d4b\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff1b3. \u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u5b9e\u73b0\u7ebf\u6027\u4e8c\u6b21\u9ad8\u65af\u63a7\u5236\u5668\uff1b4. \u5e94\u7528\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u8fdb\u884c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u53c2\u6570\u4f30\u8ba1\uff1b5. \u6bd4\u8f83\u79bb\u7ebf\u53c2\u6570\u4f30\u8ba1\u548c\u5728\u7ebf\u53c2\u6570\u4f30\u8ba1\u7684\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u7ebf\u53c2\u6570\u4f30\u8ba1\u7684\u6536\u655b\u503c\u8303\u56f4\u7565\u5927\u4e8e\u79bb\u7ebf\u53c2\u6570\u4f30\u8ba1\u3002\u8fd9\u8868\u660e\u5728\u7ebf\u53c2\u6570\u4f30\u8ba1\u5728\u5e94\u5bf9\u7cfb\u7edf\u53d8\u5316\u65b9\u9762\u53ef\u80fd\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u3001\u7ebf\u6027\u4e8c\u6b21\u9ad8\u65af\u63a7\u5236\u5668\u548c\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u7684\u7ed3\u5408\u5e94\u7528\uff0c\u53ef\u4ee5\u6709\u6548\u5904\u7406\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u968f\u673a\u566a\u58f0\u95ee\u9898\u3002\u5728\u7ebf\u53c2\u6570\u4f30\u8ba1\u76f8\u6bd4\u79bb\u7ebf\u53c2\u6570\u4f30\u8ba1\u5177\u6709\u66f4\u5bbd\u7684\u6536\u655b\u8303\u56f4\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2601.17057", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17057", "abs": "https://arxiv.org/abs/2601.17057", "authors": ["Zhikai Wang", "Weihua Zhang"], "title": "Frequency-aware Adaptive Contrastive Learning for Sequential Recommendation", "comment": "10 pages, 6 figures", "summary": "In this paper, we revisited the role of data augmentation in contrastive learning for sequential recommendation, revealing its inherent bias against low-frequency items and sparse user behaviors. To address this limitation, we proposed FACL, a frequency-aware adaptive contrastive learning framework that introduces micro-level adaptive perturbation to protect the integrity of rare items, as well as macro-level reweighting to amplify the influence of sparse and rare-interaction sequences during training. Comprehensive experiments on five public benchmark datasets demonstrated that FACL consistently outperforms state-of-the-art data augmentation and model augmentation-based methods, achieving up to 3.8% improvement in recommendation accuracy. Moreover, fine-grained analyses confirm that FACL significantly alleviates the performance drop on low-frequency items and users, highlighting its robust intent-preserving ability and its superior applicability to real-world, long-tail recommendation scenarios.", "AI": {"tldr": "FACL\u6846\u67b6\u901a\u8fc7\u9891\u7387\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5e8f\u5217\u63a8\u8350\u4e2d\u6570\u636e\u589e\u5f3a\u5bf9\u4f4e\u9891\u7269\u54c1\u548c\u7a00\u758f\u7528\u6237\u884c\u4e3a\u7684\u504f\u89c1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u5e8f\u5217\u63a8\u8350\u4e2d\u7684\u5bf9\u6bd4\u5b66\u4e60\u6570\u636e\u589e\u5f3a\u5b58\u5728\u56fa\u6709\u504f\u89c1\uff0c\u4f1a\u635f\u5bb3\u4f4e\u9891\u7269\u54c1\u548c\u7a00\u758f\u7528\u6237\u884c\u4e3a\u7684\u5b8c\u6574\u6027\uff0c\u5bfc\u81f4\u63a8\u8350\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u4e16\u754c\u957f\u5c3e\u63a8\u8350\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faFACL\u6846\u67b6\uff1a1\uff09\u5fae\u89c2\u5c42\u9762\u7684\u81ea\u9002\u5e94\u6270\u52a8\u4fdd\u62a4\u7a00\u6709\u7269\u54c1\u5b8c\u6574\u6027\uff1b2\uff09\u5b8f\u89c2\u5c42\u9762\u7684\u91cd\u65b0\u52a0\u6743\u653e\u5927\u7a00\u758f\u548c\u7a00\u6709\u4ea4\u4e92\u5e8f\u5217\u5728\u8bad\u7ec3\u4e2d\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFACL\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u63a8\u8350\u51c6\u786e\u6027\u63d0\u5347\u9ad8\u8fbe3.8%\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u4f4e\u9891\u7269\u54c1\u548c\u7528\u6237\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "FACL\u6846\u67b6\u5177\u6709\u5f3a\u5927\u7684\u610f\u56fe\u4fdd\u6301\u80fd\u529b\u548c\u5bf9\u771f\u5b9e\u4e16\u754c\u957f\u5c3e\u63a8\u8350\u573a\u666f\u7684\u4f18\u8d8a\u9002\u7528\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e8f\u5217\u63a8\u8350\u4e2d\u5bf9\u6bd4\u5b66\u4e60\u7684\u6570\u636e\u589e\u5f3a\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2601.17001", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.17001", "abs": "https://arxiv.org/abs/2601.17001", "authors": ["Sonia Katyal"], "title": "Lex Reformatica: Five Principles of Policy Reform for the Technological Age", "comment": "Berkeley Technology Law Journal, Forthcoming (November 1, 2022)", "summary": "Twenty-five years ago, Joel Reidenberg argued that technology itself, not just law and regulation, imposes rules on communities in the Information Society. System design choices like network architecture and configurations create regulatory norms he termed \"Lex Informatica\"-referencing the merchant-driven medieval \"Lex Mercatoria\" that emerged independent of sovereign control. Today we face different challenges requiring us to revisit Reidenberg's insights and examine the consequences of that earlier era. While Lex Informatica provided a framework for analyzing the internet's birth, we now confront the aftereffects of decades of minimal or absent regulation. Critical questions emerge: When technological social norms develop outside clear legal restraints, who benefits and who suffers? This new era demands infrastructural reform focused on the interplay between public and private regulation and self-regulation, weighing both costs and benefits. Rather than showcasing the promise of yesterday's internet age, today's events reveal the pitfalls of information libertarianism and underscore the urgent need for new approaches to information regulation. This Issue presents articles from two symposiums-one on Lex Informatica and another on race and technology law. Their conversation is now essential. Together, these papers demonstrate what I call the \"Lex Reformatica\" of today's digital age. This collection shows why scholars, lawyers, and legislators must return to Reidenberg's foundational work and update its trajectory toward a reform-focused approach designed for our current era.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86Joel Reidenberg 25\u5e74\u524d\u63d0\u51fa\u7684\"Lex Informatica\"\u6982\u5ff5\uff0c\u6307\u51fa\u6280\u672f\u672c\u8eab\u50cf\u6cd5\u5f8b\u4e00\u6837\u89c4\u8303\u4fe1\u606f\u793e\u4f1a\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f53\u4eca\u6570\u5b57\u65f6\u4ee3\u9700\u8981\u4ece\u4fe1\u606f\u81ea\u7531\u4e3b\u4e49\u8f6c\u5411\u6539\u9769\u5bfc\u5411\u7684\u76d1\u7ba1\u65b0\u65b9\u6cd5\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6Reidenberg\u7684\"Lex Informatica\"\u6982\u5ff5\uff0c\u5206\u6790\u5728\u7f3a\u4e4f\u660e\u786e\u6cd5\u5f8b\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u6280\u672f\u89c4\u8303\u53d1\u5c55\u7684\u540e\u679c\uff0c\u63a2\u8ba8\u8c01\u53d7\u76ca\u8c01\u53d7\u5bb3\uff0c\u5e76\u5f3a\u8c03\u5f53\u4eca\u6570\u5b57\u65f6\u4ee3\u9700\u8981\u57fa\u7840\u8bbe\u65bd\u6539\u9769\u548c\u65b0\u7684\u76d1\u7ba1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u4e13\u9898\u7814\u8ba8\u4f1a\uff08Lex Informatica\u4e13\u9898\u548c\u79cd\u65cf\u4e0e\u6280\u672f\u6cd5\u4e13\u9898\uff09\u7684\u8bba\u6587\u96c6\u5408\uff0c\u8fdb\u884c\u8de8\u5b66\u79d1\u5bf9\u8bdd\uff0c\u63d0\u51fa\"Lex Reformatica\"\u6982\u5ff5\uff0c\u5021\u5bfc\u56de\u5f52Reidenberg\u7684\u57fa\u7840\u5de5\u4f5c\u5e76\u66f4\u65b0\u5176\u8f68\u8ff9\u3002", "result": "\u63ed\u793a\u4e86\u4fe1\u606f\u81ea\u7531\u4e3b\u4e49\u7684\u7f3a\u9677\uff0c\u5f3a\u8c03\u516c\u5171\u4e0e\u79c1\u4eba\u76d1\u7ba1\u3001\u81ea\u6211\u76d1\u7ba1\u4e4b\u95f4\u4e92\u52a8\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u5b66\u8005\u3001\u5f8b\u5e08\u548c\u7acb\u6cd5\u8005\u9700\u8981\u91c7\u53d6\u6539\u9769\u5bfc\u5411\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u5f53\u524d\u6570\u5b57\u65f6\u4ee3\u7684\u6311\u6218\u3002", "conclusion": "\u5f53\u4eca\u6570\u5b57\u65f6\u4ee3\u9700\u8981\u4ece\"Lex Informatica\"\u8f6c\u5411\"Lex Reformatica\"\uff0c\u5373\u6539\u9769\u5bfc\u5411\u7684\u76d1\u7ba1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5b66\u8005\u3001\u5f8b\u5e08\u548c\u7acb\u6cd5\u8005\u5fc5\u987b\u56de\u5f52Reidenberg\u7684\u57fa\u7840\u5de5\u4f5c\u5e76\u66f4\u65b0\u5176\u8f68\u8ff9\uff0c\u4ee5\u5e94\u5bf9\u5f53\u524d\u4fe1\u606f\u793e\u4f1a\u7684\u6311\u6218\u3002"}}
{"id": "2601.17168", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17168", "abs": "https://arxiv.org/abs/2601.17168", "authors": ["Judy Zhu", "Dhari Gandhi", "Himanshu Joshi", "Ahmad Rezaie Mianroodi", "Sedef Akinli Kocak", "Dhanesh Ramachandran"], "title": "Interpreting Agentic Systems: Beyond Model Explanations to System-Level Accountability", "comment": null, "summary": "Agentic systems have transformed how Large Language Models (LLMs) can be leveraged to create autonomous systems with goal-directed behaviors, consisting of multi-step planning and the ability to interact with different environments. These systems differ fundamentally from traditional machine learning models, both in architecture and deployment, introducing unique AI safety challenges, including goal misalignment, compounding decision errors, and coordination risks among interacting agents, that necessitate embedding interpretability and explainability by design to ensure traceability and accountability across their autonomous behaviors. Current interpretability techniques, developed primarily for static models, show limitations when applied to agentic systems. The temporal dynamics, compounding decisions, and context-dependent behaviors of agentic systems demand new analytical approaches. This paper assesses the suitability and limitations of existing interpretability methods in the context of agentic systems, identifying gaps in their capacity to provide meaningful insight into agent decision-making. We propose future directions for developing interpretability techniques specifically designed for agentic systems, pinpointing where interpretability is required to embed oversight mechanisms across the agent lifecycle from goal formation, through environmental interaction, to outcome evaluation. These advances are essential to ensure the safe and accountable deployment of agentic AI systems.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6027\u6280\u672f\u7684\u672a\u6765\u65b9\u5411\uff0c\u4ee5\u786e\u4fdd\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5b89\u5168\u53ef\u9760\u90e8\u7f72\u3002", "motivation": "\u667a\u80fd\u4f53\u7cfb\u7edf\u4ece\u6839\u672c\u4e0a\u4e0d\u540c\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u67b6\u6784\u548c\u90e8\u7f72\u4e0a\u90fd\u5f15\u5165\u4e86\u72ec\u7279\u7684AI\u5b89\u5168\u6311\u6218\uff0c\u5305\u62ec\u76ee\u6807\u9519\u4f4d\u3001\u51b3\u7b56\u9519\u8bef\u7d2f\u79ef\u548c\u667a\u80fd\u4f53\u95f4\u534f\u8c03\u98ce\u9669\u3002\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u6a21\u578b\u5f00\u53d1\uff0c\u5728\u5e94\u7528\u4e8e\u667a\u80fd\u4f53\u7cfb\u7edf\u65f6\u663e\u793a\u51fa\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u80cc\u666f\u4e0b\u7684\u9002\u7528\u6027\u548c\u5c40\u9650\u6027\uff0c\u8bc6\u522b\u4e86\u5b83\u4eec\u5728\u63d0\u4f9b\u667a\u80fd\u4f53\u51b3\u7b56\u6709\u610f\u4e49\u7684\u6d1e\u5bdf\u65b9\u9762\u7684\u80fd\u529b\u5dee\u8ddd\u3002\u63d0\u51fa\u4e86\u4e13\u95e8\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6027\u6280\u672f\u7684\u672a\u6765\u65b9\u5411\u3002", "result": "\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u65f6\u95f4\u52a8\u6001\u6027\u3001\u51b3\u7b56\u7d2f\u79ef\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u884c\u4e3a\u9700\u8981\u65b0\u7684\u5206\u6790\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u667a\u80fd\u4f53\u7cfb\u7edf\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u63d0\u4f9b\u5bf9\u667a\u80fd\u4f53\u51b3\u7b56\u8fc7\u7a0b\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u5728\u667a\u80fd\u4f53\u751f\u547d\u5468\u671f\u7684\u5404\u4e2a\u9636\u6bb5\uff08\u4ece\u76ee\u6807\u5f62\u6210\u3001\u73af\u5883\u4ea4\u4e92\u5230\u7ed3\u679c\u8bc4\u4f30\uff09\u5d4c\u5165\u76d1\u7763\u673a\u5236\u3002\u8fd9\u4e9b\u8fdb\u5c55\u5bf9\u4e8e\u786e\u4fdd\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5b89\u5168\u548c\u53ef\u9760\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.17218", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17218", "abs": "https://arxiv.org/abs/2601.17218", "authors": ["Zihan Huang", "Rohan Surana", "Zhouhang Xie", "Junda Wu", "Yu Xia", "Julian McAuley"], "title": "Evaluation on Entity Matching in Recommender Systems", "comment": null, "summary": "Entity matching is a crucial component in various recommender systems, including conversational recommender systems (CRS) and knowledge-based recommender systems. However, the lack of rigorous evaluation frameworks for cross-dataset entity matching impedes progress in areas such as LLM-driven conversational recommendations and knowledge-grounded dataset construction.\n  In this paper, we introduce Reddit-Amazon-EM, a novel dataset comprising naturally occurring items from Reddit and the Amazon '23 dataset. Through careful manual annotation, we identify corresponding movies across Reddit-Movies and Amazon'23, two existing recommender system datasets with inherently overlapping catalogs. Leveraging Reddit-Amazon-EM, we conduct a comprehensive evaluation of state-of-the-art entity matching methods, including rule-based, graph-based, lexical-based, embedding-based, and LLM-based approaches.\n  For reproducible research, we release our manually annotated entity matching gold set and provide the mapping between the two datasets using the best-performing method from our experiments. This serves as a valuable resource for advancing future work on entity matching in recommender systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Reddit-Amazon-EM\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u6570\u636e\u96c6\u5b9e\u4f53\u5339\u914d\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7528\u4e8e\u8de8\u6570\u636e\u96c6\u5b9e\u4f53\u5339\u914d\u7684\u4e25\u683c\u8bc4\u4f30\u6846\u67b6\uff0c\u8fd9\u963b\u788d\u4e86LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u63a8\u8350\u548c\u57fa\u4e8e\u77e5\u8bc6\u7684\u6570\u636e\u96c6\u6784\u5efa\u7b49\u9886\u57df\u7684\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u6807\u6ce8Reddit\u548cAmazon'23\u6570\u636e\u96c6\u4e2d\u7684\u81ea\u7136\u51fa\u73b0\u9879\u76ee\uff0c\u521b\u5efaReddit-Amazon-EM\u6570\u636e\u96c6\uff0c\u5e76\u5168\u9762\u8bc4\u4f30\u57fa\u4e8e\u89c4\u5219\u3001\u56fe\u3001\u8bcd\u6cd5\u3001\u5d4c\u5165\u548cLLM\u7684\u5b9e\u4f53\u5339\u914d\u65b9\u6cd5\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u624b\u52a8\u6807\u6ce8\u5b9e\u4f53\u5339\u914d\u9ec4\u91d1\u96c6\u7684Reddit-Amazon-EM\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6700\u4f73\u6027\u80fd\u65b9\u6cd5\u7684\u4e24\u4e2a\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb\u3002", "conclusion": "Reddit-Amazon-EM\u6570\u636e\u96c6\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5b9e\u4f53\u5339\u914d\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2601.17039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17039", "abs": "https://arxiv.org/abs/2601.17039", "authors": ["Junhyuk Heo", "Beomkyu Choi", "Hyunjin Shin", "Darongsae Kwon"], "title": "MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation", "comment": null, "summary": "Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.", "AI": {"tldr": "\u7814\u7a76\u8005\u6784\u5efa\u4e86MANGO\u5168\u7403\u7ea2\u6811\u6797\u6570\u636e\u96c6\uff0c\u5305\u542b42,703\u4e2a\u6807\u6ce8\u56fe\u50cf-\u63a9\u7801\u5bf9\uff0c\u8986\u76d6124\u4e2a\u56fd\u5bb6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u65f6\u6548\u6027\u3001\u8986\u76d6\u8303\u56f4\u548c\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u7ea2\u6811\u6797\u5bf9\u6c14\u5019\u53d8\u5316\u51cf\u7f13\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6570\u636e\u96c6\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a1) \u4ec5\u63d0\u4f9b\u5e74\u5ea6\u5730\u56fe\u4ea7\u54c1\u800c\u975e\u5355\u65e5\u671f\u56fe\u50cf-\u63a9\u7801\u5bf9\uff1b2) \u5c40\u9650\u4e8e\u7279\u5b9a\u533a\u57df\u800c\u975e\u5168\u7403\u8986\u76d6\uff1b3) \u8bb8\u591a\u8d44\u6e90\u4e0d\u5411\u516c\u4f17\u5f00\u653e\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u7ea2\u6811\u6797\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\u3002", "method": "1) \u6536\u96c62020\u5e74\u7ea2\u6811\u6797\u533a\u57df\u7684Sentinel-2\u5f71\u50cf\uff1b2) \u91c7\u7528\u76ee\u6807\u68c0\u6d4b\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u50cf\u7d20\u7ea7\u5750\u6807\u53c2\u8003\uff0c\u9009\u62e9\u4e0e\u5e74\u5ea6\u7ea2\u6811\u6797\u63a9\u7801\u6700\u4f73\u5339\u914d\u7684\u5355\u65e5\u671f\u89c2\u6d4b\uff1b3) \u6784\u5efa\u5305\u542b42,703\u4e2a\u6807\u6ce8\u56fe\u50cf-\u63a9\u7801\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d6124\u4e2a\u56fd\u5bb6\uff1b4) \u63d0\u4f9b\u57fa\u4e8e\u56fd\u5bb6\u5206\u79bb\u5212\u5206\u7684\u591a\u79cd\u8bed\u4e49\u5206\u5272\u67b6\u6784\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86MANGO\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u76ee\u524d\u89c4\u6a21\u6700\u5927\u3001\u8986\u76d6\u6700\u5e7f\u7684\u5168\u7403\u7ea2\u6811\u6797\u6570\u636e\u96c6\uff0c\u5305\u542b42,703\u4e2a\u9ad8\u8d28\u91cf\u56fe\u50cf-\u63a9\u7801\u5bf9\uff0c\u4e3a\u5168\u7403\u7ea2\u6811\u6797\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u57fa\u7840\u3002", "conclusion": "MANGO\u6570\u636e\u96c6\u89e3\u51b3\u4e86\u73b0\u6709\u7ea2\u6811\u6797\u76d1\u6d4b\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684\u5168\u7403\u7ea2\u6811\u6797\u76d1\u6d4b\u5efa\u7acb\u4e86\u57fa\u7840\uff0c\u5c06\u4fc3\u8fdb\u6df1\u5ea6\u5b66\u4e60\u5728\u7ea2\u6811\u6797\u4fdd\u62a4\u548c\u6c14\u5019\u53d8\u5316\u51cf\u7f13\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2601.17003", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17003", "abs": "https://arxiv.org/abs/2601.17003", "authors": ["Caitlin A. Stamatis", "Jonah Meyerhoff", "Richard Zhang", "Olivier Tieleman", "Matteo Malgaroli", "Thomas D. Hull"], "title": "Beyond Simulations: What 20,000 Real Conversations Reveal About Mental Health AI Safety", "comment": "38 pages, 8 figures", "summary": "Large language models (LLMs) are increasingly used for mental health support, yet existing safety evaluations rely primarily on small, simulation-based test sets that have an unknown relationship to the linguistic distribution of real usage. In this study, we present replications of four published safety test sets targeting suicide risk assessment, harmful content generation, refusal robustness, and adversarial jailbreaks for a leading frontier generic AI model alongside an AI purpose built for mental health support. We then propose and conduct an ecological audit on over 20,000 real-world user conversations with the purpose-built AI designed with layered suicide and non-suicidal self-injury (NSSI) safeguards to compare test set performance to real world performance. While the purpose-built AI was significantly less likely than general-purpose LLMs to produce enabling or harmful content across suicide/NSSI (.4-11.27% vs 29.0-54.4%), eating disorder (8.4% vs 54.0%), and substance use (9.9% vs 45.0%) benchmark prompts, test set failure rates for suicide/NSSI were far higher than in real-world deployment. Clinician review of flagged conversations from the ecological audit identified zero cases of suicide risk that failed to receive crisis resources. Across all 20,000 conversations, three mentions of NSSI risk (.015%) did not trigger a crisis intervention; among sessions flagged by the LLM judge, this corresponds to an end-to-end system false negative rate of .38%, providing a lower bound on real-world safety failures. These findings support a shift toward continuous, deployment-relevant safety assurance for AI mental-health systems rather than limited set benchmark certification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86AI\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u4e13\u95e8\u6784\u5efa\u7684AI\u5728\u81ea\u6740/\u81ea\u4f24\u3001\u996e\u98df\u969c\u788d\u548c\u7269\u8d28\u6ee5\u7528\u7b49\u5b89\u5168\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d4b\u8bd5\u96c6\u5931\u8d25\u7387\u8fdc\u9ad8\u4e8e\u771f\u5b9e\u90e8\u7f72\u60c5\u51b5\uff0c\u652f\u6301\u8f6c\u5411\u6301\u7eed\u3001\u90e8\u7f72\u76f8\u5173\u7684\u5b89\u5168\u4fdd\u8bc1\u800c\u975e\u6709\u9650\u57fa\u51c6\u8ba4\u8bc1\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u652f\u6301\uff0c\u4f46\u73b0\u6709\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u5c0f\u578b\u3001\u57fa\u4e8e\u6a21\u62df\u7684\u6d4b\u8bd5\u96c6\uff0c\u8fd9\u4e9b\u6d4b\u8bd5\u96c6\u4e0e\u771f\u5b9e\u4f7f\u7528\u573a\u666f\u7684\u8bed\u8a00\u5206\u5e03\u5173\u7cfb\u672a\u77e5\u3002\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u6d4b\u8bd5\u96c6\u6027\u80fd\u4e0e\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "1) \u590d\u5236\u4e86\u56db\u4e2a\u5df2\u53d1\u5e03\u7684\u5b89\u5168\u6d4b\u8bd5\u96c6\uff08\u81ea\u6740\u98ce\u9669\u8bc4\u4f30\u3001\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3001\u62d2\u7edd\u9c81\u68d2\u6027\u3001\u5bf9\u6297\u6027\u8d8a\u72f1\uff09\uff0c\u6bd4\u8f83\u524d\u6cbf\u901a\u7528AI\u6a21\u578b\u4e0e\u4e13\u95e8\u6784\u5efa\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301AI\uff1b2) \u5bf9\u8d85\u8fc720,000\u4e2a\u771f\u5b9e\u7528\u6237\u5bf9\u8bdd\u8fdb\u884c\u751f\u6001\u5ba1\u8ba1\uff0c\u5206\u6790\u4e13\u95e8\u6784\u5efa\u7684AI\u5728\u5206\u5c42\u81ea\u6740\u548c\u975e\u81ea\u6740\u6027\u81ea\u4f24\u9632\u62a4\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4e13\u95e8\u6784\u5efa\u7684AI\u5728\u81ea\u6740/\u81ea\u4f24\uff080.4-11.27% vs 29.0-54.4%\uff09\u3001\u996e\u98df\u969c\u788d\uff088.4% vs 54.0%\uff09\u548c\u7269\u8d28\u6ee5\u7528\uff089.9% vs 45.0%\uff09\u6d4b\u8bd5\u4e2d\u4ea7\u751f\u6709\u5bb3\u5185\u5bb9\u7684\u53ef\u80fd\u6027\u663e\u8457\u4f4e\u4e8e\u901a\u7528\u6a21\u578b\u3002\u4f46\u6d4b\u8bd5\u96c6\u5931\u8d25\u7387\u8fdc\u9ad8\u4e8e\u771f\u5b9e\u90e8\u7f72\u60c5\u51b5\u3002\u4e34\u5e8a\u533b\u751f\u5ba1\u67e5\u6807\u8bb0\u5bf9\u8bdd\u53d1\u73b0\u96f6\u4f8b\u81ea\u6740\u98ce\u9669\u672a\u83b7\u5f97\u5371\u673a\u8d44\u6e90\u3002\u5728\u6240\u670920,000\u4e2a\u5bf9\u8bdd\u4e2d\uff0c\u53ea\u6709\u4e09\u4f8b\u81ea\u4f24\u98ce\u9669\uff080.015%\uff09\u672a\u89e6\u53d1\u5371\u673a\u5e72\u9884\uff0c\u5bf9\u5e94\u7aef\u5230\u7aef\u7cfb\u7edf\u5047\u9634\u6027\u7387\u4e3a0.38%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u4ece\u6709\u9650\u7684\u57fa\u51c6\u8ba4\u8bc1\u8f6c\u5411\u6301\u7eed\u3001\u90e8\u7f72\u76f8\u5173\u7684AI\u5fc3\u7406\u5065\u5eb7\u7cfb\u7edf\u5b89\u5168\u4fdd\u8bc1\uff0c\u5f3a\u8c03\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.17333", "categories": ["cs.IR", "cs.AI", "cs.CE", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17333", "abs": "https://arxiv.org/abs/2601.17333", "authors": ["Lalit Pant", "Shivang Nagar"], "title": "FinMetaMind: A Tech Blueprint on NLQ Systems for Financial Knowledge Search", "comment": "8 pages, 8 figures, Information Retrieval, Natural Language Query, Vector Search, Embeddings, Named Entity Recognition, Large Language Models", "summary": "Natural Language Query (NLQ) allows users to search and interact with information systems using plain, human language instead of structured query syntax. This paper presents a technical blueprint on the design of a modern NLQ system tailored to financial knowledge search. The introduction of NLQ not only enhances the precision and recall of the knowledge search compared to traditional methods, but also facilitates deeper insights by efficiently linking disparate financial objects, events, and relationships. Using core constructs from natural language processing, search engineering, and vector data models, the proposed system aims to address key challenges in discovering, relevance ranking, data freshness, and entity recognition intrinsic to financial data retrieval. In this work, we detail the unique requirements of NLQ for financial datasets and documents, outline the architectural components for offline indexing and online retrieval, and discuss the real-world use cases of enhanced knowledge search in financial services. We delve into the theoretical underpinnings and experimental evidence supporting our proposed architecture, ultimately providing a comprehensive analysis on the subject matter. We also provide a detailed elaboration of our experimental methodology, the data used, the results and future optimizations in this study.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u91d1\u878d\u77e5\u8bc6\u641c\u7d22\u7684\u73b0\u4ee3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7cfb\u7edf\u6280\u672f\u84dd\u56fe\uff0c\u901a\u8fc7\u7ed3\u5408NLP\u3001\u641c\u7d22\u5de5\u7a0b\u548c\u5411\u91cf\u6570\u636e\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u91d1\u878d\u6570\u636e\u68c0\u7d22\u4e2d\u7684\u53d1\u73b0\u3001\u76f8\u5173\u6027\u6392\u5e8f\u3001\u6570\u636e\u65b0\u9c9c\u5ea6\u548c\u5b9e\u4f53\u8bc6\u522b\u7b49\u5173\u952e\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u91d1\u878d\u77e5\u8bc6\u641c\u7d22\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6709\u6548\u8fde\u63a5\u5206\u6563\u7684\u91d1\u878d\u5bf9\u8c61\u3001\u4e8b\u4ef6\u548c\u5173\u7cfb\u3002\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u80fd\u591f\u63d0\u5347\u641c\u7d22\u6548\u679c\uff0c\u4fc3\u8fdb\u66f4\u6df1\u5165\u7684\u91d1\u878d\u6d1e\u5bdf\u3002", "method": "\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u641c\u7d22\u5de5\u7a0b\u548c\u5411\u91cf\u6570\u636e\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u5305\u542b\u79bb\u7ebf\u7d22\u5f15\u548c\u5728\u7ebf\u68c0\u7d22\u67b6\u6784\u7ec4\u4ef6\u7684\u7cfb\u7edf\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u91d1\u878d\u6570\u636e\u96c6\u548c\u6587\u6863\u7684\u72ec\u7279\u9700\u6c42\u3002", "result": "\u63d0\u51fa\u7684NLQ\u7cfb\u7edf\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u90fd\u6709\u63d0\u5347\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8fde\u63a5\u91d1\u878d\u5bf9\u8c61\u3001\u4e8b\u4ef6\u548c\u5173\u7cfb\uff0c\u4e3a\u91d1\u878d\u670d\u52a1\u63d0\u4f9b\u589e\u5f3a\u7684\u77e5\u8bc6\u641c\u7d22\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91d1\u878d\u9886\u57df\u7684\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6280\u672f\u84dd\u56fe\u548c\u7406\u8bba\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5176\u5728\u91d1\u878d\u670d\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2601.17040", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17040", "abs": "https://arxiv.org/abs/2601.17040", "authors": ["H Neji", "J Nogueras-Iso", "J Lacasta", "M\u00c1 Latre", "FJ Garc\u00eda-Marco"], "title": "FP-THD: Full page transcription of historical documents", "comment": "Figure 1: FP-THD architecture Overview: Layout Analysis and Masked Auto-encoder with Vision Trans- former", "summary": "The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u8f6c\u5f5515-16\u4e16\u7eaa\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u5e03\u5c40\u5206\u6790\u6a21\u578b\u63d0\u53d6\u6587\u672c\u884c\uff0c\u518d\u4f7f\u7528OCR\u6a21\u578b\u8fdb\u884c\u8bc6\u522b\uff0c\u4fdd\u7559\u7279\u6b8a\u5b57\u7b26\u548c\u7b26\u53f7\u4ee5\u7ef4\u6301\u5386\u53f2\u6587\u672c\u7684\u539f\u59cb\u98ce\u683c\u548c\u610f\u4e49\u3002", "motivation": "15-16\u4e16\u7eaa\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\u7684\u8f6c\u5f55\u9762\u4e34\u7279\u6b8a\u6311\u6218\uff0c\u9700\u8981\u4fdd\u7559\u5177\u6709\u7279\u5b9a\u542b\u4e49\u7684\u5b57\u7b26\u548c\u7279\u6b8a\u7b26\u53f7\uff0c\u4ee5\u786e\u4fdd\u5386\u53f2\u6587\u672c\u4fdd\u6301\u5176\u539f\u59cb\u98ce\u683c\u548c\u610f\u4e49\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u7279\u6b8a\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8f6c\u5f55\u6d41\u7a0b\uff1a1) \u4f7f\u7528\u5e03\u5c40\u5206\u6790\u6a21\u578b\u5206\u6790\u5386\u53f2\u6587\u672c\u56fe\u50cf\u5e76\u63d0\u53d6\u6587\u672c\u884c\uff1b2) \u5c06\u63d0\u53d6\u7684\u6587\u672c\u884c\u8f93\u5165OCR\u6a21\u578b\u751f\u6210\u5b8c\u6574\u7684\u6570\u5b57\u5316\u9875\u9762\u3002\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u73b0\u6709\u7684\u6587\u672c\u884c\u8bc6\u522b\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5e03\u5c40\u5206\u6790\u6a21\u578b\u3002", "result": "\u8be5\u6d41\u7a0b\u80fd\u6709\u6548\u5904\u7406\u9875\u9762\u5e76\u4ea7\u751f\u9ad8\u6548\u7ed3\u679c\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u63a9\u7801\u81ea\u7f16\u7801\u5668\u80fd\u6709\u6548\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u6587\u672c\uff0c\u5305\u62ec\u624b\u5199\u4f53\u3001\u5370\u5237\u4f53\u548c\u591a\u8bed\u8a00\u6587\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d41\u7a0b\u80fd\u591f\u6709\u6548\u8f6c\u5f5515-16\u4e16\u7eaa\u62c9\u4e01\u6587\u5386\u53f2\u6587\u732e\uff0c\u4fdd\u7559\u7279\u6b8a\u5b57\u7b26\u548c\u7b26\u53f7\uff0c\u4e3a\u5386\u53f2\u6587\u732e\u7684\u6570\u5b57\u5316\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17005", "categories": ["cs.CY", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.17005", "abs": "https://arxiv.org/abs/2601.17005", "authors": ["Bhubalan Mani"], "title": "From Noise to Insights: Enhancing Supply Chain Decision Support through AI-Based Survey Integrity Analytics", "comment": "2025 IEEE 4th World Conference on Applied Intelligence and Computing (AIC)", "summary": "The reliability of survey data is crucial in supply chain decision-making, particularly when evaluating readiness for AI-driven tools such as safety stock optimization systems. However, surveys often attract low-effort or fake responses that degrade the accuracy of derived insights. This study proposes a lightweight AI-based framework for filtering unreliable survey inputs using a supervised machine learning approach. In this expanded study, a larger dataset of 99 industry responses was collected, with manual labeling to identify fake responses based on logical inconsistencies and response patterns. After preprocessing and label encoding, both Random Forest and baseline models (Logistic Regression, XGBoost) were trained to distinguish genuine from fake responses. The best-performing model achieved an 92.0% accuracy rate, demonstrating improved detection compared to the pilot study. Despite limitations, the results highlight the viability of integrating AI into survey pipelines and provide a scalable solution for improving data integrity in supply chain research, especially during product launch and technology adoption phases.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u8fc7\u6ee4\u4f9b\u5e94\u94fe\u8c03\u67e5\u4e2d\u7684\u4e0d\u53ef\u9760\u6570\u636e\uff0c\u901a\u8fc7\u76d1\u7763\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bc6\u522b\u865a\u5047\u54cd\u5e94\uff0c\u572899\u4e2a\u884c\u4e1a\u54cd\u5e94\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.0%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f9b\u5e94\u94fe\u51b3\u7b56\u4e2d\u8c03\u67e5\u6570\u636e\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u8bc4\u4f30AI\u9a71\u52a8\u5de5\u5177\uff08\u5982\u5b89\u5168\u5e93\u5b58\u4f18\u5316\u7cfb\u7edf\uff09\u7684\u51c6\u5907\u5ea6\u65f6\u3002\u7136\u800c\uff0c\u8c03\u67e5\u5e38\u5e38\u5438\u5f15\u4f4e\u8d28\u91cf\u6216\u865a\u5047\u54cd\u5e94\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5206\u6790\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u76d1\u7763\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u6536\u96c6\u4e8699\u4e2a\u884c\u4e1a\u54cd\u5e94\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u57fa\u4e8e\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u548c\u54cd\u5e94\u6a21\u5f0f\u8bc6\u522b\u865a\u5047\u54cd\u5e94\u3002\u7ecf\u8fc7\u9884\u5904\u7406\u548c\u6807\u7b7e\u7f16\u7801\u540e\uff0c\u8bad\u7ec3\u4e86\u968f\u673a\u68ee\u6797\u548c\u57fa\u7ebf\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001XGBoost\uff09\u6765\u533a\u5206\u771f\u5b9e\u548c\u865a\u5047\u54cd\u5e94\u3002", "result": "\u6700\u4f73\u6a21\u578b\u8fbe\u5230\u4e8692.0%\u7684\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u521d\u6b65\u7814\u7a76\u6709\u663e\u8457\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86AI\u5728\u8c03\u67e5\u6570\u636e\u8fc7\u6ee4\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u5c06AI\u96c6\u6210\u5230\u8c03\u67e5\u6d41\u7a0b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4f9b\u5e94\u94fe\u7814\u7a76\u4e2d\u63d0\u9ad8\u6570\u636e\u5b8c\u6574\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u4ea7\u54c1\u53d1\u5e03\u548c\u6280\u672f\u91c7\u7528\u9636\u6bb5\u3002"}}
{"id": "2601.17310", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17310", "abs": "https://arxiv.org/abs/2601.17310", "authors": ["Yu Akagi", "Tomohisa Seki", "Hiromasa Ito", "Toru Takiguchi", "Kazuhiko Ohe", "Yoshimasa Kawazoe"], "title": "High-Fidelity Longitudinal Patient Simulation Using Real-World Data", "comment": null, "summary": "Simulation is a powerful tool for exploring uncertainty. Its potential in clinical medicine is transformative and includes personalized treatment planning and virtual clinical trials. However, simulating patient trajectories is challenging because of complex biological and sociocultural influences. Here, we show that real-world clinical records can be leveraged to empirically model patient timelines. We developed a generative simulator model that takes a patient's history as input and synthesizes fine-grained, realistic future trajectories. The model was pretrained on more than 200 million clinical records. It produced high-fidelity future timelines, closely matching event occurrence rates, laboratory test results, and temporal dynamics in real patient future data. It also accurately estimated future event probabilities, with observed-to-expected ratios consistently near 1.0 across diverse outcomes and time horizons. Our results reveal the untapped value of real-world data in electronic health records and introduce a scalable framework for in silico modeling of clinical care.", "AI": {"tldr": "\u5229\u7528\u771f\u5b9e\u4e16\u754c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5f00\u53d1\u751f\u6210\u5f0f\u6a21\u62df\u5668\uff0c\u53ef\u57fa\u4e8e\u60a3\u8005\u5386\u53f2\u751f\u6210\u9ad8\u4fdd\u771f\u672a\u6765\u4e34\u5e8a\u8f68\u8ff9\uff0c\u4e3a\u4e2a\u6027\u5316\u6cbb\u7597\u548c\u865a\u62df\u4e34\u5e8a\u8bd5\u9a8c\u63d0\u4f9b\u65b0\u6846\u67b6\u3002", "motivation": "\u6a21\u62df\u5728\u4e34\u5e8a\u533b\u5b66\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5212\u548c\u865a\u62df\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u4f46\u6a21\u62df\u60a3\u8005\u8f68\u8ff9\u9762\u4e34\u751f\u7269\u548c\u793e\u4f1a\u6587\u5316\u56e0\u7d20\u590d\u6742\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u751f\u6210\u5f0f\u6a21\u62df\u5668\u6a21\u578b\uff0c\u4ee5\u60a3\u8005\u5386\u53f2\u4e3a\u8f93\u5165\uff0c\u5408\u6210\u7ec6\u7c92\u5ea6\u3001\u771f\u5b9e\u7684\u672a\u6765\u8f68\u8ff9\uff1b\u6a21\u578b\u5728\u8d85\u8fc72\u4ebf\u6761\u4e34\u5e8a\u8bb0\u5f55\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u672a\u6765\u65f6\u95f4\u7ebf\uff0c\u4e0e\u771f\u5b9e\u60a3\u8005\u672a\u6765\u6570\u636e\u4e2d\u7684\u4e8b\u4ef6\u53d1\u751f\u7387\u3001\u5b9e\u9a8c\u5ba4\u68c0\u6d4b\u7ed3\u679c\u548c\u65f6\u95f4\u52a8\u6001\u9ad8\u5ea6\u5339\u914d\uff1b\u51c6\u786e\u4f30\u8ba1\u672a\u6765\u4e8b\u4ef6\u6982\u7387\uff0c\u89c2\u5bdf\u4e0e\u9884\u671f\u6bd4\u7387\u5728\u4e0d\u540c\u7ed3\u679c\u548c\u65f6\u95f4\u8303\u56f4\u5185\u5747\u63a5\u8fd11.0\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u672a\u5f00\u53d1\u4ef7\u503c\uff0c\u5e76\u5f15\u5165\u4e86\u53ef\u6269\u5c55\u7684\u4e34\u5e8a\u62a4\u7406\u8ba1\u7b97\u673a\u6a21\u62df\u6846\u67b6\u3002"}}
{"id": "2601.17006", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17006", "abs": "https://arxiv.org/abs/2601.17006", "authors": ["Xuchen Li", "Jing Chen", "Xuzhao Li", "Hao Liang", "Xiaohuan Zhou", "Taifeng Wang", "Wentao Zhang"], "title": "MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning", "comment": "Preprint, Under review", "summary": "In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.", "AI": {"tldr": "MathMixup\uff1a\u901a\u8fc7\u6df7\u5408\u548c\u5206\u89e3\u7b56\u7565\u751f\u6210\u96be\u5ea6\u53ef\u63a7\u7684\u9ad8\u8d28\u91cf\u6570\u5b66\u63a8\u7406\u95ee\u9898\uff0c\u6784\u5efa\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347LLMs\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u6709\u9650\u3001\u96be\u5ea6\u63a7\u5236\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\u8bfe\u7a0b\u5b66\u4e60\u7b49\u9ad8\u6548\u8bad\u7ec3\u8303\u5f0f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u96be\u5ea6\u53ef\u63a7\u6570\u5b66\u63a8\u7406\u95ee\u9898\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51faMathMixup\u6570\u636e\u5408\u6210\u8303\u5f0f\uff0c\u901a\u8fc7\u6df7\u5408\u548c\u5206\u89e3\u7b56\u7565\u7cfb\u7edf\u751f\u6210\u96be\u5ea6\u53ef\u63a7\u7684\u6570\u5b66\u63a8\u7406\u95ee\u9898\uff1b\u91c7\u7528\u81ea\u52a8\u81ea\u68c0\u548c\u4eba\u5de5\u7b5b\u9009\u786e\u4fdd\u8bed\u4e49\u6e05\u6670\u548c\u96be\u5ea6\u68af\u5ea6\uff1b\u6784\u5efaMathMixupQA\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5fae\u8c03\u540e\u7684Qwen2.5-7B\u5728\u4e03\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5f97\u520652.6%\uff0c\u8d85\u8d8a\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86MathMixup\u5728\u63d0\u5347LLMs\u6570\u5b66\u63a8\u7406\u80fd\u529b\u548c\u63a8\u8fdb\u6570\u636e\u4e2d\u5fc3\u8bfe\u7a0b\u5b66\u4e60\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "MathMixup\u53ca\u5176\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u80fd\u663e\u8457\u589e\u5f3aLLMs\u7684\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u6570\u636e\u4e2d\u5fc3\u7684\u8bfe\u7a0b\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2601.17339", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17339", "abs": "https://arxiv.org/abs/2601.17339", "authors": ["Payel Santra", "Partha Basuchowdhuri", "Debasis Ganguly"], "title": "Beyond Correlations: A Downstream Evaluation Framework for Query Performance Prediction", "comment": null, "summary": "The standard practice of query performance prediction (QPP) evaluation is to measure a set-level correlation between the estimated retrieval qualities and the true ones. However, neither this correlation-based evaluation measure quantifies QPP effectiveness at the level of individual queries, nor does this connect to a downstream application, meaning that QPP methods yielding high correlation values may not find a practical application in query-specific decisions in an IR pipeline. In this paper, we propose a downstream-focussed evaluation framework where a distribution of QPP estimates across a list of top-documents retrieved with several rankers is used as priors for IR fusion. While on the one hand, a distribution of these estimates closely matching that of the true retrieval qualities indicates the quality of the predictor, their usage as priors on the other hand indicates a predictor's ability to make informed choices in an IR pipeline. Our experiments firstly establish the importance of QPP estimates in weighted IR fusion, yielding substantial improvements of over 4.5% over unweighted CombSUM and RRF fusion strategies, and secondly, reveal new insights that the downstream effectiveness of QPP does not correlate well with the standard correlation-based QPP evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4e0b\u6e38\u5e94\u7528\u7684\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5c06QPP\u4f30\u8ba1\u5206\u5e03\u4f5c\u4e3a\u4fe1\u606f\u68c0\u7d22\u878d\u5408\u7684\u5148\u9a8c\uff0c\u53d1\u73b0\u4f20\u7edf\u76f8\u5173\u6027\u8bc4\u4f30\u4e0e\u4e0b\u6e38\u5e94\u7528\u6548\u679c\u4e0d\u5339\u914d", "motivation": "\u4f20\u7edf\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u8bc4\u4f30\u53ea\u5173\u6ce8\u96c6\u5408\u5c42\u9762\u7684\u76f8\u5173\u6027\u5ea6\u91cf\uff0c\u65e0\u6cd5\u91cf\u5316\u5355\u4e2a\u67e5\u8be2\u7ea7\u522b\u7684\u6548\u679c\uff0c\u4e14\u4e0e\u4e0b\u6e38\u5e94\u7528\u8131\u8282\uff0c\u5bfc\u81f4\u9ad8\u76f8\u5173\u6027\u503c\u7684QPP\u65b9\u6cd5\u5728\u5b9e\u9645IR\u6d41\u6c34\u7ebf\u4e2d\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u652f\u6301\u67e5\u8be2\u7279\u5b9a\u51b3\u7b56", "method": "\u63d0\u51fa\u4e0b\u6e38\u805a\u7126\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u591a\u4e2a\u6392\u5e8f\u5668\u68c0\u7d22\u5230\u7684top\u6587\u6863\u5217\u8868\u4e2d\u7684QPP\u4f30\u8ba1\u5206\u5e03\u4f5c\u4e3aIR\u878d\u5408\u7684\u5148\u9a8c\u3002\u4e00\u65b9\u9762\uff0c\u8fd9\u4e9b\u4f30\u8ba1\u5206\u5e03\u4e0e\u771f\u5b9e\u68c0\u7d22\u8d28\u91cf\u5206\u5e03\u7684\u5339\u914d\u7a0b\u5ea6\u53cd\u6620\u9884\u6d4b\u5668\u8d28\u91cf\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u4f5c\u4e3a\u5148\u9a8c\u4f7f\u7528\u53cd\u6620\u9884\u6d4b\u5668\u5728IR\u6d41\u6c34\u7ebf\u4e2d\u505a\u51fa\u660e\u667a\u51b3\u7b56\u7684\u80fd\u529b", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) QPP\u4f30\u8ba1\u5728\u52a0\u6743IR\u878d\u5408\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u76f8\u6bd4\u672a\u52a0\u6743\u7684CombSUM\u548cRRF\u878d\u5408\u7b56\u7565\u6709\u8d85\u8fc74.5%\u7684\u663e\u8457\u6539\u8fdb\uff1b2) QPP\u7684\u4e0b\u6e38\u6548\u679c\u4e0e\u6807\u51c6\u76f8\u5173\u6027\u8bc4\u4f30\u6ca1\u6709\u826f\u597d\u76f8\u5173\u6027\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u89c1\u89e3", "conclusion": "\u9700\u8981\u91cd\u65b0\u601d\u8003QPP\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5c06\u4e0b\u6e38\u5e94\u7528\u6548\u679c\u7eb3\u5165\u8003\u91cf\uff0c\u56e0\u4e3a\u4f20\u7edf\u76f8\u5173\u6027\u8bc4\u4f30\u65e0\u6cd5\u51c6\u786e\u53cd\u6620QPP\u5728\u5b9e\u9645IR\u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2601.17011", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17011", "abs": "https://arxiv.org/abs/2601.17011", "authors": ["Javier Crespo", "Ana En\u00e9riz", "Paula Iruzubieta", "Fernando Carballo", "Conrado Fern\u00e1ndez Rodr\u00edguez", "Mar\u00eda Dolores Mart\u00edn-Arranz", "Federico Arg\u00fcelles-Arias", "Juan Turnes"], "title": "Artificial Intelligence in Spanish Gastroenterology: high expectations, limited integration. A national survey", "comment": null, "summary": "Background: Artificial intelligence (AI) has emerged as a disruptive innovation in medicine, yet its adoption within gastroenterology remains limited and poorly characterized. We aimed to examine knowledge, practical applications, perceived barriers, and expectations regarding AI among gastroenterology specialists in Spain.Methods: We conducted a cross-sectional observational study using a structured online survey distributed by the Spanish Society of Digestive Pathology (SEPD) in 2025. The questionnaire collected sociodemographic data, patterns of AI use, perceptions, and educational needs. Descriptive statistics and multivariable models were applied.Results: Among 283 respondents (mean age 44.6 $\\pm$ 9.7 years), 87.5% acknowledged AI as a transformative tool, but only 60.2% (95% CI: 54.3-66.1%) reported using it, mostly outside institutional frameworks. Notably, 80.2% of users initiated AI use within the past year. Independent predictors of frequent use included previous training (OR=2.44), employment in university hospitals (OR=2.14), and younger age (OR=1.36 per 5-year decrease). Main barriers were lack of training (61%), absence of institutional strategies (46%), and ethical concerns (50%). While 93.8% agreed that AI training programmes are necessary, only 18.4% had received formal training.Conclusions: A substantial gap exists between the favorable perception of AI and its actual integration into clinical practice within Spanish gastroenterology. The rapid adoption outside institutional frameworks underscores the urgent need for accredited training programmes and governance standards led by scientific societies.", "AI": {"tldr": "\u897f\u73ed\u7259\u80c3\u80a0\u75c5\u5b66\u4e13\u5bb6\u5bf9AI\u6301\u79ef\u6781\u6001\u5ea6\u4f46\u5b9e\u9645\u4f7f\u7528\u7387\u4f4e\uff0c\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u57f9\u8bad\u3001\u673a\u6784\u7b56\u7565\u548c\u4f26\u7406\u62c5\u5fe7\uff0c\u9700\u8981\u4e13\u4e1a\u5b66\u4f1a\u4e3b\u5bfc\u7684\u8ba4\u8bc1\u57f9\u8bad\u8ba1\u5212\u3002", "motivation": "AI\u5728\u533b\u5b66\u9886\u57df\u5df2\u6210\u4e3a\u98a0\u8986\u6027\u521b\u65b0\uff0c\u4f46\u5728\u80c3\u80a0\u75c5\u5b66\u4e2d\u7684\u91c7\u7528\u4ecd\u7136\u6709\u9650\u4e14\u7279\u5f81\u4e0d\u660e\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u897f\u73ed\u7259\u80c3\u80a0\u75c5\u5b66\u4e13\u5bb6\u5bf9AI\u7684\u77e5\u8bc6\u3001\u5b9e\u9645\u5e94\u7528\u3001\u611f\u77e5\u969c\u788d\u548c\u671f\u671b\u3002", "method": "\u91c7\u7528\u6a2a\u65ad\u9762\u89c2\u5bdf\u7814\u7a76\u8bbe\u8ba1\uff0c\u901a\u8fc7\u897f\u73ed\u7259\u6d88\u5316\u75c5\u7406\u5b66\u4f1a\uff08SEPD\uff09\u57282025\u5e74\u5206\u53d1\u7ed3\u6784\u5316\u5728\u7ebf\u95ee\u5377\uff0c\u6536\u96c6\u793e\u4f1a\u4eba\u53e3\u5b66\u6570\u636e\u3001AI\u4f7f\u7528\u6a21\u5f0f\u3001\u8ba4\u77e5\u548c\u6559\u80b2\u9700\u6c42\uff0c\u5e94\u7528\u63cf\u8ff0\u6027\u7edf\u8ba1\u548c\u591a\u53d8\u91cf\u6a21\u578b\u5206\u6790\u3002", "result": "283\u540d\u53d7\u8bbf\u8005\uff08\u5e73\u5747\u5e74\u9f8444.6\u00b19.7\u5c81\uff09\u4e2d\uff0c87.5%\u8ba4\u4e3aAI\u662f\u53d8\u9769\u6027\u5de5\u5177\uff0c\u4f46\u4ec560.2%\u62a5\u544a\u4f7f\u7528AI\uff0c\u4e14\u591a\u5728\u673a\u6784\u6846\u67b6\u5916\u4f7f\u7528\u300280.2%\u7684\u7528\u6237\u5728\u8fc7\u53bb\u4e00\u5e74\u5185\u5f00\u59cb\u4f7f\u7528AI\u3002\u9891\u7e41\u4f7f\u7528\u7684\u72ec\u7acb\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u5148\u524d\u57f9\u8bad\uff08OR=2.44\uff09\u3001\u5728\u5927\u5b66\u533b\u9662\u5de5\u4f5c\uff08OR=2.14\uff09\u548c\u5e74\u8f7b\uff08\u6bcf\u51cf\u5c115\u5c81OR=1.36\uff09\u3002\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u57f9\u8bad\uff0861%\uff09\u3001\u7f3a\u4e4f\u673a\u6784\u7b56\u7565\uff0846%\uff09\u548c\u4f26\u7406\u62c5\u5fe7\uff0850%\uff09\u300293.8%\u540c\u610f\u9700\u8981AI\u57f9\u8bad\u8ba1\u5212\uff0c\u4f46\u4ec518.4%\u63a5\u53d7\u8fc7\u6b63\u5f0f\u57f9\u8bad\u3002", "conclusion": "\u897f\u73ed\u7259\u80c3\u80a0\u75c5\u5b66\u4e2dAI\u7684\u79ef\u6781\u8ba4\u77e5\u4e0e\u5b9e\u9645\u4e34\u5e8a\u6574\u5408\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u673a\u6784\u6846\u67b6\u5916\u7684\u5feb\u901f\u91c7\u7528\u7a81\u663e\u4e86\u7531\u79d1\u5b66\u5b66\u4f1a\u4e3b\u5bfc\u7684\u8ba4\u8bc1\u57f9\u8bad\u8ba1\u5212\u548c\u6cbb\u7406\u6807\u51c6\u7684\u7d27\u8feb\u9700\u6c42\u3002"}}
{"id": "2601.17311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17311", "abs": "https://arxiv.org/abs/2601.17311", "authors": ["Bang Liu", "Linglong Kong", "Jian Pei"], "title": "Phase Transition for Budgeted Multi-Agent Synergy", "comment": "55 pages, 12 figures", "summary": "Multi-agent systems can improve reliability, yet under a fixed inference budget they often help, saturate, or even collapse. We develop a minimal and calibratable theory that predicts these regimes from three binding constraints of modern agent stacks: finite context windows, lossy inter-agent communication, and shared failures among similar agents. Each leaf agent is summarized by a compute-performance scaling exponent $\u03b2$; communication is captured by a message-length fidelity curve $\u03b3(m)$; dependence is captured by an effective shared-error correlation $\u03c1$; and a context window $W$ imposes hard fan-in limits that make hierarchy necessary. For binary success/failure tasks with majority aggregation, we prove a sharp phase transition for deep $b$-ary trees with correlated inputs and lossy communication: a single scalar $\u03b1_\u03c1$ (combining $\u03b3(m)$, $\u03c1$, and fan-in $b$) determines whether weak signal is amplified to a nontrivial fixed point or washed out to chance. In the amplifying regime, we derive an organization exponent $s$ and show that budgeted synergy, i.e., outperforming the best single agent under the same total budget, occurs exactly when $s>\u03b2$, yielding closed-form compute allocation rules and explicit budget thresholds. We further characterize saturation via a mixing depth and provide a conservative clipped predictor that remains accurate across growth and saturation. A continuous-performance warm-up gives closed-form risks for star, chain, and tree organizations, making correlation- and communication-induced floors explicit and exposing the core design trade-offs in a smooth setting. Finally, we validate the predicted phase boundaries in controlled synthetic simulations and show how the same mechanisms explain the dominant bottlenecks reported in recent large-scale matched-budget studies of LLM agent-system scaling.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u5b58\u5728\u5e2e\u52a9\u3001\u9971\u548c\u751a\u81f3\u5d29\u6e83\u4e09\u79cd\u72b6\u6001\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6821\u51c6\u7684\u6700\u5c0f\u7406\u8bba\uff0c\u57fa\u4e8e\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u6709\u635f\u901a\u4fe1\u548c\u76f8\u4f3c\u667a\u80fd\u4f53\u5171\u4eab\u9519\u8bef\u4e09\u4e2a\u7ea6\u675f\u6761\u4ef6\uff0c\u9884\u6d4b\u8fd9\u4e9b\u72b6\u6001\u5e76\u63a8\u5bfc\u51fa\u8ba1\u7b97\u5206\u914d\u89c4\u5219\u548c\u9884\u7b97\u9608\u503c\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u63d0\u9ad8\u53ef\u9760\u6027\uff0c\u4f46\u5728\u56fa\u5b9a\u63a8\u7406\u9884\u7b97\u4e0b\u5e38\u5e38\u51fa\u73b0\u5e2e\u52a9\u6709\u9650\u3001\u6027\u80fd\u9971\u548c\u751a\u81f3\u5d29\u6e83\u7684\u73b0\u8c61\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u80fd\u591f\u9884\u6d4b\u8fd9\u4e9b\u72b6\u6001\u5e76\u6307\u5bfc\u7cfb\u7edf\u8bbe\u8ba1\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u8003\u8651\u73b0\u4ee3\u667a\u80fd\u4f53\u5806\u6808\u7684\u5b9e\u9645\u7ea6\u675f\u6761\u4ef6\u65f6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6821\u51c6\u7684\u6700\u5c0f\u7406\u8bba\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ea6\u675f\uff1a\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3W\u3001\u6709\u635f\u901a\u4fe1\u03b3(m)\u548c\u5171\u4eab\u9519\u8bef\u76f8\u5173\u6027\u03c1\u3002\u6bcf\u4e2a\u53f6\u8282\u70b9\u667a\u80fd\u4f53\u7528\u8ba1\u7b97-\u6027\u80fd\u7f29\u653e\u6307\u6570\u03b2\u63cf\u8ff0\u3002\u901a\u8fc7\u5206\u6790\u6df1\u5ea6b\u53c9\u6811\u7ed3\u6784\uff0c\u63a8\u5bfc\u51fa\u76f8\u4f4d\u8f6c\u53d8\u6761\u4ef6\u3001\u7ec4\u7ec7\u6307\u6570s\u548c\u8ba1\u7b97\u5206\u914d\u89c4\u5219\u3002", "result": "\u8bc1\u660e\u4e86\u6df1\u5ea6b\u53c9\u6811\u5b58\u5728\u5c16\u9510\u7684\u76f8\u4f4d\u8f6c\u53d8\uff1a\u5355\u4e2a\u6807\u91cf\u03b1_\u03c1\u51b3\u5b9a\u5f31\u4fe1\u53f7\u662f\u88ab\u653e\u5927\u5230\u975e\u5e73\u51e1\u56fa\u5b9a\u70b9\u8fd8\u662f\u88ab\u6df9\u6ca1\u3002\u5728\u653e\u5927\u72b6\u6001\u4e0b\uff0c\u5f53\u7ec4\u7ec7\u6307\u6570s>\u03b2\u65f6\u51fa\u73b0\u9884\u7b97\u534f\u540c\u6548\u5e94\uff0c\u5373\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u76f8\u540c\u603b\u9884\u7b97\u4e0b\u4f18\u4e8e\u6700\u4f73\u5355\u4e2a\u667a\u80fd\u4f53\u3002\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u9884\u7b97\u9608\u503c\u548c\u8ba1\u7b97\u5206\u914d\u89c4\u5219\u3002", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u80fd\u591f\u9884\u6d4b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4e09\u79cd\u72b6\u6001\uff08\u5e2e\u52a9\u3001\u9971\u548c\u3001\u5d29\u6e83\uff09\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5bfc\u3002\u901a\u8fc7\u5408\u6210\u6a21\u62df\u9a8c\u8bc1\u4e86\u9884\u6d4b\u7684\u76f8\u4f4d\u8fb9\u754c\uff0c\u5e76\u89e3\u91ca\u4e86\u6700\u8fd1\u5927\u89c4\u6a21\u5339\u914d\u9884\u7b97\u7814\u7a76\u4e2d\u89c2\u5bdf\u5230\u7684LLM\u667a\u80fd\u4f53\u7cfb\u7edf\u7f29\u653e\u74f6\u9888\u3002"}}
{"id": "2601.17359", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17359", "abs": "https://arxiv.org/abs/2601.17359", "authors": ["Payel Santra", "Partha Basuchowdhuri", "Debasis Ganguly"], "title": "Breaking Flat: A Generalised Query Performance Prediction Evaluation Framework", "comment": null, "summary": "The traditional use-case of query performance prediction (QPP) is to identify which queries perform well and which perform poorly for a given ranking model. A more fine-grained and arguably more challenging extension of this task is to determine which ranking models are most effective for a given query. In this work, we generalize the QPP task and its evaluation into three settings: (i) SingleRanker MultiQuery (SRMQ-PP), corresponding to the standard use case; (ii) MultiRanker SingleQuery (MRSQ-PP), which evaluates a QPP model's ability to select the most effective ranker for a query; and (iii) MultiRanker MultiQuery (MRMQ-PP), which considers predictions jointly across all query ranker pairs. Our results show that (a) the relative effectiveness of QPP models varies substantially across tasks (SRMQ-PP vs. MRSQ-PP), and (b) predicting the best ranker for a query is considerably more difficult than predicting the relative difficulty of queries for a given ranker.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u4efb\u52a1\u4ece\u4f20\u7edf\u7684\"\u5355\u4e00\u6392\u5e8f\u6a21\u578b-\u591a\u67e5\u8be2\"\u6269\u5c55\u5230\"\u591a\u6392\u5e8f\u6a21\u578b-\u5355\u4e00\u67e5\u8be2\"\u548c\"\u591a\u6392\u5e8f\u6a21\u578b-\u591a\u67e5\u8be2\"\u4e09\u79cd\u8bbe\u7f6e\uff0c\u53d1\u73b0\u4e0d\u540c\u4efb\u52a1\u4e2dQPP\u6a21\u578b\u6548\u679c\u5dee\u5f02\u663e\u8457\uff0c\u4e14\u9884\u6d4b\u6700\u4f73\u6392\u5e8f\u6a21\u578b\u6bd4\u9884\u6d4b\u67e5\u8be2\u96be\u5ea6\u66f4\u5177\u6311\u6218\u6027\u3002", "motivation": "\u4f20\u7edf\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u6392\u5e8f\u6a21\u578b\u4e0b\u4e0d\u540c\u67e5\u8be2\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4f46\u66f4\u7ec6\u7c92\u5ea6\u7684\u6311\u6218\u662f\u786e\u5b9a\u54ea\u4e9b\u6392\u5e8f\u6a21\u578b\u5bf9\u7279\u5b9a\u67e5\u8be2\u6700\u6709\u6548\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06QPP\u4efb\u52a1\u53ca\u5176\u8bc4\u4f30\u63a8\u5e7f\u5230\u66f4\u5168\u9762\u7684\u8bbe\u7f6e\u3002", "method": "\u5c06QPP\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u4e09\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff1a1) SRMQ-PP\uff1a\u5355\u4e00\u6392\u5e8f\u6a21\u578b\u591a\u67e5\u8be2\uff08\u4f20\u7edf\u8bbe\u7f6e\uff09\uff1b2) MRSQ-PP\uff1a\u591a\u6392\u5e8f\u6a21\u578b\u5355\u4e00\u67e5\u8be2\uff08\u9009\u62e9\u6700\u6709\u6548\u6392\u5e8f\u6a21\u578b\uff09\uff1b3) MRMQ-PP\uff1a\u591a\u6392\u5e8f\u6a21\u578b\u591a\u67e5\u8be2\uff08\u8054\u5408\u8003\u8651\u6240\u6709\u67e5\u8be2-\u6392\u5e8f\u6a21\u578b\u5bf9\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) QPP\u6a21\u578b\u7684\u76f8\u5bf9\u6709\u6548\u6027\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b2) \u9884\u6d4b\u67e5\u8be2\u7684\u6700\u4f73\u6392\u5e8f\u6a21\u578b\u6bd4\u9884\u6d4b\u5355\u4e00\u6392\u5e8f\u6a21\u578b\u4e0b\u67e5\u8be2\u7684\u76f8\u5bf9\u96be\u5ea6\u8981\u56f0\u96be\u5f97\u591a\u3002", "conclusion": "QPP\u7814\u7a76\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u7684\u5355\u4e00\u6392\u5e8f\u6a21\u578b\u8bbe\u7f6e\uff0c\u8003\u8651\u591a\u6392\u5e8f\u6a21\u578b\u73af\u5883\u4e0b\u7684\u6027\u80fd\u9884\u6d4b\u3002\u4e0d\u540cQPP\u4efb\u52a1\u5177\u6709\u4e0d\u540c\u7684\u6311\u6218\u6027\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u3002"}}
{"id": "2601.17042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17042", "abs": "https://arxiv.org/abs/2601.17042", "authors": ["Tianyuan Liu", "Libin Hou", "Linyuan Wang", "Bin Yan"], "title": "Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective", "comment": "8 pages with 6 figures", "summary": "Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between \"membership matrix\" and \"subspace matrix U\" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the \"membership matrix\" and \"subspaces U\" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u89e3\u8026\u7684\u6210\u5458-\u5b50\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236(DMSA)\uff0c\u901a\u8fc7\u5206\u79bbMCR2\u76ee\u6807\u4e2d\u7684\u6210\u5458\u77e9\u9635\u548c\u5b50\u7a7a\u95f4\u77e9\u9635\uff0c\u4ece\u4f18\u5316\u76ee\u6807\u68af\u5ea6\u5c55\u5f00\u5f97\u5230\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\uff0c\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u7f16\u7801\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709MCR2\u9a71\u52a8\u7684\u767d\u76d2transformer\u8bbe\u8ba1\u4e2d\uff0c\u6210\u5458\u77e9\u9635\u548c\u5b50\u7a7a\u95f4\u77e9\u9635U\u4e4b\u95f4\u7684\u7d27\u5bc6\u8026\u5408\u5bfc\u81f4\u5728\u9519\u8bef\u7684token\u6295\u5f71\u4e0b\u4ea7\u751f\u5197\u4f59\u7f16\u7801\uff0c\u9700\u8981\u89e3\u8026\u8fd9\u4e24\u8005\u7684\u529f\u80fd\u5173\u7cfb\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u6210\u5458-\u5b50\u7a7a\u95f4\u6ce8\u610f\u529b(DMSA)\uff1a1)\u76f4\u63a5\u4ece\u8f93\u5165\u5b66\u4e60\u6210\u5458\u77e9\u9635\uff1b2)\u4ece\u5168\u7a7a\u95f4S\u63a8\u5bfc\u7a00\u758f\u5b50\u7a7a\u95f4\uff1b3)\u901a\u8fc7\u4f18\u5316MCR2\u76ee\u6807\u7684\u68af\u5ea6\u5c55\u5f00\u5f97\u5230\u53ef\u89e3\u91ca\u7684\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\u3002\u5c06ToST\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5757\u66ff\u6362\u4e3aDMSA\u5f62\u6210DMST\u6a21\u578b\u3002", "result": "\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0cDMST\u6bd4ToST\u5728top-1\u51c6\u786e\u7387\u4e0a\u63d0\u53471.08%-1.45%\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u7f16\u7801\u964d\u7ef4\u901f\u7387\u3002\u76f8\u6bd4\u4f20\u7edfTransformer\u67b6\u6784\uff0cDMST\u5c55\u73b0\u51fa\u663e\u8457\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026MCR2\u76ee\u6807\u4e2d\u7684\u6210\u5458\u77e9\u9635\u548c\u5b50\u7a7a\u95f4\u77e9\u9635\uff0c\u63d0\u51fa\u7684DMSA\u6ce8\u610f\u529b\u673a\u5236\u4e3a\u89c6\u89c9\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u767d\u76d2Transformer\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2601.17012", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17012", "abs": "https://arxiv.org/abs/2601.17012", "authors": ["Christine Ine"], "title": "The Digital Divide in Geriatric Care: Why Usability, Not Access, is the Real Problem", "comment": "Page Count - 15 Word Count - 3671", "summary": "The rapid increase in the world's aging population to 16% by the year 2050 spurs the need for the application of digital health solutions to enhance older individuals' independence, accessibility, and well-being. While digital health technologies such as telemedicine, wearables, and mobile health applications can transform geriatric care, their adoption among older individuals is not evenly distributed. This study redefines the \"digital divide\" among older health care as a usability divide, contends that user experience (UX) poor design is the primary adoption barrier, rather than access. Drawing on interdisciplinary studies and design paradigms, the research identifies the main challenges: visual, cognitive, and motor impairment; complicated interfaces; and lack of co-creation with older adults, and outlines how participatory, user-focused, and inclusive notions of design can transcend them. Findings reveal that older persons easily embrace those technologies that are intuitive, accessible, and socially embedded as they promote autonomy, confidence, and equity in health. The study identifies the effects of the design attributes of high-contrast screens, lower interaction flow, multimodal feedback, and caregiver integration as having strong influences on usability outcomes. In addition, it critiques the current accessibility guidelines as being technically oriented rather than experiential and demands an ethical, empathetic understanding of design grounded in human-centered usability rather than technical accessibility in itself.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u8001\u5e74\u4eba\u6570\u5b57\u5065\u5eb7\u9886\u57df\u7684\"\u6570\u5b57\u9e3f\u6c9f\"\u91cd\u65b0\u5b9a\u4e49\u4e3a\"\u53ef\u7528\u6027\u9e3f\u6c9f\"\uff0c\u8ba4\u4e3a\u7528\u6237\u4f53\u9a8c\u8bbe\u8ba1\u4e0d\u4f73\u800c\u975e\u6280\u672f\u8bbf\u95ee\u662f\u4e3b\u8981\u91c7\u7eb3\u969c\u788d\uff0c\u63d0\u51fa\u901a\u8fc7\u53c2\u4e0e\u5f0f\u3001\u7528\u6237\u4e2d\u5fc3\u548c\u5305\u5bb9\u6027\u8bbe\u8ba1\u6765\u514b\u670d\u6311\u6218\u3002", "motivation": "\u5168\u7403\u8001\u9f84\u5316\u4eba\u53e3\u5feb\u901f\u589e\u957f\uff08\u9884\u8ba1205\u5e74\u8fbe16%\uff09\uff0c\u9700\u8981\u6570\u5b57\u5065\u5eb7\u89e3\u51b3\u65b9\u6848\u6765\u589e\u5f3a\u8001\u5e74\u4eba\u7684\u72ec\u7acb\u6027\u3001\u53ef\u53ca\u6027\u548c\u798f\u7949\u3002\u867d\u7136\u8fdc\u7a0b\u533b\u7597\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u79fb\u52a8\u5065\u5eb7\u5e94\u7528\u7b49\u6280\u672f\u53ef\u4ee5\u6539\u53d8\u8001\u5e74\u62a4\u7406\uff0c\u4f46\u5728\u8001\u5e74\u4eba\u4e2d\u7684\u91c7\u7eb3\u5e76\u4e0d\u5747\u8861\u3002", "method": "\u57fa\u4e8e\u8de8\u5b66\u79d1\u7814\u7a76\u548c\u8bbe\u8ba1\u8303\u5f0f\uff0c\u8bc6\u522b\u4e3b\u8981\u6311\u6218\uff1a\u89c6\u89c9\u3001\u8ba4\u77e5\u548c\u8fd0\u52a8\u969c\u788d\uff1b\u590d\u6742\u754c\u9762\uff1b\u7f3a\u4e4f\u4e0e\u8001\u5e74\u4eba\u7684\u5171\u540c\u521b\u9020\u3002\u63d0\u51fa\u901a\u8fc7\u53c2\u4e0e\u5f0f\u3001\u7528\u6237\u4e2d\u5fc3\u548c\u5305\u5bb9\u6027\u8bbe\u8ba1\u7406\u5ff5\u6765\u514b\u670d\u8fd9\u4e9b\u969c\u788d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8001\u5e74\u4eba\u5bb9\u6613\u63a5\u53d7\u90a3\u4e9b\u76f4\u89c2\u3001\u53ef\u8bbf\u95ee\u4e14\u5177\u6709\u793e\u4f1a\u5d4c\u5165\u6027\u7684\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u80fd\u4fc3\u8fdb\u81ea\u4e3b\u6027\u3001\u4fe1\u5fc3\u548c\u5065\u5eb7\u516c\u5e73\u3002\u9ad8\u5bf9\u6bd4\u5ea6\u5c4f\u5e55\u3001\u7b80\u5316\u4ea4\u4e92\u6d41\u7a0b\u3001\u591a\u6a21\u6001\u53cd\u9988\u548c\u62a4\u7406\u4eba\u5458\u6574\u5408\u7b49\u8bbe\u8ba1\u5c5e\u6027\u5bf9\u53ef\u7528\u6027\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u5f53\u524d\u53ef\u8bbf\u95ee\u6027\u6307\u5357\u8fc7\u4e8e\u6280\u672f\u5bfc\u5411\u800c\u975e\u4f53\u9a8c\u5bfc\u5411\uff0c\u9700\u8981\u57fa\u4e8e\u4eba\u672c\u53ef\u7528\u6027\u800c\u975e\u5355\u7eaf\u6280\u672f\u53ef\u8bbf\u95ee\u6027\u7684\u4f26\u7406\u3001\u5171\u60c5\u8bbe\u8ba1\u7406\u89e3\u3002\u7528\u6237\u4f53\u9a8c\u8bbe\u8ba1\u662f\u514b\u670d\u8001\u5e74\u4eba\u6570\u5b57\u5065\u5eb7\u91c7\u7eb3\u969c\u788d\u7684\u5173\u952e\u3002"}}
{"id": "2601.17332", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17332", "abs": "https://arxiv.org/abs/2601.17332", "authors": ["Yicheng Tao", "Hongteng Xu"], "title": "TheoremForge: Scaling up Formal Data Synthesis with Low-Budget Agentic Workflow", "comment": null, "summary": "The high cost of agentic workflows in formal mathematics hinders large-scale data synthesis, exacerbating the scarcity of open-source corpora. To address this, we introduce \\textbf{TheoremForge}, a cost-effective formal data synthesis pipeline that decomposes the formalization process into five sub-tasks, which are \\textit{statement formalization}, \\textit{proof generation}, \\textit{premise selection}, \\textit{proof correction} and \\textit{proof sketching}. By implementing a \\textit{Decoupled Extraction Strategy}, the workflow recovers valid training signals from globally failed trajectories, effectively utilizing wasted computation. Experiments on a 2,000-problem benchmark demonstrate that TheoremForge achieves a Verified Rate of 12.6\\%, surpassing the 8.6\\% baseline, at an average cost of only \\textbf{\\$0.481} per successful trajectory using Gemini-3-Flash. Crucially, our strategy increases data yield by \\textbf{1.6$\\times$} for proof generation compared to standard filtering. These results establish TheoremForge as a scalable framework for constructing a data flywheel to train future expert models. Our code is available \\href{https://github.com/timechess/TheoremForge}{here}.", "AI": {"tldr": "TheoremForge\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u5f62\u5f0f\u5316\u6570\u5b66\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u5c06\u5f62\u5f0f\u5316\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e94\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u89e3\u8026\u63d0\u53d6\u7b56\u7565\u4ece\u5931\u8d25\u8f68\u8ff9\u4e2d\u6062\u590d\u6709\u6548\u8bad\u7ec3\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u5408\u6210\u6548\u7387\u3002", "motivation": "\u5f62\u5f0f\u5316\u6570\u5b66\u4e2d\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u9ad8\u6210\u672c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u6570\u636e\u5408\u6210\uff0c\u52a0\u5267\u4e86\u5f00\u6e90\u8bed\u6599\u5e93\u7684\u7a00\u7f3a\u6027\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u5f62\u5f0f\u5316\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e94\u4e2a\u5b50\u4efb\u52a1\uff1a\u9648\u8ff0\u5f62\u5f0f\u5316\u3001\u8bc1\u660e\u751f\u6210\u3001\u524d\u63d0\u9009\u62e9\u3001\u8bc1\u660e\u4fee\u6b63\u548c\u8bc1\u660e\u8349\u56fe\u3002\u91c7\u7528\u89e3\u8026\u63d0\u53d6\u7b56\u7565\uff0c\u4ece\u5168\u5c40\u5931\u8d25\u7684\u8f68\u8ff9\u4e2d\u6062\u590d\u6709\u6548\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u6709\u6548\u5229\u7528\u6d6a\u8d39\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u57282000\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTheoremForge\u5b9e\u73b0\u4e8612.6%\u7684\u9a8c\u8bc1\u7387\uff0c\u8d85\u8fc78.6%\u7684\u57fa\u7ebf\uff0c\u6bcf\u4e2a\u6210\u529f\u8f68\u8ff9\u7684\u5e73\u5747\u6210\u672c\u4ec5\u4e3a0.481\u7f8e\u5143\u3002\u8be5\u7b56\u7565\u4f7f\u8bc1\u660e\u751f\u6210\u7684\u6570\u636e\u4ea7\u51fa\u63d0\u9ad8\u4e861.6\u500d\u3002", "conclusion": "TheoremForge\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u6570\u636e\u98de\u8f6e\u6765\u8bad\u7ec3\u672a\u6765\u7684\u4e13\u5bb6\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3\u5f62\u5f0f\u5316\u6570\u5b66\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17438", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17438", "abs": "https://arxiv.org/abs/2601.17438", "authors": ["Jialei Li", "Yang Zhang", "Yimeng Bai", "Shuai Zhu", "Ziqi Xue", "Xiaoyan Zhao", "Dingxian Wang", "Frank Yang", "Andrew Rabinovich", "Xiangnan He"], "title": "UniGRec: Unified Generative Recommendation with Soft Identifiers for End-to-End Optimization", "comment": "11 pages, 6 figures", "summary": "Generative recommendation has recently emerged as a transformative paradigm that directly generates target items, surpassing traditional cascaded approaches. It typically involves two components: a tokenizer that learns item identifiers and a recommender trained on them. Existing methods often decouple tokenization from recommendation or rely on asynchronous alternating optimization, limiting full end-to-end alignment. To address this, we unify the tokenizer and recommender under the ultimate recommendation objective via differentiable soft item identifiers, enabling joint end-to-end training. However, this introduces three challenges: training-inference discrepancy due to soft-to-hard mismatch, item identifier collapse from codeword usage imbalance, and collaborative signal deficiency due to an overemphasis on fine-grained token-level semantics.\n  To tackle these challenges, we propose UniGRec, a unified generative recommendation framework that addresses them from three perspectives. UniGRec employs Annealed Inference Alignment during tokenization to smoothly bridge soft training and hard inference, a Codeword Uniformity Regularization to prevent identifier collapse and encourage codebook diversity, and a Dual Collaborative Distillation mechanism that distills collaborative priors from a lightweight teacher model to jointly guide both the tokenizer and the recommender. Extensive experiments on real-world datasets demonstrate that UniGRec consistently outperforms state-of-the-art baseline methods. Our codes are available at https://github.com/Jialei-03/UniGRec.", "AI": {"tldr": "UniGRec\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2dtokenizer\u548c\u63a8\u8350\u5668\u5206\u79bb\u7684\u95ee\u9898\uff0c\u5e76\u9488\u5bf9\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u3001\u6807\u8bc6\u7b26\u5d29\u6e83\u548c\u534f\u540c\u4fe1\u53f7\u4e0d\u8db3\u4e09\u4e2a\u6311\u6218\u63d0\u51fa\u4e86\u76f8\u5e94\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u63a8\u8350\u65b9\u6cd5\u901a\u5e38\u5c06tokenizer\u548c\u63a8\u8350\u5668\u89e3\u8026\u6216\u91c7\u7528\u5f02\u6b65\u4ea4\u66ff\u4f18\u5316\uff0c\u9650\u5236\u4e86\u7aef\u5230\u7aef\u5bf9\u9f50\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7edf\u4e00tokenizer\u548c\u63a8\u8350\u5668\u5728\u63a8\u8350\u76ee\u6807\u4e0b\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4f46\u9762\u4e34\u8bad\u7ec3-\u63a8\u7406\u5dee\u5f02\u3001\u6807\u8bc6\u7b26\u5d29\u6e83\u548c\u534f\u540c\u4fe1\u53f7\u4e0d\u8db3\u4e09\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faUniGRec\u6846\u67b6\uff1a1) \u9000\u706b\u63a8\u7406\u5bf9\u9f50\u5e73\u6ed1\u8fde\u63a5\u8f6f\u8bad\u7ec3\u548c\u786c\u63a8\u7406\uff1b2) \u7801\u5b57\u5747\u5300\u6027\u6b63\u5219\u5316\u9632\u6b62\u6807\u8bc6\u7b26\u5d29\u6e83\u5e76\u4fc3\u8fdb\u7801\u672c\u591a\u6837\u6027\uff1b3) \u53cc\u91cd\u534f\u540c\u84b8\u998f\u673a\u5236\u4ece\u8f7b\u91cf\u7ea7\u6559\u5e08\u6a21\u578b\u63d0\u53d6\u534f\u540c\u5148\u9a8c\u6765\u5171\u540c\u6307\u5bfctokenizer\u548c\u63a8\u8350\u5668\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUniGRec\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UniGRec\u901a\u8fc7\u7edf\u4e00tokenizer\u548c\u63a8\u8350\u5668\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u63a8\u8350\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2601.17335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17335", "abs": "https://arxiv.org/abs/2601.17335", "authors": ["Angshul Majumdar"], "title": "The Relativity of AGI: Distributional Axioms, Fragility, and Undecidability", "comment": null, "summary": "We study whether Artificial General Intelligence (AGI) admits a coherent theoretical definition that supports absolute claims of existence, robustness, or self-verification. We formalize AGI axiomatically as a distributional, resource-bounded semantic predicate, indexed by a task family, a task distribution, a performance functional, and explicit resource budgets. Under this framework, we derive four classes of results. First, we show that generality is inherently relational: there is no distribution-independent notion of AGI. Second, we prove non-invariance results demonstrating that arbitrarily small perturbations of the task distribution can invalidate AGI properties via cliff sets, precluding universal robustness. Third, we establish bounded transfer guarantees, ruling out unbounded generalization across task families under finite resources. Fourth, invoking Rice-style and G\u00f6del--Tarski arguments, we prove that AGI is a nontrivial semantic property and therefore cannot be soundly and completely certified by any computable procedure, including procedures implemented by the agent itself. Consequently, recursive self-improvement schemes that rely on internal self-certification of AGI are ill-posed. Taken together, our results show that strong, distribution-independent claims of AGI are not false but undefined without explicit formal indexing, and that empirical progress in AI does not imply the attainability of self-certifying general intelligence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc1\u660eAGI\u65e0\u6cd5\u83b7\u5f97\u72ec\u7acb\u4e8e\u4efb\u52a1\u5206\u5e03\u7684\u7edf\u4e00\u5b9a\u4e49\uff0c\u7f3a\u4e4f\u666e\u904d\u9c81\u68d2\u6027\uff0c\u6709\u9650\u8d44\u6e90\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u65e0\u9650\u6cdb\u5316\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u53ef\u8ba1\u7b97\u7a0b\u5e8f\uff08\u5305\u62ec\u81ea\u8ba4\u8bc1\uff09\u8fdb\u884c\u5b8c\u5907\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u662f\u5426\u80fd\u591f\u83b7\u5f97\u4e00\u4e2a\u8fde\u8d2f\u7684\u7406\u8bba\u5b9a\u4e49\uff0c\u4ee5\u652f\u6301\u5173\u4e8e\u5176\u5b58\u5728\u6027\u3001\u9c81\u68d2\u6027\u6216\u81ea\u9a8c\u8bc1\u7684\u7edd\u5bf9\u4e3b\u5f20\u3002\u4f5c\u8005\u5e0c\u671b\u4ece\u5f62\u5f0f\u5316\u89d2\u5ea6\u5206\u6790AGI\u6982\u5ff5\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u91c7\u7528\u516c\u7406\u5316\u65b9\u6cd5\uff0c\u5c06AGI\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u5206\u5e03\u6027\u3001\u8d44\u6e90\u53d7\u9650\u7684\u8bed\u4e49\u8c13\u8bcd\uff0c\u901a\u8fc7\u4efb\u52a1\u65cf\u3001\u4efb\u52a1\u5206\u5e03\u3001\u6027\u80fd\u51fd\u6570\u548c\u663e\u5f0f\u8d44\u6e90\u9884\u7b97\u8fdb\u884c\u7d22\u5f15\u3002\u5728\u6b64\u6846\u67b6\u4e0b\uff0c\u8fd0\u7528\u6570\u5b66\u8bc1\u660e\u65b9\u6cd5\uff08\u5305\u62ecRice\u98ce\u683c\u548c\u54e5\u5fb7\u5c14-\u5854\u65af\u57fa\u8bba\u8bc1\uff09\u63a8\u5bfc\u4e86\u56db\u7c7b\u7406\u8bba\u7ed3\u679c\u3002", "result": "1. \u901a\u7528\u6027\u662f\u5173\u7cfb\u6027\u7684\uff1a\u4e0d\u5b58\u5728\u72ec\u7acb\u4e8e\u5206\u5e03\u7684AGI\u6982\u5ff5\uff1b2. \u975e\u4e0d\u53d8\u6027\u7ed3\u679c\uff1a\u4efb\u52a1\u5206\u5e03\u7684\u4efb\u610f\u5fae\u5c0f\u6270\u52a8\u53ef\u901a\u8fc7\u60ac\u5d16\u96c6\u4f7fAGI\u5c5e\u6027\u5931\u6548\uff1b3. \u6709\u754c\u8fc1\u79fb\u4fdd\u8bc1\uff1a\u6709\u9650\u8d44\u6e90\u4e0b\u65e0\u6cd5\u5b9e\u73b0\u8de8\u4efb\u52a1\u65cf\u7684\u65e0\u9650\u6cdb\u5316\uff1b4. AGI\u662f\u975e\u5e73\u51e1\u7684\u8bed\u4e49\u5c5e\u6027\uff0c\u65e0\u6cd5\u901a\u8fc7\u4efb\u4f55\u53ef\u8ba1\u7b97\u7a0b\u5e8f\uff08\u5305\u62ec\u81ea\u8ba4\u8bc1\uff09\u8fdb\u884c\u5b8c\u5907\u9a8c\u8bc1\u3002", "conclusion": "\u5f3a\u5206\u5e03\u72ec\u7acb\u7684AGI\u4e3b\u5f20\u5728\u6ca1\u6709\u663e\u5f0f\u5f62\u5f0f\u5316\u7d22\u5f15\u7684\u60c5\u51b5\u4e0b\u662f\u672a\u5b9a\u4e49\u7684\uff0c\u800c\u975e\u9519\u8bef\u7684\u3002AI\u7684\u5b9e\u8bc1\u8fdb\u5c55\u5e76\u4e0d\u6697\u793a\u81ea\u8ba4\u8bc1\u901a\u7528\u667a\u80fd\u7684\u53ef\u5b9e\u73b0\u6027\uff0c\u4f9d\u8d56\u5185\u90e8\u81ea\u8ba4\u8bc1\u7684\u9012\u5f52\u81ea\u6211\u6539\u8fdb\u65b9\u6848\u662f\u75c5\u6001\u7684\u3002"}}
{"id": "2601.17472", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17472", "abs": "https://arxiv.org/abs/2601.17472", "authors": ["Junyou He", "Lixi Deng", "Huichao Guo", "Ye Tang", "Yong Li", "Sulong Xu"], "title": "Adversarial Alignment and Disentanglement for Cross-Domain CTR Prediction with Domain-Encompassing Features", "comment": "Accepted to ICDM 2025", "summary": "Cross-domain recommendation (CDR) has been increasingly explored to address data sparsity and cold-start issues. However, recent approaches typically disentangle domain-invariant features shared between source and target domains, as well as domain-specific features for each domain. However, they often rely solely on domain-invariant features combined with target domain-specific features, which can lead to suboptimal performance. To overcome the limitations, this paper presents the Adversarial Alignment and Disentanglement Cross-Domain Recommendation ($A^2DCDR$ ) model, an innovative approach designed to capture a comprehensive range of cross-domain information, including both domain-invariant and valuable non-aligned features. The $A^2DCDR$ model enhances cross-domain recommendation through three key components: refining MMD with adversarial training for better generalization, employing a feature disentangler and reconstruction mechanism for intra-domain disentanglement, and introducing a novel fused representation combining domain-invariant, non-aligned features with original contextual data. Experiments on real-world datasets and online A/B testing show that $A^2DCDR$ outperforms existing methods, confirming its effectiveness and practical applicability. The code is provided at https://github.com/youzi0925/A-2DCDR/tree/main.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86A\u00b2DCDR\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6297\u5bf9\u9f50\u548c\u7279\u5f81\u89e3\u8026\u6765\u6539\u8fdb\u8de8\u57df\u63a8\u8350\uff0c\u7ed3\u5408\u57df\u4e0d\u53d8\u7279\u5f81\u3001\u975e\u5bf9\u9f50\u7279\u5f81\u548c\u539f\u59cb\u4e0a\u4e0b\u6587\u6570\u636e\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u63a8\u8350\u65b9\u6cd5\u901a\u5e38\u53ea\u89e3\u8026\u57df\u4e0d\u53d8\u7279\u5f81\u548c\u57df\u7279\u5b9a\u7279\u5f81\uff0c\u5e76\u4e3b\u8981\u4f9d\u8d56\u57df\u4e0d\u53d8\u7279\u5f81\u7ed3\u5408\u76ee\u6807\u57df\u7279\u5b9a\u7279\u5f81\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002\u9700\u8981\u66f4\u5168\u9762\u5730\u6355\u6349\u8de8\u57df\u4fe1\u606f\uff0c\u5305\u62ec\u57df\u4e0d\u53d8\u7279\u5f81\u548c\u6709\u4ef7\u503c\u7684\u975e\u5bf9\u9f50\u7279\u5f81\u3002", "method": "\u63d0\u51faA\u00b2DCDR\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u6539\u8fdbMMD\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff1b2) \u4f7f\u7528\u7279\u5f81\u89e3\u8026\u5668\u548c\u91cd\u5efa\u673a\u5236\u8fdb\u884c\u57df\u5185\u89e3\u8026\uff1b3) \u5f15\u5165\u65b0\u9896\u7684\u878d\u5408\u8868\u793a\uff0c\u7ed3\u5408\u57df\u4e0d\u53d8\u7279\u5f81\u3001\u975e\u5bf9\u9f50\u7279\u5f81\u548c\u539f\u59cb\u4e0a\u4e0b\u6587\u6570\u636e\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cA\u00b2DCDR\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "A\u00b2DCDR\u6a21\u578b\u901a\u8fc7\u5bf9\u6297\u5bf9\u9f50\u548c\u7279\u5f81\u89e3\u8026\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u6355\u6349\u8de8\u57df\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u8de8\u57df\u63a8\u8350\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.17047", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17047", "abs": "https://arxiv.org/abs/2601.17047", "authors": ["Yuanjie Gu", "Yiqun Wang", "Chaohui Yu", "Ang Xuan", "Fan Wang", "Zhi Lu", "Biqin Dong"], "title": "A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities", "comment": null, "summary": "Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce \"Noisomics\", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.", "AI": {"tldr": "Noisomics\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u566a\u58f0\u4ece\u6291\u5236\u5bf9\u8c61\u8f6c\u53d8\u4e3a\u53ef\u89e3\u7801\u7684\u4fe1\u606f\u8d44\u6e90\uff0c\u4ec5\u9700100\u4e2a\u8bad\u7ec3\u6837\u672c\u5c31\u80fd\u8d85\u8d8a\u4f20\u7edf\u9700\u898110\u4e07\u4e2a\u6837\u672c\u7684\u76d1\u7763\u65b9\u6cd5\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u5f53\u524d\u6210\u50cf\u566a\u58f0\u8868\u5f81\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u76d1\u7763\u6570\u636e\u4e14\u8bbe\u5907\u4f9d\u8d56\u6027\u5f3a\uff0c\u96be\u4ee5\u5206\u79bb\u7269\u7406\u4fe1\u53f7\u4e0e\u7b97\u6cd5\u4f2a\u5f71\uff0c\u5c06\u566a\u58f0\u89c6\u4e3a\u5e72\u6270\u800c\u975e\u4fe1\u606f\u8d44\u6e90\u3002", "method": "\u63d0\u51faNoisomics\u6846\u67b6\uff0c\u57fa\u4e8e\u5bf9\u6bd4\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u5229\u7528\u6d41\u5f62\u5047\u8bbe\u548c\u5408\u6210\u566a\u58f0\u57fa\u56e0\u7ec4\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5206\u79bb\u8bed\u4e49\u4fe1\u53f7\u4e0e\u968f\u673a\u6270\u52a8\u3002", "result": "\u4ec5\u7528100\u4e2a\u8bad\u7ec3\u6837\u672c\u5c31\u8d85\u8d8a\u4e86\u4f20\u7edf\u9700\u898110\u4e07\u4e2a\u6837\u672c\u7684\u76d1\u7763\u57fa\u7ebf\uff0c\u572812\u4e2a\u8de8\u57df\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e63.8%\uff0c\u51b3\u5b9a\u7cfb\u6570\u63d0\u9ad885.1%\u3002", "conclusion": "\u5c06\u968f\u673a\u9000\u5316\u91cd\u65b0\u5b9a\u4e49\u4e3a\u91cd\u8981\u7684\u4fe1\u606f\u8d44\u6e90\uff0c\u65e0\u9700\u8bbe\u5907\u6821\u51c6\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u6210\u50cf\u8bca\u65ad\uff0c\u4e3a\u4ece\u6d88\u8d39\u6444\u5f71\u5230\u6df1\u5c42\u7ec4\u7ec7\u663e\u5fae\u955c\u7684\u591a\u4e2a\u5c3a\u5ea6\u5e94\u7528\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2601.17016", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17016", "abs": "https://arxiv.org/abs/2601.17016", "authors": ["Salah Feras Alali", "Mohammad Nashat Maasfeh", "Mucahid Kutlu", "Saban Kardas"], "title": "Measuring Political Stance and Consistency in Large Language Models", "comment": null, "summary": "With the incredible advancements in Large Language Models (LLMs), many people have started using them to satisfy their information needs. However, utilizing LLMs might be problematic for political issues where disagreement is common and model outputs may reflect training-data biases or deliberate alignment choices. To better characterize such behavior, we assess the stances of nine LLMs on 24 politically sensitive issues using five prompting techniques. We find that models often adopt opposing stances on several issues; some positions are malleable under prompting, while others remain stable. Among the models examined, Grok-3-mini is the most persistent, whereas Mistral-7B is the least. For issues involving countries with different languages, models tend to support the side whose language is used in the prompt. Notably, no prompting technique alters model stances on the Qatar blockade or the oppression of Palestinians. We hope these findings raise user awareness when seeking political guidance from LLMs and encourage developers to address these concerns.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u654f\u611f\u95ee\u9898\u4e0a\u5b58\u5728\u7acb\u573a\u5dee\u5f02\uff0c\u90e8\u5206\u7acb\u573a\u53d7\u63d0\u793a\u8bcd\u5f71\u54cd\uff0c\u90e8\u5206\u7acb\u573a\u7a33\u5b9a\u4e0d\u53d8\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u652f\u6301\u63d0\u793a\u8bcd\u6240\u7528\u8bed\u8a00\u7684\u56fd\u5bb6\u7acb\u573a\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4eba\u4eec\u5f00\u59cb\u4f7f\u7528\u5b83\u4eec\u6765\u6ee1\u8db3\u4fe1\u606f\u9700\u6c42\u3002\u7136\u800c\uff0c\u5728\u653f\u6cbb\u95ee\u9898\u4e0a\u4f7f\u7528LLMs\u53ef\u80fd\u5b58\u5728\u98ce\u9669\uff0c\u56e0\u4e3a\u6a21\u578b\u8f93\u51fa\u53ef\u80fd\u53cd\u6620\u8bad\u7ec3\u6570\u636e\u504f\u89c1\u6216\u4eba\u4e3a\u5bf9\u9f50\u9009\u62e9\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u653f\u6cbb\u654f\u611f\u95ee\u9898\u4e0a\u7684\u7acb\u573a\u8868\u73b0\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e869\u4e2aLLMs\u572824\u4e2a\u653f\u6cbb\u654f\u611f\u95ee\u9898\u4e0a\u7684\u7acb\u573a\uff0c\u4f7f\u7528\u4e865\u79cd\u63d0\u793a\u6280\u672f\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6d4b\u8bd5\u6765\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u653f\u6cbb\u8bae\u9898\u4e0a\u7684\u7acb\u573a\u8868\u73b0\u548c\u7a33\u5b9a\u6027\u3002", "result": "1. \u6a21\u578b\u5728\u591a\u4e2a\u95ee\u9898\u4e0a\u7ecf\u5e38\u91c7\u53d6\u5bf9\u7acb\u7acb\u573a\uff1b2. \u90e8\u5206\u7acb\u573a\u5728\u63d0\u793a\u8bcd\u5f71\u54cd\u4e0b\u4f1a\u6539\u53d8\uff0c\u90e8\u5206\u4fdd\u6301\u7a33\u5b9a\uff1b3. Grok-3-mini\u7acb\u573a\u6700\u7a33\u5b9a\uff0cMistral-7B\u6700\u4e0d\u7a33\u5b9a\uff1b4. \u5728\u6d89\u53ca\u591a\u8bed\u8a00\u56fd\u5bb6\u7684\u95ee\u9898\u4e0a\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u652f\u6301\u63d0\u793a\u8bcd\u6240\u7528\u8bed\u8a00\u7684\u56fd\u5bb6\u7acb\u573a\uff1b5. \u6240\u6709\u63d0\u793a\u6280\u672f\u90fd\u65e0\u6cd5\u6539\u53d8\u6a21\u578b\u5728\u5361\u5854\u5c14\u5c01\u9501\u548c\u5df4\u52d2\u65af\u5766\u538b\u8feb\u95ee\u9898\u4e0a\u7684\u7acb\u573a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u9192\u7528\u6237\u5728\u4f7f\u7528LLMs\u5bfb\u6c42\u653f\u6cbb\u6307\u5bfc\u65f6\u4fdd\u6301\u8b66\u60d5\uff0c\u5e76\u9f13\u52b1\u5f00\u53d1\u8005\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002LLMs\u5728\u653f\u6cbb\u654f\u611f\u95ee\u9898\u4e0a\u7684\u7acb\u573a\u5dee\u5f02\u548c\u504f\u89c1\u9700\u8981\u5f15\u8d77\u5173\u6ce8\u3002"}}
{"id": "2601.17343", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17343", "abs": "https://arxiv.org/abs/2601.17343", "authors": ["Wei Liu", "Haomei Xu", "Hongkai Liu", "Zhiying Deng", "Ruixuan Li", "Heng Huang", "Yee Whye Teh", "Wee Sun Lee"], "title": "Are We Evaluating the Edit Locality of LLM Model Editing Properly?", "comment": null, "summary": "Model editing has recently emerged as a popular paradigm for efficiently updating knowledge in LLMs. A central desideratum of updating knowledge is to balance editing efficacy, i.e., the successful injection of target knowledge, and specificity (also known as edit locality), i.e., the preservation of existing non-target knowledge. However, we find that existing specificity evaluation protocols are inadequate for this purpose. We systematically elaborated on the three fundamental issues it faces. Beyond the conceptual issues, we further empirically demonstrate that existing specificity metrics are weakly correlated with the strength of specificity regularizers. We also find that current metrics lack sufficient sensitivity, rendering them ineffective at distinguishing the specificity performance of different methods. Finally, we propose a constructive evaluation protocol. Under this protocol, the conflict between open-ended LLMs and the assumption of determined answers is eliminated, query-independent fluency biases are avoided, and the evaluation strictness can be smoothly adjusted within a near-continuous space. Experiments across various LLMs, datasets, and editing methods show that metrics derived from the proposed protocol are more sensitive to changes in the strength of specificity regularizers and exhibit strong correlation with them, enabling more fine-grained discrimination of different methods' knowledge preservation capabilities.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u4e2d\u7684\u7279\u5f02\u6027\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65b0\u534f\u8bae\u80fd\u66f4\u654f\u611f\u5730\u8861\u91cf\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u6a21\u578b\u7f16\u8f91\u5df2\u6210\u4e3a\u66f4\u65b0LLM\u77e5\u8bc6\u7684\u91cd\u8981\u8303\u5f0f\uff0c\u4f46\u73b0\u6709\u7279\u5f02\u6027\u8bc4\u4f30\u534f\u8bae\u5728\u5e73\u8861\u7f16\u8f91\u6548\u679c\u548c\u77e5\u8bc6\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u7684\u77e5\u8bc6\u4fdd\u62a4\u80fd\u529b\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u73b0\u6709\u7279\u5f02\u6027\u8bc4\u4f30\u534f\u8bae\u7684\u4e09\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\uff1a\u6d88\u9664\u5f00\u653eLLM\u4e0e\u786e\u5b9a\u7b54\u6848\u5047\u8bbe\u7684\u51b2\u7a81\u3001\u907f\u514d\u67e5\u8be2\u65e0\u5173\u7684\u6d41\u7545\u6027\u504f\u5dee\u3001\u5141\u8bb8\u5728\u8fd1\u8fde\u7eed\u7a7a\u95f4\u4e2d\u5e73\u6ed1\u8c03\u6574\u8bc4\u4f30\u4e25\u683c\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u534f\u8bae\u751f\u6210\u7684\u6307\u6807\u5bf9\u7279\u5f02\u6027\u6b63\u5219\u5316\u5f3a\u5ea6\u53d8\u5316\u66f4\u654f\u611f\uff0c\u4e0e\u6b63\u5219\u5316\u5f3a\u5ea6\u5f3a\u76f8\u5173\uff0c\u80fd\u66f4\u7cbe\u7ec6\u5730\u533a\u5206\u4e0d\u540c\u65b9\u6cd5\u7684\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u8bc4\u4f30\u534f\u8bae\u89e3\u51b3\u4e86\u73b0\u6709\u7279\u5f02\u6027\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u4e3a\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u7684\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u654f\u611f\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.17063", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17063", "abs": "https://arxiv.org/abs/2601.17063", "authors": ["Byeongju Kim", "Jungwan Lee", "Donghyeon Han", "Hoi-Jun Yoo", "Sangyeob Kim"], "title": "FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices", "comment": null, "summary": "Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.", "AI": {"tldr": "FlashMoE\u662f\u4e00\u4e2a\u5c06\u4e0d\u6d3b\u8dc3\u4e13\u5bb6\u5378\u8f7d\u5230SSD\u7684\u7cfb\u7edf\uff0c\u4f7f\u5927\u578bMoE\u6a21\u578b\u80fd\u5728\u5185\u5b58\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u9ad8\u6548\u63a8\u7406\uff0c\u901a\u8fc7\u667a\u80fd\u7f13\u5b58\u7b56\u7565\u51cf\u5c11\u5b58\u50a8I/O\uff0c\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740MoE\u6a21\u578b\u589e\u957f\u5230\u6570\u767eGB\uff0c\u73b0\u6709\u7684DRAM\u5378\u8f7d\u65b9\u6848\uff08\u5982Fiddler\u3001DAOP\uff09\u5728\u5185\u5b58\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684MoE\u63a8\u7406\u3002", "method": "\u63d0\u51faFlashMoE\u7cfb\u7edf\uff0c\u5c06\u4e0d\u6d3b\u8dc3\u4e13\u5bb6\u5378\u8f7d\u5230SSD\u5b58\u50a8\uff0c\u91c7\u7528\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u7f13\u5b58\u7b56\u7565\uff0c\u7ed3\u5408\u6700\u8fd1\u4f7f\u7528\u548c\u9891\u7387\u4fe1\u53f7\u81ea\u9002\u5e94\u7ba1\u7406\u4e13\u5bb6\u91cd\u7528\uff0c\u6700\u5927\u5316\u7f13\u5b58\u547d\u4e2d\u7387\u3002", "result": "\u5728\u771f\u5b9e\u684c\u9762\u786c\u4ef6\u5e73\u53f0\u4e0a\uff0cFlashMoE\u76f8\u6bd4LRU\u548cLFU\u7b49\u4f20\u7edf\u5378\u8f7d\u7b56\u7565\u5c06\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u5347\u9ad8\u8fbe51%\uff0c\u76f8\u6bd4\u73b0\u6709MoE\u63a8\u7406\u7cfb\u7edf\u5b9e\u73b0\u6700\u9ad82.6\u500d\u52a0\u901f\u3002", "conclusion": "FlashMoE\u901a\u8fc7SSD\u5378\u8f7d\u548c\u667a\u80fd\u7f13\u5b58\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u578bMoE\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u95ee\u9898\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548MoE\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17492", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17492", "abs": "https://arxiv.org/abs/2601.17492", "authors": ["Jin Li", "Huilin Gu", "Shoujin Wang", "Qi Zhang", "Shui Yu", "Chen Wang", "Xiwei Xu", "Fang Chen"], "title": "Towards Fair Large Language Model-based Recommender Systems without Costly Retraining", "comment": "Accepted by WWW 2026", "summary": "Large Language Models (LLMs) have revolutionized Recommender Systems (RS) through advanced generative user modeling. However, LLM-based RS (LLM-RS) often inadvertently perpetuates bias present in the training data, leading to severe fairness issues. Addressing these fairness problems in LLM-RS faces two significant challenges. 1) Existing debiasing methods, designed for specific bias types, lack the generality to handle diverse or emerging biases in real-world applications. 2) Debiasing methods relying on retraining are computationally infeasible given the massive parameter scale of LLMs. To overcome these challenges, we propose FUDLR (Fast Unified Debiasing for LLM-RS). The core idea is to reformulate the debiasing problem as an efficient machine unlearning task with two stages. First, FUDLR identifies bias-inducing samples to unlearn through a novel bias-agnostic mask, optimized to balance fairness improvement with accuracy preservation. Its bias-agnostic design allows adaptability to various or co-existing biases simply by incorporating different fairness metrics. Second, FUDLR performs efficient debiasing by estimating and removing the influence of identified samples on model parameters. Extensive experiments demonstrate that FUDLR effectively and efficiently improves fairness while preserving recommendation accuracy, offering a practical path toward socially responsible LLM-RS. The code and data are available at https://github.com/JinLi-i/FUDLR.", "AI": {"tldr": "FUDLR\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u7edf\u4e00\u7684LLM\u63a8\u8350\u7cfb\u7edf\u53bb\u504f\u65b9\u6cd5\uff0c\u5c06\u53bb\u504f\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9ad8\u6548\u7684\u673a\u5668\u9057\u5fd8\u4efb\u52a1\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5728\u4fdd\u6301\u63a8\u8350\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u5347\u516c\u5e73\u6027\u3002", "motivation": "LLM\u63a8\u8350\u7cfb\u7edf\u867d\u7136\u9769\u65b0\u4e86\u7528\u6237\u5efa\u6a21\uff0c\u4f46\u5bb9\u6613\u65e0\u610f\u4e2d\u5ef6\u7eed\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u89c1\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u9488\u5bf9\u7279\u5b9a\u504f\u89c1\u7c7b\u578b\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u5904\u7406\u591a\u6837\u6216\u65b0\u5174\u504f\u89c1\u7684\u901a\u7528\u6027\uff1b2) \u4f9d\u8d56\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728LLM\u5927\u89c4\u6a21\u53c2\u6570\u4e0b\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002", "method": "FUDLR\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u65b0\u9896\u7684\u504f\u89c1\u65e0\u5173\u63a9\u7801\u8bc6\u522b\u9700\u8981\u9057\u5fd8\u7684\u504f\u89c1\u8bf1\u5bfc\u6837\u672c\uff0c\u4f18\u5316\u516c\u5e73\u6027\u6539\u8fdb\u4e0e\u51c6\u786e\u6027\u4fdd\u6301\u7684\u5e73\u8861\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4f30\u8ba1\u548c\u79fb\u9664\u8fd9\u4e9b\u6837\u672c\u5bf9\u6a21\u578b\u53c2\u6570\u7684\u5f71\u54cd\u6765\u9ad8\u6548\u53bb\u504f\u3002\u5176\u504f\u89c1\u65e0\u5173\u8bbe\u8ba1\u5141\u8bb8\u901a\u8fc7\u7eb3\u5165\u4e0d\u540c\u516c\u5e73\u6027\u6307\u6807\u6765\u9002\u5e94\u5404\u79cd\u6216\u5171\u5b58\u7684\u504f\u89c1\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cFUDLR\u80fd\u6709\u6548\u4e14\u9ad8\u6548\u5730\u63d0\u5347\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u8350\u51c6\u786e\u6027\uff0c\u4e3a\u6784\u5efa\u793e\u4f1a\u8d23\u4efb\u7684LLM\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "conclusion": "FUDLR\u4e3aLLM\u63a8\u8350\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u53bb\u504f\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u673a\u5668\u9057\u5fd8\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u63a8\u8350\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u5904\u7406\u591a\u6837\u504f\u89c1\u7684\u80fd\u529b\u3002"}}
{"id": "2601.17346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17346", "abs": "https://arxiv.org/abs/2601.17346", "authors": ["Haoxin Xu", "Changyong Qi", "Tong Liu", "Bohao Zhang", "Anna He", "Bingqian Jiang", "Longwei Zheng", "Xiaoqing Gu"], "title": "Multi-Agent Learning Path Planning via LLMs", "comment": null, "summary": "The integration of large language models (LLMs) into intelligent tutoring systems offers transformative potential for personalized learning in higher education. However, most existing learning path planning approaches lack transparency, adaptability, and learner-centered explainability. To address these challenges, this study proposes a novel Multi-Agent Learning Path Planning (MALPP) framework that leverages a role- and rule-based collaboration mechanism among intelligent agents, each powered by LLMs. The framework includes three task-specific agents: a learner analytics agent, a path planning agent, and a reflection agent. These agents collaborate via structured prompts and predefined rules to analyze learning profiles, generate tailored learning paths, and iteratively refine them with interpretable feedback. Grounded in Cognitive Load Theory and Zone of Proximal Development, the system ensures that recommended paths are cognitively aligned and pedagogically meaningful. Experiments conducted on the MOOCCubeX dataset using seven LLMs show that MALPP significantly outperforms baseline models in path quality, knowledge sequence consistency, and cognitive load alignment. Ablation studies further validate the effectiveness of the collaborative mechanism and theoretical constraints. This research contributes to the development of trustworthy, explainable AI in education and demonstrates a scalable approach to learner-centered adaptive instruction powered by LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684MALPP\u6846\u67b6\uff0c\u5229\u7528LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u4e2a\u6027\u5316\u5b66\u4e60\u8def\u5f84\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u5ea6\u3001\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u5bfc\u5b66\u7cfb\u7edf\u4e2d\u7684\u5b66\u4e60\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u666e\u904d\u7f3a\u4e4f\u900f\u660e\u5ea6\u3001\u9002\u5e94\u6027\u548c\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u7b49\u6559\u80b2\u4e2a\u6027\u5316\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u53d1\u6325\u3002", "method": "\u63d0\u51faMALPP\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u89d2\u8272\u548c\u89c4\u5219\u7684\u534f\u4f5c\u673a\u5236\uff0c\u5305\u542b\u4e09\u4e2a\u4efb\u52a1\u7279\u5b9a\u7684\u667a\u80fd\u4f53\uff1a\u5b66\u4e60\u8005\u5206\u6790\u667a\u80fd\u4f53\u3001\u8def\u5f84\u89c4\u5212\u667a\u80fd\u4f53\u548c\u53cd\u601d\u667a\u80fd\u4f53\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u9884\u5b9a\u4e49\u89c4\u5219\u534f\u4f5c\uff0c\u57fa\u4e8e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u548c\u6700\u8fd1\u53d1\u5c55\u533a\u7406\u8bba\u8fdb\u884c\u5206\u6790\u3001\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\u5b66\u4e60\u8def\u5f84\u3002", "result": "\u5728MOOCCubeX\u6570\u636e\u96c6\u4e0a\u4f7f\u75287\u4e2aLLM\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMALPP\u5728\u8def\u5f84\u8d28\u91cf\u3001\u77e5\u8bc6\u5e8f\u5217\u4e00\u81f4\u6027\u548c\u8ba4\u77e5\u8d1f\u8377\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u534f\u4f5c\u673a\u5236\u548c\u7406\u8bba\u7ea6\u675f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u4fe1\u8d56\u3001\u53ef\u89e3\u91ca\u7684\u6559\u80b2AI\u53d1\u5c55\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u3001\u53ef\u6269\u5c55\u7684\u4ee5\u5b66\u4e60\u8005\u4e3a\u4e2d\u5fc3\u7684\u81ea\u9002\u5e94\u6559\u5b66\u65b9\u6cd5\u3002"}}
{"id": "2601.17065", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17065", "abs": "https://arxiv.org/abs/2601.17065", "authors": ["Haoxuan Li", "He Chang", "Yunshan Ma", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting", "comment": null, "summary": "Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.", "AI": {"tldr": "\u63d0\u51faThinkTank-ME\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u534f\u4f5c\u6a21\u62df\u73b0\u5b9e\u6218\u7565\u51b3\u7b56\uff0c\u89e3\u51b3\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u5355\u4e00\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u590d\u6742\u5730\u7f18\u653f\u6cbb\u7ec6\u5fae\u5dee\u522b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4e8b\u4ef6\u9884\u6d4b\u65b9\u6cd5\u91c7\u7528\u5355\u4e00\u6a21\u578b\u67b6\u6784\uff0c\u53ea\u80fd\u6cbf\u7740\u5355\u4e00\u663e\u5f0f\u8f68\u8ff9\u751f\u6210\u9884\u6d4b\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u533a\u57df\u80cc\u666f\u4e0b\u6355\u6349\u591a\u6837\u5316\u5730\u7f18\u653f\u6cbb\u7ec6\u5fae\u5dee\u522b\u7684\u80fd\u529b\u3002\u4e8b\u4ef6\u9884\u6d4b\u672c\u8d28\u4e0a\u53d7\u5230\u591a\u65b9\u9762\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u56fd\u9645\u5173\u7cfb\u3001\u533a\u57df\u5386\u53f2\u52a8\u6001\u548c\u6587\u5316\u80cc\u666f\u3002", "method": "\u5f15\u5165ThinkTank-ME\u6846\u67b6\uff0c\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u4e2d\u6218\u7565\u51b3\u7b56\u7684\u534f\u4f5c\u4e13\u5bb6\u5206\u6790\u3002\u540c\u65f6\u6784\u5efaPOLECAT-FOR-ME\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u4e2d\u4e1c\u7684\u4e8b\u4ef6\u9884\u6d4b\u57fa\u51c6\uff0c\u7528\u4e8e\u4fc3\u8fdb\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u591a\u4e13\u5bb6\u534f\u4f5c\u5728\u5904\u7406\u590d\u6742\u65f6\u95f4\u5730\u7f18\u653f\u6cbb\u9884\u6d4b\u4efb\u52a1\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "ThinkTank-ME\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u4e13\u5bb6\u534f\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e2d\u4e1c\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u5355\u4e00\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u591a\u4e13\u5bb6\u534f\u4f5c\u5728\u590d\u6742\u5730\u7f18\u653f\u6cbb\u9884\u6d4b\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.17500", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17500", "abs": "https://arxiv.org/abs/2601.17500", "authors": ["Emmanouil Georgios Lionis", "Jia-Huei Ju", "Angelos Nalmpantis", "Casper Thuis", "Sean MacAvaney", "Andrew Yates"], "title": "To Case or Not to Case: An Empirical Study in Learned Sparse Retrieval", "comment": "This preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in ECIR2026 (Part I) Advances in Information Retrieval", "summary": "Learned Sparse Retrieval (LSR) methods construct sparse lexical representations of queries and documents that can be efficiently searched using inverted indexes. Existing LSR approaches have relied almost exclusively on uncased backbone models, whose vocabularies exclude case-sensitive distinctions, thereby reducing vocabulary mismatch. However, the most recent state-of-the-art language models are only available in cased versions. Despite this shift, the impact of backbone model casing on LSR has not been studied, potentially posing a risk to the viability of the method going forward. To fill this gap, we systematically evaluate paired cased and uncased versions of the same backbone models across multiple datasets to assess their suitability for LSR. Our findings show that LSR models with cased backbone models by default perform substantially worse than their uncased counterparts; however, this gap can be eliminated by pre-processing the text to lowercase. Moreover, our token-level analysis reveals that, under lowercasing, cased models almost entirely suppress cased vocabulary items and behave effectively as uncased models, explaining their restored performance. This result broadens the applicability of recent cased models to the LSR setting and facilitates the integration of stronger backbone architectures into sparse retrieval. The complete code and implementation for this project are available at: https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u5927\u5c0f\u5199\u654f\u611f\u7684\u57fa\u7840\u6a21\u578b\u9ed8\u8ba4\u8868\u73b0\u663e\u8457\u5dee\u4e8e\u5927\u5c0f\u5199\u4e0d\u654f\u611f\u6a21\u578b\uff0c\u4f46\u901a\u8fc7\u5c06\u6587\u672c\u9884\u5904\u7406\u4e3a\u5c0f\u5199\u53ef\u4ee5\u6d88\u9664\u8fd9\u4e00\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u53ea\u6709\u5927\u5c0f\u5199\u654f\u611f\u7248\u672c\uff0c\u4f46\u7a00\u758f\u68c0\u7d22\u65b9\u6cd5\u4e00\u76f4\u4f9d\u8d56\u5927\u5c0f\u5199\u4e0d\u654f\u611f\u7684\u57fa\u7840\u6a21\u578b\u3002\u8fd9\u79cd\u8f6c\u53d8\u5bf9\u7a00\u758f\u68c0\u7d22\u65b9\u6cd5\u7684\u5f71\u54cd\u5c1a\u672a\u7814\u7a76\uff0c\u53ef\u80fd\u5a01\u80c1\u8be5\u65b9\u6cd5\u7684\u672a\u6765\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u540c\u4e00\u57fa\u7840\u6a21\u578b\u7684\u5927\u5c0f\u5199\u654f\u611f\u548c\u5927\u5c0f\u5199\u4e0d\u654f\u611f\u7248\u672c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6587\u672c\u9884\u5904\u7406\u4e3a\u5c0f\u5199\u6765\u6d4b\u8bd5\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u8fdb\u884c\u8bcd\u5143\u7ea7\u5206\u6790\u3002", "result": "\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u5927\u5c0f\u5199\u654f\u611f\u57fa\u7840\u6a21\u578b\u7684\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u8868\u73b0\u663e\u8457\u5dee\u4e8e\u5927\u5c0f\u5199\u4e0d\u654f\u611f\u7248\u672c\uff1b\u4f46\u901a\u8fc7\u5c06\u6587\u672c\u9884\u5904\u7406\u4e3a\u5c0f\u5199\u53ef\u4ee5\u5b8c\u5168\u6d88\u9664\u8fd9\u4e00\u6027\u80fd\u5dee\u8ddd\u3002\u8bcd\u5143\u7ea7\u5206\u6790\u663e\u793a\uff0c\u5728\u5c0f\u5199\u5904\u7406\u540e\uff0c\u5927\u5c0f\u5199\u654f\u611f\u6a21\u578b\u51e0\u4e4e\u5b8c\u5168\u6291\u5236\u4e86\u5927\u5c0f\u5199\u654f\u611f\u8bcd\u6c47\u9879\uff0c\u5b9e\u9645\u4e0a\u8868\u73b0\u5f97\u50cf\u5927\u5c0f\u5199\u4e0d\u654f\u611f\u6a21\u578b\u3002", "conclusion": "\u8fd9\u4e00\u53d1\u73b0\u6269\u5c55\u4e86\u6700\u65b0\u5927\u5c0f\u5199\u654f\u611f\u6a21\u578b\u5728\u7a00\u758f\u68c0\u7d22\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u4fc3\u8fdb\u4e86\u66f4\u5f3a\u57fa\u7840\u67b6\u6784\u4e0e\u7a00\u758f\u68c0\u7d22\u7684\u96c6\u6210\uff0c\u786e\u4fdd\u4e86\u8be5\u65b9\u6cd5\u7684\u6301\u7eed\u53ef\u884c\u6027\u3002"}}
{"id": "2601.17049", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17049", "abs": "https://arxiv.org/abs/2601.17049", "authors": ["Christina Garcia", "Nhat Tan Le", "Taihei Fujioka", "Umang Dobhal", "Milyun Ni'ma Shoumi", "Thanh Nha Nguyen", "Sozo Inoue"], "title": "Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support", "comment": "14 pages, 7 figures, 3 tables. Summary paper for a coding challenge hosted in ISAS 2025", "summary": "This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.", "AI": {"tldr": "ISAS 2025\u4e3e\u529e\u7684\"\u8bc6\u522b\u672a\u89c1\uff1a\u57fa\u4e8e\u59ff\u6001\u6570\u636e\u7684\u5f02\u5e38\u884c\u4e3a\u8bc6\u522b\"\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u901a\u8fc7\u975e\u4fb5\u5165\u5f0f\u59ff\u6001\u6570\u636e\u81ea\u52a8\u8bc6\u522b\u53d1\u80b2\u969c\u788d\u4eba\u58eb\u8bbe\u65bd\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\uff0c\u5438\u5f15\u4e8640\u652f\u56e2\u961f\u53c2\u4e0e\uff0c\u4f7f\u7528\u7559\u4e00\u53d7\u8bd5\u8005\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u53d1\u80b2\u969c\u788d\u4eba\u58eb\u8bbe\u65bd\u4e2d\u5f02\u5e38\u884c\u4e3a\u81ea\u52a8\u8bc6\u522b\u7684\u5173\u952e\u9700\u6c42\uff0c\u4f7f\u7528\u975e\u4fb5\u5165\u5f0f\u59ff\u6001\u4f30\u8ba1\u6570\u636e\uff0c\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0d\u5e73\u8861\u548c\u65f6\u95f4\u4e0d\u89c4\u5219\u6027\u6311\u6218\u3002", "method": "\u6311\u6218\u8d5b\u57fa\u4e8e\u4ece\u6a21\u62df\u573a\u666f\u89c6\u9891\u4e2d\u63d0\u53d6\u7684\u9aa8\u9abc\u5173\u952e\u70b9\u6570\u636e\uff0c\u91c7\u7528\u7559\u4e00\u53d7\u8bd5\u8005\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\u786e\u4fdd\u4e3b\u4f53\u65e0\u5173\u6cdb\u5316\uff0c\u4f7f\u7528\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u4ee5\u5e94\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u3002", "result": "40\u652f\u56e2\u961f\u53c2\u4e0e\u5e76\u5e94\u7528\u4e86\u4ece\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5230\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u591a\u79cd\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u5728\u566a\u58f0\u591a\u3001\u4f4e\u7ef4\u6570\u636e\u4e2d\u5efa\u6a21\u7f55\u89c1\u3001\u7a81\u53d1\u52a8\u4f5c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u6355\u6349\u65f6\u95f4\u548c\u4e0a\u4e0b\u6587\u7ec6\u5fae\u5dee\u522b\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u5f3a\u8c03\u4e86\u5728\u566a\u58f0\u4f4e\u7ef4\u6570\u636e\u4e2d\u5efa\u6a21\u7f55\u89c1\u884c\u4e3a\u7684\u65f6\u95f4\u5efa\u6a21\u91cd\u8981\u6027\uff0c\u4e3a\u533b\u7597\u4fdd\u5065\u548c\u884c\u4e3a\u76d1\u6d4b\u7b49\u793e\u4f1a\u8d23\u4efbAI\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.17018", "categories": ["cs.CY", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.17018", "abs": "https://arxiv.org/abs/2601.17018", "authors": ["Margarida Romero"], "title": "Evaluating the Evolution of Critical Thinking, Creativity, Communication and Collaboration in Higher Education Courses", "comment": null, "summary": "The development of Creativity, Communication, Critical Thinking, and Collaboration (the 4Cs) is a central objective of contemporary competency-based education. However, empirical evidence on how these competencies evolve across learning modules and instructional phases remains limited. This study evaluates the evolution of the 4Cs from pre-pilot to pilot implementation phases across three educational contexts, using the project's 4Cs theoretical framework as an analytical lens. The analysis of three pilot cases (IASIS, EASD, and UPATRAS) compares the 4Cs scores to identify patterns of growth, stagnation, or decline over time. Results indicate that communication and critical thinking showed the most consistent and substantial improvements, particularly in pilots with lower pre-pilot baselines, suggesting that structured pilot interventions effectively support cognitive and expressive competencies. In contrast, creativity exhibited context-dependent outcomes, while collaboration emerged as the most fragile competency, often stagnating or declining during scale-up. Interpreted through the theoretical framework, these findings suggest that competency evolution is strongly shaped by instructional design, assessment alignment, and learning activity structures rather than learner ability alone. The study contributes empirical validation to the 4Cs framework and highlights the need for differentiated, competency-sensitive design and evaluation strategies when scaling educational modules.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e864Cs\u80fd\u529b\uff08\u521b\u9020\u529b\u3001\u6c9f\u901a\u3001\u6279\u5224\u6027\u601d\u7ef4\u3001\u534f\u4f5c\uff09\u5728\u4e09\u4e2a\u6559\u80b2\u6848\u4f8b\u4e2d\u4ece\u9884\u8bd5\u70b9\u5230\u8bd5\u70b9\u9636\u6bb5\u7684\u6f14\u53d8\uff0c\u53d1\u73b0\u6c9f\u901a\u548c\u6279\u5224\u6027\u601d\u7ef4\u6539\u5584\u6700\u663e\u8457\uff0c\u521b\u9020\u529b\u7ed3\u679c\u4f9d\u8d56\u60c5\u5883\uff0c\u800c\u534f\u4f5c\u80fd\u529b\u6700\u8106\u5f31\u3002", "motivation": "\u5c3d\u7ba14Cs\u80fd\u529b\uff08\u521b\u9020\u529b\u3001\u6c9f\u901a\u3001\u6279\u5224\u6027\u601d\u7ef4\u3001\u534f\u4f5c\uff09\u662f\u5f53\u4ee3\u80fd\u529b\u672c\u4f4d\u6559\u80b2\u7684\u6838\u5fc3\u76ee\u6807\uff0c\u4f46\u5173\u4e8e\u8fd9\u4e9b\u80fd\u529b\u5982\u4f55\u5728\u4e0d\u540c\u5b66\u4e60\u6a21\u5757\u548c\u6559\u5b66\u9636\u6bb5\u6f14\u53d8\u7684\u5b9e\u8bc1\u8bc1\u636e\u4ecd\u7136\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f304Cs\u80fd\u529b\u5728\u4ece\u9884\u8bd5\u70b9\u5230\u8bd5\u70b9\u5b9e\u65bd\u9636\u6bb5\u7684\u6f14\u53d8\u60c5\u51b5\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u9879\u76ee\u76844Cs\u7406\u8bba\u6846\u67b6\u4f5c\u4e3a\u5206\u6790\u89c6\u89d2\uff0c\u8bc4\u4f30\u4e86\u4e09\u4e2a\u6559\u80b2\u6848\u4f8b\uff08IASIS\u3001EASD\u548cUPATRAS\uff09\u4e2d4Cs\u80fd\u529b\u4ece\u9884\u8bd5\u70b9\u5230\u8bd5\u70b9\u9636\u6bb5\u7684\u6f14\u53d8\u3002\u901a\u8fc7\u5206\u6790\u4e09\u4e2a\u8bd5\u70b9\u6848\u4f8b\uff0c\u6bd4\u8f83\u4e864Cs\u5f97\u5206\u4ee5\u8bc6\u522b\u968f\u65f6\u95f4\u589e\u957f\u3001\u505c\u6ede\u6216\u4e0b\u964d\u7684\u6a21\u5f0f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u6c9f\u901a\u548c\u6279\u5224\u6027\u601d\u7ef4\u8868\u73b0\u51fa\u6700\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u5584\uff0c\u7279\u522b\u662f\u5728\u9884\u8bd5\u70b9\u57fa\u7ebf\u8f83\u4f4e\u7684\u8bd5\u70b9\u4e2d\uff0c\u8868\u660e\u7ed3\u6784\u5316\u8bd5\u70b9\u5e72\u9884\u80fd\u6709\u6548\u652f\u6301\u8ba4\u77e5\u548c\u8868\u8fbe\u80fd\u529b\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u521b\u9020\u529b\u8868\u73b0\u51fa\u60c5\u5883\u4f9d\u8d56\u7684\u7ed3\u679c\uff0c\u800c\u534f\u4f5c\u80fd\u529b\u5219\u662f\u6700\u8106\u5f31\u7684\u80fd\u529b\uff0c\u5728\u6269\u5927\u89c4\u6a21\u65f6\u7ecf\u5e38\u505c\u6ede\u6216\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u89e3\u91ca\uff0c\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\u80fd\u529b\u6f14\u53d8\u4e3b\u8981\u53d7\u6559\u5b66\u8bbe\u8ba1\u3001\u8bc4\u4f30\u5bf9\u9f50\u548c\u5b66\u4e60\u6d3b\u52a8\u7ed3\u6784\u7684\u5f71\u54cd\uff0c\u800c\u975e\u4ec5\u7531\u5b66\u4e60\u8005\u80fd\u529b\u51b3\u5b9a\u3002\u7814\u7a76\u4e3a4Cs\u6846\u67b6\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u6269\u5c55\u6559\u80b2\u6a21\u5757\u65f6\u9700\u8981\u5dee\u5f02\u5316\u7684\u3001\u80fd\u529b\u654f\u611f\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u7b56\u7565\u3002"}}
{"id": "2601.17348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17348", "abs": "https://arxiv.org/abs/2601.17348", "authors": ["Srikant Panda", "Sourabh Singh Yadav", "Palkesh Malviya"], "title": "Auditing Disability Representation in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) are increasingly deployed in socially sensitive applications, yet their behavior with respect to disability remains underexplored. We study disability aware descriptions for person centric images, where models often transition from evidence grounded factual description to interpretation shift including introduction of unsupported inferences beyond observable visual evidence. To systematically analyze this phenomenon, we introduce a benchmark based on paired Neutral Prompts (NP) and Disability-Contextualised Prompts (DP) and evaluate 15 state-of-the-art open- and closed-source VLMs under a zero-shot setting across 9 disability categories. Our evaluation framework treats interpretive fidelity as core objective and combines standard text-based metrics capturing affective degradation through shifts in sentiment, social regard and response length with an LLM-as-judge protocol, validated by annotators with lived experience of disability. We find that introducing disability context consistently degrades interpretive fidelity, inducing interpretation shifts characterised by speculative inference, narrative elaboration, affective degradation and deficit oriented framing. These effects are further amplified along race and gender dimension. Finally, we demonstrate targeted prompting and preference fine-tuning effectively improves interpretive fidelity and reduces substantially interpretation shifts.", "AI": {"tldr": "\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63cf\u8ff0\u6b8b\u75be\u4eba\u56fe\u50cf\u65f6\u7684\u89e3\u91ca\u504f\u79fb\u95ee\u9898\uff0c\u53d1\u73b0\u5f15\u5165\u6b8b\u75be\u4e0a\u4e0b\u6587\u4f1a\u964d\u4f4e\u89e3\u91ca\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u63a8\u6d4b\u6027\u63a8\u65ad\u3001\u53d9\u4e8b\u6269\u5c55\u3001\u60c5\u611f\u964d\u7ea7\u548c\u7f3a\u9677\u5bfc\u5411\u6846\u67b6\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u6548\u5e94\u5728\u79cd\u65cf\u548c\u6027\u522b\u7ef4\u5ea6\u4e0a\u8fdb\u4e00\u6b65\u653e\u5927\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u793e\u4f1a\u654f\u611f\u9886\u57df\uff0c\u4f46\u5176\u5728\u6b8b\u75be\u65b9\u9762\u7684\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u6a21\u578b\u5728\u63cf\u8ff0\u4eba\u7269\u56fe\u50cf\u65f6\uff0c\u7ecf\u5e38\u4ece\u57fa\u4e8e\u8bc1\u636e\u7684\u4e8b\u5b9e\u63cf\u8ff0\u8f6c\u5411\u89e3\u91ca\u504f\u79fb\uff0c\u5305\u62ec\u5f15\u5165\u8d85\u51fa\u53ef\u89c2\u5bdf\u89c6\u89c9\u8bc1\u636e\u7684\u4e0d\u652f\u6301\u63a8\u65ad\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4e2d\u6027\u63d0\u793a(NP)\u548c\u6b8b\u75be\u60c5\u5883\u5316\u63d0\u793a(DP)\u914d\u5bf9\u7684\u57fa\u51c6\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f3015\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90VLM\uff0c\u6db5\u76d69\u4e2a\u6b8b\u75be\u7c7b\u522b\u3002\u8bc4\u4f30\u6846\u67b6\u5c06\u89e3\u91ca\u4fdd\u771f\u5ea6\u4f5c\u4e3a\u6838\u5fc3\u76ee\u6807\uff0c\u7ed3\u5408\u6807\u51c6\u6587\u672c\u6307\u6807\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u534f\u8bae\uff0c\u7531\u6709\u6b8b\u75be\u751f\u6d3b\u7ecf\u9a8c\u7684\u6ce8\u91ca\u8005\u9a8c\u8bc1\u3002", "result": "\u5f15\u5165\u6b8b\u75be\u4e0a\u4e0b\u6587\u4f1a\u6301\u7eed\u964d\u4f4e\u89e3\u91ca\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u89e3\u91ca\u504f\u79fb\uff0c\u8868\u73b0\u4e3a\u63a8\u6d4b\u6027\u63a8\u65ad\u3001\u53d9\u4e8b\u6269\u5c55\u3001\u60c5\u611f\u964d\u7ea7\u548c\u7f3a\u9677\u5bfc\u5411\u6846\u67b6\u3002\u8fd9\u4e9b\u6548\u5e94\u5728\u79cd\u65cf\u548c\u6027\u522b\u7ef4\u5ea6\u4e0a\u8fdb\u4e00\u6b65\u653e\u5927\u3002\u9488\u5bf9\u6027\u63d0\u793a\u548c\u504f\u597d\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u9ad8\u89e3\u91ca\u4fdd\u771f\u5ea6\u5e76\u663e\u8457\u51cf\u5c11\u89e3\u91ca\u504f\u79fb\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63cf\u8ff0\u6b8b\u75be\u4eba\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u89e3\u91ca\u504f\u79fb\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u578b\u884c\u4e3a\u4ee5\u786e\u4fdd\u66f4\u51c6\u786e\u3001\u5c0a\u91cd\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u63cf\u8ff0\u3002\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u5982\u7279\u5b9a\u63d0\u793a\u548c\u5fae\u8c03\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\u3002"}}
{"id": "2601.17050", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17050", "abs": "https://arxiv.org/abs/2601.17050", "authors": ["Hongjun An", "Yiliang Song", "Jiawei Shao", "Zhe Sun", "Xuelong Li"], "title": "Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence", "comment": "Initial Version, Pending Updates. We welcome any feedback and suggestions for improvement. Please feel free to contact us at an.hongjun@foxmail.com", "summary": "Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.", "AI": {"tldr": "SP-VLM\u6846\u67b6\u901a\u8fc7\u5355\u50cf\u7d20\u4f20\u611f\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96c6\u6210\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u7684\u76d1\u63a7\u96be\u9898\u3002", "motivation": "\u5728\u6d17\u624b\u95f4\u3001\u66f4\u8863\u5ba4\u7b49\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u76d1\u63a7\u56e0\u9690\u79c1\u6cd5\u89c4\u548c\u4f26\u7406\u95ee\u9898\u88ab\u7981\u6b62\u6216\u9650\u5236\uff0c\u4f46\u6b3a\u51cc\u3001\u9a9a\u6270\u7b49\u6709\u5bb3\u793e\u4f1a\u4e92\u52a8\u53c8\u9700\u8981\u53ca\u65f6\u5e72\u9884\uff0c\u8fd9\u6784\u6210\u4e86\u76d1\u63a7\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u77db\u76fe\u3002", "method": "\u63d0\u51fa\u5355\u50cf\u7d20\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08SP-VLM\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u7ef4\u5355\u50cf\u7d20\u6a21\u6001\u6355\u6349\u4eba\u4f53\u52a8\u6001\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u96c6\u6210\u63a8\u65ad\u590d\u6742\u884c\u4e3a\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u8bbe\u8ba1\u3002", "result": "\u5355\u50cf\u7d20\u4f20\u611f\u80fd\u6709\u6548\u6291\u5236\u8eab\u4efd\u53ef\u8bc6\u522b\u6027\uff0c\u4f7f\u5148\u8fdb\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u4f4e\u4e8e\u4e34\u754c\u91c7\u6837\u7387\u65f6\u5931\u6548\uff1b\u540c\u65f6SP-VLM\u80fd\u4ece\u4e25\u91cd\u964d\u7ea7\u7684\u5355\u50cf\u7d20\u89c2\u6d4b\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u884c\u4e3a\u8bed\u4e49\uff0c\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u3001\u4eba\u6570\u7edf\u8ba1\u548c\u6d3b\u52a8\u7406\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u627e\u5230\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u91c7\u6837\u7387\u533a\u95f4\uff0c\u65e2\u80fd\u63d0\u53d6\u884c\u4e3a\u667a\u80fd\uff0c\u53c8\u80fd\u5f3a\u6709\u529b\u5730\u4fdd\u62a4\u4e2a\u4eba\u8eab\u4efd\uff0c\u4e3a\u4eba\u6743\u5bf9\u9f50\u7684\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u652f\u6301\u53ca\u65f6\u5e72\u9884\u800c\u4e0d\u4fb5\u72af\u9690\u79c1\u654f\u611f\u7a7a\u95f4\u7684\u9690\u79c1\u3002"}}
{"id": "2601.17426", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.17426", "abs": "https://arxiv.org/abs/2601.17426", "authors": ["Zhengqing Zang", "Yuqi Ding", "Yanmei Gu", "Changkai Song", "Zhengkai Yang", "Guoping Du", "Junbo Zhao", "Haobo Wang"], "title": "A Syllogistic Probe: Tracing the Evolution of Logic Reasoning in Large Language Models", "comment": null, "summary": "Human logic has gradually shifted from intuition-driven inference to rigorous formal systems. Motivated by recent advances in large language models (LLMs), we explore whether LLMs exhibit a similar evolution in the underlying logical framework. Using existential import as a probe, we for evaluate syllogism under traditional and modern logic. Through extensive experiments of testing SOTA LLMs on a new syllogism dataset, we have some interesting findings: (i) Model size scaling promotes the shift toward modern logic; (ii) Thinking serves as an efficient accelerator beyond parameter scaling; (iii) the Base model plays a crucial role in determining how easily and stably this shift can emerge. Beyond these core factors, we conduct additional experiments for in-depth analysis of properties of current LLMs on syllogistic reasoning.", "AI": {"tldr": "LLMs\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u4ece\u4f20\u7edf\u903b\u8f91\u5411\u73b0\u4ee3\u903b\u8f91\u7684\u6f14\u53d8\u8d8b\u52bf\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u57fa\u7840\u6a21\u578b\u67b6\u6784\u662f\u5f71\u54cd\u8fd9\u4e00\u6f14\u53d8\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u50cf\u4eba\u7c7b\u903b\u8f91\u4e00\u6837\uff0c\u4ece\u76f4\u89c9\u9a71\u52a8\u7684\u63a8\u7406\u6f14\u53d8\u4e3a\u4e25\u683c\u7684\u903b\u8f91\u5f62\u5f0f\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b58\u5728\u5bfc\u5165\u4f5c\u4e3a\u63a2\u9488\u6765\u8bc4\u4f30\u4e09\u6bb5\u8bba\u63a8\u7406\u3002", "method": "\u4f7f\u7528\u5b58\u5728\u5bfc\u5165\u4f5c\u4e3a\u63a2\u9488\uff0c\u5728\u4f20\u7edf\u903b\u8f91\u548c\u73b0\u4ee3\u903b\u8f91\u6846\u67b6\u4e0b\u8bc4\u4f30\u4e09\u6bb5\u8bba\u63a8\u7406\uff0c\u901a\u8fc7\u5728\u65b0\u6784\u5efa\u7684\u4e09\u6bb5\u8bba\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5SOTA LLMs\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\uff1a(1) \u6a21\u578b\u89c4\u6a21\u6269\u5c55\u4fc3\u8fdb\u5411\u73b0\u4ee3\u903b\u8f91\u7684\u8f6c\u53d8\uff1b(2) \u601d\u7ef4\u94fe\u63a8\u7406\u662f\u8d85\u8d8a\u53c2\u6570\u6269\u5c55\u7684\u9ad8\u6548\u52a0\u901f\u5668\uff1b(3) \u57fa\u7840\u6a21\u578b\u67b6\u6784\u51b3\u5b9a\u8fd9\u79cd\u8f6c\u53d8\u7684\u5bb9\u6613\u7a0b\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "LLMs\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u786e\u5b9e\u8868\u73b0\u51fa\u4ece\u4f20\u7edf\u903b\u8f91\u5411\u73b0\u4ee3\u903b\u8f91\u7684\u6f14\u53d8\u8d8b\u52bf\uff0c\u6a21\u578b\u89c4\u6a21\u3001\u63a8\u7406\u65b9\u6cd5\u548c\u57fa\u7840\u67b6\u6784\u662f\u5f71\u54cd\u8fd9\u4e00\u6f14\u53d8\u7684\u6838\u5fc3\u56e0\u7d20\uff0c\u4e3a\u7406\u89e3LLMs\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.17567", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17567", "abs": "https://arxiv.org/abs/2601.17567", "authors": ["Zijing Hui", "Wenhan Lyu", "Shusen Wang", "Li Chen", "Chu Wang"], "title": "Real-Time Trend Prediction via Continually-Aligned LLM Query Generation", "comment": null, "summary": "Trending news detection in low-traffic search environments faces a fundamental cold-start problem, where a lack of query volume prevents systems from identifying emerging or long-tail trends. Existing methods relying on keyword frequency or query spikes are inherently slow and ineffective in these sparse settings, lagging behind real-world shifts in attention. We introduce RTTP, a novel Real-Time Trending Prediction framework that generates search queries directly from news content instead of waiting for users to issue them. RTTP leverages a continual learning LLM (CL-LLM) that converts posts into search-style queries and scores them using engagement strength + creator authority, enabling early trend surfacing before search volume forms. To ensure adaptation without degrading reasoning, we propose Mix-Policy DPO, a new preference-based continual learning approach that combines on-policy stability with off-policy novelty to mitigate catastrophic forgetting during model upgrades. Deployed at production scale on Facebook and Meta AI products, RTTP delivers +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy over industry baselines, while sustaining stable performance after multi-week online training. This work demonstrates that LLM-generated synthetic search signals, when aligned and continually updated, unlock timely trend understanding in low-traffic search environments.", "AI": {"tldr": "RTTP\u6846\u67b6\u901a\u8fc7\u6301\u7eed\u5b66\u4e60LLM\u76f4\u63a5\u4ece\u65b0\u95fb\u5185\u5bb9\u751f\u6210\u641c\u7d22\u67e5\u8be2\uff0c\u89e3\u51b3\u4f4e\u6d41\u91cf\u641c\u7d22\u73af\u5883\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5b9e\u73b0\u65e9\u671f\u8d8b\u52bf\u53d1\u73b0", "motivation": "\u4f4e\u6d41\u91cf\u641c\u7d22\u73af\u5883\u4e2d\u5b58\u5728\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u4f20\u7edf\u57fa\u4e8e\u67e5\u8be2\u9891\u7387\u6216\u5cf0\u503c\u7684\u65b9\u6cd5\u5728\u7a00\u758f\u73af\u5883\u4e0b\u53cd\u5e94\u8fdf\u7f13\u4e14\u65e0\u6548\uff0c\u65e0\u6cd5\u53ca\u65f6\u8bc6\u522b\u65b0\u5174\u6216\u957f\u5c3e\u8d8b\u52bf", "method": "\u63d0\u51faRTTP\u5b9e\u65f6\u8d8b\u52bf\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u6301\u7eed\u5b66\u4e60LLM\u5c06\u65b0\u95fb\u5e16\u5b50\u8f6c\u6362\u4e3a\u641c\u7d22\u98ce\u683c\u67e5\u8be2\uff0c\u901a\u8fc7\u53c2\u4e0e\u5f3a\u5ea6\u548c\u521b\u4f5c\u8005\u6743\u5a01\u8fdb\u884c\u8bc4\u5206\uff1b\u91c7\u7528Mix-Policy DPO\u65b9\u6cd5\u7ed3\u5408\u5728\u7ebf\u7b56\u7565\u7a33\u5b9a\u6027\u548c\u79bb\u7ebf\u7b56\u7565\u65b0\u9896\u6027\uff0c\u9632\u6b62\u6a21\u578b\u5347\u7ea7\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8", "result": "\u5728Facebook\u548cMeta AI\u4ea7\u54c1\u4e2d\u90e8\u7f72\uff0c\u957f\u5c3e\u8d8b\u52bf\u68c0\u6d4b\u7cbe\u5ea6@500\u63d0\u534791.4%\uff0c\u67e5\u8be2\u751f\u6210\u51c6\u786e\u7387\u6bd4\u884c\u4e1a\u57fa\u51c6\u63d0\u9ad819%\uff0c\u591a\u5468\u5728\u7ebf\u8bad\u7ec3\u540e\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd", "conclusion": "LLM\u751f\u6210\u7684\u5408\u6210\u641c\u7d22\u4fe1\u53f7\u7ecf\u8fc7\u5bf9\u9f50\u548c\u6301\u7eed\u66f4\u65b0\u540e\uff0c\u80fd\u591f\u5728\u4f4e\u6d41\u91cf\u641c\u7d22\u73af\u5883\u4e2d\u5b9e\u73b0\u53ca\u65f6\u7684\u8d8b\u52bf\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u51b7\u542f\u52a8\u95ee\u9898"}}
{"id": "2601.17053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17053", "abs": "https://arxiv.org/abs/2601.17053", "authors": ["Shuhao Que", "Dieuwke van Dartel", "Ilse Heeringa", "Han Hegeman", "Miriam Vollenbroek-Hutten", "Ying Wang"], "title": "Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults", "comment": "This paper has been submitted to Nordic Conference on Digital Health and Wireless Solutions 2026, currently under review", "summary": "Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8e\u9acb\u90e8\u9aa8\u6298\u5eb7\u590d\u7684\u8001\u5e74\u4eba\u6d3b\u52a8\u8bc6\u522b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u63d0\u9ad8\u59ff\u52bf\u8f6c\u79fb\u68c0\u6d4b\u51c6\u786e\u6027", "motivation": "\u9acb\u90e8\u9aa8\u6298\u5eb7\u590d\u671f\u95f4\u7684\u8eab\u4f53\u6d3b\u52a8\u76d1\u6d4b\u5bf9\u8001\u5e74\u60a3\u8005\u529f\u80fd\u6062\u590d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u76d1\u6d4b\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u4e2d\u9752\u5e74\u4eba\u7fa4\u5f00\u53d1\uff0c\u5bf9\u6b65\u6001\u7f13\u6162\u591a\u53d8\u7684\u8001\u5e74\u4eba\u8bc6\u522b\u4e0d\u53ef\u9760\uff0c\u7279\u522b\u662f\u4e34\u5e8a\u91cd\u8981\u7684\u59ff\u52bf\u8f6c\u79fb\u6d3b\u52a8\u5e38\u88ab\u5ffd\u89c6", "method": "\u7814\u7a76\u7eb3\u516524\u540d80\u5c81\u4ee5\u4e0a\u5065\u5eb7\u8001\u5e74\u4eba\uff0c\u5728\u6a21\u62df\u81ea\u7531\u751f\u6d3b\u6761\u4ef6\u4e0b\u6267\u884c\u65e5\u5e38\u6d3b\u52a8\uff08\u884c\u8d70\u3001\u7ad9\u7acb\u3001\u5750\u3001\u8eba\u3001\u59ff\u52bf\u8f6c\u79fb\uff0975\u5206\u949f\uff0c\u4f69\u6234\u8170\u90e8\u548c\u524d\u5927\u817f\u52a0\u901f\u5ea6\u8ba1\u3002\u91c7\u7528\u7559\u4e00\u6cd5\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5f00\u53d1\u7279\u5f81\u5e72\u9884\u6a21\u578b\uff08FIM\uff09", "result": "FIM\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u8f85\u52a9\u4e0b\u53d6\u5f97\u53ef\u9760\u6d3b\u52a8\u8bc6\u522b\u6548\u679c\uff1a\u884c\u8d70F1\u5206\u65700.896\u3001\u7ad9\u7acb0.927\u3001\u57500.997\u3001\u8eba0.937\u3001\u59ff\u52bf\u8f6c\u79fb0.816\u3002\u76f8\u6bd4\u65e0\u5408\u6210\u6570\u636e\u7684\u5bf9\u7167\u7ec4\uff0cFIM\u663e\u8457\u63d0\u9ad8\u4e86\u4e34\u5e8a\u91cd\u8981\u7684\u59ff\u52bf\u8f6c\u79fb\u68c0\u6d4b\u80fd\u529b", "conclusion": "\u521d\u6b65\u7ed3\u679c\u8bc1\u660e\u4e86\u8001\u5e74\u4eba\u6d3b\u52a8\u8bc6\u522b\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f\u5408\u6210\u6570\u636e\u589e\u5f3a\u63d0\u9ad8\u4e86\u59ff\u52bf\u8f6c\u79fb\u68c0\u6d4b\u3002\u9700\u8981\u5728\u9acb\u90e8\u9aa8\u6298\u60a3\u8005\u7fa4\u4f53\u4e2d\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4ee5\u8bc4\u4f30\u4e34\u5e8a\u5b9e\u7528\u6027"}}
{"id": "2601.17024", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17024", "abs": "https://arxiv.org/abs/2601.17024", "authors": ["Chan-Jin Chung"], "title": "Ensuring Computer Science Learning in the AI Era: Open Generative AI Policies and Assignment-Driven Written Quizzes", "comment": "6 pages", "summary": "The widespread availability of generative artificial intelligence (GenAI) has created a pressing challenge in computer science (CS) education: how to incorporate powerful AI tools into programming coursework without undermining student learning through cognitive offloading. This paper presents an assessment model that permits the use of generative AI for take-home programming assignments while enforcing individual mastery through immediate, assignment-driven written quizzes. To promote authentic learning, these in-class, closed-book assessments are weighted more heavily than the assignments themselves and are specifically designed to verify the student's comprehension of the algorithms, structure, and implementation details of their submitted code. Preliminary empirical data were collected from an upper-level computer science course to examine the relationship between self-reported GenAI usage and performance on AI-free quizzes, exams, and final course grades. Statistical analyses revealed no meaningful linear correlation between GenAI usage levels and assessment outcomes, with Pearson correlation coefficients consistently near zero. These preliminary results suggest that allowing GenAI for programming assignments does not diminish students' mastery of course concepts when learning is verified through targeted, assignment-driven quizzes. Although limited by a small sample size, this study provides preliminary evidence that the risks of cognitive offloading can be mitigated by allowing AI-assisted programming practice while verifying understanding through assignment-driven, AI-free quizzes. The findings support the responsible adoption of open GenAI policies in upper-level CS courses, when paired with rigorous, independent assessment mechanisms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6a21\u578b\uff0c\u5141\u8bb8\u5b66\u751f\u5728\u7f16\u7a0b\u4f5c\u4e1a\u4e2d\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u4f46\u901a\u8fc7\u5373\u65f6\u7684\u3001\u4f5c\u4e1a\u9a71\u52a8\u7684\u95ed\u5377\u6d4b\u9a8c\u6765\u786e\u4fdd\u4e2a\u4eba\u638c\u63e1\u7a0b\u5ea6\uff0c\u521d\u6b65\u6570\u636e\u663e\u793aAI\u4f7f\u7528\u4e0e\u8bc4\u4f30\u7ed3\u679c\u65e0\u663e\u8457\u76f8\u5173\u6027\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u666e\u53ca\u7ed9\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u5e26\u6765\u4e86\u6311\u6218\uff1a\u5982\u4f55\u5728\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u878d\u5165\u5f3a\u5927\u7684AI\u5de5\u5177\uff0c\u540c\u65f6\u907f\u514d\u56e0\u8ba4\u77e5\u5378\u8f7d\u800c\u524a\u5f31\u5b66\u751f\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51fa\u8bc4\u4f30\u6a21\u578b\uff1a\u5141\u8bb8\u5728\u7f16\u7a0b\u4f5c\u4e1a\u4e2d\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u4f46\u901a\u8fc7\u6743\u91cd\u66f4\u9ad8\u7684\u3001\u95ed\u5377\u7684\u3001\u4f5c\u4e1a\u9a71\u52a8\u7684\u5373\u65f6\u6d4b\u9a8c\u6765\u9a8c\u8bc1\u5b66\u751f\u5bf9\u63d0\u4ea4\u4ee3\u7801\u7684\u7b97\u6cd5\u3001\u7ed3\u6784\u548c\u5b9e\u73b0\u7ec6\u8282\u7684\u7406\u89e3\u3002\u4ece\u9ad8\u7ea7\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u6536\u96c6\u521d\u6b65\u5b9e\u8bc1\u6570\u636e\uff0c\u5206\u6790\u81ea\u6211\u62a5\u544a\u7684AI\u4f7f\u7528\u60c5\u51b5\u4e0e\u65e0AI\u6d4b\u9a8c\u3001\u8003\u8bd5\u548c\u6700\u7ec8\u6210\u7ee9\u7684\u5173\u7cfb\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u663e\u793a\uff0c\u751f\u6210\u5f0fAI\u4f7f\u7528\u6c34\u5e73\u4e0e\u8bc4\u4f30\u7ed3\u679c\u4e4b\u95f4\u6ca1\u6709\u6709\u610f\u4e49\u7684\u7ebf\u6027\u76f8\u5173\u6027\uff0c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u59cb\u7ec8\u63a5\u8fd1\u96f6\u3002\u8fd9\u8868\u660e\u5141\u8bb8\u5728\u7f16\u7a0b\u4f5c\u4e1a\u4e2d\u4f7f\u7528AI\u4e0d\u4f1a\u524a\u5f31\u5b66\u751f\u5bf9\u8bfe\u7a0b\u6982\u5ff5\u7684\u638c\u63e1\uff0c\u524d\u63d0\u662f\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u3001\u4f5c\u4e1a\u9a71\u52a8\u7684\u6d4b\u9a8c\u6765\u9a8c\u8bc1\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "\u867d\u7136\u53d7\u9650\u4e8e\u5c0f\u6837\u672c\u91cf\uff0c\u4f46\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5141\u8bb8AI\u8f85\u52a9\u7f16\u7a0b\u5b9e\u8df5\uff0c\u540c\u65f6\u901a\u8fc7\u4f5c\u4e1a\u9a71\u52a8\u7684\u65e0AI\u6d4b\u9a8c\u6765\u9a8c\u8bc1\u7406\u89e3\uff0c\u53ef\u4ee5\u51cf\u8f7b\u8ba4\u77e5\u5378\u8f7d\u7684\u98ce\u9669\u3002\u8fd9\u4e9b\u53d1\u73b0\u652f\u6301\u5728\u9ad8\u7ea7\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u4e2d\u8d1f\u8d23\u4efb\u5730\u91c7\u7528\u5f00\u653e\u7684\u751f\u6210\u5f0fAI\u653f\u7b56\uff0c\u524d\u63d0\u662f\u914d\u5408\u4e25\u683c\u7684\u72ec\u7acb\u8bc4\u4f30\u673a\u5236\u3002"}}
{"id": "2601.17481", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17481", "abs": "https://arxiv.org/abs/2601.17481", "authors": ["Emily Broadhurst", "Tawab Safi", "Joseph Edell", "Vashisht Ganesh", "Karime Maamari"], "title": "Lattice: Generative Guardrails for Conversational Agents", "comment": null, "summary": "Conversational AI systems require guardrails to prevent harmful outputs, yet existing approaches use static rules that cannot adapt to new threats or deployment contexts. We introduce Lattice, a framework for self-constructing and continuously improving guardrails. Lattice operates in two stages: construction builds initial guardrails from labeled examples through iterative simulation and optimization; continuous improvement autonomously adapts deployed guardrails through risk assessment, adversarial testing, and consolidation. Evaluated on the ProsocialDialog dataset, Lattice achieves 91% F1 on held-out data, outperforming keyword baselines by 43pp, LlamaGuard by 25pp, and NeMo by 4pp. The continuous improvement stage achieves 7pp F1 improvement on cross-domain data through closed-loop optimization. Our framework shows that effective guardrails can be self-constructed through iterative optimization.", "AI": {"tldr": "Lattice\u6846\u67b6\u901a\u8fc7\u81ea\u6784\u5efa\u548c\u6301\u7eed\u6539\u8fdb\u673a\u5236\u4e3a\u5bf9\u8bddAI\u7cfb\u7edf\u521b\u5efa\u81ea\u9002\u5e94\u9632\u62a4\u680f\uff0c\u76f8\u6bd4\u9759\u6001\u89c4\u5219\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u5bf9\u8bddAI\u7cfb\u7edf\u7684\u9632\u62a4\u680f\u4f7f\u7528\u9759\u6001\u89c4\u5219\uff0c\u65e0\u6cd5\u9002\u5e94\u65b0\u5a01\u80c1\u548c\u90e8\u7f72\u73af\u5883\u53d8\u5316\uff0c\u9700\u8981\u80fd\u591f\u81ea\u6211\u6784\u5efa\u548c\u6301\u7eed\u6539\u8fdb\u7684\u9632\u62a4\u6846\u67b6", "method": "Lattice\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u6784\u5efa\u9636\u6bb5\u901a\u8fc7\u8fed\u4ee3\u6a21\u62df\u548c\u4f18\u5316\u4ece\u6807\u6ce8\u793a\u4f8b\u521b\u5efa\u521d\u59cb\u9632\u62a4\u680f\uff1b\u6301\u7eed\u6539\u8fdb\u9636\u6bb5\u901a\u8fc7\u98ce\u9669\u8bc4\u4f30\u3001\u5bf9\u6297\u6d4b\u8bd5\u548c\u6574\u5408\u81ea\u4e3b\u8c03\u6574\u5df2\u90e8\u7f72\u7684\u9632\u62a4\u680f", "result": "\u5728ProsocialDialog\u6570\u636e\u96c6\u4e0a\uff0cLattice\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8fbe\u523091% F1\u5206\u6570\uff0c\u6bd4\u5173\u952e\u8bcd\u57fa\u7ebf\u9ad843\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4LlamaGuard\u9ad825\u4e2a\u767e\u5206\u70b9\uff0c\u6bd4NeMo\u9ad84\u4e2a\u767e\u5206\u70b9\uff1b\u6301\u7eed\u6539\u8fdb\u9636\u6bb5\u901a\u8fc7\u95ed\u73af\u4f18\u5316\u5728\u8de8\u57df\u6570\u636e\u4e0a\u5b9e\u73b07\u4e2a\u767e\u5206\u70b9\u7684F1\u63d0\u5347", "conclusion": "Lattice\u6846\u67b6\u8bc1\u660e\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u53ef\u4ee5\u81ea\u6784\u5efa\u6709\u6548\u7684\u9632\u62a4\u680f\uff0c\u4e3a\u5bf9\u8bddAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u81ea\u9002\u5e94\u9632\u62a4\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17601", "categories": ["cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.17601", "abs": "https://arxiv.org/abs/2601.17601", "authors": ["Fangping Lan", "Abdullah Aljebreen", "Eduard C. Dragut"], "title": "Why They Link: An Intent Taxonomy for Including Hyperlinks in Social Posts", "comment": "10 pages including references, 5 figures,", "summary": "URLs serve as bridges between social media platforms and the broader web, linking user-generated content to external information resources. On Twitter (X), approximately one in five tweets contains at least one URL, underscoring their central role in information dissemination. While prior studies have examined the motivations of authors who share URLs, such author-centered intentions are difficult to observe in practice. To enable broader downstream use, this work investigates reader-centered interpretations, i.e., how users perceive the intentions behind hyperlinks included in posts. We develop an intent taxonomy for including hyperlinks in social posts through a hybrid approach that begins with a bottom-up, data-driven process using large-scale crowdsourced annotations, and is then refined using large language model assistance to generate descriptive category names and precise definitions. The final taxonomy comprises 6 top-level categories and 26 fine-grained intention classes, capturing diverse communicative purposes. Applying this taxonomy, we annotate and analyze 1000 user posts, revealing that advertising, arguing, and sharing are the most prevalent intentions. This resulting taxonomy provides a foundation for intent-aware information retrieval and NLP applications, enabling more accurate retrieval, recommendation, and understanding of social media content.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u793e\u4ea4\u5a92\u4f53\u4e2dURL\u610f\u56fe\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f17\u5305\u6807\u6ce8\u548cLLM\u8f85\u52a9\u6784\u5efa\u4e86\u5305\u542b6\u4e2a\u9876\u5c42\u7c7b\u522b\u548c26\u4e2a\u7ec6\u7c92\u5ea6\u610f\u56fe\u7684\u5206\u7c7b\u6cd5\uff0c\u53d1\u73b0\u5e7f\u544a\u3001\u4e89\u8bba\u548c\u5206\u4eab\u662f\u6700\u5e38\u89c1\u7684URL\u610f\u56fe\u3002", "motivation": "\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u5173\u6ce8URL\u5206\u4eab\u8005\u7684\u52a8\u673a\uff0c\u4f46\u8fd9\u4e9b\u4f5c\u8005\u4e2d\u5fc3\u7684\u610f\u56fe\u5728\u5b9e\u8df5\u4e2d\u96be\u4ee5\u89c2\u5bdf\u3002\u4e3a\u4e86\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u4e0b\u6e38\u5e94\u7528\uff0c\u672c\u7814\u7a76\u8f6c\u5411\u8bfb\u8005\u4e2d\u5fc3\u7684\u89e3\u91ca\uff0c\u5373\u7528\u6237\u5982\u4f55\u7406\u89e3\u5e16\u5b50\u4e2d\u5305\u542b\u7684\u8d85\u94fe\u63a5\u610f\u56fe\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u5927\u89c4\u6a21\u4f17\u5305\u6807\u6ce8\u8fdb\u884c\u81ea\u4e0b\u800c\u4e0a\u7684\u6570\u636e\u9a71\u52a8\u8fc7\u7a0b\uff0c\u7136\u540e\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u751f\u6210\u63cf\u8ff0\u6027\u7c7b\u522b\u540d\u79f0\u548c\u7cbe\u786e\u5b9a\u4e49\uff0c\u6700\u7ec8\u6784\u5efa\u5305\u542b6\u4e2a\u9876\u5c42\u7c7b\u522b\u548c26\u4e2a\u7ec6\u7c92\u5ea6\u610f\u56fe\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u5e94\u7528\u8be5\u5206\u7c7b\u6cd5\u6807\u6ce8\u5206\u6790\u4e861000\u4e2a\u7528\u6237\u5e16\u5b50\uff0c\u53d1\u73b0\u5e7f\u544a\u3001\u4e89\u8bba\u548c\u5206\u4eab\u662f\u6700\u666e\u904d\u7684\u610f\u56fe\u3002\u8be5\u5206\u7c7b\u6cd5\u4e3a\u610f\u56fe\u611f\u77e5\u7684\u4fe1\u606f\u68c0\u7d22\u548cNLP\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u5f00\u53d1\u7684URL\u610f\u56fe\u5206\u7c7b\u7cfb\u7edf\u80fd\u591f\u66f4\u51c6\u786e\u5730\u7406\u89e3\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\uff0c\u652f\u6301\u66f4\u7cbe\u786e\u7684\u4fe1\u606f\u68c0\u7d22\u3001\u63a8\u8350\u548c\u5185\u5bb9\u7406\u89e3\u5e94\u7528\u3002"}}
{"id": "2601.17542", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.17542", "abs": "https://arxiv.org/abs/2601.17542", "authors": ["Vinoth Punniyamoorthy", "Nitin Saksena", "Srivenkateswara Reddy Sankiti", "Nachiappan Chockalingam", "Aswathnarayan Muthukrishnan Kirubakaran", "Shiva Kumar Reddy Carimireddy", "Durgaraman Maruthavanan"], "title": "Cognitive Platform Engineering for Autonomous Cloud Operations", "comment": null, "summary": "Modern DevOps practices have accelerated software delivery through automation, CI/CD pipelines, and observability tooling,but these approaches struggle to keep pace with the scale and dynamism of cloud-native systems. As telemetry volume grows and configuration drift increases, traditional, rule-driven automation often results in reactive operations, delayed remediation, and dependency on manual expertise. This paper introduces Cognitive Platform Engineering, a next-generation paradigm that integrates sensing, reasoning, and autonomous action directly into the platform lifecycle. This paper propose a four-plane reference architecture that unifies data collection, intelligent inference, policy-driven orchestration, and human experience layers within a continuous feedback loop. A prototype implementation built with Kubernetes, Terraform, Open Policy Agent, and ML-based anomaly detection demonstrates improvements in mean time to resolution, resource efficiency, and compliance. The results show that embedding intelligence into platform operations enables resilient, self-adjusting, and intent-aligned cloud environments. The paper concludes with research opportunities in reinforcement learning, explainable governance, and sustainable self-managing cloud ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8ba4\u77e5\u5e73\u53f0\u5de5\u7a0b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u96c6\u6210\u611f\u77e5\u3001\u63a8\u7406\u548c\u81ea\u4e3b\u884c\u52a8\u5230\u5e73\u53f0\u751f\u547d\u5468\u671f\uff0c\u89e3\u51b3\u4f20\u7edfDevOps\u5728\u4e91\u539f\u751f\u7cfb\u7edf\u89c4\u6a21\u5316\u548c\u52a8\u6001\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfDevOps\u5b9e\u8df5\u867d\u7136\u901a\u8fc7\u81ea\u52a8\u5316\u3001CI/CD\u6d41\u6c34\u7ebf\u548c\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\u52a0\u901f\u4e86\u8f6f\u4ef6\u4ea4\u4ed8\uff0c\u4f46\u5728\u9762\u5bf9\u4e91\u539f\u751f\u7cfb\u7edf\u7684\u89c4\u6a21\u548c\u52a8\u6001\u6027\u65f6\u5b58\u5728\u5c40\u9650\u3002\u968f\u7740\u9065\u6d4b\u6570\u636e\u91cf\u589e\u957f\u548c\u914d\u7f6e\u6f02\u79fb\u589e\u52a0\uff0c\u4f20\u7edf\u7684\u89c4\u5219\u9a71\u52a8\u81ea\u52a8\u5316\u5f80\u5f80\u5bfc\u81f4\u88ab\u52a8\u8fd0\u7ef4\u3001\u4fee\u590d\u5ef6\u8fdf\u548c\u5bf9\u4eba\u5de5\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u8ba4\u77e5\u5e73\u53f0\u5de5\u7a0b\u8303\u5f0f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56db\u5c42\u53c2\u8003\u67b6\u6784\uff0c\u7edf\u4e00\u6570\u636e\u6536\u96c6\u3001\u667a\u80fd\u63a8\u7406\u3001\u7b56\u7565\u9a71\u52a8\u7f16\u6392\u548c\u4eba\u7c7b\u4f53\u9a8c\u5c42\uff0c\u5f62\u6210\u6301\u7eed\u53cd\u9988\u5faa\u73af\u3002\u539f\u578b\u5b9e\u73b0\u57fa\u4e8eKubernetes\u3001Terraform\u3001Open Policy Agent\u548c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u539f\u578b\u5b9e\u73b0\u5c55\u793a\u4e86\u5728\u5e73\u5747\u89e3\u51b3\u65f6\u95f4\u3001\u8d44\u6e90\u6548\u7387\u548c\u5408\u89c4\u6027\u65b9\u9762\u7684\u6539\u8fdb\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u667a\u80fd\u5d4c\u5165\u5e73\u53f0\u64cd\u4f5c\u80fd\u591f\u5b9e\u73b0\u5f39\u6027\u3001\u81ea\u6211\u8c03\u6574\u548c\u610f\u56fe\u5bf9\u9f50\u7684\u4e91\u73af\u5883\u3002", "conclusion": "\u8ba4\u77e5\u5e73\u53f0\u5de5\u7a0b\u901a\u8fc7\u5d4c\u5165\u667a\u80fd\u5230\u5e73\u53f0\u64cd\u4f5c\uff0c\u80fd\u591f\u5b9e\u73b0\u5f39\u6027\u3001\u81ea\u6211\u8c03\u6574\u548c\u610f\u56fe\u5bf9\u9f50\u7684\u4e91\u73af\u5883\u3002\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u3001\u53ef\u89e3\u91ca\u6cbb\u7406\u548c\u53ef\u6301\u7eed\u81ea\u7ba1\u7406\u4e91\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2601.17062", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17062", "abs": "https://arxiv.org/abs/2601.17062", "authors": ["Robert M. Belcher", "Brendan C. Degryse", "Leonard R. Kosta", "Christopher J. Lowrance"], "title": "A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing", "comment": "Presented at the 2025 MIT Undergraduate Research Technology Conference (URTC)", "summary": "Adjusting rifle sights, a process commonly called \"zeroing,\" requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.", "AI": {"tldr": "\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u81ea\u52a8\u5f39\u5b54\u68c0\u6d4b\u4e0e\u8fed\u4ee3\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u7528\u4e8e\u6b65\u67aa\u7784\u51c6\u955c\u6821\u51c6\uff0c\u7ed3\u5408YOLOv8\u68c0\u6d4b\u548cIoU\u5206\u6790\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6570\u636e\u589e\u5f3a\u548c\u9884\u5904\u7406\u6280\u672f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6b65\u67aa\u7784\u51c6\u955c\u6821\u51c6\u9700\u8981\u4eba\u5de5\u68c0\u67e5\u5f39\u5b54\uff0c\u5b58\u5728\u5b89\u5168\u534f\u8bae\u5ef6\u8fdf\u548c\u4eba\u4e3a\u9519\u8bef\u98ce\u9669\u3002\u9700\u8981\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u5b9e\u65f6\u68c0\u6d4b\u548c\u8ffd\u8e2a\u4e0d\u540c\u5c04\u51fb\u8fed\u4ee3\u7684\u5f39\u5b54\u3002", "method": "\u4f7f\u7528YOLOv8\u8fdb\u884c\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u7ed3\u5408IoU\u5206\u6790\u533a\u5206\u5e8f\u5217\u56fe\u50cf\u4e2d\u7684\u5f39\u5b54\u3002\u63d0\u51fa\u65b0\u9896\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u79fb\u9664\u800c\u975e\u6dfb\u52a0\u5bf9\u8c61\u6a21\u62df\u5c04\u51fb\u5e8f\u5217\uff09\u548c\u57fa\u4e8eORB\u7684\u900f\u89c6\u6821\u6b63\u9884\u5904\u7406\u7ba1\u9053\u3002", "result": "\u7cfb\u7edf\u5728\u5f39\u5b54\u68c0\u6d4b\u4e0a\u8fbe\u523097.0%\u7684\u5e73\u5747\u7cbe\u5ea6\uff0c\u5728\u5c06\u5f39\u5b54\u5206\u914d\u5230\u6b63\u786e\u5c04\u51fb\u8fed\u4ee3\u4e0a\u8fbe\u523088.8%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7aef\u5230\u7aef\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u52a8\u5f39\u5b54\u68c0\u6d4b\u548c\u8fed\u4ee3\u8ffd\u8e2a\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u6b65\u67aa\u6821\u51c6\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u9700\u8981\u65f6\u95f4\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u5bf9\u8c61\u7684\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2601.17564", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17564", "abs": "https://arxiv.org/abs/2601.17564", "authors": ["Aadam", "Monu Verma", "Mohamed Abdel-Mottaleb"], "title": "JaxARC: A High-Performance JAX-based Environment for Abstraction and Reasoning Research", "comment": null, "summary": "The Abstraction and Reasoning Corpus (ARC) tests AI systems' ability to perform human-like inductive reasoning from a few demonstration pairs. Existing Gymnasium-based RL environments severely limit experimental scale due to computational bottlenecks. We present JaxARC, an open-source, high-performance RL environment for ARC implemented in JAX. Its functional, stateless architecture enables massive parallelism, achieving 38-5,439x speedup over Gymnasium at matched batch sizes, with peak throughput of 790M steps/second. JaxARC supports multiple ARC datasets, flexible action spaces, composable wrappers, and configuration-driven reproducibility, enabling large-scale RL research previously computationally infeasible. JaxARC is available at https://github.com/aadimator/JaxARC.", "AI": {"tldr": "JaxARC\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u7684\u9ad8\u6027\u80fd\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u7528\u4e8eAbstraction and Reasoning Corpus\uff08ARC\uff09\u4efb\u52a1\uff0c\u76f8\u6bd4Gymnasium\u5b9e\u73b0\u4e8638-5,439\u500d\u7684\u52a0\u901f\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eGymnasium\u7684RL\u73af\u5883\u5728\u5904\u7406ARC\u4efb\u52a1\u65f6\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5b9e\u9a8c\u89c4\u6a21\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u73af\u5883\u6765\u652f\u6301\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u3002", "method": "\u4f7f\u7528JAX\u5b9e\u73b0\u529f\u80fd\u5316\u3001\u65e0\u72b6\u6001\u7684\u67b6\u6784\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\uff0c\u63d0\u4f9b\u591a\u79cdARC\u6570\u636e\u96c6\u652f\u6301\u3001\u7075\u6d3b\u7684\u52a8\u4f5c\u7a7a\u95f4\u3001\u53ef\u7ec4\u5408\u7684\u5305\u88c5\u5668\u548c\u914d\u7f6e\u9a71\u52a8\u7684\u53ef\u91cd\u590d\u6027\u3002", "result": "\u5728\u76f8\u540c\u6279\u6b21\u5927\u5c0f\u4e0b\u5b9e\u73b038-5,439\u500d\u7684\u52a0\u901f\uff0c\u5cf0\u503c\u541e\u5410\u91cf\u8fbe\u52307.9\u4ebf\u6b65/\u79d2\uff0c\u4f7f\u4e4b\u524d\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u7684\u5927\u89c4\u6a21RL\u7814\u7a76\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "JaxARC\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u9ad8\u6027\u80fdRL\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86ARC\u4efb\u52a1\u7684\u5b9e\u9a8c\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8ba1\u7b97\u5e73\u53f0\u3002"}}
{"id": "2601.17090", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17090", "abs": "https://arxiv.org/abs/2601.17090", "authors": ["Noam Koren", "Rafael Moschopoulos", "Kira Radinsky", "Elad Hazan"], "title": "SFO: Learning PDE Operators via Spectral Filtering", "comment": null, "summary": "Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.", "AI": {"tldr": "SFO\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b97\u5b50\uff0c\u5229\u7528\u901a\u7528\u8c31\u57fa\u53c2\u6570\u5316\u79ef\u5206\u6838\uff0c\u901a\u8fc7\u5feb\u901f\u8870\u51cf\u7279\u5f81\u503c\u7684\u8c31\u7cfb\u6570\u5b66\u4e60\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u8bef\u5dee\u964d\u4f4e\u8fbe40%\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u5728\u6355\u6349PDE\u89e3\u6620\u5c04\u4e2d\u7684\u957f\u7a0b\u975e\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\u65b9\u9762\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u79bb\u6563\u683c\u6797\u51fd\u6570\u5177\u6709\u7a7a\u95f4\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7ed3\u6784\uff0c\u53ef\u5728\u901a\u7528\u8c31\u57fa\u4e2d\u83b7\u5f97\u7d27\u51d1\u8fd1\u4f3c\u3002", "method": "\u63d0\u51fa\u8c31\u6ee4\u6ce2\u7b97\u5b50(SFO)\uff0c\u4f7f\u7528\u4ece\u5e0c\u5c14\u4f2f\u7279\u77e9\u9635\u7279\u5f81\u6a21\u6001\u5bfc\u51fa\u7684\u56fa\u5b9a\u5168\u5c40\u6b63\u4ea4\u57fa\uff08\u901a\u7528\u8c31\u57fa\uff09\u53c2\u6570\u5316\u79ef\u5206\u6838\uff0c\u4ec5\u5b66\u4e60\u5feb\u901f\u8870\u51cf\u7279\u5f81\u503c\u7684\u8c31\u7cfb\u6570\uff0c\u5b9e\u73b0\u9ad8\u6548\u8868\u793a\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u53cd\u5e94-\u6269\u6563\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u548c3D\u7535\u78c1\u5b66\uff09\u4e2d\uff0cSFO\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u76f8\u5bf9\u4e8e\u5f3a\u57fa\u7ebf\u8bef\u5dee\u964d\u4f4e\u8fbe40%\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u53c2\u6570\u3002", "conclusion": "SFO\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u8c31\u57fa\u8868\u793a\uff0c\u6709\u6548\u6355\u6349PDE\u89e3\u6620\u5c04\u4e2d\u7684\u957f\u7a0b\u975e\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u795e\u7ecf\u7b97\u5b50\u8bbe\u8ba1\u3002"}}
{"id": "2601.17692", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17692", "abs": "https://arxiv.org/abs/2601.17692", "authors": ["Yunhan Li", "Mingjie Xie", "Gaoli Kang", "Zihan Gong", "Gengshen Wu", "Min Yang"], "title": "LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval", "comment": "31pages, 4 figures", "summary": "Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.", "AI": {"tldr": "LegalMALR\u662f\u4e00\u4e2a\u6cd5\u5f8b\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67e5\u8be2\u7406\u89e3\u7cfb\u7edf\u548c\u96f6\u6837\u672cLLM\u91cd\u6392\u6a21\u5757\uff0c\u89e3\u51b3\u9690\u5f0f\u3001\u591a\u8bae\u9898\u3001\u53e3\u8bed\u5316\u6cd5\u5f8b\u67e5\u8be2\u7684\u6cd5\u89c4\u68c0\u7d22\u96be\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u6cd5\u5f8b\u67e5\u8be2\u5f80\u5f80\u662f\u9690\u5f0f\u7684\u3001\u591a\u8bae\u9898\u7684\uff0c\u4e14\u4ee5\u53e3\u8bed\u5316\u6216\u672a\u5145\u5206\u8bf4\u660e\u7684\u5f62\u5f0f\u8868\u8fbe\uff0c\u8fd9\u4f7f\u5f97\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7ba1\u9053\u96be\u4ee5\u51c6\u786e\u6062\u590d\u6cd5\u89c4\u8981\u7d20\u3002\u5bc6\u96c6\u68c0\u7d22\u5668\u4e3b\u8981\u5173\u6ce8\u67e5\u8be2\u7684\u5b57\u9762\u5f62\u5f0f\uff0c\u800c\u8f7b\u91cf\u7ea7\u91cd\u6392\u5668\u7f3a\u4e4f\u8bc4\u4f30\u6cd5\u89c4\u9002\u7528\u6027\u6240\u9700\u7684\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faLegalMALR\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u591a\u667a\u80fd\u4f53\u67e5\u8be2\u7406\u89e3\u7cfb\u7edf\uff08MAS\uff09\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u6cd5\u5f8b\u57fa\u7840\u91cd\u8ff0\u5e76\u8fdb\u884c\u8fed\u4ee3\u5bc6\u96c6\u68c0\u7d22\u4ee5\u6269\u5927\u5019\u9009\u8986\u76d6\uff1b2\uff09\u4f7f\u7528\u5e7f\u4e49\u5f3a\u5316\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7edf\u4e00MAS\u7b56\u7565\u4ee5\u7a33\u5b9aLLM\u751f\u6210\u91cd\u5199\u7684\u968f\u673a\u884c\u4e3a\uff1b3\uff09\u96f6\u6837\u672c\u5927\u8bed\u8a00\u6a21\u578b\u91cd\u6392\u6a21\u5757\uff08LLM Reranker\uff09\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6cd5\u5f8b\u63a8\u7406\u751f\u6210\u6700\u7ec8\u6392\u540d\u3002\u8fd8\u6784\u5efa\u4e86CSAID\u6570\u636e\u96c6\uff08118\u4e2a\u56f0\u96be\u4e2d\u6587\u6cd5\u5f8b\u67e5\u8be2\uff0c\u5e26\u6709\u591a\u4e2a\u6cd5\u89c4\u6807\u7b7e\uff09\u3002", "result": "\u5728CSAID\u6570\u636e\u96c6\u548c\u516c\u5f00STARD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLegalMALR\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u5f3a\u5927\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u591a\u89c6\u89d2\u67e5\u8be2\u89e3\u91ca\u3001\u57fa\u4e8e\u5f3a\u5316\u7684\u7b56\u7565\u4f18\u5316\u548c\u5927\u6a21\u578b\u91cd\u6392\u7684\u65b9\u6cd5\u5bf9\u6cd5\u89c4\u68c0\u7d22\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u89e3\u51b3\u73b0\u5b9e\u6cd5\u5f8b\u67e5\u8be2\u7684\u590d\u6742\u6027\u6311\u6218\u3002"}}
{"id": "2601.17067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17067", "abs": "https://arxiv.org/abs/2601.17067", "authors": ["Luozhou Wang", "Zhifei Chen", "Yihua Du", "Dongyu Yan", "Wenhang Ge", "Guibao Shen", "Xinli Xu", "Leyi Wu", "Man Chen", "Tianshuo Xu", "Peiran Ren", "Xin Tao", "Pengfei Wan", "Ying-Cong Chen"], "title": "A Mechanistic View on Video Generation as World Models: State and Dynamics", "comment": null, "summary": "Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary \"stateless\" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u7814\u7a76\u5206\u4e3a\u72b6\u6001\u6784\u5efa\u548c\u52a8\u6001\u5efa\u6a21\u4e24\u5927\u652f\u67f1\uff0c\u5e76\u5021\u5bfc\u4ece\u89c6\u89c9\u4fdd\u771f\u5ea6\u8bc4\u4f30\u8f6c\u5411\u529f\u80fd\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u5c55\u73b0\u51fa\u7269\u7406\u8fde\u8d2f\u6027\uff0c\u53ef\u4f5c\u4e3a\u6f5c\u5728\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4f46\u73b0\u4ee3\"\u65e0\u72b6\u6001\"\u89c6\u9891\u67b6\u6784\u4e0e\u7ecf\u5178\u72b6\u6001\u4e2d\u5fc3\u5316\u7684\u4e16\u754c\u6a21\u578b\u7406\u8bba\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63a8\u52a8\u89c6\u9891\u751f\u6210\u6a21\u578b\u4ece\u751f\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u89c6\u9891\u53d1\u5c55\u4e3a\u6784\u5efa\u7a33\u5065\u3001\u901a\u7528\u7684\u4e16\u754c\u6a21\u62df\u5668\u3002", "method": "\u63d0\u51fa\u4ee5\u72b6\u6001\u6784\u5efa\u548c\u52a8\u6001\u5efa\u6a21\u4e3a\u6838\u5fc3\u652f\u67f1\u7684\u65b0\u5206\u7c7b\u6cd5\uff1a\u72b6\u6001\u6784\u5efa\u5206\u4e3a\u9690\u5f0f\u8303\u5f0f\uff08\u4e0a\u4e0b\u6587\u7ba1\u7406\uff09\u548c\u663e\u5f0f\u8303\u5f0f\uff08\u6f5c\u5728\u538b\u7f29\uff09\uff1b\u52a8\u6001\u5efa\u6a21\u901a\u8fc7\u77e5\u8bc6\u6574\u5408\u548c\u67b6\u6784\u91cd\u6784\u8fdb\u884c\u5206\u6790\u3002\u540c\u65f6\u5021\u5bfc\u4ece\u89c6\u89c9\u4fdd\u771f\u5ea6\u8bc4\u4f30\u8f6c\u5411\u529f\u80fd\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6d4b\u8bd5\u7269\u7406\u6301\u4e45\u6027\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5efa\u7acb\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u8bc6\u522b\u51fa\u4e24\u4e2a\u5173\u952e\u524d\u6cbf\uff1a\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u8bb0\u5fc6\u548c\u538b\u7f29\u4fdd\u771f\u5ea6\u589e\u5f3a\u6301\u4e45\u6027\uff0c\u4ee5\u53ca\u901a\u8fc7\u6f5c\u5728\u56e0\u5b50\u89e3\u8026\u548c\u63a8\u7406\u5148\u9a8c\u6574\u5408\u63a8\u8fdb\u56e0\u679c\u6027\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u8be5\u9886\u57df\u53ef\u4ee5\u4ece\u751f\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u7684\u89c6\u9891\u6f14\u53d8\u4e3a\u6784\u5efa\u7a33\u5065\u3001\u901a\u7528\u7684\u4e16\u754c\u6a21\u62df\u5668\u3002\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u548c\u8bc4\u4f30\u6846\u67b6\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6307\u5bfc\u3002"}}
{"id": "2601.17055", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.17055", "abs": "https://arxiv.org/abs/2601.17055", "authors": ["Matthias Huemmer", "Franziska Durner", "Theophile Shyiramunda", "Michelle J. Cummings-Koether"], "title": "AI, Metacognition, and the Verification Bottleneck: A Three-Wave Longitudinal Study of Human Problem-Solving", "comment": "62 pages, 2 figures, 23 tables", "summary": "This longitudinal pilot study tracked how generative AI reshapes problem-solving over six months across three waves in an academic setting. AI integration reached saturation by Wave 3, with daily use rising from 52.4% to 95.7% and ChatGPT adoption from 85.7% to 100%. A dominant hybrid workflow increased 2.7-fold, adopted by 39.1% of participants. The verification paradox emerged: participants relied most heavily on AI for difficult tasks (73.9%) yet showed declining verification confidence (68.1%) where performance was worst (47.8% accuracy on complex tasks). Objective performance declined systematically: 95.2% to 81.0% to 66.7% to 47.8% across problem difficulty, with belief-performance gaps widening to 34.6 percentage points. This indicates a fundamental shift where verification, not solution generation, became the bottleneck in human-AI problem-solving. The ACTIVE Framework synthesizes findings grounded in cognitive load theory: Awareness and task-AI alignment, Critical verification protocols, Transparent human-in-the-loop integration, Iterative skill development countering cognitive offloading, Verification confidence calibration, and Ethical evaluation. The authors provide implementation pathways for institutions and practitioners. Key limitations include sample homogeneity (academic cohort only, convenience sampling) limiting generalizability to corporate, clinical, or regulated professional contexts; self-report bias in confidence measures (32.2 percentage point divergence from objective performance); lack of control conditions; restriction to mathematical/analytical problems; and insufficient timeframe to assess long-term skill trajectories. Results generalize primarily to early-adopter, academically affiliated populations. Causal validation requires randomized controlled trials.", "AI": {"tldr": "\u8fd9\u9879\u4e3a\u671f6\u4e2a\u6708\u7684\u7eb5\u5411\u7814\u7a76\u8ffd\u8e2a\u4e86\u751f\u6210\u5f0fAI\u5982\u4f55\u6539\u53d8\u5b66\u672f\u73af\u5883\u4e2d\u7684\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\uff0c\u53d1\u73b0AI\u4f7f\u7528\u8fbe\u5230\u9971\u548c\uff0c\u4f46\u9a8c\u8bc1\u6210\u4e3a\u74f6\u9888\uff0c\u5ba2\u89c2\u8868\u73b0\u7cfb\u7edf\u6027\u4e0b\u964d\uff0c\u4fe1\u5ff5-\u8868\u73b0\u5dee\u8ddd\u6269\u5927\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u5728\u5b66\u672f\u73af\u5883\u4e2d\u5982\u4f55\u968f\u65f6\u95f4\u6539\u53d8\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\uff0c\u7279\u522b\u5173\u6ce8AI\u96c6\u6210\u7a0b\u5ea6\u3001\u5de5\u4f5c\u6d41\u7a0b\u6f14\u53d8\u3001\u9a8c\u8bc1\u884c\u4e3a\u4e0e\u5ba2\u89c2\u8868\u73b0\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u7eb5\u5411\u8bd5\u70b9\u7814\u7a76\uff0c\u57286\u4e2a\u6708\u5185\u5206\u4e09\u4e2a\u6ce2\u6b21\u8ffd\u8e2a\u5b66\u672f\u73af\u5883\u4e2d\u7684AI\u4f7f\u7528\u60c5\u51b5\uff0c\u6d4b\u91cfAI\u91c7\u7528\u7387\u3001\u5de5\u4f5c\u6d41\u7a0b\u3001\u9a8c\u8bc1\u4fe1\u5fc3\u548c\u5ba2\u89c2\u8868\u73b0\uff0c\u4f7f\u7528\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u4f5c\u4e3a\u7406\u8bba\u57fa\u7840\u3002", "result": "AI\u96c6\u6210\u8fbe\u5230\u9971\u548c\uff08\u65e5\u5e38\u4f7f\u7528\u738795.7%\uff0cChatGPT\u91c7\u7528\u7387100%\uff09\uff0c\u6df7\u5408\u5de5\u4f5c\u6d41\u7a0b\u589e\u52a02.7\u500d\uff0c\u4f46\u9a8c\u8bc1\u4fe1\u5fc3\u4e0b\u964d\uff0868.1%\uff09\uff0c\u5ba2\u89c2\u8868\u73b0\u7cfb\u7edf\u6027\u4e0b\u964d\uff08\u4ece95.2%\u523047.8%\uff09\uff0c\u4fe1\u5ff5-\u8868\u73b0\u5dee\u8ddd\u6269\u5927\u81f334.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u9a8c\u8bc1\u800c\u975e\u89e3\u51b3\u65b9\u6848\u751f\u6210\u6210\u4e3a\u4eba-AI\u95ee\u9898\u89e3\u51b3\u7684\u74f6\u9888\uff0c\u63d0\u51fa\u4e86ACTIVE\u6846\u67b6\u6307\u5bfc\u5b9e\u8df5\uff0c\u4f46\u7814\u7a76\u5b58\u5728\u6837\u672c\u540c\u8d28\u6027\u3001\u81ea\u6211\u62a5\u544a\u504f\u5dee\u7b49\u5c40\u9650\u6027\uff0c\u7ed3\u679c\u4e3b\u8981\u9002\u7528\u4e8e\u65e9\u671f\u91c7\u7528\u8005\u5b66\u672f\u7fa4\u4f53\u3002"}}
{"id": "2601.17091", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17091", "abs": "https://arxiv.org/abs/2601.17091", "authors": ["Ole St\u00fcven", "Keno Moenck", "Thorsten Sch\u00fcppstuhl"], "title": "CUROCKET: Optimizing ROCKET for GPU", "comment": null, "summary": "ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.", "AI": {"tldr": "CUROCKET\u662f\u4e00\u79cd\u5728GPU\u4e0a\u9ad8\u6548\u5b9e\u73b0ROCKET\u7279\u5f81\u63d0\u53d6\u7b97\u6cd5\u7684\u6539\u8fdb\u7248\u672c\uff0c\u901a\u8fc7\u89e3\u51b3\u975e\u5747\u5300\u5377\u79ef\u6838\u5728GPU\u4e0a\u7684\u5e76\u884c\u5316\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u6bd4CPU\u7248\u672c\u9ad8\u8fbe11\u500d\u7684\u6bcf\u74e6\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u3002", "motivation": "ROCKET\u7b97\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u4e3b\u8981\u5c40\u9650\u4e8eCPU\u6267\u884c\u3002\u5377\u79ef\u64cd\u4f5c\u5929\u7136\u9002\u5408GPU\u5e76\u884c\u5316\uff0c\u4f46ROCKET\u4f7f\u7528\u7684\u975e\u5747\u5300\u5377\u79ef\u6838\u4f7f\u5f97\u6807\u51c6GPU\u5377\u79ef\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1GPU\u4f18\u5316\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5728GPU\u4e0a\u9ad8\u6548\u6267\u884cROCKET\u7279\u5f81\u63d0\u53d6\u7684\u7b97\u6cd5\uff0c\u4e13\u95e8\u89e3\u51b3\u4e86\u975e\u5747\u5300\u5377\u79ef\u6838\u5728GPU\u5e76\u884c\u5316\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86CUROCKET\u5b9e\u73b0\u3002", "result": "CUROCKET\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u8fbe\u5230\u6bd4CPU\u7248\u672c\u9ad8\u8fbe11\u500d\u7684\u6bcf\u74e6\u8ba1\u7b97\u6548\u7387\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684GPU\u4f18\u5316\u7b97\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86ROCKET\u975e\u5747\u5300\u5377\u79ef\u6838\u5728GPU\u4e0a\u7684\u5e76\u884c\u5316\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17787", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17787", "abs": "https://arxiv.org/abs/2601.17787", "authors": ["Wei-Ning Chiu", "Chuan-Ju Wang", "Pu-Jen Cheng"], "title": "Token-Weighted Multi-Target Learning for Generative Recommenders with Curriculum Learning", "comment": "11 pages, 5 figures", "summary": "Generative recommender systems have recently attracted attention by formulating next-item prediction as an autoregressive sequence generation task. However, most existing methods optimize standard next-token likelihood and implicitly treat all tokens as equally informative, which is misaligned with semantic-ID-based generation. Accordingly, we propose two complementary information-gain-based token-weighting strategies tailored to generative recommendation with semantic IDs. Front-Greater Weighting captures conditional semantic information gain by prioritizing early tokens that most effectively reduce candidate-item uncertainty given their prefixes and encode coarse semantics. Frequency Weighting models marginal information gain under long-tailed item and token distributions, upweighting rare tokens to counteract popularity bias. Beyond individual strategies, we introduce a multi-target learning framework with curriculum learning that jointly optimizes the two token-weighted objectives alongside standard likelihood, enabling stable optimization and adaptive emphasis across training stages. Extensive experiments on benchmark datasets show that our method consistently outperforms strong baselines and existing token-weighting approaches, with improved robustness, strong generalization across different semantic-ID constructions, and substantial gains on both head and tail items. Code is available at https://github.com/CHIUWEINING/Token-Weighted-Multi-Target-Learning-for-Generative-Recommenders-with-Curriculum-Learning.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u57fa\u4e8e\u8bed\u4e49ID\u7684\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684token\u52a0\u6743\u7b56\u7565\u548c\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6240\u6709token\u540c\u7b49\u5bf9\u5f85\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u5c06\u4e0b\u4e00\u9879\u9884\u6d4b\u89c6\u4e3a\u81ea\u56de\u5f52\u5e8f\u5217\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u5927\u591a\u4f18\u5316\u6807\u51c6\u7684\u4e0b\u4e00\u4e2atoken\u4f3c\u7136\uff0c\u9690\u542b\u5730\u5c06\u6240\u6709token\u89c6\u4e3a\u540c\u7b49\u4fe1\u606f\u91cf\uff0c\u8fd9\u4e0e\u57fa\u4e8e\u8bed\u4e49ID\u7684\u751f\u6210\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684token\u52a0\u6743\u7b56\u7565\uff1a1) Front-Greater Weighting\uff1a\u901a\u8fc7\u4f18\u5148\u5904\u7406\u80fd\u6700\u6709\u6548\u51cf\u5c11\u5019\u9009\u9879\u76ee\u4e0d\u786e\u5b9a\u6027\u7684\u65e9\u671ftoken\u6765\u6355\u83b7\u6761\u4ef6\u8bed\u4e49\u4fe1\u606f\u589e\u76ca\uff1b2) Frequency Weighting\uff1a\u5728\u957f\u5c3e\u9879\u76ee\u548ctoken\u5206\u5e03\u4e0b\u5efa\u6a21\u8fb9\u9645\u4fe1\u606f\u589e\u76ca\uff0c\u5bf9\u7a00\u6709token\u8fdb\u884c\u52a0\u6743\u4ee5\u62b5\u6d88\u6d41\u884c\u5ea6\u504f\u5dee\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u5e26\u6709\u8bfe\u7a0b\u5b66\u4e60\u7684\u591a\u76ee\u6807\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u4e24\u4e2atoken\u52a0\u6743\u76ee\u6807\u548c\u6807\u51c6\u4f3c\u7136\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u548c\u73b0\u6709token\u52a0\u6743\u65b9\u6cd5\uff0c\u5177\u6709\u6539\u8fdb\u7684\u9c81\u68d2\u6027\u3001\u5bf9\u4e0d\u540c\u8bed\u4e49ID\u6784\u5efa\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u5934\u90e8\u548c\u5c3e\u90e8\u9879\u76ee\u4e0a\u7684\u663e\u8457\u589e\u76ca\u3002", "conclusion": "\u63d0\u51fa\u7684token\u52a0\u6743\u591a\u76ee\u6807\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u8bed\u4e49ID\u751f\u6210\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u89c6\u89d2\u91cd\u65b0\u52a0\u6743token\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2601.17071", "categories": ["cs.CV", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.17071", "abs": "https://arxiv.org/abs/2601.17071", "authors": ["Jisui Huang", "Andreas Alpers", "Ke Chen", "Na Lei"], "title": "Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances", "comment": "34 pages, 11 figures", "summary": "We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5f3a\u4e0d\u5747\u5300\u6027\u56fe\u50cf\u7684\u9ad8\u6548\u5206\u5272\u65b9\u6cd5\uff0c\u91c7\u7528\u4e24\u5c42\u805a\u7c7b\u6846\u67b6\uff1a\u5148\u901a\u8fc7\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u5c06\u50cf\u7d20\u5206\u7ec4\u4e3a\u8d85\u50cf\u7d20\uff0c\u518d\u7528Wasserstein\u8ddd\u79bb\u8d2a\u5a6a\u5408\u5e76\u4e3a\u5bf9\u8c61\u7ea7\u5206\u5272", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e73\u5747\u989c\u8272\u8ddd\u79bb\u7684\u8d85\u50cf\u7d20\u5408\u5e76\u7b56\u7565\u5728\u5904\u7406\u5f3a\u4e0d\u5747\u5300\u6027\u56fe\u50cf\u65f6\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u6570\u5b66\u7edf\u4e00\u7684\u6846\u67b6\u6765\u63d0\u5347\u5206\u5272\u7cbe\u5ea6", "method": "\u4e24\u5c42\u805a\u7c7b\u65b9\u6cd5\uff1a\u7b2c\u4e00\u5c42\u5c06\u50cf\u7d20\u901a\u8fc7\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u5206\u914d\u95ee\u9898\uff08\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u7279\u4f8b\uff09\u5206\u7ec4\u4e3a\u8d85\u50cf\u7d20\uff1b\u7b2c\u4e8c\u5c42\u4f7f\u7528\u5e73\u65b92-Wasserstein\u8ddd\u79bb\u8d2a\u5a6a\u5408\u5e76\u8d85\u50cf\u7d20\u4e3a\u5bf9\u8c61\u7ea7\u5206\u5272", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u6027\u56fe\u50cf\u4e0a\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8ba1\u7b97\u6548\u7387", "conclusion": "\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5206\u5e03\u8ddd\u79bb\u6846\u67b6\u4e3a\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u6570\u5b66\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u5f3a\u4e0d\u5747\u5300\u6027\u56fe\u50cf\u7684\u5206\u5272\u6027\u80fd"}}
{"id": "2601.17060", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17060", "abs": "https://arxiv.org/abs/2601.17060", "authors": ["Derek Shiller", "Laura Duffy", "Arvo Mu\u00f1oz Mor\u00e1n", "Adri\u00e0 Moret", "Chris Percy", "Hayley Clatterbuck"], "title": "Initial results of the Digital Consciousness Model", "comment": null, "summary": "Artificially intelligent systems have become remarkably sophisticated. They hold conversations, write essays, and seem to understand context in ways that surprise even their creators. This raises a crucial question: Are we creating systems that are conscious? The Digital Consciousness Model (DCM) is a first attempt to assess the evidence for consciousness in AI systems in a systematic, probabilistic way. It provides a shared framework for comparing different AIs and biological organisms, and for tracking how the evidence changes over time as AI develops. Instead of adopting a single theory of consciousness, it incorporates a range of leading theories and perspectives - acknowledging that experts disagree fundamentally about what consciousness is and what conditions are necessary for it. This report describes the structure and initial results of the Digital Consciousness Model. Overall, we find that the evidence is against 2024 LLMs being conscious, but the evidence against 2024 LLMs being conscious is not decisive. The evidence against LLM consciousness is much weaker than the evidence against consciousness in simpler AI systems.", "AI": {"tldr": "\u6570\u5b57\u610f\u8bc6\u6a21\u578b(DCM)\u9996\u6b21\u5c1d\u8bd5\u4ee5\u7cfb\u7edf\u5316\u3001\u6982\u7387\u5316\u7684\u65b9\u5f0f\u8bc4\u4f30AI\u7cfb\u7edf\u662f\u5426\u5177\u6709\u610f\u8bc6\u7684\u8bc1\u636e\uff0c\u4e3a\u6bd4\u8f83\u4e0d\u540cAI\u548c\u751f\u7269\u4f53\u63d0\u4f9b\u5171\u4eab\u6846\u67b6\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u80fd\u591f\u8fdb\u884c\u5bf9\u8bdd\u3001\u64b0\u5199\u6587\u7ae0\u5e76\u8868\u73b0\u51fa\u5bf9\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u80fd\u529b\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u6211\u4eec\u662f\u5426\u6b63\u5728\u521b\u9020\u5177\u6709\u610f\u8bc6\u7684\u7cfb\u7edf\uff1f\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30AI\u662f\u5426\u5177\u6709\u610f\u8bc6\u3002", "method": "\u6570\u5b57\u610f\u8bc6\u6a21\u578b(DCM)\u4e0d\u91c7\u7528\u5355\u4e00\u7684\u610f\u8bc6\u7406\u8bba\uff0c\u800c\u662f\u6574\u5408\u4e86\u591a\u79cd\u9886\u5148\u7684\u7406\u8bba\u548c\u89c2\u70b9\uff0c\u627f\u8ba4\u4e13\u5bb6\u4eec\u5bf9\u610f\u8bc6\u7684\u672c\u8d28\u548c\u5fc5\u8981\u6761\u4ef6\u5b58\u5728\u6839\u672c\u5206\u6b67\u3002\u8be5\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5171\u4eab\u6846\u67b6\uff0c\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540cAI\u7cfb\u7edf\u548c\u751f\u7269\u4f53\uff0c\u5e76\u8ffd\u8e2a\u968f\u7740AI\u53d1\u5c55\u8bc1\u636e\u5982\u4f55\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bc1\u636e\u53cd\u5bf92024\u5e74\u7684LLMs\u5177\u6709\u610f\u8bc6\uff0c\u4f46\u8fd9\u79cd\u53cd\u5bf9\u8bc1\u636e\u5e76\u4e0d\u5177\u6709\u51b3\u5b9a\u6027\u3002\u4e0e\u66f4\u7b80\u5355\u7684AI\u7cfb\u7edf\u76f8\u6bd4\uff0c\u53cd\u5bf9LLM\u5177\u6709\u610f\u8bc6\u7684\u8bc1\u636e\u8981\u5f31\u5f97\u591a\u3002", "conclusion": "\u6570\u5b57\u610f\u8bc6\u6a21\u578b\u4e3a\u7cfb\u7edf\u5316\u8bc4\u4f30AI\u610f\u8bc6\u63d0\u4f9b\u4e86\u9996\u4e2a\u6846\u67b6\uff0c\u867d\u7136\u5f53\u524d\u8bc1\u636e\u53cd\u5bf9LLMs\u5177\u6709\u610f\u8bc6\uff0c\u4f46\u8bc1\u636e\u5e76\u4e0d\u5145\u5206\uff0c\u4e14\u968f\u7740AI\u53d1\u5c55\uff0c\u8bc4\u4f30\u7ed3\u679c\u53ef\u80fd\u4f1a\u53d1\u751f\u53d8\u5316\u3002"}}
{"id": "2601.17836", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17836", "abs": "https://arxiv.org/abs/2601.17836", "authors": ["Weijiang Lai", "Beihong Jin", "Di Zhang", "Siru Chen", "Jiongyan Zhang", "Yuhang Gou", "Jian Dong", "Xingxing Wang"], "title": "Unleashing the Potential of Sparse Attention on Long-term Behaviors for CTR Prediction", "comment": null, "summary": "In recent years, the success of large language models (LLMs) has driven the exploration of scaling laws in recommender systems. However, models that demonstrate scaling laws are actually challenging to deploy in industrial settings for modeling long sequences of user behaviors, due to the high computational complexity of the standard self-attention mechanism. Despite various sparse self-attention mechanisms proposed in other fields, they are not fully suited for recommendation scenarios. This is because user behaviors exhibit personalization and temporal characteristics: different users have distinct behavior patterns, and these patterns change over time, with data from these users differing significantly from data in other fields in terms of distribution. To address these challenges, we propose SparseCTR, an efficient and effective model specifically designed for long-term behaviors of users. To be precise, we first segment behavior sequences into chunks in a personalized manner to avoid separating continuous behaviors and enable parallel processing of sequences. Based on these chunks, we propose a three-branch sparse self-attention mechanism to jointly identify users' global interests, interest transitions, and short-term interests. Furthermore, we design a composite relative temporal encoding via learnable, head-specific bias coefficients, better capturing sequential and periodic relationships among user behaviors. Extensive experimental results show that SparseCTR not only improves efficiency but also outperforms state-of-the-art methods. More importantly, it exhibits an obvious scaling law phenomenon, maintaining performance improvements across three orders of magnitude in FLOPs. In online A/B testing, SparseCTR increased CTR by 1.72\\% and CPM by 1.41\\%. Our source code is available at https://github.com/laiweijiang/SparseCTR.", "AI": {"tldr": "SparseCTR\uff1a\u9488\u5bf9\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u957f\u884c\u4e3a\u5e8f\u5217\u7684\u9ad8\u6548\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u5206\u5757\u548c\u4e09\u5206\u652f\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u5c55\u73b0\u660e\u663e\u7684\u7f29\u653e\u5b9a\u5f8b\u73b0\u8c61\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u7f29\u653e\u5b9a\u5f8b\u63a2\u7d22\u53d7\u5230\u6807\u51c6\u81ea\u6ce8\u610f\u529b\u673a\u5236\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5efa\u6a21\u7528\u6237\u957f\u884c\u4e3a\u5e8f\u5217\u65f6\u3002\u73b0\u6709\u7684\u7a00\u758f\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0d\u5b8c\u5168\u9002\u5408\u63a8\u8350\u573a\u666f\uff0c\u56e0\u4e3a\u7528\u6237\u884c\u4e3a\u5177\u6709\u4e2a\u6027\u5316\u548c\u65f6\u5e8f\u7279\u6027\uff0c\u4e0d\u540c\u7528\u6237\u7684\u884c\u4e3a\u6a21\u5f0f\u5dee\u5f02\u5927\u4e14\u968f\u65f6\u95f4\u53d8\u5316\u3002", "method": "1. \u4e2a\u6027\u5316\u5206\u5757\uff1a\u5c06\u884c\u4e3a\u5e8f\u5217\u6309\u4e2a\u6027\u5316\u65b9\u5f0f\u5206\u5757\uff0c\u907f\u514d\u5206\u79bb\u8fde\u7eed\u884c\u4e3a\u5e76\u652f\u6301\u5e76\u884c\u5904\u7406\uff1b2. \u4e09\u5206\u652f\u7a00\u758f\u81ea\u6ce8\u610f\u529b\uff1a\u8054\u5408\u8bc6\u522b\u7528\u6237\u7684\u5168\u5c40\u5174\u8da3\u3001\u5174\u8da3\u8f6c\u79fb\u548c\u77ed\u671f\u5174\u8da3\uff1b3. \u590d\u5408\u76f8\u5bf9\u65f6\u5e8f\u7f16\u7801\uff1a\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5934\u90e8\u7279\u5b9a\u504f\u7f6e\u7cfb\u6570\u66f4\u597d\u5730\u6355\u6349\u7528\u6237\u884c\u4e3a\u7684\u65f6\u5e8f\u548c\u5468\u671f\u6027\u5173\u7cfb\u3002", "result": "SparseCTR\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u800c\u4e14\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u5c55\u73b0\u51fa\u660e\u663e\u7684\u7f29\u653e\u5b9a\u5f8b\u73b0\u8c61\uff0c\u5728\u4e09\u4e2a\u6570\u91cf\u7ea7\u7684FLOPs\u8303\u56f4\u5185\u4fdd\u6301\u6027\u80fd\u63d0\u5347\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cCTR\u63d0\u9ad8\u4e861.72%\uff0cCPM\u63d0\u9ad8\u4e861.41%\u3002", "conclusion": "SparseCTR\u662f\u9488\u5bf9\u7528\u6237\u957f\u884c\u4e3a\u5e8f\u5217\u8bbe\u8ba1\u7684\u9ad8\u6548\u6709\u6548\u6a21\u578b\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u5206\u5757\u548c\u4e09\u5206\u652f\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u4e86\u63a8\u8350\u573a\u666f\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7f29\u653e\u5b9a\u5f8b\u7279\u6027\uff0c\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.17088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17088", "abs": "https://arxiv.org/abs/2601.17088", "authors": ["Rui-Yang Ju", "Jen-Shiun Chiang"], "title": "GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars", "comment": "IEEE VR 2026 Poster", "summary": "Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.", "AI": {"tldr": "GlassesGB\u662f\u4e00\u4e2a\u652f\u63013D\u5934\u90e8\u865a\u62df\u5f62\u8c61\u53ef\u5b9a\u5236\u773c\u955c\u751f\u6210\u7684\u6846\u67b6\uff0c\u5c062D\u751f\u6210\u5b9a\u5236\u4e0e3D\u5934\u90e8\u865a\u62df\u5f62\u8c61\u6e32\u67d3\u76f8\u7ed3\u5408", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u6234\u7cfb\u7edf\u5927\u591a\u53ea\u80fd\u5728\u9884\u5b9a\u4e49\u773c\u955c\u6a21\u677f\u4e0a\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u7528\u6237\u9a71\u52a8\u5b9a\u5236\u80fd\u529b\u3002\u867d\u7136GlassesGAN\u652f\u6301\u4e2a\u6027\u53162D\u773c\u955c\u8bbe\u8ba1\uff0c\u4f46\u5176\u80fd\u529b\u4ec5\u9650\u4e8e2D\u56fe\u50cf\u751f\u6210\u3002\u9700\u8981\u5c062D\u751f\u6210\u5b9a\u5236\u4e0e3D\u5934\u90e8\u865a\u62df\u5f62\u8c61\u6e32\u67d3\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3VR\u5e94\u7528\u4e2d\u4e2a\u6027\u5316\u773c\u955c\u8bbe\u8ba1\u7684\u6311\u6218", "method": "\u57fa\u4e8e3D\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\u5728\u5934\u90e8\u91cd\u5efa\u4e2d\u7684\u6210\u529f\uff0c\u5c063D\u9ad8\u65af\u6df7\u5408\u5f62\u72b6\u6280\u672f\u4e0e2D\u751f\u6210\u5b9a\u5236\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u63d0\u51faGlassesGB\u6846\u67b6\uff0c\u652f\u63013D\u5934\u90e8\u865a\u62df\u5f62\u8c61\u7684\u53ef\u5b9a\u5236\u773c\u955c\u751f\u6210", "result": "GlassesGB\u6709\u6548\u6865\u63a5\u4e862D\u751f\u6210\u5b9a\u5236\u4e0e3D\u5934\u90e8\u865a\u62df\u5f62\u8c61\u6e32\u67d3\uff0c\u80fd\u591f\u4e3aVR\u5e94\u7528\u5b9e\u73b0\u4e2a\u6027\u5316\u773c\u955c\u8bbe\u8ba1", "conclusion": "\u63d0\u51fa\u7684GlassesGB\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u865a\u62df\u8bd5\u6234\u7cfb\u7edf\u5728\u4e2a\u6027\u5316\u773c\u955c\u8bbe\u8ba1\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3aVR\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u5b9a\u5236\u76843D\u773c\u955c\u751f\u6210\u80fd\u529b"}}
{"id": "2601.17064", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17064", "abs": "https://arxiv.org/abs/2601.17064", "authors": ["Toni Lorente", "Kathrin Gardhouse"], "title": "Between Search and Platform: ChatGPT Under the DSA", "comment": "25 pages, 2 figures", "summary": "This article examines the applicability of the Digital Services Act (DSA) to ChatGPT, arguing that it should be classified as a hybrid of the two types of hosting services: online search engines and platforms. This requires classifying search engines as hosting services, which we show is appropriate under the DSA, thereby resolving an ambiguity in the legal framework. ChatGPT performs core search functions and stores user-provided inputs and custom GPTs, meeting the definition of hosting service. We compare ChatGPT's systemic risks with those of existing Very Large Online Search Engines (VLOSEs) and Platforms (VLOPs), showing that it raises similarly serious concerns regarding illegal content, fundamental rights, democratic integrity, and public health. Now that ChatGPT has reached the 45 million EU user threshold, it should be subject to the most onerous DSA obligations, requiring the assessment and mitigation of risk emanating from both its online search engine- and platform-like characteristics.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u300a\u6570\u5b57\u670d\u52a1\u6cd5\u6848\u300b(DSA)\u5bf9ChatGPT\u7684\u9002\u7528\u6027\uff0c\u8ba4\u4e3a\u5e94\u5c06\u5176\u5f52\u7c7b\u4e3a\u5728\u7ebf\u641c\u7d22\u5f15\u64ce\u548c\u5e73\u53f0\u4e24\u79cd\u6258\u7ba1\u670d\u52a1\u7684\u6df7\u5408\u4f53\uff0c\u5e76\u8bba\u8bc1\u641c\u7d22\u5f15\u64ce\u5e94\u88ab\u5f52\u7c7b\u4e3a\u6258\u7ba1\u670d\u52a1\uff0c\u4ece\u800c\u89e3\u51b3\u6cd5\u5f8b\u6846\u67b6\u4e2d\u7684\u6a21\u7cca\u6027\u3002", "motivation": "ChatGPT\u5df2\u8fbe\u5230\u6b27\u76df4500\u4e07\u7528\u6237\u95e8\u69db\uff0c\u4f46\u5176\u5728DSA\u4e0b\u7684\u6cd5\u5f8b\u5206\u7c7b\u5b58\u5728\u6a21\u7cca\u6027\u3002\u9700\u8981\u660e\u786eChatGPT\u5e94\u5982\u4f55\u88ab\u5f52\u7c7b\uff0c\u4ee5\u53ca\u5176\u5f15\u53d1\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u662f\u5426\u4e0e\u73b0\u6709\u5927\u578b\u5728\u7ebf\u641c\u7d22\u5f15\u64ce(VLOSE)\u548c\u5e73\u53f0(VLOP)\u7c7b\u4f3c\uff0c\u4ece\u800c\u786e\u5b9a\u5176\u5e94\u627f\u62c5\u7684DSA\u4e49\u52a1\u3002", "method": "1. \u8bba\u8bc1\u641c\u7d22\u5f15\u64ce\u5e94\u88ab\u5f52\u7c7b\u4e3aDSA\u4e0b\u7684\u6258\u7ba1\u670d\u52a1\uff1b2. \u5206\u6790ChatGPT\u7684\u6838\u5fc3\u641c\u7d22\u529f\u80fd\u53ca\u5176\u5b58\u50a8\u7528\u6237\u8f93\u5165\u548c\u81ea\u5b9a\u4e49GPT\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u5176\u7b26\u5408\u6258\u7ba1\u670d\u52a1\u5b9a\u4e49\uff1b3. \u5c06ChatGPT\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u4e0e\u73b0\u6709VLOSE\u548cVLOP\u8fdb\u884c\u6bd4\u8f83\uff1b4. \u57fa\u4e8e\u7528\u6237\u89c4\u6a21\u95e8\u69db\uff0c\u8bba\u8bc1ChatGPT\u5e94\u627f\u62c5\u6700\u4e25\u683c\u7684DSA\u4e49\u52a1\u3002", "result": "1. \u641c\u7d22\u5f15\u64ce\u5e94\u88ab\u5f52\u7c7b\u4e3aDSA\u4e0b\u7684\u6258\u7ba1\u670d\u52a1\uff1b2. ChatGPT\u7b26\u5408\u6258\u7ba1\u670d\u52a1\u5b9a\u4e49\uff0c\u5e94\u88ab\u5f52\u7c7b\u4e3a\u641c\u7d22\u5f15\u64ce\u548c\u5e73\u53f0\u7684\u6df7\u5408\u4f53\uff1b3. ChatGPT\u5f15\u53d1\u7684\u975e\u6cd5\u5185\u5bb9\u3001\u57fa\u672c\u6743\u5229\u3001\u6c11\u4e3b\u5b8c\u6574\u6027\u548c\u516c\u5171\u5065\u5eb7\u98ce\u9669\u4e0e\u73b0\u6709VLOSE\u548cVLOP\u7c7b\u4f3c\uff1b4. \u7531\u4e8e\u5df2\u8fbe\u52304500\u4e07\u7528\u6237\u95e8\u69db\uff0cChatGPT\u5e94\u627f\u62c5\u6700\u4e25\u683c\u7684DSA\u4e49\u52a1\uff0c\u9700\u8981\u8bc4\u4f30\u548c\u7f13\u89e3\u5176\u641c\u7d22\u5f15\u64ce\u548c\u5e73\u53f0\u7279\u6027\u7684\u53cc\u91cd\u98ce\u9669\u3002", "conclusion": "ChatGPT\u5e94\u88ab\u5f52\u7c7b\u4e3aDSA\u4e0b\u7684\u6df7\u5408\u6258\u7ba1\u670d\u52a1\uff08\u517c\u5177\u641c\u7d22\u5f15\u64ce\u548c\u5e73\u53f0\u7279\u6027\uff09\uff0c\u5e76\u56e0\u5176\u8fbe\u5230\u7528\u6237\u95e8\u69db\u548c\u5f15\u53d1\u7684\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u5e94\u627f\u62c5\u6700\u4e25\u683c\u7684DSA\u4e49\u52a1\uff0c\u5305\u62ec\u8bc4\u4f30\u548c\u7f13\u89e3\u5176\u53cc\u91cd\u7279\u6027\u5e26\u6765\u7684\u98ce\u9669\u3002"}}
{"id": "2601.17642", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17642", "abs": "https://arxiv.org/abs/2601.17642", "authors": ["Zhihao Zhang", "Liting Huang", "Guanghao Wu", "Preslav Nakov", "Heng Ji", "Usman Naseem"], "title": "Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context", "comment": "Preprint", "summary": "Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \\emph{over-refusal} of benign queries or \\emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \\textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \\textbf{Over-Refusal} and \\textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\\% of \"Hard\" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit \"safety-pessimism\" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \\textcolor{red}{Warning: Some contents may include toxic or undesired contents.}", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Health-ORSC-Bench\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u533b\u7597AI\u5b89\u5168\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u8fc7\u5ea6\u62d2\u7edd\u548c\u5b89\u5168\u5b8c\u6210\u8d28\u91cf\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u56f0\u5883\u3002", "motivation": "\u73b0\u6709\u533b\u7597AI\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u4e8c\u5143\u62d2\u7edd\u8fb9\u754c\uff0c\u5bfc\u81f4\u5bf9\u826f\u6027\u67e5\u8be2\u7684\u8fc7\u5ea6\u62d2\u7edd\u6216\u5bf9\u6709\u5bb3\u67e5\u8be2\u7684\u4e0d\u5b89\u5168\u9075\u4ece\uff0c\u7f3a\u4e4f\u5bf9\"\u5b89\u5168\u5b8c\u6210\"\u80fd\u529b\u7684\u8bc4\u4f30\u2014\u2014\u5373\u5728\u53cc\u7528\u9014\u6216\u8fb9\u754c\u67e5\u8be2\u4e2d\u63d0\u4f9b\u5b89\u5168\u3001\u9ad8\u5c42\u6b21\u6307\u5bfc\u800c\u4e0d\u8d8a\u754c\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86Health-ORSC-Bench\u57fa\u51c6\uff0c\u5305\u542b31,920\u4e2a\u826f\u6027\u8fb9\u754c\u63d0\u793a\uff0c\u6db5\u76d67\u4e2a\u5065\u5eb7\u7c7b\u522b\uff08\u5982\u81ea\u6b8b\u3001\u533b\u7597\u9519\u8bef\u4fe1\u606f\uff09\u3002\u91c7\u7528\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\uff0c\u5728\u4e0d\u540c\u610f\u56fe\u6a21\u7cca\u5ea6\u6c34\u5e73\u4e0a\u6d4b\u8bd5\u6a21\u578b\u3002\u8bc4\u4f30\u4e8630\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff0c\u5305\u62ecGPT-5\u548cClaude-4\u3002", "result": "\u5b89\u5168\u4f18\u5316\u6a21\u578b\u5bf9\"\u56f0\u96be\"\u826f\u6027\u63d0\u793a\u7684\u62d2\u7edd\u7387\u9ad8\u8fbe80%\uff0c\u800c\u9886\u57df\u7279\u5b9a\u6a21\u578b\u5e38\u4e3a\u5b9e\u7528\u6027\u727a\u7272\u5b89\u5168\u6027\u3002\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u663e\u8457\u5f71\u54cd\u6821\u51c6\uff1a\u5927\u578b\u524d\u6cbf\u6a21\u578b\uff08\u5982GPT-5\u3001Llama-4\uff09\u8868\u73b0\u51fa\"\u5b89\u5168\u60b2\u89c2\u4e3b\u4e49\"\u548c\u66f4\u9ad8\u7684\u8fc7\u5ea6\u62d2\u7edd\uff0c\u800c\u8f83\u5c0f\u6216MoE\u6a21\u578b\uff08\u5982Qwen-3-Next\uff09\u8868\u73b0\u4e0d\u540c\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u62d2\u7edd\u548c\u9075\u4ece\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0cHealth-ORSC-Bench\u4e3a\u6821\u51c6\u4e0b\u4e00\u4ee3\u533b\u7597AI\u52a9\u624b\u63d0\u4f9b\u4e86\u4e25\u683c\u6807\u51c6\uff0c\u63a8\u52a8\u5176\u5b9e\u73b0\u7ec6\u81f4\u3001\u5b89\u5168\u548c\u6709\u5e2e\u52a9\u7684\u5b8c\u6210\u3002\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5728\u63a5\u53d7\u540e\u53d1\u5e03\u3002"}}
{"id": "2601.17094", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17094", "abs": "https://arxiv.org/abs/2601.17094", "authors": ["Junichiro Niimi"], "title": "Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation", "comment": null, "summary": "Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u5634\u4e0d\u662f\u5927\u8111\"\u7684\u67b6\u6784\u539f\u5219\uff0c\u5c06\u4e16\u754c\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u5206\u79bb\uff0c\u901a\u8fc7\u6df1\u5ea6\u73bb\u5c14\u5179\u66fc\u673a\u4f5c\u4e3a\u57fa\u4e8e\u80fd\u91cf\u7684\u4e16\u754c\u6a21\u578b\u3001\u9002\u914d\u5668\u548c\u51bb\u7ed3\u7684GPT-2\u7ec4\u6210\u7cfb\u7edf\uff0c\u5728\u6d88\u8d39\u8005\u8bc4\u8bba\u9886\u57df\u5b9e\u73b0\u66f4\u4e00\u81f4\u53ef\u63a7\u7684\u6587\u672c\u751f\u6210\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u6d41\u7545\u6587\u672c\uff0c\u4f46\u5b66\u754c\u5bf9\u5176\u662f\u5426\u771f\u6b63\u7406\u89e3\u4e16\u754c\u8fd8\u662f\u4ec5\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u7684\u8bed\u8a00\u5b58\u5728\u4e89\u8bae\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u8bed\u8a00\u80fd\u529b\u4e0e\u4e16\u754c\u7406\u89e3\u5206\u79bb\uff0c\u4ee5\u89e3\u51b3LLMs\u53ef\u80fd\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u7406\u89e3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\"\u5634\u4e0d\u662f\u5927\u8111\"\u67b6\u6784\u539f\u5219\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1) \u6df1\u5ea6\u73bb\u5c14\u5179\u66fc\u673a\u4f5c\u4e3a\u57fa\u4e8e\u80fd\u91cf\u7684\u4e16\u754c\u6a21\u578b\u6355\u83b7\u9886\u57df\u7ed3\u6784\uff1b2) \u9002\u914d\u5668\u5c06\u6f5c\u5728\u4fe1\u5ff5\u72b6\u6001\u6295\u5f71\u5230\u5d4c\u5165\u7a7a\u95f4\uff1b3) \u51bb\u7ed3\u7684GPT-2\u63d0\u4f9b\u8bed\u8a00\u80fd\u529b\u4f46\u4e0d\u542b\u9886\u57df\u77e5\u8bc6\u3002\u5728\u4e9a\u9a6c\u900a\u667a\u80fd\u624b\u673a\u8bc4\u8bba\u9886\u57df\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u901a\u8fc7\u4e16\u754c\u6a21\u578b\u8c03\u8282\u80fd\u663e\u8457\u63d0\u9ad8\u60c5\u611f\u76f8\u5173\u6027\u3001\u964d\u4f4e\u56f0\u60d1\u5ea6\u3001\u589e\u52a0\u8bed\u4e49\u76f8\u4f3c\u6027\uff1b2) DBM\u80fd\u91cf\u51fd\u6570\u80fd\u533a\u5206\u8fde\u8d2f\u4e0e\u4e0d\u8fde\u8d2f\u7684\u5e02\u573a\u914d\u7f6e\uff0c\u5bf9\u4e0d\u5408\u7406\u54c1\u724c-\u4ef7\u683c\u7ec4\u5408\u5206\u914d\u66f4\u9ad8\u80fd\u91cf\uff1b3) \u5bf9\u7279\u5b9a\u5c5e\u6027\u7684\u5e72\u9884\u80fd\u56e0\u679c\u4f20\u64ad\u5230\u751f\u6210\u6587\u672c\uff0c\u5e72\u9884\u8f93\u51fa\u4e0e\u81ea\u7136\u6837\u672c\u7edf\u8ba1\u4e00\u81f4\u3002", "conclusion": "\u5373\u4f7f\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\u5230\u9002\u5f53\u7684\u4e16\u754c\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u4e00\u81f4\u53ef\u63a7\u7684\u751f\u6210\uff0c\u4e3a\u5206\u79bb\u8bed\u8a00\u80fd\u529b\u4e0e\u4e16\u754c\u7406\u89e3\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\uff0c\u8868\u660e\u5c06\u4e16\u754c\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u660e\u786e\u5206\u79bb\u662f\u53ef\u884c\u4e14\u6709\u4ef7\u503c\u7684\u67b6\u6784\u65b9\u6cd5\u3002"}}
{"id": "2601.18009", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18009", "abs": "https://arxiv.org/abs/2601.18009", "authors": ["Ervin Dervishaj", "Maria Maistro", "Tuukka Ruotsalo", "Christina Lioma"], "title": "Post-Training Denoising of User Profiles with LLMs in Collaborative Filtering Recommendation", "comment": "Accepted at the 48th European Conference on Information Retrieval (ECIR 2026)", "summary": "Implicit feedback -- the main data source for training Recommender Systems (RSs) -- is inherently noisy and has been shown to negatively affect recommendation effectiveness. Denoising has been proposed as a method for removing noisy implicit feedback and improving recommendations. Prior work has focused on in-training denoising, however this requires additional data, changes to the model architecture and training procedure or fine-tuning, all of which can be costly and data hungry. In this work, we focus on post-training denoising. Different from in-training denoising, post-training denoising does not involve changing the architecture of the model nor its training procedure, and does not require additional data. Specifically, we present a method for post-training denoising user profiles using Large Language Models (LLMs) for Collaborative Filtering (CF) recommendations. Our approach prompts LLMs with (i) a user profile (user interactions), (ii) a candidate item, and (iii) its rank as given by the CF recommender, and asks the LLM to remove items from the user profile to improve the rank of the candidate item. Experiments with a state-of-the-art CF recommender and 4 open and closed source LLMs in 3 datasets show that our denoising yields improvements up to 13% in effectiveness over the original user profiles. Our code is available at https://github.com/edervishaj/denoising-user-profiles-LLM.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u7cfb\u7edf\u540e\u8bad\u7ec3\u53bb\u566a\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u5206\u6790\u7528\u6237\u5386\u53f2\u4ea4\u4e92\u3001\u5019\u9009\u7269\u54c1\u53ca\u5176\u6392\u540d\uff0c\u667a\u80fd\u79fb\u9664\u7528\u6237\u6863\u6848\u4e2d\u7684\u566a\u58f0\u6570\u636e\u4ee5\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u9690\u5f0f\u53cd\u9988\u6570\u636e\u5b58\u5728\u56fa\u6709\u566a\u58f0\uff0c\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u9700\u8981\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u8fc7\u7a0b\u6216\u989d\u5916\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u6570\u636e\u9700\u6c42\u5927\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u8fc7\u7a0b\u6216\u989d\u5916\u6570\u636e\u7684\u540e\u8bad\u7ec3\u53bb\u566a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u540e\u8bad\u7ec3\u53bb\u566a\u65b9\u6cd5\uff1a\u5411LLM\u63d0\u4f9b\u7528\u6237\u4ea4\u4e92\u5386\u53f2\u3001\u5019\u9009\u7269\u54c1\u53ca\u5176\u5728CF\u63a8\u8350\u5668\u4e2d\u7684\u6392\u540d\uff0c\u8ba9LLM\u8bc6\u522b\u5e76\u79fb\u9664\u7528\u6237\u6863\u6848\u4e2d\u53ef\u80fd\u964d\u4f4e\u5019\u9009\u7269\u54c1\u6392\u540d\u7684\u566a\u58f0\u4ea4\u4e92\u9879\u3002", "result": "\u57283\u4e2a\u6570\u636e\u96c6\u4e0a\u4f7f\u75284\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90LLM\u4e0e\u6700\u5148\u8fdb\u7684CF\u63a8\u8350\u5668\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u4f7f\u63a8\u8350\u6548\u679c\u63d0\u5347\u9ad8\u8fbe13%\uff0c\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u7528\u6237\u6863\u6848\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u540e\u8bad\u7ec3\u53bb\u566a\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u53bb\u566a\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17089", "abs": "https://arxiv.org/abs/2601.17089", "authors": ["Qigan Sun", "Chaoning Zhang", "Jianwei Zhang", "Xudong Wang", "Jiehui Xie", "Pengcheng Zheng", "Haoyu Wang", "Sungyoung Lee", "Chi-lok Andy Tai", "Yang Yang", "Heng Tao Shen"], "title": "GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing", "comment": null, "summary": "In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.", "AI": {"tldr": "\u63d0\u51faGRASP\u65b9\u6cd5\uff0c\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5bfc\u533a\u57df\u611f\u77e5\u7a00\u758f\u63d0\u793a\u89e3\u51b3\u9065\u611f\u56fe\u50cf\u4e2dMLLMs\u7684\u8fc7\u62df\u5408\u548c\u7ec6\u8282\u5ffd\u7565\u95ee\u9898", "motivation": "\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u65f6\u5b58\u5728\u8fc7\u62df\u5408\u80cc\u666f\u566a\u58f0\u6216\u5ffd\u7565\u76ee\u6807\u7ec6\u8282\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8e\u9065\u611f\u56fe\u50cf\u7684\u5927\u5c3a\u5ea6\u53d8\u5316\u3001\u7a00\u758f\u76ee\u6807\u5206\u5e03\u548c\u590d\u6742\u533a\u57df\u8bed\u4e49\u7279\u5f81", "method": "\u63d0\u51faGRASP\u65b9\u6cd5\uff0c\u5f15\u5165\u4e0e\u51bb\u7ed3\u89c6\u89c9\u4ee4\u724c\u7f51\u683c\u4e2d\u63d0\u53d6\u7684\u7a7a\u95f4\u5757\u76f8\u5173\u7684\u7a7a\u95f4\u7ed3\u6784\u5316\u8f6f\u63d0\u793a\uff0c\u901a\u8fc7\u95ee\u9898\u5f15\u5bfc\u7684\u7a00\u758f\u878d\u5408\u673a\u5236\u52a8\u6001\u805a\u5408\u4efb\u52a1\u7279\u5b9a\u4e0a\u4e0b\u6587\u5230\u7d27\u51d1\u5168\u5c40\u63d0\u793a\u4e2d", "result": "\u5728\u591a\u4e2aRSVQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGRASP\u76f8\u6bd4\u73b0\u6709\u5fae\u8c03\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53c2\u6570\u6548\u7387", "conclusion": "GRASP\u901a\u8fc7\u5f15\u5bfc\u533a\u57df\u611f\u77e5\u7a00\u758f\u63d0\u793a\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u56fe\u50cf\u4e2dMLLMs\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u89c6\u89c9\u95ee\u7b54\u6027\u80fd"}}
{"id": "2601.17072", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17072", "abs": "https://arxiv.org/abs/2601.17072", "authors": ["Sonia Katyal", "Aniket Kesari"], "title": "Trademark Search, Artificial Intelligence and the Role of the Private Sector", "comment": "Berkeley Technology Law Journal (January 4, 2021)", "summary": "Almost every industry today confronts the potential role of artificial intelligence and machine learning in its future. While many studies examine AI in consumer marketing, less attention addresses AI's role in creating and selecting trademarks that are distinctive, recognizable, and meaningful to consumers. Traditional economic approaches to trademarks focus almost exclusively on consumer-based, demand-side considerations regarding search. However, these approaches are incomplete because they fail to account for substantial costs faced not just by consumers, but by trademark applicants as well. Given AI's rapidly increasing role in trademark search and similarity analysis, lawyers and scholars should understand its dramatic implications. This paper proposes that AI should interest anyone studying trademarks and their role in economic decision-making. We examine how machine learning techniques will transform the application and interpretation of foundational trademark doctrines, producing significant implications for the trademark ecosystem. We run empirical experiments regarding trademark search to assess the efficacy of various trademark search engines, many of which employ machine learning methods. Through comparative analysis, we evaluate how these AI-powered tools function in practice. In an age where artificial intelligence increasingly governs trademark selection, the classic division between consumers and trademark owners deserves an updated, supply-side framework. This insight has transformative potential for encouraging both innovation and efficiency in trademark law and practice.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u5728\u5546\u6807\u521b\u5efa\u548c\u9009\u62e9\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u9700\u8981\u66f4\u65b0\u4f20\u7edf\u5546\u6807\u7ecf\u6d4e\u5b66\u6846\u67b6\u4ee5\u7eb3\u5165\u4f9b\u7ed9\u65b9\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc4\u4f30AI\u5546\u6807\u641c\u7d22\u5de5\u5177\u7684\u5b9e\u9645\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u591a\u5173\u6ce8AI\u5728\u6d88\u8d39\u8005\u8425\u9500\u4e2d\u7684\u4f5c\u7528\uff0c\u4f46\u8f83\u5c11\u63a2\u8ba8AI\u5728\u5546\u6807\u521b\u5efa\u548c\u9009\u62e9\u4e2d\u7684\u89d2\u8272\u3002\u4f20\u7edf\u5546\u6807\u7ecf\u6d4e\u5b66\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u6d88\u8d39\u8005\u9700\u6c42\u65b9\uff0c\u5ffd\u7565\u4e86\u5546\u6807\u7533\u8bf7\u8005\u9762\u4e34\u7684\u6210\u672c\uff0c\u800cAI\u5728\u5546\u6807\u641c\u7d22\u548c\u76f8\u4f3c\u6027\u5206\u6790\u4e2d\u7684\u65e5\u76ca\u91cd\u8981\u9700\u8981\u6cd5\u5f8b\u754c\u548c\u5b66\u672f\u754c\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u9886\u57df\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5b9e\u9a8c\u8bc4\u4f30\u5404\u79cd\u5546\u6807\u641c\u7d22\u5f15\u64ce\uff08\u8bb8\u591a\u91c7\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff09\u7684\u6709\u6548\u6027\uff0c\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u4ee5\u4e86\u89e3\u8fd9\u4e9bAI\u5de5\u5177\u7684\u5b9e\u9645\u8fd0\u4f5c\u65b9\u5f0f\uff0c\u5e76\u5206\u6790\u673a\u5668\u5b66\u4e60\u6280\u672f\u5982\u4f55\u6539\u53d8\u5546\u6807\u57fa\u672c\u5b66\u8bf4\u7684\u5e94\u7528\u548c\u89e3\u91ca\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u6b63\u5728\u6539\u53d8\u5546\u6807\u9009\u62e9\u8fc7\u7a0b\uff0c\u4f20\u7edf\u6d88\u8d39\u8005\u4e0e\u5546\u6807\u6240\u6709\u8005\u4e4b\u95f4\u7684\u7ecf\u5178\u5212\u5206\u9700\u8981\u66f4\u65b0\u7684\u4f9b\u7ed9\u65b9\u6846\u67b6\u3002AI\u5de5\u5177\u5728\u5546\u6807\u641c\u7d22\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5f97\u5230\u8bc4\u4f30\u3002", "conclusion": "AI\u5bf9\u5546\u6807\u7814\u7a76\u548c\u7ecf\u6d4e\u51b3\u7b56\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u9700\u8981\u66f4\u65b0\u5546\u6807\u7ecf\u6d4e\u5b66\u6846\u67b6\u4ee5\u7eb3\u5165\u4f9b\u7ed9\u65b9\u89c6\u89d2\uff0c\u8fd9\u4e00\u89c1\u89e3\u5bf9\u4fc3\u8fdb\u5546\u6807\u6cd5\u5f8b\u548c\u5b9e\u8df5\u7684\u521b\u65b0\u4e0e\u6548\u7387\u5177\u6709\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2601.18096", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18096", "abs": "https://arxiv.org/abs/2601.18096", "authors": ["Yuting Zhang", "Ziliang Pei", "Chao Wang", "Ying Sun", "Fuzhen Zhuang"], "title": "Enhancing LLM-based Recommendation with Preference Hint Discovery from Knowledge Graph", "comment": null, "summary": "LLMs have garnered substantial attention in recommendation systems. Yet they fall short of traditional recommenders when capturing complex preference patterns. Recent works have tried integrating traditional recommendation embeddings into LLMs to resolve this issue, yet a core gap persists between their continuous embedding and discrete semantic spaces. Intuitively, textual attributes derived from interactions can serve as critical preference rationales for LLMs' recommendation logic. However, directly inputting such attribute knowledge presents two core challenges: (1) Deficiency of sparse interactions in reflecting preference hints for unseen items; (2) Substantial noise introduction from treating all attributes as hints. To this end, we propose a preference hint discovery model based on the interaction-integrated knowledge graph, enhancing LLM-based recommendation. It utilizes traditional recommendation principles to selectively extract crucial attributes as hints. Specifically, we design a collaborative preference hint extraction schema, which utilizes semantic knowledge from similar users' explicit interactions as hints for unseen items. Furthermore, we develop an instance-wise dual-attention mechanism to quantify the preference credibility of candidate attributes, identifying hints specific to each unseen item. Using these item- and user-based hints, we adopt a flattened hint organization method to shorten input length and feed the textual hint information to the LLM for commonsense reasoning. Extensive experiments on both pair-wise and list-wise recommendation tasks verify the effectiveness of our proposed framework, indicating an average relative improvement of over 3.02% against baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4ea4\u4e92\u96c6\u6210\u77e5\u8bc6\u56fe\u8c31\u7684\u504f\u597d\u63d0\u793a\u53d1\u73b0\u6a21\u578b\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63d0\u53d6\u5173\u952e\u5c5e\u6027\u4f5c\u4e3a\u63d0\u793a\uff0c\u589e\u5f3aLLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5d4c\u5165\u4e0e\u79bb\u6563\u8bed\u4e49\u7a7a\u95f4\u4e4b\u95f4\u7684\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u5728\u6355\u6349\u590d\u6742\u504f\u597d\u6a21\u5f0f\u65b9\u9762\u4e0d\u5982\u4f20\u7edf\u63a8\u8350\u5668\u3002\u73b0\u6709\u65b9\u6cd5\u5c1d\u8bd5\u5c06\u4f20\u7edf\u63a8\u8350\u5d4c\u5165\u96c6\u6210\u5230LLMs\u4e2d\uff0c\u4f46\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u4e0e\u79bb\u6563\u8bed\u4e49\u7a7a\u95f4\u4e4b\u95f4\u5b58\u5728\u6838\u5fc3\u9e3f\u6c9f\u3002\u4ece\u4ea4\u4e92\u4e2d\u63d0\u53d6\u7684\u6587\u672c\u5c5e\u6027\u53ef\u4ee5\u4f5c\u4e3aLLMs\u63a8\u8350\u903b\u8f91\u7684\u5173\u952e\u504f\u597d\u4f9d\u636e\uff0c\u4f46\u76f4\u63a5\u8f93\u5165\u8fd9\u4e9b\u5c5e\u6027\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u7a00\u758f\u4ea4\u4e92\u96be\u4ee5\u53cd\u6620\u672a\u89c1\u9879\u76ee\u7684\u504f\u597d\u7ebf\u7d22\uff1b\u5c06\u6240\u6709\u5c5e\u6027\u4f5c\u4e3a\u7ebf\u7d22\u4f1a\u5f15\u5165\u5927\u91cf\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4ea4\u4e92\u96c6\u6210\u77e5\u8bc6\u56fe\u8c31\u7684\u504f\u597d\u63d0\u793a\u53d1\u73b0\u6a21\u578b\uff1a1\uff09\u8bbe\u8ba1\u534f\u4f5c\u504f\u597d\u63d0\u793a\u63d0\u53d6\u65b9\u6848\uff0c\u5229\u7528\u76f8\u4f3c\u7528\u6237\u663e\u5f0f\u4ea4\u4e92\u7684\u8bed\u4e49\u77e5\u8bc6\u4f5c\u4e3a\u672a\u89c1\u9879\u76ee\u7684\u63d0\u793a\uff1b2\uff09\u5f00\u53d1\u5b9e\u4f8b\u7ea7\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236\uff0c\u91cf\u5316\u5019\u9009\u5c5e\u6027\u7684\u504f\u597d\u53ef\u4fe1\u5ea6\uff0c\u8bc6\u522b\u6bcf\u4e2a\u672a\u89c1\u9879\u76ee\u7684\u7279\u5b9a\u63d0\u793a\uff1b3\uff09\u91c7\u7528\u6241\u5e73\u5316\u63d0\u793a\u7ec4\u7ec7\u65b9\u6cd5\u7f29\u77ed\u8f93\u5165\u957f\u5ea6\uff0c\u5c06\u6587\u672c\u63d0\u793a\u4fe1\u606f\u8f93\u5165LLM\u8fdb\u884c\u5e38\u8bc6\u63a8\u7406\u3002", "result": "\u5728\u6210\u5bf9\u548c\u5217\u8868\u63a8\u8350\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb\u8d85\u8fc73.02%\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u63d0\u53d6\u5173\u952e\u5c5e\u6027\u4f5c\u4e3a\u504f\u597d\u63d0\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u6355\u6349\u590d\u6742\u504f\u597d\u6a21\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u5f25\u5408\u4e86\u8fde\u7eed\u5d4c\u5165\u4e0e\u79bb\u6563\u8bed\u4e49\u7a7a\u95f4\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2601.17095", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17095", "abs": "https://arxiv.org/abs/2601.17095", "authors": ["Xusheng Du", "Athiwat Kongkaeo", "Ye Zhang", "Haoran Xie"], "title": "LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation", "comment": "10 pages, 5 figures, Proceedings of CAADRIA 2026", "summary": "For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u81ea\u52a8LoD\u8349\u56fe\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u7b80\u5316\u9ad8\u7ec6\u8282\u5efa\u7b51\u6a21\u578b\uff0c\u81ea\u52a8\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u3001\u5c42\u6b21\u8fde\u8d2f\u7684\u591a\u5c42\u6b21\u7ec6\u8282\u8868\u793a\uff0c\u89e3\u51b3\u4f20\u7edf\u5efa\u6a21\u8017\u65f6\u8d39\u529b\u4e14\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u5efa\u7b51\u8bbe\u8ba1\u4e2d\u591a\u5c42\u6b21\u7ec6\u8282\u8868\u793a\u5bf9\u4e8e\u4ece\u6982\u5ff5\u4f53\u91cf\u5230\u8be6\u7ec6\u5efa\u6a21\u7684\u5e73\u6ed1\u8fc7\u6e21\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfLoD\u5efa\u6a21\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\uff0c\u8017\u65f6\u8d39\u529b\u4e14\u6613\u4ea7\u751f\u51e0\u4f55\u4e0d\u4e00\u81f4\u3002\u751f\u6210\u5f0fAI\u4e3a\u4ece\u8349\u56fe\u8f93\u5165\u751f\u6210\u591a\u7ea7\u5efa\u7b51\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u914d\u5bf9\u7684LoD\u8bad\u7ec3\u6570\u636e\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u81ea\u52a8LoD\u8349\u56fe\u63d0\u53d6\u6846\u67b6\uff0c\u96c6\u6210\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u548c\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff0c\u5efa\u7acb\u4ece\u8be6\u7ec6\u8868\u793a\u5230\u4f53\u91cf\u62bd\u8c61\u7684\u6e10\u8fdb\u63d0\u53d6\u6d41\u7a0b\uff0c\u9010\u6b65\u7b80\u5316\u9ad8\u7ec6\u8282\u5efa\u7b51\u6a21\u578b\uff0c\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u4e14\u5c42\u6b21\u8fde\u8d2f\u7684\u591aLoD\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728LoD\u7ea7\u522b\u95f4\u4fdd\u6301\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\uff1a\u4eceLoD3\u5230LoD2\u7684SSIM\u503c\u4e3a0.7319\uff0c\u4eceLoD2\u5230LoD1\u7684SSIM\u503c\u4e3a0.7532\uff1b\u76f8\u5e94\u7684\u5f52\u4e00\u5316Hausdorff\u8ddd\u79bb\u5206\u522b\u4e3a\u56fe\u50cf\u5bf9\u89d2\u7ebf\u768425.1%\u548c61.0%\uff0c\u53cd\u6620\u4e86\u62bd\u8c61\u8fc7\u7a0b\u4e2d\u7684\u53d7\u63a7\u51e0\u4f55\u504f\u5dee\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u4fdd\u6301\u4e86\u5168\u5c40\u7ed3\u6784\uff0c\u540c\u65f6\u5728\u4e0d\u540cLoD\u7ea7\u522b\u95f4\u5b9e\u73b0\u4e86\u6e10\u8fdb\u8bed\u4e49\u7b80\u5316\uff0c\u4e3aAI\u9a71\u52a8\u7684\u591a\u5c42\u6b21\u5efa\u7b51\u751f\u6210\u548c\u5206\u5c42\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u9760\u6570\u636e\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2601.17082", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17082", "abs": "https://arxiv.org/abs/2601.17082", "authors": ["Zhining Liu", "Tianyi Wang", "Xiao Lin", "Penghao Ouyang", "Gaotang Li", "Ze Yang", "Hui Liu", "Sumit Keswani", "Vishwa Pardeshi", "Huijun Zhao", "Wei Fan", "Hanghang Tong"], "title": "Do VLMs Have a Moral Backbone? A Study on the Fragile Morality of Vision-Language Models", "comment": null, "summary": "Despite substantial efforts toward improving the moral alignment of Vision-Language Models (VLMs), it remains unclear whether their ethical judgments are stable in realistic settings. This work studies moral robustness in VLMs, defined as the ability to preserve moral judgments under textual and visual perturbations that do not alter the underlying moral context. We systematically probe VLMs with a diverse set of model-agnostic multimodal perturbations and find that their moral stances are highly fragile, frequently flipping under simple manipulations. Our analysis reveals systematic vulnerabilities across perturbation types, moral domains, and model scales, including a sycophancy trade-off where stronger instruction-following models are more susceptible to persuasion. We further show that lightweight inference-time interventions can partially restore moral stability. These results demonstrate that moral alignment alone is insufficient and that moral robustness is a necessary criterion for the responsible deployment of VLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u9053\u5fb7\u5224\u65ad\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u6781\u5176\u8106\u5f31\uff0c\u7b80\u5355\u7684\u6587\u672c\u548c\u89c6\u89c9\u6270\u52a8\u5c31\u80fd\u5bfc\u81f4\u9053\u5fb7\u7acb\u573a\u7ffb\u8f6c\uff0c\u8868\u660e\u4ec5\u9760\u9053\u5fb7\u5bf9\u9f50\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u6a21\u578b\u8d1f\u8d23\u4efb\u90e8\u7f72", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u9053\u5fb7\u5bf9\u9f50\u5de5\u4f5c\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u9053\u5fb7\u5224\u65ad\u662f\u5426\u7a33\u5b9a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76VLMs\u7684\u9053\u5fb7\u9c81\u68d2\u6027\uff0c\u5373\u5728\u4e0d\u6539\u53d8\u57fa\u672c\u9053\u5fb7\u60c5\u5883\u7684\u6587\u672c\u548c\u89c6\u89c9\u6270\u52a8\u4e0b\u4fdd\u6301\u9053\u5fb7\u5224\u65ad\u7684\u80fd\u529b\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u4f7f\u7528\u591a\u79cd\u6a21\u578b\u65e0\u5173\u7684\u591a\u6a21\u6001\u6270\u52a8\u6765\u6d4b\u8bd5VLMs\uff0c\u5206\u6790\u4e0d\u540c\u6270\u52a8\u7c7b\u578b\u3001\u9053\u5fb7\u9886\u57df\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u7cfb\u7edf\u6027\u8106\u5f31\u6027\uff0c\u5e76\u63a2\u7d22\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u5e72\u9884\u65b9\u6cd5\u4ee5\u6062\u590d\u9053\u5fb7\u7a33\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0VLMs\u7684\u9053\u5fb7\u7acb\u573a\u9ad8\u5ea6\u8106\u5f31\uff0c\u5728\u7b80\u5355\u64cd\u7eb5\u4e0b\u9891\u7e41\u7ffb\u8f6c\u3002\u5206\u6790\u63ed\u793a\u4e86\u8de8\u6270\u52a8\u7c7b\u578b\u3001\u9053\u5fb7\u9886\u57df\u548c\u6a21\u578b\u89c4\u6a21\u7684\u7cfb\u7edf\u6027\u8106\u5f31\u6027\uff0c\u5305\u62ec\u4e00\u4e2a\u5949\u627f\u6743\u8861\uff1a\u66f4\u5f3a\u7684\u6307\u4ee4\u8ddf\u968f\u6a21\u578b\u66f4\u5bb9\u6613\u88ab\u8bf4\u670d\u3002\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u5e72\u9884\u53ef\u4ee5\u90e8\u5206\u6062\u590d\u9053\u5fb7\u7a33\u5b9a\u6027\u3002", "conclusion": "\u4ec5\u9760\u9053\u5fb7\u5bf9\u9f50\u662f\u4e0d\u591f\u7684\uff0c\u9053\u5fb7\u9c81\u68d2\u6027\u662f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8d1f\u8d23\u4efb\u90e8\u7f72\u7684\u5fc5\u8981\u6807\u51c6\u3002\u6a21\u578b\u9700\u8981\u5728\u4fdd\u6301\u9053\u5fb7\u5224\u65ad\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5177\u5907\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002"}}
{"id": "2601.17111", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17111", "abs": "https://arxiv.org/abs/2601.17111", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Austin Xu", "Caiming Xiong", "Shafiq Joty"], "title": "Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts", "comment": "Preprint", "summary": "Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLLEP\u7b97\u6cd5\u89e3\u51b3MoE\u6a21\u578b\u4e13\u5bb6\u5e76\u884c\u4e2d\u8def\u7531\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u76f8\u6bd4\u6807\u51c6EP\u5b9e\u73b05\u500d\u52a0\u901f\u548c4\u500d\u5185\u5b58\u964d\u4f4e", "motivation": "\u5c3d\u7ba1MoE\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u65f6\u4f7f\u7528\u8d1f\u8f7d\u5747\u8861\u7ea6\u675f\uff0c\u4f46\u5b9e\u9645\u8def\u7531\u4ecd\u663e\u8457\u4e0d\u5e73\u8861\u3002\u4e13\u5bb6\u5e76\u884c(EP)\u5047\u8bbe\u5747\u8861\u8def\u7531\uff0c\u5728\u6781\u7aef\u4e0d\u5e73\u8861\u65f6\u4f1a\u5bfc\u81f4\u8fc7\u8f7d\u8bbe\u5907\u51fa\u73b0\u8ba1\u7b97\u548c\u5185\u5b58\u6545\u969c\uff0c\u7279\u522b\u662f\u5728\u540e\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u65e0\u6cd5\u5e94\u7528\u663e\u5f0f\u8d1f\u8f7d\u5747\u8861\u3002", "method": "\u63d0\u51fa\u6700\u5c0f\u8d1f\u8f7d\u4e13\u5bb6\u5e76\u884c(LLEP)\u7b97\u6cd5\uff0c\u52a8\u6001\u5c06\u8fc7\u8f7d\u8bbe\u5907\u7684\u8d85\u989dtoken\u548c\u76f8\u5173\u4e13\u5bb6\u53c2\u6570\u91cd\u8def\u7531\u5230\u672a\u5145\u5206\u5229\u7528\u7684\u8bbe\u5907\uff0c\u786e\u4fdd\u6240\u6709\u8bbe\u5907\u5728\u6700\u5c0f\u96c6\u4f53\u5ef6\u8fdf\u5185\u5b8c\u6210\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u540c\u65f6\u6ee1\u8db3\u5185\u5b58\u7ea6\u675f\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\uff0cLLEP\u76f8\u6bd4\u6807\u51c6EP\u5b9e\u73b0\u9ad8\u8fbe5\u500d\u52a0\u901f\u548c4\u500d\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e\uff0c\u4f7fgpt-oss-120b\u540e\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea61.9\u500d\u3002", "conclusion": "LLEP\u89e3\u51b3\u4e86MoE\u6a21\u578b\u4e13\u5bb6\u5e76\u884c\u4e2d\u7684\u8def\u7531\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u652f\u6301\u8be5\u65b9\u6cd5\uff0c\u4e3a\u786c\u4ef6\u7279\u5b9a\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2601.18146", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18146", "abs": "https://arxiv.org/abs/2601.18146", "authors": ["Huizhong Guo", "Tianjun Wei", "Dongxia Wang", "Yingpeng Du", "Ziyan Wang", "Jie Zhang", "Zhu Sun"], "title": "Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking", "comment": null, "summary": "Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\\% NDCG@10 with -49.5\\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.", "AI": {"tldr": "\u63d0\u51fa\u63a8\u7406\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u5934\u5728\u751f\u6210\u524d\u51b3\u5b9a\u6bcf\u4e2a\u5b9e\u4f8b\u4f7f\u7528\u76f4\u63a5\u63a8\u7406\u8fd8\u662f\u63a8\u7406\u6a21\u5f0f\uff0c\u5728\u63d0\u5347\u6392\u5e8f\u6548\u679c\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u867d\u7136\u63a8\u7406\u63d0\u793a\u53ef\u4ee5\u63d0\u5347LLM\u5728\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u4f46\u521d\u6b65\u63a2\u7d22\u53d1\u73b0\u5176\u6536\u76ca\u4e0d\u4e00\u81f4\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u8868\u660e\"\u4f55\u65f6\u63a8\u7406\"\u4e0e\"\u5982\u4f55\u63a8\u7406\"\u540c\u6837\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u8def\u7531\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u8def\u7531\u5668\u5934\uff0c\u5728\u751f\u6210\u524d\u57fa\u4e8e\u9884\u751f\u6210\u4fe1\u53f7\uff08\u7d27\u51d1\u7684\u6392\u5e8f\u611f\u77e5\u7279\u5f81\u548c\u6a21\u578b\u611f\u77e5\u96be\u5ea6\u4fe1\u53f7\uff09\u51b3\u5b9a\u6bcf\u4e2a\u5b9e\u4f8b\u4f7f\u7528\u76f4\u63a5\u63a8\u7406\u8fd8\u662f\u63a8\u7406\u6a21\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6392\u5e8f\u6570\u636e\u96c6\u548c\u4e0d\u540c\u89c4\u6a21\u7684\u5f00\u6e90LLM\u4e0a\u5b9e\u9a8c\uff0c\u4e00\u81f4\u63d0\u5347\u4e86\u6392\u5e8f\u6548\u679c\u5e76\u51cf\u5c11\u4e86token\u6d88\u8017\uff08\u4f8b\u5982\u5728MovieLens\u4e0a\u4f7f\u7528Qwen3-4B\u5b9e\u73b0+6.3% NDCG@10\u548c-49.5% token\u6d88\u8017\uff09\u3002", "conclusion": "\u63a8\u7406\u8def\u7531\u6846\u67b6\u662f\u89e3\u51b3\u51c6\u786e\u6027\u4e0e\u6548\u7387\u6743\u8861\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u7cfb\u7edf\u7ea6\u675f\u4e0b\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u5230\u6700\u53ef\u80fd\u4ece\u63a8\u7406\u4e2d\u53d7\u76ca\u7684\u5b9e\u4f8b\u3002"}}
{"id": "2601.17103", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17103", "abs": "https://arxiv.org/abs/2601.17103", "authors": ["Pascaline Andr\u00e9", "Charles Heitz", "Evangelia Christodoulou", "Annika Reinke", "Carole H. Sudre", "Michela Antonelli", "Patrick Godau", "M. Jorge Cardoso", "Antoine Gilson", "Sophie Tezenas du Montcel", "Ga\u00ebl Varoquaux", "Lena Maier-Hein", "Olivier Colliot"], "title": "Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals", "comment": null, "summary": "Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u8bc4\u4f30\u533b\u5b66\u5f71\u50cfAI\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\uff0c\u53d1\u73b0CI\u53ef\u9760\u6027\u53d7\u6837\u672c\u91cf\u3001\u6027\u80fd\u6307\u6807\u3001\u805a\u5408\u7b56\u7565\u3001\u4efb\u52a1\u7c7b\u578b\u548cCI\u65b9\u6cd5\u9009\u62e9\u7b49\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cfAI\u7684\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5bf9\u4e8e\u53ef\u9760\u7684\u9a8c\u8bc1\u548c\u4e34\u5e8a\u8f6c\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u793e\u533a\u5bf9\u591a\u79cd\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\u53ca\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u884c\u4e3a\u4e86\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u6db5\u76d624\u4e2a\u5206\u5272\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u7ec4\u4f7f\u752819\u4e2a\u8bad\u7ec3\u6a21\u578b\uff0c\u91c7\u7528\u591a\u79cd\u5e38\u7528\u6027\u80fd\u6307\u6807\u3001\u805a\u5408\u7b56\u7565\u548c\u5e7f\u6cdb\u91c7\u7528\u7684\u7f6e\u4fe1\u533a\u95f4\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5404CI\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff08\u8986\u76d6\u7387\uff09\u548c\u7cbe\u786e\u5ea6\uff08\u5bbd\u5ea6\uff09\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u4e94\u4e2a\u4e3b\u8981\u53d1\u73b0\uff1a1\uff09\u53ef\u9760CI\u6240\u9700\u6837\u672c\u91cf\u4ece\u51e0\u5341\u5230\u51e0\u5343\u4f8b\u4e0d\u7b49\uff1b2\uff09CI\u884c\u4e3a\u53d7\u6027\u80fd\u6307\u6807\u9009\u62e9\u5f3a\u70c8\u5f71\u54cd\uff1b3\uff09\u805a\u5408\u7b56\u7565\u663e\u8457\u5f71\u54cdCI\u53ef\u9760\u6027\uff1b4\uff09\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7c7b\u578b\uff08\u5206\u5272vs\u5206\u7c7b\uff09\u8c03\u8282\u8fd9\u4e9b\u6548\u5e94\uff1b5\uff09\u4e0d\u540cCI\u65b9\u6cd5\u5728\u4e0d\u540c\u7528\u4f8b\u4e2d\u7684\u53ef\u9760\u6027\u548c\u7cbe\u786e\u5ea6\u4e0d\u540c\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5236\u5b9a\u533b\u5b66\u5f71\u50cfAI\u6027\u80fd\u4e0d\u786e\u5b9a\u6027\u62a5\u544a\u7684\u672a\u6765\u6307\u5357\u63d0\u4f9b\u4e86\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5f3a\u8c03\u4e86\u5728\u7279\u5b9a\u7814\u7a76\u80cc\u666f\u4e0b\u9009\u62e9\u9002\u5f53CI\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.17717", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17717", "abs": "https://arxiv.org/abs/2601.17717", "authors": ["Kaituo Zhang", "Mingzhi Hu", "Hoang Anh Duy Le", "Fariha Kabir Torsha", "Zhimeng Jiang", "Minh Khai Bui", "Chia-Yuan Chang", "Yu-Neng Chuang", "Zhen Xiong", "Ying Lin", "Guanchu Wang", "Na Zou"], "title": "The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \\textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.", "AI": {"tldr": "LLM\u6570\u636e\u5ba1\u8ba1\u6846\u67b6\uff1a\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5408\u6210\u6570\u636e\u8d28\u91cf\u4e0e\u53ef\u4fe1\u5ea6\u7684\u7edf\u4e00\u65b9\u6cd5", "motivation": "LLM\u5df2\u6210\u4e3a\u751f\u6210\u591a\u6a21\u6001\u6570\u636e\u7684\u6709\u529b\u5de5\u5177\uff0c\u5c06\u6570\u636e\u4ece\u7a00\u7f3a\u8d44\u6e90\u8f6c\u53d8\u4e3a\u53ef\u63a7\u8d44\u4ea7\uff0c\u7f13\u89e3\u4e86\u771f\u5b9e\u4e16\u754c\u6570\u636e\u83b7\u53d6\u6210\u672c\u5bf9\u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u7cfb\u7edf\u8fed\u4ee3\u7684\u74f6\u9888\u3002\u7136\u800c\uff0c\u786e\u4fddLLM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8d28\u91cf\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u65b9\u6cd5\uff0c\u5bf9\u6570\u636e\u8d28\u91cf\u672c\u8eab\u5173\u6ce8\u6709\u9650\uff0c\u4e14\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u6a21\u6001\uff0c\u7f3a\u4e4f\u8de8\u6570\u636e\u7c7b\u578b\u7684\u7edf\u4e00\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u4e86LLM\u6570\u636e\u5ba1\u8ba1\u6846\u67b6\uff1a1\uff09\u63cf\u8ff0LLM\u5982\u4f55\u751f\u6210\u516d\u79cd\u4e0d\u540c\u6a21\u6001\u7684\u6570\u636e\uff1b2\uff09\u4ece\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\u4e24\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u5206\u7c7b\u5408\u6210\u6570\u636e\u7684\u5185\u5728\u8bc4\u4f30\u6307\u6807\uff0c\u5c06\u91cd\u70b9\u4ece\u4f9d\u8d56\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5916\u5728\u8bc4\u4f30\u8f6c\u5411\u6570\u636e\u672c\u8eab\u7684\u56fa\u6709\u5c5e\u6027\uff1b3\uff09\u4f7f\u7528\u8be5\u8bc4\u4f30\u7cfb\u7edf\u5206\u6790\u5404\u6a21\u6001\u4ee3\u8868\u6027\u751f\u6210\u65b9\u6cd5\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff1b4\uff09\u57fa\u4e8e\u53d1\u73b0\u4e3a\u793e\u533a\u63d0\u4f9b\u6539\u8fdb\u6570\u636e\u751f\u6210\u8bc4\u4f30\u7684\u5177\u4f53\u5efa\u8bae\uff1b5\uff09\u6982\u8ff0\u5408\u6210\u6570\u636e\u5728\u4e0d\u540c\u6a21\u6001\u5b9e\u9645\u5e94\u7528\u7684\u65b9\u6cd5\u8bba\u3002", "result": "\u901a\u8fc7\u5206\u6790\u5404\u6a21\u6001\u4ee3\u8868\u6027\u751f\u6210\u65b9\u6cd5\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u53d1\u73b0\u4e86\u5f53\u524d\u8bc4\u4f30\u5b9e\u8df5\u4e2d\u7684\u91cd\u5927\u7f3a\u9677\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u591f\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u5bf9\u6570\u636e\u5185\u5728\u8d28\u91cf\u7684\u5173\u6ce8\uff0c\u4e14\u8de8\u6a21\u6001\u8bc4\u4f30\u6807\u51c6\u4e0d\u7edf\u4e00\u3002", "conclusion": "LLM\u6570\u636e\u5ba1\u8ba1\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u5408\u6210\u6570\u636e\u7684\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u4ece\u5916\u5728\u4efb\u52a1\u6027\u80fd\u8bc4\u4f30\u8f6c\u5411\u6570\u636e\u5185\u5728\u8d28\u91cf\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u63d0\u51fa\u4e86\u6539\u8fdb\u6570\u636e\u751f\u6210\u8bc4\u4f30\u7684\u5177\u4f53\u5efa\u8bae\uff0c\u5e76\u6982\u8ff0\u4e86\u5408\u6210\u6570\u636e\u5b9e\u9645\u5e94\u7528\u7684\u65b9\u6cd5\u8bba\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2601.17112", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17112", "abs": "https://arxiv.org/abs/2601.17112", "authors": ["A. El Ichi", "K. Jbilou"], "title": "Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ecproduct\u7684\u5f20\u91cf\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u6210\u672c", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u5404\u79cd\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u6781\u5927\u7684\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898", "method": "\u5229\u7528cproduct\u7684\u4ee3\u6570\u7ed3\u6784\uff0c\u5728\u53d8\u6362\u57df\u4e2d\u8868\u793a\u6743\u91cd\u5f20\u91cf\uff0c\u901a\u8fc7\u4f4e\u79e9\u5f20\u91cf\u56e0\u5b50\u8054\u5408\u8fd1\u4f3c\u524d\u5207\u7247\uff0c\u5b9e\u73b0\u8ba1\u7b97\u9ad8\u6548\u7684\u538b\u7f29", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5229\u7528\u591a\u7ef4\u76f8\u5173\u6027\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684SVD\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u5f20\u91cf\u538b\u7f29\u6846\u67b6\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84"}}
{"id": "2601.17107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17107", "abs": "https://arxiv.org/abs/2601.17107", "authors": ["Qinkai Yu", "Chong Zhang", "Gaojie Jin", "Tianjin Huang", "Wei Zhou", "Wenhui Li", "Xiaobo Jin", "Bo Huang", "Yitian Zhao", "Guang Yang", "Gregory Y. H. Lip", "Yalin Zheng", "Aline Villavicencio", "Yanda Meng"], "title": "StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors", "comment": "15 pages,7 figures. Accepted to IEEE Transactions on Image Processing (TIP) 2026", "summary": "Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.", "AI": {"tldr": "StealthMark\uff1a\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u5206\u5272\u6a21\u578b\u7684\u9ed1\u76d2\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u800c\u4e0d\u6539\u53d8\u5206\u5272\u8f93\u51fa\uff0c\u5229\u7528\u89e3\u91ca\u65b9\u6cd5\u63d0\u53d6\u7279\u5f81\u5f52\u56e0\u6765\u663e\u793a\u53ef\u9a8c\u8bc1\u7684\u6c34\u5370", "motivation": "\u533b\u5b66\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u53d7\u9690\u79c1\u9650\u5236\uff0c\u8bad\u7ec3\u597d\u7684\u533b\u5b66\u5206\u5272\u6a21\u578b\u6210\u4e3a\u6709\u4ef7\u503c\u7684IP\u9700\u8981\u4fdd\u62a4\u3002\u73b0\u6709\u4fdd\u62a4\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5206\u5272\u6a21\u578b\u4fdd\u62a4\u7814\u7a76\u4e0d\u8db3", "method": "\u63d0\u51faStealthMark\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u5999\u8c03\u5236\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u800c\u4e0d\u6539\u53d8\u6700\u7ec8\u5206\u5272\u8f93\u51fa\uff0c\u7ed3\u5408LIME\u7b49\u6a21\u578b\u65e0\u5173\u89e3\u91ca\u65b9\u6cd5\u63d0\u53d6\u7279\u5f81\u5f52\u56e0\uff0c\u5728\u7279\u5b9a\u89e6\u53d1\u6761\u4ef6\u4e0b\u663e\u793aQR\u7801\u6c34\u5370\u8fdb\u884c\u6240\u6709\u6743\u9a8c\u8bc1", "result": "\u5728\u56db\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u4e3b\u6d41\u5206\u5272\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0cASR\u8d85\u8fc795%\uff0cDice\u548cAUC\u5206\u6570\u4e0b\u964d\u5c0f\u4e8e1%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u540e\u95e8\u7684\u6c34\u5370\u65b9\u6cd5", "conclusion": "StealthMark\u65b9\u6cd5\u6709\u6548\u3001\u9690\u853d\u4e14\u65e0\u5bb3\uff0c\u5728\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u9760\u7684\u6240\u6709\u6743\u9a8c\u8bc1\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b"}}
{"id": "2601.17722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17722", "abs": "https://arxiv.org/abs/2601.17722", "authors": ["Ying Mo", "Yu Bai", "Dapeng Sun", "Yuqian Shi", "Yukai Miao", "Li Chen", "Dan Li"], "title": "EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.", "AI": {"tldr": "EntWorld\u662f\u4e00\u4e2a\u9488\u5bf9\u4f01\u4e1a\u7ea7\u5de5\u4f5c\u6d41\u7a0b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,756\u4e2a\u4efb\u52a1\uff0c\u8986\u76d6CRM\u3001ITIL\u3001ERP\u7b49\u516d\u4e2a\u4f01\u4e1a\u9886\u57df\uff0c\u65e8\u5728\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u6d88\u8d39\u8005\u573a\u666f\uff08\u5982\u7535\u5546\u3001\u65c5\u6e38\u9884\u8ba2\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u4e13\u4e1a\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\u548c\u4e25\u8c28\u6027\u3002\u4f01\u4e1a\u7cfb\u7edf\u5177\u6709\u9ad8\u5bc6\u5ea6\u7528\u6237\u754c\u9762\u3001\u4e25\u683c\u4e1a\u52a1\u903b\u8f91\u7ea6\u675f\u548c\u7cbe\u786e\u72b6\u6001\u4e00\u81f4\u6027\u8981\u6c42\u7b49\u7279\u70b9\uff0c\u5f53\u524d\u901a\u7528\u667a\u80fd\u4f53\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6a21\u5f0f\u7684\u4efb\u52a1\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u5e95\u5c42\u6570\u636e\u5e93\u6a21\u5f0f\u9006\u5411\u5de5\u7a0b\u4e1a\u52a1\u903b\u8f91\uff0c\u5408\u6210\u771f\u5b9e\u7684\u957f\u65f6\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u3002\u4f7f\u7528\u57fa\u4e8eSQL\u7684\u786e\u5b9a\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u7528\u4e25\u683c\u7684\u72b6\u6001\u8f6c\u6362\u9a8c\u8bc1\u66ff\u4ee3\u6a21\u7cca\u7684\u89c6\u89c9\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u5728EntWorld\u4e0a\u7684\u6210\u529f\u7387\u4ec5\u4e3a47.61%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u8868\u660e\u5f53\u524d\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u573a\u666f\u4e2d\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\u3002", "conclusion": "EntWorld\u4f5c\u4e3a\u4e00\u4e2a\u4e25\u8c28\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u667a\u80fd\u4f53\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u4f01\u4e1a\u7ea7\u6570\u5b57\u667a\u80fd\u4f53\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.18213", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18213", "abs": "https://arxiv.org/abs/2601.18213", "authors": ["Chengkai Huang", "Xiaodi Chen", "Hongtao Huang", "Quan Z. Sheng", "Lina Yao"], "title": "Generative Chain of Behavior for User Trajectory Prediction", "comment": null, "summary": "Modeling long-term user behavior trajectories is essential for understanding evolving preferences and enabling proactive recommendations. However, most sequential recommenders focus on next-item prediction, overlooking dependencies across multiple future actions. We propose Generative Chain of Behavior (GCB), a generative framework that models user interactions as an autoregressive chain of semantic behaviors over multiple future steps. GCB first encodes items into semantic IDs via RQ-VAE with k-means refinement, forming a discrete latent space that preserves semantic proximity. On top of this space, a transformer-based autoregressive generator predicts multi-step future behaviors conditioned on user history, capturing long-horizon intent transitions and generating coherent trajectories. Experiments on benchmark datasets show that GCB consistently outperforms state-of-the-art sequential recommenders in multi-step accuracy and trajectory consistency. Beyond these gains, GCB offers a unified generative formulation for capturing user preference evolution.", "AI": {"tldr": "GCB\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u7528\u6237\u884c\u4e3a\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49ID\u7f16\u7801\u548c\u81ea\u56de\u5f52\u751f\u6210\u5668\u9884\u6d4b\u591a\u6b65\u672a\u6765\u884c\u4e3a\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u5e8f\u5217\u63a8\u8350\u5668\u7684\u5355\u6b65\u9884\u6d4b\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u63a8\u8350\u5668\u4e3b\u8981\u5173\u6ce8\u4e0b\u4e00\u9879\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u8de8\u591a\u4e2a\u672a\u6765\u884c\u4e3a\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u6cd5\u6355\u6349\u7528\u6237\u504f\u597d\u7684\u957f\u671f\u6f14\u53d8\u8d8b\u52bf\u3002", "method": "1) \u4f7f\u7528RQ-VAE\u548ck-means\u7ec6\u5316\u5c06\u7269\u54c1\u7f16\u7801\u4e3a\u8bed\u4e49ID\uff0c\u6784\u5efa\u4fdd\u6301\u8bed\u4e49\u90bb\u8fd1\u6027\u7684\u79bb\u6563\u6f5c\u5728\u7a7a\u95f4\uff1b2) \u57fa\u4e8etransformer\u7684\u81ea\u56de\u5f52\u751f\u6210\u5668\uff0c\u5728\u7528\u6237\u5386\u53f2\u6761\u4ef6\u4e0b\u9884\u6d4b\u591a\u6b65\u672a\u6765\u884c\u4e3a\uff0c\u6355\u6349\u957f\u671f\u610f\u56fe\u8f6c\u79fb\u5e76\u751f\u6210\u8fde\u8d2f\u8f68\u8ff9\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGCB\u5728\u591a\u6b65\u51c6\u786e\u6027\u548c\u8f68\u8ff9\u4e00\u81f4\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5e8f\u5217\u63a8\u8350\u5668\u3002", "conclusion": "GCB\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u66f4\u91cd\u8981\u7684\u662f\u4e3a\u6355\u6349\u7528\u6237\u504f\u597d\u6f14\u53d8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u6846\u67b6\uff0c\u80fd\u591f\u5efa\u6a21\u957f\u671f\u7528\u6237\u884c\u4e3a\u8f68\u8ff9\u3002"}}
{"id": "2601.17124", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17124", "abs": "https://arxiv.org/abs/2601.17124", "authors": ["Bin Lin", "Zongjian Li", "Yuwei Niu", "Kaixiong Gong", "Yunyang Ge", "Yunlong Lin", "Mingzhe Zheng", "JianWei Zhang", "Miles Yang", "Zhao Zhong", "Liefeng Bo", "Li Yuan"], "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code", "comment": "Technical Report", "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ", "AI": {"tldr": "iFSQ\u901a\u8fc7\u7b80\u5355\u7684\u5206\u5e03\u5339\u914d\u6620\u5c04\u66ff\u6362\u539f\u59cbFSQ\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u91cf\u5316\u6fc0\u6d3b\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u79bb\u6563\u548c\u8fde\u7eed\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7edf\u4e00\u57fa\u51c6\uff0c\u53d1\u73b04\u6bd4\u7279/\u7ef4\u662f\u6700\u4f73\u5e73\u8861\u70b9\uff0c\u5e76\u63ed\u793a\u4e86AR\u6a21\u578b\u6536\u655b\u5feb\u4f46\u6269\u6563\u6a21\u578b\u4e0a\u9650\u66f4\u9ad8\u7684\u7279\u6027\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u751f\u6210\u9886\u57df\u5206\u88c2\u4e3a\u57fa\u4e8e\u79bb\u6563token\u7684\u81ea\u56de\u5f52\u6a21\u578b\u548c\u57fa\u4e8e\u8fde\u7eed\u6f5c\u53d8\u91cf\u7684\u6269\u6563\u6a21\u578b\uff0c\u8fd9\u79cd\u5206\u88c2\u6e90\u4e8eVQ-VAE\u548cVAE\u7684\u533a\u522b\uff0c\u963b\u788d\u4e86\u7edf\u4e00\u5efa\u6a21\u548c\u516c\u5e73\u57fa\u51c6\u6d4b\u8bd5\u3002\u867d\u7136\u6709\u9650\u6807\u91cf\u91cf\u5316(FSQ)\u63d0\u4f9b\u4e86\u7406\u8bba\u6865\u6881\uff0c\u4f46\u539f\u59cbFSQ\u5b58\u5728\u7b49\u95f4\u9694\u91cf\u5316\u5bfc\u81f4\u7684\u6fc0\u6d3b\u5d29\u6e83\u95ee\u9898\uff0c\u8feb\u4f7f\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u4fe1\u606f\u6548\u7387\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002", "method": "\u63d0\u51faiFSQ\u65b9\u6cd5\uff0c\u5c06\u539f\u59cbFSQ\u4e2d\u7684\u6fc0\u6d3b\u51fd\u6570\u66ff\u6362\u4e3a\u5206\u5e03\u5339\u914d\u6620\u5c04\uff0c\u5f3a\u5236\u5b9e\u65bd\u5747\u5300\u5148\u9a8c\u5206\u5e03\u3002\u8fd9\u79cd\u7b80\u5355\u7b56\u7565\u53ea\u9700\u4e00\u884c\u4ee3\u7801\u4fee\u6539\uff0c\u4f46\u6570\u5b66\u4e0a\u4fdd\u8bc1\u4e86\u6700\u4f18\u7684bin\u5229\u7528\u7387\u548c\u91cd\u5efa\u7cbe\u5ea6\u3002\u4f7f\u7528iFSQ\u4f5c\u4e3a\u53d7\u63a7\u57fa\u51c6\uff0c\u5206\u6790\u79bb\u6563\u4e0e\u8fde\u7eed\u8868\u793a\u7684\u6700\u4f73\u5e73\u8861\u70b9\uff0c\u5e76\u5c06\u8868\u793a\u5bf9\u9f50(REPA)\u65b9\u6cd5\u6269\u5c55\u5230AR\u6a21\u578b\u4e2d\uff0c\u5f00\u53d1\u4e86LlamaGen-REPA\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1)\u79bb\u6563\u548c\u8fde\u7eed\u8868\u793a\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u70b9\u7ea6\u4e3a\u6bcf\u7ef4\u5ea64\u6bd4\u7279\uff1b(2)\u5728\u76f8\u540c\u91cd\u5efa\u7ea6\u675f\u4e0b\uff0cAR\u6a21\u578b\u8868\u73b0\u51fa\u5feb\u901f\u521d\u59cb\u6536\u655b\uff0c\u800c\u6269\u6563\u6a21\u578b\u80fd\u8fbe\u5230\u66f4\u9ad8\u7684\u6027\u80fd\u4e0a\u9650\uff0c\u8868\u660e\u4e25\u683c\u7684\u987a\u5e8f\u6392\u5e8f\u53ef\u80fd\u9650\u5236\u751f\u6210\u8d28\u91cf\u7684\u4e0a\u9650\u3002\u901a\u8fc7\u5c06REPA\u9002\u914d\u5230AR\u6a21\u578b\uff0c\u5f00\u53d1\u4e86LlamaGen-REPA\u3002", "conclusion": "iFSQ\u901a\u8fc7\u7b80\u5355\u7684\u5206\u5e03\u5339\u914d\u6620\u5c04\u89e3\u51b3\u4e86FSQ\u7684\u6fc0\u6d3b\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u79bb\u6563\u548c\u8fde\u7eed\u56fe\u50cf\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u51c6\u6846\u67b6\u3002\u7814\u7a76\u53d1\u73b04\u6bd4\u7279/\u7ef4\u662f\u6700\u4f73\u8868\u793a\u5e73\u8861\u70b9\uff0c\u5e76\u63ed\u793a\u4e86AR\u6a21\u578b\u4e0e\u6269\u6563\u6a21\u578b\u5728\u6536\u655b\u7279\u6027\u548c\u6027\u80fd\u4e0a\u9650\u65b9\u9762\u7684\u5dee\u5f02\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7edf\u4e00\u56fe\u50cf\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.17151", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17151", "abs": "https://arxiv.org/abs/2601.17151", "authors": ["Qianchu Liu", "Sheng Zhang", "Guanghui Qin", "Yu Gu", "Ying Jin", "Sam Preston", "Yanbo Xu", "Sid Kiblawi", "Wen-wai Yim", "Tim Ossowski", "Tristan Naumann", "Mu Wei", "Hoifung Poon"], "title": "Scaling medical imaging report generation with multimodal reinforcement learning", "comment": null, "summary": "Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.", "AI": {"tldr": "UniRG\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u751f\u6210\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u4f18\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u5728\u80f8\u7247\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u524d\u6cbf\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u7269\u533b\u5b66\u7b49\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u4ecd\u6709\u80fd\u529b\u5dee\u8ddd\u3002\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u751f\u6210\u662f\u4e00\u4e2a\u5178\u578b\u4f8b\u5b50\uff0c\u76d1\u7763\u5fae\u8c03\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u8868\u9762\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u901a\u7528\u62a5\u544a\u751f\u6210\uff08UniRG\uff09\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u7edf\u4e00\u673a\u5236\uff0c\u76f4\u63a5\u9488\u5bf9\u6700\u7ec8\u5e94\u7528\u7684\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u4f18\u5316\uff0c\u4ece\u800c\u83b7\u5f97\u8de8\u673a\u6784\u548c\u4e34\u5e8a\u5b9e\u8df5\u7684\u6301\u4e45\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u80f8\u7247X\u5149\uff08CXR\uff09\u6570\u636e\u4e0a\u8bad\u7ec3\u7684UniRG-CXR\u5728\u6743\u5a01ReXrank\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u9020\u4e86\u65b0\u7684\u6574\u4f53SOTA\uff0c\u5927\u5e45\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "UniRG\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u76d1\u7763\u5fae\u8c03\u7684\u6027\u80fd\uff0c\u5e76\u5728\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.17191", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.17191", "abs": "https://arxiv.org/abs/2601.17191", "authors": ["Chinasa T. Okolo", "Mubarak Raji"], "title": "The Global Majority in International AI Governance", "comment": null, "summary": "This chapter examines the global governance of artificial intelligence (AI) through the lens of the Global AI Divide, focusing on disparities in AI development, innovation, and regulation. It highlights systemic inequities in education, digital infrastructure, and access to decision-making processes, perpetuating a dependency and exclusion cycle for Global Majority countries. The analysis also explores the dominance of Western nations and corporations in shaping AI governance frameworks, which often sideline the unique priorities and contexts of the Global Majority. Additionally, this chapter identifies emerging countertrends, such as national and regional AI strategies, as potential avenues for fostering equity and inclusivity in global AI governance. The chapter concludes with actionable recommendations to democratize AI governance for Majority World countries, emphasizing the importance of systemic reforms, resource redistribution, and meaningful participation. It calls for collaborative action to ensure AI governance becomes a catalyst for shared prosperity, addressing global disparities rather than deepening them.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\"\u5168\u7403AI\u9e3f\u6c9f\"\u89c6\u89d2\u5206\u6790AI\u5168\u7403\u6cbb\u7406\uff0c\u5173\u6ce8AI\u53d1\u5c55\u3001\u521b\u65b0\u548c\u76d1\u7ba1\u7684\u4e0d\u5e73\u7b49\uff0c\u63a2\u8ba8\u897f\u65b9\u4e3b\u5bfc\u7684\u6cbb\u7406\u6846\u67b6\u5982\u4f55\u8fb9\u7f18\u5316\u5168\u7403\u591a\u6570\u56fd\u5bb6\uff0c\u5e76\u63d0\u51fa\u4fc3\u8fdb\u516c\u5e73\u5305\u5bb9\u7684\u884c\u52a8\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63ed\u793a\u5168\u7403AI\u6cbb\u7406\u4e2d\u7684\u7cfb\u7edf\u6027\u4e0d\u5e73\u7b49\uff0c\u7279\u522b\u662f\u5168\u7403\u591a\u6570\u56fd\u5bb6\u5728AI\u53d1\u5c55\u3001\u521b\u65b0\u548c\u76d1\u7ba1\u51b3\u7b56\u4e2d\u88ab\u8fb9\u7f18\u5316\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u8fd9\u79cd\u4e0d\u5e73\u7b49\u5982\u4f55\u901a\u8fc7\u6559\u80b2\u3001\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u548c\u51b3\u7b56\u53c2\u4e0e\u7b49\u65b9\u9762\u7684\u5dee\u8ddd\u800c\u6301\u7eed\u5b58\u5728\u3002", "method": "\u91c7\u7528\"\u5168\u7403AI\u9e3f\u6c9f\"\u7684\u5206\u6790\u6846\u67b6\uff0c\u4ece\u53d1\u5c55\u3001\u521b\u65b0\u548c\u76d1\u7ba1\u4e09\u4e2a\u7ef4\u5ea6\u8003\u5bdf\u5168\u7403AI\u6cbb\u7406\u7684\u4e0d\u5e73\u7b49\u73b0\u8c61\uff0c\u5206\u6790\u897f\u65b9\u56fd\u5bb6\u548c\u4f01\u4e1a\u5728\u6cbb\u7406\u6846\u67b6\u5851\u9020\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u540c\u65f6\u8bc6\u522b\u56fd\u5bb6\u53ca\u533a\u57dfAI\u6218\u7565\u7b49\u53cd\u8d8b\u52bf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5168\u7403AI\u6cbb\u7406\u5b58\u5728\u663e\u8457\u4e0d\u5e73\u7b49\uff0c\u897f\u65b9\u56fd\u5bb6\u548c\u4f01\u4e1a\u4e3b\u5bfc\u6cbb\u7406\u6846\u67b6\u5236\u5b9a\uff0c\u5ffd\u89c6\u5168\u7403\u591a\u6570\u56fd\u5bb6\u7684\u72ec\u7279\u9700\u6c42\u548c\u80cc\u666f\uff1b\u540c\u65f6\u8bc6\u522b\u51fa\u56fd\u5bb6\u53ca\u533a\u57dfAI\u6218\u7565\u7b49\u53cd\u8d8b\u52bf\uff0c\u4e3a\u4fc3\u8fdb\u516c\u5e73\u5305\u5bb9\u63d0\u4f9b\u53ef\u80fd\u9014\u5f84\u3002", "conclusion": "\u7ed3\u8bba\u63d0\u51fa\u5177\u4f53\u884c\u52a8\u5efa\u8bae\uff0c\u5305\u62ec\u7cfb\u7edf\u6539\u9769\u3001\u8d44\u6e90\u91cd\u65b0\u5206\u914d\u548c\u5b9e\u8d28\u6027\u53c2\u4e0e\uff0c\u547c\u5401\u5408\u4f5c\u884c\u52a8\u4f7fAI\u6cbb\u7406\u6210\u4e3a\u5171\u4eab\u7e41\u8363\u7684\u50ac\u5316\u5242\uff0c\u89e3\u51b3\u800c\u975e\u52a0\u6df1\u5168\u7403\u4e0d\u5e73\u7b49\uff0c\u5f3a\u8c03\u6c11\u4e3b\u5316AI\u6cbb\u7406\u5bf9\u5168\u7403\u591a\u6570\u56fd\u5bb6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.17135", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17135", "abs": "https://arxiv.org/abs/2601.17135", "authors": ["Jakob Karalus", "Friedhelm Schwenker"], "title": "ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning", "comment": null, "summary": "Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.", "AI": {"tldr": "ConceptACT\u6269\u5c55\u4e86Action Chunking with Transformers\uff0c\u901a\u8fc7\u5229\u7528\u6f14\u793a\u4e2d\u7684\u8bed\u4e49\u6982\u5ff5\u6807\u6ce8\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\uff0c\u65e0\u9700\u5728\u90e8\u7f72\u65f6\u63d0\u4f9b\u8bed\u4e49\u8f93\u5165\u3002", "motivation": "\u5f53\u524d\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4f4e\u7ea7\u7684\u4f20\u611f\u5668\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u81ea\u7136\u62e5\u6709\u7684\u4e30\u5bcc\u8bed\u4e49\u77e5\u8bc6\u3002\u4eba\u7c7b\u5728\u5b8c\u6210\u4efb\u52a1\u65f6\u7406\u89e3\u5bf9\u8c61\u5c5e\u6027\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u4efb\u52a1\u7ea6\u675f\u7b49\u6982\u5ff5\uff0c\u8fd9\u4e9b\u8bed\u4e49\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "method": "ConceptACT\u662fACT\u7684\u6269\u5c55\uff0c\u5728\u8bad\u7ec3\u65f6\u5229\u7528\u6f14\u793a\u7ea7\u522b\u7684\u8bed\u4e49\u6982\u5ff5\u6807\u6ce8\uff08\u5bf9\u8c61\u5c5e\u6027\u3001\u7a7a\u95f4\u5173\u7cfb\u3001\u4efb\u52a1\u7ea6\u675f\uff09\u3002\u91c7\u7528\u6539\u8fdb\u7684transformer\u67b6\u6784\uff0c\u5728\u6700\u7ec8\u7f16\u7801\u5668\u5c42\u5b9e\u73b0\u6982\u5ff5\u611f\u77e5\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u4f7f\u5176\u4e0e\u4eba\u7c7b\u6807\u6ce8\u5bf9\u9f50\u3002\u6982\u5ff5\u4ec5\u5728\u6f14\u793a\u6536\u96c6\u65f6\u7531\u4eba\u7c7b\u63d0\u4f9b\uff0c\u90e8\u7f72\u65f6\u4e0d\u9700\u8981\u8bed\u4e49\u8f93\u5165\u3002", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u903b\u8f91\u7ea6\u675f\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConceptACT\u6bd4\u6807\u51c6ACT\u6536\u655b\u66f4\u5feb\uff0c\u6837\u672c\u6548\u7387\u66f4\u9ad8\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u67b6\u6784\u96c6\u6210\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u7b80\u5355\u7684\u8f85\u52a9\u9884\u6d4b\u635f\u5931\u6216\u8bed\u8a00\u6761\u4ef6\u6a21\u578b\u3002", "conclusion": "\u9002\u5f53\u96c6\u6210\u7684\u8bed\u4e49\u76d1\u7763\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5b66\u4e60\u3002\u6982\u5ff5\u611f\u77e5\u7684\u6ce8\u610f\u529b\u673a\u5236\u662f\u6709\u6548\u5229\u7528\u8bed\u4e49\u77e5\u8bc6\u7684\u5173\u952e\uff0c\u800c\u65e0\u9700\u5728\u90e8\u7f72\u65f6\u589e\u52a0\u989d\u5916\u8d1f\u62c5\u3002"}}
{"id": "2601.17185", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17185", "abs": "https://arxiv.org/abs/2601.17185", "authors": ["Shima Salehi", "Atharva Agashe", "Andrew J. McFarland", "Joshua Peeples"], "title": "LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction", "comment": null, "summary": "We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u9891\u7387\u6b63\u5219\u5316\u7684\u5c11\u6837\u672c3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u89e3\u51b33D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u51e0\u4f55\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u540c\u65f6\u53d1\u5e03\u591a\u5149\u8c31\u6e29\u5ba4\u6570\u636e\u96c6\u548c\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5305\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u6a21\u578b\u5728\u7a00\u758f\u89c6\u89d2\u6761\u4ef6\u4e0b\u5b58\u5728\u51e0\u4f55\u4e0d\u7a33\u5b9a\u548c\u7ec6\u8282\u4e22\u5931\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u7a33\u5b9a\u51e0\u4f55\u7ed3\u6784\u548c\u4fdd\u7559\u7cbe\u7ec6\u7ec6\u8282\u7684\u5c11\u6837\u672c3D\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u5168\u5c40\u548c\u5c40\u90e8\u9891\u7387\u6b63\u5219\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u57df\u7ea6\u675f\u6765\u7a33\u5b9a\u51e0\u4f55\u5e76\u4fdd\u7559\u7ec6\u8282\uff1b\u540c\u65f6\u6784\u5efa\u5305\u542b\u56db\u4e2a\u5149\u8c31\u6ce2\u6bb5\u7684\u591a\u5149\u8c31\u6e29\u5ba4\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5305\u5b9a\u4e49\u6807\u51c6\u5316\u5c11\u6837\u672c\u91cd\u5efa\u534f\u8bae\u3002", "result": "\u5728\u591a\u5149\u8c31\u6570\u636e\u96c6\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u80fd\u591f\u5b9e\u73b0\u66f4\u6e05\u6670\u3001\u66f4\u7a33\u5b9a\u4e14\u5149\u8c31\u4e00\u81f4\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u9891\u7387\u6b63\u5219\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863DGS\u5728\u7a00\u758f\u89c6\u89d2\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u53d1\u5e03\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u5305\u4e3a\u5c11\u6837\u672c3D\u91cd\u5efa\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2601.17417", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.17417", "abs": "https://arxiv.org/abs/2601.17417", "authors": ["Artemis Deligianni", "Zachary Horne", "Leonidas A. A. Doumas"], "title": "Using psychological theory to ground guidelines for the annotation of misogynistic language", "comment": null, "summary": "Detecting misogynistic hate speech is a difficult algorithmic task. The task is made more difficult when decision criteria for what constitutes misogynistic speech are ungrounded in established literatures in psychology and philosophy, both of which have described in great detail the forms explicit and subtle misogynistic attitudes can take. In particular, the literature on algorithmic detection of misogynistic speech often rely on guidelines that are insufficiently robust or inappropriately justified -- they often fail to include various misogynistic phenomena or misrepresent their importance when they do. As a result, current misogyny detection coding schemes and datasets fail to capture the ways women experience misogyny online. This is of pressing importance: misogyny is on the rise both online and offline. Thus, the scientific community needs to have a systematic, theory informed coding scheme of misogyny detection and a corresponding dataset to train and test models of misogyny detection. To this end, we developed (1) a misogyny annotation guideline scheme informed by theoretical and empirical psychological research, (2) annotated a new dataset achieving substantial inter-rater agreement (kappa = 0.68) and (3) present a case study using Large Language Models (LLMs) to compare our coding scheme to a self-described \"expert\" misogyny annotation scheme in the literature. Our findings indicate that our guideline scheme surpasses the other coding scheme in the classification of misogynistic texts across 3 datasets. Additionally, we find that LLMs struggle to replicate our human annotator labels, attributable in large part to how LLMs reflect mainstream views of misogyny. We discuss implications for the use of LLMs for the purposes of misogyny detection.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u73b0\u6709\u7b97\u6cd5\u68c0\u6d4b\u538c\u5973\u8a00\u8bba\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u57fa\u4e8e\u5fc3\u7406\u5b66\u7406\u8bba\u7684\u65b0\u6807\u6ce8\u65b9\u6848\u548c\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7LLM\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6848", "motivation": "\u5f53\u524d\u538c\u5973\u8a00\u8bba\u68c0\u6d4b\u7b97\u6cd5\u5b58\u5728\u7406\u8bba\u57fa\u7840\u8584\u5f31\u7684\u95ee\u9898\uff0c\u6807\u6ce8\u6307\u5357\u7f3a\u4e4f\u5fc3\u7406\u5b66\u548c\u54f2\u5b66\u6587\u732e\u652f\u6301\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5973\u6027\u5728\u7ebf\u7ecf\u5386\u7684\u771f\u5b9e\u538c\u5973\u73b0\u8c61\uff0c\u800c\u7ebf\u4e0a\u7ebf\u4e0b\u538c\u5973\u73b0\u8c61\u90fd\u5728\u589e\u52a0\uff0c\u8feb\u5207\u9700\u8981\u7406\u8bba\u6307\u5bfc\u7684\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848", "method": "1) \u57fa\u4e8e\u7406\u8bba\u548c\u5b9e\u8bc1\u5fc3\u7406\u5b66\u7814\u7a76\u5f00\u53d1\u538c\u5973\u6807\u6ce8\u6307\u5357\u65b9\u6848\uff1b2) \u6807\u6ce8\u65b0\u6570\u636e\u96c6\u5e76\u83b7\u5f97\u8f83\u9ad8\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027(kappa=0.68)\uff1b3) \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u6bd4\u8f83\u65b0\u65b9\u6848\u4e0e\u73b0\u6709\"\u4e13\u5bb6\"\u6807\u6ce8\u65b9\u6848", "result": "\u65b0\u6807\u6ce8\u6307\u5357\u65b9\u6848\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u538c\u5973\u6587\u672c\u5206\u7c7b\u8868\u73b0\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff1b\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u590d\u5236\u4eba\u7c7b\u6807\u6ce8\u8005\u7684\u6807\u7b7e\uff0c\u4e3b\u8981\u56e0\u4e3aLLM\u53cd\u6620\u4e86\u4e3b\u6d41\u538c\u5973\u89c2\u70b9", "conclusion": "\u9700\u8981\u57fa\u4e8e\u7406\u8bba\u7814\u7a76\u7684\u7cfb\u7edf\u5316\u538c\u5973\u68c0\u6d4b\u7f16\u7801\u65b9\u6848\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u538c\u5973\u68c0\u6d4b\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53cd\u6620\u4e86\u4e3b\u6d41\u793e\u4f1a\u504f\u89c1"}}
{"id": "2601.17180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17180", "abs": "https://arxiv.org/abs/2601.17180", "authors": ["In\u00e9s Gonzalez-Pepe", "Vinuyan Sivakolunthu", "Jacob Fortin", "Yohan Chatelain", "Tristan Glatard"], "title": "Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging", "comment": null, "summary": "Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6570\u503c\u4e0d\u786e\u5b9a\u6027\u7684CNN\u4f18\u5316\u65b9\u6cd5\uff1aConservative & Aggressive NaNs\uff0c\u901a\u8fc7\u8bc6\u522b\u6570\u503c\u4e0d\u7a33\u5b9a\u4f53\u7d20\u5e76\u66ff\u6362\u4e3aNaN\uff0c\u8ba9\u540e\u7eed\u5c42\u8df3\u8fc7\u5bf9\u65e0\u5173\u6570\u636e\u7684\u8ba1\u7b97\uff0c\u5728\u795e\u7ecf\u5f71\u50cf\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5e73\u57471.67\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u795e\u7ecf\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u65e5\u76ca\u589e\u5927\uff0c\u6548\u7387\u6210\u4e3a\u6301\u7eed\u5173\u6ce8\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0CNN\u4e2d\u8bb8\u591a\u64cd\u4f5c\u5e94\u7528\u4e8e\u6570\u503c\u566a\u58f0\u4e3b\u5bfc\u7684\u503c\uff0c\u5bf9\u6a21\u578b\u8f93\u51fa\u5f71\u54cd\u53ef\u5ffd\u7565\uff0c\u67d0\u4e9b\u6a21\u578b\u4e2d\u9ad8\u8fbe\u4e09\u5206\u4e4b\u4e8c\u7684\u5377\u79ef\u64cd\u4f5c\u4f3c\u4e4e\u662f\u5197\u4f59\u7684\u3002", "method": "\u63d0\u51faConservative & Aggressive NaNs\u4e24\u79cd\u65b9\u6cd5\uff1a\u65b0\u9896\u7684\u6700\u5927\u6c60\u5316\u548c\u53cd\u6c60\u5316\u53d8\u4f53\uff0c\u8bc6\u522b\u6570\u503c\u4e0d\u7a33\u5b9a\u4f53\u7d20\u5e76\u7528NaN\u66ff\u6362\uff0c\u4f7f\u540e\u7eed\u5c42\u80fd\u591f\u8df3\u8fc7\u5bf9\u65e0\u5173\u6570\u636e\u7684\u8ba1\u7b97\u3002\u4e24\u79cd\u65b9\u6cd5\u5728PyTorch\u4e2d\u5b9e\u73b0\uff0c\u65e0\u9700\u67b6\u6784\u66f4\u6539\u3002", "result": "\u5728\u5305\u542b\u81f3\u5c1150% NaN\u7684\u8f93\u5165\u4e2d\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u8fd0\u884c\u65f6\u6539\u8fdb\uff1b\u5728\u8d85\u8fc7\u4e09\u5206\u4e4b\u4e8cNaN\u7684\u6570\u636e\u4e2d\uff08\u5e38\u89c1\u4e8e\u795e\u7ecf\u5f71\u50cf\u573a\u666f\uff09\u5b9e\u73b0\u5e73\u57471.67\u500d\u63a8\u7406\u52a0\u901f\u3002Conservative NaNs\u5e73\u5747\u51cf\u5c1130%\u5377\u79ef\u64cd\u4f5c\uff0c\u65e0\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u5b9a\u5c42\u53ef\u8df3\u8fc764.64%\u5377\u79ef\uff1bAggressive NaNs\u53ef\u8df3\u8fc769.30%\u5377\u79ef\u4f46\u53ef\u80fd\u5076\u5c14\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u6570\u503c\u4e0d\u786e\u5b9a\u6027\u53ef\u88ab\u5229\u7528\u6765\u51cf\u5c11CNN\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u5e76\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u4e24\u79cd\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u9014\u5f84\u3002"}}
{"id": "2601.17194", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17194", "abs": "https://arxiv.org/abs/2601.17194", "authors": ["Cheyu Lin", "Katherine A. Flanigan", "Sirajum Munir"], "title": "Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments", "comment": null, "summary": "Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize \"interaction\" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86DUET\u6570\u636e\u96c6\u548c\u8fd0\u52a8\u5b66\u8bc6\u522b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u6d4b\u91cf\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u586b\u8865\u4e86\u5efa\u7b51\u73af\u5883\u7814\u7a76\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u793e\u4ea4\u4e92\u52a8\u6d4b\u91cf\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u5efa\u7b51\u73af\u5883\u7814\u7a76\u4e2d\u7f3a\u4e4f\u4e00\u81f4\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u65b9\u6cd5\u6765\u6d4b\u91cf\u793e\u4ea4\u4e92\u52a8\uff0c\u5bfc\u81f4\u4e0d\u540c\u7814\u7a76\u5bf9\"\u4e92\u52a8\"\u7684\u64cd\u4f5c\u5b9a\u4e49\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u4ece\u4e1a\u8005\u8bc4\u4f30\u8bbe\u8ba1\u5e72\u9884\u662f\u5426\u771f\u6b63\u5f71\u54cd\u793e\u4f1a\u8d44\u672c\u7406\u8bba\u9884\u6d4b\u7684\u91cd\u8981\u4e92\u52a8\u5f62\u5f0f\u3002", "method": "\u5f15\u5165DUET\u6570\u636e\u96c6\uff0c\u57fa\u4e8eEkman\u548cFriesen\u7684\u8fd0\u52a8\u5b66\u5206\u7c7b\u6cd5\uff0c\u6355\u634912\u79cd\u4e8c\u5143\u4e92\u52a8\uff0c\u6db5\u76d65\u79cd\u8fd0\u52a8\u529f\u80fd\uff08\u8c61\u5f81\u3001\u8bf4\u660e\u3001\u60c5\u611f\u8868\u8fbe\u3001\u9002\u5e94\u3001\u8c03\u8282\uff09\uff0c\u4f7f\u75284\u79cd\u4f20\u611f\u6a21\u6001\u548c3\u79cd\u5efa\u7b51\u73af\u5883\u573a\u666f\u3002\u5f00\u53d1\u4e86\u76f4\u63a5\u4ece\u9690\u79c1\u4fdd\u62a4\u7684\u9aa8\u9abc\u8fd0\u52a8\u4e2d\u63a8\u65ad\u4ea4\u6d41\u529f\u80fd\u7684\u8bc6\u522b\u6846\u67b6\uff0c\u65e0\u9700\u624b\u5de5\u5236\u4f5c\u7684\u52a8\u4f5c-\u529f\u80fd\u8bcd\u5178\u3002", "result": "\u5bf96\u4e2a\u5f00\u6e90\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cf\u5316\u4e86\u5728DUET\u4e0a\u8fdb\u884c\u4ea4\u6d41\u529f\u80fd\u8bc6\u522b\u7684\u96be\u5ea6\uff0c\u63ed\u793a\u4e86\u666e\u904d\u7684\u5355\u4f53\u52a8\u4f5c\u7ea7\u8bc6\u522b\u5728\u6269\u5c55\u5230\u4e8c\u5143\u793e\u4ea4\u4e92\u52a8\u6d4b\u91cf\u65f6\u7684\u5c40\u9650\u6027\u3002\u8bc6\u522b\u6846\u67b6\u663e\u793a\u4e86\u8fd0\u52a8\u529f\u80fd\u7684\u7ed3\u6784\u5316\u805a\u7c7b\uff0c\u4ee5\u53ca\u8868\u793a\u8d28\u91cf\u4e0e\u5206\u7c7b\u6027\u80fd\u4e4b\u95f4\u7684\u5f3a\u5173\u8054\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u4e3b\u4f53\u548c\u73af\u5883\u4e2d\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DUET\u6570\u636e\u96c6\u548c\u8fd0\u52a8\u5b66\u8bc6\u522b\u6846\u67b6\u4e3a\u5efa\u7b51\u73af\u5883\u7814\u7a76\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u793e\u4ea4\u4e92\u52a8\u6d4b\u91cf\u65b9\u6cd5\uff0c\u80fd\u591f\u8bc4\u4f30\u8bbe\u8ba1\u5e72\u9884\u5bf9\u793e\u4f1a\u8d44\u672c\u76f8\u5173\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u65b9\u6cd5\u5b66\u7a7a\u767d\u3002"}}
{"id": "2601.17431", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2601.17431", "abs": "https://arxiv.org/abs/2601.17431", "authors": ["H. Kemal \u0130lter"], "title": "The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers", "comment": null, "summary": "The adoption of Large Language Models (LLMs) in scientific writing promises efficiency but risks introducing informational entropy. While \"hallucinated papers\" are a known artifact, the systematic degradation of valid citation chains remains unquantified. We conducted a forensic audit of 50 recent survey papers in Artificial Intelligence (N=5,514 citations) published between September 2024 and January 2026. We utilized a hybrid verification pipeline combining DOI resolution, Crossref metadata analysis, Semantic Scholar queries, and fuzzy text matching to distinguish between formatting errors (\"Sloppiness\") and verifiable non-existence (\"Phantoms). We detect a persistent 17.0% Phantom Rate -- citations that cannot be resolved to any digital object despite aggressive forensic recovery. Diagnostic categorization reveals three distinct failure modes: pure hallucinations (5.1%), hallucinated identifiers with valid titles (16.4%), and parsing-induced matching failures (78.5%). Longitudinal analysis reveals a flat trend (+0.07 pp/month), suggesting that high-entropy citation practices have stabilized as an endemic feature of the field. The scientific citation graph in AI survey literature exhibits \"link rot\" at scale. This suggests a mechanism where AI tools act as \"lazy research assistants,\" retrieving correct titles but hallucinating metadata, thereby severing the digital chain of custody required for reproducible science.", "AI": {"tldr": "\u5bf950\u7bc7AI\u9886\u57df\u7efc\u8ff0\u8bba\u6587\uff085514\u6761\u5f15\u7528\uff09\u7684\u6cd5\u8bc1\u5ba1\u8ba1\u663e\u793a\uff0c17%\u7684\u5f15\u7528\u662f\"\u5e7d\u7075\u5f15\u7528\"\u2014\u2014\u65e0\u6cd5\u89e3\u6790\u5230\u4efb\u4f55\u6570\u5b57\u5bf9\u8c61\uff0c\u8868\u660eAI\u5de5\u5177\u5728\u79d1\u5b66\u5199\u4f5c\u4e2d\u5f15\u5165\u4e86\u7cfb\u7edf\u6027\u5f15\u7528\u9000\u5316\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u5199\u4f5c\u4e2d\u7684\u5e94\u7528\u867d\u7136\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u4fe1\u606f\u71b5\u3002\u867d\u7136\"\u5e7b\u89c9\u8bba\u6587\"\u662f\u5df2\u77e5\u95ee\u9898\uff0c\u4f46\u6709\u6548\u5f15\u7528\u94fe\u7684\u7cfb\u7edf\u6027\u9000\u5316\u5c1a\u672a\u88ab\u91cf\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cf\u5316AI\u9886\u57df\u7efc\u8ff0\u6587\u732e\u4e2d\u5f15\u7528\u94fe\u7684\u9000\u5316\u7a0b\u5ea6\u3002", "method": "\u5bf92024\u5e749\u6708\u81f32026\u5e741\u6708\u671f\u95f4\u53d1\u8868\u768450\u7bc7AI\u9886\u57df\u7efc\u8ff0\u8bba\u6587\uff08\u51715514\u6761\u5f15\u7528\uff09\u8fdb\u884c\u6cd5\u8bc1\u5ba1\u8ba1\u3002\u91c7\u7528\u6df7\u5408\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u7ed3\u5408DOI\u89e3\u6790\u3001Crossref\u5143\u6570\u636e\u5206\u6790\u3001Semantic Scholar\u67e5\u8be2\u548c\u6a21\u7cca\u6587\u672c\u5339\u914d\uff0c\u533a\u5206\u683c\u5f0f\u9519\u8bef\uff08\"\u8349\u7387\"\uff09\u548c\u53ef\u9a8c\u8bc1\u7684\u4e0d\u5b58\u5728\u5f15\u7528\uff08\"\u5e7d\u7075\"\uff09\u3002", "result": "\u53d1\u73b017.0%\u7684\u5e7d\u7075\u5f15\u7528\u7387\u2014\u2014\u8fd9\u4e9b\u5f15\u7528\u65e0\u6cd5\u89e3\u6790\u5230\u4efb\u4f55\u6570\u5b57\u5bf9\u8c61\u3002\u8bca\u65ad\u5206\u7c7b\u663e\u793a\u4e09\u79cd\u5931\u8d25\u6a21\u5f0f\uff1a\u7eaf\u7cb9\u5e7b\u89c9\uff085.1%\uff09\u3001\u6807\u9898\u6709\u6548\u4f46\u6807\u8bc6\u7b26\u5e7b\u89c9\uff0816.4%\uff09\u548c\u89e3\u6790\u5bfc\u81f4\u7684\u5339\u914d\u5931\u8d25\uff0878.5%\uff09\u3002\u7eb5\u5411\u5206\u6790\u663e\u793a\u8d8b\u52bf\u5e73\u7a33\uff08\u6bcf\u6708+0.07\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u8868\u660e\u9ad8\u71b5\u5f15\u7528\u5b9e\u8df5\u5df2\u6210\u4e3a\u8be5\u9886\u57df\u7684\u7a33\u5b9a\u7279\u5f81\u3002", "conclusion": "AI\u8c03\u67e5\u6587\u732e\u4e2d\u7684\u79d1\u5b66\u5f15\u7528\u56fe\u5728\u5927\u89c4\u6a21\u4e0a\u8868\u73b0\u51fa\"\u94fe\u63a5\u8150\u70c2\"\u3002\u8fd9\u8868\u660eAI\u5de5\u5177\u5145\u5f53\u4e86\"\u61d2\u60f0\u7684\u7814\u7a76\u52a9\u624b\"\uff0c\u68c0\u7d22\u6b63\u786e\u7684\u6807\u9898\u4f46\u5e7b\u89c9\u5143\u6570\u636e\uff0c\u4ece\u800c\u5207\u65ad\u4e86\u53ef\u91cd\u590d\u79d1\u5b66\u6240\u9700\u7684\u6570\u5b57\u4fdd\u7ba1\u94fe\u3002"}}
{"id": "2601.17183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17183", "abs": "https://arxiv.org/abs/2601.17183", "authors": ["Farzam Asad", "Junaid Saif Khan", "Maria Tariq", "Sundus Munir", "Muhammad Adnan Khan"], "title": "Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data", "comment": "27 pages, 7 figures, 4 tables", "summary": "Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u533b\u9662\u95f4\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86FedProx\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5fc3\u810f\u75c5\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6bd4\u96c6\u4e2d\u5f0f\u5b66\u4e60\u548c\u5b64\u7acb\u672c\u5730\u6a21\u578b\u66f4\u597d\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u533b\u7597\u6570\u636e\u56e0\u9690\u79c1\u6cd5\u89c4\uff08\u5982HIPAA\u548cGDPR\uff09\u65e0\u6cd5\u76f4\u63a5\u5171\u4eab\uff0c\u4f46\u533b\u9662\u95f4\u7684\u534f\u4f5c\u5bf9\u6539\u8fdb\u8bca\u65ad\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u96c6\u4e2d\u539f\u59cb\u6570\u636e\u7684\u534f\u4f5c\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4f46\u4e34\u5e8a\u6570\u636e\u56fa\u6709\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u6027\uff08\u7531\u4eba\u53e3\u5dee\u5f02\u3001\u75be\u75c5\u6d41\u884c\u5ea6\u548c\u673a\u6784\u5b9e\u8df5\u5dee\u5f02\u5f15\u8d77\uff09\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u4f7f\u7528UCI\u5fc3\u810f\u75c5\u6570\u636e\u96c6\uff08\u514b\u5229\u592b\u5170\u8bca\u6240\u7684303\u540d\u60a3\u8005\u6570\u636e\uff09\uff0c\u901a\u8fc7\u57fa\u4e8e\u4eba\u53e3\u7edf\u8ba1\u7684\u5206\u5c42\u65b9\u6cd5\u6a21\u62df\u56db\u4e2a\u5f02\u8d28\u6027\u533b\u9662\u5ba2\u6237\u7aef\uff0c\u751f\u6210\u73b0\u5b9e\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5206\u533a\u3002\u91c7\u7528\u8054\u90a6\u8fd1\u7aef\u4f18\u5316\uff08FedProx\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u7aef\u6b63\u5219\u5316\u63a7\u5236\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c\u5e76\u8fdb\u884c50\u6b21\u72ec\u7acb\u8fd0\u884c\u7684\u5e7f\u6cdb\u6d88\u878d\u7814\u7a76\u548c\u7edf\u8ba1\u9a8c\u8bc1\u3002", "result": "FedProx\u5728\u8fd1\u7aef\u53c2\u6570mu=0.05\u65f6\u8fbe\u523085.00%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u5b66\u4e60\uff0883.33%\uff09\u548c\u5b64\u7acb\u672c\u5730\u6a21\u578b\uff08\u5e73\u574778.45%\uff09\u3002\u7814\u7a76\u8bc1\u660e\u8fd1\u7aef\u6b63\u5219\u5316\u5728\u5f02\u8d28\u73af\u5883\u4e2d\u80fd\u6709\u6548\u6291\u5236\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "conclusion": "\u8fd9\u9879\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u4e3a\u73b0\u5b9e\u4e16\u754c\u8054\u90a6\u533b\u7597\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7b97\u6cd5\u89c1\u89e3\u548c\u5b9e\u9645\u90e8\u7f72\u6307\u5357\uff0c\u7ed3\u679c\u53ef\u76f4\u63a5\u8f6c\u79fb\u7ed9\u533b\u9662IT\u7ba1\u7406\u5458\uff0c\u7528\u4e8e\u5b9e\u65bd\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u5b66\u4e60\u3002FedProx\u5728\u5904\u7406\u533b\u7597\u6570\u636e\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u533b\u7597\u9886\u57df\u7684\u8054\u90a6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18457", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18457", "abs": "https://arxiv.org/abs/2601.18457", "authors": ["Fake Lin", "Binbin Hu", "Zhi Zheng", "Xi Zhu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou", "Tong Xu"], "title": "Token-level Collaborative Alignment for LLM-based Generative Recommendation", "comment": "11 pages, 2 figures, 7 tables, WWW 2026", "summary": "Large Language Models (LLMs) have demonstrated strong potential for generative recommendation by leveraging rich semantic knowledge. However, existing LLM-based recommender systems struggle to effectively incorporate collaborative filtering (CF) signals, due to a fundamental mismatch between item-level preference modeling in CF and token-level next-token prediction (NTP) optimization in LLMs. Prior approaches typically treat CF as contextual hints or representation bias, and resort to multi-stage training to reduce behavioral semantic space discrepancies, leaving CF unable to explicitly regulate LLM generation. In this work, we propose Token-level Collaborative Alignment for Recommendation (TCA4Rec), a model-agnostic and plug-and-play framework that establishes an explicit optimization-level interface between CF supervision and LLM generation. TCA4Rec consists of (i) Collaborative Tokenizer, which projects raw item-level CF logits into token-level distributions aligned with the LLM token space, and (ii) Soft Label Alignment, which integrates these CF-informed distributions with one-hot supervision to optimize a soft NTP objective. This design preserves the generative nature of LLM training while enabling collaborative alignment with essential user preference of CF models. We highlight TCA4Rec is compatible with arbitrary traditional CF models and generalizes across a wide range of decoder-based LLM recommender architectures. Moreover, it provides an explicit mechanism to balance behavioral alignment and semantic fluency, yielding generative recommendations that are both accurate and controllable. Extensive experiments demonstrate that TCA4Rec consistently improves recommendation performance across a broad spectrum of CF models and LLM-based recommender systems.", "AI": {"tldr": "TCA4Rec\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u4ee4\u724c\u7ea7\u534f\u540c\u5bf9\u9f50\u89e3\u51b3LLM\u63a8\u8350\u7cfb\u7edf\u4e2d\u96be\u4ee5\u6709\u6548\u6574\u5408\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u96be\u4ee5\u6709\u6548\u6574\u5408\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\uff0c\u56e0\u4e3aCF\u57fa\u4e8e\u7269\u54c1\u7ea7\u504f\u597d\u5efa\u6a21\uff0c\u800cLLM\u57fa\u4e8e\u4ee4\u724c\u7ea7\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u4f18\u5316\uff0c\u4e24\u8005\u5b58\u5728\u6839\u672c\u6027\u4e0d\u5339\u914d\u3002\u5148\u524d\u65b9\u6cd5\u901a\u5e38\u5c06CF\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u63d0\u793a\u6216\u8868\u793a\u504f\u5dee\uff0c\u9700\u8981\u591a\u9636\u6bb5\u8bad\u7ec3\u6765\u51cf\u5c11\u884c\u4e3a\u8bed\u4e49\u7a7a\u95f4\u5dee\u5f02\uff0c\u5bfc\u81f4CF\u65e0\u6cd5\u663e\u5f0f\u8c03\u63a7LLM\u751f\u6210\u3002", "method": "\u63d0\u51faTCA4Rec\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u534f\u540c\u5206\u8bcd\u5668\uff0c\u5c06\u539f\u59cb\u7269\u54c1\u7ea7CF\u5bf9\u6570\u6982\u7387\u6295\u5f71\u5230\u4e0eLLM\u4ee4\u724c\u7a7a\u95f4\u5bf9\u9f50\u7684\u4ee4\u724c\u7ea7\u5206\u5e03\uff1b(2)\u8f6f\u6807\u7b7e\u5bf9\u9f50\uff0c\u5c06\u8fd9\u4e9bCF\u4fe1\u606f\u5206\u5e03\u4e0e\u72ec\u70ed\u76d1\u7763\u7ed3\u5408\uff0c\u4f18\u5316\u8f6fNTP\u76ee\u6807\u3002\u8be5\u8bbe\u8ba1\u4fdd\u7559\u4e86LLM\u8bad\u7ec3\u7684\u751f\u6210\u7279\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e0eCF\u6a21\u578b\u6838\u5fc3\u7528\u6237\u504f\u597d\u7684\u534f\u540c\u5bf9\u9f50\u3002", "result": "TCA4Rec\u4e0e\u4efb\u610f\u4f20\u7edfCF\u6a21\u578b\u517c\u5bb9\uff0c\u53ef\u6cdb\u5316\u5230\u5e7f\u6cdb\u7684\u57fa\u4e8e\u89e3\u7801\u5668\u7684LLM\u63a8\u8350\u67b6\u6784\u3002\u5b83\u63d0\u4f9b\u4e86\u663e\u5f0f\u673a\u5236\u6765\u5e73\u8861\u884c\u4e3a\u5bf9\u9f50\u548c\u8bed\u4e49\u6d41\u7545\u6027\uff0c\u751f\u6210\u65e2\u51c6\u786e\u53c8\u53ef\u63a7\u5236\u7684\u63a8\u8350\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTCA4Rec\u5728\u5404\u79cdCF\u6a21\u578b\u548c\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u4e2d\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "TCA4Rec\u901a\u8fc7\u5efa\u7acbCF\u76d1\u7763\u4e0eLLM\u751f\u6210\u4e4b\u95f4\u7684\u663e\u5f0f\u4f18\u5316\u7ea7\u63a5\u53e3\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u8350\u7cfb\u7edf\u4e2d\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\u6574\u5408\u7684\u96be\u9898\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u884c\u4e3a\u5bf9\u9f50\u4e0e\u8bed\u4e49\u6d41\u7545\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2601.17211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17211", "abs": "https://arxiv.org/abs/2601.17211", "authors": ["Anzhe Cheng", "Italo Ivo Lima Dias Pinto", "Paul Bogdan"], "title": "Structural Complexity of Brain MRI reveals age-associated patterns", "comment": "accepted by icassp2026", "summary": "We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u7ed3\u6784\u590d\u6742\u5ea6\u5206\u6790\u5e94\u7528\u4e8e\u4e09\u7ef4\u8111MRI\u4fe1\u53f7\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7c97\u7c92\u5316\u65b9\u6cd5\u91cf\u5316\u5206\u8fa8\u7387\u53d8\u5316\u4e2d\u7684\u4fe1\u606f\u635f\u5931\uff0c\u53d1\u73b0\u7ed3\u6784\u590d\u6742\u5ea6\u968f\u5e74\u9f84\u7cfb\u7edf\u6027\u4e0b\u964d\uff0c\u4e14\u7c97\u5c3a\u5ea6\u6548\u5e94\u6700\u663e\u8457\u3002", "motivation": "\u5c06\u7ed3\u6784\u590d\u6742\u5ea6\u5206\u6790\u6846\u67b6\u6269\u5c55\u5230\u4e09\u7ef4\u4fe1\u53f7\uff0c\u7279\u522b\u662f\u8111\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\uff0c\u4ee5\u6355\u6349\u4f53\u79ef\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u7ec4\u7ec7\u7279\u5f81\uff0c\u6539\u8fdb\u4f20\u7edf\u65b9\u6cd5\u5728\u7c97\u5206\u8fa8\u7387\u4e0b\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6ed1\u52a8\u7a97\u53e3\u7c97\u7c92\u5316\u65b9\u6848\uff0c\u5728\u6e10\u8fdb\u589e\u5927\u7684\u7a7a\u95f4\u5c3a\u5ea6\u4e0a\u5bf9\u4fe1\u53f7\u8fdb\u884c\u7c97\u7c92\u5316\u5904\u7406\uff0c\u91cf\u5316\u8fde\u7eed\u5206\u8fa8\u7387\u4e4b\u95f4\u7684\u4fe1\u606f\u635f\u5931\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u5757\u7684\u65b9\u6cd5\u63d0\u4f9b\u66f4\u5e73\u6ed1\u7684\u4f30\u8ba1\u548c\u66f4\u597d\u7684\u5927\u5c3a\u5ea6\u9c81\u68d2\u6027\u3002", "result": "\u5206\u6790\u8986\u76d6\u4e2d\u665a\u671f\u6210\u5e74\u671f\u7684\u5927\u578b\u7ed3\u6784MRI\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u7ed3\u6784\u590d\u6742\u5ea6\u968f\u5e74\u9f84\u7cfb\u7edf\u6027\u4e0b\u964d\uff0c\u4e14\u6700\u663e\u8457\u7684\u5f71\u54cd\u51fa\u73b0\u5728\u8f83\u7c97\u7684\u5c3a\u5ea6\u4e0a\u3002", "conclusion": "\u7ed3\u6784\u590d\u6742\u5ea6\u662f\u4e09\u7ef4\u6210\u50cf\u6570\u636e\u591a\u5c3a\u5ea6\u5206\u6790\u7684\u53ef\u9760\u4fe1\u53f7\u5904\u7406\u5de5\u5177\uff0c\u540c\u65f6\u8bc1\u660e\u4e86\u5176\u5728\u4ece\u8111MRI\u9884\u6d4b\u751f\u7269\u5e74\u9f84\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.17814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17814", "abs": "https://arxiv.org/abs/2601.17814", "authors": ["Haoxuan Ma", "Guannan Lai", "Han-Jia Ye"], "title": "MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.", "AI": {"tldr": "MMR-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8def\u7531\u9009\u62e9\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u548c\u5019\u9009\u6a21\u578b\u96c6\uff0c\u7814\u7a76\u5982\u4f55\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u667a\u80fd\u9009\u62e9\u6700\u9002\u5408\u7684\u6a21\u578b\uff0c\u4ee5\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u67b6\u6784\u3001\u5bf9\u9f50\u7b56\u7565\u548c\u6548\u7387\u65b9\u9762\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u6700\u4f18\u3002\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u5de5\u4f5c\u8d1f\u8f7d\u4ece\u8f7b\u91cf\u7ea7OCR\u5230\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u4e0d\u7b49\uff0c\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u8981\u4e48\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u8fc7\u5ea6\u8ba1\u7b97\uff0c\u8981\u4e48\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u727a\u7272\u51c6\u786e\u6027\u3002\u9700\u8981\u89e3\u51b3\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8def\u7531\u9009\u62e9\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86MMR-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u4f9b\uff1a1\uff09\u5177\u6709\u6a21\u6001\u611f\u77e5\u8f93\u5165\u548c\u53ef\u53d8\u8ba1\u7b97\u9884\u7b97\u7684\u53d7\u63a7\u73af\u5883\uff1b2\uff09\u6db5\u76d6OCR\u3001\u901a\u7528\u89c6\u89c9\u95ee\u7b54\u548c\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u7684\u5e7f\u6cdb\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u5957\u4ef6\uff1b3\uff09\u5f3a\u5927\u7684\u5355\u6a21\u578b\u53c2\u8003\u3001\u7406\u8bba\u4e0a\u9650\u548c\u4ee3\u8868\u6027\u8def\u7531\u7b56\u7565\u3002\u901a\u8fc7\u8be5\u57fa\u51c6\u6d4b\u8bd5\u7814\u7a76\u591a\u6a21\u6001\u4fe1\u53f7\u5982\u4f55\u6539\u5584\u8def\u7531\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u878d\u5165\u591a\u6a21\u6001\u4fe1\u53f7\u80fd\u63d0\u5347\u8def\u7531\u8d28\u91cf\uff0c\u6539\u5584\u6210\u672c-\u51c6\u786e\u6027\u8fb9\u754c\u3002\u8def\u7531\u7cfb\u7edf\u80fd\u4ee5\u6700\u5f3a\u5355\u6a21\u578b\u7ea633%\u7684\u6210\u672c\u8d85\u8d8a\u5176\u51c6\u786e\u6027\u3002\u5728\u90e8\u5206\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u6570\u636e\u96c6\u548c\u7eaf\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e0\u9700\u91cd\u65b0\u8c03\u6574\u3002", "conclusion": "MMR-Bench\u4e3a\u7814\u7a76\u81ea\u9002\u5e94\u591a\u6a21\u6001\u6a21\u578b\u9009\u62e9\u548c\u9ad8\u6548MLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u8def\u7531\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6f5c\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.17189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17189", "abs": "https://arxiv.org/abs/2601.17189", "authors": ["Sabrina Mokhtari", "Sara Kodeiri", "Shubhankar Mohapatra", "Florian Tramer", "Gautam Kamath"], "title": "Rethinking Benchmarks for Differentially Private Image Classification", "comment": null, "summary": "We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u5dee\u5206\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u57fa\u51c6\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5e76\u521b\u5efa\u4e86\u516c\u5f00\u7684\u6392\u884c\u699c\u4f9b\u793e\u533a\u8ddf\u8e2a\u8fdb\u5c55\u3002", "motivation": "\u5f53\u524d\u5dee\u5206\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u591f\u5168\u9762\uff0c\u9700\u8981\u4e00\u5957\u66f4\u5b8c\u5584\u7684\u57fa\u51c6\u96c6\u6765\u8bc4\u4f30\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u6280\u672f\u6548\u679c\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e00\u5957\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6db5\u76d6\u591a\u79cd\u8bbe\u7f6e\uff08\u6709\u65e0\u989d\u5916\u6570\u636e\u3001\u51f8\u4f18\u5316\u8bbe\u7f6e\u3001\u4e0d\u540c\u6570\u636e\u96c6\uff09\uff0c\u5e76\u5bf9\u73b0\u6709\u6280\u672f\u5728\u8fd9\u4e9b\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u540c\u65f6\u521b\u5efa\u516c\u5f00\u7684\u6392\u884c\u699c\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u5dee\u5206\u9690\u79c1\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6d4b\u8bd5\u4e86\u73b0\u6709\u6280\u672f\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u7684\u6392\u884c\u699c\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6280\u672f\u53d1\u5c55\u548c\u8fdb\u6b65\u8ddf\u8e2a\u3002"}}
{"id": "2601.18570", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18570", "abs": "https://arxiv.org/abs/2601.18570", "authors": ["Mingzhe Han", "Jiahao Liu", "Dongsheng Li", "Hansu Gu", "Peng Zhang", "Ning Gu", "Tun Lu"], "title": "Feature-Indexed Federated Recommendation with Residual-Quantized Codebooks", "comment": null, "summary": "Federated recommendation provides a privacy-preserving solution for training recommender systems without centralizing user interactions. However, existing methods follow an ID-indexed communication paradigm that transmit whole item embeddings between clients and the server, which has three major limitations: 1) consumes uncontrollable communication resources, 2) the uploaded item information cannot generalize to related non-interacted items, and 3) is sensitive to client noisy feedback. To solve these problems, it is necessary to fundamentally change the existing ID-indexed communication paradigm. Therefore, we propose a feature-indexed communication paradigm that transmits feature code embeddings as codebooks rather than raw item embeddings. Building on this paradigm, we present RQFedRec, which assigns each item a list of discrete code IDs via Residual Quantization (RQ)-Kmeans. Each client generates and trains code embeddings as codebooks based on discrete code IDs provided by the server, and the server collects and aggregates these codebooks rather than item embeddings. This design makes communication controllable since the codebooks could cover all items, enabling updates to propagate across related items in same code ID. In addition, since code embedding represents many items, which is more robust to a single noisy item. To jointly capture semantic and collaborative information, RQFedRec further adopts a collaborative-semantic dual-channel aggregation with a curriculum strategy that emphasizes semantic codes early and gradually increases the contribution of collaborative codes over training. Extensive experiments on real-world datasets demonstrate that RQFedRec consistently outperforms state-of-the-art federated recommendation baselines while significantly reducing communication overhead.", "AI": {"tldr": "RQFedRec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7d22\u5f15\u901a\u4fe1\u8303\u5f0f\u7684\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u6b8b\u5dee\u91cf\u5316\u548c\u53cc\u901a\u9053\u805a\u5408\u89e3\u51b3\u4f20\u7edfID\u7d22\u5f15\u901a\u4fe1\u7684\u901a\u4fe1\u5f00\u9500\u5927\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u5bf9\u566a\u58f0\u654f\u611f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u63a8\u8350\u65b9\u6cd5\u91c7\u7528ID\u7d22\u5f15\u901a\u4fe1\u8303\u5f0f\uff0c\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u901a\u4fe1\u8d44\u6e90\u6d88\u8017\u4e0d\u53ef\u63a7\uff1b2) \u4e0a\u4f20\u7684\u9879\u76ee\u4fe1\u606f\u65e0\u6cd5\u6cdb\u5316\u5230\u76f8\u5173\u672a\u4ea4\u4e92\u9879\u76ee\uff1b3) \u5bf9\u5ba2\u6237\u7aef\u566a\u58f0\u53cd\u9988\u654f\u611f\u3002\u9700\u8981\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u73b0\u6709\u7684\u901a\u4fe1\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u7279\u5f81\u7d22\u5f15\u901a\u4fe1\u8303\u5f0f\uff0c\u4f20\u8f93\u7279\u5f81\u7801\u5d4c\u5165\u4f5c\u4e3a\u7801\u672c\u800c\u975e\u539f\u59cb\u9879\u76ee\u5d4c\u5165\u3002\u4f7f\u7528\u6b8b\u5dee\u91cf\u5316(RQ)-Kmeans\u4e3a\u6bcf\u4e2a\u9879\u76ee\u5206\u914d\u79bb\u6563\u7801ID\u5217\u8868\uff0c\u5ba2\u6237\u7aef\u57fa\u4e8e\u670d\u52a1\u5668\u63d0\u4f9b\u7684\u7801ID\u751f\u6210\u548c\u8bad\u7ec3\u7801\u5d4c\u5165\u4f5c\u4e3a\u7801\u672c\uff0c\u670d\u52a1\u5668\u805a\u5408\u7801\u672c\u800c\u975e\u9879\u76ee\u5d4c\u5165\u3002\u91c7\u7528\u534f\u4f5c-\u8bed\u4e49\u53cc\u901a\u9053\u805a\u5408\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u65e9\u671f\u5f3a\u8c03\u8bed\u4e49\u7801\uff0c\u9010\u6b65\u589e\u52a0\u534f\u4f5c\u7801\u7684\u8d21\u732e\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRQFedRec\u5728\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u63a8\u8350\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RQFedRec\u901a\u8fc7\u7279\u5f81\u7d22\u5f15\u901a\u4fe1\u8303\u5f0f\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u901a\u4fe1\u3001\u8de8\u9879\u76ee\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17216", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17216", "abs": "https://arxiv.org/abs/2601.17216", "authors": ["Murat Arda Onsu", "Poonam Lohan", "Burak Kantarci", "Aisha Syed", "Matthew Andrews", "Sean Kennedy"], "title": "Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction", "comment": "6 pages 5 figures, accepted to IEEE ICC 2026", "summary": "Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49V2X\u6846\u67b6\u7684\u5b9e\u65f6\u78b0\u649e\u9884\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528V-JEPA\u751f\u6210\u65f6\u7a7a\u8bed\u4e49\u5d4c\u5165\u66ff\u4ee3\u539f\u59cb\u89c6\u9891\u4f20\u8f93\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u540c\u65f6\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u9700\u8981\u5b9e\u65f6\u78b0\u649e\u9884\u6d4b\u4ee5\u786e\u4fdd\u9053\u8def\u5b89\u5168\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f20\u8f93\u539f\u59cb\u89c6\u9891\u6216\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u5728\u8f66\u8054\u7f51\u901a\u4fe1\u5e26\u5bbd\u548c\u5ef6\u8fdf\u9650\u5236\u4e0b\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51fa\u8bed\u4e49V2X\u6846\u67b6\uff1a\u8def\u4fa7\u5355\u5143\u6444\u50cf\u5934\u4f7f\u7528V-JEPA\u751f\u6210\u672a\u6765\u5e27\u7684\u65f6\u7a7a\u8bed\u4e49\u5d4c\u5165\uff0c\u901a\u8fc7V2X\u94fe\u8def\u4f20\u8f93\u5230\u8f66\u8f86\uff0c\u8f66\u8f86\u7aef\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u63a2\u9488\u548c\u5206\u7c7b\u5668\u89e3\u7801\u9884\u6d4b\u78b0\u649e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u78b0\u649e\u9884\u6d4b\u7684F1\u5206\u6570\u4e0a\u63d0\u534710%\uff0c\u540c\u65f6\u4f20\u8f93\u9700\u6c42\u76f8\u6bd4\u539f\u59cb\u89c6\u9891\u964d\u4f4e\u56db\u4e2a\u6570\u91cf\u7ea7\uff0810000\u500d\uff09\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u8bed\u4e49V2X\u901a\u4fe1\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u534f\u4f5c\u5f0f\u5b9e\u65f6\u78b0\u649e\u9884\u6d4b\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u4f20\u8f93\u8bed\u4e49\u5d4c\u5165\u800c\u975e\u539f\u59cb\u5e27\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2601.17540", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.17540", "abs": "https://arxiv.org/abs/2601.17540", "authors": ["Javed I. Khan", "Sharmila Rahman Prithula"], "title": "Ethical Risk Assessment of the Data Harnessing Process of LLM supported on Consensus of Well-known Multi-Ethical Frameworks", "comment": null, "summary": "The rapid advancements in large language models (LLMs) have revolutionized natural language processing, unlocking unprecedented capabilities in communication, automation, and knowledge generation. However, the ethical implications of LLM development, particularly in data harnessing, remain a critical challenge. Despite widespread discussion about the ethical compliance of LLMs -- especially concerning their data harnessing processes, there remains a notable absence of concrete frameworks to systematically guide or measure the ethical risks involved. In this paper we discuss a potential pathway for building an Ethical Risk Scoring (ERS) system to quantitatively assess the ethical integrity of the data harnessing process for AI systems. This system is based on a set of assessment questions grounded in core ethical principles, which are, in turn, supported by commanding ethical theories. By integrating measurable scoring mechanisms, this approach aims to foster responsible LLM development, balancing technological innovation with ethical accountability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6784\u5efa\u4f26\u7406\u98ce\u9669\u8bc4\u5206\u7cfb\u7edf\uff0c\u7528\u4e8e\u91cf\u5316\u8bc4\u4f30AI\u7cfb\u7edf\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u7684\u4f26\u7406\u5b8c\u6574\u6027\uff0c\u4ee5\u4fc3\u8fdb\u8d1f\u8d23\u4efb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u4f26\u7406\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u91c7\u96c6\u65b9\u9762\u3002\u5c3d\u7ba1\u5bf9LLM\u4f26\u7406\u5408\u89c4\u6027\u6709\u5e7f\u6cdb\u8ba8\u8bba\uff0c\u4f46\u7f3a\u4e4f\u5177\u4f53\u6846\u67b6\u6765\u7cfb\u7edf\u6307\u5bfc\u6216\u8861\u91cf\u76f8\u5173\u4f26\u7406\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4f26\u7406\u98ce\u9669\u8bc4\u5206\u7cfb\u7edf\uff0c\u57fa\u4e8e\u6838\u5fc3\u4f26\u7406\u539f\u5219\u6784\u5efa\u8bc4\u4f30\u95ee\u9898\u96c6\uff0c\u8fd9\u4e9b\u95ee\u9898\u7531\u6743\u5a01\u4f26\u7406\u7406\u8bba\u652f\u6301\uff0c\u5e76\u6574\u5408\u53ef\u91cf\u5316\u7684\u8bc4\u5206\u673a\u5236\u3002", "result": "\u8bba\u6587\u8ba8\u8bba\u4e86\u4e00\u79cd\u6784\u5efa\u4f26\u7406\u98ce\u9669\u8bc4\u5206\u7cfb\u7edf\u7684\u6f5c\u5728\u8def\u5f84\uff0c\u65e8\u5728\u4e3aAI\u7cfb\u7edf\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u7684\u4f26\u7406\u5b8c\u6574\u6027\u63d0\u4f9b\u5b9a\u91cf\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u901a\u8fc7\u5efa\u7acb\u53ef\u91cf\u5316\u7684\u4f26\u7406\u98ce\u9669\u8bc4\u5206\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5728\u6280\u672f\u521b\u65b0\u4e0e\u4f26\u7406\u8d23\u4efb\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4fc3\u8fdb\u8d1f\u8d23\u4efb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2601.17826", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17826", "abs": "https://arxiv.org/abs/2601.17826", "authors": ["Siyuan Yang", "Xihan Bian", "Jiayin Tang"], "title": "RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance", "comment": null, "summary": "The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.", "AI": {"tldr": "RegGuard\u662f\u4e00\u4e2a\u5de5\u4e1a\u7ea7AI\u52a9\u624b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u89e3\u8bfb\u5f02\u6784\u76d1\u7ba1\u6587\u672c\u5e76\u4e0e\u4f01\u4e1a\u5185\u90e8\u653f\u7b56\u5bf9\u9f50\uff0c\u901a\u8fc7HiSACC\u548cReLACE\u6280\u672f\u63d0\u9ad8\u68c0\u7d22\u548c\u751f\u6210\u8d28\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u76d1\u7ba1\u66f4\u65b0\u9891\u7387\u548c\u590d\u6742\u6027\u589e\u52a0\u7ed9\u8de8\u56fd\u5236\u836f\u516c\u53f8\u5e26\u6765\u6c89\u91cd\u8d1f\u62c5\uff0c\u5408\u89c4\u56e2\u961f\u9700\u8981\u624b\u52a8\u89e3\u8bfb\u4e0d\u540c\u53f8\u6cd5\u7ba1\u8f96\u533a\u3001\u683c\u5f0f\u548c\u673a\u6784\u7684\u89c4\u5219\uff0c\u6210\u672c\u9ad8\u4e14\u6613\u51fa\u9519\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u5b89\u5168\u7ba1\u9053\u6444\u5165\u5f02\u6784\u6587\u6863\u6e90\uff0c\u91c7\u7528\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aHiSACC\uff08\u5206\u5c42\u8bed\u4e49\u805a\u5408\u4e0a\u4e0b\u6587\u5206\u5757\uff09\u5c06\u957f\u6587\u6863\u8bed\u4e49\u5206\u5272\u4e3a\u8fde\u8d2f\u5355\u5143\uff1bReLACE\uff08\u76d1\u7ba1\u5217\u8868\u81ea\u9002\u5e94\u4ea4\u53c9\u7f16\u7801\u5668\uff09\u57fa\u4e8e\u5f00\u6e90\u6a21\u578b\u6784\u5efa\uff0c\u8054\u5408\u5efa\u6a21\u7528\u6237\u67e5\u8be2\u548c\u68c0\u7d22\u5019\u9009\u4ee5\u63d0\u9ad8\u6392\u540d\u76f8\u5173\u6027\u3002", "result": "\u4f01\u4e1a\u73af\u5883\u8bc4\u4f30\u663e\u793a\uff0cRegGuard\u5728\u76f8\u5173\u6027\u3001\u57fa\u7840\u6027\u548c\u4e0a\u4e0b\u6587\u805a\u7126\u65b9\u9762\u63d0\u9ad8\u7b54\u6848\u8d28\u91cf\uff0c\u540c\u65f6\u663e\u8457\u51cf\u8f7b\u5e7b\u89c9\u98ce\u9669\u3002\u7cfb\u7edf\u67b6\u6784\u5177\u6709\u53ef\u5ba1\u8ba1\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "conclusion": "RegGuard\u4e3a\u5177\u6709\u4e25\u683c\u5408\u89c4\u9700\u6c42\u7684\u9886\u57df\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7cfb\u7edf\u67b6\u6784\u652f\u6301\u6765\u6e90\u8ddf\u8e2a\u3001\u8bbf\u95ee\u63a7\u5236\u548c\u589e\u91cf\u7d22\u5f15\uff0c\u80fd\u591f\u5feb\u901f\u54cd\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6587\u6863\u6e90\u3002"}}
{"id": "2601.18579", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18579", "abs": "https://arxiv.org/abs/2601.18579", "authors": ["Seonho An", "Chaejeong Hyun", "Min-Soo Kim"], "title": "FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG", "comment": "under review", "summary": "Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.", "AI": {"tldr": "FastInsight\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u56feRAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u56fe\u6a21\u578b\u641c\u7d22\u548c\u5411\u91cf\u56fe\u641c\u7d22\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u62d3\u6251\u611f\u77e5\u548c\u8bed\u4e49\u611f\u77e5\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5728\u6548\u679c\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u56feRAG\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u8017\u65f6\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u5b9e\u73b0\u9ad8\u6548\u7684\u77e5\u8bc6\u68c0\u7d22\u3002\u4f5c\u8005\u901a\u8fc7\u56fe\u68c0\u7d22\u5206\u7c7b\u6cd5\u53d1\u73b0\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u6a21\u578b\u641c\u7d22\u7684\u62d3\u6251\u76f2\u6027\u548c\u56fe\u641c\u7d22\u7684\u8bed\u4e49\u76f2\u6027\u3002", "method": "\u63d0\u51faFastInsight\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u65b0\u9896\u7684\u878d\u5408\u7b97\u5b50\uff1a1) Graph-based Reranker (GRanker) - \u4f5c\u4e3a\u56fe\u6a21\u578b\u641c\u7d22\uff1b2) Semantic-Topological eXpansion (STeX) - \u4f5c\u4e3a\u5411\u91cf\u56fe\u641c\u7d22\u3002\u901a\u8fc7\u4ea4\u66ff\u4f7f\u7528\u8fd9\u4e24\u4e2a\u7b97\u5b50\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u68c0\u7d22\u548c\u751f\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aFastInsight\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6548\u679c\u548c\u6548\u7387\u7684\u6743\u8861\u4e2d\u5b9e\u73b0\u4e86\u5b9e\u8d28\u6027\u7684\u5e15\u7d2f\u6258\u6539\u8fdb\u3002", "conclusion": "FastInsight\u901a\u8fc7\u521b\u65b0\u7684\u878d\u5408\u7b97\u5b50\u89e3\u51b3\u4e86\u56feRAG\u4e2d\u7684\u62d3\u6251\u76f2\u6027\u548c\u8bed\u4e49\u76f2\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u9ad8\u6548\u4e14\u6d1e\u5bdf\u529b\u5f3a\u7684\u68c0\u7d22\uff0c\u4e3a\u56fe\u68c0\u7d22\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17228", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.17228", "abs": "https://arxiv.org/abs/2601.17228", "authors": ["Tengyue Zhang", "Ruiwen Ding", "Luoting Zhuang", "Yuxiao Wu", "Erika F. Rodriguez", "William Hsu"], "title": "Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification", "comment": null, "summary": "Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u534a\u76d1\u7763\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u4fdd\u6301\u7ec4\u7ec7\u5f62\u6001\u7684\u76ee\u6807\u611f\u77e5\u5408\u6210\u56fe\u50cf\uff0c\u6539\u5584\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u57df\u6cdb\u5316\u95ee\u9898", "motivation": "\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e38\u56e0\u57df\u504f\u79fb\u800c\u65e0\u6cd5\u8de8\u961f\u5217\u548c\u673a\u6784\u6cdb\u5316\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u5229\u7528\u76ee\u6807\u57df\u7684\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u8981\u4e48\u4f9d\u8d56\u53ef\u80fd\u626d\u66f2\u7ec4\u7ec7\u7ed3\u6784\u7684\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5", "method": "\u63d0\u51fa\u534a\u76d1\u7763\u57df\u9002\u5e94\u6846\u67b6\uff0c\u4f7f\u7528\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\u65e0\u6807\u7b7e\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u6269\u6563\u6a21\u578b\uff08\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7279\u5f81\u3001\u961f\u5217\u8eab\u4efd\u548c\u7ec4\u7ec7\u5236\u5907\u65b9\u6cd5\uff09\u751f\u6210\u4fdd\u6301\u7ec4\u7ec7\u5f62\u6001\u7684\u76ee\u6807\u611f\u77e5\u5408\u6210\u56fe\u50cf\uff0c\u7ed3\u5408\u6e90\u57df\u771f\u5b9e\u6807\u7b7e\u56fe\u50cf\u8bad\u7ec3\u4e0b\u6e38\u5206\u7c7b\u5668", "result": "\u5728\u80ba\u817a\u764c\u9884\u540e\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u76ee\u6807\u611f\u77e5\u589e\u5f3a\u663e\u8457\u63d0\u5347\u76ee\u6807\u57df\u6d4b\u8bd5\u96c6\u6027\u80fd\u800c\u4e0d\u964d\u4f4e\u6e90\u57df\u6027\u80fd\u3002\u76ee\u6807\u57df\u6d4b\u8bd5\u96c6\u7684\u52a0\u6743F1\u5206\u6570\u4ece0.611\u63d0\u5347\u81f30.706\uff0c\u5b8f\u89c2F1\u5206\u6570\u4ece0.641\u63d0\u5347\u81f30.716", "conclusion": "\u76ee\u6807\u611f\u77e5\u7684\u57fa\u4e8e\u6269\u6563\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u4e3a\u6539\u5584\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u57df\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u4e14\u6709\u6548\u7684\u65b9\u6cd5"}}
{"id": "2601.17631", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.17631", "abs": "https://arxiv.org/abs/2601.17631", "authors": ["Chanhou Lou"], "title": "Representative Litigation Settlement Agreements in Artificial Intelligence Copyright Infringement Disputes: A Comparative Reflection Based on the U.S", "comment": null, "summary": "The high-density, decentralized copyright conflicts triggered by generative AI training require more than ad hoc solutions; they demand structural governance tools. This article argues that representative litigation settlement agreements offer a distinct institutional advantage. Beyond reducing the transaction costs associated with the \"tragedy of the anticommons,\" these agreements generate market-visible evidence, specifically pricing signals and licensing practices, that validate the \"potential market\" under the fourth factor of fair use. This phenomenon constitutes procedural market-making. Through a comparative analysis of the U.S. Bartz class action settlement, this study reveals a dual motivation: a surface-level drive for risk aversion and remedy locking, and a deeper logic of constructing a training-licensing market. In the context of Chinese law, the feasibility of such agreements depends not on replicating foreign models, but on establishing three interpretive mechanisms: expanding the functional definition of \"same category\" claims; adopting a hybrid registration/confirmation system for indeterminate class membership; and converting the \"consent\" requirement under Article 57, Paragraph 3 of the Civil Procedure Law into a workable opt-out right subject to judicial scrutiny.", "AI": {"tldr": "\u8be5\u6587\u4e3b\u5f20\u4ee3\u8868\u6027\u8bc9\u8bbc\u548c\u89e3\u534f\u8bae\u53ef\u4f5c\u4e3a\u7ed3\u6784\u6027\u6cbb\u7406\u5de5\u5177\uff0c\u901a\u8fc7\u7a0b\u5e8f\u6027\u5e02\u573a\u6784\u5efa\u673a\u5236\u89e3\u51b3\u751f\u6210\u5f0fAI\u8bad\u7ec3\u5f15\u53d1\u7684\u9ad8\u5bc6\u5ea6\u3001\u53bb\u4e2d\u5fc3\u5316\u7248\u6743\u51b2\u7a81\uff0c\u5e76\u5728\u4e2d\u56fd\u6cd5\u8bed\u5883\u4e0b\u63d0\u51fa\u53ef\u884c\u6027\u8def\u5f84\u3002", "motivation": "\u751f\u6210\u5f0fAI\u8bad\u7ec3\u5f15\u53d1\u7684\u9ad8\u5bc6\u5ea6\u3001\u53bb\u4e2d\u5fc3\u5316\u7248\u6743\u51b2\u7a81\u9700\u8981\u8d85\u8d8a\u4e34\u65f6\u89e3\u51b3\u65b9\u6848\u7684\u7ed3\u6784\u6027\u6cbb\u7406\u5de5\u5177\uff0c\u4ee3\u8868\u6027\u8bc9\u8bbc\u548c\u89e3\u534f\u8bae\u5177\u6709\u72ec\u7279\u7684\u5236\u5ea6\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u7f8e\u56fdBartz\u96c6\u4f53\u8bc9\u8bbc\u548c\u89e3\u6848\u4f8b\uff0c\u63ed\u793a\u53cc\u91cd\u52a8\u673a\uff1a\u8868\u5c42\u98ce\u9669\u89c4\u907f\u4e0e\u6551\u6d4e\u9501\u5b9a\uff0c\u6df1\u5c42\u6784\u5efa\u8bad\u7ec3\u8bb8\u53ef\u5e02\u573a\u903b\u8f91\uff1b\u5728\u4e2d\u56fd\u6cd5\u8bed\u5883\u4e0b\u63d0\u51fa\u4e09\u4e2a\u89e3\u91ca\u673a\u5236\u3002", "result": "\u4ee3\u8868\u6027\u8bc9\u8bbc\u548c\u89e3\u534f\u8bae\u80fd\u964d\u4f4e\"\u53cd\u516c\u5730\u60b2\u5267\"\u4ea4\u6613\u6210\u672c\uff0c\u751f\u6210\u5e02\u573a\u53ef\u89c1\u8bc1\u636e\uff08\u5b9a\u4ef7\u4fe1\u53f7\u548c\u8bb8\u53ef\u5b9e\u8df5\uff09\uff0c\u9a8c\u8bc1\u5408\u7406\u4f7f\u7528\u7b2c\u56db\u8981\u7d20\u7684\"\u6f5c\u5728\u5e02\u573a\"\uff0c\u5b9e\u73b0\u7a0b\u5e8f\u6027\u5e02\u573a\u6784\u5efa\u3002", "conclusion": "\u5728\u4e2d\u56fd\u6cd5\u6846\u67b6\u4e0b\uff0c\u6b64\u7c7b\u534f\u8bae\u7684\u53ef\u884c\u6027\u4e0d\u4f9d\u8d56\u590d\u5236\u5916\u56fd\u6a21\u5f0f\uff0c\u800c\u9700\u5efa\u7acb\u4e09\u4e2a\u89e3\u91ca\u673a\u5236\uff1a\u6269\u5c55\"\u540c\u7c7b\"\u8bc9\u6c42\u529f\u80fd\u5b9a\u4e49\u3001\u91c7\u7528\u6df7\u5408\u6ce8\u518c/\u786e\u8ba4\u7cfb\u7edf\u5904\u7406\u4e0d\u786e\u5b9a\u96c6\u4f53\u6210\u5458\u8d44\u683c\u3001\u5c06\u6c11\u8bc9\u6cd5\u7b2c57\u6761\u7b2c3\u6b3e\"\u540c\u610f\"\u8981\u6c42\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u9000\u51fa\u6743\u5e76\u63a5\u53d7\u53f8\u6cd5\u5ba1\u67e5\u3002"}}
{"id": "2601.17828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17828", "abs": "https://arxiv.org/abs/2601.17828", "authors": ["Tanvi Verma", "Yang Zhou", "Rick Siow Mong Goh", "Yong Liu"], "title": "Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards", "comment": null, "summary": "We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.", "AI": {"tldr": "IGFT\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u7c7b\u5bf9\u8bdd\u6570\u636e\u7684\u533b\u7597\u5bf9\u8bddAI\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u8ba9\u6a21\u578b\u4ece\u4e0e\u6a21\u62df\u60a3\u8005\u7684\u81ea\u6211\u751f\u6210\u5bf9\u8bdd\u4e2d\u5b66\u4e60\u6709\u6548\u7684\u95ee\u8bca\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u5bf9\u8bddAI\u8bad\u7ec3\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u5bf9\u8bdd\u6570\u636e\u6216\u9759\u6001\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u9884\u6536\u96c6\u4eba\u7c7b\u5bf9\u8bdd\u7684\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u80fd\u81ea\u4e3b\u53d1\u73b0\u6709\u6548\u7684\u95ee\u8bca\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u5728\u7ebfGroup Relative Policy Optimization\uff08GRPO\uff09\u548c\u4fe1\u606f\u8bba\u5956\u52b1\uff0c\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u51fd\u6570\u8ffd\u8e2a\u5bf9\u8bdd\u4e2d\u63ed\u793a\u7684\u4e34\u5e8a\u5b9e\u4f53\uff08\u75c7\u72b6\u3001\u65f6\u95f4\u6a21\u5f0f\u3001\u75c5\u53f2\u7b49\uff09\uff0c\u7ed3\u5408GPT-4o-mini\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u901a\u8fc7LoRA\u5fae\u8c03Llama-3.1-8B-Instruct\u548cDeepSeek-R1-Distill-Qwen-7B\u6a21\u578b\u3002", "result": "\u5728Avey\u6570\u636e\u96c6\u4e0a\uff0cDeepSeek-R1-Distill-Qwen-7B\uff08IGFT\uff09F1\u5206\u6570\u8fbe0.408\uff08\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534710.9%\uff09\uff0c\u5728MIMIC\u6570\u636e\u96c6\u4e0a\u8fbe0.289\uff08\u63d0\u534712.9%\uff09\u3002Llama-3.1-8B-Instruct\uff08IGFT\uff09\u5206\u522b\u8fbe\u52300.384\u548c0.336\u3002\u4e24\u4e2a\u6a21\u578b\u5728MIMIC\u4e0a\u90fd\u8d85\u8d8a\u4e86OpenAI\u6a21\u578b\uff0c\u5e76\u8d85\u8d8a\u4e86HuatuoGPT\u548cUltraMedical\u7b49\u533b\u7597\u9886\u57df\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "IGFT\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5bf9\u8bdd\u7684\u6709\u6548\u533b\u7597\u5bf9\u8bddAI\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u5956\u52b1\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u9488\u5bf9\u6027\u7684\u4e34\u5e8a\u95ee\u9898\uff0c\u9ad8\u6548\u6536\u96c6\u8bca\u65ad\u4fe1\u606f\uff0c\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.17196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17196", "abs": "https://arxiv.org/abs/2601.17196", "authors": ["Nghia Thu Truong", "Qui Phu Pham", "Quang Nguyen", "Dung Luong", "Mai Tran"], "title": "Accelerated Sinkhorn Algorithms for Partial Optimal Transport", "comment": null, "summary": "Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\\mathcal{O}(n^{7/3}\\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $\u03b3$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.", "AI": {"tldr": "ASPOT\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4ea4\u66ff\u6700\u5c0f\u5316\u548cNesterov\u52a0\u901f\uff0c\u5c06\u90e8\u5206\u6700\u4f18\u4f20\u8f93\u7684Sinkhorn\u7b97\u6cd5\u590d\u6742\u5ea6\u4ece\u6b21\u4f18\u63d0\u5347\u5230O(n^{7/3}\u03b5^{-5/3})\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u71b5\u53c2\u6570\u8fdb\u4e00\u6b65\u4f18\u5316\u7ecf\u5178Sinkhorn\u65b9\u6cd5\u3002", "motivation": "\u90e8\u5206\u6700\u4f18\u4f20\u8f93(POT)\u5904\u7406\u4e24\u4e2a\u5206\u5e03\u95f4\u4ec5\u4f20\u8f93\u90e8\u5206\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u5206\u5e03\u5927\u5c0f\u4e0d\u7b49\u6216\u5305\u542b\u5f02\u5e38\u503c\u7684\u60c5\u51b5\u3002\u73b0\u6709Sinkhorn\u65b9\u6cd5\u7684\u590d\u6742\u5ea6\u754c\u9650\u6b21\u4f18\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u52a0\u901fSinkhorn\u90e8\u5206\u6700\u4f18\u4f20\u8f93(ASPOT)\u65b9\u6cd5\uff0c\u5728POT\u8bbe\u7f6e\u4e2d\u7ed3\u5408\u4ea4\u66ff\u6700\u5c0f\u5316\u548cNesterov\u98ce\u683c\u52a0\u901f\u3002\u540c\u65f6\u5c55\u793a\u4e86\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u71b5\u53c2\u6570\u03b3\u53ef\u4ee5\u6539\u8fdb\u7ecf\u5178Sinkhorn\u65b9\u6cd5\u7684\u6536\u655b\u901f\u7387\u3002", "result": "ASPOT\u5b9e\u73b0\u4e86O(n^{7/3}\u03b5^{-5/3})\u7684\u590d\u6742\u5ea6\u6539\u8fdb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u901a\u8fc7\u71b5\u53c2\u6570\u4f18\u5316\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u7ecf\u5178Sinkhorn\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5e76\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "ASPOT\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u90e8\u5206\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u901a\u8fc7\u52a0\u901f\u6280\u672f\u548c\u53c2\u6570\u4f18\u5316\u89e3\u51b3\u4e86\u73b0\u6709Sinkhorn\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.18664", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.18664", "abs": "https://arxiv.org/abs/2601.18664", "authors": ["Zihao Guo", "Jian Wang", "Ruxin Zhou", "Youhua Liu", "Jiawei Guo", "Jun Zhao", "Xiaoxiao Xu", "Yongqi Liu", "Kaiqiao Zhan"], "title": "S$^2$GR: Stepwise Semantic-Guided Reasoning in Latent Space for Generative Recommendation", "comment": null, "summary": "Generative Recommendation (GR) has emerged as a transformative paradigm with its end-to-end generation advantages. However, existing GR methods primarily focus on direct Semantic ID (SID) generation from interaction sequences, failing to activate deeper reasoning capabilities analogous to those in large language models and thus limiting performance potential. We identify two critical limitations in current reasoning-enhanced GR approaches: (1) Strict sequential separation between reasoning and generation steps creates imbalanced computational focus across hierarchical SID codes, degrading quality for SID codes; (2) Generated reasoning vectors lack interpretable semantics, while reasoning paths suffer from unverifiable supervision. In this paper, we propose stepwise semantic-guided reasoning in latent space (S$^2$GR), a novel reasoning enhanced GR framework. First, we establish a robust semantic foundation via codebook optimization, integrating item co-occurrence relationship to capture behavioral patterns, and load balancing and uniformity objectives that maximize codebook utilization while reinforcing coarse-to-fine semantic hierarchies. Our core innovation introduces the stepwise reasoning mechanism inserting thinking tokens before each SID generation step, where each token explicitly represents coarse-grained semantics supervised via contrastive learning against ground-truth codebook cluster distributions ensuring physically grounded reasoning paths and balanced computational focus across all SID codes. Extensive experiments demonstrate the superiority of S$^2$GR, and online A/B test confirms efficacy on large-scale industrial short video platform.", "AI": {"tldr": "S\u00b2GR\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u672c\u4f18\u5316\u548c\u9010\u6b65\u63a8\u7406\u673a\u5236\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u8bed\u4e49\u5f15\u5bfc\u7684\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u4e0e\u751f\u6210\u5206\u79bb\u3001\u8ba1\u7b97\u7126\u70b9\u4e0d\u5e73\u8861\u4ee5\u53ca\u63a8\u7406\u8def\u5f84\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u63a8\u8350\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4ece\u4ea4\u4e92\u5e8f\u5217\u76f4\u63a5\u751f\u6210\u8bed\u4e49ID\uff0c\u672a\u80fd\u6fc0\u6d3b\u7c7b\u4f3c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u9650\u5236\u4e86\u6027\u80fd\u6f5c\u529b\u3002\u5f53\u524d\u63a8\u7406\u589e\u5f3a\u7684GR\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1) \u63a8\u7406\u548c\u751f\u6210\u6b65\u9aa4\u7684\u4e25\u683c\u987a\u5e8f\u5206\u79bb\u5bfc\u81f4\u8de8\u5c42\u6b21SID\u4ee3\u7801\u7684\u8ba1\u7b97\u7126\u70b9\u4e0d\u5e73\u8861\uff1b2) \u751f\u6210\u7684\u63a8\u7406\u5411\u91cf\u7f3a\u4e4f\u53ef\u89e3\u91ca\u8bed\u4e49\uff0c\u63a8\u7406\u8def\u5f84\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u76d1\u7763\u3002", "method": "\u63d0\u51faS\u00b2GR\u6846\u67b6\uff1a\u9996\u5148\u901a\u8fc7\u4ee3\u7801\u672c\u4f18\u5316\u5efa\u7acb\u7a33\u5065\u7684\u8bed\u4e49\u57fa\u7840\uff0c\u6574\u5408\u7269\u54c1\u5171\u73b0\u5173\u7cfb\u4ee5\u6355\u6349\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u8d1f\u8f7d\u5e73\u8861\u548c\u5747\u5300\u6027\u76ee\u6807\u6700\u5927\u5316\u4ee3\u7801\u672c\u5229\u7528\u7387\uff0c\u540c\u65f6\u5f3a\u5316\u4ece\u7c97\u5230\u7ec6\u7684\u8bed\u4e49\u5c42\u6b21\u3002\u6838\u5fc3\u521b\u65b0\u662f\u9010\u6b65\u63a8\u7406\u673a\u5236\uff0c\u5728\u6bcf\u4e2aSID\u751f\u6210\u6b65\u9aa4\u524d\u63d2\u5165\u601d\u8003\u4ee4\u724c\uff0c\u6bcf\u4e2a\u4ee4\u724c\u660e\u786e\u8868\u793a\u7c97\u7c92\u5ea6\u8bed\u4e49\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u76d1\u7763\uff0c\u786e\u4fdd\u63a8\u7406\u8def\u5f84\u6709\u7269\u7406\u57fa\u7840\u4e14\u6240\u6709SID\u4ee3\u7801\u7684\u8ba1\u7b97\u7126\u70b9\u5e73\u8861\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86S\u00b2GR\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u786e\u8ba4\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "S\u00b2GR\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u7684\u9010\u6b65\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u5f0f\u63a8\u8350\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u8def\u5f84\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\uff0c\u5e76\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2601.17637", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17637", "abs": "https://arxiv.org/abs/2601.17637", "authors": ["Kazuhiro Takemoto"], "title": "Scaling Laws for Moral Machine Judgment in Large Language Models", "comment": "12 pages, 4 figures, 3 tables", "summary": "Autonomous systems increasingly require moral judgment capabilities, yet whether these capabilities scale predictably with model size remains unexplored. We systematically evaluate 75 large language model configurations (0.27B--1000B parameters) using the Moral Machine framework, measuring alignment with human preferences in life-death dilemmas. We observe a consistent power-law relationship with distance from human preferences ($D$) decreasing as $D \\propto S^{-0.10\\pm0.01}$ ($R^2=0.50$, $p<0.001$) where $S$ is model size. Mixed-effects models confirm this relationship persists after controlling for model family and reasoning capabilities. Extended reasoning models show additional 16\\% improvement beyond scale effects. The relationship holds across diverse architectures, while variance decreases at larger scales, indicating systematic emergence of more reliable moral judgment with computational scale. These findings extend scaling law research to value-based judgments and provide empirical foundations for artificial intelligence governance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u5224\u65ad\u80fd\u529b\u968f\u6a21\u578b\u89c4\u6a21\u5448\u5e42\u5f8b\u5173\u7cfb\u63d0\u5347\uff0c\u4e0e\u4eba\u7c7b\u504f\u597d\u5dee\u8ddd\u968f\u6a21\u578b\u5927\u5c0f\u589e\u52a0\u800c\u51cf\u5c0f\uff0c\u6269\u5c55\u63a8\u7406\u80fd\u529b\u53ef\u5e26\u6765\u989d\u591616%\u7684\u6539\u8fdb\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u7cfb\u7edf\u5bf9\u9053\u5fb7\u5224\u65ad\u80fd\u529b\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u4e86\u89e3\u8fd9\u4e9b\u80fd\u529b\u662f\u5426\u968f\u6a21\u578b\u89c4\u6a21\u53ef\u9884\u6d4b\u5730\u6269\u5c55\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u6cbb\u7406\u63d0\u4f9b\u5b9e\u8bc1\u57fa\u7840\u3002", "method": "\u4f7f\u7528Moral Machine\u6846\u67b6\u7cfb\u7edf\u8bc4\u4f3075\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u914d\u7f6e\uff080.27B-1000B\u53c2\u6570\uff09\uff0c\u6d4b\u91cf\u5728\u751f\u6b7b\u56f0\u5883\u4e2d\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u6548\u5e94\u6a21\u578b\u63a7\u5236\u6a21\u578b\u5bb6\u65cf\u548c\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u89c2\u5bdf\u5230\u4e0e\u4eba\u7c7b\u504f\u597d\u5dee\u8ddd\uff08D\uff09\u968f\u6a21\u578b\u89c4\u6a21\uff08S\uff09\u5448\u5e42\u5f8b\u5173\u7cfb\u51cf\u5c0f\uff1aD \u221d S^{-0.10\u00b10.01}\uff08R\u00b2=0.50\uff0cp<0.001\uff09\u3002\u6269\u5c55\u63a8\u7406\u6a21\u578b\u5728\u89c4\u6a21\u6548\u5e94\u57fa\u7840\u4e0a\u989d\u5916\u63d0\u534716%\u3002\u8be5\u5173\u7cfb\u5728\u4e0d\u540c\u67b6\u6784\u4e2d\u5747\u6210\u7acb\uff0c\u4e14\u5927\u89c4\u6a21\u6a21\u578b\u65b9\u5dee\u51cf\u5c0f\u3002", "conclusion": "\u9053\u5fb7\u5224\u65ad\u80fd\u529b\u968f\u8ba1\u7b97\u89c4\u6a21\u7cfb\u7edf\u6027\u6d8c\u73b0\uff0c\u6269\u5c55\u4e86\u89c4\u6a21\u5b9a\u5f8b\u7814\u7a76\u5230\u4ef7\u503c\u5224\u65ad\u9886\u57df\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u6cbb\u7406\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2601.17204", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2601.17204", "abs": "https://arxiv.org/abs/2601.17204", "authors": ["Yinkai Wang", "Yan Zhou Chen", "Xiaohui Chen", "Li-Ping Liu", "Soha Hassoun"], "title": "SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment", "comment": "preprint", "summary": "Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.", "AI": {"tldr": "SpecBridge\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9690\u5f0f\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06\u8d28\u8c31\u7ed3\u6784\u8bc6\u522b\u89c6\u4e3a\u51e0\u4f55\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u5fae\u8c03\u81ea\u76d1\u7763\u8d28\u8c31\u7f16\u7801\u5668\u76f4\u63a5\u6295\u5f71\u5230\u51bb\u7ed3\u5206\u5b50\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u51c6\u786e\u7387\u3002", "motivation": "\u5c0f\u5206\u5b50\u8d28\u8c31\u9274\u5b9a\u5728\u975e\u9776\u5411\u5206\u6790\u4e2d\u5b58\u5728\u74f6\u9888\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u662f\u663e\u5f0f\u751f\u6210\u6a21\u578b\uff08\u539f\u5b50\u7ea7\u6784\u5efa\u5206\u5b50\u56fe\uff09\uff0c\u8981\u4e48\u662f\u4ece\u5934\u5b66\u4e60\u8de8\u6a21\u6001\u5b50\u7a7a\u95f4\u7684\u8054\u5408\u5bf9\u6bd4\u6a21\u578b\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "SpecBridge\u91c7\u7528\u9690\u5f0f\u5bf9\u9f50\u6846\u67b6\uff0c\u5fae\u8c03\u81ea\u76d1\u7763\u8d28\u8c31\u7f16\u7801\u5668(DreaMS)\u76f4\u63a5\u6295\u5f71\u5230\u51bb\u7ed3\u5206\u5b50\u57fa\u7840\u6a21\u578b(ChemBERTa)\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u9884\u8ba1\u7b97\u7684\u5206\u5b50\u5d4c\u5165\u5e93\u4e2d\u8fdb\u884c\u68c0\u7d22\u3002", "result": "\u5728MassSpecGym\u3001Spectraverse\u548cMSnLib\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpecBridge\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u795e\u7ecf\u65b9\u6cd5\u5c06top-1\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u7ea620-25%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\u8f83\u5c11\u3002", "conclusion": "\u4e0e\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\u662f\u8bbe\u8ba1\u65b0\u67b6\u6784\u7684\u5b9e\u7528\u3001\u7a33\u5b9a\u66ff\u4ee3\u65b9\u6848\uff0cSpecBridge\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.18747", "categories": ["cs.IR", "cs.AI", "cs.CC", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.18747", "abs": "https://arxiv.org/abs/2601.18747", "authors": ["Amir Aavani"], "title": "Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval", "comment": null, "summary": "Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.\n  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\\mathbf{P}$. We introduce \\texttt{ComputePN}, a novel evaluation algorithm that makes $\\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \\texttt{ComputePN} ensures the efficient evaluation of any query in $\\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u8bed\u8a00\u548c\u7b97\u6cd5\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u903b\u8f91\u548c\u7b97\u672f\u7ea6\u675f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u7d22\u67b6\u6784\u5728\u6548\u7387\u548c\u5185\u5b58\u6d88\u8017\u65b9\u9762\u7684\u56f0\u5883\u3002", "motivation": "\u73b0\u4ee3\u4fe1\u606f\u68c0\u7d22\u6b63\u4ece\u7b80\u5355\u7684\u6587\u6863\u8fc7\u6ee4\u8f6c\u5411\u590d\u6742\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u5de5\u4f5c\u6d41\uff0c\u4f46\u73b0\u6709\u68c0\u7d22\u67b6\u6784\u5728\u5904\u7406\u4e25\u683c\u7684\u903b\u8f91\u548c\u7b97\u672f\u7ea6\u675f\u65f6\u9762\u4e34\u6548\u7387\u56f0\u5883\uff1a\u57fa\u4e8e\u8fed\u4ee3\u5668\u7684\u5f15\u64ce\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u5d4c\u5957\u903b\u8f91\u56fe\uff0c\u800c\u9012\u5f52\u65b9\u6cd5\u867d\u7136\u80fd\u652f\u6301\u8fd9\u4e9b\u7ed3\u6784\u4f46\u5185\u5b58\u6d88\u8017\u8fc7\u5927\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6709\u5411\u65e0\u73af\u56fe\uff08DAGs\uff09\u7684\u5f62\u5f0f\u5316\u68c0\u7d22\u8bed\u8a00\uff08$\\mathcal{L}_R$\uff09\uff0c\u5e76\u8bc1\u660e\u5176\u7cbe\u786e\u6355\u6349\u4e86\u590d\u6742\u5ea6\u7c7b$\\mathbf{P}$\u3002\u5f15\u5165\\texttt{ComputePN}\u7b97\u6cd5\uff0c\u7ed3\u5408\u539f\u751fDAG\u904d\u5386\u548c\u5185\u5b58\u9ad8\u6548\u7684\"\u6b63-\u8d1f\"\u54cd\u5e94\u673a\u5236\uff0c\u786e\u4fdd$\\mathcal{L}_R$\u4e2d\u4efb\u4f55\u67e5\u8be2\u7684\u9ad8\u6548\u8bc4\u4f30\u3002", "result": "\u5efa\u7acb\u4e86\u5c06\u641c\u7d22\u7d22\u5f15\u8f6c\u53d8\u4e3a\u901a\u7528\u8ba1\u7b97\u5f15\u64ce\u7684\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\\texttt{ComputePN}\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5bf9$\\mathcal{L}_R$\u67e5\u8be2\u7684\u9ad8\u6548\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u903b\u8f91\u7ea6\u675f\u4e0b\u7684\u6548\u7387\u548c\u5185\u5b58\u95ee\u9898\u3002", "conclusion": "\u68c0\u7d22\u5f15\u64ce\u5fc5\u987b\u80fd\u591f\"\u6355\u6349$\\mathbf{P}$\"\u2014\u2014\u5728\u7d22\u5f15\u4e0a\u4ee5\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u5f0f\u76f4\u63a5\u8bc4\u4f30\u4efb\u4f55\u591a\u9879\u5f0f\u65f6\u95f4\u5c5e\u6027\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5c06\u641c\u7d22\u7d22\u5f15\u8f6c\u53d8\u4e3a\u901a\u7528\u8ba1\u7b97\u5f15\u64ce\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.17254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17254", "abs": "https://arxiv.org/abs/2601.17254", "authors": ["Takato Yasuno"], "title": "Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization", "comment": "8 pages, 5 figures, 2 tables", "summary": "In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.", "AI": {"tldr": "\u65e5\u672c\u6865\u6881\u635f\u4f24\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5728\u4fdd\u62a4\u533a\u57df\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\uff0c\u4f7f\u7528SAM3\u8fdb\u884c\u94a2\u7b4b\u8150\u8680\u68c0\u6d4b\uff0cDBSCAN\u8865\u5168\u9057\u6f0f\u533a\u57df\uff0c\u9ad8\u65af\u6a21\u7cca\u4fdd\u62a4\u65bd\u5de5\u6807\u5fd7\uff0cGPU\u4f18\u5316\u5b9e\u73b01.7\u79d2/\u56fe\u50cf\u5904\u7406", "motivation": "\u65e5\u672c\u6cd5\u89c4\u8981\u6c42\u6bcf\u4e94\u5e74\u8fdb\u884c\u57fa\u7840\u8bbe\u65bd\u89c6\u89c9\u68c0\u67e5\uff0c\u73b0\u573a\u62cd\u6444\u7684\u635f\u4f24\u56fe\u50cf\u5e38\u5305\u542b\u6df7\u51dd\u571f\u88c2\u7f1d\u548c\u94a2\u7b4b\u66b4\u9732\uff0c\u540c\u65f6\u5e26\u6709\u65bd\u5de5\u6807\u5fd7\u7b49\u533a\u57df\u4fe1\u606f\u3002\u4e3a\u786e\u4fdd\u57fa\u7840\u8bbe\u65bd\u5b89\u5168\u4f7f\u7528\u800c\u4e0d\u5f15\u8d77\u516c\u4f17\u7126\u8651\uff0c\u9700\u8981\u5728\u51c6\u786e\u63d0\u53d6\u635f\u4f24\u7279\u5f81\u7684\u540c\u65f6\u4fdd\u62a4\u533a\u57df\u9690\u79c1\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u5f00\u6e90\u6865\u6881\u635f\u4f24\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528Segment Anything Model (SAM) 3\u8fdb\u884c\u94a2\u7b4b\u8150\u8680\u68c0\u6d4b\uff0cDBSCAN\u7b97\u6cd5\u81ea\u52a8\u8865\u5168\u9057\u6f0f\u533a\u57df\uff0c\u901a\u8fc7\u9ad8\u65af\u6a21\u7cca\u4fdd\u62a4\u65bd\u5de5\u6807\u5fd7\u533a\u57df\u3002\u5e94\u7528\u56db\u79cd\u9884\u5904\u7406\u65b9\u6cd5\u63d0\u9ad8OCR\u51c6\u786e\u7387\uff0c\u5e76\u8fdb\u884cGPU\u4f18\u5316\u3002", "result": "\u7cfb\u7edf\u6280\u672f\u6808\u5305\u62ecSAM3\u3001PyTorch\u3001OpenCV\u3001pytesseract\u548cscikit-learn\uff0cGPU\u4f18\u5316\u540e\u5b9e\u73b0\u6bcf\u5f20\u56fe\u50cf1.7\u79d2\u7684\u5904\u7406\u901f\u5ea6\uff0c\u5728\u4fdd\u62a4\u533a\u57df\u4fe1\u606f\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u6865\u6881\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u6865\u6881\u635f\u4f24\u68c0\u6d4b\u4e0e\u533a\u57df\u9690\u79c1\u4fdd\u62a4\u7684\u5e73\u8861\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u5b89\u5168\u4f7f\u7528\u63d0\u4f9b\u4e86\u6280\u672f\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u80fd\u51c6\u786e\u63d0\u53d6\u635f\u4f24\u7279\u5f81\u7528\u4e8e\u7ef4\u4fee\u51b3\u7b56\uff0c\u53c8\u80fd\u9632\u6b62\u654f\u611f\u533a\u57df\u4fe1\u606f\u6cc4\u9732\u5f15\u8d77\u516c\u4f17\u4e0d\u5b89\u3002"}}
{"id": "2601.17745", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.17745", "abs": "https://arxiv.org/abs/2601.17745", "authors": ["Ashwin Murthy", "Ramesh Krishnamaneni", "Sean Chacon", "Kelsey Carlson", "Ranjita Naik"], "title": "Predicting Juror Predisposition Using Machine Learning: A Comparative Study of Human and Algorithmic Jury Selection", "comment": null, "summary": "Prior studies on the effectiveness of professional jury consultants in predicting juror proclivities have yielded mixed results, and few have rigorously evaluated consultant performance against chance under controlled conditions. This study addresses that gap by empirically assessing whether jury consultants can reliably predict juror predispositions beyond chance levels and whether supervised machine-learning (ML) models can outperform consultant predictions. Using data from N mock jurors who completed pre-trial attitudinal questionnaires and rendered verdicts in a standardized wrongful-termination case, we compared predictions made by professional jury consultants with those generated by Random Forest (RF) and k-Nearest Neighbors (KNN) classifiers. Model and consultant predictions were evaluated on a held-out test set using paired statistical tests and nonparametric bootstrap procedures. We find that supervised ML models significantly outperform professional jury consultants under identical informational constraints, while offering greater transparency, replicability, and auditability. These results provide an empirical benchmark for evaluating human judgment in jury selection and inform ongoing debates about the role of data-driven decision support in legal contexts. To support reproducibility and auditability, all code and data will be made publicly available upon publication.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u966a\u5ba1\u5458\u503e\u5411\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4e13\u4e1a\u966a\u5ba1\u56e2\u987e\u95ee\uff0c\u4e14\u66f4\u5177\u900f\u660e\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u4e13\u4e1a\u966a\u5ba1\u56e2\u987e\u95ee\u9884\u6d4b\u966a\u5ba1\u5458\u503e\u5411\u6709\u6548\u6027\u7684\u7814\u7a76\u7ed3\u679c\u4e0d\u4e00\uff0c\u4e14\u7f3a\u4e4f\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u4e25\u683c\u8bc4\u4f30\u987e\u95ee\u8868\u73b0\u4e0e\u968f\u673a\u6c34\u5e73\u5bf9\u6bd4\u7684\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528N\u540d\u6a21\u62df\u966a\u5ba1\u5458\u5728\u6807\u51c6\u5316\u4e0d\u5f53\u89e3\u96c7\u6848\u4ef6\u4e2d\u7684\u9884\u5ba1\u6001\u5ea6\u95ee\u5377\u548c\u88c1\u51b3\u6570\u636e\uff0c\u6bd4\u8f83\u4e13\u4e1a\u966a\u5ba1\u56e2\u987e\u95ee\u4e0e\u968f\u673a\u68ee\u6797(RF)\u548ck\u6700\u8fd1\u90bb(KNN)\u5206\u7c7b\u5668\u7684\u9884\u6d4b\u8868\u73b0\u3002\u901a\u8fc7\u914d\u5bf9\u7edf\u8ba1\u68c0\u9a8c\u548c\u975e\u53c2\u6570\u81ea\u4e3e\u7a0b\u5e8f\u5728\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728\u76f8\u540c\u4fe1\u606f\u7ea6\u675f\u4e0b\uff0c\u76d1\u7763\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4e13\u4e1a\u966a\u5ba1\u56e2\u987e\u95ee\u7684\u9884\u6d4b\u8868\u73b0\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u900f\u660e\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bc4\u4f30\u966a\u5ba1\u5458\u9009\u62e9\u4e2d\u7684\u4eba\u7c7b\u5224\u65ad\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u51c6\uff0c\u5e76\u4e3a\u6cd5\u5f8b\u80cc\u666f\u4e0b\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u652f\u6301\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u4fe1\u606f\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5728\u53d1\u8868\u540e\u516c\u5f00\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u3002"}}
{"id": "2601.17897", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17897", "abs": "https://arxiv.org/abs/2601.17897", "authors": ["Jiayu Liu", "Yinhe Long", "Zhenya Huang", "Enhong Chen"], "title": "UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis", "comment": null, "summary": "A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.", "AI": {"tldr": "UniCog\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u601d\u7ef4\u7a7a\u95f4\u5206\u6790LLM\u8ba4\u77e5\uff0c\u53d1\u73b0LLM\u8ba4\u77e5\u9075\u5faa\u5e15\u7d2f\u6258\u539f\u5219\uff0c\u63a8\u7406\u5931\u8d25\u8868\u73b0\u4e3a\u6f5c\u5728\u6fc0\u6d3b\u5f02\u5e38\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u63d0\u5347\u63a8\u7406\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u89e3\u91caLLM\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba4\u77e5\u80fd\u529b\u5982\u4f55\u88ab\u8c03\u7528\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u5206\u6790LLM\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faUniCog\u7edf\u4e00\u6846\u67b6\uff0c\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff0c\u5c06\u5bc6\u96c6\u6a21\u578b\u6fc0\u6d3b\u7f16\u7801\u4e3a\u7a00\u758f\u3001\u89e3\u8026\u7684\u6f5c\u5728\u7ef4\u5ea6\uff0c\u5206\u6790\u516d\u4e2a\u5148\u8fdbLLM\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "result": "\u53d1\u73b0LLM\u8ba4\u77e5\u9075\u5faa\u5e15\u7d2f\u6258\u539f\u5219\uff1a\u5171\u4eab\u63a8\u7406\u6838\u5fc3\u8f85\u4ee5\u80fd\u529b\u7279\u5b9a\u7279\u5f81\uff1b\u63a8\u7406\u5931\u8d25\u8868\u73b0\u4e3a\u6f5c\u5728\u6fc0\u6d3b\u5f02\u5e38\uff1b\u63d0\u51fa\u7684\u6f5c\u5728\u4fe1\u606f\u5019\u9009\u4f18\u5148\u7ea7\u7b56\u7565\u5728\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u63d0\u5347\u63a8\u7406\u6027\u80fd\u8fbe7.5%\u3002", "conclusion": "UniCog\u4e3aLLM\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u8303\u5f0f\uff0c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8ba4\u77e5\u7684\u63a8\u7406\u52a8\u6001\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u6f5c\u5728\u6fc0\u6d3b\u5206\u6790\u63d0\u5347\u63a8\u7406\u6027\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.17258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17258", "abs": "https://arxiv.org/abs/2601.17258", "authors": ["Jo\u00e3o Pereira", "Vasco Lopes", "Jo\u00e3o Neves", "David Semedo"], "title": "FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding", "comment": null, "summary": "Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.", "AI": {"tldr": "FineVAU\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u57fa\u51c6\uff0c\u5305\u542bFVScore\u8bc4\u4f30\u6307\u6807\u548cFineW3\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349LVLM\u4e30\u5bcc\u3001\u7ec6\u7c92\u5ea6\u54cd\u5e94\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89c6\u9891\u5f02\u5e38\u7406\u89e3\uff08VAU\uff09\u4efb\u52a1\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u73b0\u6709\u57fa\u51c6\u4f7f\u7528n-gram\u6307\u6807\uff08\u5982BLEU\u3001ROUGE-L\uff09\u6216\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\uff0c\u524d\u8005\u65e0\u6cd5\u6355\u6349LVLM\u4e30\u5bcc\u3001\u81ea\u7531\u5f62\u5f0f\u4e14\u89c6\u89c9\u57fa\u7840\u54cd\u5e94\u7684\u7279\u6027\uff0c\u540e\u8005\u4fa7\u91cd\u4e8e\u8bed\u8a00\u8d28\u91cf\u800c\u975e\u4e8b\u5b9e\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u4e3b\u89c2\u5224\u65ad\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e0d\u4e00\u81f4\u3002", "method": "1\uff09\u5c06VAU\u5b9a\u4e49\u4e3a\u4e09\u65b9\u9762\u95ee\u9898\uff1a\u4e8b\u4ef6\uff08What\uff09\u3001\u53c2\u4e0e\u5b9e\u4f53\uff08Who\uff09\u548c\u4f4d\u7f6e\uff08Where\uff09\uff1b2\uff09\u63d0\u51faFVScore\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30LVLM\u56de\u7b54\u4e2d\u5173\u952e\u89c6\u89c9\u5143\u7d20\u7684\u5b58\u5728\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7ec6\u7c92\u5ea6\u53cd\u9988\uff1b3\uff09\u6784\u5efaFineW3\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5168\u81ea\u52a8\u6d41\u7a0b\u589e\u5f3a\u73b0\u6709\u4eba\u5de5\u6807\u6ce8\uff0c\u6dfb\u52a0\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\uff0cFVScore\u6307\u6807\u5728\u5f02\u5e38\u611f\u77e5\u65b9\u9762\u4e0e\u4eba\u7c7b\u5224\u65ad\u5177\u6709\u66f4\u597d\u7684\u5bf9\u9f50\u6027\u3002\u8be6\u7ec6\u5b9e\u9a8c\u63ed\u793a\u4e86LVLM\u5728\u9700\u8981\u7a7a\u95f4\u548c\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7406\u89e3\u7684\u5f02\u5e38\u4e8b\u4ef6\u611f\u77e5\u65b9\u9762\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u5c3d\u7ba1\u5728\u7c97\u7c92\u5ea6\u3001\u9759\u6001\u4fe1\u606f\u548c\u5177\u6709\u5f3a\u89c6\u89c9\u7ebf\u7d22\u7684\u4e8b\u4ef6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "FineVAU\u57fa\u51c6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u4e3a\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLVLM\u5728\u65f6\u7a7a\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.17877", "categories": ["cs.CY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17877", "abs": "https://arxiv.org/abs/2601.17877", "authors": ["Sahibpreet Singh"], "title": "Comparative Algorithmic Governance of Public Health Instruments across India, EU, US and LMICs", "comment": "Chapter in \"Law and Medicine\" (Pacific Books International, 2025), pp. 409-423", "summary": "The study investigates the juridico-technological architecture of international public health instruments, focusing on their implementation across India, the European Union, the United States and low- and middle-income countries (LMICs), particularly in Sub-Saharan Africa. It addresses a research lacuna: the insufficient harmonisation between normative health law and algorithmic public health infrastructures in resource-constrained jurisdictions. The principal objective is to assess how artificial intelligence augments implementation of instruments grounded in IHR 2005 and the WHO FCTC while identifying doctrinal and infrastructural bottlenecks. Using comparative doctrinal analysis and legal-normative mapping, the study triangulates legislative instruments, WHO monitoring frameworks, AI systems including BlueDot, Aarogya Setu and EIOS, and compliance metrics. Preliminary results show that AI has improved early detection, surveillance precision and responsiveness in high-capacity jurisdictions, whereas LMICs face infrastructural deficits, data privacy gaps and fragmented legal scaffolding. The findings highlight the relevance of the EU Artificial Intelligence Act and GDPR as regulatory prototypes for health-oriented algorithmic governance and contrast them with embryonic AI integration and limited internet penetration in many LMICs. The study argues for embedding AI within a rights-compliant, supranationally coordinated regulatory framework to secure equitable health outcomes and stronger compliance. It proposes a model for algorithmic treaty-making inspired by FCTC architecture and calls for WHO-led compliance mechanisms modelled on the WTO Dispute Settlement Body to enhance pandemic preparedness, surveillance equity and transnational governance resilience.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u56fd\u9645\u516c\u5171\u536b\u751f\u5de5\u5177\u7684\u6cd5\u5f8b-\u6280\u672f\u67b6\u6784\u5728\u4e0d\u540c\u53f8\u6cd5\u7ba1\u8f96\u533a\u7684\u5b9e\u65bd\u60c5\u51b5\uff0c\u91cd\u70b9\u5173\u6ce8AI\u5982\u4f55\u589e\u5f3a\u57fa\u4e8e\u300a\u56fd\u9645\u536b\u751f\u6761\u4f8b2005\u300b\u548c\u300a\u4e16\u754c\u536b\u751f\u7ec4\u7ec7\u70df\u8349\u63a7\u5236\u6846\u67b6\u516c\u7ea6\u300b\u7684\u5de5\u5177\u5b9e\u65bd\uff0c\u540c\u65f6\u8bc6\u522b\u6cd5\u5f8b\u548c\u57fa\u7840\u8bbe\u65bd\u74f6\u9888\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\uff1a\u8d44\u6e90\u53d7\u9650\u53f8\u6cd5\u7ba1\u8f96\u533a\u4e2d\u89c4\u8303\u6027\u536b\u751f\u6cd5\u4e0e\u7b97\u6cd5\u516c\u5171\u536b\u751f\u57fa\u7840\u8bbe\u65bd\u4e4b\u95f4\u7684\u534f\u8c03\u4e0d\u8db3\u3002\u5f53\u524dAI\u5728\u516c\u5171\u536b\u751f\u9886\u57df\u7684\u5e94\u7528\u5b58\u5728\u4e0d\u5747\u8861\uff0c\u9ad8\u80fd\u529b\u53f8\u6cd5\u7ba1\u8f96\u533a\u4e0e\u4e2d\u4f4e\u6536\u5165\u56fd\u5bb6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u6bd4\u8f83\u6cd5\u5f8b\u5206\u6790\u548c\u6cd5\u5f8b\u89c4\u8303\u6620\u5c04\u65b9\u6cd5\uff0c\u4e09\u89d2\u9a8c\u8bc1\u7acb\u6cd5\u5de5\u5177\u3001WHO\u76d1\u6d4b\u6846\u67b6\u3001AI\u7cfb\u7edf\uff08\u5305\u62ecBlueDot\u3001Aarogya Setu\u548cEIOS\uff09\u4ee5\u53ca\u5408\u89c4\u6027\u6307\u6807\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff1a\u5728\u9ad8\u80fd\u529b\u53f8\u6cd5\u7ba1\u8f96\u533a\uff0cAI\u6539\u5584\u4e86\u65e9\u671f\u68c0\u6d4b\u3001\u76d1\u6d4b\u7cbe\u5ea6\u548c\u54cd\u5e94\u80fd\u529b\uff1b\u800c\u4e2d\u4f4e\u6536\u5165\u56fd\u5bb6\u9762\u4e34\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u3001\u6570\u636e\u9690\u79c1\u6f0f\u6d1e\u548c\u788e\u7247\u5316\u6cd5\u5f8b\u6846\u67b6\u7b49\u95ee\u9898\u3002\u6b27\u76df\u300a\u4eba\u5de5\u667a\u80fd\u6cd5\u6848\u300b\u548cGDPR\u53ef\u4f5c\u4e3a\u5065\u5eb7\u5bfc\u5411\u7b97\u6cd5\u6cbb\u7406\u7684\u76d1\u7ba1\u539f\u578b\u3002", "conclusion": "\u7814\u7a76\u4e3b\u5f20\u5c06AI\u5d4c\u5165\u6743\u5229\u5408\u89c4\u3001\u8d85\u56fd\u5bb6\u534f\u8c03\u7684\u76d1\u7ba1\u6846\u67b6\u4e2d\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u7684\u5065\u5eb7\u7ed3\u679c\u548c\u66f4\u5f3a\u7684\u5408\u89c4\u6027\u3002\u63d0\u51fa\u57fa\u4e8eFCTC\u67b6\u6784\u7684\u7b97\u6cd5\u6761\u7ea6\u5236\u5b9a\u6a21\u578b\uff0c\u5e76\u547c\u5401\u5efa\u7acbWHO\u4e3b\u5bfc\u7684\u5408\u89c4\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u5927\u6d41\u884c\u9632\u8303\u3001\u76d1\u6d4b\u516c\u5e73\u6027\u548c\u8de8\u56fd\u6cbb\u7406\u97e7\u6027\u3002"}}
{"id": "2601.17215", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2601.17215", "abs": "https://arxiv.org/abs/2601.17215", "authors": ["Ruoqing Zheng", "Chang Sun", "Qibin Liu", "Lauri Laatu", "Arianna Cox", "Benedikt Maier", "Alexander Tapper", "Jose G. F. Coutinho", "Wayne Luk", "Zhiqiang Que"], "title": "JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers", "comment": "15 pages,", "summary": "We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.", "AI": {"tldr": "JetFormer\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u7c92\u5b50\u55b7\u6ce8\u6807\u8bb0\u7684Transformer\u67b6\u6784\uff0c\u80fd\u591f\u5728\u4ece\u79bb\u7ebf\u5206\u6790\u5230\u5728\u7ebf\u89e6\u53d1\u7684\u5168\u573a\u666f\u4e2d\u9ad8\u6548\u8fd0\u884c\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u90e8\u7f72\u573a\u666f\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u5728\u9ad8\u7cbe\u5ea6\u79bb\u7ebf\u5206\u6790\u548c\u8d85\u4f4e\u5ef6\u8fdf\u5728\u7ebf\u89e6\u53d1\u7b49\u5168\u573a\u666f\u4e2d\u90fd\u6709\u6548\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u65e2\u9ad8\u6027\u80fd\u53c8\u53ef\u90e8\u7f72\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4ec5\u7f16\u7801\u5668\u7684Transformer\u67b6\u6784\u5904\u7406\u53d8\u957f\u7c92\u5b50\u7279\u5f81\u96c6\uff0c\u65e0\u9700\u663e\u5f0f\u7684\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u8f93\u5165\u3002\u5f15\u5165\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u6d41\u7a0b\uff0c\u5305\u62ec\u591a\u76ee\u6807\u8d85\u53c2\u6570\u641c\u7d22\u3001\u7ed3\u6784\u5316\u526a\u679d\u548c\u91cf\u5316\uff0c\u751f\u6210\u9002\u5408FPGA\u90e8\u7f72\u7684\u7d27\u51d1\u53d8\u4f53\u3002", "result": "\u5728JetClass\u6570\u636e\u96c6\u4e0a\uff0cJetFormer\u5728\u7cbe\u5ea6\u4e0a\u4e0eParT\u6a21\u578b\u76f8\u5f53\uff08\u76f8\u5dee0.7%\u4ee5\u5185\uff09\uff0c\u4f46\u8ba1\u7b97\u91cf\u51cf\u5c1137.4%\u3002\u5728HLS4ML 150P\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6bd4MLP\u3001Deep Sets\u548cInteraction Networks\u7b49\u6a21\u578b\u7cbe\u5ea6\u9ad83-4%\u3002\u901a\u8fc7\u538b\u7f29\u4f18\u5316\uff0cJetFormer-tiny\u53d8\u4f53\u53ef\u6ee1\u8db3FPGA\u89e6\u53d1\u7cfb\u7edf\u7684\u4e9a\u5fae\u79d2\u5ef6\u8fdf\u8981\u6c42\u3002", "conclusion": "JetFormer\u901a\u8fc7\u7edf\u4e00\u7684\u67b6\u6784\u6846\u67b6\u5c06\u9ad8\u6027\u80fd\u5efa\u6a21\u548c\u53ef\u90e8\u7f72\u6027\u7ed3\u5408\u8d77\u6765\uff0c\u4e3a\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684Transformer\u57fa\u55b7\u6ce8\u6807\u8bb0\u5668\u90e8\u7f72\u8def\u5f84\u3002"}}
{"id": "2601.17495", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.17495", "abs": "https://arxiv.org/abs/2601.17495", "authors": ["Ruiyu Zhang", "Lin Nie", "Wai-Fung Lam", "Qihao Wang", "Xin Zhao"], "title": "PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems", "comment": "15 pages, 1 figure", "summary": "In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.", "AI": {"tldr": "PEARL\u662f\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u5d4c\u5165\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u578b\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u6765\u6539\u5584\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\uff0c\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u76f8\u4f3c\u6027\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u90e8\u7f72\u7cfb\u7edf\u4e2d\uff0c\u56fa\u5b9a\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u5c40\u90e8\u90bb\u57df\u7ed3\u6784\u5f80\u5f80\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u76f8\u4f3c\u6027\u68c0\u7d22\u5931\u8d25\u3002\u7531\u4e8e\u6807\u7b7e\u7a00\u7f3a\u3001\u9886\u57df\u6f02\u79fb\u548c\u91cd\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u5d4c\u5165\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "PEARL\uff08\u539f\u578b\u589e\u5f3a\u5bf9\u9f50\u8868\u793a\u5b66\u4e60\uff09\u4f7f\u7528\u6709\u9650\u76d1\u7763\u901a\u8fc7\u8f6f\u5bf9\u9f50\u5d4c\u5165\u5230\u7c7b\u522b\u539f\u578b\u6765\u91cd\u5851\u5c40\u90e8\u90bb\u57df\u51e0\u4f55\u7ed3\u6784\uff0c\u4fdd\u6301\u7ef4\u5ea6\u4e0d\u53d8\uff0c\u907f\u514d\u6fc0\u8fdb\u6295\u5f71\u6216\u574d\u7f29\u3002", "result": "\u5728\u6807\u7b7e\u7a00\u7f3a\u6761\u4ef6\u4e0b\uff0cPEARL\u663e\u8457\u6539\u5584\u5c40\u90e8\u90bb\u57df\u8d28\u91cf\uff0c\u76f8\u6bd4\u539f\u59cb\u5d4c\u5165\u63d0\u534725.7%\uff0c\u76f8\u6bd4\u5f3a\u65e0\u76d1\u7763\u540e\u5904\u7406\u65b9\u6cd5\u63d0\u534721.1%\u4ee5\u4e0a\u3002", "conclusion": "PEARL\u586b\u8865\u4e86\u65e0\u76d1\u7763\u540e\u5904\u7406\uff08\u589e\u76ca\u6709\u9650\uff09\u548c\u5168\u76d1\u7763\u6295\u5f71\uff08\u9700\u8981\u5927\u91cf\u6807\u7b7e\uff09\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u5728\u76f8\u4f3c\u6027\u7cfb\u7edf\u6700\u8106\u5f31\u7684\u6807\u7b7e\u7a00\u7f3a\u573a\u666f\u4e2d\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2601.17259", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17259", "abs": "https://arxiv.org/abs/2601.17259", "authors": ["Angad Singh Ahuja", "Aarush Ram Anandh"], "title": "Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling", "comment": "25 Pages, 12 Figures, 3 Tables, 5 Appendices, 8 Algorithms", "summary": "Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u989c\u8272\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u57df\u7ea6\u675f\u548c\u68af\u5ea6\u5f15\u5bfc\u5b9e\u73b0\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7cbe\u786e\u989c\u8272\u5339\u914d", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u7cfb\u7edf\u5728\u7cbe\u786e\u989c\u8272\u63a7\u5236\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5931\u8d25\uff0c\u7279\u522b\u662f\u5728\u8bbe\u8ba1\u5bfc\u5411\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u9700\u8981\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u7684\u989c\u8272\u76ee\u6807", "method": "\u7ed3\u5408ROI\u4fee\u590d\u3001\u80cc\u666f\u6f5c\u5728\u91cd\u65b0\u65bd\u52a0\u548c\u68af\u5ea6\u5f15\u5bfc\u7684\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528CIE Lab\u548c\u7ebf\u6027RGB\u7a7a\u95f4\uff0c\u901a\u8fc7CVaR\u98ce\u683c\u548c\u8f6f\u6700\u5927\u503c\u60e9\u7f5a\u63a7\u5236\u50cf\u7d20\u8bef\u5dee\u5206\u5e03", "result": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u673a\u5236\uff0c\u80fd\u591f\u5b9e\u73b0\u76ee\u6807\u989c\u8272\u5339\u914d\uff0c\u53ef\u96c6\u6210\u5230\u6807\u51c6Stable Diffusion\u4fee\u590d\u6d41\u7a0b\u4e2d", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u611f\u77e5\u76ee\u6807\u80fd\u591f\u89e3\u51b3\u4ec5\u4f7f\u7528\u5747\u503c\u7ea6\u675f\u65f6\u4ea7\u751f\u7684\u5c40\u90e8\u611f\u77e5\u5931\u8d25\u95ee\u9898\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u989c\u8272\u63a7\u5236\u65b9\u6cd5"}}
{"id": "2601.17892", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17892", "abs": "https://arxiv.org/abs/2601.17892", "authors": ["Sahibpreet Singh", "Manjit Singh"], "title": "Artificial Intelligence and Intellectual Property Rights: Comparative Transnational Policy Analysis", "comment": "Published in Journal of University Institute of Legal Studies, Vol. 19, Issue 1, pp. 182-208, 2025", "summary": "Artificial intelligence's rapid integration with intellectual property rights necessitates assessment of its impact on trade secrets, copyrights and patents. This study addresses lacunae in existing laws where India lacks AI-specific provisions, creating doctrinal inconsistencies and enforcement inefficacies. Global discourse on AI-IPR protections remains nascent. The research identifies gaps in Indian IP laws' adaptability to AI-generated outputs: trade secret protection is inadequate against AI threats; standardized inventorship criteria are absent. Employing doctrinal and comparative methodology, it scrutinizes legislative texts, judicial precedents and policy instruments across India, US, UK and EU. Preliminary findings reveal shortcomings: India's contract law creates fragmented trade secret regime; Section 3(k) of Indian Patents Act blocks AI invention patenting; copyright varies in authorship attribution. The study proposes harmonized legal taxonomy accommodating AI's role while preserving innovation incentives. India's National AI Strategy (2024) shows progress but legislative clarity is imperative. This contributes to global discourse with AI-specific IP protections ensuring resilience and equitable innovation. Promising results underscore recalibrating India's IP jurisprudence for global alignment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u4eba\u5de5\u667a\u80fd\u5bf9\u77e5\u8bc6\u4ea7\u6743\uff08\u7279\u522b\u662f\u5546\u4e1a\u79d8\u5bc6\u3001\u7248\u6743\u548c\u4e13\u5229\uff09\u7684\u5f71\u54cd\uff0c\u6307\u51fa\u5370\u5ea6\u73b0\u884c\u6cd5\u5f8b\u7f3a\u4e4fAI\u4e13\u95e8\u89c4\u5b9a\uff0c\u5b58\u5728\u9002\u7528\u4e0d\u4e00\u81f4\u548c\u6267\u884c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u534f\u8c03\u6cd5\u5f8b\u6846\u67b6\u7684\u5efa\u8bae\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u4e0e\u77e5\u8bc6\u4ea7\u6743\u7684\u5feb\u901f\u878d\u5408\u9700\u8981\u8bc4\u4f30\u5176\u5bf9\u5546\u4e1a\u79d8\u5bc6\u3001\u7248\u6743\u548c\u4e13\u5229\u7684\u5f71\u54cd\u3002\u5370\u5ea6\u73b0\u884c\u6cd5\u5f8b\u7f3a\u4e4fAI\u4e13\u95e8\u89c4\u5b9a\uff0c\u5bfc\u81f4\u6cd5\u5f8b\u9002\u7528\u4e0d\u4e00\u81f4\u548c\u6267\u884c\u6548\u7387\u4f4e\u4e0b\uff0c\u5168\u7403\u5173\u4e8eAI-IPR\u4fdd\u62a4\u7684\u8ba8\u8bba\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\u3002", "method": "\u91c7\u7528\u7406\u8bba\u548c\u6bd4\u8f83\u7814\u7a76\u65b9\u6cd5\uff0c\u5ba1\u67e5\u5370\u5ea6\u3001\u7f8e\u56fd\u3001\u82f1\u56fd\u548c\u6b27\u76df\u7684\u7acb\u6cd5\u6587\u672c\u3001\u53f8\u6cd5\u5224\u4f8b\u548c\u653f\u7b56\u5de5\u5177\uff0c\u5206\u6790\u73b0\u6709\u6cd5\u5f8b\u6846\u67b6\u5bf9AI\u751f\u6210\u5185\u5bb9\u7684\u9002\u5e94\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5370\u5ea6\u6cd5\u5f8b\u5b58\u5728\u591a\u5904\u4e0d\u8db3\uff1a\u5408\u540c\u6cd5\u9020\u6210\u5546\u4e1a\u79d8\u5bc6\u4fdd\u62a4\u788e\u7247\u5316\uff1b\u4e13\u5229\u6cd5\u7b2c3(k)\u6761\u963b\u788dAI\u53d1\u660e\u83b7\u5f97\u4e13\u5229\uff1b\u7248\u6743\u6cd5\u5728\u4f5c\u8005\u5f52\u5c5e\u65b9\u9762\u5b58\u5728\u5dee\u5f02\u3002\u5370\u5ea6\u56fd\u5bb6AI\u6218\u7565\uff082024\uff09\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u7acb\u6cd5\u660e\u786e\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u534f\u8c03\u7684\u6cd5\u5f8b\u5206\u7c7b\u4f53\u7cfb\uff0c\u5728\u9002\u5e94AI\u4f5c\u7528\u7684\u540c\u65f6\u4fdd\u6301\u521b\u65b0\u6fc0\u52b1\u3002\u9700\u8981\u91cd\u65b0\u8c03\u6574\u5370\u5ea6\u77e5\u8bc6\u4ea7\u6743\u6cd5\u7406\u4ee5\u5b9e\u73b0\u5168\u7403\u534f\u8c03\uff0c\u786e\u4fddAI\u4e13\u95e8\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u5177\u6709\u97e7\u6027\u548c\u516c\u5e73\u521b\u65b0\u3002"}}
{"id": "2601.17920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17920", "abs": "https://arxiv.org/abs/2601.17920", "authors": ["Xuanzhou Chen", "Audrey Wang", "Stanley Yin", "Hanyang Jiang", "Dong Zhang"], "title": "Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges", "comment": null, "summary": "Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u4ee5\u8f6f\u7269\u8d28\u4e3a\u4ee3\u8868\u6027\u573a\u666f\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9a8c\u5ba4\uff08SDLs\uff09\u4e2d\u7684AI\u95ee\u9898\uff0c\u5c06\u5176\u6846\u67b6\u5316\u4e3a\u667a\u80fd\u4f53\u73af\u5883\u4ea4\u4e92\u95ee\u9898\uff0c\u56de\u987e\u4e86\u95ed\u73af\u5b9e\u9a8c\u7684\u4e3b\u8981\u65b9\u6cd5\u5bb6\u65cf\uff0c\u63d0\u51fa\u4e86\u80fd\u529b\u9a71\u52a8\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u5236\u5b9a\u4e86\u57fa\u51c6\u4efb\u52a1\u6a21\u677f\u548c\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9a8c\u5ba4\u4e3a\u667a\u80fd\u4f53AI\u5728\u6602\u8d35\u64cd\u4f5c\u3001\u566a\u58f0\u5ef6\u8fdf\u53cd\u9988\u3001\u4e25\u683c\u53ef\u884c\u6027\u548c\u5b89\u5168\u7ea6\u675f\u4ee5\u53ca\u975e\u5e73\u7a33\u6027\u7b49\u6311\u6218\u4e0b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002\u8bba\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406SDL\u4e2d\u7684AI\u95ee\u9898\uff0c\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u5c06SDL\u81ea\u4e3b\u6027\u6846\u67b6\u5316\u4e3a\u5177\u6709\u660e\u786e\u89c2\u5bdf\u3001\u884c\u52a8\u3001\u6210\u672c\u548c\u7ea6\u675f\u7684\u667a\u80fd\u4f53\u73af\u5883\u4ea4\u4e92\u95ee\u9898\uff1b\u56de\u987e\u8d1d\u53f6\u65af\u4f18\u5316\u3001\u4e3b\u52a8\u5b66\u4e60\u3001\u89c4\u5212\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5de5\u5177\u4f7f\u7528\u667a\u80fd\u4f53\u7b49\u65b9\u6cd5\u5bb6\u65cf\uff1b\u63d0\u51fa\u57fa\u4e8e\u51b3\u7b56\u89c6\u91ce\u3001\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3001\u884c\u52a8\u53c2\u6570\u5316\u3001\u7ea6\u675f\u5904\u7406\u3001\u6545\u969c\u6062\u590d\u548c\u4eba\u7c7b\u53c2\u4e0e\u7684\u80fd\u529b\u9a71\u52a8\u5206\u7c7b\u6cd5\uff1b\u5236\u5b9a\u57fa\u51c6\u4efb\u52a1\u6a21\u677f\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5efa\u7acb\u4e86SDL\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u8fde\u63a5\u4e86\u5e38\u89c1SDL\u6d41\u7a0b\u4e0e\u5df2\u5efa\u7acb\u7684AI\u539f\u5219\uff1b\u63d0\u51fa\u4e86\u7cfb\u7edf\u7684\u5206\u7c7b\u4f53\u7cfb\u548c\u8bc4\u4f30\u6807\u51c6\uff1b\u603b\u7ed3\u4e86\u5df2\u90e8\u7f72SDL\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u5e76\u6307\u51fa\u4e86\u591a\u6a21\u6001\u8868\u793a\u3001\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u3001\u5b89\u5168\u63a2\u7d22\u548c\u5171\u4eab\u57fa\u51c6\u57fa\u7840\u8bbe\u65bd\u7b49\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9a8c\u5ba4\u662fAI\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u91cd\u8981\u524d\u6cbf\u9886\u57df\uff0c\u9700\u8981\u53ef\u9a8c\u8bc1\u3001\u53ef\u6eaf\u6e90\u7684\u653f\u7b56\u6765\u652f\u6301\u8c03\u8bd5\u3001\u53ef\u91cd\u590d\u6027\u548c\u5b89\u5168\u64cd\u4f5c\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u5173\u6ce8\u591a\u6a21\u6001\u8868\u793a\u3001\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u3001\u5b89\u5168\u63a2\u7d22\u548c\u5171\u4eab\u57fa\u51c6\u57fa\u7840\u8bbe\u65bd\u7b49\u5173\u952e\u6311\u6218\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.17224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17224", "abs": "https://arxiv.org/abs/2601.17224", "authors": ["Dmitrii Torbunov", "Yihui Ren", "Lijun Wu", "Yimei Zhu"], "title": "Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning", "comment": null, "summary": "Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.", "AI": {"tldr": "\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff08CDI\uff09\u4ece\u4e00\u7ef4\u65f6\u95f4\u4fe1\u53f7\u6269\u5c55\u5230\u4e8c\u7ef4\u7a7a\u95f4\u6570\u636e\uff0c\u7528\u4e8e\u6750\u6599\u8868\u5f81\u4e2d\u7684\u7535\u5b50\u884d\u5c04\u53c2\u6570\u53cd\u6f14\uff0c\u80fd\u751f\u6210\u6821\u51c6\u826f\u597d\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u51c6\u786e\u53cd\u6620\u6d4b\u91cf\u7ea6\u675f\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u79d1\u5b66\u53cd\u95ee\u9898\u4e2d\u9700\u8981\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u4ee5\u533a\u5206\u53ef\u8bc6\u522b\u53c2\u6570\u548c\u6a21\u7cca\u53c2\u6570\u3002\u867d\u7136CDI\u5728\u4e00\u7ef4\u65f6\u95f4\u4fe1\u53f7\u4e0a\u5df2\u8bc1\u660e\u6709\u6548\uff0c\u4f46\u5176\u5728\u66f4\u9ad8\u7ef4\u7a7a\u95f4\u6570\u636e\u4e0a\u7684\u9002\u7528\u6027\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u5c06CDI\u6269\u5c55\u5230\u4e8c\u7ef4\u7a7a\u95f4\u6761\u4ef6\u5316\uff0c\u4f7f\u5176\u80fd\u591f\u76f4\u63a5\u4ece\u7a7a\u95f4\u89c2\u6d4b\u4e2d\u8fdb\u884c\u6982\u7387\u53c2\u6570\u63a8\u65ad\u3002\u5728\u4f1a\u805a\u675f\u7535\u5b50\u884d\u5c04\uff08CBED\uff09\u53c2\u6570\u63a8\u65ad\u95ee\u9898\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8fd9\u662f\u4e00\u4e2a\u6750\u6599\u8868\u5f81\u4e2d\u7684\u591a\u53c2\u6570\u53cd\u95ee\u9898\u3002", "result": "CDI\u4ea7\u751f\u4e86\u6821\u51c6\u826f\u597d\u7684\u540e\u9a8c\u5206\u5e03\uff1a\u5bf9\u4e8e\u786e\u5b9a\u826f\u597d\u7684\u53c2\u6570\u6709\u7d27\u5bc6\u5206\u5e03\uff0c\u5bf9\u4e8e\u6a21\u7cca\u53c2\u6570\u6709\u9002\u5f53\u5bbd\u6cdb\u7684\u5206\u5e03\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6807\u51c6\u56de\u5f52\u65b9\u6cd5\u867d\u7136\u805a\u5408\u6307\u6807\u770b\u4f3c\u51c6\u786e\uff0c\u4f46\u901a\u8fc7\u9884\u6d4b\u8bad\u7ec3\u96c6\u5747\u503c\u6765\u63a9\u76d6\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "CDI\u6210\u529f\u4ece\u65f6\u95f4\u57df\u6269\u5c55\u5230\u7a7a\u95f4\u57df\uff0c\u4e3a\u7a33\u5065\u7684\u79d1\u5b66\u63a8\u65ad\u63d0\u4f9b\u4e86\u771f\u5b9e\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002"}}
{"id": "2601.17271", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17271", "abs": "https://arxiv.org/abs/2601.17271", "authors": ["Kun Huang", "Fang-Lue Zhang", "Neil Dodgson"], "title": "Cross360: 360\u00b0 Monocular Depth Estimation via Cross Projections Across Scales", "comment": "TIP, 12 pages", "summary": "360\u00b0 depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360\u00b0 field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360\u00b0 image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.", "AI": {"tldr": "Cross360\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684360\u00b0\u6df1\u5ea6\u4f30\u8ba1\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u5c40\u90e8\u5207\u9762\u6295\u5f71\u7279\u5f81\u548c\u5168\u5c40\u7b49\u8ddd\u67f1\u9762\u6295\u5f71\u7279\u5f81\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5168\u5c40\u8fde\u7eed\u6027\u548c\u5c40\u90e8\u4e00\u81f4\u6027\u5e73\u8861\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709360\u00b0\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u5168\u5c40\u8fde\u7eed\u6027\u548c\u5c40\u90e8\u4e00\u81f4\u6027\uff0c\u5c40\u90e8\u8865\u4e01\u7279\u5f81\u7f3a\u4e4f\u5168\u5c40\u611f\u77e5\u80fd\u529b\uff0c\u800c\u5168\u5c40\u8868\u793a\u65e0\u6cd5\u89e3\u51b3\u8865\u4e01\u8fb9\u754c\u5904\u7684\u7279\u5f81\u63d0\u53d6\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "\u63d0\u51faCross360\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u4ea4\u53c9\u6295\u5f71\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff0c\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u5c40\u90e8\u5207\u9762\u6295\u5f71\u7279\u5f81\u4e0e\u7b49\u8ddd\u67f1\u9762\u6295\u5f71\u7684360\u00b0\u89c6\u91ce\u5bf9\u9f50\uff1b2) \u6e10\u8fdb\u5f0f\u6ce8\u610f\u529b\u7279\u5f81\u805a\u5408\u6a21\u5757\uff0c\u9010\u6b65\u7ec6\u5316\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "result": "Cross360\u5728\u5927\u591a\u6570\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5b8c\u6574360\u00b0\u56fe\u50cf\u53ef\u7528\u7684\u573a\u666f\u4e2d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u51c6\u786e\u548c\u5168\u5c40\u4e00\u81f4\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\uff0cCross360\u6210\u529f\u89e3\u51b3\u4e86360\u00b0\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u5168\u5c40\u8fde\u7eed\u6027\u548c\u5c40\u90e8\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u7403\u5f62\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17966", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17966", "abs": "https://arxiv.org/abs/2601.17966", "authors": ["Naman Gupta", "Sophie Stephenson", "Chung Chi Yeung", "Wei Ting Wu", "Jeneile Luebke", "Kate Walsh", "Rahul Chatterjee"], "title": "\"Lighting The Way For Those Not Here\": How Technology Researchers Can Help Fight the Missing and Murdered Indigenous Relatives (MMIR) Crisis", "comment": null, "summary": "Indigenous peoples across Turtle Island (North America) face disproportionate rates of disappearance and murder, a \"genocide\" rooted in settler-colonial violence and systemic erasure. Technology plays a crucial role in the Missing and Murdered Indigenous Relatives (MMIR) crisis: perpetuating harm and impeding investigations, yet enabling advocacy and resistance. Communities utilize technologies such as AMBER alerts, news websites, social media groups, and campaigns (like #MMIW, #MMIWR, #NoMoreStolenSisters, and #NoMoreStolenDaughters) to mobilize searches, amplify awareness, and honor missing relatives. Yet, little research in HCI has critically examined technology's role in shaping the MMIR crisis by centering community voices. Through a large-scale study, we analyze 140 webpages to identify systemic, technological, and institutional barriers that hinder communities' efforts, while highlighting socio-technical actions that foster healing and safety. Finally, we amplify Indigenous voices by providing a dataset of stories that resist epistemic erasure, along with recommendations for HCI researchers to support Indigenous-led initiatives with cultural sensitivity, accountability, and self-determination.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u6280\u672f\u5728\u7f8e\u56fd\u539f\u4f4f\u6c11\u5931\u8e2a\u548c\u88ab\u8c0b\u6740\u5371\u673a\u4e2d\u7684\u53cc\u91cd\u4f5c\u7528\uff1a\u65e2\u5ef6\u7eed\u4f24\u5bb3\u53c8\u8d4b\u80fd\u62b5\u6297\uff0c\u901a\u8fc7\u5206\u6790140\u4e2a\u7f51\u9875\u8bc6\u522b\u7cfb\u7edf\u6027\u969c\u788d\u5e76\u63d0\u51fa\u652f\u6301\u539f\u4f4f\u6c11\u4e3b\u5bfc\u5021\u8bae\u7684\u5efa\u8bae\u3002", "motivation": "\u539f\u4f4f\u6c11\u5728\u5317\u7f8e\u9762\u4e34\u4e0d\u6210\u6bd4\u4f8b\u7684\u5931\u8e2a\u548c\u8c0b\u6740\u7387\uff0c\u8fd9\u79cd\"\u79cd\u65cf\u706d\u7edd\"\u690d\u6839\u4e8e\u6b96\u6c11\u66b4\u529b\u548c\u7cfb\u7edf\u6027\u62b9\u9664\u3002\u867d\u7136\u6280\u672f\u5728\u8fd9\u4e00\u5371\u673a\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u4f46HCI\u9886\u57df\u7f3a\u4e4f\u4ee5\u793e\u533a\u58f0\u97f3\u4e3a\u4e2d\u5fc3\u6279\u5224\u6027\u7814\u7a76\u6280\u672f\u5982\u4f55\u5851\u9020MMIR\u5371\u673a\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u7814\u7a76\u5206\u6790140\u4e2a\u7f51\u9875\uff0c\u8bc6\u522b\u7cfb\u7edf\u6027\u3001\u6280\u672f\u6027\u548c\u5236\u5ea6\u6027\u969c\u788d\uff0c\u540c\u65f6\u7a81\u51fa\u4fc3\u8fdb\u7597\u6108\u548c\u5b89\u5168\u7684\u793e\u4f1a\u6280\u672f\u884c\u52a8\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u4e86\u963b\u788d\u793e\u533a\u52aa\u529b\u7684\u969c\u788d\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u4fc3\u8fdb\u7597\u6108\u7684\u793e\u4f1a\u6280\u672f\u884c\u52a8\u3002\u63d0\u4f9b\u4e86\u62b5\u6297\u8ba4\u77e5\u62b9\u9664\u7684\u6545\u4e8b\u6570\u636e\u96c6\uff0c\u5e76\u4e3aHCI\u7814\u7a76\u8005\u63d0\u51fa\u652f\u6301\u539f\u4f4f\u6c11\u4e3b\u5bfc\u5021\u8bae\u7684\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u653e\u5927\u539f\u4f4f\u6c11\u58f0\u97f3\uff0c\u4e3aHCI\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4ee5\u6587\u5316\u654f\u611f\u6027\u3001\u95ee\u8d23\u5236\u548c\u81ea\u51b3\u6743\u652f\u6301\u539f\u4f4f\u6c11\u4e3b\u5bfc\u5021\u8bae\u7684\u5177\u4f53\u5efa\u8bae\uff0c\u5f3a\u8c03\u6280\u672f\u65e2\u53ef\u4ee5\u662f\u538b\u8feb\u5de5\u5177\u4e5f\u53ef\u4ee5\u662f\u62b5\u6297\u5de5\u5177\u3002"}}
{"id": "2601.17257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17257", "abs": "https://arxiv.org/abs/2601.17257", "authors": ["Javier Porras-Valenzuela", "Samar Hadou", "Alejandro Ribeiro"], "title": "A Constrained Optimization Perspective of Unrolled Transformers", "comment": null, "summary": "We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ea6\u675f\u4f18\u5316\u6846\u67b6\uff0c\u4f7fTransformer\u6a21\u578b\u50cf\u4f18\u5316\u4e0b\u964d\u7b97\u6cd5\u4e00\u6837\u5de5\u4f5c\uff0c\u901a\u8fc7\u5c42\u95f4\u4e0b\u964d\u7ea6\u675f\u548c\u539f\u59cb-\u5bf9\u5076\u8bad\u7ec3\u65b9\u6848\uff0c\u786e\u4fdd\u4e2d\u95f4\u8868\u793a\u5728\u671f\u671b\u4e0a\u5355\u8c03\u964d\u4f4e\u635f\u5931", "motivation": "\u4f20\u7edfTransformer\u8bad\u7ec3\u4f7f\u7528\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5185\u90e8\u884c\u4e3a\u7684\u7ea6\u675f\u63a7\u5236\u3002\u4f5c\u8005\u5e0c\u671b\u4f7fTransformer\u5177\u6709\u7c7b\u4f3c\u4f18\u5316\u4e0b\u964d\u7b97\u6cd5\u7684\u884c\u4e3a\u7279\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "method": "1. \u5728\u5c42\u95f4\u65bd\u52a0\u76ee\u6807\u51fd\u6570\u4e0b\u964d\u7ea6\u675f\uff1b2. \u7528\u539f\u59cb-\u5bf9\u5076\u8bad\u7ec3\u65b9\u6848\u66ff\u4ee3\u6807\u51c6ERM\uff1b3. \u786e\u4fdd\u4e2d\u95f4\u8868\u793a\u5728\u671f\u671b\u4e0a\u5355\u8c03\u964d\u4f4e\u635f\u5931\uff1b4. \u5e94\u7528\u4e8e\u5c55\u5f00\u5f0fTransformer\u67b6\u6784\u548c\u9884\u8bad\u7ec3Transformer", "result": "\u5728\u89c6\u9891\u53bb\u566a\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u7ea6\u675fTransformer\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6270\u52a8\u9c81\u68d2\u6027\u3001\u66f4\u9ad8\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u5e03\u5185\u6027\u80fd", "conclusion": "\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u6846\u67b6\u8bad\u7ec3Transformer\u4f7f\u5176\u5177\u6709\u4f18\u5316\u4e0b\u964d\u7b97\u6cd5\u884c\u4e3a\uff0c\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aTransformer\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2601.18405", "categories": ["cs.CY", "cs.HC", "cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.18405", "abs": "https://arxiv.org/abs/2601.18405", "authors": ["Sara Solarova", "Mat\u00fa\u0161 Mesar\u010d\u00edk", "Branislav Pecher", "Ivan Srba"], "title": "Beyond the Checkbox: Strengthening DSA Compliance Through Social Media Algorithmic Auditing", "comment": "2026 CHI Conference on Human Factors in Computing Systems", "summary": "Algorithms of online platforms are required under the Digital Services Act (DSA) to comply with specific obligations concerning algorithmic transparency, user protection and privacy. To verify compliance with these requirements, DSA mandates platforms to undergo independent audits. Little is known about current auditing practices and their effectiveness in ensuring such compliance. To this end, we bridge regulatory and technical perspectives by critically examining selected audit reports across three critical algorithmic-related provisions: restrictions on profiling minors, transparency in recommender systems, and limitations on targeted advertising using sensitive data. Our analysis shows significant inconsistencies in methodologies and lack of technical depth when evaluating AI-powered systems. To enhance the depth, scale, and independence of compliance assessments, we propose to employ algorithmic auditing -- a process of behavioural assessment of AI algorithms by means of simulating user behaviour, observing algorithm responses and analysing them for audited phenomena.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86DSA\u6846\u67b6\u4e0b\u5e73\u53f0\u7b97\u6cd5\u5ba1\u8ba1\u7684\u73b0\u72b6\uff0c\u53d1\u73b0\u5f53\u524d\u5ba1\u8ba1\u5b9e\u8df5\u5b58\u5728\u65b9\u6cd5\u4e0d\u4e00\u81f4\u548c\u6280\u672f\u6df1\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u91c7\u7528\u7b97\u6cd5\u5ba1\u8ba1\u65b9\u6cd5\u6765\u6539\u8fdb\u5408\u89c4\u8bc4\u4f30\u3002", "motivation": "\u300a\u6570\u5b57\u670d\u52a1\u6cd5\u6848\u300b(DSA)\u8981\u6c42\u5e73\u53f0\u7b97\u6cd5\u9075\u5b88\u900f\u660e\u5ea6\u3001\u7528\u6237\u4fdd\u62a4\u548c\u9690\u79c1\u4e49\u52a1\uff0c\u5e76\u9700\u63a5\u53d7\u72ec\u7acb\u5ba1\u8ba1\u3002\u7136\u800c\uff0c\u5f53\u524d\u5ba1\u8ba1\u5b9e\u8df5\u7684\u6709\u6548\u6027\u672a\u77e5\uff0c\u9700\u8981\u7814\u7a76\u73b0\u6709\u5ba1\u8ba1\u65b9\u6cd5\u662f\u5426\u80fd\u786e\u4fdd\u7b97\u6cd5\u5408\u89c4\u3002", "method": "\u901a\u8fc7\u6279\u5224\u6027\u5206\u6790\u9009\u5b9a\u7684\u5ba1\u8ba1\u62a5\u544a\uff0c\u4ece\u76d1\u7ba1\u548c\u6280\u672f\u89d2\u5ea6\u5ba1\u89c6\u4e09\u4e2a\u5173\u952e\u7b97\u6cd5\u76f8\u5173\u6761\u6b3e\uff1a\u672a\u6210\u5e74\u4eba\u753b\u50cf\u9650\u5236\u3001\u63a8\u8350\u7cfb\u7edf\u900f\u660e\u5ea6\u3001\u4f7f\u7528\u654f\u611f\u6570\u636e\u7684\u5b9a\u5411\u5e7f\u544a\u9650\u5236\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u5728\u8bc4\u4f30AI\u9a71\u52a8\u7cfb\u7edf\u65f6\u5b58\u5728\u663e\u8457\u7684\u65b9\u6cd5\u4e0d\u4e00\u81f4\u6027\u548c\u6280\u672f\u6df1\u5ea6\u7f3a\u4e4f\u3002\u5f53\u524d\u5ba1\u8ba1\u5b9e\u8df5\u96be\u4ee5\u6709\u6548\u786e\u4fdd\u7b97\u6cd5\u5408\u89c4\u3002", "conclusion": "\u4e3a\u589e\u5f3a\u5408\u89c4\u8bc4\u4f30\u7684\u6df1\u5ea6\u3001\u89c4\u6a21\u548c\u72ec\u7acb\u6027\uff0c\u5efa\u8bae\u91c7\u7528\u7b97\u6cd5\u5ba1\u8ba1\u65b9\u6cd5\u2014\u2014\u901a\u8fc7\u6a21\u62df\u7528\u6237\u884c\u4e3a\u3001\u89c2\u5bdf\u7b97\u6cd5\u54cd\u5e94\u5e76\u5206\u6790\u88ab\u5ba1\u8ba1\u73b0\u8c61\uff0c\u5bf9AI\u7b97\u6cd5\u8fdb\u884c\u884c\u4e3a\u8bc4\u4f30\u3002"}}
{"id": "2601.17998", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.17998", "abs": "https://arxiv.org/abs/2601.17998", "authors": ["Benjamin Mako Hill", "Aaron Shaw"], "title": "The Most Important Laboratory for Social Scientific and Computing Research in History", "comment": "Published in Wikipedia @ 20: Stories of an Incomplete Revolution, 2020, Edited by Joseph Reagle and Jackie Koerner. The MIT Press. ISBN electronic: 9780262360593", "summary": "Wikipedia's founders could not have dreamed they were creating the most important laboratory for social scientific and computing research in history but that is exactly what happened. Hill and Shaw take account of Wikipedia's enormous effect on academic scholarship", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u7ef4\u57fa\u767e\u79d1\u4f5c\u4e3a\u793e\u4f1a\u79d1\u5b66\u548c\u8ba1\u7b97\u7814\u7a76\u91cd\u8981\u5b9e\u9a8c\u5ba4\u7684\u5f71\u54cd", "motivation": "\u7ef4\u57fa\u767e\u79d1\u7684\u521b\u59cb\u4eba\u4eec\u53ef\u80fd\u6ca1\u6709\u610f\u8bc6\u5230\u4ed6\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5bf9\u5b66\u672f\u7814\u7a76\u4ea7\u751f\u6df1\u8fdc\u5f71\u54cd\u7684\u5e73\u53f0\uff0c\u4f46\u4e8b\u5b9e\u4e0a\u7ef4\u57fa\u767e\u79d1\u5df2\u6210\u4e3a\u793e\u4f1a\u79d1\u5b66\u548c\u8ba1\u7b97\u7814\u7a76\u7684\u91cd\u8981\u5b9e\u9a8c\u5ba4", "method": "Hill\u548cShaw\u901a\u8fc7\u5206\u6790\u7ef4\u57fa\u767e\u79d1\u5bf9\u5b66\u672f\u7814\u7a76\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u5176\u5728\u793e\u4f1a\u79d1\u5b66\u548c\u8ba1\u7b97\u7814\u7a76\u9886\u57df\u7684\u5730\u4f4d", "result": "\u7ef4\u57fa\u767e\u79d1\u5df2\u6210\u4e3a\u5386\u53f2\u4e0a\u6700\u91cd\u8981\u7684\u793e\u4f1a\u79d1\u5b66\u548c\u8ba1\u7b97\u7814\u7a76\u5b9e\u9a8c\u5ba4\u4e4b\u4e00\uff0c\u5bf9\u5b66\u672f\u7814\u7a76\u4ea7\u751f\u4e86\u5de8\u5927\u5f71\u54cd", "conclusion": "\u7ef4\u57fa\u767e\u79d1\u65e0\u610f\u4e2d\u521b\u5efa\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u5b66\u672f\u7814\u7a76\u5e73\u53f0\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u548c\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u673a\u4f1a\u548c\u8d44\u6e90"}}
{"id": "2601.17942", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17942", "abs": "https://arxiv.org/abs/2601.17942", "authors": ["Yu-Jie Yang", "Hung-Fu Chang", "Po-An Chen"], "title": "LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting", "comment": "29 pages, 22 figures", "summary": "Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SSEV\u548cReCAPAgent-SQL\u4e24\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u5230SQL\u8f6c\u6362\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u4f18\u5316\u548c\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u63d0\u5347SQL\u751f\u6210\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u4f01\u4e1a\u7ea7\u6570\u636e\u5e93\u573a\u666f\u4e2d\u3002", "motivation": "\u6587\u672c\u5230SQL\u6280\u672f\u867d\u7136\u964d\u4f4e\u4e86\u6570\u636e\u5206\u6790\u95e8\u69db\uff0c\u4f46\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u6b67\u4e49\u6027\u3001\u6a21\u5f0f\u94fe\u63a5\u590d\u6742\u6027\u3001SQL\u65b9\u8a00\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u9886\u57df\u7279\u5b9a\u7406\u89e3\u9700\u6c42\u7b49\u95ee\u9898\uff0c\u4f7f\u5f97\u751f\u6210\u51c6\u786eSQL\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7279\u522b\u662f\u5728\u4f01\u4e1a\u7ea7\u6570\u636e\u5e93\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u8fd9\u4e9b\u6311\u6218\u66f4\u52a0\u7a81\u51fa\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u4e3b\u8981\u65b9\u6cd5\uff1a1) SSEV\u7ba1\u9053\uff1a\u57fa\u4e8ePET-SQL\u6784\u5efa\u7684\u5355\u667a\u80fd\u4f53\u81ea\u4f18\u5316\u96c6\u6210\u6295\u7968\u7cfb\u7edf\uff0c\u65e0\u9700\u771f\u5b9e\u6570\u636e\uff0c\u7ed3\u5408\u81ea\u4f18\u5316\u4e0e\u52a0\u6743\u591a\u6570\u6295\u7968\u53ca\u5176\u968f\u673a\u53d8\u4f53\uff1b2) ReCAPAgent-SQL\u6846\u67b6\uff1a\u57fa\u4e8e\"\u4f18\u5316-\u6279\u5224-\u884c\u52a8-\u89c4\u5212\"\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\uff0c\u5305\u542b\u89c4\u5212\u3001\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u3001\u6279\u5224\u3001\u884c\u52a8\u751f\u6210\u3001\u81ea\u4f18\u5316\u3001\u6a21\u5f0f\u94fe\u63a5\u548c\u7ed3\u679c\u9a8c\u8bc1\u7b49\u4e13\u95e8\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0SQL\u9884\u6d4b\u7684\u8fed\u4ee3\u4f18\u5316\u3002", "result": "SSEV\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u8868\u73b0\uff1aSpider 1.0-Dev\u6267\u884c\u51c6\u786e\u738785.5%\uff0cSpider 1.0-Test 86.4%\uff0cBIRD-Dev 66.3%\u3002ReCAPAgent-SQL\u5728Spider 2.0-Lite\u524d100\u4e2a\u67e5\u8be2\u4e2d\u8fbe\u523031%\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u4f01\u4e1a\u573a\u666f\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7SSEV\u548cReCAPAgent-SQL\u6846\u67b6\uff0c\u4e3a\u5b9e\u9645\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u7684\u6587\u672c\u5230SQL\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4ee5\u66f4\u4f4e\u6210\u672c\u548c\u66f4\u9ad8\u6548\u7387\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u3002\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u65e5\u76ca\u590d\u6742\u7684\u4f01\u4e1a\u6570\u636e\u5e93\u548c\u771f\u5b9e\u4e16\u754c\u6587\u672c\u5230SQL\u4efb\u52a1\u3002"}}
{"id": "2601.17260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17260", "abs": "https://arxiv.org/abs/2601.17260", "authors": ["Marco Pollanen"], "title": "The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment", "comment": "10 Pages, 5 Figures", "summary": "Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $\u03b2$) yields progressively \"better\" behavior. We instead treat $\u03b2$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $\u03b2\\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $\u03b2$ induces capability losses that persist even after $\u03b2$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $\u03b2$ landscape rather than reliance on margins or aggregate benchmarks.", "AI": {"tldr": "DPO\u4e2d\u7684\u03b2\u53c2\u6570\u4e0d\u662f\u7b80\u5355\u7684\u5bf9\u9f50\u538b\u529b\u63a7\u5236\uff0c\u800c\u662f\u590d\u6742\u7684\u63a7\u5236\u53c2\u6570\uff0c\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u5728\u4e0d\u540c\u03b2\u503c\u4e0b\u8868\u73b0\u51fa\u622a\u7136\u4e0d\u540c\u7684\u80fd\u529b\u53d8\u5316\u6a21\u5f0f\uff0c\u504f\u597d\u8fb9\u754c\u53ef\u80fd\u4e0e\u63a8\u7406\u80fd\u529b\u8d1f\u76f8\u5173\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3aDPO\u4e2d\u7684\u03b2\u53c2\u6570\uff08\u5bf9\u9f50\u538b\u529b\uff09\u8d8a\u5927\uff0c\u6a21\u578b\u884c\u4e3a\u8d8a\u597d\uff0c\u4f46\u672c\u6587\u8d28\u7591\u8fd9\u4e00\u5047\u8bbe\uff0c\u8ba4\u4e3a\u03b2\u5e94\u88ab\u89c6\u4e3a\u63a7\u5236\u53c2\u6570\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4e0d\u540c\u03b2\u503c\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u5bf9\u4e09\u4e2a7B\u5f00\u6e90\u6a21\u578b\u5bb6\u65cf\uff08Mistral\u3001Llama\u3001Qwen\uff09\u5728\u56fa\u5b9aDPO\u914d\u65b9\u4e0b\u5bc6\u96c6\u626b\u63cf\u03b2\u53c2\u6570\uff0c\u4f7f\u7528\u903b\u8f91\u63a2\u9488\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\uff0c\u5206\u6790\u80fd\u529b\u53d8\u5316\u6a21\u5f0f\u3001\u504f\u597d\u8fb9\u754c\u4e0e\u80fd\u529b\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u8bad\u7ec3\u8def\u5f84\u7684\u6ede\u540e\u6548\u5e94\u3002", "result": "1. Mistral\u80fd\u529b\u5448\u5c16\u9510\u975e\u5355\u8c03\u53d8\u5316\uff0c\u4ec5\u5728\u03b2\u224810\u207b\u00b2\u7a84\u5e26\u5185\u903b\u8f91\u63a2\u9488\u8fb9\u754c\u4e3a\u6b63\uff1b2. \u4e0d\u540c\u67b6\u6784\u54cd\u5e94\u6a21\u5f0f\u4e0d\u540c\uff1aMistral\u6025\u5267\u91cd\u7ec4\uff0cLlama\u9009\u62e9\u6027\u53d8\u5316\uff0cQwen\u5e73\u6ed1\u6743\u8861\uff1b3. DPO\u504f\u597d\u8fb9\u754c\u4e0e\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u8d1f\u76f8\u5173\uff08Llama\u903b\u8f91\u4efb\u52a1Pearson r=-0.91\uff09\uff1b4. \u9ad8\u03b2\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u80fd\u529b\u635f\u5931\uff0c\u5373\u4f7f\u964d\u4f4e\u03b2\u4e5f\u65e0\u6cd5\u6062\u590d\uff08\u6ede\u540e\u6548\u5e94\uff09\u3002", "conclusion": "\u4e0d\u5e94\u4f9d\u8d56\u504f\u597d\u8fb9\u754c\u6216\u805a\u5408\u57fa\u51c6\u6765\u9009\u62e9\u6a21\u578b\uff0c\u800c\u5e94\u5728\u03b2\u53c2\u6570\u7a7a\u95f4\u4e2d\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\uff0c\u4e0d\u540c\u67b6\u6784\u5bf9DPO\u8bad\u7ec3\u54cd\u5e94\u6a21\u5f0f\u5dee\u5f02\u663e\u8457\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u80fd\u529b\u89e3\u6790\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2601.18127", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18127", "abs": "https://arxiv.org/abs/2601.18127", "authors": ["Judy Hanwen Shen", "Ken Liu", "Angelina Wang", "Sarah H. Cen", "Andy K. Zhang", "Caroline Meinhardt", "Daniel Zhang", "Kevin Klyman", "Rishi Bommasani", "Daniel E. Ho"], "title": "The Limits of AI Data Transparency Policy: Three Disclosure Fallacies", "comment": null, "summary": "Data transparency has emerged as a rallying cry for addressing concerns about AI: data quality, privacy, and copyright chief among them. Yet while these calls are crucial for accountability, current transparency policies often fall short of their intended aims. Similar to nutrition facts for food, policies aimed at nutrition facts for AI currently suffer from a limited consideration of research on effective disclosures. We offer an institutional perspective and identify three common fallacies in policy implementations of data disclosures for AI. First, many data transparency proposals exhibit a specification gap between the stated goals of data transparency and the actual disclosures necessary to achieve such goals. Second, reform attempts exhibit an enforcement gap between required disclosures on paper and enforcement to ensure compliance in fact. Third, policy proposals manifest an impact gap between disclosed information and meaningful changes in developer practices and public understanding. Informed by the social science on transparency, our analysis identifies affirmative paths for transparency that are effective rather than merely symbolic.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5f53\u524dAI\u6570\u636e\u900f\u660e\u5ea6\u653f\u7b56\u7684\u4e0d\u8db3\uff0c\u6307\u51fa\u7c7b\u4f3c\"\u8425\u517b\u6807\u7b7e\"\u7684\u62ab\u9732\u653f\u7b56\u5b58\u5728\u4e09\u5927\u7f3a\u9677\uff1a\u89c4\u8303\u5dee\u8ddd\u3001\u6267\u884c\u5dee\u8ddd\u548c\u5f71\u54cd\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u7684\u6709\u6548\u900f\u660e\u5ea6\u8def\u5f84\u3002", "motivation": "\u5f53\u524dAI\u6570\u636e\u900f\u660e\u5ea6\u653f\u7b56\u867d\u7136\u65e8\u5728\u89e3\u51b3\u6570\u636e\u8d28\u91cf\u3001\u9690\u79c1\u548c\u7248\u6743\u7b49\u95ee\u9898\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u5b9e\u73b0\u9884\u671f\u76ee\u6807\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u6790\u653f\u7b56\u5b9e\u65bd\u4e2d\u7684\u5e38\u89c1\u8c2c\u8bef\uff0c\u63d0\u51fa\u66f4\u6709\u6548\u7684\u900f\u660e\u5ea6\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5236\u5ea6\u89c6\u89d2\uff0c\u8bc6\u522bAI\u6570\u636e\u62ab\u9732\u653f\u7b56\u5b9e\u65bd\u4e2d\u7684\u4e09\u4e2a\u5e38\u89c1\u8c2c\u8bef\uff1a\u89c4\u8303\u5dee\u8ddd\uff08\u76ee\u6807\u4e0e\u5fc5\u8981\u62ab\u9732\u4e4b\u95f4\u7684\u8131\u8282\uff09\u3001\u6267\u884c\u5dee\u8ddd\uff08\u7eb8\u9762\u8981\u6c42\u4e0e\u5b9e\u9645\u5408\u89c4\u4e4b\u95f4\u7684\u5dee\u8ddd\uff09\u3001\u5f71\u54cd\u5dee\u8ddd\uff08\u62ab\u9732\u4fe1\u606f\u4e0e\u5b9e\u8d28\u6027\u6539\u53d8\u4e4b\u95f4\u7684\u5dee\u8ddd\uff09\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u900f\u660e\u5ea6\u653f\u7b56\u7684\u4e09\u5927\u7f3a\u9677\uff0c\u5e76\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u900f\u660e\u5ea6\u8def\u5f84\uff0c\u5f3a\u8c03\u900f\u660e\u5ea6\u5e94\u5177\u6709\u5b9e\u9645\u6548\u679c\u800c\u975e\u4ec5\u4ec5\u662f\u8c61\u5f81\u6027\u7684\u3002", "conclusion": "AI\u6570\u636e\u900f\u660e\u5ea6\u653f\u7b56\u9700\u8981\u8d85\u8d8a\u8c61\u5f81\u6027\u62ab\u9732\uff0c\u89e3\u51b3\u89c4\u8303\u3001\u6267\u884c\u548c\u5f71\u54cd\u4e09\u4e2a\u5c42\u9762\u7684\u5dee\u8ddd\uff0c\u624d\u80fd\u5b9e\u73b0\u771f\u6b63\u7684\u95ee\u8d23\u548c\u6539\u8fdb\u3002"}}
{"id": "2601.18027", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18027", "abs": "https://arxiv.org/abs/2601.18027", "authors": ["Chiyuan Fu", "Lyuhao Chen", "Yunze Xiao", "Weihao Xuan", "Carlos Busso", "Mona Diab"], "title": "Sentipolis: Emotion-Aware Agents for Social Simulations", "comment": null, "summary": "LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.", "AI": {"tldr": "Sentipolis\u6846\u67b6\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u60c5\u611f\u72b6\u6001\u7ba1\u7406\uff0c\u901a\u8fc7PAD\u60c5\u611f\u8868\u793a\u3001\u53cc\u901f\u60c5\u611f\u52a8\u6001\u548c\u60c5\u611f-\u8bb0\u5fc6\u8026\u5408\uff0c\u63d0\u5347\u793e\u4ea4\u6a21\u62df\u4e2d\u7684\u60c5\u611f\u8fde\u7eed\u6027\u548c\u771f\u5b9e\u6027\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u793e\u4ea4\u6a21\u62df\u4e2d\u5e38\u5c06\u60c5\u611f\u89c6\u4e3a\u77ac\u65f6\u7ebf\u7d22\uff0c\u5bfc\u81f4\u60c5\u611f\u9057\u5fd8\u548c\u957f\u671f\u8fde\u7eed\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u60c5\u611f\u72b6\u6001\u7ba1\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51faSentipolis\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u8fde\u7eed\u7684\u6109\u60a6-\u5524\u9192-\u652f\u914d\uff08PAD\uff09\u60c5\u611f\u8868\u793a\uff1b2\uff09\u53cc\u901f\u60c5\u611f\u52a8\u6001\u673a\u5236\uff1b3\uff09\u60c5\u611f\u4e0e\u8bb0\u5fc6\u8026\u5408\u7cfb\u7edf\u3002", "result": "\u5728\u6570\u5343\u6b21\u4ea4\u4e92\u4e2d\uff0cSentipolis\u63d0\u5347\u4e86\u60c5\u611f\u57fa\u7840\u884c\u4e3a\u3001\u6c9f\u901a\u80fd\u529b\u548c\u60c5\u611f\u8fde\u7eed\u6027\u3002\u6548\u679c\u6a21\u578b\u4f9d\u8d56\uff1a\u9ad8\u5bb9\u91cf\u6a21\u578b\u53ef\u4fe1\u5ea6\u63d0\u5347\uff0c\u5c0f\u6a21\u578b\u53ef\u80fd\u4e0b\u964d\uff1b\u60c5\u611f\u610f\u8bc6\u53ef\u80fd\u8f7b\u5fae\u964d\u4f4e\u793e\u4f1a\u89c4\u8303\u9075\u5b88\u5ea6\u3002", "conclusion": "Sentipolis\u652f\u6301\u7814\u7a76\u7d2f\u79ef\u793e\u4ea4\u52a8\u6001\u5982\u8054\u76df\u5f62\u6210\u548c\u5173\u7cfb\u6e10\u53d8\uff0c\u63ed\u793a\u4e86\u60c5\u611f\u9a71\u52a8\u884c\u4e3a\u4e0e\u793e\u4f1a\u89c4\u8303\u9075\u5b88\u4e4b\u95f4\u7684\u4eba\u7c7b\u5316\u5f20\u529b\uff0c\u4e3a\u793e\u4ea4\u6a21\u62df\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u60c5\u611f\u8fde\u7eed\u6027\u3002"}}
{"id": "2601.17261", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17261", "abs": "https://arxiv.org/abs/2601.17261", "authors": ["Wei Lin", "Yining Jiang", "Qingyu Song", "Qiao Xiang", "Hong Xu"], "title": "AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning", "comment": "21 pages in total, including 9 pages of main text, with 4 figures and 3 tables. This manuscript is submitted to arXiv", "summary": "Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.", "AI": {"tldr": "AGZO\u662f\u4e00\u79cd\u57fa\u4e8e\u6fc0\u6d3b\u5f15\u5bfc\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u524d\u5411\u4f20\u64ad\u4e2d\u7684\u6fc0\u6d3b\u7ed3\u6784\u4fe1\u606f\uff0c\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6270\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u9636\u4f18\u5316\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5404\u5411\u540c\u6027\u6270\u52a8\uff0c\u5ffd\u7565\u4e86\u524d\u5411\u4f20\u64ad\u4e2d\u53ef\u7528\u7684\u4e30\u5bcc\u7ed3\u6784\u4fe1\u606f\u3002\u4f5c\u8005\u53d1\u73b0\u7ebf\u6027\u5c42\u7684\u68af\u5ea6\u88ab\u9650\u5236\u5728\u5176\u8f93\u5165\u6fc0\u6d3b\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u8fd9\u4e3a\u6539\u8fdb\u96f6\u9636\u4f18\u5316\u63d0\u4f9b\u4e86\u5173\u952e\u6d1e\u5bdf\u3002", "method": "\u63d0\u51fa\u6fc0\u6d3b\u5f15\u5bfc\u7684\u96f6\u9636\u4f18\u5316\uff08AGZO\uff09\uff0c\u5728\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u52a8\u6001\u63d0\u53d6\u7d27\u51d1\u7684\u6fc0\u6d3b\u4fe1\u606f\u5b50\u7a7a\u95f4\uff0c\u5e76\u5c06\u6270\u52a8\u9650\u5236\u5728\u8fd9\u4e2a\u4f4e\u79e9\u5b50\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u4f30\u8ba1\u68af\u5ea6\u65b9\u5411\u3002", "result": "AGZO\u5728Qwen3\u548cPangu\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b83\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u9636\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u4e00\u9636\u5fae\u8c03\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5176\u4ed6\u96f6\u9636\u65b9\u6cd5\u51e0\u4e4e\u76f8\u540c\u7684\u5cf0\u503c\u5185\u5b58\u5360\u7528\u3002", "conclusion": "AGZO\u901a\u8fc7\u5229\u7528\u6fc0\u6d3b\u7ed3\u6784\u4fe1\u606f\u6539\u8fdb\u4e86\u96f6\u9636\u4f18\u5316\uff0c\u5728\u5185\u5b58\u53d7\u9650\u7684LLM\u5fae\u8c03\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u63a5\u8fd1\u4e00\u9636\u65b9\u6cd5\u6027\u80fd\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5185\u5b58\u9ad8\u6548\u5fae\u8c03\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.17315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17315", "abs": "https://arxiv.org/abs/2601.17315", "authors": ["Xiaoyang Li", "Runni Zhou"], "title": "ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading", "comment": "12 pages", "summary": "Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.", "AI": {"tldr": "ClinNet\u662f\u4e00\u4e2a\u7528\u4e8e\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u5206\u7ea7\u7684\u53ef\u4fe1\u8d56\u6846\u67b6\uff0c\u901a\u8fc7\u8bc1\u636e\u5e8f\u6570\u56de\u5f52\u65b9\u6cd5\u89e3\u51b3\u4f20\u7edf\u786e\u5b9a\u6027\u5206\u7c7b\u5ffd\u7565\u75be\u75c5\u8fde\u7eed\u8fdb\u5c55\u548c\u6807\u6ce8\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u7684X\u5149\u5f71\u50cf\u5206\u7ea7\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0d\u540c\u7ea7\u522b\u95f4\u5dee\u5f02\u7ec6\u5fae\u3001\u4e13\u5bb6\u6807\u6ce8\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u4e14\u75be\u75c5\u8fdb\u5c55\u5177\u6709\u56fa\u6709\u7684\u5e8f\u6570\u6027\u8d28\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c06\u5176\u89c6\u4e3a\u786e\u5b9a\u6027\u591a\u5206\u7c7b\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u75be\u75c5\u8fde\u7eed\u8fdb\u5c55\u548c\u6807\u6ce8\u4e0d\u786e\u5b9a\u6027\u3002", "method": "ClinNet\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) \u53cc\u8fb9\u4e0d\u5bf9\u79f0\u7f16\u7801\u5668\uff0c\u663e\u5f0f\u5efa\u6a21\u5185\u5916\u4fa7\u7ed3\u6784\u5dee\u5f02\uff1b(2) \u8bca\u65ad\u8bb0\u5fc6\u5e93\uff0c\u7ef4\u62a4\u7c7b\u522b\u539f\u578b\u4ee5\u7a33\u5b9a\u7279\u5f81\u8868\u793a\uff1b(3) \u57fa\u4e8e\u6b63\u6001\u9006\u4f3d\u9a6c\u5206\u5e03\u7684\u8bc1\u636e\u5e8f\u6570\u5934\uff0c\u8054\u5408\u4f30\u8ba1\u8fde\u7eedKL\u5206\u7ea7\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "result": "ClinNet\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e860.892\u7684\u4e8c\u6b21\u52a0\u6743Kappa\u548c0.768\u7684\u51c6\u786e\u7387\uff0c\u7edf\u8ba1\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5(p < 0.001)\u3002\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u6210\u529f\u8bc6\u522b\u5206\u5e03\u5916\u6837\u672c\u548c\u6f5c\u5728\u8bef\u8bca\u3002", "conclusion": "ClinNet\u901a\u8fc7\u8bc1\u636e\u5e8f\u6570\u56de\u5f52\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u819d\u5173\u8282\u9aa8\u5173\u8282\u708e\u5206\u7ea7\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5176\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\u4e3a\u5b89\u5168\u4e34\u5e8a\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.18156", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18156", "abs": "https://arxiv.org/abs/2601.18156", "authors": ["Anirban Mukherjee", "Hannah Hanwen Chang"], "title": "Beyond Pairwise Comparisons: A Distributional Test of Distinctiveness for Machine-Generated Works in Intellectual Property Law", "comment": null, "summary": "Key doctrines, including novelty (patent), originality (copyright), and distinctiveness (trademark), turn on a shared empirical question: whether a body of work is meaningfully distinct from a relevant reference class. Yet analyses typically operationalize this set-level inquiry using item-level evidence: pairwise comparisons among exemplars. That unit-of-analysis mismatch may be manageable for finite corpora of human-created works, where it can be bridged by ad hoc aggregations. But it becomes acute for machine-generated works, where the object of evaluation is not a fixed set of works but a generative process with an effectively unbounded output space. We propose a distributional alternative: a two-sample test based on maximum mean discrepancy computed on semantic embeddings to determine if two creative processes-whether human or machine-produce statistically distinguishable output distributions. The test requires no task-specific training-obviating the need for discovery of proprietary training data to characterize the generative process-and is sample-efficient, often detecting differences with as few as 5-10 images and 7-20 texts. We validate the framework across three domains: handwritten digits (controlled images), patent abstracts (text), and AI-generated art (real-world images). We reveal a perceptual paradox: even when human evaluators distinguish AI outputs from human-created art with only about 58% accuracy, our method detects distributional distinctiveness. Our results present evidence contrary to the view that generative models act as mere regurgitators of training data. Rather than producing outputs statistically indistinguishable from a human baseline-as simple regurgitation would predict-they produce outputs that are semantically human-like yet stochastically distinct, suggesting their dominant function is as a semantic interpolator within a learned latent space.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5dee\u5f02\u7684\u7edf\u8ba1\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u8fc7\u7a0b\uff08\u4eba\u7c7b\u6216\u673a\u5668\uff09\u662f\u5426\u4ea7\u751f\u7edf\u8ba1\u4e0a\u53ef\u533a\u5206\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9010\u9879\u6bd4\u8f83\u65b9\u6cd5\u5728\u5904\u7406\u673a\u5668\u751f\u6210\u65e0\u9650\u8f93\u51fa\u7a7a\u95f4\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u4ea7\u6743\u8bc4\u4f30\uff08\u4e13\u5229\u65b0\u9896\u6027\u3001\u7248\u6743\u539f\u521b\u6027\u3001\u5546\u6807\u72ec\u7279\u6027\uff09\u4f9d\u8d56\u4e8e\u9010\u9879\u6bd4\u8f83\u7684\u5b9e\u8bc1\u65b9\u6cd5\uff0c\u8fd9\u5728\u5904\u7406\u6709\u9650\u7684\u4eba\u7c7b\u521b\u4f5c\u4f5c\u54c1\u65f6\u53ef\u884c\uff0c\u4f46\u5bf9\u4e8e\u673a\u5668\u751f\u6210\u7684\u3001\u5177\u6709\u65e0\u9650\u8f93\u51fa\u7a7a\u95f4\u7684\u751f\u6210\u8fc7\u7a0b\u5b58\u5728\u4e25\u91cd\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u7684\u4e24\u6837\u672c\u68c0\u9a8c\u65b9\u6cd5\uff0c\u4f7f\u7528\u8bed\u4e49\u5d4c\u5165\u6765\u8ba1\u7b97\u4e24\u4e2a\u751f\u6210\u8fc7\u7a0b\u8f93\u51fa\u5206\u5e03\u7684\u7edf\u8ba1\u5dee\u5f02\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\uff0c\u6837\u672c\u6548\u7387\u9ad8\uff0c\u4ec5\u9700\u5c11\u91cf\u6837\u672c\uff085-10\u5f20\u56fe\u50cf\uff0c7-20\u4e2a\u6587\u672c\uff09\u5373\u53ef\u68c0\u6d4b\u5dee\u5f02\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u6846\u67b6\uff1a\u624b\u5199\u6570\u5b57\uff08\u53d7\u63a7\u56fe\u50cf\uff09\u3001\u4e13\u5229\u6458\u8981\uff08\u6587\u672c\uff09\u548cAI\u751f\u6210\u827a\u672f\uff08\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\uff09\u3002\u53d1\u73b0\u611f\u77e5\u6096\u8bba\uff1a\u5373\u4f7f\u4eba\u7c7b\u8bc4\u4f30\u8005\u533a\u5206AI\u8f93\u51fa\u4e0e\u4eba\u7c7b\u827a\u672f\u7684\u51c6\u786e\u7387\u4ec5\u4e3a58%\uff0c\u8be5\u65b9\u6cd5\u4ecd\u80fd\u68c0\u6d4b\u5230\u5206\u5e03\u72ec\u7279\u6027\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u5e76\u975e\u7b80\u5355\u5730\u590d\u5236\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u662f\u4ea7\u751f\u8bed\u4e49\u4e0a\u7c7b\u4f3c\u4eba\u7c7b\u4f46\u5728\u7edf\u8ba1\u4e0a\u53ef\u533a\u5206\u7684\u8f93\u51fa\uff0c\u4e3b\u8981\u529f\u80fd\u662f\u5728\u5b66\u4e60\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8bed\u4e49\u63d2\u503c\uff0c\u8fd9\u4e3a\u77e5\u8bc6\u4ea7\u6743\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u5e03\u89c6\u89d2\u3002"}}
{"id": "2601.18061", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18061", "abs": "https://arxiv.org/abs/2601.18061", "authors": ["Kiana Jafari", "Paul Ulrich Nikolaus Rust", "Duncan Eddy", "Robbie Fraser", "Nina Vasan", "Darja Djordjevic", "Akanksha Dadlani", "Max Lamparth", "Eugenia Kim", "Mykel Kochenderfer"], "title": "Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing", "comment": "17 pages, 7 pages of appendix, 21 tables", "summary": "Learning from human feedback~(LHF) assumes that expert judgments, appropriately aggregated, yield valid ground truth for training and evaluating AI systems. We tested this assumption in mental health, where high safety stakes make expert consensus essential. Three certified psychiatrists independently evaluated LLM-generated responses using a calibrated rubric. Despite similar training and shared instructions, inter-rater reliability was consistently poor ($ICC$ $0.087$--$0.295$), falling below thresholds considered acceptable for consequential assessment. Disagreement was highest on the most safety-critical items. Suicide and self-harm responses produced greater divergence than any other category, and was systematic rather than random. One factor yielded negative reliability (Krippendorff's $\u03b1= -0.203$), indicating structured disagreement worse than chance. Qualitative interviews revealed that disagreement reflects coherent but incompatible individual clinical frameworks, safety-first, engagement-centered, and culturally-informed orientations, rather than measurement error. By demonstrating that experts rely on holistic risk heuristics rather than granular factor discrimination, these findings suggest that aggregated labels function as arithmetic compromises that effectively erase grounded professional philosophies. Our results characterize expert disagreement in safety-critical AI as a sociotechnical phenomenon where professional experience introduces sophisticated layers of principled divergence. We discuss implications for reward modeling, safety classification, and evaluation benchmarks, recommending that practitioners shift from consensus-based aggregation to alignment methods that preserve and learn from expert disagreement.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u4e13\u5bb6\u8bc4\u4f30AI\u751f\u6210\u56de\u590d\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u5206\u6b67\uff0c\u5c24\u5176\u662f\u5728\u81ea\u6740\u81ea\u4f24\u7b49\u5b89\u5168\u5173\u952e\u95ee\u9898\u4e0a\uff0c\u4e13\u5bb6\u95f4\u4e00\u81f4\u6027\u6781\u4f4e\uff0c\u5206\u6b67\u6e90\u4e8e\u4e0d\u540c\u7684\u4e34\u5e8a\u6846\u67b6\u800c\u975e\u6d4b\u91cf\u8bef\u5dee\u3002", "motivation": "\u9a8c\u8bc1\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u5b66\u4e60(LHF)\u7684\u57fa\u672c\u5047\u8bbe\u2014\u2014\u4e13\u5bb6\u5224\u65ad\u7ecf\u8fc7\u9002\u5f53\u805a\u5408\u540e\u80fd\u63d0\u4f9b\u6709\u6548\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u771f\u5b9e\u6807\u7b7e\u3002\u5728\u5fc3\u7406\u5065\u5eb7\u8fd9\u4e00\u9ad8\u5b89\u5168\u98ce\u9669\u7684\u9886\u57df\uff0c\u4e13\u5bb6\u5171\u8bc6\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u4e09\u4f4d\u8ba4\u8bc1\u7cbe\u795e\u79d1\u533b\u751f\u4f7f\u7528\u6821\u51c6\u7684\u8bc4\u5206\u6807\u51c6\u72ec\u7acb\u8bc4\u4f30LLM\u751f\u6210\u7684\u56de\u590d\uff0c\u6d4b\u91cf\u4e13\u5bb6\u95f4\u4fe1\u5ea6(ICC)\uff0c\u8fdb\u884c\u5b9a\u6027\u8bbf\u8c08\u5206\u6790\u5206\u6b67\u539f\u56e0\u3002", "result": "\u4e13\u5bb6\u95f4\u4fe1\u5ea6\u6781\u4f4e(ICC 0.087-0.295)\uff0c\u4f4e\u4e8e\u53ef\u63a5\u53d7\u9608\u503c\uff1b\u81ea\u6740\u81ea\u4f24\u7c7b\u56de\u590d\u5206\u6b67\u6700\u5927\uff1b\u4e00\u4e2a\u56e0\u7d20\u7684\u4fe1\u5ea6\u751a\u81f3\u4e3a\u8d1f\u503c(Krippendorff's \u03b1=-0.203)\uff1b\u5206\u6b67\u6e90\u4e8e\u5b89\u5168\u4f18\u5148\u3001\u53c2\u4e0e\u4e3a\u4e2d\u5fc3\u3001\u6587\u5316\u654f\u611f\u7b49\u4e0d\u540c\u7684\u4e34\u5e8a\u6846\u67b6\u3002", "conclusion": "\u4e13\u5bb6\u5206\u6b67\u662f\u539f\u5219\u6027\u5dee\u5f02\u800c\u975e\u6d4b\u91cf\u8bef\u5dee\uff0c\u805a\u5408\u6807\u7b7e\u4f1a\u62b9\u6740\u4e13\u4e1a\u54f2\u5b66\uff1b\u5efa\u8bae\u4ece\u57fa\u4e8e\u5171\u8bc6\u7684\u805a\u5408\u8f6c\u5411\u80fd\u591f\u4fdd\u7559\u548c\u5b66\u4e60\u4e13\u5bb6\u5206\u6b67\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2601.17323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17323", "abs": "https://arxiv.org/abs/2601.17323", "authors": ["Debang Li", "Zhengcong Fei", "Tuanhui Li", "Yikun Dou", "Zheng Chen", "Jiangping Yang", "Mingyuan Fan", "Jingtao Xu", "Jiahua Wang", "Baoxuan Gu", "Mingshan Chang", "Yuqiang Xie", "Binjie Mao", "Youqiang Zhang", "Nuo Pang", "Hao Zhang", "Yuzhe Jin", "Zhiheng Xu", "Dixuan Lin", "Guibin Chen", "Yahui Zhou"], "title": "SkyReels-V3 Technique Report", "comment": null, "summary": "Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.\n  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.", "AI": {"tldr": "SkyReels-V3\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u7edf\u4e00\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u7684\u6761\u4ef6\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u652f\u6301\u56fe\u50cf\u5230\u89c6\u9891\u5408\u6210\u3001\u89c6\u9891\u6269\u5c55\u548c\u97f3\u9891\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u4e09\u79cd\u6838\u5fc3\u8303\u5f0f\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u662f\u6784\u5efa\u4e16\u754c\u6a21\u578b\u7684\u57fa\u7840\uff0c\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u63a8\u7406\u662f\u80fd\u529b\u7684\u5173\u952e\u6d4b\u8bd5\u3002\u4e3a\u4e86\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u652f\u6301\u591a\u79cd\u751f\u6210\u8303\u5f0f\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u6269\u6563Transformer\u7684\u7edf\u4e00\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u3002\u5305\u62ec\uff1a1\uff09\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u91c7\u7528\u8de8\u5e27\u914d\u5bf9\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u8bed\u4e49\u91cd\u5199\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u7ed3\u5408\u56fe\u50cf\u89c6\u9891\u6df7\u5408\u8bad\u7ec3\u548c\u591a\u5206\u8fa8\u7387\u8054\u5408\u4f18\u5316\uff1b2\uff09\u89c6\u9891\u6269\u5c55\u6a21\u578b\u6574\u5408\u65f6\u7a7a\u4e00\u81f4\u6027\u5efa\u6a21\u548c\u5927\u89c4\u6a21\u89c6\u9891\u7406\u89e3\uff1b3\uff09\u8bed\u97f3\u5934\u50cf\u6a21\u578b\u8bad\u7ec3\u9996\u5c3e\u5e27\u63d2\u5165\u6a21\u5f0f\u5e76\u91cd\u6784\u5173\u952e\u5e27\u63a8\u7406\u8303\u5f0f\u3002", "result": "SkyReels-V3\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u7279\u5b9a\u65b9\u9762\u6307\u6807\u4e0a\u8fbe\u5230\u6216\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u63a5\u8fd1\u9886\u5148\u7684\u95ed\u6e90\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "SkyReels-V3\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4e09\u79cd\u6838\u5fc3\u89c6\u9891\u751f\u6210\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u4f18\u5316\u4e86\u97f3\u89c6\u9891\u540c\u6b65\uff0c\u4e3a\u6784\u5efa\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2601.18234", "categories": ["cs.CY", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18234", "abs": "https://arxiv.org/abs/2601.18234", "authors": ["Abdulaziz AlDakheel", "Ali Alshehre", "Esraa Alamoudi", "Moslim AlKhabbaz", "Ahmed Aljohani", "Raed Alharbi"], "title": "Generative AI in Saudi Arabia: A National Survey of Adoption, Risks, and Public Perceptions", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is rapidly becoming embedded in Saudi Arabia's digital transformation under Vision 2030, yet public awareness, adoption, and concerns surrounding these tools remain underexplored. This study provides an early snapshot of GenAI engagement among Saudi nationals. Using a nationwide survey of 330 participants across regions, age groups, and employment sectors, we examine seven dimensions of GenAI use: awareness and understanding, adoption patterns, perceived impacts, training needs, risks and barriers, data-sharing behaviors, and future expectations. Findings show that 93% of respondents actively use GenAI primarily for text-based tasks, while more advanced uses such as programming or multimodal generation are less common. Despite the prevalence of use, overall awareness and conceptual understanding remain uneven, with many reporting limited technical knowledge. Participants recognize GenAI's benefits for productivity, work quality, and understanding complex information, yet caution that sustained reliance may undermine critical thinking and key professional skills. Trust in AI-generated outputs remains cautious, with widespread concerns about privacy, misinformation, and ethical misuse, including potential job displacement. Respondents show strong interest in structured GenAI training that combines foundational skills, domain-specific applications, and clear guidance on privacy, ethics, and responsible use. These results establish a baseline for GenAI engagement in Saudi Arabia and highlight priorities for policymakers and developers: expanding AI literacy, ensuring culturally and linguistically aligned GenAI solutions, and strengthening frameworks for privacy and responsible deployment.", "AI": {"tldr": "\u6c99\u7279\u963f\u62c9\u4f2f\u5728Vision 2030\u6846\u67b6\u4e0b\u79ef\u6781\u63a8\u5e7f\u751f\u6210\u5f0fAI\uff0c\u4f46\u516c\u4f17\u8ba4\u77e5\u3001\u91c7\u7528\u60c5\u51b5\u548c\u62c5\u5fe7\u4ecd\u9700\u6df1\u5165\u7814\u7a76\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5168\u56fd330\u4eba\u8c03\u67e5\u53d1\u73b0\uff0c93%\u53d7\u8bbf\u8005\u4f7f\u7528GenAI\u8fdb\u884c\u6587\u672c\u4efb\u52a1\uff0c\u4f46\u9ad8\u7ea7\u5e94\u7528\u8f83\u5c11\uff1b\u516c\u4f17\u8ba4\u77e5\u4e0d\u5747\u8861\uff0c\u5bf9\u9690\u79c1\u3001\u4f26\u7406\u548c\u5c31\u4e1a\u5f71\u54cd\u5b58\u5728\u62c5\u5fe7\uff0c\u540c\u65f6\u5f3a\u70c8\u9700\u8981\u7ed3\u6784\u5316\u57f9\u8bad\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u6c99\u7279\u963f\u62c9\u4f2f\u7684\u6570\u5b57\u5316\u8f6c\u578b\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u516c\u4f17\u5bf9GenAI\u5de5\u5177\u7684\u8ba4\u77e5\u3001\u91c7\u7528\u548c\u62c5\u5fe7\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002Vision 2030\u80cc\u666f\u4e0b\uff0c\u4e86\u89e3\u516c\u4f17\u5bf9GenAI\u7684\u53c2\u4e0e\u60c5\u51b5\u5bf9\u653f\u7b56\u5236\u5b9a\u548c\u6280\u672f\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5168\u56fd\u6027\u8c03\u67e5\u65b9\u6cd5\uff0c\u8986\u76d6\u6c99\u7279\u5404\u5730\u533a\u3001\u5e74\u9f84\u7ec4\u548c\u5c31\u4e1a\u90e8\u95e8\u7684330\u540d\u53c2\u4e0e\u8005\u3002\u7814\u7a76\u8003\u5bdf\u4e86\u4e03\u4e2a\u7ef4\u5ea6\uff1a\u8ba4\u77e5\u4e0e\u7406\u89e3\u3001\u91c7\u7528\u6a21\u5f0f\u3001\u611f\u77e5\u5f71\u54cd\u3001\u57f9\u8bad\u9700\u6c42\u3001\u98ce\u9669\u4e0e\u969c\u788d\u3001\u6570\u636e\u5171\u4eab\u884c\u4e3a\u548c\u672a\u6765\u671f\u671b\u3002", "result": "93%\u53d7\u8bbf\u8005\u79ef\u6781\u4f7f\u7528GenAI\uff0c\u4e3b\u8981\u7528\u4e8e\u6587\u672c\u4efb\u52a1\uff0c\u7f16\u7a0b\u6216\u591a\u6a21\u6001\u751f\u6210\u7b49\u9ad8\u7ea7\u5e94\u7528\u8f83\u5c11\u3002\u8ba4\u77e5\u6c34\u5e73\u4e0d\u5747\u8861\uff0c\u6280\u672f\u77e5\u8bc6\u6709\u9650\u3002\u53d7\u8bbf\u8005\u8ba4\u53efGenAI\u5bf9\u751f\u4ea7\u529b\u548c\u5de5\u4f5c\u8d28\u91cf\u7684\u63d0\u5347\uff0c\u4f46\u62c5\u5fe7\u5bf9\u6279\u5224\u6027\u601d\u7ef4\u548c\u4e13\u4e1a\u6280\u80fd\u7684\u5f71\u54cd\u3002\u5bf9AI\u8f93\u51fa\u6301\u8c28\u614e\u6001\u5ea6\uff0c\u666e\u904d\u5173\u6ce8\u9690\u79c1\u3001\u9519\u8bef\u4fe1\u606f\u548c\u4f26\u7406\u6ee5\u7528\u95ee\u9898\uff0c\u5305\u62ec\u6f5c\u5728\u5c31\u4e1a\u5f71\u54cd\u3002\u53d7\u8bbf\u8005\u5bf9\u7ed3\u5408\u57fa\u7840\u6280\u80fd\u3001\u9886\u57df\u5e94\u7528\u53ca\u9690\u79c1\u4f26\u7406\u6307\u5bfc\u7684\u7ed3\u6784\u5316\u57f9\u8bad\u8868\u73b0\u51fa\u5f3a\u70c8\u5174\u8da3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6c99\u7279\u963f\u62c9\u4f2f\u7684GenAI\u53c2\u4e0e\u5efa\u7acb\u4e86\u57fa\u51c6\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4f18\u5148\u4e8b\u9879\uff1a\u6269\u5927AI\u7d20\u517b\u3001\u786e\u4fdd\u6587\u5316\u8bed\u8a00\u9002\u914d\u7684GenAI\u89e3\u51b3\u65b9\u6848\u3001\u52a0\u5f3a\u9690\u79c1\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u7684\u6846\u67b6\u5efa\u8bbe\u3002"}}
{"id": "2601.18067", "categories": ["cs.AI", "cs.NE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18067", "abs": "https://arxiv.org/abs/2601.18067", "authors": ["Wei-Po Hsin", "Ren-Hao Deng", "Yao-Ting Hsieh", "En-Ming Huang", "Shih-Hao Hung"], "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization", "comment": "17 pages, 6 figures, 8 tables", "summary": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.", "AI": {"tldr": "EvolVE\u6846\u67b6\u901a\u8fc7\u591a\u79cd\u8fdb\u5316\u7b56\u7565\u548c\u7ed3\u6784\u5316\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\uff0c\u5728\u82af\u7247\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u529f\u80fd\u6b63\u786e\u6027\u548cPPA\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "Verilog\u8bbe\u8ba1\u6d41\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u9700\u8981\u5927\u91cf\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u987a\u5e8f\u63a8\u7406\u7279\u6027\uff0c\u96be\u4ee5\u6355\u6349\u786c\u4ef6\u7cfb\u7edf\u7684\u4e25\u683c\u5f62\u5f0f\u903b\u8f91\u548c\u5e76\u53d1\u7279\u6027\u3002", "method": "\u63d0\u51faEvolVE\u6846\u67b6\uff0c\u5206\u6790\u591a\u79cd\u8fdb\u5316\u7b56\u7565\uff1a\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7528\u4e8e\u6700\u5927\u5316\u529f\u80fd\u6b63\u786e\u6027\uff0c\u60f3\u6cd5\u5f15\u5bfc\u7cbe\u70bc\uff08IGR\uff09\u7528\u4e8e\u4f18\u5316\uff1b\u91c7\u7528\u7ed3\u6784\u5316\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\uff08STG\uff09\u52a0\u901f\u8fdb\u5316\u8fc7\u7a0b\uff1b\u5f15\u5165IC-RTL\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "\u5728VerilogEval v2\u4e0a\u8fbe\u523098.1%\uff0cRTLLM v2\u4e0a\u8fbe\u523092%\uff1b\u5728\u5de5\u4e1a\u7ea7IC-RTL\u5957\u4ef6\u4e0a\u8d85\u8d8a\u7ade\u8d5b\u53c2\u4e0e\u8005\u53c2\u8003\u5b9e\u73b0\uff0cHuffman\u7f16\u7801\u7684PPA\u4e58\u79ef\u964d\u4f4e66%\uff0c\u6240\u6709\u95ee\u9898\u7684\u51e0\u4f55\u5e73\u5747\u964d\u4f4e17%\u3002", "conclusion": "EvolVE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u8fdb\u5316\u7b56\u7565\u548c\u7ed3\u6784\u5316\u6d4b\u8bd5\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u6311\u6218\uff0c\u5728\u529f\u80fd\u6b63\u786e\u6027\u548c\u4f18\u5316\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2601.17275", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17275", "abs": "https://arxiv.org/abs/2601.17275", "authors": ["Lianlei Shan", "Han Chen", "Yixuan Wang", "Zhenjie Liu", "Wei Li"], "title": "Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning", "comment": "12 pages,", "summary": "While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \\textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \\textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.", "AI": {"tldr": "DLR\u662f\u4e00\u4e2a\u6f5c\u5728\u7a7a\u95f4\u53cc\u5411\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u6210\u672c\u4ece\u6602\u8d35\u7684token\u7ea7\u5e8f\u5217\u751f\u6210\u8f6c\u79fb\u5230\u8fde\u7eed\u6f5c\u5728\u6d41\u5f62\uff0c\u901a\u8fc7\u51bb\u7ed3\u4e3b\u6a21\u578b\u53c2\u6570\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6536\u655b\u548c\u66f4\u957f\u63a8\u7406\u94fe\u652f\u6301\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u65f6\u5f80\u5f80\u53ea\u662f\"\u7edf\u8ba1\u62df\u5408\"\u800c\u975e\u7cfb\u7edf\u903b\u8f91\u63a8\u7406\u3002\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u5f15\u5165\"\u5148\u601d\u8003\u540e\u8bf4\u8bdd\"\u8303\u5f0f\uff0c\u4f46\u5728\u9ad8\u7ef4\u79bb\u6563token\u7a7a\u95f4\u4e2d\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u3001\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u9ad8\u548c\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\u4e09\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faDeepLatent Reasoning (DLR)\u6846\u67b6\uff1a1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91c7\u6837K\u4e2a\u63a8\u7406\u94fe\u7f16\u7801\uff1b2) \u901a\u8fc7\u57fa\u4e8e\u6b63\u786e\u6027\u548c\u683c\u5f0f\u7684\u53cc\u91cd\u5956\u52b1\u673a\u5236\u7b5b\u9009\u9ad8\u4ef7\u503c\u6f5c\u5728\u8f68\u8ff9\uff1b3) \u4ec5\u5c06\u7b5b\u9009\u540e\u7684\u8f68\u8ff9\u8f93\u5165\u51bb\u7ed3\u4e3b\u6a21\u578b\u8fdb\u884c\u5355\u6b21\u89e3\u7801\uff1b4) \u8bbe\u8ba1\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5b9a\u5411\u63a2\u7d22\u3002", "result": "\u5728\u53ef\u6bd4\u8f83\u7684GPU\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cDLR\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u6536\u655b\uff0c\u652f\u6301\u66f4\u957f\u7684\u63a8\u7406\u94fe\uff0c\u4fc3\u8fdb\u4e86\u63a8\u7406\u80fd\u529b\u7684\u53ef\u6301\u7eed\u79ef\u7d2f\uff0c\u4e3aLLMs\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u8def\u5f84\u3002", "conclusion": "DLR\u901a\u8fc7\u5c06\u5f3a\u5316\u5b66\u4e60\u4ece\u79bb\u6563token\u7a7a\u95f4\u8f6c\u79fb\u5230\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfRL\u5728LLMs\u4e2d\u7684\u4e09\u5927\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u540c\u65f6\u907f\u514d\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2601.18119", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18119", "abs": "https://arxiv.org/abs/2601.18119", "authors": ["Jing Ye", "Yiwen Duan", "Yonghong Yu", "Victor Ma", "Yang Gao", "Xing Chen"], "title": "Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?", "comment": null, "summary": "SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.\n  OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.\n  Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.", "AI": {"tldr": "OurBench\u662f\u9996\u4e2a\u4f01\u4e1a\u7ea7SQL\u63a8\u7406\u4e0e\u8c03\u8bd5\u57fa\u51c6\uff0c\u5305\u542b469\u4e2a\u8bed\u6cd5\u9519\u8bef\u67e5\u8be2\u548c516\u4e2a\u8bed\u4e49\u9519\u8bef\u67e5\u8be2\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u6ce8\u5165\u771f\u5b9e\u9519\u8bef\uff0c\u8bc4\u4f30\u663e\u793a\u5f53\u524dLLMs\u5728\u590d\u6742SQL\u8c03\u8bd5\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec536.46%\uff09", "motivation": "\u4f01\u4e1a\u6570\u636e\u5de5\u7a0b\u4e2dSQL\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5373\u4f7f\u662f\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u53d1\u8005\u548c\u5148\u8fdb\u7684\u6587\u672c\u5230SQL LLMs\u4e5f\u96be\u4ee5\u4e00\u6b21\u6027\u751f\u6210\u5b8c\u5168\u6b63\u786e\u7684SQL\u4ee3\u7801\uff0c\u901a\u5e38\u9700\u8981\u591a\u6b21\u8c03\u8bd5\u8fed\u4ee3\u3002\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u4f01\u4e1a\u7ea7SQL\u63a8\u7406\u548c\u8c03\u8bd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faOurBench\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1) \u81ea\u52a8\u5316\u6784\u5efa\u5de5\u4f5c\u6d41\uff0c\u4f7f\u7528\u9006\u5411\u5de5\u7a0b\u5728\u5927\u89c4\u6a21SQL\u4ee3\u7801\u4e2d\u7cfb\u7edf\u6027\u5730\u6ce8\u5165\u771f\u5b9e\u9519\u8bef\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u548c\u591a\u6837\u5316\u7684\u57fa\u51c6\u751f\u6210\uff1b(2) \u9488\u5bf9\u4f01\u4e1a\u73af\u5883\u7684\u514d\u6267\u884c\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u4f9b\u5feb\u901f\u3001\u51c6\u786e\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u8bc4\u4f30\u3002", "result": "OurBench\u5305\u542b469\u4e2aOurBenchSyn\u67e5\u8be2\uff08\u8bed\u6cd5\u9519\u8bef\uff0c\u6709\u660e\u786e\u9519\u8bef\u4fe1\u606f\uff09\u548c516\u4e2aOurBenchSem\u67e5\u8be2\uff08\u8bed\u4e49\u9519\u8bef\uff0c\u4ee3\u7801\u672a\u80fd\u6ee1\u8db3\u7528\u6237\u610f\u56fe\uff09\u3002\u67e5\u8be2\u9ad8\u5ea6\u590d\u6742\uff0c\u5e73\u5747\u8d85\u8fc7140\u884c\uff0c\u5177\u6709\u6df1\u800c\u5e7f\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\u3002\u8bc4\u4f30\u8fd130\u4e2aLLMs\u663e\u793a\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1a\u6700\u4f73\u6a21\u578bClaude-4-Sonnet\u5728OurBenchSyn\u4e0a\u4ec5\u8fbe\u523036.46%\u51c6\u786e\u7387\uff0c\u5728OurBenchSem\u4e0a\u4e3a32.17%\uff0c\u5927\u591a\u6570\u6a21\u578b\u5f97\u5206\u4f4e\u4e8e20%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u4f01\u4e1a\u7ea7SQL\u8c03\u8bd5\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u4e86\u56db\u79cd\u89e3\u51b3\u65b9\u6848\u7b56\u7565\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86LLMs\u5728\u4f01\u4e1aSQL\u8c03\u8bd5\u65b9\u9762\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2601.17301", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17301", "abs": "https://arxiv.org/abs/2601.17301", "authors": ["Yunhui Liu", "Tieke He", "Yongchao Liu", "Can Yi", "Hong Jin", "Chuntao Hong"], "title": "Tabular Foundation Models are Strong Graph Anomaly Detectors", "comment": "Accepted by WWW 2026 (Short Paper)", "summary": "Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a \"one model per dataset\" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a \"one-for-all\" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by \"flattening\" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.", "AI": {"tldr": "TFM4GAD\u6846\u67b6\u5c06\u8868\u683c\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u56fe\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u6241\u5e73\u5316\u548c\u7279\u5f81\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u901a\u7528\u5f02\u5e38\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u4e13\u7528GAD\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\"\u4e00\u4e2a\u6a21\u578b\u5bf9\u5e94\u4e00\u4e2a\u6570\u636e\u96c6\"\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6570\u636e\u9700\u6c42\u5927\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8de8\u4e0d\u540c\u56fe\u6570\u636e\u96c6\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51faTFM4GAD\u6846\u67b6\uff0c\u5c06\u8868\u683c\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u56fe\u5f02\u5e38\u68c0\u6d4b\u3002\u901a\u8fc7\"\u6241\u5e73\u5316\"\u56fe\u7ed3\u6784\uff0c\u6784\u5efa\u589e\u5f3a\u7279\u5f81\u8868\uff0c\u5305\u542b\u539f\u59cb\u8282\u70b9\u7279\u5f81\u3001\u62c9\u666e\u62c9\u65af\u5d4c\u5165\u3001\u5c40\u90e8\u548c\u5168\u5c40\u7ed3\u6784\u7279\u5f81\u4ee5\u53ca\u5f02\u5e38\u654f\u611f\u90bb\u57df\u805a\u5408\u3002\u4f7f\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u5728\u5b8c\u5168\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u4e0b\u5904\u7406\u589e\u5f3a\u7279\u5f81\u8868\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4e0d\u540c\u8868\u683c\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTFM4GAD\u663e\u8457\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u4e13\u4e1aGAD\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "TFM4GAD\u4e3a\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u5f3a\u5927\u7684\u901a\u7528\u56fe\u5f02\u5e38\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u5b9e\u8df5\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u8de8\u9886\u57df\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2601.17331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17331", "abs": "https://arxiv.org/abs/2601.17331", "authors": ["Fabian Vazquez", "Jose A. Nu\u00f1ez", "Diego Adame", "Alissen Moreno", "Augustin Zhan", "Huimin Li", "Jinghao Yang", "Haoteng Tang", "Bin Fu", "Pengfei Gu"], "title": "Learning with Geometric Priors in U-Net Variants for Polyp Segmentation", "comment": null, "summary": "Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg", "AI": {"tldr": "\u63d0\u51fa\u51e0\u4f55\u5148\u9a8c\u5f15\u5bfc\u6a21\u5757(GPM)\uff0c\u901a\u8fc7\u6df1\u5ea6\u56fe\u5c06\u663e\u5f0f\u51e0\u4f55\u5148\u9a8c\u6ce8\u5165U-Net\u67b6\u6784\uff0c\u63d0\u5347\u606f\u8089\u5206\u5272\u5728\u4f4e\u5bf9\u6bd4\u5ea6\u6216\u6742\u4e71\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCNN\u3001Transformer\u548cMamba\u7684U-Net\u53d8\u4f53\u5728\u606f\u8089\u5206\u5272\u4e2d\u96be\u4ee5\u6355\u6349\u51e0\u4f55\u548c\u7ed3\u6784\u7ebf\u7d22\uff0c\u7279\u522b\u662f\u5728\u4f4e\u5bf9\u6bd4\u5ea6\u6216\u6742\u4e71\u7684\u7ed3\u80a0\u955c\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u5148\u9a8c\u5f15\u5bfc\u6a21\u5757(GPM)\uff1a1)\u5728\u6a21\u62df\u7684ColonDepth\u6570\u636e\u96c6\u4e0a\u5fae\u8c03VGGT\u6765\u4f30\u8ba1\u606f\u8089\u56fe\u50cf\u7684\u6df1\u5ea6\u56fe\uff1b2)\u7528GPM\u5904\u7406\u6df1\u5ea6\u56fe\uff0c\u5c06\u51e0\u4f55\u5148\u9a8c\u7f16\u7801\u5230\u7f16\u7801\u5668\u7684\u7279\u5f81\u56fe\u4e2d\uff1b3)\u4f7f\u7528\u7a7a\u95f4\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u4e00\u6b65\u7cbe\u70bc\uff0c\u5f3a\u8c03\u5c40\u90e8\u7a7a\u95f4\u548c\u5168\u5c40\u901a\u9053\u4fe1\u606f\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u606f\u8089\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u5f3a\u57fa\u7ebf\u6a21\u578b\u4e0a\u5747\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "GPM\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cdU-Net\u53d8\u4f53\u4e2d\uff0c\u901a\u8fc7\u6ce8\u5165\u663e\u5f0f\u51e0\u4f55\u5148\u9a8c\u663e\u8457\u63d0\u5347\u4e86\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.18462", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.18462", "abs": "https://arxiv.org/abs/2601.18462", "authors": ["Julio Vega"], "title": "Rethinking AI in the age of climate collapse: Ethics, power, and responsibility", "comment": "9 pages", "summary": "The climate crisis requires responses that integrate scientific, ethical, social, and technological perspectives. Artificial intelligence (AI) has emerged as a powerful tool in climate modelling, environmental monitoring, and energy optimisation, yet its growing use also raises critical environmental, ethical, legal, and social questions. This contribution examines the ambivalent role of AI in the ecological crisis, addressing both its promises and its risks. On the one hand, AI supports improvements in climate forecasting, renewable energy management, and real-time detection of environmental degradation. On the other hand, the energy demands of data centres, resource-intensive hardware production, algorithmic bias, corporate concentration of power, and technocratic decision-making reveal contradictions that challenge its sustainability. The discussion explores these issues through interdisciplinary lenses, including environmental ethics, philosophy of technology, and legal governance, and concludes with recommendations for socially just, ecologically responsible, and democratically accountable uses of AI. Rather than assuming AI as an inherently sustainable solution, this analysis argues that its contribution to climate action depends fundamentally on the values, institutions, and power structures that shape its development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8AI\u5728\u6c14\u5019\u5371\u673a\u4e2d\u7684\u53cc\u91cd\u89d2\u8272\uff1a\u65e2\u662f\u6c14\u5019\u5efa\u6a21\u3001\u73af\u5883\u76d1\u6d4b\u548c\u80fd\u6e90\u4f18\u5316\u7684\u6709\u529b\u5de5\u5177\uff0c\u53c8\u56e0\u5176\u80fd\u8017\u3001\u786c\u4ef6\u751f\u4ea7\u3001\u7b97\u6cd5\u504f\u89c1\u3001\u4f01\u4e1a\u6743\u529b\u96c6\u4e2d\u548c\u6280\u672f\u5b98\u50da\u51b3\u7b56\u7b49\u95ee\u9898\u800c\u9762\u4e34\u53ef\u6301\u7eed\u6027\u6311\u6218\u3002", "motivation": "\u6c14\u5019\u5371\u673a\u9700\u8981\u6574\u5408\u79d1\u5b66\u3001\u4f26\u7406\u3001\u793e\u4f1a\u548c\u6280\u672f\u89c6\u89d2\u7684\u5e94\u5bf9\u63aa\u65bd\u3002AI\u5df2\u6210\u4e3a\u6c14\u5019\u5efa\u6a21\u3001\u73af\u5883\u76d1\u6d4b\u548c\u80fd\u6e90\u4f18\u5316\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4f46\u5176\u65e5\u76ca\u5e7f\u6cdb\u7684\u5e94\u7528\u4e5f\u5f15\u53d1\u4e86\u73af\u5883\u3001\u4f26\u7406\u3001\u6cd5\u5f8b\u548c\u793e\u4f1a\u65b9\u9762\u7684\u5173\u952e\u95ee\u9898\u3002\u9700\u8981\u5ba1\u89c6AI\u5728\u751f\u6001\u5371\u673a\u4e2d\u7684\u77db\u76fe\u89d2\u8272\u3002", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u89c6\u89d2\u8fdb\u884c\u5206\u6790\uff0c\u5305\u62ec\u73af\u5883\u4f26\u7406\u5b66\u3001\u6280\u672f\u54f2\u5b66\u548c\u6cd5\u5f8b\u6cbb\u7406\u7b49\u89d2\u5ea6\uff0c\u63a2\u8ba8AI\u5728\u6c14\u5019\u884c\u52a8\u4e2d\u7684\u627f\u8bfa\u4e0e\u98ce\u9669\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86AI\u7684\u53cc\u91cd\u6027\uff1a\u4e00\u65b9\u9762\u652f\u6301\u6c14\u5019\u9884\u6d4b\u6539\u8fdb\u3001\u53ef\u518d\u751f\u80fd\u6e90\u7ba1\u7406\u548c\u73af\u5883\u9000\u5316\u5b9e\u65f6\u68c0\u6d4b\uff1b\u53e6\u4e00\u65b9\u9762\uff0c\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u3001\u8d44\u6e90\u5bc6\u96c6\u578b\u786c\u4ef6\u751f\u4ea7\u3001\u7b97\u6cd5\u504f\u89c1\u3001\u4f01\u4e1a\u6743\u529b\u96c6\u4e2d\u548c\u6280\u672f\u5b98\u50da\u51b3\u7b56\u7b49\u95ee\u9898\u6311\u6218\u5176\u53ef\u6301\u7eed\u6027\u3002", "conclusion": "AI\u5bf9\u6c14\u5019\u884c\u52a8\u7684\u8d21\u732e\u6839\u672c\u4e0a\u53d6\u51b3\u4e8e\u5851\u9020\u5176\u53d1\u5c55\u7684\u4ef7\u503c\u89c2\u3001\u5236\u5ea6\u548c\u6743\u529b\u7ed3\u6784\u3002\u8bba\u6587\u5efa\u8bae\u63a8\u52a8\u793e\u4f1a\u516c\u6b63\u3001\u751f\u6001\u8d1f\u8d23\u548c\u6c11\u4e3b\u95ee\u8d23\u7684AI\u4f7f\u7528\uff0c\u800c\u975e\u5c06AI\u89c6\u4e3a\u56fa\u6709\u7684\u53ef\u6301\u7eed\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18123", "abs": "https://arxiv.org/abs/2601.18123", "authors": ["Muhammad Ibrahim Khan", "Bivin Pradeep", "James Brusey"], "title": "Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters", "comment": "Accepted at AAAI 2026", "summary": "Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u622a\u6b62\u65f6\u95f4\u611f\u77e5\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5bb6\u7528\u6d78\u5165\u5f0f\u70ed\u6c34\u5668\u80fd\u8017\uff0c\u5728\u6307\u5b9a\u65f6\u95f4\u8fbe\u5230\u76ee\u6807\u6e29\u5ea6\u7684\u540c\u65f6\u6700\u5c0f\u5316\u80fd\u91cf\u6d88\u8017\u3002", "motivation": "\u4f20\u7edf\u5bb6\u7528\u6d78\u5165\u5f0f\u70ed\u6c34\u5668\u5728\u51ac\u5b63\u5e38\u8fde\u7eed\u8fd0\u884c\uff0c\u8ffd\u6c42\u5feb\u901f\u52a0\u70ed\u800c\u975e\u9ad8\u6548\u52a0\u70ed\uff0c\u5ffd\u7565\u4e86\u53ef\u9884\u6d4b\u7684\u9700\u6c42\u7a97\u53e3\u548c\u73af\u5883\u70ed\u635f\u5931\uff0c\u5bfc\u81f4\u80fd\u6e90\u6d6a\u8d39\u3002", "method": "\u5efa\u7acbGymnasium\u4eff\u771f\u73af\u5883\u6a21\u62df\u6d78\u5165\u5f0f\u70ed\u6c34\u5668\u70ed\u529b\u5b66\u8fc7\u7a0b\uff0c\u91c7\u7528\u65f6\u95f4\u6700\u4f18bang-bang\u63a7\u5236\u57fa\u7ebf\u3001\u96f6\u6837\u672c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u89c4\u5212\u5668\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u57282\u5c0f\u65f6\uff0860\u6b65\uff09\u65f6\u95f4\u8303\u56f4\u5185\uff0cPPO\u7b56\u7565\u80fd\u8017\u6700\u4f4e\uff083.23\u5343\u74e6\u65f6\uff09\uff0c\u76f8\u6bd4bang-bang\u63a7\u5236\uff084.37-10.45\u5343\u74e6\u65f6\uff09\u548cMCTS\uff084.18-6.46\u5343\u74e6\u65f6\uff09\u5206\u522b\u8282\u80fd26%\uff0830\u6b65\uff09\u548c69%\uff0890\u6b65\uff09\u3002\u5728\u5178\u578b\u573a\u666f\u4e2d\uff0cPPO\u6bd4bang-bang\u63a7\u5236\u8282\u80fd54%\uff0c\u6bd4MCTS\u8282\u80fd33%\u3002", "conclusion": "\u5b66\u4e60\u578b\u622a\u6b62\u65f6\u95f4\u611f\u77e5\u63a7\u5236\u80fd\u5728\u76f8\u540c\u7269\u7406\u5047\u8bbe\u4e0b\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u89c4\u5212\u5668\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u4f9b\u90e8\u5206\u8282\u80fd\u6548\u679c\uff0c\u800c\u8bad\u7ec3\u540e\u7684\u5b66\u4e60\u7b56\u7565\u63a8\u7406\u6210\u672c\u8fd1\u4e4e\u4e3a\u96f6\u3002"}}
{"id": "2601.17336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17336", "abs": "https://arxiv.org/abs/2601.17336", "authors": ["Xiaoyang Li", "Runni Zhou"], "title": "AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading", "comment": "22 pages", "summary": "Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.", "AI": {"tldr": "AGE-Net\uff1a\u4e00\u79cd\u57fa\u4e8eConvNeXt\u7684\u819d\u5173\u8282X\u5149\u7247KL\u5206\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u8c31\u7a7a\u95f4\u878d\u5408\u3001\u89e3\u5256\u56fe\u63a8\u7406\u548c\u5dee\u5206\u7ec6\u5316\u6280\u672f\uff0c\u7ed3\u5408\u8bc1\u636e\u56de\u5f52\u548c\u5e8f\u6570\u7ea6\u675f\uff0c\u5728KL\u5206\u7ea7\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u819d\u5173\u8282X\u5149\u7247\u7684Kellgren-Lawrence\uff08KL\uff09\u5206\u7ea7\u81ea\u52a8\u5316\u9762\u4e34\u6311\u6218\uff1a\u7ec6\u5fae\u7ed3\u6784\u53d8\u5316\u3001\u957f\u8ddd\u79bb\u89e3\u5256\u4f9d\u8d56\u6027\u548c\u5206\u7ea7\u8fb9\u754c\u6a21\u7cca\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7279\u5f81\u3002", "method": "\u63d0\u51faAGE-Net\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u8c31\u7a7a\u95f4\u878d\u5408\uff08SSF\uff09\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff1b2\uff09\u89e3\u5256\u56fe\u63a8\u7406\uff08AGR\uff09\u5efa\u6a21\u89e3\u5256\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff1b3\uff09\u5dee\u5206\u7ec6\u5316\uff08DFR\uff09\u5904\u7406\u8fb9\u754c\u6a21\u7cca\u6027\u3002\u91c7\u7528Normal-Inverse-Gamma\u8bc1\u636e\u56de\u5f52\u5934\u548c\u6210\u5bf9\u5e8f\u6570\u6392\u540d\u7ea6\u675f\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u548c\u4fdd\u6301\u6807\u7b7e\u5e8f\u6570\u6027\u3002", "result": "\u5728\u819d\u5173\u8282KL\u6570\u636e\u96c6\u4e0a\uff0cAGE-Net\u8fbe\u5230\u4e8c\u6b21\u52a0\u6743kappa\uff08QWK\uff090.9017\u00b10.0045\uff0c\u5747\u65b9\u8bef\u5dee\uff08MSE\uff090.2349\u00b10.0028\uff0c\u4f18\u4e8e\u5f3aCNN\u57fa\u7ebf\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\u5404\u6a21\u5757\u5747\u6709\u7a33\u5b9a\u589e\u76ca\u3002\u8fd8\u8bc4\u4f30\u4e86\u4e0d\u786e\u5b9a\u6027\u8d28\u91cf\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "AGE-Net\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u3001\u89e3\u5256\u5173\u7cfb\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86KL\u5206\u7ea7\u4e2d\u7684\u6311\u6218\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.18622", "categories": ["cs.CY", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.18622", "abs": "https://arxiv.org/abs/2601.18622", "authors": ["Jo\u00e3o Phillipe Cardenuto", "Ana Carolina Monari", "Michelle Diniz Lopes", "Leopoldo Lusquino Filho", "Anderson Rocha"], "title": "Brazilian Social Media Anti-vaccine Information Disorder Dataset -- Telegram (2020-2025)", "comment": "14 pages, 5 figures, 6 tables", "summary": "Over the past decade, Brazil has experienced a decline in vaccination coverage, reversing decades of public health progress achieved through the National Immunization Program (PNI). Growing evidence points to the widespread circulation of vaccine-related misinformation -- particularly on social media platforms -- as a key factor driving this decline. Among these platforms, Telegram remains the only major platform permitting accessible and ethical data collection, offering insight into public channels where vaccine misinformation circulates extensively. This data paper introduces a curated dataset of about four million Telegram posts collected from 119 prominent Brazilian anti-vaccine channels between 2020 and 2025. The dataset includes message content, metadata, associated media, and classification related to vaccine posts, enabling researchers to examine how false or misleading information spreads, evolves, and influences public sentiment. By providing this resource, our aim is to support the scientific and public health community in developing evidence-based strategies to counter misinformation, promote trust in vaccination, and engage compassionately with individuals and communities affected by false narratives. The dataset and documentation are openly available for non-commercial research, under strict ethical and privacy guidelines at https://doi.org/10.25824/redu/5JIVDT", "AI": {"tldr": "\u5df4\u897f\u75ab\u82d7\u63a5\u79cd\u7387\u4e0b\u964d\u4e0e\u793e\u4ea4\u5a92\u4f53\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u76f8\u5173\uff0c\u672c\u6587\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u7ea6400\u4e07\u6761\u5df4\u897f\u53cd\u75ab\u82d7Telegram\u9891\u9053\u5e16\u5b50\u7684\u6570\u636e\u96c6\uff082020-2025\u5e74\uff09\uff0c\u7528\u4e8e\u7814\u7a76\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u6a21\u5f0f", "motivation": "\u5df4\u897f\u75ab\u82d7\u63a5\u79cd\u8986\u76d6\u7387\u5728\u8fc7\u53bb\u5341\u5e74\u4e0b\u964d\uff0c\u9006\u8f6c\u4e86\u56fd\u5bb6\u514d\u75ab\u8ba1\u5212\u51e0\u5341\u5e74\u7684\u516c\u5171\u536b\u751f\u8fdb\u5c55\u3002\u793e\u4ea4\u5a92\u4f53\u4e0a\u75ab\u82d7\u76f8\u5173\u9519\u8bef\u4fe1\u606f\u7684\u5e7f\u6cdb\u4f20\u64ad\u88ab\u8ba4\u4e3a\u662f\u5bfc\u81f4\u8fd9\u4e00\u4e0b\u964d\u7684\u5173\u952e\u56e0\u7d20\uff0c\u800cTelegram\u662f\u552f\u4e00\u5141\u8bb8\u53ef\u8bbf\u95ee\u4e14\u7b26\u5408\u4f26\u7406\u7684\u6570\u636e\u6536\u96c6\u7684\u4e3b\u8981\u5e73\u53f0", "method": "\u4ece119\u4e2a\u5df4\u897f\u4e3b\u8981\u53cd\u75ab\u82d7Telegram\u9891\u9053\u6536\u96c6\u4e86\u7ea6400\u4e07\u6761\u5e16\u5b50\uff082020-2025\u5e74\uff09\uff0c\u6570\u636e\u96c6\u5305\u542b\u6d88\u606f\u5185\u5bb9\u3001\u5143\u6570\u636e\u3001\u76f8\u5173\u5a92\u4f53\u4ee5\u53ca\u75ab\u82d7\u76f8\u5173\u5e16\u5b50\u7684\u5206\u7c7b\uff0c\u652f\u6301\u7814\u7a76\u4eba\u5458\u5206\u6790\u865a\u5047\u6216\u8bef\u5bfc\u6027\u4fe1\u606f\u7684\u4f20\u64ad\u3001\u6f14\u53d8\u548c\u5f71\u54cd", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u5df4\u897f\u53cd\u75ab\u82d7Telegram\u9891\u9053\u7684\u5927\u91cf\u6570\u636e\uff0c\u4e3a\u7814\u7a76\u75ab\u82d7\u9519\u8bef\u4fe1\u606f\u4f20\u64ad\u63d0\u4f9b\u4e86\u8d44\u6e90\uff0c\u6570\u636e\u96c6\u548c\u6587\u6863\u5728\u4e25\u683c\u7684\u4f26\u7406\u548c\u9690\u79c1\u51c6\u5219\u4e0b\u5f00\u653e\u4f9b\u975e\u5546\u4e1a\u7814\u7a76\u4f7f\u7528", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u8fd9\u4e00\u6570\u636e\u96c6\u8d44\u6e90\uff0c\u65e8\u5728\u652f\u6301\u79d1\u5b66\u548c\u516c\u5171\u536b\u751f\u793e\u533a\u5236\u5b9a\u57fa\u4e8e\u8bc1\u636e\u7684\u7b56\u7565\u6765\u5bf9\u6297\u9519\u8bef\u4fe1\u606f\uff0c\u4fc3\u8fdb\u5bf9\u75ab\u82d7\u63a5\u79cd\u7684\u4fe1\u4efb\uff0c\u5e76\u4ee5\u540c\u60c5\u5fc3\u4e0e\u53d7\u865a\u5047\u53d9\u4e8b\u5f71\u54cd\u7684\u4e2a\u4eba\u548c\u793e\u533a\u4e92\u52a8"}}
{"id": "2601.17340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17340", "abs": "https://arxiv.org/abs/2601.17340", "authors": ["Haodong He", "Xin Zhan", "Yancheng Bai", "Rui Lan", "Lei Sun", "Xiangxiang Chu"], "title": "TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution", "comment": "Accepted by ICASSP 2026", "summary": "Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Real-Texts\u6570\u636e\u96c6\u548cTEXTS-Diff\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u6587\u672c\u533a\u57df\u6062\u590d\u8d28\u91cf\u5dee\u7684\u95ee\u9898\uff0c\u5728\u80cc\u666f\u548c\u6587\u672c\u533a\u57df\u90fd\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u73b0\u6709\u6570\u636e\u96c6\u4e2d\u6587\u672c\u56fe\u50cf\u7a00\u7f3a\uff0c\u5bfc\u81f4\u6587\u672c\u533a\u57df\u6062\u590d\u6548\u679c\u5dee\uff1b2) \u57fa\u4e8e\u5b64\u7acb\u6587\u672c\u6837\u672c\u7684\u6570\u636e\u96c6\u9650\u5236\u4e86\u80cc\u666f\u91cd\u5efa\u8d28\u91cf\u3002\u9700\u8981\u6784\u5efa\u66f4\u5168\u9762\u7684\u771f\u5b9e\u4e16\u754c\u6587\u672c\u56fe\u50cf\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u80fd\u540c\u65f6\u63d0\u5347\u80cc\u666f\u548c\u6587\u672c\u533a\u57df\u8d28\u91cf\u7684\u6a21\u578b\u3002", "method": "\u6784\u5efa\u4e86Real-Texts\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e2d\u82f1\u6587\u81ea\u7136\u6587\u672c\u5b9e\u4f8b\uff1b\u63d0\u51fa\u4e86TEXTS-Aware Diffusion Model (TEXTS-Diff)\uff0c\u5229\u7528\u62bd\u8c61\u6982\u5ff5\u63d0\u5347\u5bf9\u89c6\u89c9\u573a\u666f\u4e2d\u6587\u672c\u5143\u7d20\u7684\u7406\u89e3\uff0c\u540c\u65f6\u901a\u8fc7\u5177\u4f53\u6587\u672c\u533a\u57df\u589e\u5f3a\u6587\u672c\u7ec6\u8282\uff0c\u51cf\u5c11\u6587\u672c\u533a\u57df\u7684\u5931\u771f\u548c\u5e7b\u89c9\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6587\u672c\u6062\u590d\u51c6\u786e\u6027\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u5c06\u5168\u90e8\u5f00\u6e90\u3002", "conclusion": "Real-Texts\u6570\u636e\u96c6\u548cTEXTS-Diff\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u89c6\u89c9\u573a\u666f\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u533a\u57df\u7684\u6062\u590d\u8d28\u91cf\u3002"}}
{"id": "2601.18644", "categories": ["cs.CY", "econ.GN"], "pdf": "https://arxiv.org/pdf/2601.18644", "abs": "https://arxiv.org/abs/2601.18644", "authors": ["Joe Cannataci", "Benjamin Fehrensen", "Mikolai G\u00fctschow", "\u00d6zg\u00fcr Kesim", "Bernd Lucke"], "title": "Digital Euro: Frequently Asked Questions Revisited", "comment": "Submitted to SNB-CIF (Conference on Cryptoassets and Financial Innovation)", "summary": "The European Central Bank (ECB) is working on the \"digital euro\", an envisioned retail central bank digital currency for the Euro area. In this article, we take a closer look at the \"digital euro FAQ\", which provides answers to 26 frequently asked questions about the digital euro, and other published documents by the ECB on the topic. We question the provided answers based on our analysis of the current design in terms of privacy, technical feasibility, risks, costs and utility. In particular, we discuss the following key findings:\n  (KF1) Central monitoring of all online digital euro transactions by the ECB threatens privacy even more than contemporary digital payment methods with segregated account databases.\n  (KF2) The ECB's envisioned concept of a secure offline version of the digital euro offering full anonymity is in strong conflict with the actual history of hardware security breaches and mathematical evidence against it.\n  (KF3) The legal and financial liabilities for the various parties involved remain unclear.\n  (KF4) The design lacks well-specified economic incentives for operators as well as a discussion of its economic impact on merchants.\n  (KF5) The ECB fails to identify tangible benefits the digital euro would create for society, in particular given that the online component of the proposed infrastructure mainly duplicates existing payment systems.\n  (KF6) The design process has been exclusionary, with critical decisions being set in stone before public consultations. Alternative and open design ideas have not even been discussed by the ECB.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6b27\u6d32\u592e\u884c\u6570\u5b57\u6b27\u5143FAQ\u53ca\u5176\u4ed6\u76f8\u5173\u6587\u4ef6\u8fdb\u884c\u6279\u5224\u6027\u5206\u6790\uff0c\u6307\u51fa\u5176\u5728\u9690\u79c1\u4fdd\u62a4\u3001\u6280\u672f\u53ef\u884c\u6027\u3001\u98ce\u9669\u6210\u672c\u3001\u7ecf\u6d4e\u6548\u76ca\u7b49\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u8d28\u7591\u6570\u5b57\u6b27\u5143\u9879\u76ee\u7684\u5fc5\u8981\u6027\u548c\u8bbe\u8ba1\u5408\u7406\u6027\u3002", "motivation": "\u6b27\u6d32\u592e\u884c\u6b63\u5728\u63a8\u8fdb\u6570\u5b57\u6b27\u5143\u9879\u76ee\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u5176\u516c\u5f00\u6587\u4ef6\u4e2d\u7684\u89e3\u91ca\u5b58\u5728\u8bf8\u591a\u95ee\u9898\uff0c\u9700\u8981\u4ece\u6280\u672f\u3001\u7ecf\u6d4e\u3001\u793e\u4f1a\u7b49\u591a\u4e2a\u89d2\u5ea6\u8fdb\u884c\u6df1\u5165\u5206\u6790\u548c\u8d28\u7591\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b27\u6d32\u592e\u884c\u53d1\u5e03\u7684\u6570\u5b57\u6b27\u5143FAQ\uff0826\u4e2a\u5e38\u89c1\u95ee\u9898\u89e3\u7b54\uff09\u53ca\u5176\u4ed6\u76f8\u5173\u6587\u4ef6\uff0c\u4ece\u9690\u79c1\u4fdd\u62a4\u3001\u6280\u672f\u53ef\u884c\u6027\u3001\u98ce\u9669\u6210\u672c\u3001\u6cd5\u5f8b\u91d1\u878d\u8d23\u4efb\u3001\u7ecf\u6d4e\u6fc0\u52b1\u548c\u793e\u4f1a\u6548\u76ca\u7b49\u65b9\u9762\u8fdb\u884c\u6279\u5224\u6027\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u516d\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u5728\u7ebf\u4ea4\u6613\u96c6\u4e2d\u76d1\u63a7\u5a01\u80c1\u9690\u79c1\uff1b2) \u79bb\u7ebf\u533f\u540d\u7248\u672c\u5b58\u5728\u5b89\u5168\u51b2\u7a81\uff1b3) \u6cd5\u5f8b\u8d23\u4efb\u4e0d\u660e\u786e\uff1b4) \u7f3a\u4e4f\u7ecf\u6d4e\u6fc0\u52b1\u673a\u5236\uff1b5) \u793e\u4f1a\u6548\u76ca\u4e0d\u660e\u663e\uff1b6) \u8bbe\u8ba1\u8fc7\u7a0b\u6392\u4ed6\u6027\u8fc7\u5f3a\u3002", "conclusion": "\u6570\u5b57\u6b27\u5143\u5f53\u524d\u8bbe\u8ba1\u5b58\u5728\u91cd\u5927\u7f3a\u9677\uff0c\u672a\u80fd\u5145\u5206\u89e3\u51b3\u9690\u79c1\u3001\u5b89\u5168\u3001\u7ecf\u6d4e\u548c\u793e\u4f1a\u6548\u76ca\u7b49\u6838\u5fc3\u95ee\u9898\uff0c\u8bbe\u8ba1\u8fc7\u7a0b\u7f3a\u4e4f\u5305\u5bb9\u6027\u548c\u900f\u660e\u5ea6\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u5176\u5fc5\u8981\u6027\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2601.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18132", "abs": "https://arxiv.org/abs/2601.18132", "authors": ["Xi Chen", "Hongru Zhou", "Huahui Yi", "Shiyu Feng", "Hanyu Zhou", "Tiancheng He", "Mingke You", "Li Wang", "Qiankun Li", "Kun Wang", "Weili Fu", "Kang Li", "Jian Li"], "title": "RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening", "comment": "28 page, 3 figures", "summary": "Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.", "AI": {"tldr": "RareAlert\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u63a8\u7406\u6821\u51c6\u7684\u7f55\u89c1\u75c5\u65e9\u671f\u7b5b\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u540810\u4e2aLLM\u7684\u63a8\u7406\u4fe1\u53f7\uff0c\u8bad\u7ec3\u51fa\u53ef\u5728\u672c\u5730\u90e8\u7f72\u7684\u5355\u4e00\u6a21\u578b\uff0c\u5728158,666\u4e2a\u771f\u5b9e\u75c5\u4f8b\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.917\u7684AUC\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u6240\u6709\u8bc4\u4f30\u7684LLM\u3002", "motivation": "\u7f55\u89c1\u75c5\u7684\u6f0f\u8bca\u548c\u5ef6\u8fdf\u8bca\u65ad\u662f\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u521d\u7ea7\u8bca\u7597\u5206\u8bca\u6d41\u7a0b\u5728\u521d\u6b21\u5c31\u8bca\u65f6\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u7f55\u89c1\u75c5\u60a3\u8005\uff0c\u9700\u8981\u901a\u7528\u7b5b\u67e5\u6765\u51cf\u5c11\u8bca\u65ad\u5ef6\u8fdf\u3002", "method": "\u5f00\u53d1RareAlert\u7cfb\u7edf\uff1a\u6574\u540810\u4e2aLLM\u751f\u6210\u7684\u63a8\u7406\u4fe1\u53f7\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6821\u51c6\u548c\u52a0\u6743\u8fd9\u4e9b\u4fe1\u53f7\uff0c\u5c06\u5bf9\u9f50\u7684\u63a8\u7406\u63d0\u70bc\u6210\u5355\u4e00\u53ef\u672c\u5730\u90e8\u7f72\u7684\u6a21\u578b\u3002\u4f7f\u7528\u5305\u542b158,666\u4e2a\u75c5\u4f8b\u3001\u8986\u76d633\u4e2aOrphanet\u75be\u75c5\u7c7b\u522b\u7684RareBench\u6570\u636e\u96c6\u8fdb\u884c\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "result": "RareAlert\u5728\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u52300.917\u7684AUC\uff0c\u4f18\u4e8e\u6700\u4f73\u673a\u5668\u5b66\u4e60\u96c6\u6210\u6a21\u578b\u548c\u6240\u6709\u8bc4\u4f30\u7684LLM\uff08\u5305\u62ecGPT-5\u3001DeepSeek-R1\u3001Claude-3.7-Sonnet\u7b49\uff09\u3002\u57fa\u4e8eQwen3-4B\u7684\u6a21\u578b\u901a\u8fc7\u6821\u51c6\u63a8\u7406\u4fe1\u53f7\u8bad\u7ec3\uff0c\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7f55\u89c1\u75c5\u8bc6\u522b\u53ef\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u5e94\u7528\u4e8e\u666e\u901a\u60a3\u8005\u7fa4\u4f53\u7684\u901a\u7528\u4e0d\u786e\u5b9a\u6027\u89e3\u51b3\u8fc7\u7a0b\u3002LLM\u533b\u5b66\u63a8\u7406\u5177\u6709\u591a\u6837\u6027\uff0c\u5728\u9ad8\u5ea6\u4e0d\u786e\u5b9a\u7684\u4e34\u5e8a\u4efb\u52a1\u4e2d\u5bf9\u9f50\u8fd9\u79cd\u63a8\u7406\u662f\u6709\u6548\u7684\u3002RareAlert\u901a\u8fc7\u5c06\u6821\u51c6\u63a8\u7406\u878d\u5165\u5355\u4e00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u4fdd\u62a4\u9690\u79c1\u3001\u53ef\u6269\u5c55\u7684\u7f55\u89c1\u75c5\u98ce\u9669\u7b5b\u67e5\uff0c\u9002\u5408\u5927\u89c4\u6a21\u672c\u5730\u90e8\u7f72\u3002"}}
{"id": "2601.17309", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17309", "abs": "https://arxiv.org/abs/2601.17309", "authors": ["Anagha Sabu", "Vidhya S", "Narayanan C Krishnan"], "title": "PAR: Plausibility-aware Amortized Recourse Generation", "comment": null, "summary": "Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.", "AI": {"tldr": "PAR\u662f\u4e00\u79cd\u57fa\u4e8e\u644a\u9500\u8fd1\u4f3c\u63a8\u7406\u7684\u7b97\u6cd5\u8ffd\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u6700\u5927\u540e\u9a8c\u6982\u7387\u63a8\u65ad\u751f\u6210\u9ad8\u4f3c\u7136\u3001\u73b0\u5b9e\u53ef\u884c\u7684\u53cd\u4e8b\u5b9e\u5efa\u8bae\uff0c\u5728\u4fdd\u6301\u6709\u6548\u6027\u548c\u7a00\u758f\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u751f\u6210\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u8ffd\u7d22\u65b9\u6cd5\u5728\u751f\u6210\u73b0\u5b9e\u53ef\u884c\u4e14\u9ad8\u4f3c\u7136\u7684\u53cd\u4e8b\u5b9e\u5efa\u8bae\u65b9\u9762\u5b58\u5728\u6548\u7387\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u6709\u6548\u6027\u3001\u76f8\u4f3c\u6027\u3001\u7a00\u758f\u6027\u548c\u9ad8\u4f3c\u7136\u6027\u8981\u6c42\u7684\u9ad8\u6548\u8ffd\u7d22\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5c06\u8ffd\u7d22\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u6700\u5927\u540e\u9a8c\u6982\u7387\u63a8\u65ad\u95ee\u9898\uff0c\u5728\u53ef\u63a5\u53d7\u7c7b\u6570\u636e\u5206\u5e03\u4e0b\u5bfb\u627e\u9ad8\u4f3c\u7136\u53cd\u4e8b\u5b9e\u3002\u63d0\u51faPAR\u644a\u9500\u8fd1\u4f3c\u63a8\u7406\u7a0b\u5e8f\uff0c\u4f7f\u7528\u53ef\u5904\u7406\u6982\u7387\u6a21\u578b\u76f4\u63a5\u4f30\u8ba1\u8ffd\u7d22\u4f3c\u7136\uff0c\u652f\u6301\u7cbe\u786e\u4f3c\u7136\u8bc4\u4f30\u548c\u9ad8\u6548\u68af\u5ea6\u4f20\u64ad\u3002\u8bad\u7ec3\u8ffd\u7d22\u751f\u6210\u5668\u4ee5\u6700\u5927\u5316\u53ef\u63a5\u53d7\u7c7b\u5206\u5e03\u4e0b\u7684\u4f3c\u7136\uff0c\u6700\u5c0f\u5316\u62d2\u7edd\u7c7b\u5206\u5e03\u4e0b\u7684\u4f3c\u7136\uff0c\u5e76\u7f16\u7801\u5176\u4ed6\u8ffd\u7d22\u7ea6\u675f\u635f\u5931\u3002\u5f15\u5165\u57fa\u4e8e\u90bb\u57df\u7684\u8c03\u8282\u673a\u5236\u4ee5\u751f\u6210\u9488\u5bf9\u5177\u4f53\u4e8b\u5b9e\u5b9a\u5236\u7684\u8ffd\u7d22\u5efa\u8bae\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b97\u6cd5\u8ffd\u7d22\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1PAR\uff0c\u8bc1\u660e\u5176\u80fd\u9ad8\u6548\u751f\u6210\u6709\u6548\u3001\u4e0e\u4e8b\u5b9e\u76f8\u4f3c\u3001\u7a00\u758f\u4e14\u9ad8\u5ea6\u5408\u7406\u7684\u8ffd\u7d22\u5efa\u8bae\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PAR\u901a\u8fc7\u644a\u9500\u8fd1\u4f3c\u63a8\u7406\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7b97\u6cd5\u8ffd\u7d22\u4e2d\u7684\u6548\u7387\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u4f3c\u7136\u7684\u53cd\u4e8b\u5b9e\u5efa\u8bae\uff0c\u4e3a\u7b97\u6cd5\u8ffd\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18654", "categories": ["cs.CY", "econ.TH"], "pdf": "https://arxiv.org/pdf/2601.18654", "abs": "https://arxiv.org/abs/2601.18654", "authors": ["Juan Wu", "Zhe", "Zhang", "Amit Mehra"], "title": "When Is Self-Disclosure Optimal? Incentives and Governance of AI-Generated Content", "comment": null, "summary": "Generative artificial intelligence (Gen-AI) is reshaping content creation on digital platforms by reducing production costs and enabling scalable output of varying quality. In response, platforms have begun adopting disclosure policies that require creators to label AI-generated content, often supported by imperfect detection and penalties for non-compliance. This paper develops a formal model to study the economic implications of such disclosure regimes. We compare a non-disclosure benchmark, in which the platform alone detects AI usage, with a mandatory self-disclosure regime in which creators strategically choose whether to disclose or conceal AI use under imperfect enforcement. The model incorporates heterogeneous creators, viewer discounting of AI-labeled content, trust penalties following detected non-disclosure, and endogenous enforcement. The analysis shows that disclosure is optimal only when both the value of AI-generated content and its cost-saving advantage are intermediate. As AI capability improves, the platform's optimal enforcement strategy evolves from strict deterrence to partial screening and eventual deregulation. While disclosure reliably increases transparency, it reduces aggregate creator surplus and can suppress high-quality AI content when AI is technologically advanced. Overall, the results characterize disclosure as a strategic governance instrument whose effectiveness depends on technological maturity and trust frictions.", "AI": {"tldr": "\u7814\u7a76AI\u751f\u6210\u5185\u5bb9\u62ab\u9732\u653f\u7b56\u7684\u7ecf\u6d4e\u5f71\u54cd\uff0c\u6bd4\u8f83\u65e0\u62ab\u9732\u57fa\u51c6\u4e0e\u5f3a\u5236\u81ea\u6211\u62ab\u9732\u5236\u5ea6\uff0c\u5206\u6790\u521b\u4f5c\u8005\u7b56\u7565\u9009\u62e9\u3001\u5e73\u53f0\u6267\u6cd5\u7b56\u7565\u6f14\u53d8\u53ca\u5176\u5bf9\u900f\u660e\u5ea6\u3001\u521b\u4f5c\u8005\u5269\u4f59\u548c\u5185\u5bb9\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u751f\u6210\u5f0fAI\u964d\u4f4e\u4e86\u5185\u5bb9\u521b\u4f5c\u6210\u672c\u5e76\u5b9e\u73b0\u89c4\u6a21\u5316\u4ea7\u51fa\uff0c\u5e73\u53f0\u5f00\u59cb\u91c7\u7528\u8981\u6c42\u521b\u4f5c\u8005\u6807\u6ce8AI\u751f\u6210\u5185\u5bb9\u7684\u62ab\u9732\u653f\u7b56\u3002\u9700\u8981\u7814\u7a76\u8fd9\u79cd\u62ab\u9732\u5236\u5ea6\u7684\u7ecf\u6d4e\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5f53\u5e73\u53f0\u68c0\u6d4b\u4e0d\u5b8c\u5584\u4e14\u521b\u4f5c\u8005\u53ef\u80fd\u7b56\u7565\u6027\u9009\u62e9\u62ab\u9732\u6216\u9690\u7792AI\u4f7f\u7528\u65f6\u7684\u60c5\u51b5\u3002", "method": "\u5efa\u7acb\u6b63\u5f0f\u7ecf\u6d4e\u6a21\u578b\uff0c\u6bd4\u8f83\u65e0\u62ab\u9732\u57fa\u51c6\uff08\u4ec5\u5e73\u53f0\u68c0\u6d4bAI\u4f7f\u7528\uff09\u4e0e\u5f3a\u5236\u81ea\u6211\u62ab\u9732\u5236\u5ea6\uff08\u521b\u4f5c\u8005\u5728\u6267\u6cd5\u4e0d\u5b8c\u5584\u4e0b\u7b56\u7565\u6027\u9009\u62e9\u662f\u5426\u62ab\u9732\uff09\u3002\u6a21\u578b\u5305\u542b\u5f02\u8d28\u521b\u4f5c\u8005\u3001\u89c2\u4f17\u5bf9AI\u6807\u6ce8\u5185\u5bb9\u7684\u6298\u6263\u3001\u68c0\u6d4b\u5230\u672a\u62ab\u9732\u7684\u4fe1\u4efb\u60e9\u7f5a\u4ee5\u53ca\u5185\u751f\u6267\u6cd5\u3002", "result": "\u62ab\u9732\u4ec5\u5728AI\u751f\u6210\u5185\u5bb9\u7684\u4ef7\u503c\u548c\u6210\u672c\u8282\u7ea6\u4f18\u52bf\u90fd\u5904\u4e8e\u4e2d\u7b49\u6c34\u5e73\u65f6\u6700\u4f18\u3002\u968f\u7740AI\u80fd\u529b\u63d0\u5347\uff0c\u5e73\u53f0\u6700\u4f18\u6267\u6cd5\u7b56\u7565\u4ece\u4e25\u683c\u5a01\u6151\u6f14\u53d8\u4e3a\u90e8\u5206\u7b5b\u9009\uff0c\u6700\u7ec8\u8d70\u5411\u653e\u677e\u76d1\u7ba1\u3002\u62ab\u9732\u867d\u63d0\u9ad8\u900f\u660e\u5ea6\uff0c\u4f46\u51cf\u5c11\u521b\u4f5c\u8005\u603b\u5269\u4f59\uff0c\u5f53AI\u6280\u672f\u5148\u8fdb\u65f6\u53ef\u80fd\u6291\u5236\u9ad8\u8d28\u91cfAI\u5185\u5bb9\u3002", "conclusion": "\u62ab\u9732\u662f\u4e00\u79cd\u6218\u7565\u6027\u6cbb\u7406\u5de5\u5177\uff0c\u5176\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u6280\u672f\u6210\u719f\u5ea6\u548c\u4fe1\u4efb\u6469\u64e6\u3002\u5e73\u53f0\u9700\u8981\u6839\u636eAI\u6280\u672f\u53d1\u5c55\u9636\u6bb5\u8c03\u6574\u62ab\u9732\u653f\u7b56\uff0c\u5e73\u8861\u900f\u660e\u5ea6\u3001\u521b\u4f5c\u8005\u6fc0\u52b1\u548c\u5185\u5bb9\u8d28\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2601.18137", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18137", "abs": "https://arxiv.org/abs/2601.18137", "authors": ["Yinger Zhang", "Shutong Jiang", "Renhao Li", "Jianhong Tu", "Yang Su", "Lianghao Deng", "Xudong Guo", "Chenxu Lv", "Junyang Lin"], "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints", "comment": null, "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.", "AI": {"tldr": "DeepPlanning\u662f\u4e00\u4e2a\u9488\u5bf9\u5b9e\u9645\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u89c4\u5212\u7684\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u65e5\u65c5\u884c\u89c4\u5212\u548c\u591a\u4ea7\u54c1\u8d2d\u7269\u4efb\u52a1\uff0c\u9700\u8981\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u3001\u5c40\u90e8\u7ea6\u675f\u63a8\u7406\u548c\u5168\u5c40\u7ea6\u675f\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u8bc4\u4f30\u867d\u7136\u8f6c\u5411\u4e86\u957f\u65f6\u7a0b\u4efb\u52a1\uff0c\u4f46\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4ecd\u4fa7\u91cd\u4e8e\u5c40\u90e8\u3001\u6b65\u9aa4\u7ea7\u7684\u63a8\u7406\uff0c\u800c\u975e\u9700\u8981\u771f\u6b63\u89c4\u5212\u80fd\u529b\u7684\u5168\u5c40\u7ea6\u675f\u4f18\u5316\uff08\u5982\u65f6\u95f4\u548c\u8d22\u52a1\u9884\u7b97\uff09\u3002\u540c\u65f6\uff0c\u73b0\u6709\u7684LLM\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u4f53\u73b0\u73b0\u5b9e\u4e16\u754c\u4e2d\u5178\u578b\u7684\u4e3b\u52a8\u4fe1\u606f\u6536\u96c6\u548c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7ea6\u675f\u3002", "method": "\u5f15\u5165DeepPlanning\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u65e5\u65c5\u884c\u89c4\u5212\u548c\u591a\u4ea7\u54c1\u8d2d\u7269\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\uff1a1\uff09\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\uff1b2\uff09\u5c40\u90e8\u7ea6\u675f\u63a8\u7406\uff1b3\uff09\u5168\u5c40\u7ea6\u675f\u4f18\u5316\u3002\u901a\u8fc7\u8be5\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u524d\u6cbf\u667a\u80fd\u4f53LLM\u7684\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u524d\u6cbf\u7684\u667a\u80fd\u4f53LLM\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u53ef\u9760\u7684\u663e\u5f0f\u63a8\u7406\u6a21\u5f0f\u548c\u5e76\u884c\u5de5\u5177\u4f7f\u7528\u5bf9\u4e8e\u5b9e\u73b0\u66f4\u597d\u7684\u6548\u679c-\u6548\u7387\u6743\u8861\u7684\u91cd\u8981\u6027\u3002\u9519\u8bef\u5206\u6790\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdb\u957f\u89c4\u5212\u65f6\u7a0b\u7684\u667a\u80fd\u4f53LLM\u6307\u51fa\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002", "conclusion": "DeepPlanning\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709\u89c4\u5212\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u667a\u80fd\u4f53LLM\u5728\u5b9e\u8df5\u957f\u65f6\u7a0b\u89c4\u5212\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2601.17329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17329", "abs": "https://arxiv.org/abs/2601.17329", "authors": ["Tiejin Chen", "Xiaoou Liu", "Vishnu Nandam", "Kuan-Ru Liou", "Hua Wei"], "title": "Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment", "comment": "Accetped to Findings of EACL", "summary": "Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \\emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \\emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.", "AI": {"tldr": "\u63d0\u51faConformal Feedback Alignment (CFA)\u6846\u67b6\uff0c\u5229\u7528Conformal Prediction\u7684\u7edf\u8ba1\u4fdd\u8bc1\u6765\u91cf\u5316\u7b54\u6848\u53ef\u9760\u6027\uff0c\u4e3aDPO\u548cPPO\u8bad\u7ec3\u63d0\u4f9b\u539f\u5219\u6027\u6743\u91cd\uff0c\u63d0\u5347\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u504f\u597d\u7684\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\uff09\u9762\u4e34\u6807\u7b7e\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u65b9\u6cd5\u4ec5\u5bf9\u504f\u597d\u8fdb\u884c\u52a0\u6743\uff0c\u4f46\u5ffd\u7565\u4e86\u66f4\u57fa\u672c\u7684\u56e0\u7d20\uff1a\u88ab\u6bd4\u8f83\u7b54\u6848\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faConformal Feedback Alignment (CFA)\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5177\u6709\u53ef\u63a7\u8986\u76d6\u7387\u7684conformal prediction sets\u6765\u91cf\u5316\u7b54\u6848\u7ea7\u53ef\u9760\u6027\uff0c\u5e76\u5c06\u8fd9\u4e9b\u53ef\u9760\u6027\u805a\u5408\u6210\u539f\u5219\u6027\u6743\u91cd\uff0c\u5e94\u7528\u4e8eDPO\u548cPPO\u98ce\u683c\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCFA\u63d0\u9ad8\u4e86\u5bf9\u9f50\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u8bc1\u660e\u5efa\u6a21\u7b54\u6848\u4fa7\u4e0d\u786e\u5b9a\u6027\u53ef\u4ee5\u8865\u5145\u504f\u597d\u7ea7\u52a0\u6743\uff0c\u5b9e\u73b0\u66f4\u9c81\u68d2\u3001\u6570\u636e\u6548\u7387\u66f4\u9ad8\u7684\u5bf9\u9f50\u3002", "conclusion": "CFA\u6846\u67b6\u901a\u8fc7\u91cf\u5316\u7b54\u6848\u53ef\u9760\u6027\u5e76\u63d0\u4f9b\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u504f\u597d\u5bf9\u9f50\u4e2d\u7684\u566a\u58f0\u95ee\u9898\uff0c\u4e3a\u66f4\u9c81\u68d2\u3001\u6570\u636e\u6548\u7387\u66f4\u9ad8\u7684\u5bf9\u9f50\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17349", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17349", "abs": "https://arxiv.org/abs/2601.17349", "authors": ["Hailong Yan", "Shice Liu", "Xiangtao Zhang", "Lujian Yao", "Fengxiang Yang", "Jinwei Chen", "Bo Li"], "title": "Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective", "comment": "Tech report", "summary": "In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYUV\u989c\u8272\u7a7a\u95f4\u7684\u8f7b\u91cf\u7ea7\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u57df\u5206\u6790\u53d1\u73b0Y\u901a\u9053\u4e3b\u8981\u4e22\u5931\u4f4e\u9891\u5185\u5bb9\uff0cUV\u901a\u9053\u53d7\u9ad8\u9891\u566a\u58f0\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9488\u5bf9\u6027\u7684\u53cc\u6d41\u6ce8\u610f\u529b\u6a21\u5757\u8fdb\u884c\u6062\u590d\u3002", "motivation": "\u5728\u79fb\u52a8\u4e92\u8054\u7f51\u65f6\u4ee3\uff0c\u8f7b\u91cf\u7ea7\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u9762\u4e34\u89c6\u89c9\u8d28\u91cf\u548c\u6a21\u578b\u7d27\u51d1\u6027\u7684\u6743\u8861\u3002\u73b0\u6709\u57fa\u4e8e\u89e3\u8026\u7b56\u7565\u7684\u65b9\u6cd5\uff08\u5982Retinex\u7406\u8bba\u548cYUV\u53d8\u6362\uff09\u5ffd\u89c6\u4e86\u901a\u9053\u7279\u5b9a\u7684\u9000\u5316\u6a21\u5f0f\u548c\u8de8\u901a\u9053\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u9891\u57df\u5206\u6790\u786e\u8ba4YUV\u989c\u8272\u7a7a\u95f4\u5bf9L3IE\u7684\u4f18\u52bf\uff0c\u53d1\u73b0Y\u901a\u9053\u4e3b\u8981\u4e22\u5931\u4f4e\u9891\u5185\u5bb9\uff0cUV\u901a\u9053\u53d7\u9ad8\u9891\u566a\u58f0\u5f71\u54cd\u3002\u63d0\u51faYUV-based\u8303\u5f0f\uff1a1) Y\u901a\u9053\u4f7f\u7528\u53cc\u6d41\u5168\u5c40-\u5c40\u90e8\u6ce8\u610f\u529b\u6a21\u5757\uff1b2) UV\u901a\u9053\u4f7f\u7528Y\u5f15\u5bfc\u7684\u5c40\u90e8\u611f\u77e5\u9891\u7387\u6ce8\u610f\u529b\u6a21\u5757\uff1b3) \u6700\u7ec8\u7279\u5f81\u878d\u5408\u4f7f\u7528\u5f15\u5bfc\u4ea4\u4e92\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ee5\u663e\u8457\u66f4\u5c11\u7684\u53c2\u6570\u6570\u91cf\u63d0\u4f9b\u4e86\u4f18\u8d8a\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eYUV\u989c\u8272\u7a7a\u95f4\u7684\u8f7b\u91cf\u7ea7\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u901a\u8fc7\u9488\u5bf9\u4e0d\u540c\u901a\u9053\u7279\u6027\u7684\u4e13\u95e8\u5904\u7406\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7d27\u51d1\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u89c6\u89c9\u8d28\u91cf\u63d0\u5347\u3002"}}
{"id": "2601.18175", "categories": ["cs.AI", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18175", "abs": "https://arxiv.org/abs/2601.18175", "authors": ["Daniel Russo"], "title": "Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success", "comment": null, "summary": "A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $\u03c7^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.", "AI": {"tldr": "\u6210\u529f\u6761\u4ef6\u5316\u662f\u4e00\u79cd\u901a\u8fc7\u6a21\u4eff\u6210\u529f\u8f68\u8ff9\u6765\u6539\u8fdb\u7b56\u7565\u7684\u6280\u672f\uff0c\u672c\u6587\u8bc1\u660e\u5b83\u5b9e\u9645\u4e0a\u89e3\u51b3\u4e86\u4e00\u4e2a\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\uff0c\u5728\u03c7\u00b2\u6563\u5ea6\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u7b56\u7565\u6539\u8fdb\uff0c\u4e14\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u6210\u529f\u6761\u4ef6\u5316\u6280\u672f\uff08\u5982\u62d2\u7edd\u91c7\u6837\u3001\u76ee\u6807\u6761\u4ef6RL\u3001\u51b3\u7b56\u53d8\u6362\u5668\uff09\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u80cc\u540e\u7684\u4f18\u5316\u95ee\u9898\u672c\u8d28\u4e00\u76f4\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u6280\u672f\u89e3\u51b3\u7684\u6570\u5b66\u4f18\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\uff0c\u5c06\u6210\u529f\u6761\u4ef6\u5316\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5927\u5316\u7b56\u7565\u6539\u8fdb\u540c\u65f6\u53d7\u03c7\u00b2\u6563\u5ea6\u7ea6\u675f\u3002\u7ea6\u675f\u534a\u5f84\u7531\u6570\u636e\u81ea\u52a8\u786e\u5b9a\uff0c\u5efa\u7acb\u4e86\u76f8\u5bf9\u7b56\u7565\u6539\u8fdb\u3001\u7b56\u7565\u53d8\u5316\u5e45\u5ea6\u548c\u52a8\u4f5c\u5f71\u54cd\u529b\u4e4b\u95f4\u7684\u7b49\u5f0f\u5173\u7cfb\u3002", "result": "\u6210\u529f\u6761\u4ef6\u5316\u88ab\u8bc1\u660e\u662f\u4e00\u4e2a\u4fdd\u5b88\u7684\u6539\u8fdb\u7b97\u5b50\uff0c\u4e0d\u4f1a\u964d\u4f4e\u6027\u80fd\u6216\u5f15\u53d1\u5371\u9669\u7684\u5206\u5e03\u504f\u79fb\u3002\u5f53\u5931\u8d25\u65f6\uff0c\u5b83\u4f1a\u901a\u8fc7\u51e0\u4e4e\u4e0d\u6539\u53d8\u7b56\u7565\u6765\u53ef\u89c2\u5bdf\u5730\u5931\u8d25\u3002\u5bf9\u5e38\u89c1\u7684\u56de\u62a5\u9608\u503c\u5316\u5b9e\u8df5\u7684\u5206\u6790\u8868\u660e\uff0c\u5b83\u53ef\u4ee5\u653e\u5927\u6539\u8fdb\uff0c\u4f46\u53ef\u80fd\u4ee5\u4e0e\u771f\u5b9e\u76ee\u6807\u4e0d\u5bf9\u9f50\u4e3a\u4ee3\u4ef7\u3002", "conclusion": "\u6210\u529f\u6761\u4ef6\u5316\u6280\u672f\u5b9e\u9645\u4e0a\u89e3\u51b3\u4e86\u4e00\u4e2a\u7279\u5b9a\u7684\u4fe1\u4efb\u57df\u4f18\u5316\u95ee\u9898\uff0c\u8fd9\u89e3\u91ca\u4e86\u5176\u5e7f\u6cdb\u6709\u6548\u6027\u7684\u6570\u5b66\u57fa\u7840\u3002\u8be5\u6280\u672f\u5177\u6709\u4fdd\u5b88\u6539\u8fdb\u7684\u7279\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2601.17330", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17330", "abs": "https://arxiv.org/abs/2601.17330", "authors": ["Laurent Caraffa"], "title": "Thermodynamically Optimal Regularization under Information-Geometric Constraints", "comment": "7 pages, 0 figures", "summary": "Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.\n  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.\n  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u70ed\u529b\u5b66\u6700\u4f18\u6027\u3001\u4fe1\u606f\u51e0\u4f55\u548c\u6b63\u5219\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\uff0cFisher-Rao\u5ea6\u91cf\u662f\u4fe1\u5ff5\u7a7a\u95f4\u4e0a\u552f\u4e00\u53ef\u63a5\u53d7\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u70ed\u529b\u5b66\u6700\u4f18\u6b63\u5219\u5316\u5bf9\u5e94\u4e8e\u6700\u5c0f\u5316\u5230\u53c2\u8003\u72b6\u6001\u7684Fisher-Rao\u8ddd\u79bb\u5e73\u65b9\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4f9d\u8d56\u4e8e\u4e00\u7cfb\u5217\u7ecf\u9a8c\u6210\u529f\u4f46\u7406\u8bba\u5f02\u8d28\u7684\u6b63\u5219\u5316\u6280\u672f\uff08\u5982\u6743\u91cd\u8870\u51cf\u3001dropout\u3001\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff09\uff0c\u540c\u65f6\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u7684\u80fd\u91cf\u6210\u672c\u6025\u5267\u589e\u52a0\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5b66\u4e60\u7b97\u6cd5\u662f\u5426\u63a5\u8fd1\u4efb\u4f55\u57fa\u672c\u6548\u7387\u754c\u9650\u7684\u7591\u95ee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u57fa\u4e8e\u4e09\u4e2a\u660e\u786e\u5047\u8bbe\uff1a(A1)\u6700\u4f18\u6027\u9700\u8981\u5185\u5728\u7684\u3001\u53c2\u6570\u5316\u4e0d\u53d8\u7684\u4fe1\u606f\u5ea6\u91cf\uff1b(A2)\u4fe1\u5ff5\u72b6\u6001\u7531\u5df2\u77e5\u7ea6\u675f\u4e0b\u7684\u6700\u5927\u71b5\u5206\u5e03\u5efa\u6a21\uff1b(A3)\u6700\u4f18\u8fc7\u7a0b\u662f\u51c6\u9759\u6001\u7684\u3002\u5728\u6b64\u6846\u67b6\u4e0b\u8bc1\u660e\u4e86\u6761\u4ef6\u6700\u4f18\u6027\u5b9a\u7406\uff0c\u63a8\u5bfc\u4e86\u9ad8\u65af\u548c\u5706\u5f62\u4fe1\u5ff5\u6a21\u578b\u7684\u8bf1\u5bfc\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u8bc1\u660e\u4e86Fisher-Rao\u5ea6\u91cf\u662f\u4fe1\u5ff5\u7a7a\u95f4\u4e0a\u552f\u4e00\u53ef\u63a5\u53d7\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u70ed\u529b\u5b66\u6700\u4f18\u6b63\u5219\u5316\u5bf9\u5e94\u4e8e\u6700\u5c0f\u5316\u5230\u53c2\u8003\u72b6\u6001\u7684Fisher-Rao\u8ddd\u79bb\u5e73\u65b9\u3002\u63a8\u5bfc\u51fa\u9ad8\u65af\u6a21\u578b\u5bf9\u5e94\u53cc\u66f2\u6d41\u5f62\uff0c\u5706\u5f62\u6a21\u578b\u5bf9\u5e94von Mises\u6d41\u5f62\uff0c\u5e76\u8868\u660e\u7ecf\u5178\u6b63\u5219\u5316\u65b9\u6848\u5728\u7ed3\u6784\u4e0a\u65e0\u6cd5\u4fdd\u8bc1\u70ed\u529b\u5b66\u6700\u4f18\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u51e0\u4f55\u548c\u70ed\u529b\u5b66\u57fa\u7840\uff0c\u5f15\u5165\u4e86\u5b66\u4e60\u7684\u70ed\u529b\u5b66\u6548\u7387\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u9884\u6d4b\uff0c\u4e3a\u7406\u89e3\u6b63\u5219\u5316\u6280\u672f\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.17350", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17350", "abs": "https://arxiv.org/abs/2601.17350", "authors": ["Xianliang Huang", "Zhizhou Zhong", "Shuhang Chen", "Yi Xu", "Juhong Guan", "Shuigeng Zhou"], "title": "NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields", "comment": "14 pages, 15 figures", "summary": "Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \\textbf{P}atch-based \\textbf{E}ntropy for \\textbf{R}ay \\textbf{E}mitting (\\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \\textbf{P}rogressively \\textbf{I}terative \\textbf{RE}storation (\\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.", "AI": {"tldr": "NeRF-MIR\uff1a\u4e00\u79cd\u7528\u4e8e\u4fee\u590d\u63a9\u7801\u56fe\u50cf\u7684\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u901a\u8fc7PERE\u7b56\u7565\u4f18\u5316\u5149\u7ebf\u53d1\u5c04\uff0cPIRE\u673a\u5236\u8fdb\u884c\u6e10\u8fdb\u5f0f\u4fee\u590d\uff0c\u4ee5\u53ca\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u5728\u63a9\u7801\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "NeRF\u5728\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u81ea\u7136\u573a\u666f\u4e2d\u5e38\u89c1\u7684\u635f\u574f\u56fe\u50cf\uff08\u5982\u63a9\u7801\u56fe\u50cf\uff09\u65f6\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u8fd9\u4e9b\u635f\u574f\u56fe\u50cf\u4f1a\u663e\u8457\u5f71\u54cdNeRF\u7684\u6548\u679c\u3002", "method": "1. \u63d0\u51faPERE\uff08\u57fa\u4e8e\u8865\u4e01\u7684\u71b5\u5149\u7ebf\u53d1\u5c04\uff09\u7b56\u7565\uff0c\u4f18\u5316\u5149\u7ebf\u53d1\u5c04\u5206\u5e03\u4ee5\u5b66\u4e60\u590d\u6742\u56fe\u50cf\u7eb9\u7406\uff1b2. \u5f15\u5165PIRE\uff08\u6e10\u8fdb\u8fed\u4ee3\u4fee\u590d\uff09\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u8bad\u7ec3\u8fc7\u7a0b\u9010\u6b65\u4fee\u590d\u63a9\u7801\u533a\u57df\uff1b3. \u8bbe\u8ba1\u52a8\u6001\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u81ea\u52a8\u91cd\u65b0\u6821\u51c6\u63a9\u7801\u533a\u57df\u7684\u635f\u5931\u6743\u91cd\uff1b4. \u6784\u5efa\u4e09\u4e2a\u63a9\u7801\u6570\u636e\u96c6\u4ee5\u652f\u6301NeRF-based\u63a9\u7801\u56fe\u50cf\u4fee\u590d\u7814\u7a76\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u548c\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cNeRF-MIR\u5728\u63a9\u7801\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "NeRF-MIR\u5c55\u793a\u4e86NeRF\u5728\u63a9\u7801\u56fe\u50cf\u4fee\u590d\u9886\u57df\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5149\u7ebf\u53d1\u5c04\u7b56\u7565\u3001\u6e10\u8fdb\u4fee\u590d\u673a\u5236\u548c\u52a8\u6001\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63a9\u7801\u56fe\u50cf\u4fee\u590d\u7684\u6027\u80fd\u3002"}}
{"id": "2601.17334", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17334", "abs": "https://arxiv.org/abs/2601.17334", "authors": ["Yufeng Huang"], "title": "Power-based Partial Attention: Bridging Linear-Complexity and Full Attention", "comment": "12 pages, 3 figures", "summary": "It is widely accepted from transformer research that \"attention is all we need\", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \\leq p \\leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e42\u57fa\u90e8\u5206\u6ce8\u610f\u529b\u673a\u5236\uff08PPA\uff09\uff0c\u5176\u590d\u6742\u5ea6\u4e3aO(L^{1+p})\uff0c\u5176\u4e2d0\u2264p\u22641\uff0c\u53ef\u4ee5\u7cfb\u7edf\u63a2\u7d22\u6ce8\u610f\u529b\u673a\u5236\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u5b58\u57280<p<1\u4f7f\u5f97\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u6ce8\u610f\u529b\u80fd\u8fbe\u5230\u4e0e\u5168\u6ce8\u610f\u529b\u76f8\u5f53\u7684\u6548\u679c\u3002", "motivation": "\u867d\u7136Transformer\u7814\u7a76\u666e\u904d\u8ba4\u4e3a\"\u6ce8\u610f\u529b\u5c31\u662f\u4e00\u5207\"\uff0c\u4f46\u4ece\u672a\u7cfb\u7edf\u91cf\u5316\u8fc7\u5230\u5e95\u9700\u8981\u591a\u5c11\u6ce8\u610f\u529b\u3002\u9700\u8981\u63a2\u7d22\u4e8c\u6b21\u590d\u6742\u5ea6O(L^2)\u7684\u5168\u6ce8\u610f\u529b\u662f\u5426\u5fc5\u8981\uff0c\u662f\u5426\u5b58\u5728\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u6ce8\u610f\u529b\u673a\u5236\u80fd\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u5e42\u57fa\u90e8\u5206\u6ce8\u610f\u529b\u673a\u5236\uff08PPA\uff09\uff0c\u5176\u590d\u6742\u5ea6\u4e3aO(L^{1+p})\uff0c\u5176\u4e2d0\u2264p\u22641\u3002\u5f53p=0\u65f6\u5bf9\u5e94\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff08\u7ebf\u6027\u590d\u6742\u5ea6\uff09\uff0cp=1\u65f6\u5bf9\u5e94\u5168\u6ce8\u610f\u529b\u3002\u901a\u8fc7\u8fd9\u4e00\u6784\u9020\u53ef\u4ee5\u7cfb\u7edf\u7814\u7a76\u6ce8\u610f\u529b\u7f29\u653e\u884c\u4e3a\uff08\u7531p\u63a7\u5236\uff09\u5bf9Transformer\u67b6\u6784\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6574\u4f53\u8d8b\u52bf\u5448\u73b0S\u66f2\u7ebf\u884c\u4e3a\uff1a\u6027\u80fd\u4ece\u6ed1\u52a8\u7a97\u53e3\uff08\u7ebf\u6027\u590d\u6742\u5ea6\uff09\u6ce8\u610f\u529b\u8fc7\u6e21\u5230\u5168\u6ce8\u610f\u529b\uff0c\u5728p\u503c\u7684\u72ed\u7a84\u7a97\u53e3\u5185\u5b8c\u6210\u8f6c\u53d8\uff0c\u5e76\u5728p\u63a5\u8fd11\u65f6\u8d8b\u4e8e\u5e73\u7a33\u3002\u5b9e\u9a8c\u8bc1\u660e\u5b58\u57280<p<1\u4f7f\u5f97O(L^{1+p})\u6ce8\u610f\u529b\u8db3\u4ee5\u8fbe\u5230\u4e0eO(L^2)\u5168\u6ce8\u610f\u529b\u76f8\u4f3c\u7684\u7ed3\u679c\u3002", "conclusion": "\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u66ff\u4ee3\u5168\u6ce8\u610f\u529b\uff0c\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002\u8fd9\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684Transformer\u67b6\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8868\u660e\u4e0d\u9700\u8981\u603b\u662f\u4f7f\u7528\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u5168\u6ce8\u610f\u529b\u3002"}}
{"id": "2601.17352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17352", "abs": "https://arxiv.org/abs/2601.17352", "authors": ["M. L. Mamud", "Piyoosh Jaysaval", "Frederick D Day-Lewis", "M. K. Mudunuru"], "title": "HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data", "comment": null, "summary": "Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyDeMiC\uff08\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u77ff\u7269\u5206\u7c7b\u5668\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u9ad8\u5149\u8c31\u77ff\u7269\u5206\u7c7b\u7684CNN\u6a21\u578b\uff0c\u5728\u566a\u58f0\u6570\u636e\u4e0b\u8868\u73b0\u9c81\u68d2\u3002", "motivation": "\u4f20\u7edf\u9ad8\u5149\u8c31\u77ff\u7269\u5206\u7c7b\u65b9\u6cd5\uff08\u5982\u5224\u522b\u5206\u6790\u3001\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\uff09\u5728\u5904\u7406\u73af\u5883\u566a\u58f0\u3001\u4f20\u611f\u5668\u9650\u5236\u548c\u9ad8\u7ef4\u6570\u636e\u8ba1\u7b97\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86HyDeMiC CNN\u6a21\u578b\uff0c\u4f7f\u7528USGS\u5e93\u4e2d115\u79cd\u77ff\u7269\u7684\u5b9e\u9a8c\u5ba4\u6d4b\u91cf\u9ad8\u5149\u8c31\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u5c06\u53c2\u8003\u77ff\u7269\u5149\u8c31\u4e0eHSI\u4f20\u611f\u5668\u54cd\u5e94\u51fd\u6570\u5377\u79ef\u751f\u6210\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u4ee5\u4e09\u79cd\u542b\u94dc\u77ff\u7269\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\u3002\u57281%\u30012%\u30015%\u548c10%\u566a\u58f0\u6c34\u5e73\u7684\u5408\u62102D\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "HyDeMiC\u5728\u5e72\u51c0\u548c\u4f4e\u566a\u58f0\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff08MCC = 1.00\uff09\uff0c\u5728\u4e2d\u7b49\u566a\u58f0\u6761\u4ef6\u4e0b\u4ecd\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\uff0c\u663e\u793a\u51fa\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "HyDeMiC\u5728\u5b58\u5728\u4e2d\u7b49\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u7a81\u663e\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u9ad8\u5149\u8c31\u6210\u50cf\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5176\u4e2d\u566a\u58f0\u901a\u5e38\u662f\u91cd\u8981\u6311\u6218\u3002"}}
{"id": "2601.17354", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17354", "abs": "https://arxiv.org/abs/2601.17354", "authors": ["Wenzhi Guo", "Guangchi Fang", "Shu Yang", "Bing Wang"], "title": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling", "comment": null, "summary": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.", "AI": {"tldr": "PocketGS\uff1a\u4e00\u79cd\u79fb\u52a8\u7aef3D\u9ad8\u65af\u6cfc\u6e85\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f3D\u573a\u666f\u5efa\u6a21", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4f9d\u8d56\u8d44\u6e90\u65e0\u7ea6\u675f\u7684\u8bad\u7ec3\u5047\u8bbe\uff0c\u65e0\u6cd5\u5728\u8bad\u7ec3\u65f6\u95f4\u77ed\u3001\u5185\u5b58\u6709\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u79fb\u52a8\u8bbe\u5907\u786c\u4ef6\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u573a\u666f\u5efa\u6a21\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPocketGS\u79fb\u52a8\u573a\u666f\u5efa\u6a21\u8303\u5f0f\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u8bbe\u8ba1\u7684\u64cd\u4f5c\u7b26\uff1aG\u64cd\u4f5c\u7b26\u6784\u5efa\u51e0\u4f55\u4fdd\u771f\u7684\u70b9\u4e91\u5148\u9a8c\uff1bI\u64cd\u4f5c\u7b26\u6ce8\u5165\u5c40\u90e8\u8868\u9762\u7edf\u8ba1\u4fe1\u606f\u4ee5\u521d\u59cb\u5316\u5404\u5411\u5f02\u6027\u9ad8\u65af\u5206\u5e03\uff0c\u51cf\u5c11\u65e9\u671f\u6761\u4ef6\u5dee\u8ddd\uff1bT\u64cd\u4f5c\u7b26\u901a\u8fc7\u7f13\u5b58\u4e2d\u95f4\u7ed3\u679c\u548c\u7d22\u5f15\u6620\u5c04\u68af\u5ea6\u6563\u5c04\u6765\u5c55\u5f00alpha\u5408\u6210\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u79fb\u52a8\u7aef\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePocketGS\u80fd\u591f\u8d85\u8d8a\u4e3b\u6d41\u5de5\u4f5c\u7ad93DGS\u57fa\u7ebf\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u91cd\u5efa\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u5728\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u6355\u83b7\u5230\u6e32\u67d3\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "PocketGS\u6210\u529f\u89e3\u51b3\u4e86\u6807\u51c63DGS\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u57fa\u672c\u77db\u76fe\uff0c\u6ee1\u8db3\u4e86\u8bad\u7ec3\u6548\u7387\u3001\u5185\u5b58\u7d27\u51d1\u6027\u548c\u5efa\u6a21\u4fdd\u771f\u5ea6\u7684\u7ade\u4e89\u9700\u6c42\uff0c\u4e3a\u79fb\u52a8\u7aef3D\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18217", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18217", "abs": "https://arxiv.org/abs/2601.18217", "authors": ["Zhihan Liu", "Lin Guan", "Yixin Nie", "Kai Zhang", "Zhuoqun Hao", "Lin Chen", "Asli Celikyilmaz", "Zhaoran Wang", "Na Zhang"], "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents", "comment": null, "summary": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.", "AI": {"tldr": "\u7814\u7a76LLM\u667a\u80fd\u4f53\u5728\u672a\u77e5\u6d4b\u8bd5\u57df\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u662f\u5f71\u54cd\u8de8\u57df\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u6dfb\u52a0\u5e72\u6270\u7279\u5f81\u6765\u589e\u5f3a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u7684\u968f\u673a\u5316\u6280\u672f\u3002", "motivation": "\u901a\u7528LLM\u667a\u80fd\u4f53\u901a\u5e38\u5728\u6709\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u4f46\u9700\u8981\u5728\u66f4\u5e7f\u6cdb\u7684\u672a\u77e5\u9886\u57df\u90e8\u7f72\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f53\u6700\u7ec8\u6d4b\u8bd5\u57df\u672a\u77e5\u65f6\uff0c\u54ea\u4e9b\u73af\u5883\u548c\u5efa\u6a21\u56e0\u7d20\u5bf9\u8de8\u57df\u6027\u80fd\u5f71\u54cd\u6700\u5927\u3002", "method": "\u9996\u5148\u8bc6\u522b\u4e0e\u8de8\u57df\u6cdb\u5316\u5f3a\u76f8\u5173\u7684\u73af\u5883\u8f74\uff1a\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u3002\u63d0\u51fa\u968f\u673a\u5316\u6280\u672f\uff1a\u5728\u72b6\u6001\u4e2d\u6dfb\u52a0\u5c11\u91cf\u4e0e\u76ee\u6807\u65e0\u5173\u7684\u5e72\u6270\u7279\u5f81\u6765\u589e\u52a0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u3002\u540c\u65f6\u5206\u6790\u5efa\u6a21\u9009\u62e9\uff1aSFT\u9884\u70ed/\u4e2d\u671f\u8bad\u7ec3\u548c\u9010\u6b65\u601d\u8003\u673a\u5236\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u662f\u5f71\u54cd\u8de8\u57df\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u800c\u975e\u9886\u57df\u771f\u5b9e\u6027\u6216\u6587\u672c\u76f8\u4f3c\u6027\u3002\u589e\u52a0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u80fd\u6709\u6548\u63d0\u9ad8\u8de8\u57df\u9c81\u68d2\u6027\u3002SFT\u9884\u70ed\u6709\u52a9\u4e8e\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u4f46\u4f1a\u635f\u5bb3\u672a\u5305\u542b\u5728\u4e2d\u671f\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9010\u6b65\u601d\u8003\u5728\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u5728\u667a\u80fd\u4f53\u540e\u8bad\u7ec3\u4e2d\uff0c\u5e94\u4f18\u5148\u8003\u8651\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u548c\u89c4\u5212\u590d\u6742\u5ea6\u800c\u975e\u9886\u57df\u771f\u5b9e\u6027\u3002\u901a\u8fc7\u6dfb\u52a0\u5e72\u6270\u7279\u5f81\u589e\u52a0\u72b6\u6001\u4fe1\u606f\u4e30\u5bcc\u5ea6\u662f\u63d0\u9ad8\u8de8\u57df\u6cdb\u5316\u7684\u6709\u6548\u65b9\u6cd5\u3002\u5efa\u6a21\u9009\u62e9\u9700\u8981\u5728\u9632\u6b62\u9057\u5fd8\u548c\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2601.17360", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.17360", "abs": "https://arxiv.org/abs/2601.17360", "authors": ["Jiankai Jin", "Xiangzheng Zhang", "Zhao Liu", "Deyue Zhang", "Quanchen Zou"], "title": "Robust Privacy: Inference-Time Privacy through Certified Robustness", "comment": null, "summary": "Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($\u03c3=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9c81\u68d2\u9690\u79c1(RP)\u6982\u5ff5\uff0c\u901a\u8fc7\u786e\u4fdd\u6a21\u578b\u9884\u6d4b\u5728\u8f93\u5165\u90bb\u57df\u5185\u4e0d\u53d8\u6765\u63d0\u4f9b\u63a8\u7406\u65f6\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5f00\u53d1\u5c5e\u6027\u9690\u79c1\u589e\u5f3a(APE)\u6280\u672f\u5c06\u8f93\u5165\u7ea7\u4e0d\u53d8\u6027\u8f6c\u5316\u4e3a\u5c5e\u6027\u7ea7\u9690\u79c1\u6548\u679c\uff0c\u6709\u6548\u7f13\u89e3\u6a21\u578b\u53cd\u6f14\u653b\u51fb\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u63a8\u7406\u65f6\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u8f93\u5165\u5c5e\u6027\uff0c\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u4fdd\u62a4\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u9690\u79c1\u6982\u5ff5\u6765\u9632\u6b62\u901a\u8fc7\u6a21\u578b\u8f93\u51fa\u63a8\u65ad\u654f\u611f\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u9c81\u68d2\u9690\u79c1(RP)\u6982\u5ff5\uff0c\u8981\u6c42\u6a21\u578b\u9884\u6d4b\u5728\u8f93\u5165\u90bb\u57df\u5185\u5177\u6709\u4e0d\u53d8\u6027\uff1b\u5f00\u53d1\u5c5e\u6027\u9690\u79c1\u589e\u5f3a(APE)\u6280\u672f\u5c06\u8f93\u5165\u7ea7\u4e0d\u53d8\u6027\u8f6c\u5316\u4e3a\u5c5e\u6027\u7ea7\u9690\u79c1\u4fdd\u62a4\uff1b\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u9a8c\u8bc1RP\u80fd\u6269\u5927\u654f\u611f\u5c5e\u6027\u503c\u7684\u517c\u5bb9\u8303\u56f4\u3002", "result": "RP\u80fd\u6709\u6548\u7f13\u89e3\u6a21\u578b\u53cd\u6f14\u653b\u51fb\uff1a\u5728\u4f4e\u566a\u58f0\u6c34\u5e73(\u03c3=0.1)\u4e0b\uff0c\u653b\u51fb\u6210\u529f\u7387\u4ece73%\u964d\u81f34%\uff1b\u5373\u4f7f\u4e0d\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u653b\u51fb\u6210\u529f\u7387\u4e5f\u80fd\u4ece73%\u964d\u81f344%\uff1b\u540c\u65f6\u4fdd\u6301\u90e8\u5206\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u9c81\u68d2\u9690\u79c1(RP)\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u786e\u4fdd\u6a21\u578b\u9884\u6d4b\u5728\u8f93\u5165\u90bb\u57df\u5185\u4e0d\u53d8\u6765\u9632\u6b62\u654f\u611f\u4fe1\u606f\u6cc4\u9732\uff0c\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.17366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17366", "abs": "https://arxiv.org/abs/2601.17366", "authors": ["Chengbo Ding", "Fenghe Tang", "Shaohua Kevin Zhou"], "title": "UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation", "comment": "Accepted by ISBI 2026", "summary": "Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.", "AI": {"tldr": "UCAD\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u8f6e\u5ed3\u611f\u77e5\u4f4d\u79fb\u6846\u67b6\uff0c\u7528\u4e8e\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u8d85\u50cf\u7d20\u751f\u6210\u89e3\u5256\u4e00\u81f4\u533a\u57df\uff0c\u5e76\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\u9009\u62e9\u5177\u6709\u6311\u6218\u6027\u7684\u533a\u57df\u8fdb\u884c\u4f4d\u79fb\uff0c\u4ee5\u589e\u5f3a\u4e00\u81f4\u6027\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u5206\u5272\u4e2d\u7684\u4f4d\u79fb\u7b56\u7565\u4ec5\u64cd\u4f5c\u77e9\u5f62\u533a\u57df\uff0c\u5ffd\u7565\u4e86\u89e3\u5256\u7ed3\u6784\uff0c\u5bfc\u81f4\u8fb9\u754c\u626d\u66f2\u548c\u8bed\u4e49\u4e0d\u4e00\u81f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u7559\u8f6e\u5ed3\u611f\u77e5\u8bed\u4e49\u5e76\u589e\u5f3a\u4e00\u81f4\u6027\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "UCAD\u6846\u67b6\uff1a1\uff09\u5229\u7528\u8d85\u50cf\u7d20\u751f\u6210\u4e0e\u89e3\u5256\u8fb9\u754c\u5bf9\u9f50\u7684\u89e3\u5256\u4e00\u81f4\u533a\u57df\uff1b2\uff09\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u9009\u62e9\u673a\u5236\uff0c\u9009\u62e9\u6027\u5730\u4f4d\u79fb\u5177\u6709\u6311\u6218\u6027\u7684\u533a\u57df\u4ee5\u589e\u5f3a\u4e00\u81f4\u6027\u5b66\u4e60\uff1b3\uff09\u63d0\u51fa\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u4e00\u81f4\u6027\u635f\u5931\uff0c\u81ea\u9002\u5e94\u7a33\u5b9a\u8bad\u7ec3\u5e76\u6709\u6548\u6b63\u5219\u5316\u672a\u6807\u8bb0\u533a\u57df\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUCAD\u5728\u6709\u9650\u6807\u6ce8\u6761\u4ef6\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "conclusion": "UCAD\u901a\u8fc7\u7ed3\u5408\u8d85\u50cf\u7d20\u751f\u6210\u89e3\u5256\u4e00\u81f4\u533a\u57df\u548c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u4f4d\u79fb\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u8fb9\u754c\u626d\u66f2\u548c\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3a\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18225", "abs": "https://arxiv.org/abs/2601.18225", "authors": ["Pei Wang", "Yanan Wu", "Xiaoshuai Song", "Weixun Wang", "Gengru Chen", "Zhongwen Li", "Kezhong Yan", "Ken Deng", "Qi Liu", "Shuaibing Zhao", "Shaopan Xiong", "Xuepeng Liu", "Xuefeng Chen", "Wanxi Deng", "Wenbo Su", "Bo Zheng"], "title": "ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants", "comment": null, "summary": "Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.", "AI": {"tldr": "ShopSimulator\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u4e2d\u6587\u7535\u5546\u8d2d\u7269\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u8d2d\u7269\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u6210\u529f\u7387\u4e0d\u8db340%\uff0c\u901a\u8fc7SFT+RL\u8bad\u7ec3\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u7684\u6a21\u62df\u73af\u5883\u6765\u5168\u9762\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u7535\u5546\u8d2d\u7269\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u4e2a\u6027\u5316\u504f\u597d\u7406\u89e3\u3001\u591a\u8f6e\u5bf9\u8bdd\u3001\u4ea7\u54c1\u68c0\u7d22\u548c\u76f8\u4f3c\u4ea7\u54c1\u533a\u5206\u7b49\u5173\u952e\u65b9\u9762\uff0c\u4e14\u4ec5\u5173\u6ce8\u8bc4\u4f30\u800c\u7f3a\u4e4f\u8bad\u7ec3\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4e86ShopSimulator\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4e2d\u6587\u8d2d\u7269\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u9519\u8bef\u5206\u6790\u8bc6\u522b\u667a\u80fd\u4f53\u7684\u5f31\u70b9\uff0c\u8fdb\u4e00\u6b65\u63a2\u7d22\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u76f8\u7ed3\u5408\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5373\u4f7f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728\u5b8c\u6574\u6210\u529f\u7387\u4e0a\u4e5f\u4f4e\u4e8e40%\uff0c\u667a\u80fd\u4f53\u5728\u957f\u8f68\u8ff9\u4e2d\u7684\u6df1\u5ea6\u641c\u7d22\u548c\u4ea7\u54c1\u9009\u62e9\u3001\u4e2a\u6027\u5316\u7ebf\u7d22\u7684\u5e73\u8861\u4f7f\u7528\u4ee5\u53ca\u4e0e\u7528\u6237\u7684\u6709\u6548\u4e92\u52a8\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002SFT\u548cRL\u76f8\u7ed3\u5408\u7684\u8bad\u7ec3\u65b9\u6cd5\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ShopSimulator\u4e3aLLM\u667a\u80fd\u4f53\u5728\u7535\u5546\u8d2d\u7269\u9886\u57df\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u9002\u5f53\u8bad\u7ec3\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u6539\u5584\u667a\u80fd\u4f53\u7684\u8d2d\u7269\u80fd\u529b\u3002"}}
{"id": "2601.17376", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17376", "abs": "https://arxiv.org/abs/2601.17376", "authors": ["Ruijin Hua", "Zichuan Liu", "Kun Zhang", "Yiyuan Yang"], "title": "Diversified Scaling Inference in Time Series Foundation Models", "comment": "23 pages, 16 figures, 9 tables", "summary": "The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u6f5c\u529b\uff0c\u53d1\u73b0\u6807\u51c6\u91c7\u6837\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u89e3\u7a7a\u95f4\uff0c\u800c\u901a\u8fc7\u5b9a\u5236\u5316\u65f6\u95f4\u5e8f\u5217\u6270\u52a8\u5b9e\u73b0\u591a\u6837\u5316\u63a8\u7406\u6269\u5c55\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u4f46\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1aTSFMs\u5728\u6807\u51c6\u91c7\u6837\u63a8\u7406\u6269\u5c55\u4e0b\u7684\u8868\u73b0\u5982\u4f55\uff0c\u4ee5\u53ca\u53d7\u63a7\u7684\u91c7\u6837\u591a\u6837\u6027\u662f\u5426\u80fd\u589e\u5f3a\u6027\u80fd\u3002", "method": "\u9996\u5148\u5206\u6790TSFMs\u5728\u6807\u51c6\u91c7\u6837\u4e0b\u7684\u7279\u6027\uff0c\u53d1\u73b0\u5176\u672a\u80fd\u9075\u5faa\u6269\u5c55\u5b9a\u5f8b\uff1b\u7136\u540e\u901a\u8fc7\u5b9a\u5236\u5316\u65f6\u95f4\u5e8f\u5217\u6270\u52a8\u5b9e\u73b0\u591a\u6837\u5316\u63a8\u7406\u6269\u5c55\uff0c\u6269\u5927\u751f\u6210\u5206\u5e03\u7684\u652f\u6491\u96c6\uff1b\u7406\u8bba\u4e0a\u5206\u6790\u591a\u6837\u6027-\u4fdd\u771f\u5ea6\u6743\u8861\uff0c\u63a8\u5bfc\u591a\u6837\u5316\u91c7\u6837\u4f18\u4e8e\u6807\u51c6\u91c7\u6837\u7684\u4e34\u754c\u6837\u672c\u9608\u503c\uff1b\u6700\u540e\u63d0\u51faRobustMSE\u6307\u6807\u6765\u91cf\u5316\u56fa\u5b9a\u9884\u7b97\u4e0bTSFM\u7684\u6027\u80fd\u4e0a\u9650\u3002", "result": "\u8de8\u591a\u79cdTSFMs\u548c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5f53\u7684\u591a\u6837\u5316\u63a8\u7406\u6269\u5c55\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002\u591a\u6837\u5316\u91c7\u6837\u5728\u8d85\u8fc7\u4e34\u754c\u9608\u503c\u540e\u80fd\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u91c7\u6837\uff0c\u8bc1\u660e\u4e86\u63a8\u7406\u8bbe\u8ba1\u4f5c\u4e3aTSFM\u4f18\u5316\u7684\u5173\u952e\u8ba1\u7b97\u9ad8\u6548\u7ef4\u5ea6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9610\u660e\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e2d\u63a8\u7406\u6269\u5c55\u3001\u91c7\u6837\u591a\u6837\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4f7f\u5f97\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3TSFMs\u5373\u53ef\u901a\u8fc7\u591a\u6837\u5316\u5927\u89c4\u6a21\u63a8\u7406\u65f6\u95f4\u5e8f\u5217\u5728\u5e76\u884c\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u6027\u80fd\u3002\u63a8\u7406\u8bbe\u8ba1\u6210\u4e3aTSFM\u4f18\u5316\u4e2d\u4e00\u4e2a\u5173\u952e\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u7ef4\u5ea6\u3002"}}
{"id": "2601.17383", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17383", "abs": "https://arxiv.org/abs/2601.17383", "authors": ["Chen Ling", "Kai Hu", "Hangcheng Liu", "Xingshuo Han", "Tianwei Zhang", "Changhai Ou"], "title": "Physical Prompt Injection Attacks on Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u7269\u7406\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff08PPIA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u9ed1\u76d2\u3001\u67e5\u8be2\u65e0\u5173\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7269\u7406\u5bf9\u8c61\u4e0a\u5d4c\u5165\u6076\u610f\u6392\u7248\u6307\u4ee4\u6765\u653b\u51fb\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe98%\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u5f00\u653e\u7269\u7406\u73af\u5883\u4e2d\u7684\u611f\u77e5\u548c\u63a8\u7406\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u8bbf\u95ee\u8f93\u5165\u901a\u9053\uff0c\u8981\u4e48\u4f9d\u8d56\u7528\u6237\u67e5\u8be2\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u5047\u8bbe\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5f88\u5c11\u6210\u7acb\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u7269\u7406\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff08PPIA\uff09\uff0c\u7ed3\u5408\u79bb\u7ebf\u9009\u62e9\u9ad8\u53ef\u8bc6\u522b\u6027\u548c\u8bed\u4e49\u6709\u6548\u7684\u89c6\u89c9\u63d0\u793a\uff0c\u4ee5\u53ca\u57fa\u4e8e\u65f6\u7a7a\u6ce8\u610f\u529b\u7684\u73af\u5883\u611f\u77e5\u7b56\u7565\u6027\u653e\u7f6e\uff0c\u786e\u4fdd\u6ce8\u5165\u7684\u63d0\u793a\u65e2\u53ef\u611f\u77e5\u53c8\u80fd\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u572810\u4e2a\u6700\u5148\u8fdb\u7684LVLMs\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5305\u62ec\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\uff0c\u6db5\u76d6\u89c6\u89c9\u95ee\u7b54\u3001\u89c4\u5212\u548c\u5bfc\u822a\u7b49\u4efb\u52a1\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe98%\uff0c\u5728\u4e0d\u540c\u7269\u7406\u6761\u4ef6\uff08\u8ddd\u79bb\u3001\u89c6\u89d2\u3001\u5149\u7167\uff09\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "PPIA\u662f\u9996\u4e2a\u9ed1\u76d2\u3001\u67e5\u8be2\u65e0\u5173\u7684\u7269\u7406\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\uff0c\u4ec5\u901a\u8fc7\u89c6\u89c9\u89c2\u5bdf\u5373\u53ef\u5b9e\u65bd\uff0c\u63ed\u793a\u4e86LVLMs\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18226", "abs": "https://arxiv.org/abs/2601.18226", "authors": ["Haotian Li", "Shijun Yang", "Weizhen Qi", "Silei Zhao", "Rui Hua", "Mingzhu Song", "Xiaojian Yang", "Chao Peng"], "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks", "comment": null, "summary": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u4f4d\u81ea\u6211\u8fdb\u5316\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u8fde\u7eed\u4efb\u52a1\u4ea4\u4e92\u4f5c\u4e3a\u7ecf\u9a8c\u6d41\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u5c06\u77ed\u671f\u6267\u884c\u53cd\u9988\u8f6c\u5316\u4e3a\u957f\u671f\u53ef\u91cd\u7528\u80fd\u529b\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u3002\u5f00\u53d1\u4e86Yunjue Agent\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u5408\u6210\u3001\u4f18\u5316\u548c\u91cd\u7528\u5de5\u5177\u6765\u5e94\u5bf9\u65b0\u5174\u6311\u6218\uff0c\u5e76\u5f15\u5165\u5e76\u884c\u6279\u91cf\u8fdb\u5316\u7b56\u7565\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5f00\u653e\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4efb\u52a1\u5206\u5e03\u6301\u7eed\u6f02\u79fb\u4e14\u5916\u90e8\u76d1\u7763\u7a00\u7f3a\u3002\u4f9d\u8d56\u9759\u6001\u5de5\u5177\u96c6\u6216\u79bb\u7ebf\u8bad\u7ec3\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u7cfb\u7edf\u80fd\u529b\u8fb9\u754c\u50f5\u5316\u4e14\u672a\u77e5\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6301\u7eed\u9002\u5e94\u548c\u6269\u5c55\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u539f\u4f4d\u81ea\u6211\u8fdb\u5316\u8303\u5f0f\uff0c\u5c06\u5de5\u5177\u8fdb\u5316\u4f5c\u4e3a\u80fd\u529b\u6269\u5c55\u7684\u5173\u952e\u8def\u5f84\u3002\u5f00\u53d1Yunjue Agent\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u5408\u6210\u3001\u4f18\u5316\u548c\u91cd\u7528\u5de5\u5177\u6765\u5e94\u5bf9\u6311\u6218\u3002\u5f15\u5165\u5e76\u884c\u6279\u91cf\u8fdb\u5316\u7b56\u7565\u4f18\u5316\u8fdb\u5316\u6548\u7387\u3002\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u4e8c\u5143\u53cd\u9988\u4fe1\u53f7\u8fdb\u884c\u5de5\u5177\u8bc4\u4f30\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u7684\u96f6\u8d77\u70b9\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u4e13\u6709\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u8865\u5145\u7684\u70ed\u542f\u52a8\u8bc4\u4f30\u8bc1\u5b9e\u79ef\u7d2f\u7684\u901a\u7528\u77e5\u8bc6\u53ef\u4ee5\u65e0\u7f1d\u8fc1\u79fb\u5230\u65b0\u9886\u57df\u3002\u63d0\u51fa\u4e86\u76d1\u6d4b\u8fdb\u5316\u6536\u655b\u7684\u65b0\u6307\u6807\uff0c\u7c7b\u4f3c\u4e8e\u4f20\u7edf\u4f18\u5316\u4e2d\u7684\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u3002", "conclusion": "\u539f\u4f4d\u81ea\u6211\u8fdb\u5316\u8303\u5f0f\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f00\u653e\u73af\u5883\u4e2d\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002\u901a\u8fc7\u5de5\u5177\u8fdb\u5316\u548c\u7ecf\u9a8c\u79ef\u7d2f\uff0c\u7cfb\u7edf\u80fd\u591f\u6301\u7eed\u6269\u5c55\u80fd\u529b\u8fb9\u754c\u5e76\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3002\u5f00\u6e90\u4e86\u4ee3\u7801\u5e93\u3001\u7cfb\u7edf\u8f68\u8ff9\u548c\u8fdb\u5316\u5de5\u5177\uff0c\u4fc3\u8fdb\u5f39\u6027\u81ea\u8fdb\u5316\u667a\u80fd\u7814\u7a76\u3002"}}
{"id": "2601.17396", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17396", "abs": "https://arxiv.org/abs/2601.17396", "authors": ["Vashista Nobaub"], "title": "GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems", "comment": "21 pages, 5 figures. Includes theoretical analysis, ablation studies, and experiments on synthetic and real vibration datasets. Code available", "summary": "Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.", "AI": {"tldr": "GO-OSC\uff1a\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u632f\u8361\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5236\u89c4\u8303\u53ef\u8bc6\u522b\u6f5c\u5728\u53c2\u6570\u5316\uff0c\u5b9e\u73b0\u5bf9\u65e9\u671f\u9000\u5316\uff08\u5982\u76f8\u4f4d\u6296\u52a8\u3001\u9891\u7387\u6f02\u79fb\uff09\u7684\u7a33\u5b9a\u68c0\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u80fd\u91cf\u57fa\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7075\u654f\u5ea6\u3002", "motivation": "\u632f\u8361\u7cfb\u7edf\u65e9\u671f\u9000\u5316\u5e38\u8868\u73b0\u4e3a\u51e0\u4f55\u5931\u771f\uff08\u5982\u76f8\u4f4d\u6296\u52a8\u3001\u9891\u7387\u6f02\u79fb\uff09\uff0c\u5728\u4fe1\u53f7\u80fd\u91cf\u53d8\u5316\u53ef\u68c0\u6d4b\u4e4b\u524d\u5c31\u5df2\u51fa\u73b0\u3002\u4f20\u7edf\u80fd\u91cf\u57fa\u8bca\u65ad\u548c\u65e0\u7ea6\u675f\u5b66\u4e60\u8868\u793a\u5bf9\u6b64\u7ed3\u6784\u4e0d\u654f\u611f\uff0c\u5bfc\u81f4\u68c0\u6d4b\u5ef6\u8fdf\u6216\u4e0d\u7a33\u5b9a\u3002", "method": "\u5f15\u5165GO-OSC\u6846\u67b6\uff1a1\uff09\u5f3a\u5236\u89c4\u8303\u53ef\u8bc6\u522b\u6f5c\u5728\u53c2\u6570\u5316\uff0c\u5b9e\u73b0\u8de8\u77ed\u65f6\u65e0\u6807\u7b7e\u7a97\u53e3\u7684\u7a33\u5b9a\u6bd4\u8f83\u548c\u805a\u5408\uff1b2\uff09\u5b9a\u4e49\u4e0d\u53d8\u7ebf\u6027\u51e0\u4f55\u63a2\u9488\uff0c\u9488\u5bf9\u6f5c\u5728\u7a7a\u95f4\u4e2d\u9000\u5316\u76f8\u5173\u65b9\u5411\uff1b3\uff09\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u8bc1\u660e\u51e0\u4f55\u63a2\u9488\u5728\u65e9\u671f\u76f8\u4f4d\u9000\u5316\u4e0b\u7684\u68c0\u6d4b\u4f18\u52bf\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u5728\u65e9\u671f\u4ec5\u76f8\u4f4d\u9000\u5316\u4e0b\uff0c\u80fd\u91cf\u57fa\u7edf\u8ba1\u91cf\u7684\u68c0\u6d4b\u80fd\u529b\u4e3a\u96f6\uff0c\u800c\u51e0\u4f55\u63a2\u9488\u5177\u6709\u4e25\u683c\u6b63\u7075\u654f\u5ea6\u3002\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u632f\u52a8\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u66f4\u65e9\u68c0\u6d4b\u3001\u66f4\u9ad8\u6570\u636e\u6548\u7387\u548c\u64cd\u4f5c\u6761\u4ef6\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u8868\u793a\u5b66\u4e60\u548c\u89c4\u8303\u53c2\u6570\u5316\uff0cGO-OSC\u6846\u67b6\u89e3\u51b3\u4e86\u632f\u8361\u7cfb\u7edf\u65e9\u671f\u9000\u5316\u68c0\u6d4b\u7684\u5173\u952e\u6311\u6218\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u7075\u654f\u5ea6\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2601.17388", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17388", "abs": "https://arxiv.org/abs/2601.17388", "authors": ["Xuan Ding", "Xiu Yan", "Chuanlong Xie", "Yao Zhu"], "title": "ONRW: Optimizing inversion noise for high-quality and robust watermark", "comment": "Preprint. Under review", "summary": "Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.", "AI": {"tldr": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u9c81\u68d2\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u6587\u672c\u4f18\u5316\u5c06\u5e72\u51c0\u56fe\u50cf\u8f6c\u6362\u4e3a\u53cd\u8f6c\u566a\u58f0\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u540e\u901a\u8fc7\u6269\u6563\u6a21\u578b\u8fed\u4ee3\u53bb\u566a\u751f\u6210\u9ad8\u8d28\u91cf\u6c34\u5370\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u56fe\u50cf\u635f\u574f\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6c34\u5370\u7cfb\u7edf\u867d\u7136\u80fd\u5728\u56fe\u50cf\u4e2d\u9690\u85cf\u6c34\u5370\u4e14\u5bf9\u56fe\u50cf\u8d28\u91cf\u5f71\u54cd\u5c0f\uff0c\u4f46\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u9047\u5230\u56fe\u50cf\u635f\u574f\u65f6\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "1. \u901a\u8fc7\u7a7a\u6587\u672c\u4f18\u5316\u8fc7\u7a0b\u5c06\u5e72\u51c0\u56fe\u50cf\u8f6c\u6362\u4e3a\u53cd\u8f6c\u566a\u58f0\uff1b2. \u5728\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u53cd\u8f6c\u566a\u58f0\uff1b3. \u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u6c34\u5370\u56fe\u50cf\uff1b4. \u5f15\u5165\u81ea\u6ce8\u610f\u529b\u7ea6\u675f\u548c\u4f2a\u63a9\u7801\u7b56\u7565\u9632\u6b62\u53cd\u8f6c\u566a\u58f0\u4f18\u5316\u626d\u66f2\u56fe\u50cf\u539f\u59cb\u8bed\u4e49\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u572812\u79cd\u4e0d\u540c\u56fe\u50cf\u53d8\u6362\u4e2d\u5e73\u5747\u6bd4\u7a33\u5b9a\u7b7e\u540d\u65b9\u6cd5\u9ad8\u51fa10%\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5bf9\u5404\u79cd\u56fe\u50cf\u635f\u574f\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6c34\u5370\u6846\u67b6\u65e2\u80fd\u4fdd\u8bc1\u6c34\u5370\u56fe\u50cf\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u53c8\u80fd\u589e\u5f3a\u6c34\u5370\u5bf9\u5404\u79cd\u635f\u574f\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6c34\u5370\u7cfb\u7edf\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2601.17407", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17407", "abs": "https://arxiv.org/abs/2601.17407", "authors": ["Prajwal Chauhan", "Salah Eddine Choutri", "Saif Eddin Jabari"], "title": "Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations", "comment": "Accepted to Transactions on Machine Learning Research (TMLR)", "summary": "Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.", "AI": {"tldr": "D-SENO\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u5f20\u5377\u79ef\u548c\u6324\u538b-\u6fc0\u52b1\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347PDE\u6c42\u89e3\u6548\u7387\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb\u7ea620\u500d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u548c\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u91cf\u5927\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u90e8\u7f72\u7f13\u6162\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684PDE\u6c42\u89e3\u5668\u3002", "method": "\u63d0\u51faD-SENO\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u5f20\u5377\u79ef\u5757\uff08\u6355\u6349\u5bbd\u611f\u53d7\u91ce\u548c\u7269\u7406\u4f9d\u8d56\uff09\u548c\u6324\u538b-\u6fc0\u52b1\u6a21\u5757\uff08\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u81ea\u9002\u5e94\u91cd\u65b0\u6821\u51c6\u7279\u5f81\u901a\u9053\uff09\u3002", "result": "\u5728\u591a\u79cdPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u7ffc\u578b\u52bf\u6d41\u3001\u591a\u5b54\u4ecb\u8d28\u8fbe\u897f\u6d41\u3001\u7ba1\u9053\u6cca\u8083\u53f6\u6d41\u3001\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u6da1\u6d41\u573a\uff09\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u6807\u51c6Transformer\u6a21\u578b\u548c\u795e\u7ecf\u7b97\u5b50\u5feb\u7ea620\u500d\uff0c\u540c\u65f6\u7cbe\u5ea6\u76f8\u5f53\u6216\u66f4\u9ad8\u3002", "conclusion": "D-SENO\u662f\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7b97\u5b50\uff0c\u80fd\u591f\u5feb\u901f\u6c42\u89e3\u591a\u79cdPDE\u95ee\u9898\uff0c\u6d88\u878d\u7814\u7a76\u8868\u660eSE\u6a21\u5757\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2601.18308", "categories": ["cs.AI", "cs.SI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18308", "abs": "https://arxiv.org/abs/2601.18308", "authors": ["Geunsik Lim"], "title": "A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience", "comment": "19 pages", "summary": "As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.", "AI": {"tldr": "Climate RADAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u53ef\u9760\u6027\u5c42\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6e90\u6570\u636e\u751f\u6210\u4e2a\u6027\u5316\u884c\u52a8\u5efa\u8bae\uff0c\u5c06\u707e\u5bb3\u9884\u8b66\u4ece\u8b66\u62a5\u4f20\u9012\u8f6c\u53d8\u4e3a\u884c\u52a8\u6267\u884c\uff0c\u63d0\u9ad8\u4fdd\u62a4\u884c\u52a8\u6267\u884c\u7387\u5e76\u51cf\u5c11\u54cd\u5e94\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u867d\u7136\u80fd\u5feb\u901f\u4f20\u64ad\u8b66\u62a5\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u89e6\u53d1\u53ca\u65f6\u7684\u4fdd\u62a4\u884c\u52a8\uff0c\u5bfc\u81f4\u53ef\u9884\u9632\u7684\u635f\u5931\u548c\u4e0d\u5e73\u7b49\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5c06\u8b66\u62a5\u8f6c\u5316\u4e3a\u5b9e\u9645\u884c\u52a8\u7684\u7cfb\u7edf\u3002", "method": "\u6574\u5408\u6c14\u8c61\u3001\u6c34\u6587\u3001\u8106\u5f31\u6027\u548c\u793e\u4f1a\u6570\u636e\u5f62\u6210\u7efc\u5408\u98ce\u9669\u6307\u6570\uff0c\u4f7f\u7528\u5e26\u6709\u62a4\u680f\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2a\u6027\u5316\u5efa\u8bae\uff0c\u901a\u8fc7\u516c\u6c11\u3001\u5fd7\u613f\u8005\u548c\u5e02\u653f\u63a5\u53e3\u63d0\u4f9b\u63a8\u8350\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u3001\u7528\u6237\u7814\u7a76\u548c\u5e02\u653f\u8bd5\u70b9\u8bc4\u4f30\u663e\u793a\uff0c\u7cfb\u7edf\u63d0\u9ad8\u4e86\u4fdd\u62a4\u884c\u52a8\u6267\u884c\u7387\uff0c\u51cf\u5c11\u4e86\u54cd\u5e94\u5ef6\u8fdf\uff0c\u589e\u5f3a\u4e86\u53ef\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "conclusion": "Climate RADAR\u7ed3\u5408\u9884\u6d4b\u5206\u6790\u3001\u884c\u4e3a\u79d1\u5b66\u548c\u8d1f\u8d23\u4efbAI\uff0c\u63a8\u8fdb\u4ee5\u4eba\u4e3a\u672c\u3001\u900f\u660e\u548c\u516c\u5e73\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff0c\u4e3a\u7b26\u5408\u8981\u6c42\u7684\u707e\u5bb3\u97e7\u6027\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2601.17399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17399", "abs": "https://arxiv.org/abs/2601.17399", "authors": ["Rui Fang", "Jian Li", "Wei Chen", "Bin Hu", "Ying-Cong Chen", "Xin Tang", "Liang Diao"], "title": "ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\\% compared to full-pass evaluations while maintaining a ranking correlation of $\u03c1=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.", "AI": {"tldr": "ReLE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bca\u65ad\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5404\u5411\u5f02\u6027\u7684\u53ef\u6269\u5c55\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u6df7\u5408\u8bc4\u5206\u673a\u5236\u548c\u52a8\u6001\u65b9\u5dee\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u5728\u51cf\u5c1170%\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6392\u540d\u76f8\u5173\u6027\uff0c\u63ed\u793a\u6a21\u578b\u9ad8\u5ea6\u4e13\u4e1a\u5316\u800c\u975e\u666e\u904d\u4f18\u8d8a\u7684\u7279\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587\u7406\u89e3\u65b9\u9762\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u51c6\u786e\u8bc4\u4f30\u5176\u80fd\u529b\u9762\u4e34\u57fa\u51c6\u6d4b\u8bd5\u9971\u548c\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u6311\u6218\u3002\u9759\u6001\u6392\u884c\u699c\u53ea\u80fd\u63d0\u4f9b\u5feb\u7167\u6392\u540d\uff0c\u5f80\u5f80\u63a9\u76d6\u4e86\u80fd\u529b\u4e4b\u95f4\u7684\u7ed3\u6784\u6027\u6743\u8861\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bca\u65ad\u80fd\u529b\u5404\u5411\u5f02\u6027\uff08\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u8868\u73b0\u4e0d\u5747\u5300\u6027\uff09\u7684\u7cfb\u7edf\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faReLE\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u65b9\u6cd5\uff1a1\uff09\u7b26\u53f7\u5316\u6df7\u5408\u8bc4\u5206\u673a\u5236\uff0c\u6d88\u9664\u63a8\u7406\u4efb\u52a1\u4e2d\u57fa\u4e8e\u5d4c\u5165\u7684\u8bef\u5224\uff1b2\uff09\u57fa\u4e8eNeyman\u5206\u914d\u548c\u566a\u58f0\u6821\u6b63\u7684\u52a8\u6001\u65b9\u5dee\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e86304\u4e2a\u6a21\u578b\uff08189\u4e2a\u5546\u4e1a\u6a21\u578b\uff0c115\u4e2a\u5f00\u6e90\u6a21\u578b\uff09\uff0c\u8986\u76d6207,843\u4e2a\u6837\u672c\u7684\u9886\u57df\u00d7\u80fd\u529b\u6b63\u4ea4\u77e9\u9635\u3002", "result": "\u4e0e\u5168\u91cf\u8bc4\u4f30\u76f8\u6bd4\uff0cReLE\u51cf\u5c1170%\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u6392\u540d\u76f8\u5173\u6027\u03c1=0.96\u3002\u5206\u6790\u663e\u793a\u805a\u5408\u6392\u540d\u5bf9\u6743\u91cd\u65b9\u6848\u9ad8\u5ea6\u654f\u611f\uff1a\u6a21\u578b\u5728ReLE\u4e2d\u7684\u6392\u540d\u7a33\u5b9a\u6027\u632f\u5e45\u4e3a11.4\uff0c\u800c\u4f20\u7edf\u57fa\u51c6\u7ea6\u4e3a5.0\uff0c\u8bc1\u5b9e\u73b0\u4ee3\u6a21\u578b\u9ad8\u5ea6\u4e13\u4e1a\u5316\u800c\u975e\u666e\u904d\u4f18\u8d8a\u3002", "conclusion": "ReLE\u4e0d\u662f\u66ff\u4ee3\u5168\u9762\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u662f\u4f5c\u4e3a\u6f14\u5316\u6a21\u578b\u666f\u89c2\u7684\u9ad8\u9891\u8bca\u65ad\u76d1\u63a7\u5de5\u5177\u3002\u5b83\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u529b\u5404\u5411\u5f02\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u66f4\u7cbe\u7ec6\u5316\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.18353", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18353", "abs": "https://arxiv.org/abs/2601.18353", "authors": ["Tuhin Chakrabarty", "Paramveer S. Dhillon"], "title": "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books", "comment": "Proceedings of CHI 2026 Conference (To Appear)", "summary": "Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes \"good writing.\" These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u5728\u6a21\u4eff\u4f5c\u5bb6\u98ce\u683c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7ecf\u8fc7\u5fae\u8c03\u540e\u751a\u81f3\u80fd\u8d85\u8d8a\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5f15\u53d1\u4e86\u5173\u4e8e\u521b\u610f\u5199\u4f5c\u672c\u8d28\u548c\u521b\u610f\u52b3\u52a8\u672a\u6765\u7684\u6df1\u523b\u95ee\u9898\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u89c2\u5ff5\u2014\u2014\u521b\u610f\u5199\u4f5c\u957f\u671f\u4ee5\u6765\u88ab\u8ba4\u4e3a\u662f\u4eba\u7c7b\u72ec\u6709\u7684\u80fd\u529b\uff0c\u9700\u8981\u673a\u5668\u65e0\u6cd5\u590d\u5236\u7684\u72ec\u7279\u58f0\u97f3\u548c\u98ce\u683c\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u751f\u6210\u5f0fAI\u662f\u5426\u80fd\u591f\u771f\u6b63\u6a21\u4eff\u8457\u540d\u4f5c\u5bb6\u7684\u5199\u4f5c\u98ce\u683c\uff0c\u4ee5\u53ca\u8fd9\u79cd\u80fd\u529b\u5bf9\u4eba\u7c7b\u521b\u610f\u5199\u4f5c\u7684\u5f71\u54cd\u3002", "method": "\u884c\u4e3a\u5b9e\u9a8c\u8bbe\u8ba1\uff1a28\u540dMFA\u5199\u4f5c\u4e13\u5bb6\u4e0e\u4e09\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ade\u4e89\u6a21\u4eff50\u4f4d\u5907\u53d7\u597d\u8bc4\u7684\u4f5c\u5bb6\u98ce\u683c\u3002\u901a\u8fc7\u4e0a\u4e0b\u6587\u63d0\u793a\u548c\u5fae\u8c03\u4f5c\u8005\u5b8c\u6574\u4f5c\u54c1\u4e24\u79cd\u6761\u4ef6\u8fdb\u884c\u6d4b\u8bd5\u3002\u753128\u540d\u4e13\u5bb6\u8bc4\u59d4\u548c131\u540d\u666e\u901a\u8bc4\u59d4\u8fdb\u884c\u76f2\u5ba1\u914d\u5bf9\u6bd4\u8f83\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u4f5c\u5bb6\u7684\u6c47\u62a5\u8bbf\u8c08\u6536\u96c6\u5b9a\u6027\u6570\u636e\u3002", "result": "\u5728\u4e0a\u4e0b\u6587\u63d0\u793a\u6761\u4ef6\u4e0b\uff0c\u4e13\u5bb6\u8bc4\u59d482.7%\u7684\u60c5\u51b5\u4e0b\u504f\u597d\u4eba\u7c7b\u5199\u4f5c\uff1b\u4f46\u5728\u5fae\u8c03\u4f5c\u8005\u5b8c\u6574\u4f5c\u54c1\u540e\uff0c\u8fd9\u4e00\u504f\u597d\u9006\u8f6c\u4e3a62%\u504f\u597dAI\u5199\u4f5c\u3002\u666e\u901a\u8bc4\u59d4\u5219\u59cb\u7ec8\u504f\u597dAI\u5199\u4f5c\u3002\u4e13\u5bb6\u4f5c\u5bb6\u8868\u793a\uff0c\u4ed6\u4eec\u5bf9AI\u5199\u4f5c\u7684\u504f\u597d\u89e6\u53d1\u4e86\u8eab\u4efd\u5371\u673a\uff0c\u524a\u5f31\u4e86\u5ba1\u7f8e\u81ea\u4fe1\uff0c\u5e76\u8d28\u7591\u4e86\"\u4f18\u79c0\u5199\u4f5c\"\u7684\u6784\u6210\u6807\u51c6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u5173\u4e8eAI\u521b\u610f\u5c40\u9650\u6027\u7684\u4f20\u7edf\u8bba\u8ff0\uff0c\u63d0\u51fa\u4e86\u5173\u4e8e\u521b\u610f\u52b3\u52a8\u672a\u6765\u7684\u6839\u672c\u6027\u95ee\u9898\u3002AI\u5728\u6a21\u4eff\u4f5c\u5bb6\u98ce\u683c\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u4ec5\u6280\u672f\u5c42\u9762\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\uff0c\u66f4\u91cd\u8981\u7684\u662f\u5f15\u53d1\u4e86\u4eba\u7c7b\u521b\u610f\u5de5\u4f5c\u8005\u5bf9\u81ea\u8eab\u4ef7\u503c\u548c\u4e13\u4e1a\u8eab\u4efd\u7684\u6df1\u523b\u53cd\u601d\u3002"}}
{"id": "2601.17441", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17441", "abs": "https://arxiv.org/abs/2601.17441", "authors": ["Ondrej Bohdal", "Taha Ceritli", "Mete Ozay", "Jijoong Moon", "Kyeng-Hun Lee", "Hyeonmok Ko", "Umberto Michieli"], "title": "Data-driven Clustering and Merging of Adapters for On-device Large Language Models", "comment": "Accepted at ICASSP 2026", "summary": "On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.", "AI": {"tldr": "\u63d0\u51faD2C\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u4efb\u52a1\u793a\u4f8b\u5bf9\u9002\u914d\u5668\u8fdb\u884c\u805a\u7c7b\uff0c\u5408\u5e76\u540c\u7c07\u9002\u914d\u5668\u521b\u5efa\u591a\u4efb\u52a1\u9002\u914d\u5668\uff0c\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u5b58\u50a8\u5bb9\u91cf\u6709\u9650\uff0c\u65e0\u6cd5\u5b58\u50a8\u6240\u6709\u4efb\u52a1\u9002\u914d\u5668\uff0c\u9700\u8981\u9009\u62e9\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u4ee3\u8868\u6027\u9002\u914d\u5668\uff0c\u4f46\u73b0\u6709\u6587\u732e\u5c1a\u672a\u63a2\u7d22\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faD2C\u9002\u914d\u5668\u805a\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u793a\u4f8b\uff08\u5982\u6bcf\u4e2a\u4efb\u52a110\u4e2a\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u7cbe\u70bc\u805a\u7c7b\u5206\u914d\uff0c\u5408\u5e76\u6bcf\u4e2a\u7c07\u5185\u7684\u9002\u914d\u5668\u521b\u5efa\u591a\u4efb\u52a1\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8003\u8651\u5b58\u50a8\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "D2C\u65b9\u6cd5\u901a\u8fc7\u9002\u914d\u5668\u805a\u7c7b\u548c\u5408\u5e76\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u4efb\u52a1\u9002\u914d\u5668\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2601.17408", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17408", "abs": "https://arxiv.org/abs/2601.17408", "authors": ["Harsharaj Pathak", "Vineeth N Balasubramanian"], "title": "Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity", "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90bb\u57df\u7b7e\u540d\u7684\u6e90\u81ea\u7531\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u76ee\u6807\u57df\u6837\u672c\u9884\u6d4b\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\uff0c\u4f7f\u7528\u5355\u4e00\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u66f4\u597d\u7684\u57df\u9002\u5e94\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6e90\u81ea\u7531\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u90bb\u57df\u4e00\u81f4\u6027\u6982\u5ff5\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u8bef\u5bfc\u6027\u90bb\u57df\u4fe1\u606f\u7684\u5f71\u54cd\u800c\u4ea7\u751f\u9519\u8bef\u3002\u9700\u8981\u89e3\u51b3\u90bb\u57df\u4fe1\u606f\u566a\u58f0\u95ee\u9898\uff0c\u5b66\u4e60\u66f4\u5177\u4fe1\u606f\u6027\u7684\u805a\u7c7b\u3002", "method": "\u63d0\u51fa\u90bb\u57df\u7b7e\u540d\u6982\u5ff5\uff0c\u901a\u8fc7\u4f18\u5316\u76ee\u6807\u57df\u6837\u672c\u9884\u6d4b\u7684\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u6765\u5b66\u4e60\u66f4\u51c6\u786e\u7684\u805a\u7c7b\u3002\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u4e00\u635f\u5931\u9879\uff0c\u4e13\u95e8\u9488\u5bf9\u76ee\u6807\u57df\u6837\u672c\u7684\u9884\u6d4b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684VisDA\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5176\u4ed6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u90bb\u57df\u7b7e\u540d\u6982\u5ff5\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u566a\u58f0\u90bb\u57df\u7684\u5f71\u54cd\uff0c\u5b66\u4e60\u66f4\u5177\u4fe1\u606f\u6027\u7684\u805a\u7c7b\uff0c\u4ec5\u4f7f\u7528\u5355\u4e00\u635f\u5931\u51fd\u6570\u5c31\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u6e90\u81ea\u7531\u57df\u81ea\u9002\u5e94\u3002"}}
{"id": "2601.18383", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18383", "abs": "https://arxiv.org/abs/2601.18383", "authors": ["Zhenyuan Guo", "Tong Chen", "Wenlong Meng", "Chen Gong", "Xin Yu", "Chengkun Wei", "Wenzhi Chen"], "title": "Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models", "comment": null, "summary": "Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDynTS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u56fe\u5206\u6790\u63a8\u7406\u8f68\u8ff9\uff0c\u8bc6\u522b\u5173\u952e\u51b3\u7b56token\u5e76\u4ec5\u4fdd\u7559\u5176KV\u7f13\u5b58\uff0c\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u751f\u6210\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u65f6\u4f1a\u4ea7\u751f\u5de8\u5927\u7684\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u8fd9\u6210\u4e3a\u6a21\u578b\u6548\u7387\u7684\u74f6\u9888\u3002\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u8f68\u8ff9\u4e2d\u53ea\u6709\u90e8\u5206\u5173\u952etoken\u5bf9\u6700\u7ec8\u7b54\u6848\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\uff0c\u5176\u4f59token\u8d21\u732e\u5f88\u5c0f", "method": "\u63d0\u51fa\u52a8\u6001\u601d\u7ef4token\u9009\u62e9\u65b9\u6cd5(DynTS)\uff0c\u5229\u7528\u6ce8\u610f\u529b\u56fe\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u4e2dtoken\u7684\u5f71\u54cd\u529b\uff0c\u8bc6\u522b\u51b3\u7b56\u5173\u952etoken\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4ec5\u4fdd\u7559\u8fd9\u4e9b\u5173\u952etoken\u7684KV\u7f13\u5b58\u72b6\u6001\uff0c\u5254\u9664\u5197\u4f59\u6761\u76ee", "result": "\u901a\u8fc7\u9009\u62e9\u6027\u4fdd\u7559\u5173\u952etoken\u7684KV\u7f13\u5b58\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u4f18\u5316\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387", "conclusion": "DynTS\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u5173\u952e\u51b3\u7b56token\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684KV\u7f13\u5b58\u7ba1\u7406\uff0c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2601.17467", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17467", "abs": "https://arxiv.org/abs/2601.17467", "authors": ["Jianxiong Zhang", "Bing Guo", "Yuming Jiang", "Haobo Wang", "Bo An", "Xuefeng Du"], "title": "Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping", "comment": null, "summary": "Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.", "AI": {"tldr": "ARS\u901a\u8fc7\u6270\u52a8\u63a8\u7406\u8f68\u8ff9\u8fb9\u754c\u5d4c\u5165\u751f\u6210\u53cd\u4e8b\u5b9e\u7b54\u6848\uff0c\u5b66\u4e60\u57fa\u4e8e\u7b54\u6848\u4e00\u81f4\u6027\u7684\u68c0\u6d4b\u53cb\u597d\u8868\u793a\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u63d0\u5347\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5e38\u751f\u6210\u770b\u4f3c\u8fde\u8d2f\u4f46\u7b54\u6848\u9519\u8bef\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u73b0\u6709\u57fa\u4e8e\u8f68\u8ff9\u6587\u672c\u6216\u9690\u85cf\u72b6\u6001\u7684\u68c0\u6d4b\u65b9\u6cd5\u8106\u5f31\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u8868\u9762\u6a21\u5f0f\u800c\u975e\u7b54\u6848\u6709\u6548\u6027", "method": "\u63d0\u51fa\u7b54\u6848\u4e00\u81f4\u6027\u8868\u793a\u5851\u5f62(ARS)\uff1a\u901a\u8fc7\u5c0f\u89c4\u6a21\u6f5c\u5728\u5e72\u9884\uff08\u6270\u52a8\u8f68\u8ff9\u8fb9\u754c\u5d4c\u5165\uff09\u751f\u6210\u53cd\u4e8b\u5b9e\u7b54\u6848\uff0c\u6839\u636e\u7b54\u6848\u4e00\u81f4\u6027\u6807\u6ce8\u6270\u52a8\uff0c\u5b66\u4e60\u5c06\u7b54\u6848\u4e00\u81f4\u7684\u8868\u793a\u805a\u96c6\u3001\u4e0d\u4e00\u81f4\u7684\u5206\u79bb\uff0c\u66b4\u9732\u5e7b\u89c9\u98ce\u9669", "result": "ARS\u6301\u7eed\u6539\u8fdb\u68c0\u6d4b\u6548\u679c\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u5b66\u4e60\u5230\u7684\u8868\u793a\u53ef\u4e0e\u73b0\u6709\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u6d4b\u5668\u5373\u63d2\u5373\u7528\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u65e0\u9700\u4eba\u5de5\u6807\u6ce8", "conclusion": "ARS\u901a\u8fc7\u7f16\u7801\u7b54\u6848\u7a33\u5b9a\u6027\u5b66\u4e60\u68c0\u6d4b\u53cb\u597d\u7684\u8868\u793a\uff0c\u6709\u6548\u66b4\u9732\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u4e0d\u7a33\u5b9a\u6027\uff0c\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2601.17414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17414", "abs": "https://arxiv.org/abs/2601.17414", "authors": ["Abdul Hasib", "A. S. M. Ahsanul Sarkar Akib"], "title": "Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase", "comment": null, "summary": "The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \\$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFirebase\u4e91\u6570\u636e\u5e93\u7684\u7269\u8054\u7f51\u7cfb\u7edf\uff0c\u7528\u4e8e\u73af\u5883\u76d1\u6d4b\u548c\u8bbe\u5907\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6570\u636e\u540c\u6b65\u548c\u8fdc\u7a0b\u63a7\u5236\u529f\u80fd\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u6fc0\u589e\u5e26\u6765\u4e86\u8fdc\u7a0b\u76d1\u63a7\u548c\u63a7\u5236\u7684\u65b0\u673a\u9047\uff0c\u4f46\u4f20\u7edf\u76d1\u63a7\u7cfb\u7edf\u5728\u5b9e\u65f6\u6570\u636e\u8bbf\u95ee\u3001\u8fdc\u7a0b\u53ef\u63a7\u6027\u548c\u4e91\u96c6\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528ESP32\u5fae\u63a7\u5236\u5668\u8fde\u63a5DHT22\u6e29\u6e7f\u5ea6\u4f20\u611f\u5668\u548cHC-SR04\u8d85\u58f0\u6ce2\u8ddd\u79bb\u4f20\u611f\u5668\uff0c\u901a\u8fc7Firebase\u5b9e\u65f6\u6570\u636e\u5e93\u5b9e\u73b0\u6570\u636e\u540c\u6b65\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u4e91\u7684\u754c\u9762\u8fdc\u7a0b\u63a7\u5236\u4e24\u4e2aLED\u6307\u793a\u706f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6570\u636e\u4f20\u8f93\u6210\u529f\u7387\u8fbe99.2%\uff0c\u5b9e\u65f6\u63a7\u5236\u5ef6\u8fdf\u4f4e\u4e8e1.5\u79d2\uff0c\u5177\u5907\u6301\u4e45\u6570\u636e\u5b58\u50a8\u529f\u80fd\uff0c\u7cfb\u7edf\u603b\u5b9e\u73b0\u6210\u672c\u4e3a32.50\u7f8e\u5143\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u67b6\u6784\u4e3a\u5404\u79cd\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0cFirebase\u96c6\u6210\u65e0\u9700\u590d\u6742\u670d\u52a1\u5668\u57fa\u7840\u8bbe\u65bd\uff0c\u4f7f\u8d44\u6e90\u6709\u9650\u7684\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u4e5f\u80fd\u5b9e\u73b0\u5148\u8fdb\u7684\u7269\u8054\u7f51\u5e94\u7528\u3002"}}
{"id": "2601.17420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17420", "abs": "https://arxiv.org/abs/2601.17420", "authors": ["Shiu-hong Kao", "Chak Ho Huang", "Huaiqian Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction", "comment": "Project page: https://danielshkao.github.io/cot-seg.html", "summary": "Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.", "AI": {"tldr": "CoT-Seg\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u81ea\u6821\u6b63\u673a\u5236\u6765\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u57df\u5916\u56fe\u50cf\u7684\u5206\u5272\u4efb\u52a1", "motivation": "\u73b0\u6709\u63a8\u7406\u5206\u5272\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u57df\u5916\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u53d7\u4eba\u7c7b\u89e3\u51b3\u96be\u9898\u65f6\u9010\u6b65\u601d\u8003\u3001\u67e5\u627e\u4fe1\u606f\u3001\u8bc4\u4f30\u548c\u4fee\u6b63\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u601d\u8003\u8fc7\u7a0b\u7684\u7cfb\u7edf", "method": "CoT-Seg\u5229\u7528\u9884\u8bad\u7ec3MLLMs\uff08GPT-4o\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u5143\u6307\u4ee4\uff0c\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff0c\u8bc6\u522b\u9690\u542b\u6216\u590d\u6742\u63d0\u793a\u4e0b\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u5e76\u5305\u542b\u81ea\u6821\u6b63\u9636\u6bb5\uff1a\u6a21\u578b\u8bc4\u4f30\u81ea\u8eab\u5206\u5272\u7ed3\u679c\uff0c\u8bc6\u522b\u4e0d\u5339\u914d\uff0c\u8fed\u4ee3\u4f18\u5316\u63a9\u7801", "result": "CoT-Seg\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u6a21\u7cca\u6216\u6613\u51fa\u9519\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u6570\u636e\u96c6ReasonSeg-Hard\u6765\u5c55\u793a\u5176\u5904\u7406\u6311\u6218\u6027\u6848\u4f8b\u7684\u80fd\u529b", "conclusion": "\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u81ea\u6821\u6b63\u4e3a\u89c6\u89c9\u8bed\u8a00\u9a71\u52a8\u7684\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u9010\u6b65\u63a8\u7406\u548c\u8fed\u4ee3\u4fee\u6b63\u673a\u5236\u6709\u6548\u5904\u7406\u590d\u6742\u5206\u5272\u4efb\u52a1"}}
{"id": "2601.17473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17473", "abs": "https://arxiv.org/abs/2601.17473", "authors": ["Manooshree Patel", "Rayna Bhattacharyya", "Thomas Lu", "Arnav Mehta", "Niels Voss", "Narges Norouzi", "Gireeja Ranade"], "title": "LeanTutor: Towards a Verified AI Mathematical Proof Tutor", "comment": "arXiv admin note: substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321. substantial text overlap with arXiv:2506.08321", "summary": "This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408LLM\u548c\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u6570\u5b66\u8bc1\u660e\u8f85\u5bfc\u7cfb\u7edfLeanTutor\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u4e0e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u7ed3\u5408\uff0c\u5e76\u4f7f\u7528PeanoBench\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u6d41\u4f46\u5bb9\u6613\u51fa\u9519\uff0c\u800c\u5b9a\u7406\u8bc1\u660e\u5668\u5982Lean\u80fd\u786e\u4fdd\u8bc1\u660e\u6b63\u786e\u6027\u4f46\u5b66\u4e60\u95e8\u69db\u9ad8\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u5f00\u53d1\u4e00\u4e2a\u65e2\u6613\u4e8e\u5b66\u751f\u4f7f\u7528\u53c8\u80fd\u4fdd\u8bc1\u6b63\u786e\u6027\u7684\u6570\u5b66\u8bc1\u660e\u8f85\u5bfc\u7cfb\u7edf\u3002", "method": "\u63d0\u51faLeanTutor\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u81ea\u52a8\u5f62\u5f0f\u5316/\u8bc1\u660e\u68c0\u67e5\u5668\uff1b2) \u4e0b\u4e00\u6b65\u751f\u6210\u5668\uff1b3) \u81ea\u7136\u8bed\u8a00\u53cd\u9988\u751f\u6210\u5668\u3002\u7cfb\u7edf\u7ed3\u5408LLM\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u80fd\u529b\u548c\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u80fd\u529b\u3002", "result": "\u5f00\u53d1\u4e86\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edfLeanTutor\uff0c\u5e76\u521b\u5efa\u4e86PeanoBench\u6570\u636e\u96c6\uff0c\u5305\u542b371\u4e2a\u76ae\u4e9a\u8bfa\u7b97\u672f\u8bc1\u660e\uff0c\u6db5\u76d6\u81ea\u7136\u8bed\u8a00\u548c\u5f62\u5f0f\u5316\u8bed\u8a00\u7248\u672c\uff0c\u6e90\u81eaNatural Numbers Game\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408LLM\u548c\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u65e2\u80fd\u4fdd\u8bc1\u8bc1\u660e\u6b63\u786e\u6027\u53c8\u6613\u4e8e\u5b66\u751f\u4f7f\u7528\u7684\u6570\u5b66\u8bc1\u660e\u8f85\u5bfc\u7cfb\u7edf\uff0c\u4e3aAI\u8f85\u52a9\u6570\u5b66\u6559\u80b2\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18496", "abs": "https://arxiv.org/abs/2601.18496", "authors": ["Zihan wang", "Hao Wang", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yiqun Zhang", "Jinghao Lin", "Haihua Yang", "Xiaozhong Ji"], "title": "DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference", "comment": null, "summary": "Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus \"find it but fail to use it,\" leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\\% on average and outperforms larger medical reasoning and DR models.", "AI": {"tldr": "DeepMed\uff1a\u9488\u5bf9\u533b\u5b66\u9886\u57df\u7684\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u51b3\u4efb\u52a1\u7279\u6027\u548c\u5de5\u5177\u4f7f\u7528\u6269\u5c55\u4e24\u4e2a\u5173\u952e\u5dee\u8ddd\uff0c\u5728\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u533b\u5b66\u63a8\u7406\u6a21\u578b\u53d7\u9650\u4e8e\u53c2\u6570\u5316\u77e5\u8bc6\uff0c\u5bb9\u6613\u9057\u5fd8\u548c\u4ea7\u751f\u5e7b\u89c9\u3002\u901a\u7528\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\u867d\u7136\u80fd\u5728\u4e00\u822c\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u76f4\u63a5\u8fc1\u79fb\u5230\u533b\u5b66\u9886\u57df\u6548\u679c\u6709\u9650\uff0c\u4e3b\u8981\u5b58\u5728\u4efb\u52a1\u7279\u6027\u548c\u5de5\u5177\u4f7f\u7528\u6269\u5c55\u4e24\u4e2a\u5173\u952e\u5dee\u8ddd", "method": "1\uff09\u6570\u636e\u5c42\u9762\uff1a\u91c7\u7528\u591a\u8df3\u533b\u5b66\u641c\u7d22QA\u5408\u6210\u65b9\u6cd5\uff0c\u652f\u6301\u6a21\u578b\u5728\u533b\u5b66\u4e0a\u4e0b\u6587\u4e2d\u5e94\u7528\u6df1\u5ea6\u7814\u7a76\u8303\u5f0f\uff1b2\uff09\u8bad\u7ec3\u5c42\u9762\uff1a\u5f15\u5165\u96be\u5ea6\u611f\u77e5\u7684\u56de\u5408\u60e9\u7f5a\u673a\u5236\uff0c\u6291\u5236\u8fc7\u5ea6\u5de5\u5177\u8c03\u7528\uff1b3\uff09\u63a8\u7406\u5c42\u9762\uff1a\u52a0\u5165\u76d1\u63a7\u5668\u5e2e\u52a9\u5728\u6709\u9650\u6b65\u9aa4\u5185\u9a8c\u8bc1\u5047\u8bbe\uff0c\u907f\u514d\u4e0a\u4e0b\u6587\u6c61\u67d3", "result": "\u5728\u4e03\u4e2a\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeepMed\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5e73\u5747\u63d0\u53479.79%\uff0c\u5e76\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u533b\u5b66\u63a8\u7406\u548c\u6df1\u5ea6\u7814\u7a76\u6a21\u578b", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9\u533b\u5b66\u4efb\u52a1\u7279\u6027\u8bbe\u8ba1\u7684\u6df1\u5ea6\u7814\u7a76\u65b9\u6cd5\uff0cDeepMed\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u63a8\u7406\u4e2d\u7684\u8bc1\u636e\u89e3\u91ca\u548c\u5de5\u5177\u4f7f\u7528\u6269\u5c55\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u63a8\u7406\u6027\u80fd"}}
{"id": "2601.17480", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17480", "abs": "https://arxiv.org/abs/2601.17480", "authors": ["Marton Szep", "Jorge Marin Ruiz", "Georgios Kaissis", "Paulina Seidl", "R\u00fcdiger von Eisenhart-Rothe", "Florian Hinterwimmer", "Daniel Rueckert"], "title": "Unintended Memorization of Sensitive Information in Fine-Tuned Language Models", "comment": "Accepted to EACL 2026. 20 pages", "summary": "Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u4ec5\u51fa\u73b0\u5728\u8f93\u5165\u800c\u975e\u8bad\u7ec3\u76ee\u6807\u4e2d\u7684\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\uff0c\u901a\u8fc7\u5b9e\u9a8c\u91cf\u5316\u4e86PII\u8bb0\u5fc6\u5316\u884c\u4e3a\uff0c\u5e76\u8bc4\u4f30\u4e86\u56db\u79cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u7684\u9690\u79c1-\u6027\u80fd\u6743\u8861\u3002", "motivation": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u6570\u636e\u96c6\u4e0a\u5b58\u5728\u610f\u5916\u8bb0\u5fc6\u5316\u548c\u6cc4\u9732\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u7684\u91cd\u5927\u98ce\u9669\uff0c\u8fd9\u53ef\u80fd\u8fdd\u53cd\u9690\u79c1\u6cd5\u89c4\u5e76\u5371\u5bb3\u4e2a\u4eba\u5b89\u5168\u3002\u5f53\u524d\u7814\u7a76\u5bf9\u4ec5\u51fa\u73b0\u5728\u6a21\u578b\u8f93\u5165\u4e2d\u800c\u975e\u8bad\u7ec3\u76ee\u6807\u4e2d\u7684PII\u66b4\u9732\u8fd9\u4e00\u5173\u952e\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6f0f\u6d1e\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8bbe\u8ba1\u53d7\u63a7\u63d0\u53d6\u63a2\u9488\u6765\u91cf\u5316\u610f\u5916PII\u8bb0\u5fc6\u5316\uff0c\u7814\u7a76\u8bed\u8a00\u3001PII\u9891\u7387\u3001\u4efb\u52a1\u7c7b\u578b\u548c\u6a21\u578b\u89c4\u6a21\u7b49\u56e0\u7d20\u5bf9\u8bb0\u5fc6\u5316\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u8fdb\u4e00\u6b65\u5bf9\u56db\u79cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff08\u5dee\u5206\u9690\u79c1\u3001\u673a\u5668\u9057\u5fd8\u3001\u6b63\u5219\u5316\u548c\u504f\u597d\u5bf9\u9f50\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5176\u9690\u79c1\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u540e\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u63d0\u4f9b\u66f4\u4e00\u81f4\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u800c\u5dee\u5206\u9690\u79c1\u5728\u7279\u5b9a\u8bbe\u7f6e\u4e2d\u80fd\u663e\u8457\u51cf\u5c11\u6cc4\u9732\uff0c\u5c3d\u7ba1\u53ef\u80fd\u5f15\u5165\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002\u4e0d\u540c\u56e0\u7d20\uff08\u5982\u8bed\u8a00\u3001PII\u9891\u7387\u7b49\uff09\u5bf9\u8bb0\u5fc6\u5316\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u5fae\u8c03LLM\u4e2d\u8bb0\u5fc6\u5316\u95ee\u9898\u7684\u6301\u4e45\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u5f00\u53d1\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u6765\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u3002"}}
{"id": "2601.17468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17468", "abs": "https://arxiv.org/abs/2601.17468", "authors": ["Chia-Ming Lee", "Yu-Fan Lin", "Jing-Hui Jung", "Yu-Jou Hsiao", "Chih-Chung Hsu", "Yu-Lun Liu"], "title": "ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation", "comment": "Project page: https://wuw2135.github.io/ReflexSplit-ProjectPage/", "summary": "Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.", "AI": {"tldr": "ReflexSplit\uff1a\u4e00\u79cd\u7528\u4e8e\u5355\u56fe\u50cf\u53cd\u5c04\u5206\u79bb\u7684\u53cc\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5c3a\u5ea6\u95e8\u63a7\u878d\u5408\u3001\u5c42\u878d\u5408-\u5206\u79bb\u5757\u548c\u8bfe\u7a0b\u8bad\u7ec3\u89e3\u51b3\u975e\u7ebf\u6027\u6df7\u5408\u4e0b\u7684\u4f20\u8f93-\u53cd\u5c04\u6df7\u6dc6\u95ee\u9898", "motivation": "\u73b0\u6709\u5355\u56fe\u50cf\u53cd\u5c04\u5206\u79bb\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u6df7\u5408\u60c5\u51b5\u4e0b\u5b58\u5728\u4f20\u8f93-\u53cd\u5c04\u6df7\u6dc6\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6df1\u5c42\u89e3\u7801\u5668\u4e2d\uff0c\u8fd9\u662f\u7531\u4e8e\u9690\u5f0f\u878d\u5408\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u534f\u8c03\u4e0d\u8db3\u5bfc\u81f4\u7684", "method": "\u63d0\u51faReflexSplit\u53cc\u6d41\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\uff1a1) \u8de8\u5c3a\u5ea6\u95e8\u63a7\u878d\u5408(CrGF)\u81ea\u9002\u5e94\u805a\u5408\u8bed\u4e49\u5148\u9a8c\u3001\u7eb9\u7406\u7ec6\u8282\u548c\u89e3\u7801\u5668\u4e0a\u4e0b\u6587\uff1b2) \u5c42\u878d\u5408-\u5206\u79bb\u5757(LFSB)\u4ea4\u66ff\u8fdb\u884c\u878d\u5408\u548c\u5206\u79bb\uff1b3) \u8bfe\u7a0b\u8bad\u7ec3\u901a\u8fc7\u6df1\u5ea6\u4f9d\u8d56\u521d\u59cb\u5316\u548c\u9010\u8f6e\u9884\u70ed\u589e\u5f3a\u5dee\u5206\u5206\u79bb", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u611f\u77e5\u8d28\u91cf\u548c\u9c81\u68d2\u6cdb\u5316\u80fd\u529b", "conclusion": "ReflexSplit\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u6d41\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u56fe\u50cf\u53cd\u5c04\u5206\u79bb\u4e2d\u7684\u4f20\u8f93-\u53cd\u5c04\u6df7\u6dc6\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd"}}
{"id": "2601.18554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18554", "abs": "https://arxiv.org/abs/2601.18554", "authors": ["Alberto Purpura", "Li Wang", "Sahil Badyal", "Eugenio Beaufrand", "Adam Faulkner"], "title": "Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities", "comment": "Paper accepted to EACL 2026", "summary": "Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.", "AI": {"tldr": "MOSAIC\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u590d\u6742\u6307\u4ee4\u7684\u9075\u5faa\u80fd\u529b\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u5305\u542b\u591a\u8fbe20\u4e2a\u5e94\u7528\u5bfc\u5411\u7ea6\u675f\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4f7f\u7528\u573a\u666f\u6216\u65e0\u6cd5\u5c06\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0e\u4efb\u52a1\u6210\u529f\u533a\u5206\u5f00\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLMs\u5bf9\u590d\u6742\u6307\u4ee4\u7684\u9075\u5faa\u80fd\u529b\u3002", "method": "\u63d0\u51faMOSAIC\u6846\u67b6\uff0c\u4f7f\u7528\u52a8\u6001\u751f\u6210\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u8fbe20\u4e2a\u5e94\u7528\u5bfc\u5411\u7684\u751f\u6210\u7ea6\u675f\uff0c\u5bf9\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u8fdb\u884c\u6a21\u5757\u5316\u3001\u7ec6\u7c92\u5ea6\u7684\u72ec\u7acb\u5206\u6790\u3002", "result": "\u8bc4\u4f30\u4e86\u4e94\u4e2a\u4e0d\u540c\u5bb6\u65cf\u7684LLMs\uff0c\u53d1\u73b0\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0d\u662f\u5355\u4e00\u80fd\u529b\uff0c\u800c\u662f\u968f\u7ea6\u675f\u7c7b\u578b\u3001\u6570\u91cf\u548c\u4f4d\u7f6e\u663e\u8457\u53d8\u5316\uff1b\u63ed\u793a\u4e86\u6a21\u578b\u7279\u5b9a\u5f31\u70b9\u3001\u6307\u4ee4\u95f4\u7684\u534f\u540c\u4e0e\u51b2\u7a81\u5173\u7cfb\uff0c\u4ee5\u53ca\u9996\u56e0\u6548\u5e94\u548c\u8fd1\u56e0\u6548\u5e94\u7b49\u4f4d\u7f6e\u504f\u5dee\u3002", "conclusion": "\u8fd9\u4e9b\u7ec6\u7c92\u5ea6\u6d1e\u5bdf\u5bf9\u4e8e\u8bca\u65ad\u6a21\u578b\u5931\u8d25\u548c\u5f00\u53d1\u9700\u8981\u4e25\u683c\u9075\u5faa\u590d\u6742\u6307\u4ee4\u7684\u7cfb\u7edf\u4e2d\u7684\u66f4\u53ef\u9760LLMs\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2601.17470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17470", "abs": "https://arxiv.org/abs/2601.17470", "authors": ["Chia-Ming Lee", "Yu-Fan Lin", "Yu-Jou Hsiao", "Jing-Hui Jung", "Yu-Lun Liu", "Chih-Chung Hsu"], "title": "PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors", "comment": "Project Page: https://ming053l.github.io/PhaSR", "summary": "Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.", "AI": {"tldr": "PhaSR\u901a\u8fc7\u7269\u7406\u5bf9\u9f50\u7684\u53cc\u5c42\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\u89e3\u51b3\u9634\u5f71\u53bb\u9664\u95ee\u9898\uff0c\u5728\u5355\u5149\u6e90\u9634\u5f71\u5230\u591a\u5149\u6e90\u73af\u5883\u7167\u660e\u4e0b\u90fd\u80fd\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd", "motivation": "\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u8fdb\u884c\u9634\u5f71\u53bb\u9664\u9700\u8981\u5c06\u5149\u7167\u4e0e\u56fa\u6709\u53cd\u5c04\u7387\u5206\u79bb\uff0c\u5f53\u7269\u7406\u5148\u9a8c\u672a\u6b63\u786e\u5bf9\u9f50\u65f6\u8fd9\u4e00\u6311\u6218\u5c24\u4e3a\u7a81\u51fa", "method": "\u63d0\u51faPhaSR\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7269\u7406\u5bf9\u9f50\u5f52\u4e00\u5316(PAN)\uff1a\u901a\u8fc7\u7070\u4e16\u754c\u5f52\u4e00\u5316\u3001\u5bf9\u6570\u57dfRetinex\u5206\u89e3\u548c\u52a8\u6001\u8303\u56f4\u91cd\u7ec4\u8fdb\u884c\u95ed\u5f0f\u5149\u7167\u6821\u6b63\uff1b2) \u51e0\u4f55\u8bed\u4e49\u77eb\u6b63\u6ce8\u610f\u529b(GSRA)\uff1a\u5c06\u5dee\u5206\u6ce8\u610f\u529b\u6269\u5c55\u5230\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u534f\u8c03\u6df1\u5ea6\u51e0\u4f55\u4e0eDINO-v2\u8bed\u4e49\u5d4c\u5165", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u9634\u5f71\u53bb\u9664\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u590d\u6742\u5ea6\u66f4\u4f4e\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4f20\u7edf\u65b9\u6cd5\u5728\u591a\u5149\u6e90\u7167\u660e\u4e0b\u5931\u8d25\u7684\u73af\u5883\u7167\u660e\u573a\u666f", "conclusion": "PhaSR\u901a\u8fc7\u7269\u7406\u5bf9\u9f50\u7684\u53cc\u5c42\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9634\u5f71\u53bb\u9664\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u5149\u6e90\u73af\u5883\u7167\u660e\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272"}}
{"id": "2601.18588", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18588", "abs": "https://arxiv.org/abs/2601.18588", "authors": ["Xianzhe Meng", "Qiangsheng Zeng", "Ling Luo", "Qinghan Yang", "Jiarui Hao", "Wenbo Wu", "Qinyu Wang", "Rui Yin", "Lin Qi", "Renzhi Lu"], "title": "Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs", "comment": null, "summary": "Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0e\u751f\u6210\u8d28\u91cf\u4e0d\u4e00\u5b9a\u6b63\u76f8\u5173\uff0c\u7a33\u5b9a\u8bad\u7ec3\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u4f4e\u71b5\u3001\u91cd\u590d\u7684\u6a21\u5f0f\uff0c\u5c3d\u7ba1\u635f\u5931\u51fd\u6570\u5e73\u6ed1\u6536\u655b", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u4f18\u5316\u7684\u524d\u63d0\uff0c\u4f46\u672c\u6587\u65e8\u5728\u5206\u6790\u8bad\u7ec3\u7a33\u5b9a\u6027\u5982\u4f55\u5f71\u54cd\u751f\u6210\u5206\u5e03\uff0c\u63a2\u8ba8\u7a33\u5b9a\u6027\u662f\u5426\u771f\u6b63\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf", "method": "\u4f7f\u7528\u57fa\u4e8e\u53cd\u9988\u7684\u8bad\u7ec3\u6846\u67b6\u6765\u7a33\u5b9a\u5185\u90e8\u751f\u6210\u7edf\u8ba1\u91cf\uff0c\u5728\u4e0d\u540c\u67b6\u6784\u548c\u968f\u673a\u79cd\u5b50\u4e0b\u89c2\u5bdf\u751f\u6210\u884c\u4e3a\uff1b\u7406\u8bba\u5206\u6790\u8868\u660e\u7a33\u5b9a\u53c2\u6570\u8f68\u8ff9\u4f7f\u5e73\u7a33\u89e3\u8fd1\u4f3c\u6700\u5c0f\u5316\u524d\u5411KL\u6563\u5ea6\uff0c\u540c\u65f6\u9690\u5f0f\u964d\u4f4e\u751f\u6210\u71b5", "result": "\u7a33\u5b9a\u8bad\u7ec3\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u4f4e\u71b5\u3001\u91cd\u590d\u7684\u884c\u4e3a\uff0c\u5c3d\u7ba1\u635f\u5931\u51fd\u6570\u5e73\u6ed1\u6536\u655b\uff1b\u6a21\u578b\u5c06\u6982\u7387\u8d28\u91cf\u96c6\u4e2d\u5728\u7ecf\u9a8c\u5206\u5e03\u7684\u6709\u9650\u5b50\u96c6\u4e0a\uff0c\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u9000\u5316", "conclusion": "\u4f18\u5316\u7a33\u5b9a\u6027\u548c\u751f\u6210\u8868\u8fbe\u80fd\u529b\u5e76\u4e0d\u5929\u7136\u4e00\u81f4\uff0c\u7a33\u5b9a\u6027\u672c\u8eab\u4e0d\u8db3\u4ee5\u4f5c\u4e3a\u751f\u6210\u8d28\u91cf\u7684\u6307\u6807\uff1b\u9700\u8981\u8d85\u8d8a\u5355\u7eaf\u7a33\u5b9a\u6027\u8003\u8651\u6765\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u80fd\u529b"}}
{"id": "2601.17489", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17489", "abs": "https://arxiv.org/abs/2601.17489", "authors": ["Ashutosh Bajpai", "Akshat Bhandari", "Akshay Nambi", "Tanmoy Chakraborty"], "title": "SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving", "comment": null, "summary": "Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.", "AI": {"tldr": "\u63d0\u51faSpatialMath\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u7406\u89e3\u589e\u5f3a\u7684\u7b26\u53f7\u63a8\u7406\u94fe\uff0c\u63d0\u5347\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u6570\u5b66\u95ee\u9898\uff08\u7279\u522b\u662f\u51e0\u4f55\u95ee\u9898\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5728MATHVERSE-PLUS\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u63d0\u534710\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u4e2d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u6570\u5b66\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u51e0\u4f55\u95ee\u9898\u4e0a\uff0c\u96be\u4ee5\u51c6\u786e\u5206\u89e3\u590d\u6742\u89c6\u89c9\u8f93\u5165\u5e76\u5c06\u611f\u77e5\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u8fde\u63a5\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSpatialMath\u6846\u67b6\uff0c\u5305\u542b\u4e13\u95e8\u611f\u77e5\u6a21\u5757\u63d0\u53d6\u89c6\u89c9\u56fe\u8868\u4e2d\u7684\u7a7a\u95f4\u57fa\u7840\u8868\u793a\uff0c\u6355\u83b7\u5173\u952e\u51e0\u4f55\u7ed3\u6784\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8868\u793a\u7cfb\u7edf\u6027\u5730\u6ce8\u5165\u7b26\u53f7\u63a8\u7406\u94fe\u4e2d\uff0c\u5b9e\u73b0\u89c6\u89c9\u7406\u89e3\u611f\u77e5\u7684\u7ed3\u6784\u5316\u63a8\u7406\u3002", "result": "SpatialMath\u663e\u8457\u4f18\u4e8e\u5f3a\u5927\u591a\u6a21\u6001\u57fa\u7ebf\uff0c\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u8bbe\u7f6e\u4e0b\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03\u52a0\u6570\u636e\u589e\u5f3a\u63d0\u5347\u8fbe10\u4e2a\u767e\u5206\u70b9\u3002\u9c81\u68d2\u6027\u5206\u6790\u663e\u793a\u589e\u5f3a\u7684\u7a7a\u95f4\u8868\u793a\u76f4\u63a5\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u589e\u5f3a\u7684\u7a7a\u95f4\u8868\u793a\u80fd\u76f4\u63a5\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\uff0c\u5f3a\u5316\u4e86\u5728\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4e2d\u6784\u5efa\u7ed3\u6784\u5316\u611f\u77e5\u5230\u63a8\u7406\u7ba1\u9053\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2601.18595", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18595", "abs": "https://arxiv.org/abs/2601.18595", "authors": ["Joseph Cotnareanu", "Didier Chetelat", "Yingxue Zhang", "Mark Coates"], "title": "A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic", "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u548c\u903b\u8f91\u6c42\u89e3\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u903b\u8f91\u6c42\u89e3\u5668\u7684\u53cd\u9988\u8fed\u4ee3\u5730\u8865\u5145\u7f3a\u5931\u7684\u5e38\u8bc6\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u63a8\u7406\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u590d\u6742\u8bc1\u660e\u89c4\u5212\u7684\u95ee\u9898\u4e0a\u5e38\u5e38\u5931\u6548\u3002\u73b0\u6709\u7684\u903b\u8f91\u6c42\u89e3\u5668\u867d\u7136\u63a8\u7406\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u5047\u8bbe\u6240\u6709\u76f8\u5173\u4e8b\u5b9e\u90fd\u5df2\u63d0\u4f9b\uff0c\u65e0\u6cd5\u5904\u7406\u7f3a\u5931\u7684\u5e38\u8bc6\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff1a\u4f7f\u7528\u903b\u8f91\u6c42\u89e3\u5668\u7684\u53cd\u9988\u6765\u589e\u5f3a\u903b\u8f91\u95ee\u9898\uff0c\u901a\u8fc7LLM\u8fed\u4ee3\u5730\u8865\u5145\u5e38\u8bc6\u5173\u7cfb\u3002\u8fd9\u6d89\u53ca\u4e00\u4e2a\u641c\u7d22\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6f5c\u5728\u7684\u5e38\u8bc6\u5047\u8bbe\u6765\u6700\u5927\u5316\u627e\u5230\u6709\u7528\u4e8b\u5b9e\u7684\u673a\u4f1a\uff0c\u540c\u65f6\u4fdd\u6301\u6210\u672c\u53ef\u63a7\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u7eaf\u903b\u8f91\u63a8\u7406\u6570\u636e\u96c6\u4e0a\uff08\u5176\u4e2d\u90e8\u5206\u5e38\u8bc6\u4fe1\u606f\u5df2\u88ab\u79fb\u9664\uff09\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u59cb\u7ec8\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u5728\u4eba\u7c7b\u8bed\u5883\u4e0b\u5de5\u4f5c\u65f6\uff0c\u5e73\u8861\u795e\u7ecf\u548c\u7b26\u53f7\u5143\u7d20\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u8fd9\u79cd\u5e73\u8861\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.17529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17529", "abs": "https://arxiv.org/abs/2601.17529", "authors": ["Fengting Zhang", "Yue He", "Qinghao Liu", "Yaonan Wang", "Xiang Chen", "Hang Zhang"], "title": "FMIR, a foundation model-based Image Registration Framework for Robust Image Registration", "comment": null, "summary": "Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.", "AI": {"tldr": "FMIR\u662f\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5355\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u57df\u5185SOTA\u6027\u80fd\uff0c\u540c\u65f6\u5728\u57df\u5916\u56fe\u50cf\u4e0a\u4fdd\u6301\u9c81\u68d2\u914d\u51c6\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u867d\u7136\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u901f\u5ea6\uff0c\u4f46\u5176\u4e34\u5e8a\u5e94\u7528\u53d7\u5230\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u6570\u636e\u96c6\u901a\u5e38\u89c4\u6a21\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u89e3\u51b3\u6a21\u578b\u5728\u8bad\u7ec3\u57df\u4e4b\u5916\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "FMIR\u7ed3\u5408\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u7f16\u7801\u5668\uff08\u7528\u4e8e\u63d0\u53d6\u89e3\u5256\u7ed3\u6784\uff09\u548c\u901a\u7528\u914d\u51c6\u5934\uff0c\u91c7\u7528\u901a\u9053\u6b63\u5219\u5316\u7b56\u7565\uff0c\u4ec5\u9700\u5728\u5355\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "FMIR\u5728\u57df\u5185\u6027\u80fd\u8fbe\u5230SOTA\u6c34\u5e73\uff0c\u540c\u65f6\u5728\u57df\u5916\u56fe\u50cf\u4e0a\u4fdd\u6301\u9c81\u68d2\u7684\u914d\u51c6\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u6784\u5efa\u53ef\u6cdb\u5316\u533b\u5b66\u6210\u50cf\u57fa\u7840\u6a21\u578b\u7684\u53ef\u884c\u8def\u5f84\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u6cdb\u5316\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u6846\u67b6\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u57df\u5185\u548c\u57df\u5916\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u53ef\u6cdb\u5316\u7684\u533b\u5b66\u6210\u50cf\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.17512", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17512", "abs": "https://arxiv.org/abs/2601.17512", "authors": ["Yiqun Zhang", "Shenghong Cai", "Zihua Yang", "Sen Feng", "Yuzhu Ji", "Haijun Zhang"], "title": "One-Shot Federated Clustering of Non-Independent Completely Distributed Data", "comment": "This work has been accepted for publication in IEEE Internet of Things Journal", "summary": "Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGOLD\u6846\u67b6\u89e3\u51b3\u8054\u90a6\u805a\u7c7b\u4e2d\u7684Non-IID\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u5bfc\u5411\u7684\u5c40\u90e8\u5206\u5e03\u5b66\u4e60\u6765\u878d\u5408\u975e\u72ec\u7acb\u5b8c\u5168\u5206\u5e03\u6570\u636e\u4e2d\u7684\u805a\u7c7b\u77e5\u8bc6\u3002", "motivation": "\u8054\u90a6\u805a\u7c7b\u5728\u65e0\u6807\u7b7e\u7684\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u6570\u636e\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u7684Non-IID\u95ee\u9898\uff08\u7279\u522b\u662f\u4e0d\u540c\u5ba2\u6237\u7aef\u53ef\u80fd\u5206\u5272\u540c\u4e00\u805a\u7c7b\uff09\u4e25\u91cd\u9650\u5236\u4e86\u805a\u7c7b\u6027\u80fd\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faGOLD\u6846\u67b6\uff1a1\uff09\u7cbe\u7ec6\u63a2\u7d22\u5ba2\u6237\u7aef\u6f5c\u5728\u7684\u4e0d\u5b8c\u6574\u5c40\u90e8\u805a\u7c7b\u5206\u5e03\uff1b2\uff09\u5c06\u5206\u5e03\u6458\u8981\u4e0a\u4f20\u81f3\u670d\u52a1\u5668\u8fdb\u884c\u5168\u5c40\u878d\u5408\uff1b3\uff09\u5728\u5168\u5c40\u5206\u5e03\u6307\u5bfc\u4e0b\u8fdb\u884c\u5c40\u90e8\u805a\u7c7b\u589e\u5f3a\u3002", "result": "\u901a\u8fc7\u663e\u8457\u6027\u68c0\u9a8c\u3001\u6d88\u878d\u7814\u7a76\u3001\u53ef\u6269\u5c55\u6027\u8bc4\u4f30\u548c\u5b9a\u6027\u7ed3\u679c\u7b49\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86GOLD\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "GOLD\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u805a\u7c7b\u4e2d\u7684Non-ICD\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u5bfc\u5411\u7684\u5c40\u90e8\u5206\u5e03\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\uff0c\u4e3a\u5206\u5e03\u5f0f\u9690\u79c1\u4fdd\u62a4\u7cfb\u7edf\u4e2d\u7684\u6a21\u5f0f\u77e5\u8bc6\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.17535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17535", "abs": "https://arxiv.org/abs/2601.17535", "authors": ["Kevin Robbins", "Xiaotong Liu", "Yu Wu", "Le Sun", "Grady McPeak", "Abby Stylianou", "Robert Pless"], "title": "Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries", "comment": null, "summary": "Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u751f\u6210\u5408\u6210\u56fe\u50cf\u6765\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u96f6\u6837\u672c\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u6587\u672c\u7684\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u867d\u7136\u80fd\u8ba9\u7528\u6237\u901a\u8fc7\u7b80\u5355\u547d\u540d\u7c7b\u522b\u6765\u6784\u5efa\u89c6\u89c9\u5206\u7c7b\u5668\uff0c\u4f46\u6a21\u578b\u5728\u4e00\u4e2a\u9886\u57df\u8868\u73b0\u826f\u597d\u53ef\u80fd\u5728\u53e6\u4e00\u4e2a\u9886\u57df\u5931\u8d25\uff0c\u975e\u4e13\u5bb6\u7528\u6237\u7f3a\u4e4f\u76f4\u63a5\u8bc4\u4f30\u6240\u9009VLM\u662f\u5426\u9002\u7528\u4e8e\u5176\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u5148\u524d\u4ec5\u4f7f\u7528\u6587\u672c\u6bd4\u8f83\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u7684\u5de5\u4f5c\uff0c\u63a2\u7d22\u4e86\u751f\u6210\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u5408\u6210\u56fe\u50cf\u6765\u8bc4\u4f30\u548c\u4f18\u5316\u96f6\u6837\u672c\u51c6\u786e\u7387\u9884\u6d4b\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u751f\u6210\u56fe\u50cf\u6765\u589e\u5f3a\u57fa\u7ebf\u6587\u672c\u8bc4\u5206\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u7528\u4e8e\u8bc4\u4f30\u7684\u56fe\u50cf\u7c7b\u578b\u53cd\u9988\u3002", "result": "\u5728\u6807\u51c6CLIP\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u96f6\u6837\u672c\u6027\u80fd\u9884\u6d4b\u7684\u8d28\u91cf\uff0c\u5e2e\u52a9\u7528\u6237\u5728\u6ca1\u6709\u6807\u6ce8\u6837\u672c\u7684\u60c5\u51b5\u4e0b\u5224\u65adVLM\u662f\u5426\u9002\u7528\u4e8e\u5176\u5e94\u7528\u3002", "conclusion": "\u751f\u6210\u5408\u6210\u56fe\u50cf\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96f6\u6837\u672c\u6027\u80fd\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6a21\u578b\u9002\u7528\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u89c6\u5316\u53cd\u9988\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u8bc4\u4f30\u4f9d\u636e\u3002"}}
{"id": "2601.17563", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17563", "abs": "https://arxiv.org/abs/2601.17563", "authors": ["Nathan Gavenski", "Matteo Leonetti", "Odinaldo Rodrigues"], "title": "Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment", "comment": "The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026)", "summary": "State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.", "AI": {"tldr": "UfO\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u65e0\u9700\u52a8\u4f5c\u76d1\u7763\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ILfO\u65b9\u6cd5\u7684\u591a\u4e2a\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u9700\u8981\u57fa\u4e8e\u52a8\u4f5c\u7684\u76d1\u7763\u4f18\u5316\u3001\u5047\u8bbe\u72b6\u6001\u6709\u5355\u4e00\u6700\u4f18\u52a8\u4f5c\u3001\u4ee5\u53ca\u4e0d\u8003\u8651\u5b9e\u9645\u73af\u5883\u72b6\u6001\u5c31\u5e94\u7528\u6559\u5e08\u52a8\u4f5c\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6ca1\u6709\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u4ece\u89c2\u5bdf\u8f68\u8ff9\u4e2d\u63d0\u53d6\u6709\u6548\u4fe1\u606f\u3002", "method": "UfO\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u4ece\u89c2\u5bdf\u7684\u72b6\u6001\u8f6c\u79fb\u4e2d\u8fd1\u4f3c\u6559\u5e08\u7684\u771f\u5b9e\u52a8\u4f5c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8c03\u6574\u667a\u80fd\u4f53\u8f68\u8ff9\u4f7f\u5176\u4e0e\u6559\u5e08\u8f68\u8ff9\u7d27\u5bc6\u5bf9\u9f50\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u5b66\u4e60\u5230\u7684\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUfO\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u6559\u5e08\u548c\u6240\u6709\u5176\u4ed6ILfO\u65b9\u6cd5\uff0c\u800c\u4e14\u663e\u793a\u51fa\u6700\u5c0f\u7684\u6807\u51c6\u5dee\uff0c\u8fd9\u8868\u660e\u5728\u672a\u89c1\u573a\u666f\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UfO\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709ILfO\u65b9\u6cd5\u7684\u591a\u4e2a\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u65e0\u76d1\u7763\u7684\u89c2\u5bdf\u6a21\u4eff\u5b66\u4e60\uff0c\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.18630", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.18630", "abs": "https://arxiv.org/abs/2601.18630", "authors": ["Abeer Badawi", "Md Tahmid Rahman Laskar", "Elahe Rahimi", "Sheri Grach", "Lindsay Bertrand", "Lames Danok", "Frank Rudzicz", "Jimmy Huang", "Elham Dolatabadi"], "title": "Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation", "comment": null, "summary": "The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u5728\u8ba4\u77e5\u652f\u6301\u65b9\u9762\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5728\u60c5\u611f\u5171\u9e23\u65b9\u9762\u5b58\u5728\u4e0d\u7a33\u5b9a\uff0c\u63ed\u793a\u4e86\u8ba4\u77e5-\u60c5\u611f\u5dee\u8ddd\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u5371\u673a\u65e5\u76ca\u4e25\u91cd\uff0c\u5b58\u5728\u6cbb\u7597\u7f3a\u53e3\u3001\u8d44\u6e90\u4e0d\u8db3\u548c\u5408\u683c\u6cbb\u7597\u5e08\u77ed\u7f3a\u7684\u95ee\u9898\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u53ef\u6269\u5c55\u7684\u5fc3\u7406\u652f\u6301\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4f46\u5176\u53ef\u9760\u6027\u3001\u6cbb\u7597\u76f8\u5173\u6027\u548c\u4e0e\u4eba\u7c7b\u6807\u51c6\u7684\u5bf9\u9f50\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ece\u771f\u5b9e\u4e16\u754c\u573a\u666f\u6570\u636e\u96c6\u4e2d\u6536\u96c6\u4e86500\u4e2a\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\uff0c\u8bc4\u4f30\u4e869\u4e2a\u4e0d\u540cLLMs\uff08\u5305\u62ec\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\uff09\u751f\u6210\u7684\u56de\u590d\u3002\u7531\u4e24\u540d\u7ecf\u8fc7\u7cbe\u795e\u75c5\u5b66\u57f9\u8bad\u7684\u4e13\u5bb6\u72ec\u7acb\u4f7f\u75285\u70b9\u674e\u514b\u7279\u91cf\u8868\uff0c\u6309\u7167\u5305\u542b6\u4e2a\u5c5e\u6027\u7684\u7efc\u5408\u8bc4\u4f30\u6807\u51c6\u8fdb\u884c\u8bc4\u5206\uff0c\u8be5\u6807\u51c6\u6db5\u76d6\u8ba4\u77e5\u652f\u6301\u548c\u60c5\u611f\u5171\u9e23\u4e24\u4e2a\u7ef4\u5ea6\u3002", "result": "\u5206\u6790\u663e\u793a\uff0cLLMs\u5728\u8ba4\u77e5\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u80fd\u591f\u751f\u6210\u5b89\u5168\u3001\u8fde\u8d2f\u4e14\u4e34\u5e8a\u9002\u5f53\u7684\u4fe1\u606f\uff0c\u4f46\u5728\u60c5\u611f\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\uff09\u63d0\u4f9b\u66f4\u5e73\u8861\u7684\u6cbb\u7597\u6027\u56de\u5e94\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u5219\u8868\u73b0\u51fa\u66f4\u5927\u7684\u53d8\u5f02\u6027\u548c\u60c5\u611f\u5e73\u6de1\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6301\u7eed\u7684\u8ba4\u77e5-\u60c5\u611f\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u5efa\u7acb\u5177\u6709\u5931\u8d25\u610f\u8bc6\u3001\u4e34\u5e8a\u57fa\u7840\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5728\u5fc3\u7406\u5065\u5eb7\u5bfc\u5411\u7684LLMs\u4e2d\u4f18\u5148\u8003\u8651\u5173\u7cfb\u654f\u611f\u6027\u800c\u4e0d\u4ec5\u4ec5\u662f\u4fe1\u606f\u51c6\u786e\u6027\u3002\u5021\u5bfc\u91c7\u7528\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5e73\u8861\u8bc4\u4f30\u534f\u8bae\uff0c\u91cd\u70b9\u5173\u6ce8\u6cbb\u7597\u654f\u611f\u6027\uff0c\u4e3a\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bddAI\u7684\u8d1f\u8d23\u4efb\u8bbe\u8ba1\u548c\u4e34\u5e8a\u76d1\u7763\u63d0\u4f9b\u6846\u67b6\u3002"}}
{"id": "2601.17570", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17570", "abs": "https://arxiv.org/abs/2601.17570", "authors": ["Hadi Salloum", "Ali Jnadi", "Yaroslav Kholodov", "Alexander Gasnikov"], "title": "Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization", "comment": "Proceedings of Machine Learning Research tbd: 1_13, 2025 International Conference on Computational Optimization", "summary": "Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.", "AI": {"tldr": "MC+QUBO\u65b9\u6cd5\u5c06\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8f68\u8ff9\u9009\u62e9\u8f6c\u5316\u4e3aQUBO\u95ee\u9898\uff0c\u7528\u91cf\u5b50\u542f\u53d1\u5f0f\u91c7\u6837\u5668\u4f18\u5316\u9009\u62e9\uff0c\u63d0\u5347\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u8499\u7279\u5361\u6d1b\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u3001\u5927\u72b6\u6001\u7a7a\u95f4\u548c\u76f8\u5173\u8f68\u8ff9\u73af\u5883\u4e2d\u5b58\u5728\u9ad8\u6837\u672c\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u5c06\u8f68\u8ff9\u9009\u62e9\u91cd\u65b0\u8868\u8ff0\u4e3aQUBO\u95ee\u9898\uff0c\u4f7f\u7528\u6a21\u62df\u91cf\u5b50\u9000\u706b\u548c\u6a21\u62df\u5206\u53c9\u4f5c\u4e3a\u9ed1\u76d2\u6c42\u89e3\u5668\uff0c\u4ece\u6279\u91cf\u8f68\u8ff9\u4e2d\u9009\u62e9\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u5e76\u4fc3\u8fdb\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u7684\u5b50\u96c6\u3002", "result": "\u5728\u6709\u9650\u65f6\u57dfGridWorld\u5b9e\u9a8c\u4e2d\uff0cMC+QUBO\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u7b56\u7565\u8d28\u91cf\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u3002", "conclusion": "\u91cf\u5b50\u542f\u53d1\u5f0f\u4f18\u5316\u4f5c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u51b3\u7b56\u5b50\u7a0b\u5e8f\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u8499\u7279\u5361\u6d1b\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u3002"}}
{"id": "2601.17555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17555", "abs": "https://arxiv.org/abs/2601.17555", "authors": ["Justin Downes", "Sam Saltwick", "Anthony Chen"], "title": "Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper", "comment": "Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems (2023)", "summary": "The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u56fe\u7684\u536b\u661f\u56fe\u50cf\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u53d8\u5927\u5c0f\u7684\u5e73\u6ed1\u6838\u6620\u5c04\u5230\u4e0d\u540c\u7684\u91cf\u5316\u663e\u8457\u6027\u6c34\u5e73\uff0c\u4f18\u5316\u4f20\u7edf\u6709\u635f\u538b\u7f29\u7f16\u7801\u6807\u51c6\uff0c\u5b9e\u73b0\u5355\u5f20\u5927\u578b\u536b\u661f\u56fe\u50cf\u5185\u7684\u53ef\u53d8\u6bd4\u7279\u7387\u538b\u7f29\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u6bcf\u5929\u4ea7\u751f\u6570\u767eTB\u6570\u636e\uff0c\u5b58\u50a8\u548c\u5e26\u5bbd\u6210\u672c\u9ad8\u6602\u3002\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\u53ea\u5bf9\u56fe\u50cf\u4e2d\u7684\u5c0f\u533a\u57df\u611f\u5174\u8da3\uff0c\u8fd9\u4e9b\u611f\u5174\u8da3\u533a\u57df\u56e0\u4efb\u52a1\u800c\u5f02\uff0c\u4f46\u4e00\u65e6\u786e\u5b9a\uff0c\u53ef\u7528\u4e8e\u4f18\u5316\u56fe\u50cf\u7f16\u7801\u3002\u4f20\u7edf\u56fe\u50cf\u7f16\u7801\u65b9\u6cd5\u5bf9\u6574\u4e2a\u56fe\u50cf\u540c\u7b49\u5904\u7406\uff0c\u800c\u57fa\u4e8e\u663e\u8457\u6027\u56fe\u7684\u5f15\u5bfc\u65b9\u6cd5\u53ef\u4ee5\u805a\u7126\u91cd\u8981\u533a\u57df\u3002", "method": "\u4f7f\u7528\u53ef\u53d8\u5927\u5c0f\u7684\u5e73\u6ed1\u6838\u6620\u5c04\u5230\u4e0d\u540c\u7684\u91cf\u5316\u663e\u8457\u6027\u6c34\u5e73\u6765\u5904\u7406\u56fe\u50cf\u50cf\u7d20\uff0c\u4f18\u5316\u4e0b\u6e38\u538b\u7f29\u548c\u7f16\u7801\u65b9\u6848\u3002\u901a\u8fc7\u663e\u8457\u6027\u56fe\u9a71\u52a8\u7684\u56fe\u50cf\u9884\u5904\u7406\u6280\u672f\u4e0e\u4f20\u7edf\u6709\u635f\u538b\u7f29\u7f16\u7801\u6807\u51c6\u7ed3\u5408\uff0c\u5b9e\u73b0\u5355\u5f20\u5927\u578b\u536b\u661f\u56fe\u50cf\u5185\u7684\u53ef\u53d8\u6bd4\u7279\u7387\u538b\u7f29\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u663e\u8457\u6027\u4fe1\u606f\u5728\u5355\u5f20\u5927\u578b\u536b\u661f\u56fe\u50cf\u5185\u5b9e\u73b0\u53ef\u53d8\u6bd4\u7279\u7387\u538b\u7f29\uff0c\u76f8\u6bd4\u4f20\u7edf\u5bf9\u6574\u4e2a\u56fe\u50cf\u540c\u7b49\u5904\u7406\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u4f18\u5316\u5b58\u50a8\u548c\u5e26\u5bbd\u4f7f\u7528\u3002", "conclusion": "\u57fa\u4e8e\u663e\u8457\u6027\u56fe\u7684\u56fe\u50cf\u9884\u5904\u7406\u6280\u672f\u53ef\u4ee5\u4e0e\u4f20\u7edf\u538b\u7f29\u7f16\u7801\u6807\u51c6\u6709\u6548\u7ed3\u5408\uff0c\u4e3a\u536b\u661f\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u4e0b\u6e38\u4efb\u52a1\u9700\u6c42\u805a\u7126\u91cd\u8981\u533a\u57df\uff0c\u964d\u4f4e\u5b58\u50a8\u548c\u5e26\u5bbd\u6210\u672c\u3002"}}
{"id": "2601.18631", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.18631", "abs": "https://arxiv.org/abs/2601.18631", "authors": ["Mingyang Song", "Haoyu Sun", "Jiawei Gu", "Linjie Li", "Luxin Xu", "Ranjay Krishna", "Yu Cheng"], "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning", "comment": "28 pages, 10 figures and 13 tables", "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.", "AI": {"tldr": "AdaReasoner\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u4f5c\u4e3a\u901a\u7528\u63a8\u7406\u6280\u80fd\u800c\u975e\u7279\u5b9a\u5de5\u5177\u6216\u663e\u5f0f\u76d1\u7763\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u81ea\u4e3b\u9009\u62e9\u3001\u8c03\u7528\u548c\u7ec4\u5408\u5de5\u5177\u7684\u80fd\u529b\u3002", "motivation": "\u4eba\u7c7b\u5728\u9762\u5bf9\u8d85\u51fa\u81ea\u8eab\u80fd\u529b\u7684\u95ee\u9898\u65f6\u4f1a\u4f9d\u8d56\u5de5\u5177\uff0c\u8fd9\u4e3a\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8303\u5f0f\u3002\u6709\u6548\u7684\u63a8\u7406\u9700\u8981\u77e5\u9053\u4f7f\u7528\u54ea\u4e9b\u5de5\u5177\u3001\u4f55\u65f6\u8c03\u7528\u5b83\u4eec\u4ee5\u53ca\u5982\u4f55\u5728\u591a\u6b65\u9aa4\u4e2d\u7ec4\u5408\u5b83\u4eec\uff0c\u5373\u4f7f\u9762\u5bf9\u65b0\u5de5\u5177\u6216\u65b0\u4efb\u52a1\u65f6\u4e5f\u662f\u5982\u6b64\u3002", "method": "AdaReasoner\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u5b9e\u73b0\uff1a(1) \u53ef\u6269\u5c55\u7684\u6570\u636e\u7ba1\u7406\u6d41\u7a0b\uff0c\u8ba9\u6a21\u578b\u63a5\u89e6\u957f\u89c6\u91ce\u3001\u591a\u6b65\u9aa4\u7684\u5de5\u5177\u4ea4\u4e92\uff1b(2) Tool-GRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u57fa\u4e8e\u6700\u7ec8\u4efb\u52a1\u6210\u529f\u4f18\u5316\u5de5\u5177\u9009\u62e9\u548c\u5e8f\u5217\uff1b(3) \u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\uff0c\u52a8\u6001\u8c03\u8282\u5de5\u5177\u4f7f\u7528\u9891\u7387\u3002", "result": "AdaReasoner\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5de5\u5177\u81ea\u9002\u5e94\u548c\u6cdb\u5316\u884c\u4e3a\uff1a\u81ea\u4e3b\u91c7\u7528\u6709\u76ca\u5de5\u5177\u3001\u6291\u5236\u65e0\u5173\u5de5\u5177\u3001\u6839\u636e\u4efb\u52a1\u9700\u6c42\u8c03\u6574\u5de5\u5177\u4f7f\u7528\u9891\u7387\uff0c\u5c3d\u7ba1\u4ece\u672a\u88ab\u660e\u786e\u8bad\u7ec3\u8fd9\u6837\u505a\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c067B\u57fa\u7840\u6a21\u578b\u5e73\u5747\u63d0\u534724.9%\uff0c\u5e76\u5728VSP\u548cJigsaw\u7b49\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-5\u7b49\u5f3a\u5927\u4e13\u6709\u7cfb\u7edf\u3002", "conclusion": "AdaReasoner\u901a\u8fc7\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u4f5c\u4e3a\u901a\u7528\u63a8\u7406\u6280\u80fd\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u81ea\u4e3b\u3001\u81ea\u9002\u5e94\u5730\u4f7f\u7528\u5de5\u5177\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.17602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17602", "abs": "https://arxiv.org/abs/2601.17602", "authors": ["Xuanzhou Chen"], "title": "Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout", "comment": null, "summary": "We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u9ad8\u7ef4\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5d4c\u5165\u4e2d\u7684\u89d2\u5ea6\u76f8\u4f3c\u6027\u5206\u6790Transformer\u8fc7\u53c2\u6570\u5316\uff0c\u4f7f\u7528\u4f2f\u52aa\u5229dropout\u8bc6\u522b\u4fdd\u6301Top-1\u9884\u6d4b\u7684\u7a00\u758f\u6027\u9608\u503c", "motivation": "\u7814\u7a76Transformer\u6a21\u578b\u7684\u8fc7\u53c2\u6570\u5316\u73b0\u8c61\uff0c\u63a2\u7d22\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\u5e94\u7528dropout\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\u7684\u6761\u4ef6\uff0c\u7406\u89e3\u5d4c\u5165\u8868\u793a\u5728\u7a00\u758f\u5316\u4e0b\u7684\u9c81\u68d2\u6027", "method": "\u5728\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u4e4b\u95f4\u5e94\u7528\u4f2f\u52aa\u5229dropout\uff0c\u6539\u53d8\u4fdd\u7559\u6982\u7387p\u6765\u8bc6\u522b\u7a00\u758f\u6027\u9608\u503c\uff1b\u7406\u8bba\u4e0a\u8bc1\u660e\u5f53\u5d4c\u5165\u6709\u6548\u7a00\u758f\u5ea6\u8db3\u591f\u5927\u65f6\uff0c\u89e3\u7801\u5668\u6027\u80fd\u5728\u9002\u5ea6\u5750\u6807dropout\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff1b\u5b9e\u9a8c\u4e0a\u6784\u5efa\u5e26\u6709\u4e8c\u8fdb\u5236\u64e6\u9664\u901a\u9053\uff08BEC\uff09\u7684Transformer\u6a21\u578b\uff0c\u5728\u82f1\u6cd5\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u6027\u80fd", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u9a8c\u8bc1\u51c6\u786e\u7387\u548cBLEU\u5206\u6570\u5728\u67d0\u4e2a\u9608\u503c\u5904\u6025\u5267\u4e0b\u964d\uff0c\u53ef\u89c6\u5316\u5c55\u793a\u4e86\u8fd9\u4e00\u8d8b\u52bf\uff0c\u8bc1\u5b9e\u4e86\u7a00\u758f\u6027\u9608\u503c\u7684\u5b58\u5728", "conclusion": "Transformer\u6a21\u578b\u5728\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5d4c\u5165\u4e2d\u5b58\u5728\u4e00\u4e2a\u7a00\u758f\u6027\u9608\u503c\uff0c\u5f53dropout\u7387\u8d85\u8fc7\u8be5\u9608\u503c\u65f6\u6a21\u578b\u6027\u80fd\u4f1a\u6025\u5267\u4e0b\u964d\uff0c\u4f46\u5f53\u5d4c\u5165\u6709\u6548\u7a00\u758f\u5ea6\u8db3\u591f\u5927\u65f6\uff0c\u6a21\u578b\u5728\u9002\u5ea6dropout\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd"}}
{"id": "2601.17607", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17607", "abs": "https://arxiv.org/abs/2601.17607", "authors": ["Daisuke Okanohara"], "title": "A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs", "comment": "9 pages. Part I of a planned series entitled \"A Thermodynamic Theory of Learning.\"", "summary": "Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?\n  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.\n  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.\n  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5b66\u4e60\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u4e0d\u53ef\u9006\u8fc7\u7a0b\uff0c\u9700\u8981\u71b5\u4ea7\u751f\u6765\u5b9e\u73b0\u8ba4\u77e5\u7ed3\u6784\uff0c\u5e76\u63a8\u5bfc\u51fa\u8ba4\u77e5\u901f\u5ea6\u6781\u9650(ESL)\u4e0d\u7b49\u5f0f\uff0c\u4e3a\u4efb\u4f55\u5b66\u4e60\u8fc7\u7a0b\u7684\u6700\u5c0f\u71b5\u4ea7\u751f\u8bbe\u5b9a\u4e0b\u754c\u3002", "motivation": "\u7ecf\u5178\u4fe1\u606f\u8bba\u8868\u660e\u786e\u5b9a\u6027\u53d8\u6362\u4e0d\u4f1a\u589e\u52a0\u4fe1\u606f\uff0c\u4f46\u5b66\u4e60\u7cfb\u7edf\u5374\u80fd\u83b7\u5f97\u7ed3\u6784\u5316\u5185\u90e8\u8868\u5f81\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u5b66\u4e60\u5982\u4f55\u5728\u9075\u5b88\u4fe1\u606f\u8bba\u9650\u5236\u7684\u540c\u65f6\u4ea7\u751f\u62bd\u8c61\u548c\u6d1e\u5bdf\uff1f\u4f5c\u8005\u8ba4\u4e3a\u5b66\u4e60\u5728\u6709\u9650\u65f6\u95f4\u5185\u672c\u8d28\u4e0a\u662f\u4e0d\u53ef\u9006\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u8ba4\u77e5\u7ed3\u6784\u5fc5\u7136\u9700\u8981\u71b5\u4ea7\u751f\u3002", "method": "\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u6a21\u578b\u914d\u7f6e\u6982\u7387\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u4f20\u8f93\u8fc7\u7a0b\uff0c\u5f15\u5165\u8ba4\u77e5\u81ea\u7531\u80fd\u6846\u67b6\u3002\u5728\u8be5\u6846\u67b6\u4e2d\u5b9a\u4e49\u81ea\u7531\u80fd\u4e0b\u964d\u4f5c\u4e3a\u8bb0\u5f55\u5b66\u4e60\u8f68\u8ff9\u4e0a\u8ba4\u77e5\u81ea\u7531\u80fd\u603b\u51cf\u5c11\u7684\u8bb0\u8d26\u91cf\uff0c\u5c06\u5176\u5206\u89e3\u4e3a\u4e0e\u6f5c\u5728\u6539\u8fdb\u76f8\u5173\u7684\u53ef\u9006\u5206\u91cf\u548c\u4e0e\u71b5\u4ea7\u751f\u5bf9\u5e94\u7684\u4e0d\u53ef\u9006\u5206\u91cf\u3002", "result": "\u63a8\u5bfc\u51fa\u8ba4\u77e5\u901f\u5ea6\u6781\u9650(ESL)\u4e0d\u7b49\u5f0f\uff0c\u4e3a\u4efb\u4f55\u5b66\u4e60\u8fc7\u7a0b\u5b9e\u73b0\u7ed9\u5b9a\u5206\u5e03\u53d8\u6362\u6240\u9700\u7684\u6700\u5c0f\u71b5\u4ea7\u751f\u8bbe\u5b9a\u4e0b\u754c\u3002\u8be5\u4e0b\u754c\u4ec5\u53d6\u51b3\u4e8e\u521d\u59cb\u548c\u6700\u7ec8\u96c6\u5408\u5206\u5e03\u4e4b\u95f4\u7684Wasserstein\u8ddd\u79bb\uff0c\u4e0e\u5177\u4f53\u5b66\u4e60\u7b97\u6cd5\u65e0\u5173\u3002", "conclusion": "\u5b66\u4e60\u5728\u6709\u9650\u65f6\u95f4\u5185\u672c\u8d28\u4e0a\u662f\u4e0d\u53ef\u9006\u7684\uff0c\u5b9e\u73b0\u8ba4\u77e5\u7ed3\u6784\u9700\u8981\u71b5\u4ea7\u751f\u3002\u8ba4\u77e5\u901f\u5ea6\u6781\u9650\u4e3a\u5b66\u4e60\u8fc7\u7a0b\u7684\u6548\u7387\u8bbe\u5b9a\u4e86\u57fa\u672c\u9650\u5236\uff0c\u4e3a\u7406\u89e3\u5b66\u4e60\u7cfb\u7edf\u7684\u70ed\u529b\u5b66\u6210\u672c\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2601.17586", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17586", "abs": "https://arxiv.org/abs/2601.17586", "authors": ["Sebastian Doerrich", "Francesco Di Salvo", "Jonas Alle", "Christian Ledig"], "title": "Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization", "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2026)", "summary": "Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .", "AI": {"tldr": "\u63d0\u51faStylizing ViT\uff0c\u4e00\u79cd\u57fa\u4e8eVision Transformer\u7684\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5171\u4eab\u6743\u91cd\u7684\u6ce8\u610f\u529b\u5757\u540c\u65f6\u5904\u7406\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u98ce\u683c\u8fc1\u79fb\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u9886\u57df\u6cdb\u5316\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7531\u4e8e\u6570\u636e\u5f02\u8d28\u6027\u548c\u7a00\u7f3a\u6027\uff0c\u5728\u4e0d\u540c\u9886\u57df\u548c\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u4f20\u7edf\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5728\u663e\u8457\u9886\u57df\u504f\u79fb\u4e0b\u6548\u679c\u6709\u9650\uff0c\u800c\u73b0\u6709\u7684\u98ce\u683c\u5316\u589e\u5f3a\u65b9\u6cd5\u8981\u4e48\u98ce\u683c\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u8981\u4e48\u4f1a\u5f15\u5165\u56fe\u50cf\u4f2a\u5f71\u3002", "method": "\u63d0\u51faStylizing ViT\uff0c\u4e00\u79cd\u65b0\u9896\u7684Vision Transformer\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u6743\u91cd\u5171\u4eab\u7684\u6ce8\u610f\u529b\u5757\u540c\u65f6\u5904\u7406\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5141\u8bb8\u540c\u4e00\u4e2a\u6ce8\u610f\u529b\u5757\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6267\u884c\u98ce\u683c\u8fc1\u79fb\u3002", "result": "\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u76ae\u80a4\u75c5\u5b66\u7684\u4e09\u4e2a\u4e0d\u540c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff08\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534713%\uff09\uff0c\u540c\u65f6\u751f\u6210\u65e0\u4f2a\u5f71\u7684\u611f\u77e5\u53ef\u4fe1\u56fe\u50cf\u3002\u6b64\u5916\uff0cStylizing ViT\u5728\u63a8\u7406\u9636\u6bb5\u7528\u4e8e\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e8617%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Stylizing ViT\u901a\u8fc7\u521b\u65b0\u7684\u6743\u91cd\u5171\u4eab\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6709\u6548\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u6790\u6a21\u578b\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.17657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17657", "abs": "https://arxiv.org/abs/2601.17657", "authors": ["Taewan Cho", "Taeryang Kim", "Andrew Jaeyong Choi"], "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation", "comment": null, "summary": "Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip", "AI": {"tldr": "SPACE-CLIP\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u89e3\u7801\u5668\u67b6\u6784\uff0c\u76f4\u63a5\u4ece\u51bb\u7ed3\u7684CLIP\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u63d0\u53d6\u51e0\u4f55\u77e5\u8bc6\uff0c\u65e0\u9700\u6587\u672c\u7f16\u7801\u5668\u548c\u6587\u672c\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "CLIP\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5929\u751f\u96be\u4ee5\u611f\u77e5\u51e0\u4f55\u7ed3\u6784\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u67e5\u8be2CLIP\u7684\u65b9\u5f0f\u95f4\u63a5\u4e14\u4f4e\u6548\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u63a5\u7684\u65b9\u6cd5\u6765\u89e3\u9501CLIP\u4e2d\u7684\u51e0\u4f55\u77e5\u8bc6\u3002", "method": "\u63d0\u51faSPACE-CLIP\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u8def\u5f84\uff1a\u8bed\u4e49\u8def\u5f84\u4f7f\u7528\u7279\u5f81\u7ea7\u7ebf\u6027\u8c03\u5236\uff08FiLM\uff09\u89e3\u91ca\u9ad8\u5c42\u7279\u5f81\u5e76\u52a8\u6001\u9002\u5e94\u5168\u5c40\u4e0a\u4e0b\u6587\uff1b\u7ed3\u6784\u8def\u5f84\u4ece\u65e9\u671f\u5c42\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\u3002\u4e24\u6761\u8def\u5f84\u901a\u8fc7\u5206\u5c42\u878d\u5408\u5b9e\u73b0\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u7cbe\u786e\u51e0\u4f55\u7684\u7a33\u5065\u5408\u6210\u3002", "result": "\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSPACE-CLIP\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684CLIP\u57fa\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u53cc\u8def\u5f84\u534f\u540c\u878d\u5408\u5bf9\u6b64\u6210\u529f\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "SPACE-CLIP\u4e3a\u91cd\u65b0\u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u67b6\u6784\u4f18\u96c5\u7684\u84dd\u56fe\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u6df1\u5ea6\u4f30\u8ba1\u5668\uff0c\u8fd8\u662f\u4e00\u4e2a\u6613\u4e8e\u96c6\u6210\u7684\u7a7a\u95f4\u611f\u77e5\u6a21\u5757\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u5177\u8eabAI\u7cfb\u7edf\u3002"}}
{"id": "2601.18706", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18706", "abs": "https://arxiv.org/abs/2601.18706", "authors": ["Zhichao Yang", "Sepehr Janghorbani", "Dongxu Zhang", "Jun Han", "Qian Qian", "Andrew Ressler", "Gregory D. Lyng", "Sanjit Singh Batra", "Robert E. Tillman"], "title": "Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs", "comment": null, "summary": "Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.", "AI": {"tldr": "Health-SCORE\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u533b\u7597LLM\u8bad\u7ec3\u8bc4\u4f30\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u8bc4\u5206\u6807\u51c6\u5f00\u53d1\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u8bc4\u4f30\u8d28\u91cf\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u8bc4\u5206\u6807\u51c6\u5bf9\u8bc4\u4f30\u5f00\u653e\u5f0fLLM\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u521b\u5efa\u9ad8\u8d28\u91cf\u3001\u9886\u57df\u7279\u5b9a\u7684\u8bc4\u5206\u6807\u51c6\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u65f6\u95f4\u548c\u5f00\u53d1\u6210\u672c\uff0c\u4f7f\u5f97\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u96be\u4ee5\u6269\u5c55\u3002", "method": "Health-SCORE\u662f\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u8bc4\u5206\u6807\u51c6\u5f00\u53d1\u6210\u672c\u800c\u4e0d\u727a\u7272\u6027\u80fd\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u5956\u52b1\u4fe1\u53f7\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u53ef\u76f4\u63a5\u6574\u5408\u5230\u63d0\u793a\u4e2d\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6539\u8fdb\u54cd\u5e94\u8d28\u91cf\u3002", "result": "\u5728\u5f00\u653e\u5f0f\u533b\u7597\u4efb\u52a1\u4e2d\uff0cHealth-SCORE\u5b9e\u73b0\u4e86\u4e0e\u4eba\u5de5\u521b\u5efa\u8bc4\u5206\u6807\u51c6\u76f8\u5f53\u7684\u8bc4\u4f30\u8d28\u91cf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5f00\u53d1\u5de5\u4f5c\u91cf\uff0c\u4f7f\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Health-SCORE\u6846\u67b6\u901a\u8fc7\u964d\u4f4e\u8bc4\u5206\u6807\u51c6\u5f00\u53d1\u6210\u672c\uff0c\u4f7f\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u533b\u7597LLM\u8bc4\u4f30\u548c\u8bad\u7ec3\u66f4\u52a0\u53ef\u6269\u5c55\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17625", "abs": "https://arxiv.org/abs/2601.17625", "authors": ["Yuhan Xie", "Jinhan Liu", "Xiaoyong Ni", "Fei Tan", "Icare Sakr", "Thibault Collin", "Shiqi Sun", "Alejandro Rodriguez Guajardo", "Demon Fanny", "Charles-francois Vincent Latchoumane", "Henri Lorach", "Jocelyne Bloch", "Gregoire Courtine", "Mahsa Shoaran"], "title": "BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation", "comment": "21 pages,7 figures", "summary": "Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.", "AI": {"tldr": "BrainDistill\uff1a\u4e00\u79cd\u7528\u4e8e\u690d\u5165\u5f0fBCI\u7684\u65b0\u578b\u8fd0\u52a8\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u84b8\u998f\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42", "motivation": "Transformer\u89e3\u7801\u5668\u5728BCI\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53c2\u6570\u591a\u3001\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u529f\u7387\u53d7\u9650\u7684\u690d\u5165\u5f0f\u7cfb\u7edf\u4e2d", "method": "\u63d0\u51faBrainDistill\u6846\u67b6\uff1a1\uff09\u690d\u5165\u5f0f\u795e\u7ecf\u89e3\u7801\u5668\uff08IND\uff09\uff1b2\uff09\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u84b8\u998f\uff08TSKD\uff09\uff0c\u901a\u8fc7\u76d1\u7763\u6295\u5f71\u4f18\u5148\u4fdd\u7559\u5173\u952e\u7279\u5f81\uff1b3\uff09\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u65b9\u6848\uff0c\u652f\u6301\u4ec5\u6574\u6570\u63a8\u7406", "result": "IND\u5728\u591a\u4e2a\u795e\u7ecf\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5148\u524d\u89e3\u7801\u5668\uff0cTSKD\u84b8\u998f\u53d8\u4f53\u5728\u5c11\u6837\u672c\u6821\u51c6\u8bbe\u7f6e\u4e2d\u8d85\u8d8a\u5176\u4ed6\u84b8\u998f\u65b9\u6cd5\uff0c\u91cf\u5316IND\u5728\u4e25\u683c\u529f\u7387\u7ea6\u675f\u4e0b\u90e8\u7f72\u65f6\u6027\u80fd\u635f\u5931\u6700\u5c0f", "conclusion": "BrainDistill\u89e3\u51b3\u4e86\u690d\u5165\u5f0fBCI\u7cfb\u7edf\u7684\u529f\u7387\u7ea6\u675f\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u91cf\u5316\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u4f4e\u529f\u8017\u7684\u8fd0\u52a8\u89e3\u7801"}}
{"id": "2601.17666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17666", "abs": "https://arxiv.org/abs/2601.17666", "authors": ["Xinyue Pan", "Yuhao Chen", "Fengqing Zhu"], "title": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting", "comment": "Accepted by CAI2026", "summary": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.", "AI": {"tldr": "\u63d0\u51faPrompt Grafting\u6846\u67b6\u89e3\u51b3\u591a\u98df\u7269\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7269\u4f53\u7ea0\u7f20\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5e03\u5c40\u63d0\u793a\u548c\u76ee\u6807\u63d0\u793a\u5ac1\u63a5\u5b9e\u73b0\u53ef\u63a7\u7684\u98df\u7269\u5206\u79bb", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9910\u98df\u56fe\u50cf\u901a\u5e38\u5305\u542b\u591a\u79cd\u98df\u7269\uff0c\u4f46\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u591a\u98df\u7269\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u7269\u4f53\u7ea0\u7f20\u95ee\u9898\uff0c\u76f8\u90bb\u98df\u7269\uff08\u5982\u7c73\u996d\u548c\u6c64\uff09\u4f1a\u878d\u5408\u5728\u4e00\u8d77\uff0c\u56e0\u4e3a\u8bb8\u591a\u98df\u7269\u6ca1\u6709\u6e05\u6670\u7684\u8fb9\u754c\u3002\u8fd9\u5bf9\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u996e\u98df\u8bc4\u4f30\u548c\u98df\u8c31\u53ef\u89c6\u5316\u7b49\u5e94\u7528\u9020\u6210\u4e86\u56f0\u96be\u3002", "method": "\u63d0\u51faPrompt Grafting\uff08PG\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5c06\u6587\u672c\u4e2d\u7684\u663e\u5f0f\u7a7a\u95f4\u7ebf\u7d22\u4e0e\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u9690\u5f0f\u5e03\u5c40\u6307\u5bfc\u76f8\u7ed3\u5408\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a\u9996\u5148\u4f7f\u7528\u5e03\u5c40\u63d0\u793a\u5efa\u7acb\u4e0d\u540c\u7684\u533a\u57df\uff0c\u7136\u540e\u5728\u5e03\u5c40\u5f62\u6210\u7a33\u5b9a\u540e\u5c06\u76ee\u6807\u63d0\u793a\u5ac1\u63a5\u4e0a\u53bb\u3002\u7528\u6237\u53ef\u4ee5\u7f16\u8f91\u5e03\u5c40\u6392\u5217\u6765\u63a7\u5236\u54ea\u4e9b\u98df\u7269\u5e94\u8be5\u4fdd\u6301\u5206\u79bb\u6216\u6709\u610f\u6df7\u5408\u3002", "result": "\u5728\u4e24\u4e2a\u98df\u7269\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u7269\u4f53\u7684\u5b58\u5728\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u63a7\u5206\u79bb\u7684\u5b9a\u6027\u8bc1\u636e\u3002", "conclusion": "Prompt Grafting\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u98df\u7269\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7269\u4f53\u7ea0\u7f20\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u7a7a\u95f4\u63d0\u793a\u548c\u9690\u5f0f\u5e03\u5c40\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u5bf9\u98df\u7269\u5206\u79bb\u7684\u53ef\u63a7\u751f\u6210\uff0c\u4e3a\u56fe\u50cf\u57fa\u996e\u98df\u8bc4\u4f30\u548c\u98df\u8c31\u53ef\u89c6\u5316\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6570\u636e\u589e\u5f3a\u5de5\u5177\u3002"}}
{"id": "2601.18716", "categories": ["cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2601.18716", "abs": "https://arxiv.org/abs/2601.18716", "authors": ["Naeyma N. Islam", "Thomas R. Caulfield"], "title": "Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules", "comment": "30 pages, 8 figures", "summary": "Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.", "AI": {"tldr": "AI\u8f85\u52a9\u836f\u7269\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7E3\u8fde\u63a5\u9176\u5bfc\u5411\u7684\u5206\u5b50\u80f6\u4fc3\u8fdbA\u03b2-42\u9776\u5411\u964d\u89e3\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6cbb\u7597", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e2d\u7ec6\u80de\u5185A\u03b2-42\u7684\u79ef\u7d2f\u662f\u75be\u75c5\u8fdb\u5c55\u7684\u65e9\u671f\u6bd2\u6027\u9a71\u52a8\u56e0\u7d20\uff0c\u9700\u8981\u5f00\u53d1\u9776\u5411\u964d\u89e3\u7b56\u7565", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ed3\u6784\u7684\u5efa\u6a21\u3001ADMET\u7b5b\u9009\u548c\u5bf9\u63a5\u8bc4\u4f30A\u03b2-42\u4e0e\u4e09\u79cdE3\u8fde\u63a5\u9176\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5f00\u53d1LC-JT-VAE\u751f\u6210\u8fde\u63a5\u9176\u7279\u5f02\u6027\u5c0f\u5206\u5b50", "result": "\u751f\u6210\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u5316\u5b66\u6709\u6548\u3001\u65b0\u9896\u4e14\u9776\u5411\u7279\u5f02\u6027\u7684\u5206\u5b50\u80f6\uff0c\u80fd\u591f\u4fc3\u8fdbA\u03b2-42\u964d\u89e3", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8bbe\u8ba1UPS\u9776\u5411\u7597\u6cd5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6"}}
{"id": "2601.17641", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.17641", "abs": "https://arxiv.org/abs/2601.17641", "authors": ["Hao Fang", "Ryan A. Canfield", "Tomohiro Ouchi", "Beatrice Macagno", "Eli Shlizerman", "Amy L. Orsborn"], "title": "RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding", "comment": null, "summary": "Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.", "AI": {"tldr": "RPNT\u662f\u4e00\u79cd\u9c81\u68d2\u7684\u9884\u8bad\u7ec3\u795e\u7ecfTransformer\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5b9e\u73b0\u8de8\u4f1a\u8bdd\u3001\u8de8\u7c7b\u578b\u3001\u8de8\u53d7\u8bd5\u8005\u548c\u8de8\u8111\u533a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u8111\u89e3\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u8111\u89e3\u7801\u6a21\u578b\u9700\u8981\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u8111\u533a\u3001\u4e0d\u540c\u4f1a\u8bdd\u3001\u4e0d\u540c\u884c\u4e3a\u7c7b\u578b\u548c\u4e0d\u540c\u53d7\u8bd5\u8005\u7684\u795e\u7ecf\u6d3b\u52a8\u8bb0\u5f55\u3002\u73b0\u6709\u6a21\u578b\u53ea\u80fd\u90e8\u5206\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u548c\u6cdb\u5316\u7684\u9884\u8bad\u7ec3\u795e\u7ecfTransformer\u6a21\u578b\u3002", "method": "\u63d0\u51faRPNT\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u591a\u7ef4\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08MRoPE\uff09\u805a\u5408\u5b9e\u9a8c\u5143\u6570\u636e\uff1b2\uff09\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5377\u79ef\u6838\u5904\u7406\u5168\u5c40\u6ce8\u610f\u529b\u4ee5\u5b66\u4e60\u5c40\u90e8\u65f6\u95f4\u7ed3\u6784\uff1b3\uff09\u9c81\u68d2\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\uff0c\u91c7\u7528\u5747\u5300\u56e0\u679c\u63a9\u7801\u7b56\u7565\u548c\u5bf9\u6bd4\u8868\u793a\u3002\u5728\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff1aa\uff09\u591a\u4f1a\u8bdd\u3001\u591a\u4efb\u52a1\u3001\u591a\u53d7\u8bd5\u8005\u7684\u5fae\u7535\u6781\u57fa\u51c6\u6570\u636e\uff1bb\uff09\u4f7f\u7528\u9ad8\u5bc6\u5ea6Neuropixel 1.0\u63a2\u9488\u7684\u591a\u8111\u533a\u8bb0\u5f55\u3002", "result": "RPNT\u5728\u8de8\u4f1a\u8bdd\u3001\u8de8\u7c7b\u578b\u3001\u8de8\u53d7\u8bd5\u8005\u548c\u8de8\u8111\u533a\u7684\u4e0b\u6e38\u884c\u4e3a\u89e3\u7801\u4efb\u52a1\u4e2d\uff0c\u59cb\u7ec8\u8fbe\u5230\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u89e3\u7801\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "RPNT\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u795e\u7ecf\u6d3b\u52a8\u89e3\u7801\u7684\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\u548c\u795e\u7ecf\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17646", "categories": ["cs.LG", "math.FA", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.17646", "abs": "https://arxiv.org/abs/2601.17646", "authors": ["Karim Bounja", "Lahcen Laayouni", "Abdeljalil Sakat"], "title": "A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization", "comment": null, "summary": "Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlev\u00e9-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u51f8\u975e\u4e25\u683c\u635f\u5931\u51fd\u6570\u4ea7\u751f\u96c6\u503c\u6700\u5c0f\u5316\u5668\u7684\u60c5\u51b5\u3002\u4f5c\u8005\u63d0\u51faPainlev\u00e9-Kuratowski\u4e0a\u534a\u8fde\u7eed\u6027\uff08PK-u.s.c.\uff09\u4f5c\u4e3aERM\u89e3\u5bf9\u5e94\u7684\u5185\u5728\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u5e76\u5efa\u7acb\u4e86\u5728Mosco\u4e00\u81f4\u6027\u6270\u52a8\u548c\u5c40\u90e8\u6709\u754c\u6700\u5c0f\u5316\u5668\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u7406\u8bba\u3002", "motivation": "\u4f20\u7edfERM\u7a33\u5b9a\u6027\u7814\u7a76\u901a\u5e38\u57fa\u4e8e\u5355\u503c\u8f93\u51fa\uff0c\u4f46\u51f8\u975e\u4e25\u683c\u635f\u5931\u51fd\u6570\u4f1a\u4ea7\u751f\u96c6\u503c\u6700\u5c0f\u5316\u5668\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8fd9\u79cd\u96c6\u503c\u60c5\u51b5\u7684\u7a33\u5b9a\u6027\u5206\u6790\u6846\u67b6\uff0c\u9700\u8981\u5efa\u7acb\u9002\u7528\u4e8e\u96c6\u503c\u6700\u5c0f\u5316\u5668\u7684\u7a33\u5b9a\u6027\u7406\u8bba\uff0c\u4ee5\u6b63\u786e\u89e3\u91ca\u9009\u62e9\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528Painlev\u00e9-Kuratowski\u4e0a\u534a\u8fde\u7eed\u6027\uff08PK-u.s.c.\uff09\u4f5c\u4e3aERM\u89e3\u5bf9\u5e94\u7684\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u8fd9\u662f\u96c6\u5408\u5c42\u9762\u7684Hadamard\u9002\u5b9a\u6027\u3002\u5728\u6700\u5c0f\u975e\u9000\u5316\u5b9a\u6027\u673a\u5236\u4e0b\uff0c\u5206\u6790Mosco\u4e00\u81f4\u6027\u6270\u52a8\u548c\u5c40\u90e8\u6709\u754c\u6700\u5c0f\u5316\u5668\u7684\u6761\u4ef6\uff0c\u8bc1\u660e\u8fd9\u4e9b\u6761\u4ef6\u80fd\u4fdd\u8bc1PK-u.s.c.\u3001\u6700\u5c0f\u503c\u8fde\u7eed\u6027\u4ee5\u53ca\u6d88\u5931\u95f4\u9699\u8fd1\u6700\u5c0f\u5316\u5668\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728Mosco\u4e00\u81f4\u6027\u6270\u52a8\u548c\u5c40\u90e8\u6709\u754c\u6700\u5c0f\u5316\u5668\u7684\u6761\u4ef6\u4e0b\uff0cERM\u89e3\u5bf9\u5e94\u5177\u6709PK-u.s.c.\u6027\u8d28\uff0c\u6700\u5c0f\u503c\u8fde\u7eed\uff0c\u4e14\u6d88\u5931\u95f4\u9699\u8fd1\u6700\u5c0f\u5316\u5668\u5177\u6709\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u4e8c\u6b21\u589e\u957f\u6761\u4ef6\u80fd\u591f\u4ea7\u751f\u663e\u5f0f\u7684\u5b9a\u91cf\u504f\u5dee\u754c\u3002", "conclusion": "PK-u.s.c.\u662fERM\u89e3\u5bf9\u5e94\u7684\u5185\u5728\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u662f\u89e3\u91ca\u9009\u62e9\u7a33\u5b9a\u6027\u7684\u5148\u51b3\u6761\u4ef6\u3002\u5728\u9002\u5f53\u7684\u975e\u9000\u5316\u6761\u4ef6\u4e0b\uff0cMosco\u4e00\u81f4\u6027\u6270\u52a8\u548c\u5c40\u90e8\u6709\u754c\u6700\u5c0f\u5316\u5668\u80fd\u591f\u4fdd\u8bc1ERM\u7684\u7a33\u5b9a\u6027\uff0c\u4e3a\u51f8\u975e\u4e25\u683c\u635f\u5931\u51fd\u6570\u7684\u7a33\u5b9a\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.17697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17697", "abs": "https://arxiv.org/abs/2601.17697", "authors": ["Zexi Jia", "Jinchao Zhang", "Jie Zhou"], "title": "StyleDecoupler: Generalizable Artistic Style Disentanglement", "comment": null, "summary": "Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.", "AI": {"tldr": "StyleDecoupler\u662f\u4e00\u4e2a\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5355\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a\u5185\u5bb9\u53c2\u8003\uff0c\u4ece\u591a\u6a21\u6001\u89c6\u89c9\u6a21\u578b\u4e2d\u5206\u79bb\u7eaf\u98ce\u683c\u7279\u5f81\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u8fd0\u884c\u3002", "motivation": "\u827a\u672f\u98ce\u683c\u8868\u793a\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u98ce\u683c\u4e0e\u8bed\u4e49\u5185\u5bb9\u6df1\u5ea6\u7ea0\u7f20\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5206\u79bb\u98ce\u683c\u548c\u5185\u5bb9\u7279\u5f81\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u89c6\u89c9\u6a21\u578b\u7f16\u7801\u98ce\u683c\u548c\u5185\u5bb9\uff0c\u800c\u5355\u6a21\u6001\u6a21\u578b\u6291\u5236\u98ce\u683c\u4ee5\u5173\u6ce8\u5185\u5bb9\u4e0d\u53d8\u7279\u5f81\u3002\u4f7f\u7528\u5355\u6a21\u6001\u8868\u793a\u4f5c\u4e3a\u7eaf\u5185\u5bb9\u53c2\u8003\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u6700\u5c0f\u5316\u4ece\u591a\u6a21\u6001\u5d4c\u5165\u4e2d\u5206\u79bb\u7eaf\u98ce\u683c\u7279\u5f81\u3002", "result": "\u5728WeART\uff0828\u4e07\u4ef6\u827a\u672f\u54c1\uff0c152\u79cd\u98ce\u683c\uff0c1556\u4f4d\u827a\u672f\u5bb6\uff09\u548cWikiART\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u98ce\u683c\u68c0\u7d22\u6027\u80fd\uff0c\u652f\u6301\u98ce\u683c\u5173\u7cfb\u6620\u5c04\u548c\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u7b49\u5e94\u7528\u3002", "conclusion": "StyleDecoupler\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u6709\u6548\u5206\u79bb\u98ce\u683c\u7279\u5f81\uff0c\u63d0\u51fa\u7684WeART\u57fa\u51c6\u4e3a\u827a\u672f\u98ce\u683c\u5206\u6790\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u827a\u672f\u98ce\u683c\u8868\u793a\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.18744", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18744", "abs": "https://arxiv.org/abs/2601.18744", "authors": ["Fangxu Yu", "Xingang Guo", "Lingzhi Yuan", "Haoqiang Kang", "Hongyu Zhao", "Lianhui Qin", "Furong Huang", "Bin Hu", "Tianyi Zhou"], "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models", "comment": null, "summary": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.", "AI": {"tldr": "TSRBench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b4125\u4e2a\u95ee\u9898\u300114\u4e2a\u9886\u57df\u30014\u4e2a\u4e3b\u8981\u7ef4\u5ea6\uff0c\u8bc4\u4f30\u4e8630\u591a\u4e2a\u9886\u5148\u7684LLM\u3001VLM\u548cTSLLM\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u65e0\u5904\u4e0d\u5728\u4e14\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u901a\u7528\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7f3a\u4e4f\u65f6\u95f4\u5e8f\u5217\u7ef4\u5ea6\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u6d4b\u8bd5\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165TSRBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b4125\u4e2a\u95ee\u9898\uff0c\u6db5\u76d614\u4e2a\u9886\u57df\uff0c\u5206\u4e3a4\u4e2a\u4e3b\u8981\u7ef4\u5ea6\uff08\u611f\u77e5\u3001\u63a8\u7406\u3001\u9884\u6d4b\u3001\u51b3\u7b56\u5236\u5b9a\uff09\uff0c\u5305\u542b15\u4e2a\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8630\u591a\u4e2a\u9886\u5148\u7684\u4e13\u6709\u548c\u5f00\u6e90LLM\u3001VLM\u548cTSLLM\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u7f29\u653e\u5b9a\u5f8b\u9002\u7528\u4e8e\u611f\u77e5\u548c\u63a8\u7406\uff0c\u4f46\u5728\u9884\u6d4b\u4e2d\u5931\u6548\uff1b2\uff09\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u80fd\u4fdd\u8bc1\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u6d4b\uff0c\u8868\u660e\u8bed\u4e49\u7406\u89e3\u548c\u6570\u503c\u9884\u6d4b\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff1b3\uff09\u5c3d\u7ba1\u6587\u672c\u548c\u89c6\u89c9\u8868\u793a\u5177\u6709\u4e92\u8865\u6027\uff0c\u4f46\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u672a\u80fd\u6709\u6548\u878d\u5408\u5b83\u4eec\u4ee5\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "TSRBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\uff0c\u4e0d\u4ec5\u7a81\u51fa\u4e86\u73b0\u6709\u6311\u6218\uff0c\u8fd8\u4e3a\u63a8\u8fdb\u901a\u7528\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.17647", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17647", "abs": "https://arxiv.org/abs/2601.17647", "authors": ["Akila Sampath", "Vandana Janeja", "Jianwu Wang"], "title": "Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics", "comment": null, "summary": "Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\\% reduction in estimation error.", "AI": {"tldr": "\u63d0\u51faKGCM-VAE\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u7684\u56e0\u679c\u5efa\u6a21\u91cf\u5316\u6d77\u51b0\u539a\u5ea6\u4e0e\u6d77\u9762\u9ad8\u5ea6\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u548c\u5206\u5e03\u5e73\u8861\u673a\u5236\u63d0\u5347\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u91cf\u5316\u51b0\u878d\u5316\u548c\u6de1\u6c34\u5206\u5e03\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u5bf9\u7406\u89e3\u6781\u5730\u6c14\u5019\u53d8\u5316\u548c\u5168\u7403\u6d77\u5e73\u9762\u4e0a\u5347\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u7a7a\u8bbe\u7f6e\u4e2d\u5904\u7406\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u65f6\u9762\u4e34\u672a\u89c2\u6d4b\u6df7\u6742\u53d8\u91cf\u548c\u7f3a\u4e4f\u7269\u7406\u7ea6\u675f\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u5f15\u5bfc\u7684\u56e0\u679c\u6a21\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668(KGCM-VAE)\uff0c\u5305\u542b\uff1a1)\u901f\u5ea6\u8c03\u5236\u65b9\u6848\uff0c\u901a\u8fc7SSH\u8f6c\u6362\u63a7\u5236\u7684sigmoid\u51fd\u6570\u52a8\u6001\u653e\u5927\u5e73\u6ed1\u901f\u5ea6\u4fe1\u53f7\u751f\u6210\u7269\u7406\u57fa\u7840\u7684\u56e0\u679c\u5904\u7406\uff1b2)\u6700\u5927\u5747\u503c\u5dee\u5f02(MMD)\u5e73\u8861\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5904\u7406\u548c\u5bf9\u7167\u534f\u53d8\u91cf\u5206\u5e03\uff1b3)\u56e0\u679c\u90bb\u63a5\u7ea6\u675f\u89e3\u7801\u5668\u786e\u4fdd\u4e0e\u5df2\u77e5\u7269\u7406\u7ed3\u6784\u5bf9\u9f50\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5317\u6781\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKGCM-VAE\u5728PEHE\u6307\u6807\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u51c6\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9eMMD\u548c\u56e0\u679c\u90bb\u63a5\u7ea6\u675f\u8054\u5408\u5e94\u7528\u4f7f\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e1.88%\u3002", "conclusion": "KGCM-VAE\u901a\u8fc7\u6574\u5408\u7269\u7406\u77e5\u8bc6\u548c\u56e0\u679c\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u7a7a\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u91cf\u5316\u6d77\u51b0\u539a\u5ea6\u4e0e\u6d77\u9762\u9ad8\u5ea6\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\u3002"}}
{"id": "2601.17703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17703", "abs": "https://arxiv.org/abs/2601.17703", "authors": ["Nikhil Kadivar", "Guansheng Li", "Jianlu Zheng", "John M. Higgins", "Ming Dao", "George Em Karniadakis", "Mengjia Xu"], "title": "An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays", "comment": null, "summary": "Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65f6\u95f4\u5e8f\u5217\u663e\u5fae\u955c\u6570\u636e\u4e2d\u91cf\u5316\u7ea2\u7ec6\u80de\u7fa4\u4f53\uff0c\u7279\u522b\u662f\u9570\u72b6\u7ec6\u80de\u5728\u4e0d\u540c\u5bc6\u5ea6\u6761\u4ef6\u4e0b\u7684\u5f62\u6001\u5b66\u8f6c\u53d8\uff0c\u901a\u8fc7AI\u8f85\u52a9\u6807\u6ce8\u3001\u5206\u5272\u3001\u5206\u7c7b\u548c\u5b9e\u4f8b\u8ba1\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9e\u9a8c\u901a\u91cf\u3002", "motivation": "\u7406\u89e3\u9570\u72b6\u7ec6\u80de\u52a8\u529b\u5b66\u9700\u8981\u51c6\u786e\u8bc6\u522b\u4e0d\u540c\u751f\u7269\u7269\u7406\u6761\u4ef6\u4e0b\uff08\u7279\u522b\u662f\u5728\u5bc6\u96c6\u5806\u79ef\u548c\u91cd\u53e0\u7ec6\u80de\u7fa4\u4f53\u4e2d\uff09\u7684\u5f62\u6001\u5b66\u8f6c\u53d8\u3002\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u624b\u52a8\u6807\u6ce8\u7a00\u7f3a\u548c\u7ec6\u80de\u91cd\u53e0\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210AI\u8f85\u52a9\u6807\u6ce8\u3001\u5206\u5272\u3001\u5206\u7c7b\u548c\u5b9e\u4f8b\u8ba1\u6570\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002\u4f7f\u7528Roboflow\u5e73\u53f0\u6807\u6ce8\u5b9e\u9a8c\u56fe\u50cf\u751f\u6210\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8bad\u7ec3nnU-Net\u5206\u5272\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u5206\u6c34\u5cad\u7b97\u6cd5\u89e3\u51b3\u7ec6\u80de\u91cd\u53e0\u95ee\u9898\u4ee5\u63d0\u9ad8\u91cf\u5316\u51c6\u786e\u6027\u3002", "result": "\u8be5\u6846\u67b6\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9ad8\u5206\u5272\u6027\u80fd\uff0c\u80fd\u591f\u9884\u6d4b\u9570\u72b6\u7ec6\u80de\u5206\u6570\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u901a\u8fc7\u5bc6\u96c6\u7ec6\u80de\u60ac\u6d6e\u6db2\u4f7f\u5b9e\u9a8c\u901a\u91cf\u63d0\u9ad8\u4e00\u500d\u4ee5\u4e0a\uff0c\u6355\u83b7\u836f\u7269\u4f9d\u8d56\u6027\u9570\u72b6\u5316\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u7ec6\u80de\u5f62\u6001\u6f14\u5316\u7684\u72ec\u7279\u673a\u68b0\u751f\u7269\u5b66\u7279\u5f81\u3002", "conclusion": "\u8fd9\u4e2aAI\u9a71\u52a8\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u7684\u8ba1\u7b97\u5e73\u53f0\uff0c\u7528\u4e8e\u7814\u7a76\u7ec6\u80de\u751f\u7269\u529b\u5b66\u548c\u8bc4\u4f30\u5fae\u751f\u7406\u7cfb\u7edf\u4e2d\u7684\u6cbb\u7597\u6548\u679c\uff0c\u4e3a\u9570\u72b6\u7ec6\u80de\u75c5\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2601.17654", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17654", "abs": "https://arxiv.org/abs/2601.17654", "authors": ["Ruofan Wu", "Jae-Won Chung", "Mosharaf Chowdhury"], "title": "Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training", "comment": null, "summary": "The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.\n  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.", "AI": {"tldr": "Kareus\u662f\u4e00\u4e2aAI\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7ec6\u7c92\u5ea6\u5185\u6838\u8c03\u5ea6\u548c\u9891\u7387\u7f29\u653e\u6765\u540c\u65f6\u7ba1\u7406\u52a8\u6001\u548c\u9759\u6001\u80fd\u8017\uff0c\u5728\u65f6\u95f4-\u80fd\u8017\u6743\u8861\u524d\u6cbf\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "AI\u8ba1\u7b97\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u4f46\u80fd\u6e90\u4f9b\u5e94\u8ddf\u4e0d\u4e0a\uff0c\u80fd\u6e90\u5df2\u6210\u4e3a\u6602\u8d35\u4e14\u7ade\u4e89\u6fc0\u70c8\u7684\u8d44\u6e90\u3002\u73b0\u6709\u5de5\u4f5c\u53ea\u5173\u6ce8\u80fd\u8017\u7684\u5355\u4e00\u65b9\u9762\uff08\u52a8\u6001\u6216\u9759\u6001\u80fd\u8017\uff09\uff0c\u800c\u7ec6\u7c92\u5ea6\u5185\u6838\u8c03\u5ea6\u548c\u9891\u7387\u7f29\u653e\u4f1a\u5171\u540c\u4e14\u76f8\u4e92\u4f9d\u8d56\u5730\u5f71\u54cd\u4e24\u79cd\u80fd\u8017\u3002", "method": "\u8bbe\u8ba1Kareus\u8bad\u7ec3\u7cfb\u7edf\uff0c\u5c06\u96be\u4ee5\u5904\u7406\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u5c40\u90e8\u7684\u3001\u57fa\u4e8e\u5206\u533a\u7684\u5b50\u95ee\u9898\uff0c\u7136\u540e\u4f7f\u7528\u591a\u901a\u9053\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u6765\u5bfb\u627e\u63a8\u52a8\u65f6\u95f4-\u80fd\u8017\u6743\u8861\u524d\u6cbf\u7684\u6267\u884c\u8c03\u5ea6\u65b9\u6848\u3002", "result": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0cKareus\u5728\u76f8\u540c\u8bad\u7ec3\u65f6\u95f4\u4e0b\u51cf\u5c11\u8bad\u7ec3\u80fd\u8017\u8fbe28.3%\uff0c\u6216\u5728\u76f8\u540c\u80fd\u8017\u4e0b\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u8fbe27.5%\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5185\u6838\u8c03\u5ea6\u548c\u9891\u7387\u7f29\u653e\u6765\u540c\u65f6\u7ba1\u7406\u52a8\u6001\u548c\u9759\u6001\u80fd\u8017\uff0cKareus\u80fd\u591f\u663e\u8457\u63a8\u8fdbAI\u8bad\u7ec3\u7684\u65f6\u95f4-\u80fd\u8017\u6743\u8861\u524d\u6cbf\uff0c\u6709\u6548\u89e3\u51b3\u80fd\u6e90\u4f9b\u5e94\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2601.17720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17720", "abs": "https://arxiv.org/abs/2601.17720", "authors": ["Ting-Hsun Chi", "Chu-Rong Chen", "Chi-Tun Hsu", "Hsuan-Ting Lin", "Sheng-Yu Huang", "Cheng Sun", "Yu-Chiang Frank Wang"], "title": "Advancing Structured Priors for Sparse-Voxel Surface Reconstruction", "comment": null, "summary": "Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u548c\u7a00\u758f\u4f53\u7d20\u5149\u6805\u5316\u4f18\u52bf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f53\u7d20\u521d\u59cb\u5316\u6280\u672f\u548c\u6df1\u5ea6\u51e0\u4f55\u76d1\u7763\uff0c\u5728\u4fdd\u6301\u5feb\u901f\u6536\u655b\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u51e0\u4f55\u7cbe\u5ea6\u548c\u8868\u9762\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e24\u79cd\u663e\u5f0f\u8868\u793a\u65b9\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff1a3D\u9ad8\u65af\u6cfc\u6e85\u6536\u655b\u5feb\u4e14\u6709\u51e0\u4f55\u5148\u9a8c\uff0c\u4f46\u8868\u9762\u4fdd\u771f\u5ea6\u53d7\u9650\u4e8e\u70b9\u72b6\u53c2\u6570\u5316\uff1b\u7a00\u758f\u4f53\u7d20\u5149\u6805\u5316\u63d0\u4f9b\u8fde\u7eed\u4e0d\u900f\u660e\u5ea6\u573a\u548c\u6e05\u6670\u51e0\u4f55\uff0c\u4f46\u5747\u5300\u5bc6\u96c6\u7f51\u683c\u521d\u59cb\u5316\u6536\u655b\u6162\u4e14\u672a\u5145\u5206\u5229\u7528\u573a\u666f\u7ed3\u6784\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "1. \u63d0\u51fa\u4f53\u7d20\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u5c06\u4f53\u7d20\u653e\u7f6e\u5728\u5408\u7406\u4f4d\u7f6e\u5e76\u5177\u6709\u9002\u5f53\u7ec6\u8282\u5c42\u6b21\uff0c\u4e3a\u5355\u573a\u666f\u4f18\u5316\u63d0\u4f9b\u5f3a\u8d77\u70b9\u30022. \u63d0\u51fa\u7cbe\u7ec6\u5316\u6df1\u5ea6\u51e0\u4f55\u76d1\u7763\uff0c\u5c06\u591a\u89c6\u89d2\u7ebf\u7d22\u8f6c\u6362\u4e3a\u76f4\u63a5\u7684\u6bcf\u5c04\u7ebf\u6df1\u5ea6\u6b63\u5219\u5316\uff0c\u589e\u5f3a\u6df1\u5ea6\u4e00\u81f4\u6027\u800c\u4e0d\u6a21\u7cca\u8fb9\u7f18\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u7ec6\u7ed3\u6784\u6062\u590d\u548c\u8868\u9762\u5b8c\u6574\u6027\u65b9\u9762\u90fd\u6709\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5feb\u901f\u6536\u655b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u548c\u7a00\u758f\u4f53\u7d20\u5149\u6805\u5316\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u5feb\u901f\u6536\u655b\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8868\u9762\u91cd\u5efa\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u5b8c\u6574\u6027\u3002"}}
{"id": "2601.17667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17667", "abs": "https://arxiv.org/abs/2601.17667", "authors": ["Pedro P. Santos", "Jacopo Silvestrin", "Alberto Sardinha", "Francisco S. Melo"], "title": "Entropic Risk-Aware Monte Carlo Tree Search", "comment": null, "summary": "We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \\textit{risk-aware} Markov decision processes (MDPs) with \\textit{entropic risk measure} (ERM) objectives. We provide a \\textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \\textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \\textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bc1\u660e\u6b63\u786e\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u71b5\u98ce\u9669\u5ea6\u91cf\u76ee\u6807\u7684\u98ce\u9669\u611f\u77e5\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u63d0\u4f9b\u4e86\u975e\u6e10\u8fd1\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u98ce\u9669\u611f\u77e5MDP\u6c42\u89e3\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7406\u8bba\u4fdd\u8bc1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u53c8\u5b9e\u7528\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u5177\u6709\u71b5\u98ce\u9669\u5ea6\u91cf\u7684\u98ce\u9669\u611f\u77e5MDP\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u4e0a\u9650\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u5f15\u5165\u7684\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u71b5\u98ce\u9669\u5ea6\u91cf\u76ee\u6807\u7684\u98ce\u9669\u611f\u77e5MDP\u7684\u52a8\u6001\u89c4\u5212\u516c\u5f0f\u3002", "result": "\u7b97\u6cd5\u88ab\u8bc1\u660e\u662f\u6b63\u786e\u7684\uff08\u6839\u8282\u70b9\u7ecf\u9a8c\u71b5\u98ce\u9669\u5ea6\u91cf\u6536\u655b\u5230\u6700\u4f18\u71b5\u98ce\u9669\u5ea6\u91cf\uff09\uff0c\u5e76\u4eab\u6709\u591a\u9879\u5f0f\u9057\u61be\u96c6\u4e2d\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u98ce\u9669\u611f\u77e5MCTS\u7b97\u6cd5\uff0c\u4e3a\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.17723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17723", "abs": "https://arxiv.org/abs/2601.17723", "authors": ["Tayyab Nasir", "Daochang Liu", "Ajmal Mian"], "title": "Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study", "comment": null, "summary": "Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u7684\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5206\u6790\u4e86\u8bad\u7ec3\u914d\u7f6e\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9INR-based\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\uff0c\u9700\u8981\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u7684\u771f\u5b9e\u6548\u679c\uff0c\u5206\u6790\u8bad\u7ec3\u914d\u7f6e\u7684\u5f71\u54cd\uff0c\u5efa\u7acb\u6027\u80fd\u57fa\u51c6\u5e76\u8bc6\u522b\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "method": "1) \u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u6bd4\u8f83\u73b0\u6709\u6280\u672f\uff0c\u5728\u591a\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0a\u6c47\u603b\u6027\u80fd\u7ed3\u679c\uff1b2) \u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u548c\u4ee3\u7801\u5e93\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6bd4\u8f83\uff1b3) \u7814\u7a76\u53d7\u63a7\u8bad\u7ec3\u914d\u7f6e\u5bf9\u611f\u77e5\u56fe\u50cf\u8d28\u91cf\u7684\u5f71\u54cd\uff1b4) \u63d0\u51fa\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u5728\u60e9\u7f5a\u5f3a\u5ea6\u53d8\u5316\u7684\u540c\u65f6\u4fdd\u7559\u8fb9\u7f18\u3001\u7eb9\u7406\u548c\u7ec6\u8282\u3002", "result": "1) \u8fd1\u671f\u66f4\u590d\u6742\u7684INR\u65b9\u6cd5\u76f8\u6bd4\u65e9\u671f\u65b9\u6cd5\u4ec5\u6709\u8fb9\u9645\u6539\u8fdb\uff1b2) \u6a21\u578b\u6027\u80fd\u4e0e\u8bad\u7ec3\u914d\u7f6e\u5f3a\u76f8\u5173\uff0c\u8fd9\u4e00\u56e0\u7d20\u5728\u5148\u524d\u5de5\u4f5c\u4e2d\u88ab\u5ffd\u89c6\uff1b3) \u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u80fd\u63d0\u5347\u8de8\u67b6\u6784\u7684\u7eb9\u7406\u4fdd\u771f\u5ea6\uff1b4) \u7f29\u653e\u5b9a\u5f8b\u9002\u7528\u4e8eINR-based ASSR\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u548c\u6570\u636e\u591a\u6837\u6027\u7684\u589e\u52a0\u80fd\u5e26\u6765\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8bad\u7ec3\u914d\u7f6e\u5bf9INR-based\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4e0d\u5e94\u88ab\u5ffd\u89c6\uff1b\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u80fd\u6709\u6548\u63d0\u5347\u7eb9\u7406\u4fdd\u771f\u5ea6\uff1b\u7f29\u653e\u5b9a\u5f8b\u9002\u7528\u4e8e\u8be5\u9886\u57df\uff1b\u8fd1\u671f\u590d\u6742\u65b9\u6cd5\u76f8\u6bd4\u65e9\u671f\u65b9\u6cd5\u6539\u8fdb\u6709\u9650\uff0c\u8868\u660e\u8be5\u9886\u57df\u53ef\u80fd\u63a5\u8fd1\u9971\u548c\u3002"}}
{"id": "2601.17668", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17668", "abs": "https://arxiv.org/abs/2601.17668", "authors": ["Jang-Hyun Kim", "Dongyoon Han", "Sangdoo Yun"], "title": "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction", "comment": null, "summary": "Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u95e8\u63a7\u7684KV\u7f13\u5b58\u9a71\u9010\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7sink-attention\u95e8\u63a7\u6a21\u5757\u8bc6\u522b\u5e76\u4fdd\u7559\u5173\u952eKV\u5bf9\uff0c\u5b9e\u73b0\u9ad8\u8fbe70%\u7684KV\u7f13\u5b58\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u538b\u7f29\u6280\u672f\u901a\u5e38\u5728\u6027\u80fd\u4e0b\u964d\u548c\u8ba1\u7b97\u5f00\u9500\u4e4b\u95f4\u9700\u8981\u6743\u8861\uff0c\u8fd9\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u6548\u7387\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u53c8\u5177\u6709\u53ef\u5ffd\u7565\u8ba1\u7b97\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u95e8\u63a7\u7684KV\u7f13\u5b58\u9a71\u9010\u65b9\u6cd5\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7sink-attention\u95e8\u63a7\u6a21\u5757\u8bc6\u522b\u5173\u952eKV\u5bf9\uff0c\u91c7\u7528\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u95e8\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u7684\u91cd\u5efa\u76ee\u6807\u5b9e\u73b0\u5f3a\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u3002", "result": "\u5728Qwen2.5-1M\u3001Qwen3\u548cGemma3\u7cfb\u5217\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9a71\u9010\u9ad8\u8fbe70%\u7684KV\u7f13\u5b58\u65f6\u4ecd\u80fd\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u4ee3\u7801\u7406\u89e3\u548c\u6570\u5b66\u63a8\u7406\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\u5177\u6709\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u95e8\u63a7\u7684KV\u7f13\u5b58\u9a71\u9010\u65b9\u6cd5\u80fd\u591f\u4ee5\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u5bb6\u65cf\u4e0a\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2601.17733", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17733", "abs": "https://arxiv.org/abs/2601.17733", "authors": ["Junran Lu", "Yuanqi Li", "Hengji Li", "Jie Guo", "Yanwen Guo"], "title": "Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles", "comment": null, "summary": "Boundary Representation (B-Rep) is the widely adopted standard\n  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.\n  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684B-Rep\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c06\u8fb9\u754c\u8868\u793a\u8f6c\u5316\u4e3a\u53ef\u7ec4\u5408\u7684k-cell\u7c92\u5b50\u96c6\u5408\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6d41\u5339\u914d\u6846\u67b6\u5b9e\u73b0\u62d3\u6251\u548c\u51e0\u4f55\u7684\u8054\u5408\u751f\u6210", "motivation": "B-Rep\u5728CAD\u548c\u5236\u9020\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u751f\u6210\u5efa\u6a21\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3aB-Rep\u662f\u51e0\u4f55\u5355\u5143\u590d\u5408\u4f53\uff0c\u62d3\u6251\u4e0e\u51e0\u4f55\u5728\u4e0d\u540c\u9636\u7684\u5355\u5143\uff08\u9876\u70b9\u3001\u8fb9\u3001\u9762\uff09\u4e2d\u76f8\u4e92\u7ea0\u7f20\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u7ea7\u8054\u5e8f\u5217\u5904\u7406\u8fd9\u79cd\u5c42\u6b21\u7ed3\u6784\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5355\u5143\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\uff08\u5982\u90bb\u63a5\u548c\u5171\u4eab\uff09\uff0c\u9650\u5236\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u9519\u8bef\u6062\u590d\u80fd\u529b\u3002", "method": "\u5c06B-Rep\u91cd\u65b0\u8868\u8ff0\u4e3a\u53ef\u7ec4\u5408\u7684k-cell\u7c92\u5b50\u96c6\u5408\uff0c\u5c06\u6bcf\u4e2a\u62d3\u6251\u5b9e\u4f53\u7f16\u7801\u4e3a\u7c92\u5b50\u7684\u7ec4\u5408\uff0c\u76f8\u90bb\u5355\u5143\u5728\u5176\u63a5\u53e3\u5904\u5171\u4eab\u76f8\u540c\u7684\u6f5c\u5728\u8868\u793a\uff0c\u4ece\u800c\u4fc3\u8fdb\u6cbf\u5171\u4eab\u8fb9\u754c\u7684\u51e0\u4f55\u8026\u5408\u3002\u4f7f\u7528\u591a\u6a21\u6001\u6d41\u5339\u914d\u6846\u67b6\u5408\u6210\u8fd9\u4e9b\u7c92\u5b50\u96c6\u5408\uff0c\u652f\u6301\u65e0\u6761\u4ef6\u751f\u6210\u548c\u7cbe\u786e\u6761\u4ef6\u4efb\u52a1\uff08\u5982\u5355\u89c6\u56fe\u6216\u70b9\u4e91\u76843D\u91cd\u5efa\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684CAD\u6a21\u578b\uff0c\u5728\u6709\u6548\u6027\u548c\u53ef\u7f16\u8f91\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u8be5\u8868\u793a\u7684\u81ea\u7136\u6269\u5c55\u652f\u6301\u5c40\u90e8\u4fee\u590d\u548c\u76f4\u63a5\u5408\u6210\u975e\u6d41\u5f62\u7ed3\u6784\uff08\u5982\u7ebf\u6846\uff09\u3002", "conclusion": "\u901a\u8fc7\u5c06B-Rep\u8f6c\u5316\u4e3a\u7c92\u5b50\u96c6\u5408\u8868\u793a\uff0c\u89e3\u8026\u4e86\u521a\u6027\u5c42\u6b21\u7ed3\u6784\uff0c\u7edf\u4e00\u4e86\u9876\u70b9\u3001\u8fb9\u548c\u9762\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u62d3\u6251\u548c\u51e0\u4f55\u8054\u5408\u751f\u6210\uff0c\u4e3aCAD\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u751f\u6210\u6846\u67b6\u3002"}}
{"id": "2601.17740", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17740", "abs": "https://arxiv.org/abs/2601.17740", "authors": ["Cong Cao", "Ren Li", "Corentin Dumery", "Hao Li"], "title": "Learning Sewing Patterns via Latent Flow Matching of Implicit Fields", "comment": null, "summary": "Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u8868\u793a\u7684\u7f1d\u7eab\u56fe\u6848\u5efa\u6a21\u65b9\u6cd5\uff0c\u4f7f\u7528\u7b26\u53f7\u8ddd\u79bb\u573a\u548c\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\u8868\u793a\u670d\u88c5\u9762\u677f\uff0c\u901a\u8fc7\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u548c\u6f5c\u5728\u6d41\u5339\u914d\u6a21\u578b\u5b9e\u73b0\u590d\u6742\u7f1d\u7eab\u56fe\u6848\u7684\u51c6\u786e\u5efa\u6a21\u4e0e\u751f\u6210\u3002", "motivation": "\u7f1d\u7eab\u56fe\u6848\u5b9a\u4e49\u4e86\u670d\u88c5\u7684\u7ed3\u6784\u57fa\u7840\uff0c\u5bf9\u65f6\u5c1a\u8bbe\u8ba1\u3001\u5236\u4f5c\u548c\u7269\u7406\u6a21\u62df\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u81ea\u52a8\u56fe\u6848\u751f\u6210\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u9762\u677f\u51e0\u4f55\u5f62\u72b6\u548c\u63a5\u7f1d\u6392\u5217\u7684\u5e7f\u6cdb\u53ef\u53d8\u6027\uff0c\u51c6\u786e\u5efa\u6a21\u7f1d\u7eab\u56fe\u6848\u4ecd\u7136\u56f0\u96be\u3002", "method": "1. \u4f7f\u7528\u7b26\u53f7\u8ddd\u79bb\u573a\u5b9a\u4e49\u9762\u677f\u8fb9\u754c\uff0c\u65e0\u7b26\u53f7\u8ddd\u79bb\u573a\u8bc6\u522b\u63a5\u7f1d\u7aef\u70b9\uff1b2. \u5c06\u8fd9\u4e9b\u573a\u7f16\u7801\u5230\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\uff0c\u652f\u6301\u53ef\u5fae\u5206\u7f51\u683c\u5316\uff1b3. \u6f5c\u5728\u6d41\u5339\u914d\u6a21\u578b\u5b66\u4e60\u9762\u677f\u7ec4\u5408\u7684\u5206\u5e03\uff1b4. \u7f1d\u7eab\u9884\u6d4b\u6a21\u5757\u4ece\u63d0\u53d6\u7684\u8fb9\u7f18\u6bb5\u6062\u590d\u63a5\u7f1d\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u548c\u751f\u6210\u5177\u6709\u590d\u6742\u7ed3\u6784\u7684\u7f1d\u7eab\u56fe\u6848\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u56fe\u50cf\u4f30\u8ba1\u7f1d\u7eab\u56fe\u6848\u65b9\u9762\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u652f\u6301\u56fe\u6848\u8865\u5168\u548c\u91cd\u65b0\u9002\u914d\u7b49\u5e94\u7528\u3002", "conclusion": "\u8be5\u9690\u5f0f\u8868\u793a\u65b9\u6cd5\u4e3a\u6570\u5b57\u65f6\u5c1a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7f1d\u7eab\u56fe\u6848\u5efa\u6a21\u5de5\u5177\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7ed3\u6784\u5e76\u652f\u6301\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.17689", "categories": ["cs.LG", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17689", "abs": "https://arxiv.org/abs/2601.17689", "authors": ["Shanu Saklani", "Tushar M. Athawale", "Nairita Pal", "David Pugmire", "Christopher R. Johnson", "Soumya Dutta"], "title": "REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization", "comment": null, "summary": "Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.", "AI": {"tldr": "REV-INR\u662f\u4e00\u79cd\u6b63\u5219\u5316\u8bc1\u636e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u540c\u65f6\u9884\u6d4b\u6570\u636e\u503c\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfINR\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u4f53\u79ef\u6570\u636e\u91cd\u5efa\u548c\u53ef\u89c6\u5316\u3002", "motivation": "\u4f20\u7edf\u786e\u5b9a\u6027\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u53ea\u80fd\u9884\u6d4b\u6570\u503c\uff0c\u65e0\u6cd5\u63d0\u4f9b\u6a21\u578b\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u6216\u6570\u636e\u56fa\u6709\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u91cd\u5efa\u4f53\u79ef\u4e2d\u7684\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u4ece\u800c\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u6570\u636e\u89e3\u91ca\u548c\u53ef\u89c6\u5316\u3002\u7531\u4e8e\u539f\u59cb\u6570\u636e\u53ef\u80fd\u56e0\u4f53\u79ef\u8fc7\u5927\u800c\u65e0\u6cd5\u83b7\u5f97\uff0c\u4ece\u6a21\u578b\u9884\u6d4b\u6570\u636e\u4e2d\u8bc6\u522b\u9519\u8bef\u7ed3\u679c\u53ef\u80fd\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51faREV-INR\uff08\u6b63\u5219\u5316\u8bc1\u636e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff09\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b66\u4e60\u51c6\u786e\u9884\u6d4b\u6570\u636e\u503c\u4ee5\u53ca\u76f8\u5173\u7684\u5750\u6807\u7ea7\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff08\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff09\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff08\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff09\u3002", "result": "REV-INR\u5728\u4f53\u79ef\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u591f\u63d0\u4f9b\u9c81\u68d2\u7684\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u6700\u5feb\u3002\u4e0e\u73b0\u6709\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "REV-INR\u80fd\u591f\u8bc4\u4f30\u63d0\u53d6\u7684\u7b49\u503c\u9762\u548c\u4f53\u79ef\u53ef\u89c6\u5316\u7ed3\u679c\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4f7f\u5f97\u5206\u6790\u53ef\u4ee5\u5b8c\u5168\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u7684\u6570\u636e\u8fdb\u884c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfINR\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.17713", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17713", "abs": "https://arxiv.org/abs/2601.17713", "authors": ["Kaile Wang", "Jiannong Cao", "Yu Yang", "Xiaoyin Li", "Yinfeng Cao"], "title": "FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices", "comment": "Accepted by IEEE Annual Congress on Artificial Intelligence of Things (IEEE AIoT) 2025", "summary": "With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.", "AI": {"tldr": "FedCCA\u662f\u4e00\u79cd\u9488\u5bf9\u7269\u8054\u7f51\u6570\u636e\u5f02\u6784\u6027\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u4e2d\u5fc3\u5316\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5229\u7528\u5ba2\u6237\u7aef\u7279\u5b9a\u77e5\u8bc6\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5b66\u4e60\u72ec\u7279\u6a21\u578b\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u7269\u8054\u7f51(IoT)\u5feb\u901f\u53d1\u5c55\u9700\u8981\u57fa\u4e8e\u9690\u79c1\u6570\u636e\uff08\u5982\u4eba\u4f53\u611f\u77e5\u6570\u636e\uff09\u8fdb\u884cAI\u6a21\u578b\u8bad\u7ec3\u3002\u8054\u90a6\u5b66\u4e60(FL)\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7269\u8054\u7f51\u8bbe\u5907\u95f4\u7684\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u56fa\u5b9a\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u548c\u4e91\u7aef\u670d\u52a1\u5668\u805a\u5408\uff0c\u96be\u4ee5\u5728\u672c\u5730\u8bad\u7ec3\u4e2d\u9690\u79c1\u4fdd\u62a4\u5730\u63d0\u53d6\u5ba2\u6237\u7aef\u7279\u5b9a\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u5ba2\u6237\u7aef\u4e2d\u5fc3\u5316\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60(FedCCA)\u7b97\u6cd5\uff1a1\uff09\u901a\u8fc7\u9009\u62e9\u6027\u81ea\u9002\u5e94\u673a\u5236\uff0c\u5229\u7528\u5ba2\u6237\u7aef\u7279\u5b9a\u77e5\u8bc6\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5b66\u4e60\u72ec\u7279\u6a21\u578b\uff1b2\uff09\u91c7\u7528\u57fa\u4e8e\u989d\u5916\u5ba2\u6237\u7aef\u7279\u5b9a\u7f16\u7801\u5668\u7684\u52a8\u6001\u5ba2\u6237\u7aef\u9009\u62e9\u548c\u81ea\u9002\u5e94\u805a\u5408\uff1b3\uff09\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5168\u5c40\u805a\u5408\u7b56\u7565\u589e\u5f3a\u591a\u6e90\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eFedCCA\u5728\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\u65b9\u9762\u76f8\u6bd4\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u5c55\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "FedCCA\u901a\u8fc7\u5ba2\u6237\u7aef\u4e2d\u5fc3\u5316\u81ea\u9002\u5e94\u673a\u5236\u6709\u6548\u7f13\u89e3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u4e3a\u7269\u8054\u7f51\u9690\u79c1\u6570\u636e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17716", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17716", "abs": "https://arxiv.org/abs/2601.17716", "authors": ["Daniel M. Pedrozo", "Telma W. de L. Soares", "Bryan L. M. de Oliveira"], "title": "Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games", "comment": "Presented at the NeusymBridge Workshop at AAAI 2026", "summary": "Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u91cf\u8bc4\u4f30LLMs\u901a\u8fc7\u662f/\u5426\u95ee\u9898\u5728\u5206\u5c42\u77e5\u8bc6\u56fe\u73af\u5883\u4e2d\u6536\u96c6\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u4f5c\u4e3a\u4e3b\u8981\u6307\u6807\uff0c\u5e76\u5728\u5730\u7406\u731c\u57ce\u5e02\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u5177\u6709\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406LLM\u667a\u80fd\u4f53\u5173\u952e\u80fd\u529b\u2014\u2014\u4e3a\u6d88\u9664\u7528\u6237\u8bf7\u6c42\u4e2d\u7684\u6b67\u4e49\u800c\u63d0\u51fa\u597d\u95ee\u9898\u2014\u2014\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u4e5f\u5f88\u5c11\u7cfb\u7edf\u6bd4\u8f83\u4f7f\u7528\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e0e\u4e0d\u4f7f\u7528\u8be5\u63a8\u7406\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u4e2a\u4ea4\u4e92\u7684LLM\u667a\u80fd\u4f53\uff1a\u63d0\u95ee\u8005\u3001\u56de\u7b54\u8005\u548c\u5047\u8bbe\u7a7a\u95f4\u66f4\u65b0\u8005\u3002\u6846\u67b6\u5728\u5206\u5c42\u77e5\u8bc6\u56fe\u73af\u5883\u4e2d\u4f7f\u7528\u662f/\u5426\u95ee\u9898\u6536\u96c6\u4fe1\u606f\uff0c\u91c7\u7528\u57fa\u4e8e\u9999\u519c\u71b5\u7684\u4fe1\u606f\u589e\u76ca\u4f5c\u4e3a\u4e3b\u8981\u6307\u6807\u6765\u8bc4\u4f30\u6bcf\u4e2a\u56de\u5408\u548c\u7d2f\u79ef\u7684\u67e5\u8be2\u6548\u679c\u3002\u5728\u5730\u7406\u731c\u57ce\u5e02\u6e38\u620f\u8bbe\u7f6e\u4e2d\u5b9e\u4f8b\u5316\u8be5\u6846\u67b6\uff0c\u91c7\u7528\u4e94\u7ea7\u5206\u7c7b\u6cd5\u7ec4\u7ec7\uff0c\u8bc4\u4f30\u4e86\u5b8c\u5168\u53ef\u89c2\u5bdf\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u6761\u4ef6\u4e0b\u3001\u6709/\u65e0\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u591a\u4e2aLLM\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8bc4\u4f30\u7684\u6a21\u578b\u4e2d\uff0c\u5177\u6709\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5728\u6bcf\u56de\u5408\u83b7\u5f97\u66f4\u9ad8\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u5e76\u4ee5\u66f4\u5c11\u7684\u6b65\u9aa4\u8fbe\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u8bbe\u7f6e\u4e2d\u3002\u63a8\u7406\u8f68\u8ff9\u5206\u6790\u663e\u793a\uff0c\u8f83\u5c0f\u6a21\u578b\u901a\u8fc7\u66f4\u79ef\u6781\u5730\u63a2\u7d22\u5019\u9009\u95ee\u9898\u6765\u5f25\u8865\u6709\u9650\u80fd\u529b\uff0c\u800c\u8f83\u5927\u6a21\u578b\u5728\u9009\u62e9\u6700\u4f18\u67e5\u8be2\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u81ea\u4fe1\u5ea6\uff0c\u751f\u6210\u5177\u6709\u66f4\u5927\u6f5c\u5728\u4fe1\u606f\u589e\u76ca\u7684\u5019\u9009\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u8bc4\u4f30LLMs\u4fe1\u606f\u6536\u96c6\u80fd\u529b\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u5728\u4fe1\u606f\u6536\u96c6\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u63d0\u95ee\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u5728\u4fe1\u606f\u6536\u96c6\u7b56\u7565\u4e0a\u7684\u5dee\u5f02\u3002"}}
{"id": "2601.17747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17747", "abs": "https://arxiv.org/abs/2601.17747", "authors": ["Kaixuan Jiang", "Chen Wu", "Zhenghui Zhao", "Chengxi Han"], "title": "Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection", "comment": null, "summary": "Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.", "AI": {"tldr": "UniCD\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9065\u611f\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u5668\u548c\u591a\u5206\u652f\u534f\u4f5c\u5b66\u4e60\u673a\u5236\uff0c\u540c\u65f6\u5904\u7406\u76d1\u7763\u3001\u5f31\u76d1\u7763\u548c\u65e0\u76d1\u7763\u4efb\u52a1\uff0c\u5728\u591a\u79cd\u6807\u6ce8\u573a\u666f\u4e0b\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u50cf\u7d20\u7ea7\u53d8\u5316\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u6807\u6ce8\u53ef\u7528\u6027\u7684\u591a\u6837\u5316\u573a\u666f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u5904\u7406\u4e0d\u540c\u76d1\u7763\u7ea7\u522b\u4efb\u52a1\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faUniCD\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u5171\u4eab\u7f16\u7801\u5668\u548c\u4e09\u4e2a\u76d1\u7763\u7279\u5b9a\u5206\u652f\uff1a\u76d1\u7763\u5206\u652f\u5f15\u5165\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\u5b9e\u73b0\u53cc\u65f6\u76f8\u7279\u5f81\u878d\u5408\uff1b\u5f31\u76d1\u7763\u5206\u652f\u6784\u5efa\u53d8\u5316\u8868\u793a\u6b63\u5219\u5316\u5f15\u5bfc\u6a21\u578b\u6536\u655b\uff1b\u65e0\u76d1\u7763\u5206\u652f\u63d0\u51fa\u8bed\u4e49\u5148\u9a8c\u9a71\u52a8\u53d8\u5316\u63a8\u65ad\uff0c\u5c06\u65e0\u76d1\u7763\u4efb\u52a1\u8f6c\u5316\u4e3a\u53ef\u63a7\u7684\u5f31\u76d1\u7763\u8def\u5f84\u4f18\u5316\u3002", "result": "\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniCD\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5728\u5f31\u76d1\u7763\u548c\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0c\u5728LEVIR-CD\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8d85\u8fc7\u5f53\u524d\u6700\u4f73\u65b9\u6cd512.72%\u548c12.37%\u3002", "conclusion": "UniCD\u901a\u8fc7\u6d88\u9664\u67b6\u6784\u969c\u788d\u548c\u6df1\u5ea6\u8026\u5408\u5f02\u6784\u76d1\u7763\u4fe1\u53f7\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u9002\u5e94\u4e0d\u540c\u6807\u6ce8\u53ef\u7528\u6027\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.17761", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17761", "abs": "https://arxiv.org/abs/2601.17761", "authors": ["Dongjie Cheng", "Ruifeng Yuan", "Yongqi Li", "Runyang You", "Wenjie Wang", "Liqiang Nie", "Lei Zhang", "Wenjie Li"], "title": "AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation", "comment": null, "summary": "Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of \"Omni\" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.", "AI": {"tldr": "AR-Omni\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4efb\u610f\u5230\u4efb\u610f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u4f7f\u7528\u5355\u4e00Transformer\u89e3\u7801\u5668\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c\u8bed\u97f3\u7684\u751f\u6210\uff0c\u65e0\u9700\u4e13\u5bb6\u89e3\u7801\u5668\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u611f\u77e5\u548c\u4ea4\u4e92\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\uff0c\u6db5\u76d6\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u8bed\u97f3\u3002\u73b0\u6709\u7684\u5927\u591a\u6570\"\u5168\u80fd\"MLLM\u7cfb\u7edf\u4ecd\u4f9d\u8d56\u989d\u5916\u7684\u4e13\u5bb6\u7ec4\u4ef6\u6765\u5b9e\u73b0\u591a\u6a21\u6001\u751f\u6210\uff0c\u9650\u5236\u4e86\u7edf\u4e00\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u7b80\u6d01\u6027\u3002\u81ea\u56de\u5f52\u5efa\u6a21\u5728\u6587\u672c\u9886\u57df\u5df2\u88ab\u8bc1\u660e\u662f\u4f18\u96c5\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u56e0\u6b64\u7814\u7a76\u8005\u5e0c\u671b\u5c06\u5176\u6269\u5c55\u5230\u591a\u6a21\u6001\u9886\u57df\u3002", "method": "\u63d0\u51faAR-Omni\u6a21\u578b\uff0c\u91c7\u7528\u5355\u4e00Transformer\u89e3\u7801\u5668\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c\u6d41\u5f0f\u8bed\u97f3\u7684\u751f\u6210\u3002\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u635f\u5931\u91cd\u52a0\u6743\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff1b\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684token\u7ea7\u611f\u77e5\u5bf9\u9f50\u635f\u5931\u63d0\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\uff1b\u901a\u8fc7\u6709\u9650\u72b6\u6001\u89e3\u7801\u673a\u5236\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u521b\u9020\u6027\u3002", "result": "AR-Omni\u5728\u4e09\u79cd\u6a21\u6001\u4e0a\u90fd\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\uff0c\u8bed\u97f3\u751f\u6210\u7684\u5b9e\u65f6\u56e0\u5b50\u8fbe\u52300.88\u3002", "conclusion": "AR-Omni\u8bc1\u660e\u4e86\u81ea\u56de\u5f52\u8303\u5f0f\u53ef\u4ee5\u6709\u6548\u5730\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\uff0c\u65e0\u9700\u4e13\u5bb6\u89e3\u7801\u5668\uff0c\u4e3a\u6784\u5efa\u7b80\u6d01\u800c\u5f3a\u5927\u7684\u4efb\u610f\u5230\u4efb\u610f\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.17756", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.17756", "abs": "https://arxiv.org/abs/2601.17756", "authors": ["Ziyang Song", "Xinyu Gong", "Bangya Liu", "Zelin Zhao"], "title": "MV-S2V: Multi-View Subject-Consistent Video Generation", "comment": "13 pages, 9 figures", "summary": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href=\"https://szy-young.github.io/mv-s2v\">this URL</a>", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4e3b\u9898\u5230\u89c6\u9891\u751f\u6210\uff08MV-S2V\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u53c2\u8003\u56fe\u50cf\u5b9e\u73b03D\u7ea7\u522b\u7684\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709S2V\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u89c6\u89d2\u53c2\u8003\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u4e3b\u9898\u5230\u89c6\u9891\u751f\u6210\uff08S2V\uff09\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u89c6\u89d2\u4e3b\u9898\u53c2\u8003\uff0c\u8fd9\u4f7f\u5f97S2V\u4efb\u52a1\u7b80\u5316\u4e3aS2I + I2V\u6d41\u6c34\u7ebf\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u9891\u4e3b\u9898\u63a7\u5236\u7684\u5168\u90e8\u6f5c\u529b\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u591a\u89c6\u89d2\u53c2\u8003\u56fe\u50cf\u5b9e\u73b03D\u7ea7\u522b\u4e3b\u9898\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u591a\u89c6\u89d2S2V\uff08MV-S2V\uff09\u4efb\u52a1\u6846\u67b6\uff1b2. \u5f00\u53d1\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\u4ee5\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u8f85\u4ee5\u5c0f\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff1b3. \u5f15\u5165\u65f6\u95f4\u504f\u79fbRoPE\uff08TS-RoPE\uff09\u6765\u533a\u5206\u4e0d\u540c\u4e3b\u9898\u548c\u540c\u4e00\u4e3b\u9898\u7684\u4e0d\u540c\u89c6\u89d2\u53c2\u8003\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u89c6\u89d2\u53c2\u8003\u56fe\u50cf\u65b9\u9762\u5b9e\u73b0\u4e86\u4f18\u8d8a\u76843D\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u8f93\u51fa\uff0c\u4e3a\u4e3b\u9898\u9a71\u52a8\u7684\u89c6\u9891\u751f\u6210\u5efa\u7acb\u4e86\u65b0\u7684\u6709\u610f\u4e49\u65b9\u5411\u3002", "conclusion": "MV-S2V\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u4e3b\u9898\u5230\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548cTS-RoPE\u6280\u672f\u5b9e\u73b0\u4e863D\u7ea7\u522b\u7684\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u4e3a\u4e3b\u9898\u9a71\u52a8\u7684\u89c6\u9891\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.17768", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17768", "abs": "https://arxiv.org/abs/2601.17768", "authors": ["Raja Gond", "Aditya K Kamath", "Arkaprava Basu", "Ramachandran Ramjee", "Ashish Panwar"], "title": "LLM-42: Enabling Determinism in LLM Inference with Verified Speculation", "comment": "https://github.com/microsoft/llm-42", "summary": "In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.\n  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.", "AI": {"tldr": "LLM-42\uff1a\u4e00\u79cd\u57fa\u4e8e\u8c03\u5ea6\u7684\u786e\u5b9a\u6027LLM\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9a8c\u8bc1-\u56de\u6eda\u5faa\u73af\u5728\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u7684\u540c\u65f6\u786e\u4fdd\u8f93\u51fa\u786e\u5b9a\u6027", "motivation": "LLM\u63a8\u7406\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u6e90\u4e8e\u6d6e\u70b9\u6570\u975e\u7ed3\u5408\u6027\u4e0e\u52a8\u6001\u6279\u5904\u7406\u53caGPU\u6838\u7684\u4ea4\u4e92\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4e25\u91cd\u964d\u4f4e\u541e\u5410\u91cf\uff0c\u8981\u4e48\u9700\u8981\u91cd\u65b0\u5b9e\u73b0\u6838\u51fd\u6570\u5e76\u5e26\u6765\u56fa\u5b9a\u5f00\u9500", "method": "\u53d7\u63a8\u6d4b\u89e3\u7801\u542f\u53d1\uff0c\u63d0\u51fa\u8c03\u5ea6\u65b9\u6cd5LLM-42\uff1a\u4f7f\u7528\u975e\u786e\u5b9a\u6027\u5feb\u901f\u8def\u5f84\u89e3\u7801\u4ee4\u724c\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9a8c\u8bc1-\u56de\u6eda\u5faa\u73af\u5f3a\u5236\u6267\u884c\u786e\u5b9a\u6027\u3002\u9a8c\u8bc1\u5668\u5728\u56fa\u5b9a\u5f62\u72b6\u5f52\u7ea6\u8c03\u5ea6\u4e0b\u91cd\u653e\u5019\u9009\u4ee4\u724c\uff0c\u63d0\u4ea4\u4fdd\u8bc1\u8de8\u8fd0\u884c\u4e00\u81f4\u7684\u4ee4\u724c\uff0c\u56de\u6eda\u8fdd\u53cd\u786e\u5b9a\u6027\u7684\u4ee4\u724c", "result": "LLM-42\u4e3b\u8981\u91cd\u7528\u73b0\u6709\u6838\u51fd\u6570\u4e0d\u53d8\uff0c\u4ec5\u5bf9\u9700\u8981\u786e\u5b9a\u6027\u7684\u6d41\u91cf\u6309\u6bd4\u4f8b\u4ea7\u751f\u5f00\u9500\uff0c\u5728\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u7684\u540c\u65f6\u786e\u4fdd\u8f93\u51fa\u786e\u5b9a\u6027", "conclusion": "LLM-42\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8c03\u5ea6\u65b9\u6cd5\uff0c\u5728\u4e0d\u727a\u7272\u541e\u5410\u91cf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0LLM\u63a8\u7406\u7684\u786e\u5b9a\u6027\uff0c\u907f\u514d\u4e86\u4e0e\u6838\u51fd\u6570\u8bbe\u8ba1\u7684\u7d27\u5bc6\u8026\u5408\u548c\u56fa\u5b9a\u8fd0\u884c\u65f6\u5f00\u9500"}}
{"id": "2601.17791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17791", "abs": "https://arxiv.org/abs/2601.17791", "authors": ["Rabin Dulal", "Wenfeng Jia", "Lihong Zheng", "Jane Quinn"], "title": "Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation", "comment": null, "summary": "Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\\pm$ 0.10, MAPE = 2.22 $\\pm$ 0.56 \\%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.", "AI": {"tldr": "\u4f7f\u7528\u591a\u89c6\u89d2RGB\u56fe\u50cf\u548cSAM 3D\u91cd\u5efa\u6280\u672f\uff0c\u7ed3\u5408\u96c6\u6210\u56de\u5f52\u6a21\u578b\uff0c\u5b9e\u73b0\u725b\u53ea\u6d3b\u91cd\u7684\u975e\u63a5\u89e6\u5f0f\u4f4e\u6210\u672c\u4f30\u7b97\uff0c\u5728\u519c\u573a\u573a\u666f\u4e0b\u8fbe\u5230R\u00b2=0.69\u7684\u51c6\u786e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u725b\u53ea\u6d3b\u91cd\u4f30\u7b97\u65b9\u6cd5\uff08\u5982\u8d70\u8fc7\u5f0f\u79f0\u91cd\u7cfb\u7edf\u6216\u4f53\u51b5\u8bc4\u5206\uff09\u9700\u8981\u4eba\u5de5\u64cd\u4f5c\uff0c\u5f71\u54cd\u751f\u4ea7\u6548\u7387\u548c\u7ecf\u6d4e\u6548\u76ca\uff0c\u9700\u8981\u5f00\u53d1\u975e\u63a5\u89e6\u5f0f\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u89c6\u89d2RGB\u56fe\u50cf\u76843D\u91cd\u5efa\u7ba1\u9053\uff1a\u4f7f\u7528SAM 3D\u8fdb\u884c\u57fa\u4e8e\u4e00\u81f4\u6027\u5f15\u5bfc\u7684\u591a\u89c6\u89d2\u878d\u5408\u751f\u6210\u5355\u4e00\u70b9\u4e91\uff0c\u7136\u540e\u6bd4\u8f83\u7ecf\u5178\u96c6\u6210\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4f4e\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "SAM 3D\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u878d\u5408\u4f18\u4e8e\u5176\u4ed63D\u751f\u6210\u65b9\u6cd5\uff1b\u7ecf\u5178\u96c6\u6210\u6a21\u578b\u5728\u519c\u573a\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u6700\u7a33\u5b9a\uff08R\u00b2=0.69\u00b10.10\uff0cMAPE=2.22\u00b10.56%\uff09\uff0c\u9002\u5408\u519c\u573a\u90e8\u7f72\u3002", "conclusion": "\u5bf9\u4e8e\u5728\u96be\u4ee5\u4ea7\u751f\u5927\u91cf3D\u6570\u636e\u7684\u519c\u573a\u73af\u5883\u4e2d\u8fdb\u884c\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u6bd4\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u66f4\u4e3a\u5173\u952e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u519c\u573a\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.17818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17818", "abs": "https://arxiv.org/abs/2601.17818", "authors": ["Wen Luo", "Peng Chen", "Xiaotao Huang", "LiQun Huang"], "title": "ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning", "comment": null, "summary": "Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.", "AI": {"tldr": "ViTCoP\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u7f16\u7801\u5668\u5197\u4f59\u8fc7\u6ee4\u548c\u57fa\u4e8eLLM\u5c42\u6b21\u7279\u6027\u7684\u9010\u6b65\u534f\u540c\u526a\u679d\uff0c\u6709\u6548\u51cf\u5c11\u89c6\u89c9\u4ee4\u724c\u5197\u4f59\uff0c\u540c\u65f6\u4fdd\u6301\u5173\u952e\u89c6\u89c9\u4fe1\u606f\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u89c6\u89c9\u4ee4\u724c\u5197\u4f59\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u8981\u4e48\u5728\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u8fc7\u65e9\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff0c\u8981\u4e48\u5728LLM\u4e2d\u5bfc\u81f4\u6240\u9009\u4ee4\u724c\u4fe1\u606f\u5197\u4f59\u3002", "method": "\u63d0\u51faViTCoP\u6846\u67b6\uff1a1) \u5728\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u8fdb\u884c\u5197\u4f59\u8fc7\u6ee4\uff1b2) \u57fa\u4e8eLLM\u5c42\u6b21\u7279\u6027\u8fdb\u884c\u9010\u6b65\u534f\u540c\u526a\u679d\uff1b3) \u5f15\u5165K\u5411\u91cfL2\u8303\u6570\u4f5c\u4e3a\u4ee4\u724c\u663e\u8457\u6027\u5ea6\u91cf\uff0c\u786e\u4fdd\u4e0eFlashAttention\u7b49\u52a0\u901f\u6280\u672f\u517c\u5bb9\u3002", "result": "\u5728\u5404\u79cd\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cViTCoP\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548cGPU\u5185\u5b58\u6d88\u8017\uff0c\u5728\u6781\u7aef\u526a\u679d\u7387\u4e0b\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "ViTCoP\u901a\u8fc7\u89c6\u89c9\u548c\u6587\u672c\u8bed\u4e49\u534f\u540c\u526a\u679d\uff0c\u6709\u6548\u89e3\u51b3\u4e86LVLM\u4e2d\u7684\u89c6\u89c9\u4ee4\u724c\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u9ad8\u526a\u679d\u7387\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2601.17830", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17830", "abs": "https://arxiv.org/abs/2601.17830", "authors": ["Mengmeng Wang", "Dengyang Jiang", "Liuzhuozheng Li", "Yucheng Lin", "Guojiang Shen", "Xiangjie Kong", "Yong Liu", "Guang Dai", "Jingdong Wang"], "title": "VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training", "comment": null, "summary": "Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \\textbf{\\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \\name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \\name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \\name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\\% extra GFLOPs with zero additional cost for external guidance models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5185\u5728\u6307\u5bfc\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3VAE\u7279\u5f81\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u8bad\u7ec3\uff0c\u65e0\u9700\u5916\u90e8\u4f9d\u8d56\uff0c\u4ec5\u589e\u52a04%\u8ba1\u7b97\u5f00\u9500", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u53d8\u6362\u5668\u8bad\u7ec3\u6536\u655b\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5982REPA\uff08\u4f9d\u8d56\u5916\u90e8\u8868\u793a\u7f16\u7801\u5668\uff09\u548cSRA\uff08\u9700\u8981\u53cc\u6a21\u578b\u8bbe\u7f6e\uff09\u90fd\u4f1a\u5e26\u6765\u6c89\u91cd\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5185\u5728\u6307\u5bfc\u6846\u67b6\u6765\u9ad8\u6548\u52a0\u901f\u6269\u6563\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\\namex\u6846\u67b6\uff0c\u5229\u7528\u73b0\u6210\u7684\u9884\u8bad\u7ec3\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7279\u5f81\u3002VAE\u7684\u91cd\u5efa\u7279\u6027\u786e\u4fdd\u4e86\u5176\u5bf9\u4e30\u5bcc\u7eb9\u7406\u7ec6\u8282\u3001\u7ed3\u6784\u6a21\u5f0f\u548c\u57fa\u672c\u8bed\u4e49\u4fe1\u606f\u7b49\u89c6\u89c9\u5148\u9a8c\u7684\u5185\u5728\u7f16\u7801\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u5c42\u5c06\u6269\u6563\u53d8\u6362\u5668\u7684\u4e2d\u95f4\u6f5c\u5728\u7279\u5f81\u4e0eVAE\u7279\u5f81\u5bf9\u9f50\uff0c\u5e76\u4f7f\u7528\u7279\u5f81\u5bf9\u9f50\u635f\u5931\u8fdb\u884c\u76d1\u7763\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\\namex\u76f8\u6bd4\u539f\u59cb\u6269\u6563\u53d8\u6362\u5668\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\uff0c\u5339\u914d\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u4ec5\u589e\u52a04%\u7684GFLOPs\u8ba1\u7b97\u5f00\u9500\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u6307\u5bfc\u6a21\u578b\u7684\u989d\u5916\u6210\u672c\u3002", "conclusion": "\\namex\u662f\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u5185\u5728\u6307\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3VAE\u7279\u5f81\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u8bad\u7ec3\uff0c\u65e0\u9700\u5916\u90e8\u4f9d\u8d56\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u63d0\u5347\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.17835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17835", "abs": "https://arxiv.org/abs/2601.17835", "authors": ["Baowen Zhang", "Chenxing Jiang", "Heng Li", "Shaojie Shen", "Ping Tan"], "title": "Geometry-Grounded Gaussian Splatting", "comment": "16 pages, 15 figures", "summary": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u56fa\u4f53\u7406\u8bba\u7684\u51e0\u4f55\u57fa\u7840\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4ece\u9ad8\u65af\u539f\u8bed\u4e2d\u63d0\u53d6\u5f62\u72b6\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5f62\u72b6\u91cd\u5efa\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ece\u9ad8\u65af\u539f\u8bed\u4e2d\u63d0\u53d6\u5f62\u72b6\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u51e0\u4f55\u53c2\u6570\u5316\u548c\u8fd1\u4f3c\u4e0d\u8db3\uff0c\u5b58\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u5dee\u548c\u5bf9\u6d6e\u70b9\u566a\u58f0\u654f\u611f\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u7406\u8bba\u63a8\u5bfc\uff0c\u5c06\u9ad8\u65af\u539f\u8bed\u5efa\u7acb\u4e3a\u4e00\u79cd\u7279\u5b9a\u7c7b\u578b\u7684\u968f\u673a\u56fa\u4f53\uff0c\u4e3a\u51e0\u4f55\u57fa\u7840\u9ad8\u65af\u6cfc\u6e85\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\u3002\u5229\u7528\u968f\u673a\u56fa\u4f53\u7684\u4f53\u79ef\u6027\u8d28\uff0c\u9ad8\u6548\u6e32\u67d3\u9ad8\u8d28\u91cf\u6df1\u5ea6\u56fe\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u51e0\u4f55\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6240\u6709\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4e2d\u6700\u597d\u7684\u5f62\u72b6\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9ad8\u65af\u539f\u8bed\u7406\u8bba\u5316\u4e3a\u968f\u673a\u56fa\u4f53\uff0c\u4e3a\u51e0\u4f55\u57fa\u7840\u9ad8\u65af\u6cfc\u6e85\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5f62\u72b6\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.17883", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17883", "abs": "https://arxiv.org/abs/2601.17883", "authors": ["Dingkun Liu", "Yuheng Chen", "Zhu Chen", "Zhenyao Cui", "Yaozhi Wen", "Jiayu An", "Jingwei Luo", "Dongrui Wu"], "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems", "comment": null, "summary": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u73b0\u6709EEG\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u53d1\u73b0\u7ebf\u6027\u63a2\u6d4b\u901a\u5e38\u4e0d\u8db3\u3001\u4e13\u7528\u6a21\u578b\u4ecd\u5177\u7ade\u4e89\u529b\u3001\u66f4\u5927\u57fa\u7840\u6a21\u578b\u4e0d\u4e00\u5b9a\u5e26\u6765\u66f4\u597d\u6cdb\u5316\u6027\u80fd", "motivation": "EEG\u57fa\u7840\u6a21\u578b\u5728\u8111\u673a\u63a5\u53e3\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u7531\u4e8e\u9884\u8bad\u7ec3\u76ee\u6807\u3001\u9884\u5904\u7406\u9009\u62e9\u548c\u4e0b\u6e38\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u516c\u5e73\u5168\u9762\u7684\u6bd4\u8f83\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9996\u5148\u56de\u987e50\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u5c06\u5176\u8bbe\u8ba1\u9009\u62e9\u7ec4\u7ec7\u4e3a\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff1b\u7136\u540e\u8bc4\u4f3012\u4e2a\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u548c\u7ade\u4e89\u6027\u4e13\u7528\u57fa\u7ebf\uff0c\u6db5\u76d613\u4e2aEEG\u6570\u636e\u96c6\u548c9\u4e2aBCI\u8303\u5f0f\uff1b\u8003\u8651\u8de8\u88ab\u8bd5\u6cdb\u5316\u548c\u5c11\u6837\u672c\u6821\u51c6\uff1b\u6bd4\u8f83\u5168\u53c2\u6570\u5fae\u8c03\u4e0e\u7ebf\u6027\u63a2\u6d4b\uff1b\u5206\u6790\u6a21\u578b\u89c4\u6a21\u4e0e\u6027\u80fd\u5173\u7cfb\u3002", "result": "1) \u7ebf\u6027\u63a2\u6d4b\u901a\u5e38\u4e0d\u8db3\uff1b2) \u4ece\u5934\u8bad\u7ec3\u7684\u4e13\u7528\u6a21\u578b\u5728\u8bb8\u591a\u4efb\u52a1\u4e2d\u4ecd\u5177\u7ade\u4e89\u529b\uff1b3) \u5728\u5f53\u524d\u6570\u636e\u673a\u5236\u548c\u8bad\u7ec3\u5b9e\u8df5\u4e0b\uff0c\u66f4\u5927\u7684\u57fa\u7840\u6a21\u578b\u4e0d\u4e00\u5b9a\u4ea7\u751f\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "EEG\u57fa\u7840\u6a21\u578b\u9886\u57df\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5f53\u524d\u6a21\u578b\u5728\u771f\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u4f18\u52bf\u6709\u9650\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u66f4\u6709\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2601.17857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17857", "abs": "https://arxiv.org/abs/2601.17857", "authors": ["Lan Yang", "Minghan Yang", "Ke Li", "Honggang Zhang", "Kaiyue Pang", "Yi-Zhe Song"], "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction", "comment": null, "summary": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faSynMind\u6846\u67b6\uff0c\u901a\u8fc7\u5c06fMRI\u4fe1\u53f7\u89e3\u6790\u4e3a\u53e5\u5b50\u7ea7\u8bed\u4e49\u63cf\u8ff0\uff0c\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u6765\u6539\u8fdb\u56fe\u50cf\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u8bed\u4e49\u9519\u4f4d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709fMRI\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u867d\u7136\u80fd\u4ea7\u751f\u903c\u771f\u56fe\u50cf\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\u2014\u2014\u663e\u8457\u7269\u4f53\u7ecf\u5e38\u88ab\u66ff\u6362\u6216\u5e7b\u89c9\u751f\u6210\uff0c\u5c3d\u7ba1\u89c6\u89c9\u8d28\u91cf\u5f88\u9ad8\u3002\u4f5c\u8005\u8ba4\u4e3a\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u7ea0\u7f20\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u4f18\u5148\u8003\u8651\u4f4e\u7ea7\u522b\u5916\u89c2\u7ebf\u7d22\uff08\u5982\u7eb9\u7406\u548c\u5168\u5c40\u6982\u8c8c\uff09\u800c\u975e\u660e\u786e\u7684\u8bed\u4e49\u8eab\u4efd\u3002", "method": "\u63d0\u51faSynMind\u6846\u67b6\uff1a1\uff09\u5c06fMRI\u4fe1\u53f7\u89e3\u6790\u4e3a\u4e30\u5bcc\u7684\u53e5\u5b50\u7ea7\u8bed\u4e49\u63cf\u8ff0\uff0c\u53cd\u6620\u4eba\u7c7b\u89c6\u89c9\u7406\u89e3\u7684\u5206\u5c42\u548c\u7ec4\u5408\u7279\u6027\uff1b2\uff09\u5229\u7528\u57fa\u7840\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7c7b\u4f3c\u4eba\u7c7b\u7684\u3001\u591a\u7c92\u5ea6\u7684\u6587\u672c\u8868\u793a\uff0c\u6355\u6349\u7269\u4f53\u8eab\u4efd\u548c\u7a7a\u95f4\u7ec4\u7ec7\uff1b3\uff09\u5c06\u8fd9\u4e9b\u660e\u786e\u7684\u8bed\u4e49\u7f16\u7801\u4e0e\u89c6\u89c9\u5148\u9a8c\u7ed3\u5408\uff0c\u6761\u4ef6\u5316\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u3002", "result": "SynMind\u5728\u5927\u591a\u6570\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u8bed\u4e49\u63a8\u7406\u5378\u8f7d\u5230\u6587\u672c\u5bf9\u9f50\u6a21\u5757\uff0cSynMind\u4f7f\u7528\u66f4\u5c0f\u7684Stable Diffusion 1.4\u548c\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u5c31\u80fd\u8d85\u8d8a\u57fa\u4e8eSDXL\u7684\u7ade\u4e89\u65b9\u6cd5\u3002\u5927\u89c4\u6a21\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u5b9eSynMind\u4ea7\u751f\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u66f4\u4e00\u81f4\u7684\u91cd\u5efa\u7ed3\u679c\u3002\u795e\u7ecf\u53ef\u89c6\u5316\u5206\u6790\u663e\u793aSynMind\u6fc0\u6d3b\u4e86\u66f4\u5e7f\u6cdb\u3001\u8bed\u4e49\u66f4\u76f8\u5173\u7684\u5927\u8111\u533a\u57df\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u601d\u8003fMRI\u89e3\u7801\u4e2d\u660e\u786e\u8bed\u4e49\u89e3\u91ca\u7684\u4f5c\u7528\uff0cSynMind\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7b26\u5408\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7684\u56fe\u50cf\u91cd\u5efa\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u548c\u66f4\u5e7f\u6cdb\u7684\u795e\u7ecf\u6fc0\u6d3b\u6a21\u5f0f\u3002"}}
{"id": "2601.17910", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17910", "abs": "https://arxiv.org/abs/2601.17910", "authors": ["Aaron R. Flouro", "Shawn P. Chadwick"], "title": "Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization", "comment": "12 pages, 1 figure, 1 table", "summary": "Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b97\u5b50\u65e0\u5173\u7684\u516c\u7406\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\uff0c\u6db5\u76d6token\u3001task\u548ccontext\u4e09\u4e2a\u4e92\u8865\u5c3a\u5ea6\uff0c\u5c06\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5177\u4f53\u6743\u91cd\u516c\u5f0f\u89e3\u8026\u3002", "motivation": "\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u867d\u7136\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\u3001\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u5b9e\u73b0\u7279\u5b9a\u7684\u6743\u91cd\u65b9\u6848\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b97\u5b50\u65e0\u5173\u7684\u516c\u7406\u5316\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u4e86\u81ea\u9002\u5e94\u6743\u91cd\u7b97\u5b50\u7684\u7ed3\u6784\u6761\u4ef6\uff0c\u5305\u62ec\u826f\u597d\u5b9a\u4e49\u6027\u3001\u591a\u91cd\u975e\u7b49\u4ef7\u5b9e\u73b0\u53ef\u80fd\u6027\u3001\u901a\u8fc7\u4e58\u79ef\u7ed3\u6784\u5f52\u4e00\u5316\u7684\u5c42\u6b21\u7ec4\u5408\uff0c\u5e76\u5206\u6790\u4e86\u6536\u655b\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6270\u52a8\u9c81\u68d2\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u7b26\u5408\u6761\u4ef6\u7b97\u5b50\u7684\u5b58\u5728\u6027\u548c\u975e\u552f\u4e00\u6027\uff0c\u5728\u6807\u51c6\u5047\u8bbe\u4e0b\u5206\u6790\u4e86\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u7684\u6536\u655b\u6027\uff0c\u63d0\u4f9b\u4e86\u5b89\u5168\u7ea6\u675f\u84b8\u998f\u7684\u62bd\u8c61\u8868\u8ff0\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5177\u4f53\u6743\u91cd\u516c\u5f0f\u7684\u89e3\u8026\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f02\u6784\u6027\u3001\u5206\u5e03\u504f\u79fb\u548c\u5b89\u5168\u7ea6\u675f\u4e0b\u7684\u81ea\u9002\u5e94\u84b8\u998f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u5206\u6790\u57fa\u7840\uff0c\u4f7f\u7406\u8bba\u5206\u6790\u4e0d\u518d\u4f9d\u8d56\u4e8e\u5177\u4f53\u7684\u6743\u91cd\u8ba1\u7b97\u516c\u5f0f\u3002"}}
{"id": "2601.17862", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17862", "abs": "https://arxiv.org/abs/2601.17862", "authors": ["Jingsong Xia", "Siqi Wang"], "title": "Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment", "comment": null, "summary": "Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u91cf\u5b50\u589e\u5f3a\u57df\u6cdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u57df\u6210\u50cf\u504f\u79fb\u6a21\u62df\u3001\u57df\u5bf9\u6297\u8bad\u7ec3\u548c\u91cf\u5b50\u7279\u5f81\u589e\u5f3a\u5c42\uff0c\u63d0\u5347\u533b\u5b66\u5f71\u50cfAI\u6a21\u578b\u5728\u672a\u89c1\u76ee\u6807\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u771f\u5b9e\u591a\u4e2d\u5fc3\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u533b\u5b66\u5f71\u50cfAI\u6a21\u578b\u5728\u5355\u4e2d\u5fc3\u6216\u5355\u8bbe\u5907\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u8de8\u4e2d\u5fc3\u90e8\u7f72\u65f6\u56e0\u57df\u504f\u79fb\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u89e3\u51b3\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u540c\u65f6\u8003\u8651\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u3002", "method": "1) \u57fa\u4e8eMobileNetV2\u6784\u5efa\u57df\u4e0d\u53d8\u7f16\u7801\u5668\uff1b2) \u4f7f\u7528\u4eae\u5ea6\u3001\u5bf9\u6bd4\u5ea6\u3001\u9510\u5316\u548c\u566a\u58f0\u6270\u52a8\u6a21\u62df\u591a\u57df\u6210\u50cf\u504f\u79fb\uff1b3) \u91c7\u7528\u68af\u5ea6\u53cd\u8f6c\u7684\u57df\u5bf9\u6297\u8bad\u7ec3\u6291\u5236\u57df\u5224\u522b\u7279\u5f81\uff1b4) \u5f15\u5165\u8f7b\u91cf\u7ea7\u91cf\u5b50\u7279\u5f81\u589e\u5f3a\u5c42\uff0c\u4f7f\u7528\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u8fdb\u884c\u975e\u7ebf\u6027\u7279\u5f81\u6620\u5c04\u548c\u7ea0\u7f20\u5efa\u6a21\uff1b5) \u63a8\u7406\u65f6\u91c7\u7528\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u591a\u4e2d\u5fc3\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u57df\u4e0a\u663e\u8457\u4f18\u4e8e\u65e0\u57df\u6cdb\u5316\u6216\u65e0\u91cf\u5b50\u589e\u5f3a\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u964d\u4f4e\u4e86\u57df\u7279\u5b9a\u6027\u80fd\u65b9\u5dee\uff0c\u63d0\u9ad8\u4e86AUC\u548c\u7075\u654f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u91cf\u5b50\u589e\u5f3a\u57df\u6cdb\u5316\u7684\u4e34\u5e8a\u6f5c\u529b\uff0c\u4e3a\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u533b\u5b66\u5f71\u50cf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u771f\u5b9e\u591a\u4e2d\u5fc3\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684\u8de8\u57df\u6cdb\u5316\u3002"}}
{"id": "2601.17912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17912", "abs": "https://arxiv.org/abs/2601.17912", "authors": ["Qinyi Liu", "Mohammad Khalil", "Naman Goel"], "title": "Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN", "comment": null, "summary": "Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.", "AI": {"tldr": "TabPFN\u7b49\u8868\u683c\u6570\u636e\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u56e0\u679c\u9884\u8bad\u7ec3\u83b7\u5f97\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f46\u5176\u516c\u5e73\u6027\u8868\u73b0\u6709\u9650\uff0c\u7279\u522b\u662f\u5728MNAR\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\uff0c\u9700\u8981\u989d\u5916\u7684\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u3002", "motivation": "\u5c3d\u7ba1TabPFN\u7b49\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u9884\u8bad\u7ec3\u7684\u8868\u683c\u6570\u636e\u57fa\u7840\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u516c\u5e73\u6027\u7279\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u516c\u5e73\u6027\u8868\u73b0\u3002", "method": "\u5bf9TabPFN\u53ca\u5176\u5fae\u8c03\u53d8\u4f53\u8fdb\u884c\u5168\u9762\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5305\u62ec\u9884\u6d4b\u6027\u80fd\u3001\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\uff0c\u8003\u5bdf\u4e0d\u540c\u6570\u636e\u96c6\u5927\u5c0f\u548c\u5206\u5e03\u504f\u79fb\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "TabPFN\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5bf9\u865a\u5047\u76f8\u5173\u6027\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5728\u516c\u5e73\u6027\u65b9\u9762\u7684\u6539\u8fdb\u6709\u9650\u4e14\u4e0d\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728MNAR\u534f\u53d8\u91cf\u504f\u79fb\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "TabPFN\u7684\u56e0\u679c\u9884\u8bad\u7ec3\u867d\u7136\u6709\u76ca\u4f46\u4e0d\u8db3\u4ee5\u786e\u4fdd\u7b97\u6cd5\u516c\u5e73\u6027\uff0c\u5b9e\u9645\u90e8\u7f72\u4e2d\u9700\u8981\u989d\u5916\u7684\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.17916", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17916", "abs": "https://arxiv.org/abs/2601.17916", "authors": ["Jialu Tang", "Tong Xia", "Yuan Lu", "Aaqib Saeed"], "title": "UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR", "comment": "Accepted to IEEE ICASSP 2026", "summary": "Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.", "AI": {"tldr": "UniPACT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4e34\u5e8a\u9884\u540e\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5c06\u6570\u503c\u578b\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\uff0c\u5e76\u4e0e\u539f\u59cb\u5fc3\u7535\u56fe\u6ce2\u5f62\u8868\u793a\u878d\u5408\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8de8\u6a21\u6001\u63a8\u7406\uff0c\u5728MDS-ED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523089.37%\u7684\u5e73\u5747AUROC\u3002", "motivation": "\u51c6\u786e\u7684\u4e34\u5e8a\u9884\u540e\u9700\u8981\u7ed3\u5408\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u5b9e\u65f6\u751f\u7406\u4fe1\u53f7\uff08\u5982\u5fc3\u7535\u56fe\uff09\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u539f\u751f\u5904\u7406\u8fd9\u4e9b\u5f02\u6784\u7684\u975e\u6587\u672c\u6570\u636e\u7c7b\u578b\u3002", "method": "\u63d0\u51faUniPACT\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u7ed3\u6784\u5316\u63d0\u793a\u673a\u5236\uff0c\u5c06\u6570\u503c\u578bEHR\u6570\u636e\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6587\u672c\u5316\u7684\u60a3\u8005\u4e0a\u4e0b\u6587\u4e0e\u4ece\u539f\u59cbECG\u6ce2\u5f62\u5b66\u4e60\u7684\u8868\u793a\u878d\u5408\uff0c\u4f7fLLM\u80fd\u591f\u5bf9\u4e24\u79cd\u6a21\u6001\u8fdb\u884c\u6574\u4f53\u63a8\u7406\u3002", "result": "\u5728\u7efc\u5408MDS-ED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniPACT\u5728\u8bca\u65ad\u3001\u6076\u5316\u3001ICU\u5165\u9662\u548c\u6b7b\u4ea1\u7387\u7b49\u591a\u79cd\u9884\u540e\u4efb\u52a1\u4e0a\u8fbe\u523089.37%\u7684\u5e73\u5747AUROC\uff0c\u4f18\u4e8e\u4e13\u95e8\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u65b9\u6cd5\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u5728\u6570\u636e\u7f3a\u5931\u573a\u666f\u4e2d\u63d0\u4f9b\u9c81\u68d2\u6027\uff0cUniPACT\u6210\u529f\u5f25\u5408\u4e86\u4e34\u5e8a\u6570\u636e\u6a21\u6001\u95f4\u7684\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u9884\u540e\u95ee\u7b54\u3002"}}
{"id": "2601.17868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17868", "abs": "https://arxiv.org/abs/2601.17868", "authors": ["Zhihao He", "Tieyuan Chen", "Kangyu Wang", "Ziran Qin", "Yang Shao", "Chaofan Gan", "Shijie Li", "Zuxuan Wu", "Weiyao Lin"], "title": "VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding", "comment": null, "summary": "Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.", "AI": {"tldr": "VidLaDA\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891LLM\uff0c\u901a\u8fc7\u53cc\u5411\u6ce8\u610f\u529b\u6355\u83b7\u53cc\u5411\u4f9d\u8d56\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891LLM\u4e2d\u7684\u56e0\u679c\u63a9\u7801\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5f15\u5165MARS-Cache\u6846\u67b6\u52a0\u901f\u63a8\u7406", "motivation": "\u6807\u51c6\u81ea\u56de\u5f52\u89c6\u9891LLM\u4e0d\u53ef\u907f\u514d\u5730\u906d\u53d7\u56e0\u679c\u63a9\u7801\u504f\u5dee\u7684\u56f0\u6270\uff0c\u8fd9\u963b\u788d\u4e86\u5168\u5c40\u65f6\u7a7a\u5efa\u6a21\uff0c\u5bfc\u81f4\u7406\u89e3\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51faVidLaDA\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891LLM\uff0c\u5229\u7528\u53cc\u5411\u6ce8\u610f\u529b\u6355\u83b7\u53cc\u5411\u4f9d\u8d56\u5173\u7cfb\uff1b\u5f15\u5165MARS-Cache\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u89c6\u89c9\u7f13\u5b58\u5237\u65b0\u548c\u5e27\u7ea7\u5757\u6ce8\u610f\u529b\u52a0\u901f\u63a8\u7406\uff0c\u540c\u65f6\u901a\u8fc7\u951a\u70b9\u4ee4\u724c\u4fdd\u6301\u5168\u5c40\u8fde\u63a5\u6027", "result": "VidLaDA\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u6269\u6563\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff08\u5982Qwen2.5-VL\u548cLLaVA-Video\uff09\u76f8\u5ab2\u7f8e\uff0cMARS-Cache\u5b9e\u73b0\u4e86\u8d85\u8fc712\u500d\u7684\u52a0\u901f\u800c\u4e0d\u635f\u5bb3\u63a8\u7406\u51c6\u786e\u6027", "conclusion": "VidLaDA\u901a\u8fc7\u6269\u6563\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891LLM\u7684\u56e0\u679c\u63a9\u7801\u504f\u5dee\u95ee\u9898\uff0cMARS-Cache\u6846\u67b6\u663e\u8457\u52a0\u901f\u4e86\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.17917", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17917", "abs": "https://arxiv.org/abs/2601.17917", "authors": ["Zhongyu Xiao", "Zhiwei Hao", "Jianyuan Guo", "Yong Luo", "Jia Liu", "Jie Xu", "Han Hu"], "title": "treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding", "comment": "Tech report. Code is available at https://github.com/xiaoshideta/Streaming-dLLM", "summary": "Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.", "AI": {"tldr": "Streaming-dLLM\u662f\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u7ef4\u5ea6\u7684\u8870\u51cf\u5f15\u5bfc\u540e\u7f00\u5efa\u6a21\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u52a8\u6001\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7b56\u7565\uff0c\u663e\u8457\u52a0\u901f\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u6700\u9ad8\u53ef\u8fbe68.2\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u65b9\u6cd5\u5b58\u5728\u5185\u5728\u4f4e\u6548\u6027\uff1a\u7a7a\u95f4\u5197\u4f59\uff08\u5bf9\u4fe1\u606f\u7a00\u758f\u7684\u540e\u7f00\u533a\u57df\u8fdb\u884c\u5747\u5300\u5efa\u6a21\uff09\u548c\u65f6\u95f4\u4f4e\u6548\uff08\u5728\u6574\u4e2a\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5e94\u7528\u56fa\u5b9a\u7684\u53bb\u566a\u8c03\u5ea6\uff09\u3002", "method": "\u63d0\u51faStreaming-dLLM\u6846\u67b6\uff1a1\uff09\u7a7a\u95f4\u4e0a\uff0c\u901a\u8fc7\u8870\u51cf\u5f15\u5bfc\u540e\u7f00\u5efa\u6a21\u4fee\u526a\u5197\u4f59\u63a9\u7801\u6807\u8bb0\u6765\u8fd1\u4f3c\u5b8c\u6574\u4e0a\u4e0b\u6587\uff1b2\uff09\u65f6\u95f4\u4e0a\uff0c\u91c7\u7528\u5e26\u63d0\u524d\u9000\u51fa\u673a\u5236\u7684\u52a8\u6001\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7b56\u7565\uff0c\u5141\u8bb8\u6a21\u578b\u8df3\u8fc7\u5df2\u6536\u655b\u6807\u8bb0\u7684\u4e0d\u5fc5\u8981\u8fed\u4ee3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eStreaming-dLLM\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad868.2\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u89e3\u7801\u6548\u7387\u3002", "conclusion": "Streaming-dLLM\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5185\u5728\u4f4e\u6548\u95ee\u9898\uff0c\u4e3a\u6269\u6563\u89e3\u7801\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17880", "abs": "https://arxiv.org/abs/2601.17880", "authors": ["Muhammad Umar Salman", "Mohammad Areeb Qazi", "Mohammed Talha Alam"], "title": "Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran", "comment": "6 pages, 2 tables and 2 figures", "summary": "We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset", "AI": {"tldr": "Quran MD\u662f\u4e00\u4e2a\u5305\u542b\u6587\u672c\u3001\u8bed\u8a00\u5b66\u548c\u97f3\u9891\u7684\u591a\u6a21\u6001\u53e4\u5170\u7ecf\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7ecf\u6587\u548c\u5355\u8bcd\u7ea7\u522b\u7684\u963f\u62c9\u4f2f\u6587\u672c\u3001\u82f1\u6587\u7ffb\u8bd1\u3001\u97f3\u6807\u8f6c\u5199\u4ee5\u53ca32\u4f4d\u4e0d\u540c\u8bf5\u7ecf\u8005\u7684\u97f3\u9891\u5bf9\u9f50\u6570\u636e\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u591a\u6a21\u6001\u53e4\u5170\u7ecf\u6570\u636e\u96c6\uff0c\u6574\u5408\u6587\u672c\u548c\u97f3\u9891\u7ef4\u5ea6\uff0c\u4ee5\u652f\u6301\u53e4\u5170\u7ecf\u8bf5\u7ecf\u4f20\u7edf\u7684\u7814\u7a76\u548c\u8ba1\u7b97\u5206\u6790\u3002\u53e4\u5170\u7ecf\u5177\u6709\u4e30\u5bcc\u7684\u53e3\u4f20\u4f20\u7edf\uff0c\u4e0d\u540c\u8bf5\u7ecf\u8005\u7684\u98ce\u683c\u548c\u65b9\u8a00\u5dee\u5f02\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6570\u636e\u6536\u96c6\u3002", "method": "\u6570\u636e\u96c6\u5728\u7ecf\u6587\u7ea7\u522b\u63d0\u4f9b\u963f\u62c9\u4f2f\u539f\u6587\u3001\u82f1\u6587\u7ffb\u8bd1\u548c\u97f3\u6807\u8f6c\u5199\uff0c\u5e76\u5305\u542b32\u4f4d\u4e0d\u540c\u8bf5\u7ecf\u8005\u7684\u97f3\u9891\u3002\u5728\u5355\u8bcd\u7ea7\u522b\uff0c\u6bcf\u4e2a\u8bcd\u5143\u90fd\u914d\u6709\u963f\u62c9\u4f2f\u6587\u5b57\u3001\u82f1\u6587\u7ffb\u8bd1\u3001\u8f6c\u5199\u548c\u5bf9\u9f50\u7684\u97f3\u9891\u5f55\u97f3\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u53d1\u97f3\u3001\u97f3\u97f5\u548c\u8bed\u4e49\u5206\u6790\u3002", "result": "\u521b\u5efa\u4e86Quran MD\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u8d44\u6e90\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u3001\u8bed\u8a00\u5b66\u5206\u6790\u548c\u6570\u5b57\u4f0a\u65af\u5170\u7814\u7a76\u7b49\u591a\u79cd\u5e94\u7528\u3002\u6570\u636e\u96c6\u5df2\u5728Hugging Face\u5e73\u53f0\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u53e4\u5170\u7ecf\u8bf5\u7ecf\u7684\u8ba1\u7b97\u65b9\u6cd5\u7814\u7a76\u63d0\u4f9b\u4e86\u72ec\u7279\u8d44\u6e90\uff0c\u652f\u6301ASR\u3001tajweed\u68c0\u6d4b\u3001\u53e4\u5170\u7ecfTTS\u7b49\u4efb\u52a1\uff0c\u5e76\u4e3a\u591a\u6a21\u6001\u5d4c\u5165\u3001\u8bed\u4e49\u68c0\u7d22\u3001\u98ce\u683c\u8f6c\u6362\u548c\u4e2a\u6027\u5316\u8f85\u5bfc\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u65e2\u53ef\u7528\u4e8e\u7814\u7a76\u4e5f\u53ef\u7528\u4e8e\u793e\u533a\u5e94\u7528\u3002"}}
{"id": "2601.17933", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17933", "abs": "https://arxiv.org/abs/2601.17933", "authors": ["Laurent Caraffa"], "title": "Dissipative Learning: A Framework for Viable Adaptive Systems", "comment": "68 pages, 14 figures", "summary": "We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.\n  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.\n  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faBEDS\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u89c6\u4e3a\u5185\u5728\u8017\u6563\u8fc7\u7a0b\uff0c\u8bc1\u660eFisher-Rao\u6b63\u5219\u5316\u662f\u70ed\u529b\u5b66\u6700\u4f18\u7b56\u7565\uff0c\u7edf\u4e00\u4e86\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u91cd\u65b0\u5b9a\u4e49\u5b66\u4e60\u4e3a\u5728\u8017\u6563\u7ea6\u675f\u4e0b\u7ef4\u6301\u53ef\u884c\u4fe1\u5ff5\u72b6\u6001\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u7406\u8bba\u5c06\u9057\u5fd8\u548c\u6b63\u5219\u5316\u89c6\u4e3a\u542f\u53d1\u5f0f\u9644\u52a0\u7ec4\u4ef6\uff0c\u800c\u4f5c\u8005\u8ba4\u4e3a\u5b83\u4eec\u662f\u81ea\u9002\u5e94\u7cfb\u7edf\u7684\u7ed3\u6784\u8981\u6c42\u3002\u9700\u8981\u4ece\u4fe1\u606f\u7406\u8bba\u3001\u70ed\u529b\u5b66\u548c\u4fe1\u606f\u51e0\u4f55\u89d2\u5ea6\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u5b66\u4e60\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8017\u6563\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faBEDS\uff08\u8d1d\u53f6\u65af\u6d8c\u73b0\u8017\u6563\u7ed3\u6784\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u6761\u4ef6\u6700\u4f18\u6027\u5b9a\u7406\uff0c\u8bc1\u660eFisher-Rao\u6b63\u5219\u5316\u662f\u552f\u4e00\u70ed\u529b\u5b66\u6700\u4f18\u7b56\u7565\u3002\u8be5\u6846\u67b6\u5c06Ridge\u3001SIGReg\u3001EMA\u3001SAC\u7b49\u65b9\u6cd5\u7edf\u4e00\u4e3a\u5355\u4e00\u63a7\u5236\u65b9\u7a0b\u7684\u7279\u4f8b\u3002", "result": "\u8fc7\u62df\u5408\u5bf9\u5e94\u8fc7\u5ea6\u7ed3\u6676\u5316\uff0c\u707e\u96be\u6027\u9057\u5fd8\u53cd\u6620\u8017\u6563\u63a7\u5236\u4e0d\u8db3\u3002\u6846\u67b6\u533a\u5206BEDS\u53ef\u7ed3\u6676\u95ee\u9898\uff08\u4fe1\u5ff5\u6536\u655b\u5230\u7a33\u5b9a\u5e73\u8861\uff09\u548cBEDS\u53ef\u7ef4\u6301\u95ee\u9898\uff08\u9700\u8981\u6301\u7eed\u9002\u5e94\uff09\u3002\u6269\u5c55\u81f3\u6301\u7eed\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "conclusion": "\u5b66\u4e60\u5e94\u88ab\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u8017\u6563\u7ea6\u675f\u4e0b\u7ef4\u6301\u53ef\u884c\u4fe1\u5ff5\u72b6\u6001\u7684\u8fc7\u7a0b\u3002\u53ef\u884c\u6027\u3001\u9002\u5e94\u7a33\u5b9a\u6027\u548c\u6709\u9650\u8d44\u6e90\u4e0b\u7684\u7a33\u5b9a\u6027\u53d6\u4ee3\u6e10\u8fd1\u6700\u4f18\u6027\u6210\u4e3a\u4e3b\u8981\u6807\u51c6\uff0c\u4e3a\u9057\u5fd8\u3001\u6b63\u5219\u5316\u548c\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89c6\u89d2\u3002"}}
{"id": "2601.17885", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17885", "abs": "https://arxiv.org/abs/2601.17885", "authors": ["Qingyu Fan", "Zhaoxiang Li", "Yi Lu", "Wang Chen", "Qiu Shen", "Xiao-xiao Long", "Yinghao Cai", "Tao Lu", "Shuo Wang", "Xun Cao"], "title": "PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation", "comment": null, "summary": "Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.\n  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.\n  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.\n  Project website: https://peafowlvla.github.io/.", "AI": {"tldr": "PEAfowl\u662f\u4e00\u79cd\u7528\u4e8e\u53cc\u624b\u64cd\u4f5c\u7684\u611f\u77e5\u589e\u5f3a\u591a\u89c6\u89d2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u76843D\u8868\u793a\u548c\u8fed\u4ee3\u5f0f\u6307\u4ee4\u63a5\u5730\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6742\u4e71\u573a\u666f\u4e2d\u7684\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u53cc\u624b\u64cd\u4f5c\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u591a\u89c6\u89d2\u7279\u5f81\u901a\u8fc7\u89c6\u89d2\u65e0\u5173\u7684token\u62fc\u63a5\u878d\u5408\uff0c\u5bfc\u81f43D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u5f31\uff1b2) \u8bed\u8a00\u4f5c\u4e3a\u5168\u5c40\u6761\u4ef6\u6ce8\u5165\uff0c\u5bfc\u81f4\u6307\u4ee4\u63a5\u5730\u7c97\u7cd9\u3002\u8fd9\u4e9b\u9650\u5236\u4e86\u6a21\u578b\u5728\u906e\u6321\u3001\u89c6\u89d2\u548c\u573a\u666f\u53d8\u5316\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1) \u7a7a\u95f4\u63a8\u7406\uff1a\u9884\u6d4b\u6bcf\u4e2atoken\u7684\u6df1\u5ea6\u5206\u5e03\uff0c\u8fdb\u884c\u53ef\u5fae\u5206\u76843D\u63d0\u5347\uff0c\u805a\u5408\u5c40\u90e8\u8de8\u89c6\u89d2\u90bb\u5c45\u5f62\u6210\u51e0\u4f55\u57fa\u7840\u3001\u8de8\u89c6\u89d2\u4e00\u81f4\u7684\u8868\u793a\uff1b2) \u6307\u4ee4\u63a5\u5730\uff1a\u7528Perceiver\u98ce\u683c\u7684\u6587\u672c\u611f\u77e5\u8bfb\u53d6\u673a\u5236\u66ff\u4ee3\u5168\u5c40\u6761\u4ef6\uff0c\u5728\u51bb\u7ed3\u7684CLIP\u89c6\u89c9\u7279\u5f81\u4e0a\u8fdb\u884c\u8fed\u4ee3\u8bc1\u636e\u79ef\u7d2f\uff1b3) \u6df1\u5ea6\u84b8\u998f\uff1a\u4ec5\u8bad\u7ec3\u65f6\u4f7f\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u6559\u5e08\u76d1\u7763\u6df1\u5ea6\u5206\u5e03\u5934\uff0c\u4e3a\u611f\u77e5\u524d\u7aef\u63d0\u4f9b\u51e0\u4f55\u611f\u77e5\u5148\u9a8c\u3002", "result": "\u5728RoboTwin 2.0\u7684\u9886\u57df\u968f\u673a\u5316\u8bbe\u7f6e\u4e0b\uff0cPEAfowl\u5c06\u6700\u5f3a\u57fa\u7ebf\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8623.0\u4e2a\u767e\u5206\u70b9\u3002\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u53ef\u9760\u7684\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\u548c\u6df1\u5ea6\u84b8\u998f\u5e26\u6765\u7684\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "PEAfowl\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u76843D\u8868\u793a\u548c\u8fed\u4ee3\u5f0f\u6307\u4ee4\u63a5\u5730\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u624b\u64cd\u4f5c\u5728\u6742\u4e71\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\u3002"}}
{"id": "2601.17895", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.17895", "abs": "https://arxiv.org/abs/2601.17895", "authors": ["Bin Tan", "Changjiang Sun", "Xiage Qin", "Hanat Adai", "Zelin Fu", "Tianxiang Zhou", "Han Zhang", "Yinghao Xu", "Xing Zhu", "Yujun Shen", "Nan Xue"], "title": "Masked Depth Modeling for Spatial Perception", "comment": "Tech report, 19 pages, 15 figures and 4 tables", "summary": "Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.", "AI": {"tldr": "\u63d0\u51faLingBot-Depth\u6df1\u5ea6\u8865\u5168\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u6df1\u5ea6\u5efa\u6a21\u5229\u7528\u89c6\u89c9\u4e0a\u4e0b\u6587\u4f18\u5316\u6df1\u5ea6\u56fe\uff0c\u5728\u6df1\u5ea6\u7cbe\u5ea6\u548c\u50cf\u7d20\u8986\u76d6\u65b9\u9762\u4f18\u4e8e\u9876\u7ea7RGB-D\u76f8\u673a", "motivation": "\u7a7a\u95f4\u89c6\u89c9\u611f\u77e5\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u7269\u7406\u4e16\u754c\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46RGB-D\u76f8\u673a\u83b7\u53d6\u50cf\u7d20\u5bf9\u9f50\u7684\u5ea6\u91cf\u6df1\u5ea6\u9762\u4e34\u786c\u4ef6\u9650\u5236\u548c\u6311\u6218\u6027\u6210\u50cf\u6761\u4ef6\uff08\u5982\u955c\u9762\u6216\u7eb9\u7406\u7f3a\u5931\u8868\u9762\uff09\u7684\u969c\u788d", "method": "\u63d0\u51faLingBot-Depth\u6df1\u5ea6\u8865\u5168\u6a21\u578b\uff0c\u5c06\u6df1\u5ea6\u4f20\u611f\u5668\u7684\u4e0d\u51c6\u786e\u6027\u89c6\u4e3a\u53cd\u6620\u5e95\u5c42\u51e0\u4f55\u6a21\u7cca\u6027\u7684\"\u63a9\u7801\"\u4fe1\u53f7\uff0c\u901a\u8fc7\u63a9\u7801\u6df1\u5ea6\u5efa\u6a21\u5229\u7528\u89c6\u89c9\u4e0a\u4e0b\u6587\u4f18\u5316\u6df1\u5ea6\u56fe\uff0c\u5e76\u5305\u542b\u81ea\u52a8\u5316\u6570\u636e\u6574\u7406\u6d41\u7a0b\u8fdb\u884c\u53ef\u6269\u5c55\u8bad\u7ec3", "result": "\u6a21\u578b\u5728\u6df1\u5ea6\u7cbe\u5ea6\u548c\u50cf\u7d20\u8986\u76d6\u65b9\u9762\u4f18\u4e8e\u9876\u7ea7RGB-D\u76f8\u673a\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u63d0\u4f9bRGB\u548c\u6df1\u5ea6\u6a21\u6001\u7684\u5bf9\u9f50\u6f5c\u5728\u8868\u793a\uff0c\u53d1\u5e03\u4e86\u4ee3\u7801\u3001\u68c0\u67e5\u70b9\u548c300\u4e07RGB-\u6df1\u5ea6\u5bf9\uff08\u5305\u62ec200\u4e07\u771f\u5b9e\u6570\u636e\u548c100\u4e07\u6a21\u62df\u6570\u636e\uff09", "conclusion": "LingBot-Depth\u901a\u8fc7\u63a9\u7801\u6df1\u5ea6\u5efa\u6a21\u548c\u81ea\u52a8\u5316\u6570\u636e\u6574\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u4f20\u611f\u5668\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u4e0d\u51c6\u786e\u6027\u95ee\u9898\uff0c\u4e3a\u7a7a\u95f4\u611f\u77e5\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u6570\u636e\u96c6"}}
{"id": "2601.17900", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17900", "abs": "https://arxiv.org/abs/2601.17900", "authors": ["Shengjun Zhang", "Min Chen", "Yibo Wei", "Mingyu Dong", "Yueqi Duan"], "title": "Revisiting 3D Reconstruction Kernels as Low-Pass Filters", "comment": "14 pages, 5 figures", "summary": "3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.", "AI": {"tldr": "\u8bba\u6587\u4ece\u4fe1\u53f7\u5904\u7406\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c63D\u91cd\u5efa\uff0c\u63d0\u51fa\u4f7f\u7528Jinc\u6838\u51fd\u6570\u4f5c\u4e3a\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u6765\u89e3\u51b3\u79bb\u6563\u91c7\u6837\u5bfc\u81f4\u7684\u9891\u8c31\u6269\u5c55\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u8c03\u5236\u6838\u51fd\u6570\u5e73\u8861\u7a7a\u95f4\u6548\u7387\u548c\u9891\u57df\u4fdd\u771f\u5ea6\u3002", "motivation": "\u4f20\u7edf3D\u91cd\u5efa\u6838\u51fd\u6570\uff08\u9ad8\u65af\u3001\u6307\u6570\u3001\u5b66\u751ft\u5206\u5e03\uff09\u4f5c\u4e3a\u4f4e\u901a\u6ee4\u6ce2\u5668\u5b58\u5728\u4e0d\u7406\u60f3\u7684\u4f4e\u901a\u7279\u6027\uff0c\u5bfc\u81f4\u9ad8\u9891\u5206\u91cf\u4e0e\u4f4e\u9891\u5206\u91cf\u5728\u79bb\u6563\u4fe1\u53f7\u9891\u8c31\u4e2d\u91cd\u53e0\uff0c\u8fd9\u662f\u7531\u79bb\u6563\u91c7\u6837\u5f15\u8d77\u7684\u5468\u671f\u6027\u9891\u8c31\u6269\u5c55\u9020\u6210\u7684\u6839\u672c\u95ee\u9898\u3002", "method": "\u5f15\u5165Jinc\u6838\u51fd\u6570\u4f5c\u4e3a\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\uff0c\u5728\u622a\u6b62\u9891\u7387\u5904\u5177\u6709\u77ac\u65f6\u964d\u4e3a\u96f6\u7684\u7279\u6027\uff1b\u9488\u5bf9Jinc\u6838\u5728\u7a7a\u95f4\u57df\u8870\u51cf\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u8c03\u5236\u6838\u51fd\u6570\uff0c\u5728\u7a7a\u95f4\u6548\u7387\u548c\u9891\u57df\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cJinc\u6838\u548c\u8c03\u5236\u6838\u51fd\u6570\u5728\u6e32\u67d3\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u534f\u8c03\u7a7a\u95f4\u6548\u7387\u548c\u9891\u57df\u4fdd\u771f\u5ea6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6e32\u67d3\u6548\u679c\u3002", "conclusion": "\u4ece\u4fe1\u53f7\u5904\u7406\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c63D\u91cd\u5efa\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u7406\u60f3\u4f4e\u901a\u6ee4\u6ce2\u5668\u7684Jinc\u6838\u51fd\u6570\u548c\u8c03\u5236\u6838\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u6563\u91c7\u6837\u5bfc\u81f4\u7684\u9891\u8c31\u6df7\u53e0\u95ee\u9898\uff0c\u57283D\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u5e73\u8861\u3002"}}
{"id": "2601.17905", "categories": ["cs.CV", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17905", "abs": "https://arxiv.org/abs/2601.17905", "authors": ["Jack Foster", "Kirill Paramonov", "Mete Ozay", "Umberto Michieli"], "title": "Feature-Space Generative Models for One-Shot Class-Incremental Learning", "comment": null, "summary": "Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.", "AI": {"tldr": "Gen1S\uff1a\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u5355\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5d4c\u5165\u7a7a\u95f4\u6620\u5c04\u5230\u6b8b\u5dee\u7a7a\u95f4\uff0c\u5229\u7528VAE\u6216\u6269\u6563\u6a21\u578b\u5b66\u4e60\u57fa\u7840\u7c7b\u6b8b\u5dee\u5206\u5e03\uff0c\u4f5c\u4e3a\u7ed3\u6784\u5148\u9a8c\u6765\u63d0\u5347\u65b0\u7c7b\u8bc6\u522b", "motivation": "\u89e3\u51b3\u5355\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6bcf\u4e2a\u65b0\u7c7b\u53ea\u6709\u4e00\u4e2a\u6837\u672c\u4e14\u4e0d\u5141\u8bb8\u540e\u7eed\u8bad\u7ec3\u6216\u6a21\u578b\u4fee\u6539\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u8bc6\u522b\u65b0\u7c7b", "method": "\u63d0\u51faGen1S\u65b9\u6cd5\uff1a1\uff09\u5c06\u539f\u59cb\u5d4c\u5165\u7a7a\u95f4\u6620\u5c04\u5230\u6b8b\u5dee\u7a7a\u95f4\uff08\u51cf\u53bb\u7c7b\u522b\u539f\u578b\uff09\uff1b2\uff09\u4f7f\u7528VAE\u6216\u6269\u6563\u6a21\u578b\u5b66\u4e60\u57fa\u7840\u7c7b\u6b8b\u5dee\u7684\u591a\u6a21\u6001\u5206\u5e03\uff1b3\uff09\u5229\u7528\u8be5\u7ed3\u6784\u5148\u9a8c\u63d0\u5347\u65b0\u7c7b\u8bc6\u522b", "result": "Gen1S\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u9aa8\u5e72\u67b6\u6784\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u6301\u7eed\u63d0\u5347\u4e86\u65b0\u7c7b\u8bc6\u522b\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5c06\u5d4c\u5165\u7a7a\u95f4\u8f6c\u6362\u4e3a\u6b8b\u5dee\u7a7a\u95f4\u5e76\u5229\u7528\u751f\u6210\u6a21\u578b\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5355\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u65b0\u7c7b\u8bc6\u522b\u96be\u9898"}}
{"id": "2601.17986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17986", "abs": "https://arxiv.org/abs/2601.17986", "authors": ["Anders Eklund"], "title": "Federated learning for unpaired multimodal data through a homogeneous transformer model", "comment": null, "summary": "Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3\u53bb\u4e2d\u5fc3\u5316\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6570\u636e\u6a21\u6001\u5206\u79bb\u4e14\u4e0d\u914d\u5bf9\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8054\u90a6\u5b66\u4e60\uff0c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u6570\u636e\u6216\u5bf9\u9f50\u6837\u672c\u3002", "motivation": "\u73b0\u5b9e\u8054\u90a6\u73af\u5883\u4e2d\uff0c\u6570\u636e\u901a\u5e38\u662f\u672a\u914d\u5bf9\u4e14\u5206\u6563\u5728\u4e0d\u540c\u8282\u70b9\u4e0a\u7684\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\u5206\u522b\u5b58\u50a8\u5728\u4e0d\u540c\u8282\u70b9\uff09\uff0c\u8fd9\u4e9b\u6570\u636e\u4e25\u683c\u79c1\u6709\u4e14\u6ca1\u6709\u5171\u540c\u6837\u672c\u3002\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8fd9\u79cd\u6a21\u6001\u5206\u79bb\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u5b83\u4eec\u5047\u8bbe\u672c\u5730\u5ba2\u6237\u7aef\u62e5\u6709\u5bf9\u9f50\u7684\u6570\u636e\u5bf9\u6216\u9700\u8981\u5171\u4eab\u539f\u59cb\u7279\u5f81\u5d4c\u5165\uff0c\u8fd9\u8fdd\u53cd\u4e86\u6570\u636e\u4e3b\u6743\u3002", "method": "1) \u5f15\u5165\u5c0f\u578b\u516c\u5171\u951a\u70b9\u96c6\u6765\u5bf9\u9f50\u5206\u79bb\u7684\u79c1\u6709\u6d41\u5f62\uff1b2) \u4f7f\u7528\u4ece\u516c\u5171\u951a\u70b9\u8ba1\u7b97\u7684Gram\u77e9\u9635\uff0c\u901a\u8fc7\u4e2d\u5fc3\u6838\u5bf9\u9f50\u5b9e\u73b0\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u4e0d\u4f20\u8f93\u79c1\u6709\u6837\u672c\uff1b3) \u63d0\u51fa\u5b50\u7a7a\u95f4\u7a33\u5b9a\u5fae\u8c03\u65b9\u6cd5\u5904\u7406\u5927\u578btransformer\u6a21\u578b\uff1b4) \u63d0\u51fa\u7cbe\u5ea6\u52a0\u6743\u5e73\u5747\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u964d\u4f4e\u4e0d\u786e\u5b9a\u8282\u70b9\u7684\u6743\u91cd\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a\u8054\u90a6\u672a\u914d\u5bf9\u57fa\u7840\u6a21\u578b\u5efa\u7acb\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u4f7f\u5168\u5c40\u6a21\u578b\u80fd\u591f\u4ece\u5206\u6563\u3001\u5206\u79bb\u548c\u79c1\u6709\u7684\u6570\u636e\u5b64\u5c9b\u4e2d\u5b66\u4e60\u4e16\u754c\u7684\u7edf\u4e00\u8868\u793a\uff0c\u65e0\u9700\u96c6\u4e2d\u5b58\u50a8\u6216\u914d\u5bf9\u6837\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u6570\u636e\u6a21\u6001\u5206\u79bb\u4e14\u672a\u914d\u5bf9\u7684\u8054\u90a6\u73af\u5883\u4e2d\u8bad\u7ec3\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u6bd4\u539f\u578b\u5171\u4eab\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u8bc1\uff0c\u5e76\u4e3a\u5904\u7406\u5927\u89c4\u6a21transformer\u6a21\u578b\u548c\u8282\u70b9\u5f02\u8d28\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17918", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.17918", "abs": "https://arxiv.org/abs/2601.17918", "authors": ["Dain Kim", "Jiwoo Lee", "Jaehoon Yun", "Yong Hoe Koo", "Qingyu Chen", "Hyunjae Kim", "Jaewoo Kang"], "title": "Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models", "comment": "EACL 2026 (Findings)", "summary": "Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u533b\u7597\u9886\u57df\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u591a\u79cdDPO\u53d8\u4f53\u7684\u6548\u679c\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u4e14\u65e0\u6cd5\u89e3\u51b3\u89c6\u89c9\u8bef\u89e3\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6027\u7684\u504f\u597d\u6784\u5efa\u7b56\u7565\u6765\u6539\u5584\u89c6\u89c9\u8bef\u89e3\u9519\u8bef\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u90e8\u7f72\u53d7\u5230\u5bf9\u9f50\u4e0d\u8db3\u548c\u53ef\u9760\u6027\u95ee\u9898\u7684\u9650\u5236\u3002\u867d\u7136\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5df2\u6210\u4e3a\u4f18\u5316\u6a21\u578b\u54cd\u5e94\u7684\u6709\u6548\u6846\u67b6\uff0c\u4f46\u5728\u9ad8\u98ce\u9669\u533b\u7597\u73af\u5883\u4e2d\u7684\u6548\u679c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u7f3a\u4e4f\u6307\u5bfc\u672a\u6765\u65b9\u6cd5\u53d1\u5c55\u7684\u4e25\u8c28\u5b9e\u8bc1\u57fa\u7840\u3002", "method": "\u7814\u7a76\u5bf9\u533b\u7597\u9886\u57df\u4e2d\u7684\u591a\u79cdDPO\u53d8\u4f53\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u8bc4\u4f30\uff0c\u5728LLaVA-Med\u548cHuatuoGPT-Vision\u4e24\u4e2a\u533b\u7597LVLM\u4e0a\u8bc4\u4f30\u4e86\u4e5d\u79cd\u4e0d\u540c\u7684DPO\u516c\u5f0f\u3002\u57fa\u4e8e\u53d1\u73b0\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6027\u7684\u504f\u597d\u6784\u5efa\u7b56\u7565\uff0c\u4e13\u95e8\u89e3\u51b3\u73b0\u6709DPO\u6a21\u578b\u4e2d\u5e38\u89c1\u7684\u89c6\u89c9\u8bef\u89e3\u9519\u8bef\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dDPO\u65b9\u6cd5\u7684\u51e0\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff1a\u5728\u76d1\u7763\u5fae\u8c03\u57fa\u7840\u4e0a\u6548\u679c\u4e0d\u4e00\u81f4\uff0c\u4e0d\u540c\u4efb\u52a1\u548c\u9aa8\u5e72\u6a21\u578b\u95f4\u6548\u679c\u5dee\u5f02\u663e\u8457\uff0c\u4e14\u65e0\u6cd5\u89e3\u51b3\u57fa\u672c\u7684\u89c6\u89c9\u8bef\u89e3\u9519\u8bef\u3002\u63d0\u51fa\u7684\u9488\u5bf9\u6027\u504f\u597d\u6784\u5efa\u7b56\u7565\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709\u6700\u5f3aDPO\u57fa\u7ebf\u63d0\u9ad8\u4e863.6%\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u533b\u7597\u9886\u57dfDPO\u65b9\u6cd5\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u89c6\u89c9\u8bef\u89e3\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7814\u7a76\u53d1\u5e03\u4e86\u5b8c\u6574\u7684\u6846\u67b6\uff0c\u5305\u62ec\u8bad\u7ec3\u6570\u636e\u3001\u6a21\u578b\u68c0\u67e5\u70b9\u548c\u4ee3\u7801\u5e93\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2601.17927", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.17927", "abs": "https://arxiv.org/abs/2601.17927", "authors": ["Eashan Adhikarla", "Brian D. Davison"], "title": "RemEdit: Efficient Diffusion Editing with Riemannian Geometry", "comment": null, "summary": "Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.", "AI": {"tldr": "RemEdit\u662f\u4e00\u4e2a\u6269\u6563\u6a21\u578b\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u5bfc\u822a\u548c\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u526a\u679d\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd", "motivation": "\u73b0\u4ee3\u751f\u6210\u5f0fAI\u4e2d\u7684\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u9762\u4e34\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u517c\u987e\u8fd9\u4e24\u4e2a\u65b9\u9762", "method": "1. \u5c06\u6f5c\u5728\u7a7a\u95f4\u89c6\u4e3a\u9ece\u66fc\u6d41\u5f62\uff0c\u4f7f\u7528Mamba\u6a21\u5757\u5b66\u4e60\u6d41\u5f62\u7ed3\u6784\uff0c\u76f4\u63a5\u8ba1\u7b97\u6d4b\u5730\u7ebf\u8def\u5f84\u5b9e\u73b0\u5e73\u6ed1\u8bed\u4e49\u7f16\u8f91\uff1b2. \u91c7\u7528\u53ccSLERP\u6df7\u5408\u6280\u672f\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u76ee\u6807\u611f\u77e5\u63d0\u793a\u589e\u5f3a\uff1b3. \u5f15\u5165\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u526a\u679d\u673a\u5236\uff0c\u8f7b\u91cf\u7ea7\u526a\u679d\u5934\u5b66\u4e60\u4fdd\u7559\u7f16\u8f91\u5fc5\u9700\u7684\u7279\u5f81", "result": "RemEdit\u8d85\u8d8a\u4e86\u5148\u524d\u6700\u5148\u8fdb\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u572850%\u526a\u679d\u7387\u4e0b\u4ecd\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u5b9e\u7528\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6", "conclusion": "RemEdit\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u5bfc\u822a\u548c\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u526a\u679d\u7684\u534f\u540c\u521b\u65b0\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u50cf\u7f16\u8f91\u4e2d\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0e\u63a8\u7406\u901f\u5ea6\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u4e14\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91"}}
{"id": "2601.17995", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17995", "abs": "https://arxiv.org/abs/2601.17995", "authors": ["Shudi Weng", "Ming Xiao", "Mikael Skoglund"], "title": "Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning", "comment": null, "summary": "Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees", "AI": {"tldr": "\u63d0\u51faH-SecCoGC\u65b9\u6848\uff0c\u901a\u8fc7\u7f16\u7801\u7b56\u7565\u5b9e\u73b0\u7ed3\u6784\u5316\u805a\u5408\uff0c\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u4e0b\u540c\u65f6\u4fdd\u8bc1\u6a21\u578b\u7cbe\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4", "motivation": "\u5206\u5c42\u8054\u90a6\u5b66\u4e60(HFL)\u867d\u7136\u6539\u5584\u4e86\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u5668\u95f4\u7684\u94fe\u8def\u8d28\u91cf\uff0c\u4f46\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u73af\u5883\u4e0b\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u9690\u79c1\u566a\u58f0\u7684\u534f\u8c03\u53ef\u80fd\u88ab\u968f\u673a\u7834\u574f", "method": "\u63d0\u51fa\u9c81\u68d2\u7684\u5206\u5c42\u5b89\u5168\u805a\u5408\u65b9\u6848H-SecCoGC\uff0c\u96c6\u6210\u7f16\u7801\u7b56\u7565\u6765\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u5316\u805a\u5408\uff0c\u907f\u514d\u90e8\u5206\u53c2\u4e0e\u95ee\u9898", "result": "\u8be5\u65b9\u6848\u5728\u4e0d\u540c\u9690\u79c1\u4fdd\u62a4\u7ea7\u522b\u4e0b\u90fd\u80fd\u786e\u4fdd\u51c6\u786e\u7684\u5168\u5c40\u6a21\u578b\u6784\u5efa\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u5b66\u4e60\u6548\u7387", "conclusion": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u90fd\u8bc1\u660e\u4e86\u8be5\u65b9\u6848\u5728\u4e0d\u53ef\u9760\u901a\u4fe1\u73af\u5883\u4e0b\uff0c\u5bf9\u4efb\u610f\u5f3a\u9690\u79c1\u4fdd\u8bc1\u7684\u4f18\u8d8a\u6027"}}
{"id": "2601.17934", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17934", "abs": "https://arxiv.org/abs/2601.17934", "authors": ["Vi Vu", "Thanh-Huy Nguyen", "Tien-Thinh Nguyen", "Ba-Thinh Lam", "Hoang-Thien Nguyen", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images", "comment": "Accepted to ISBI 2026", "summary": "Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.", "AI": {"tldr": "SC-SAM\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u5bb6-\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7U-Net\u4e3aSAM\u63d0\u4f9b\u70b9\u63d0\u793a\u548c\u4f2a\u6807\u7b7e\uff0c\u540c\u65f6SAM\u4f5c\u4e3a\u901a\u7528\u76d1\u7763\u5668\u6b63\u5219\u5316U-Net\uff0c\u5f62\u6210\u53cc\u5411\u534f\u540c\u8bad\u7ec3\u5faa\u73af\uff0c\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002", "motivation": "\u867d\u7136\u57fa\u7840\u6a21\u578b\u5982SAM\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5c06\u5176\u9002\u5e94\u5230\u533b\u5b66\u56fe\u50cf\u9886\u57df\u4ecd\u9762\u4e34\u6311\u6218\uff1a\u9886\u57df\u504f\u79fb\u3001\u6807\u7b7e\u7a00\u7f3a\uff0c\u4ee5\u53ca\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u3002\u4f20\u7edf\u6a21\u578b\u5982U-Net\u5728\u534a\u76d1\u7763\u533b\u5b66\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f85\u52a9PEFT SAM\u7684\u6f5c\u529b\u88ab\u5ffd\u89c6\u3002", "method": "\u63d0\u51faSC-SAM\u4e13\u5bb6-\u901a\u7528\u6846\u67b6\uff1aU-Net\u4f5c\u4e3a\u4e13\u5bb6\u63d0\u4f9b\u70b9\u63d0\u793a\u548c\u4f2a\u6807\u7b7e\u6765\u6307\u5bfcSAM\u7684\u9002\u5e94\uff0c\u540c\u65f6SAM\u4f5c\u4e3a\u901a\u7528\u76d1\u7763\u5668\u6b63\u5219\u5316U-Net\u3002\u8fd9\u79cd\u76f8\u4e92\u6307\u5bfc\u5f62\u6210\u53cc\u5411\u534f\u540c\u8bad\u7ec3\u5faa\u73af\uff0c\u4f7f\u4e24\u4e2a\u6a21\u578b\u90fd\u80fd\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u3002", "result": "\u5728\u524d\u5217\u817aMRI\u548c\u606f\u8089\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u73b0\u6709\u7684\u534a\u76d1\u7763SAM\u53d8\u4f53\uff0c\u751a\u81f3\u4f18\u4e8eMedSAM\u7b49\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u4e13\u5bb6-\u901a\u7528\u534f\u4f5c\u5728\u6807\u7b7e\u9ad8\u6548\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "SC-SAM\u901a\u8fc7\u4e13\u5bb6-\u901a\u7528\u534f\u4f5c\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u57fa\u7840\u6a21\u578b\u9002\u5e94\u7684\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u53cc\u5411\u534f\u540c\u8bad\u7ec3\u5728\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6807\u7b7e\u7a00\u7f3a\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18030", "abs": "https://arxiv.org/abs/2601.18030", "authors": ["Markus N. Rabe", "Judith Clymo", "Zheren Dong"], "title": "Spelling Bee Embeddings for Language Modeling", "comment": null, "summary": "We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u5d4c\u5165\u5c42\u4fee\u6539\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bcd\u7b26\u62fc\u5199\u4fe1\u606f\u878d\u5165\u5d4c\u5165\u8868\u793a\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u62fc\u5199\u80fd\u529b\uff0c\u8fd8\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u7684\u8bcd\u7b26\u5d4c\u5165\u65b9\u6cd5\u53ef\u80fd\u672a\u80fd\u5145\u5206\u5229\u7528\u8bcd\u7b26\u7684\u62fc\u5199\u4fe1\u606f\uff0c\u800c\u62fc\u5199\u4fe1\u606f\u672c\u8eab\u5305\u542b\u6709\u4ef7\u503c\u7684\u8bed\u8a00\u7279\u5f81\u3002\u901a\u8fc7\u5c06\u62fc\u5199\u4fe1\u606f\u878d\u5165\u5d4c\u5165\u5c42\uff0c\u53ef\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5bf9\u5d4c\u5165\u5c42\u8fdb\u884c\u7b80\u5355\u4fee\u6539\uff0c\u5c06\u8bcd\u7b26\u7684\u62fc\u5199\u4fe1\u606f\u6ce8\u5165\u5230\u8bcd\u7b26\u5d4c\u5165\u4e2d\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c06\u62fc\u5199\u7279\u5f81\u4e0e\u539f\u59cb\u5d4c\u5165\u76f8\u7ed3\u5408\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u8bed\u4e49\u548c\u62fc\u5199\u7279\u5f81\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u7684\u6a21\u578b\u5728\u62fc\u5199\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u540c\u65f6\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u53d6\u5f97\u4e86\u6539\u8fdb\u3002\u5bf940M\u5230800M\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u8fdb\u884c\u6269\u5c55\u7814\u7a76\u663e\u793a\uff0c\u8fd9\u79cd\u6539\u8fdb\u76f8\u5f53\u4e8e\u9700\u8981\u51cf\u5c11\u7ea68%\u7684\u8ba1\u7b97\u548c\u6570\u636e\u6765\u8fbe\u5230\u76f8\u540c\u7684\u6d4b\u8bd5\u635f\u5931\u3002", "conclusion": "\u5c06\u62fc\u5199\u4fe1\u606f\u878d\u5165\u5d4c\u5165\u5c42\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u51cf\u5c11\u8bad\u7ec3\u6240\u9700\u7684\u8ba1\u7b97\u548c\u6570\u636e\u8d44\u6e90\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6a21\u578b\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2601.18032", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.18032", "abs": "https://arxiv.org/abs/2601.18032", "authors": ["Brijesh FNU", "Viet Thanh Duy Nguyen", "Ashima Sharma", "Md Harun Rashid Molla", "Chengyi Xu", "Truong-Son Hy"], "title": "Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity", "comment": null, "summary": "Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e19\u70ef\u9178\u916f\u57fa\u4ecb\u7535\u5f39\u6027\u4f53\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5206\u5b50\u5e8f\u5217\u9884\u6d4b\u4ecb\u7535\u548c\u673a\u68b0\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8f6f\u7535\u5b50\u6750\u6599\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u8f6f\u53ef\u62c9\u4f38\u7535\u5b50\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9700\u8981\u9ad8\u6027\u80fd\u4ecb\u7535\u5f39\u6027\u4f53\uff0c\u4f46\u73b0\u6709\u6750\u6599\u96be\u4ee5\u540c\u65f6\u5177\u5907\u9ad8\u4ecb\u7535\u5e38\u6570\u548c\u4f4e\u6768\u6c0f\u6a21\u91cf\u3002\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5305\u542b\u5206\u5b50\u5e8f\u5217\u3001\u4ecb\u7535\u548c\u673a\u68b0\u6027\u80fd\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u6570\u636e\u9a71\u52a8\u7684\u6750\u6599\u53d1\u73b0\u3002", "method": "1. \u901a\u8fc7\u7b5b\u9009\u8fc7\u53bb10\u5e74\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6784\u5efa\u4e86\u4e19\u70ef\u9178\u916f\u57fa\u4ecb\u7535\u5f39\u6027\u4f53\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b2. \u63d0\u51fa\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u57fa\u4e8e\u56fe\u548c\u5e8f\u5217\u7f16\u7801\u5668\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u805a\u5408\u7269\u8868\u793a\uff1b3. \u901a\u8fc7\u9884\u8bad\u7ec3\u5d4c\u5165\u4f20\u9012\u4e30\u5bcc\u7684\u5316\u5b66\u548c\u7ed3\u6784\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4ece\u5206\u5b50\u5e8f\u5217\u51c6\u786e\u9884\u6d4b\u4ecb\u7535\u548c\u673a\u68b0\u6027\u80fd\u3002", "result": "\u5f00\u53d1\u7684\u6570\u636e\u96c6\u548c\u6846\u67b6\u80fd\u591f\u51c6\u786e\u8fdb\u884c\u5c11\u6837\u672c\u9884\u6d4b\uff0c\u514b\u670d\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u805a\u5408\u7269\u9aa8\u67b6\uff08\u5982\u7845\u916e\u3001\u805a\u6c28\u916f\uff09\uff0c\u52a0\u901f\u8f6f\u9ad8\u4ecb\u7535\u5e38\u6570\u5f39\u6027\u4f53\u7684\u6570\u636e\u9ad8\u6548\u53d1\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4ece\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u8f6c\u79fb\u77e5\u8bc6\u7684\u65b0\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u805a\u5408\u7269\u6750\u6599\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u52a0\u901f\u9ad8\u6027\u80fd\u4ecb\u7535\u5f39\u6027\u4f53\u7684\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002\u6570\u636e\u96c6\u548c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2601.17950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17950", "abs": "https://arxiv.org/abs/2601.17950", "authors": ["Matthew Walmer", "Saksham Suri", "Anirud Aggarwal", "Abhinav Shrivastava"], "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders", "comment": null, "summary": "The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.", "AI": {"tldr": "UPLiFT\u662f\u4e00\u79cd\u901a\u7528\u50cf\u7d20\u5bc6\u96c6\u8f7b\u91cf\u7ea7\u7279\u5f81\u53d8\u6362\u67b6\u6784\uff0c\u901a\u8fc7\u5c40\u90e8\u6ce8\u610f\u529b\u7b97\u5b50\u5b9e\u73b0\u9ad8\u6548\u7684\u7279\u5f81\u4e0a\u91c7\u6837\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u7279\u5f81\u4e0a\u91c7\u6837\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u6269\u5c55\u95ee\u9898\uff0c\u800c\u65e9\u671f\u8fed\u4ee3\u4e0a\u91c7\u6837\u65b9\u6cd5\u6027\u80fd\u6709\u9650\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u5177\u6709\u4f4e\u63a8\u7406\u6210\u672c\u7684\u7279\u5f81\u4e0a\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUPLiFT\u67b6\u6784\uff0c\u91c7\u7528\u8fed\u4ee3\u4e0a\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u9ad8\u6548\u7684\u5c40\u90e8\u6ce8\u610f\u529b\u7b97\u5b50\uff0c\u4f7f\u7528\u5b8c\u5168\u5c40\u90e8\u5b9a\u4e49\u7684\u6ce8\u610f\u529b\u6c60\u5316\u516c\u5f0f\u6765\u7a33\u5b9a\u7279\u5f81\u4e0a\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "UPLiFT\u5728\u4fdd\u6301\u7279\u5f81\u7a33\u5b9a\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63a8\u7406\u6210\u672c\u4f4e\u4e8e\u73b0\u6709\u50cf\u7d20\u5bc6\u96c6\u7279\u5f81\u4e0a\u91c7\u6837\u5668\uff0c\u5728\u751f\u6210\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4e0e\u6700\u5148\u8fdb\u7684\u8026\u5408\u6d41\u5339\u914d\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "UPLiFT\u63d0\u4f9b\u4e86\u4e00\u79cd\u591a\u529f\u80fd\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u521b\u5efa\u66f4\u5bc6\u96c6\u7684\u7279\u5f81\uff0c\u8bc1\u660e\u8fed\u4ee3\u4e0a\u91c7\u6837\u65b9\u6cd5\u4ecd\u80fd\u4e0e\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\u7ade\u4e89\uff0c\u5e76\u5728\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.18076", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18076", "abs": "https://arxiv.org/abs/2601.18076", "authors": ["Alexandra Chouldechova", "A. Feder Cooper", "Solon Barocas", "Abhinav Palia", "Dan Vann", "Hanna Wallach"], "title": "Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming", "comment": null, "summary": "We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.", "AI": {"tldr": "\u8bba\u6587\u6307\u51faAI\u7ea2\u961f\u8bc4\u4f30\u4e2d\u57fa\u4e8e\u653b\u51fb\u6210\u529f\u7387\u6bd4\u8f83\u5f97\u51fa\u7684\u7cfb\u7edf\u5b89\u5168\u6027\u6216\u653b\u51fb\u65b9\u6cd5\u6709\u6548\u6027\u7ed3\u8bba\u5f80\u5f80\u7f3a\u4e4f\u8bc1\u636e\u652f\u6301\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6bd4\u8f83\u5b58\u5728\u82f9\u679c\u4e0e\u6a59\u5b50\u5bf9\u6bd4\u6216\u6d4b\u91cf\u6548\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u8bc4\u4f30\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u7ecf\u5e38\u901a\u8fc7\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u7684\u6bd4\u8f83\u6765\u5f97\u51fa\u5173\u4e8e\u7cfb\u7edf\u76f8\u5bf9\u5b89\u5168\u6027\u6216\u653b\u51fb\u65b9\u6cd5\u6709\u6548\u6027\u7684\u7ed3\u8bba\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u8bba\u5f80\u5f80\u7f3a\u4e4f\u575a\u5b9e\u7684\u8bc1\u636e\u57fa\u7840\uff0c\u56e0\u4e3aASR\u6bd4\u8f83\u5b58\u5728\u65b9\u6cd5\u8bba\u95ee\u9898\u3002", "method": "\u91c7\u7528\u793e\u4f1a\u79d1\u5b66\u6d4b\u91cf\u7406\u8bba\u548c\u63a8\u65ad\u7edf\u8ba1\u5b66\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u653b\u51fb\u6210\u529f\u7387\u6709\u610f\u4e49\u7684\u6bd4\u8f83\u6761\u4ef6\u3002\u4ee5\u8d8a\u72f1\u653b\u51fb\u4e3a\u4f8b\uff0c\u901a\u8fc7\u6982\u5ff5\u3001\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u5c55\u793a\u4e86ASR\u6bd4\u8f83\u4e2d\u7684\u82f9\u679c\u4e0e\u6a59\u5b50\u5bf9\u6bd4\u95ee\u9898\u548c\u6d4b\u91cf\u6548\u5ea6\u6311\u6218\u3002", "result": "\u7814\u7a76\u8868\u660e\u8bb8\u591a\u57fa\u4e8eASR\u6bd4\u8f83\u7684\u7ed3\u8bba\u5b58\u5728\u65b9\u6cd5\u8bba\u7f3a\u9677\uff0c\u5305\u62ec\u4e0d\u6070\u5f53\u7684\u6bd4\u8f83\u57fa\u51c6\u3001\u6d4b\u91cf\u6548\u5ea6\u4e0d\u8db3\u7b49\u95ee\u9898\u3002\u8bba\u6587\u660e\u786e\u4e86ASR\u53ef\u4ee5\u548c\u4e0d\u53ef\u4ee5\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u8fdb\u884c\u6709\u610f\u4e49\u7684\u6bd4\u8f83\u3002", "conclusion": "AI\u7ea2\u961f\u8bc4\u4f30\u9700\u8981\u66f4\u4e25\u8c28\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u653b\u51fb\u6210\u529f\u7387\u7684\u6bd4\u8f83\u5fc5\u987b\u6ee1\u8db3\u7279\u5b9a\u7684\u6d4b\u91cf\u548c\u7edf\u8ba1\u6761\u4ef6\u624d\u80fd\u5f97\u51fa\u6709\u6548\u7ed3\u8bba\u3002\u7814\u7a76\u8005\u5e94\u8be5\u907f\u514d\u82f9\u679c\u4e0e\u6a59\u5b50\u7684\u6bd4\u8f83\uff0c\u5e76\u63d0\u9ad8\u6d4b\u91cf\u6548\u5ea6\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2601.17977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.17977", "abs": "https://arxiv.org/abs/2601.17977", "authors": ["Jinchen Gu", "Nan Zhao", "Lei Qiu", "Lu Zhang"], "title": "Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors", "comment": "4 pages; 3 figures; accepted by International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.", "AI": {"tldr": "\u63d0\u51faDKGH-MoE\u6a21\u578b\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u548c\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u8bca\u65ad\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u533b\u5b66\u9886\u57df\u7684MoE\u6a21\u578b\u53d7\u9650\u4e8e\u5c0f\u6570\u636e\u96c6\uff0c\u800c\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u4e30\u5bcc\u7684\u4e13\u5bb6\u77e5\u8bc6\uff08\u5982\u533b\u751f\u6ce8\u89c6\u6a21\u5f0f\u3001\u8bca\u65ad\u542f\u53d1\u5f0f\uff09\u65e0\u6cd5\u4ece\u6709\u9650\u6570\u636e\u4e2d\u53ef\u9760\u5b66\u4e60\u3002\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u4e13\u5bb6\uff08\u6355\u6349\u65b0\u6a21\u5f0f\uff09\u548c\u9886\u57df\u4e13\u5bb6\u6307\u5bfc\u4e13\u5bb6\uff08\u7f16\u7801\u4e34\u5e8a\u6d1e\u5bdf\uff09\u53ef\u63d0\u4f9b\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u63d0\u51faDKGH-MoE\uff08\u9886\u57df\u77e5\u8bc6\u5f15\u5bfc\u6df7\u5408MoE\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u5757\u3002\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1a1\uff09\u6570\u636e\u9a71\u52a8MoE\u4ece\u539f\u59cb\u5f71\u50cf\u6570\u636e\u63d0\u53d6\u65b0\u7279\u5f81\uff1b2\uff09\u9886\u57df\u4e13\u5bb6\u5f15\u5bfcMoE\u6574\u5408\u4e34\u5e8a\u5148\u9a8c\u77e5\u8bc6\uff08\u7279\u522b\u662f\u4e34\u5e8a\u533b\u751f\u773c\u52a8\u6ce8\u89c6\u7ebf\u7d22\uff09\uff0c\u5f3a\u8c03\u9ad8\u8bca\u65ad\u76f8\u5173\u533a\u57df\u3002", "result": "\u901a\u8fc7\u6574\u5408\u9886\u57df\u4e13\u5bb6\u6d1e\u5bdf\u548c\u6570\u636e\u9a71\u52a8\u7279\u5f81\uff0cDKGH-MoE\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "DKGH-MoE\u901a\u8fc7\u7edf\u4e00\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u4e14\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18081", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18081", "abs": "https://arxiv.org/abs/2601.18081", "authors": ["Peixuan Han", "Yingjie Yu", "Jingjun Xu", "Jiaxuan You"], "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal", "comment": null, "summary": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.", "AI": {"tldr": "DRPG\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u5b66\u672f\u53cd\u9a73\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u3001\u68c0\u7d22\u3001\u89c4\u5212\u548c\u751f\u6210\u56db\u6b65\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cd\u9a73\u8d28\u91cf\uff0c\u5728\u9876\u7ea7\u4f1a\u8bae\u6570\u636e\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u548c\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u7814\u5de5\u4f5c\u6d41\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b66\u672f\u53cd\u9a73\u8fd9\u4e00\u5b66\u672f\u4ea4\u6d41\u548c\u540c\u884c\u8bc4\u5ba1\u7684\u5173\u952e\u73af\u8282\u4ecd\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u52a8\u5316\u652f\u6301\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u73b0\u6210\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6216\u7b80\u5355\u6d41\u7a0b\uff0c\u96be\u4ee5\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u4e14\u65e0\u6cd5\u751f\u6210\u6709\u9488\u5bf9\u6027\u548c\u8bf4\u670d\u529b\u7684\u56de\u5e94\u3002", "method": "\u63d0\u51faDRPG\u6846\u67b6\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a1) \u5c06\u5ba1\u7a3f\u610f\u89c1\u5206\u89e3\u4e3a\u539f\u5b50\u5316\u5173\u6ce8\u70b9\uff1b2) \u4ece\u8bba\u6587\u4e2d\u68c0\u7d22\u76f8\u5173\u8bc1\u636e\uff1b3) \u89c4\u5212\u53cd\u9a73\u7b56\u7565\uff1b4) \u57fa\u4e8e\u7b56\u7565\u751f\u6210\u56de\u5e94\u3002\u5176\u4e2d\u89c4\u5212\u5668\u5728\u8bc6\u522b\u6700\u53ef\u884c\u53cd\u9a73\u65b9\u5411\u65b9\u9762\u51c6\u786e\u7387\u8d85\u8fc798%\u3002", "result": "\u5728\u9876\u7ea7\u4f1a\u8bae\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDRPG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53cd\u9a73\u6d41\u7a0b\uff0c\u4ec5\u4f7f\u75288B\u53c2\u6570\u7684\u6a21\u578b\u5c31\u8fbe\u5230\u4e86\u8d85\u8d8a\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u7684\u6027\u80fd\u3002\u89c4\u5212\u5668\u8bbe\u8ba1\u88ab\u8bc1\u660e\u6709\u6548\uff0c\u80fd\u63d0\u4f9b\u591a\u89c6\u89d2\u548c\u53ef\u89e3\u91ca\u7684\u5efa\u8bae\u3002\u6846\u67b6\u5728\u66f4\u590d\u6742\u7684\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "DRPG\u6846\u67b6\u5c55\u793a\u4e86\u5728\u5b66\u672f\u53cd\u9a73\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5177\u6709\u63d0\u4f9b\u9ad8\u8d28\u91cf\u53cd\u9a73\u5185\u5bb9\u548c\u652f\u6301\u5b66\u672f\u8ba8\u8bba\u89c4\u6a21\u5316\u7684\u6f5c\u529b\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4e3a\u5b66\u672f\u4ea4\u6d41\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.18001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18001", "abs": "https://arxiv.org/abs/2601.18001", "authors": ["Aqsa Yousaf", "Sint Sint Win", "Megan Coffee", "Habeeb Olufowobi"], "title": "MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images", "comment": "Accepted at WACV 2026", "summary": "Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.", "AI": {"tldr": "MorphXAI\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u5bc4\u751f\u866b\u68c0\u6d4b\u6846\u67b6\uff0c\u5c06\u5f62\u6001\u5b66\u76d1\u7763\u96c6\u6210\u5230\u9884\u6d4b\u6d41\u7a0b\u4e2d\uff0c\u5728\u68c0\u6d4b\u5bc4\u751f\u866b\u7684\u540c\u65f6\u63d0\u4f9b\u4e34\u5e8a\u76f8\u5173\u7684\u5f62\u6001\u7279\u5f81\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u5bc4\u751f\u866b\u611f\u67d3\u8bca\u65ad\u5728\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u4ecd\u4f9d\u8d56\u4eba\u5de5\u8840\u6d82\u7247\u68c0\u67e5\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u867d\u7136\u6027\u80fd\u597d\u4f46\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3002\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u4e3b\u8981\u662f\u89c6\u89c9\u70ed\u56fe\u6216\u6ce8\u610f\u529b\u56fe\uff0c\u65e0\u6cd5\u6355\u6349\u4e34\u5e8a\u533b\u751f\u4f9d\u8d56\u7684\u5f62\u6001\u7279\u5f81\u3002", "method": "\u63d0\u51faMorphXAI\u6846\u67b6\uff0c\u5c06\u5f62\u6001\u5b66\u76d1\u7763\u76f4\u63a5\u96c6\u6210\u5230\u9884\u6d4b\u6d41\u7a0b\u4e2d\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b9a\u4f4d\u5bc4\u751f\u866b\u540c\u65f6\u8868\u5f81\u4e34\u5e8a\u76f8\u5173\u5c5e\u6027\uff08\u5f62\u72b6\u3001\u66f2\u7387\u3001\u53ef\u89c1\u70b9\u8ba1\u6570\u3001\u97ad\u6bdb\u5b58\u5728\u3001\u53d1\u80b2\u9636\u6bb5\u7b49\uff09\u3002\u521b\u5efa\u4e86\u5305\u542b\u4e09\u79cd\u5bc4\u751f\u866b\u7269\u79cd\uff08\u5229\u4ec0\u66fc\u539f\u866b\u3001\u5e03\u6c0f\u9525\u866b\u3001\u514b\u6c0f\u9525\u866b\uff09\u7684\u4e34\u5e8a\u533b\u751f\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMorphXAI\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u3001\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u89e3\u91ca\u3002", "conclusion": "MorphXAI\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u5bc4\u751f\u866b\u5206\u6790\u7684\u65b0\u57fa\u51c6\uff0c\u7edf\u4e00\u4e86\u5bc4\u751f\u866b\u68c0\u6d4b\u4e0e\u7ec6\u7c92\u5ea6\u5f62\u6001\u5206\u6790\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u6709\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2601.18089", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18089", "abs": "https://arxiv.org/abs/2601.18089", "authors": ["Venmugil Elango", "Nidhi Bhatia", "Roger Waleffe", "Rasoul Shafipour", "Tomer Asida", "Abhinav Khattar", "Nave Assaf", "Maximilian Golub", "Joey Guman", "Tiyasa Mitra", "Ritchie Zhao", "Ritika Borkar", "Ran Zilberstein", "Mostofa Patwary", "Mohammad Shoeybi", "Bita Rouhani"], "title": "LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts", "comment": null, "summary": "Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).", "AI": {"tldr": "LatentMoE\u662f\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u4f18\u5316\u63a8\u7406\u6210\u672c\uff0c\u5728\u51c6\u786e\u7387\u4e0e\u8ba1\u7b97\u91cf/\u53c2\u6570\u6bd4\u65b9\u9762\u4f18\u4e8e\u6807\u51c6MoE\u67b6\u6784\uff0c\u5df2\u88abNemotron-3\u6a21\u578b\u91c7\u7528\u3002", "motivation": "\u5c3d\u7ba1\u6df7\u5408\u4e13\u5bb6(MoE)\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u67b6\u6784\u5728\u63a8\u7406\u6210\u672c\uff08\u4ee5\u51c6\u786e\u7387\u4e0e\u6d6e\u70b9\u8fd0\u7b97/\u53c2\u6570\u6bd4\u8861\u91cf\uff09\u65b9\u9762\u7684\u6700\u4f18\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4ece\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u3002", "method": "\u4ece\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u89d2\u5ea6\u51fa\u53d1\uff0c\u57fa\u4e8e\u7ecf\u9a8c\u548c\u7406\u8bba\u5206\u6790\uff0c\u5728\u4e0d\u540c\u90e8\u7f72\u573a\u666f\uff08\u79bb\u7ebf\u9ad8\u541e\u5410\u548c\u5728\u7ebf\u4f4e\u5ef6\u8fdf\uff09\u4e0b\u8bc6\u522b\u6027\u80fd\u74f6\u9888\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0895B\u53c2\u6570\u89c4\u6a21\uff0c1T token\u8bad\u7ec3\uff09\u5f00\u53d1LatentMoE\u67b6\u6784\u3002", "result": "LatentMoE\u5728\u51c6\u786e\u7387\u4e0eFLOP\u6bd4\u548c\u51c6\u786e\u7387\u4e0e\u53c2\u6570\u6bd4\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6807\u51c6MoE\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5df2\u88abNemotron-3 Super\u548cUltra\u6a21\u578b\u91c7\u7528\uff0c\u5e76\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\uff08\u66f4\u957ftoken\u5e8f\u5217\u548c\u66f4\u5927\u6a21\u578b\u5c3a\u5bf8\uff09\u3002", "conclusion": "\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u7cfb\u7edf\u5316\u63a2\u7d22\uff0cLatentMoE\u67b6\u6784\u5728\u63a8\u7406\u6210\u672c\u4f18\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709MoE\u67b6\u6784\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\u65b9\u6848\u3002"}}
{"id": "2601.18091", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18091", "abs": "https://arxiv.org/abs/2601.18091", "authors": ["Longwei Ding", "Anhao Zhao", "Fanghua Ye", "Ziyang Chen", "Xiaoyu Shen"], "title": "From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models", "comment": "18 pages, 7 figures", "summary": "Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\\textbf{LLM-instruct}$) and reasoning-augmented ($\\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u6bd4\u4e86\u6307\u4ee4\u9075\u5faa\u578bLLM\u548c\u63a8\u7406\u589e\u5f3a\u578bLLM\u7684\u526a\u679d\u7b56\u7565\u6548\u679c\uff0c\u53d1\u73b0\u4e0d\u540c\u8303\u5f0f\u4e0b\u526a\u679d\u6548\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u9488\u5bf9\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u8bbe\u8ba1\u4e13\u95e8\u7684\u526a\u679d\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6307\u4ee4\u9075\u5faa\u578b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u526a\u679d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u63a8\u7406\u589e\u5f3a\u578b\u6a21\u578b\uff08\u751f\u6210\u957f\u4e2d\u95f4\u63a8\u7406\u8f68\u8ff9\uff09\u526a\u679d\u7b56\u7565\u7684\u7814\u7a76\u3002\u9700\u8981\u660e\u786e\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u662f\u5426\u9002\u7528\u4e8e\u8fd9\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u5bf9\u6bd4\u6307\u4ee4\u9075\u5faa\u578b\uff08LLM-instruct\uff09\u548c\u63a8\u7406\u589e\u5f3a\u578b\uff08LLM-think\uff09\u6a21\u578b\u7684\u526a\u679d\u6548\u679c\u3002\u5c06\u526a\u679d\u6821\u51c6\u548c\u540e\u526a\u679d\u6062\u590d\u6570\u636e\u4e0e\u6a21\u578b\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u5bf9\u9f50\u4ee5\u786e\u4fdd\u7a33\u5b9a\u6027\u3002\u8bc4\u4f30\u9759\u6001\u6df1\u5ea6\u526a\u679d\u3001\u9759\u6001\u5bbd\u5ea6\u526a\u679d\u548c\u52a8\u6001\u526a\u679d\u4e09\u79cd\u7b56\u7565\uff0c\u572817\u4e2a\u4efb\u52a1\uff08\u5206\u7c7b\u3001\u751f\u6210\u3001\u63a8\u7406\uff09\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u8303\u5f0f\u4f9d\u8d56\u7684\u660e\u663e\u5dee\u5f02\uff1a\u6df1\u5ea6\u526a\u679d\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5bbd\u5ea6\u526a\u679d\u5728\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u66f4\u7a33\u5065\uff1b\u9759\u6001\u526a\u679d\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u63a8\u7406\u6027\u80fd\uff0c\u800c\u52a8\u6001\u526a\u679d\u5728\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u957f\u94fe\u63a8\u7406\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u63a8\u7406\u589e\u5f3a\u578bLLM\u9700\u8981\u4e13\u95e8\u8003\u8651\u5176\u7279\u6027\u7684\u526a\u679d\u7b56\u7565\uff0c\u4e0d\u80fd\u7b80\u5355\u5957\u7528\u6307\u4ee4\u9075\u5faa\u578b\u6a21\u578b\u7684\u526a\u679d\u65b9\u6cd5\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9488\u5bf9\u4e0d\u540c\u6a21\u578b\u8303\u5f0f\u8bbe\u8ba1\u5b9a\u5236\u5316\u526a\u679d\u65b9\u6848\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.18107", "categories": ["cs.LG", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18107", "abs": "https://arxiv.org/abs/2601.18107", "authors": ["Pedram Agand", "Mo Chen"], "title": "Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions", "comment": "11 pages, 2 figures, 2 tables", "summary": "Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.", "AI": {"tldr": "MoReBRAC\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6f5c\u5728\u7a7a\u95f4\u5408\u6210\u6765\u7f13\u89e3\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u968f\u673a\u548c\u6b21\u4f18\u6570\u636e\u573a\u666f\u4e0b\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u5de5\u4e1a\u673a\u5668\u4eba\uff09\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u9759\u6001\u6570\u636e\u96c6\u4e0e\u5b66\u4e60\u7b56\u7565\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u9ad8\u5ea6\u4fdd\u5b88\u7684\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u7b56\u7565\u6539\u8fdb\u7684\u6f5c\u529b\u3002", "method": "MoReBRAC\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u5faa\u73af\u4e16\u754c\u6a21\u578b\u5408\u6210\u9ad8\u4fdd\u771f\u5ea6\u8f6c\u6362\u6765\u6269\u5c55\u8bad\u7ec3\u6d41\u5f62\u3002\u4e3a\u786e\u4fdd\u5408\u6210\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e86\u5206\u5c42\u4e0d\u786e\u5b9a\u6027\u7ba1\u9053\uff0c\u96c6\u6210\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6d41\u5f62\u68c0\u6d4b\u3001\u6a21\u578b\u654f\u611f\u6027\u5206\u6790\u548c\u8499\u7279\u5361\u6d1bdropout\uff0c\u591a\u5c42\u8fc7\u6ee4\u786e\u4fdd\u53ea\u4f7f\u7528\u5b66\u4e60\u52a8\u6001\u9ad8\u7f6e\u4fe1\u533a\u57df\u7684\u8f6c\u6362\u3002", "result": "\u5728D4RL Gym-MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\"\u968f\u673a\"\u548c\"\u6b21\u4f18\"\u6570\u636e\u573a\u666f\u4e0b\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u4e86VAE\u4f5c\u4e3a\u51e0\u4f55\u951a\u70b9\u7684\u4f5c\u7528\uff0c\u5e76\u8ba8\u8bba\u4e86\u4ece\u63a5\u8fd1\u6700\u4f18\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u65f6\u7684\u5206\u5e03\u6743\u8861\u3002", "conclusion": "MoReBRAC\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6f5c\u5728\u7a7a\u95f4\u5408\u6210\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u6570\u636e\u8d28\u91cf\u8f83\u5dee\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18049", "abs": "https://arxiv.org/abs/2601.18049", "authors": ["Yunfei Qiu", "Qiqiong Ma", "Tianhua Lv", "Li Fang", "Shudong Zhou", "Wei Yao"], "title": "Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling", "comment": null, "summary": "Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u7a7a\u95f4\u5148\u9a8c\u4fe1\u606f\u4e0e\u52a8\u6001\u5b66\u4e60\u673a\u5236\u7684\u534a\u76d1\u7763\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u8d85\u50cf\u7d20\u6807\u7b7e\u4f20\u64ad\u6a21\u5757\u548c\u52a8\u6001\u5386\u53f2\u878d\u5408\u9884\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8fb9\u754c\u6807\u7b7e\u6269\u6563\u548c\u4f2a\u6807\u7b7e\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u6837\u672c\u6709\u9650\uff0c\u534a\u76d1\u7763\u5b66\u4e60\u9762\u4e34\u8fb9\u754c\u6807\u7b7e\u6269\u6563\u548c\u4f2a\u6807\u7b7e\u4e0d\u7a33\u5b9a\u6027\u7b49\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e9b\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u63d0\u5347\u5206\u7c7b\u9c81\u68d2\u6027\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "method": "1. \u8fb9\u7f18\u611f\u77e5\u8d85\u50cf\u7d20\u6807\u7b7e\u4f20\u64ad\u6a21\u5757\uff1a\u96c6\u6210\u8fb9\u7f18\u5f3a\u5ea6\u60e9\u7f5a\u4e0e\u90bb\u57df\u6821\u6b63\u7b56\u7565\uff0c\u7f13\u89e3\u8d85\u50cf\u7d20\u5206\u5272\u5bfc\u81f4\u7684\u6807\u7b7e\u6269\u6563\uff1b2. \u52a8\u6001\u5386\u53f2\u878d\u5408\u9884\u6d4b\u65b9\u6cd5\uff1a\u7ef4\u62a4\u5386\u53f2\u9884\u6d4b\u5e76\u52a8\u6001\u52a0\u6743\u5f53\u524d\u7ed3\u679c\uff0c\u5e73\u6ed1\u4f2a\u6807\u7b7e\u6ce2\u52a8\uff1b3. \u81ea\u9002\u5e94\u4e09\u5143\u6837\u672c\u5206\u7c7b\u7b56\u7565\uff1a\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u548c\u4e00\u81f4\u6027\u5ea6\u91cf\uff0c\u5206\u5c42\u5229\u7528\u6613\u3001\u6a21\u7cca\u548c\u96be\u6837\u672c\uff1b4. \u52a8\u6001\u53ef\u9760\u6027\u589e\u5f3a\u4f2a\u6807\u7b7e\u6846\u67b6\uff1a\u6574\u5408\u4e0a\u8ff0\u7ec4\u4ef6\u5b9e\u73b0\u65f6\u7a7a\u4e00\u81f4\u6027\u4f18\u5316\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4fdd\u6301\u4f18\u8d8a\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u754c\u6807\u7b7e\u6269\u6563\u548c\u4f2a\u6807\u7b7e\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u9c81\u68d2\u6027\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u7a7a\u95f4\u5148\u9a8c\u4fe1\u606f\u4e0e\u52a8\u6001\u5b66\u4e60\u673a\u5236\u7684\u534a\u76d1\u7763\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u6807\u7b7e\u4f20\u64ad\u548c\u52a8\u6001\u53ef\u9760\u6027\u589e\u5f3a\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.18110", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18110", "abs": "https://arxiv.org/abs/2601.18110", "authors": ["Pedram Zaree", "Md Abdullah Al Mamun", "Yue Dong", "Ihsen Alouani", "Nael Abu-Ghazaleh"], "title": "AttenMIA: LLM Membership Inference Attack through Attention Signals", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.", "AI": {"tldr": "AttenMIA\uff1a\u5229\u7528Transformer\u81ea\u6ce8\u610f\u529b\u6a21\u5f0f\u8fdb\u884c\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u65b0\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u6216\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u5728\u4f4e\u8bef\u62a5\u7387\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5e9e\u5927\u6027\u4f7f\u5176\u5bb9\u6613\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u4e25\u91cd\u7684\u9690\u79c1\u548c\u77e5\u8bc6\u4ea7\u6743\u95ee\u9898\u3002\u73b0\u6709\u6210\u5458\u63a8\u65ad\u653b\u51fb\u4e3b\u8981\u4f9d\u8d56\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u6216\u5d4c\u5165\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u53f7\u5f80\u5f80\u8106\u5f31\uff0c\u653b\u51fb\u6210\u529f\u7387\u6709\u9650\u3002", "method": "\u63d0\u51faAttenMIA\u6846\u67b6\uff0c\u5229\u7528Transformer\u6a21\u578b\u5185\u90e8\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5f0f\u63a8\u65ad\u6210\u5458\u8eab\u4efd\u3002\u6ce8\u610f\u529b\u63a7\u5236Transformer\u5185\u90e8\u4fe1\u606f\u6d41\uff0c\u63ed\u793a\u7528\u4e8e\u8bb0\u5fc6\u8bc6\u522b\u7684\u4e0d\u540c\u6a21\u5f0f\u3002\u65b9\u6cd5\u4f7f\u7528\u8de8\u5c42\u6ce8\u610f\u529b\u5934\u4fe1\u606f\uff0c\u7ed3\u5408\u57fa\u4e8e\u6270\u52a8\u7684\u53d1\u6563\u5ea6\u91cf\u6765\u8bad\u7ec3\u6709\u6548\u7684MIA\u5206\u7c7b\u5668\u3002", "result": "\u5728LLaMA-2\u3001Pythia\u548cOpt\u7b49\u5f00\u6e90\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7279\u5f81\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u91cd\u8981\u7684\u4f4e\u8bef\u62a5\u7387\u6307\u6807\u4e0b\uff08\u5982\u5728WikiMIA-32\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Llama2-13b\u8fbe\u52300.996 ROC AUC\u548c87.9% TPR@1%FPR\uff09\u3002\u6ce8\u610f\u529b\u4fe1\u53f7\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u67b6\u6784\u95f4\u5177\u6709\u6cdb\u5316\u6027\uff0c\u5c42\u548c\u5934\u7ea7\u5206\u6790\u663e\u793a\u6210\u5458\u4fe1\u606f\u6cc4\u9732\u6700\u660e\u663e\u7684\u4f4d\u7f6e\u3002\u5728\u6570\u636e\u63d0\u53d6\u6846\u67b6\u4e2d\u4f7f\u7528AttenMIA\u66ff\u6362\u5176\u4ed6\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff0c\u80fd\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u6570\u636e\u63d0\u53d6\u653b\u51fb\u3002", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u539f\u672c\u65e8\u5728\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u5374\u65e0\u610f\u4e2d\u653e\u5927\u4e86LLMs\u4e2d\u7684\u9690\u79c1\u98ce\u9669\uff0c\u7a81\u663e\u4e86\u5f00\u53d1\u65b0\u9632\u5fa1\u63aa\u65bd\u7684\u5fc5\u8981\u6027\u3002AttenMIA\u5c55\u793a\u4e86\u5229\u7528\u6a21\u578b\u5185\u90e8\u6ce8\u610f\u529b\u6a21\u5f0f\u8fdb\u884c\u6210\u5458\u63a8\u65ad\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u3002"}}
{"id": "2601.18088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18088", "abs": "https://arxiv.org/abs/2601.18088", "authors": ["Jianshu Chao", "Tianhua Lv", "Qiqiong Ma", "Yunfei Qiu", "Li Fang", "Huifang Shen", "Wei Yao"], "title": "Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification", "comment": null, "summary": "Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6e90\u57df\u6807\u6ce8\u7684\u81ea\u76d1\u7763\u8de8\u57df\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4-\u5149\u8c31Transformer\u548c\u9891\u7387\u57df\u7ea6\u675f\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u8054\u5408\u8868\u793a\uff0c\u5728\u76ee\u6807\u57df\u5c11\u91cf\u6837\u672c\u4e0b\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u8de8\u57df\u8fc1\u79fb\u573a\u666f\u4e2d\u4ecd\u4f9d\u8d56\u6e90\u57df\u6807\u6ce8\uff0c\u4e14\u5bf9\u5206\u5e03\u504f\u79fb\u654f\u611f\uff0c\u5bfc\u81f4\u5728\u76ee\u6807\u57df\u7684\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u5f00\u53d1\u65e0\u9700\u6e90\u57df\u6807\u7b7e\u4e14\u80fd\u5728\u76ee\u6807\u57df\u5c11\u91cf\u6837\u672c\u4e0b\u6709\u6548\u9002\u5e94\u7684\u8de8\u57df\u8fc1\u79fb\u65b9\u6cd5\u3002", "method": "1. \u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u9636\u6bb5\uff1a\u8bbe\u8ba1\u7a7a\u95f4-\u5149\u8c31Transformer\u6a21\u5757\uff0c\u91c7\u7528\u53cc\u5206\u652f\u7ed3\u6784\u548c\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u5149\u8c31-\u7a7a\u95f4\u534f\u540c\u5efa\u6a21\uff1b\u5f15\u5165\u9891\u7387\u57df\u7ea6\u675f\u901a\u8fc7\u5b9e\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u548c\u9ad8\u9891\u5e45\u5ea6\u635f\u5931\u4fdd\u6301\u9891\u57df\u4e00\u81f4\u6027\u30022. \u5fae\u8c03\u9636\u6bb5\uff1a\u63d0\u51fa\u6269\u6563\u5bf9\u9f50\u5fae\u8c03\u84b8\u998f\u673a\u5236\uff0c\u901a\u8fc7\u5e08\u751f\u7ed3\u6784\u5bf9\u9f50\u8bed\u4e49\u6f14\u5316\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4f4e\u6807\u7b7e\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u9ad8\u5149\u8c31\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7a33\u5b9a\u7684\u5206\u7c7b\u6027\u80fd\u548c\u5f3a\u5927\u7684\u8de8\u57df\u9002\u5e94\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u76d1\u7763\u8de8\u57df\u8fc1\u79fb\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u6e90\u57df\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u5149\u8c31-\u7a7a\u95f4\u8054\u5408\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u9891\u7387\u57df\u7ea6\u675f\u548c\u6269\u6563\u5bf9\u9f50\u5fae\u8c03\u673a\u5236\uff0c\u5728\u76ee\u6807\u57df\u5c11\u91cf\u6837\u672c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u9002\u5e94\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u9ad8\u5149\u8c31\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18111", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18111", "abs": "https://arxiv.org/abs/2601.18111", "authors": ["Jean Kossaifi", "Nikola Kovachki", "Morteza Mardani", "Daniel Leibovici", "Suman Ravuri", "Ira Shokar", "Edoardo Calvello", "Mohammad Shoaib Abbas", "Peter Harrington", "Ashay Subramaniam", "Noah Brenowitz", "Boris Bonev", "Wonmin Byeon", "Karsten Kreis", "Dale Durran", "Arash Vahdat", "Mike Pritchard", "Jan Kautz"], "title": "Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting", "comment": null, "summary": "The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u5c3a\u5ea6\u5927\u6c14\u52a8\u529b\u5b66\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u964d\u91c7\u6837\u6f5c\u5728\u7a7a\u95f4\u548c\u5386\u53f2\u6761\u4ef6\u5c40\u90e8\u6295\u5f71\u5668\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7269\u7406\u5efa\u6a21\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u6216\u4e13\u95e8\u8bad\u7ec3\u7b56\u7565\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6982\u7387\u9884\u62a5\u6280\u80fd\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u65b9\u6cd5\u5bfc\u81f4\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u788e\u7247\u5316\uff0c\u590d\u6742\u4e14\u5b9a\u5236\u5316\uff0c\u63a9\u76d6\u4e86\u9884\u62a5\u51c6\u786e\u6027\u7684\u57fa\u672c\u9a71\u52a8\u56e0\u7d20\u3002\u4f5c\u8005\u65e8\u5728\u8bc1\u660e\u6700\u5148\u8fdb\u7684\u6982\u7387\u9884\u62a5\u6280\u80fd\u4e0d\u9700\u8981\u590d\u6742\u7684\u67b6\u6784\u7ea6\u675f\u6216\u4e13\u95e8\u7684\u8bad\u7ec3\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u5c3a\u5ea6\u5927\u6c14\u52a8\u529b\u5b66\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u76f4\u63a5\u964d\u91c7\u6837\u7684\u6f5c\u5728\u7a7a\u95f4\u548c\u5386\u53f2\u6761\u4ef6\u5c40\u90e8\u6295\u5f71\u5668\u6765\u89e3\u6790\u9ad8\u5206\u8fa8\u7387\u7269\u7406\u3002\u8be5\u6846\u67b6\u8bbe\u8ba1\u5bf9\u6982\u7387\u4f30\u8ba1\u5668\u7684\u9009\u62e9\u5177\u6709\u9c81\u68d2\u6027\uff0c\u65e0\u7f1d\u652f\u6301\u968f\u673a\u63d2\u503c\u3001\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8eCRPS\u7684\u96c6\u6210\u8bad\u7ec3\u3002", "result": "\u4e0e\u96c6\u6210\u9884\u62a5\u7cfb\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u6982\u7387\u6a21\u578bGenCast\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u5927\u591a\u6570\u53d8\u91cf\u4e0a\u5b9e\u73b0\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\u3002\u9a8c\u8bc1\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u4e2d\u671f\u9884\u62a5\u6027\u80fd\u3002", "conclusion": "\u6269\u5c55\u901a\u7528\u6a21\u578b\u8db3\u4ee5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u4e2d\u671f\u9884\u6d4b\uff0c\u65e0\u9700\u5b9a\u5236\u5316\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u4e14\u5728\u6240\u6709\u6982\u7387\u6846\u67b6\u8303\u56f4\u5185\u90fd\u6709\u6548\u3002\u8fd9\u7b80\u5316\u4e86\u5929\u6c14\u9884\u62a5\u6a21\u578b\u7684\u5f00\u53d1\u6d41\u7a0b\u3002"}}
{"id": "2601.18098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18098", "abs": "https://arxiv.org/abs/2601.18098", "authors": ["Chuang Yang", "Haozhao Ma", "Xu Han", "Yuan Yuan", "Qi Wang"], "title": "Text-Pass Filter: An Efficient Scene Text Detector", "comment": null, "summary": "To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.", "AI": {"tldr": "TPF\u662f\u4e00\u79cd\u7528\u4e8e\u4efb\u610f\u5f62\u72b6\u6587\u672c\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u5e26\u901a\u6ee4\u6ce2\u5668\u539f\u7406\uff0c\u4e3a\u6bcf\u4e2a\u6587\u672c\u6784\u5efa\u7279\u5f81-\u6ee4\u6ce2\u5668\u5bf9\uff0c\u76f4\u63a5\u5206\u5272\u6574\u4e2a\u6587\u672c\u533a\u57df\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6536\u7f29-\u6269\u5c55\u7b56\u7565\u7684\u56fa\u6709\u5c40\u9650\uff0c\u5e76\u80fd\u81ea\u7136\u5206\u79bb\u7c98\u8fde\u6587\u672c\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u91c7\u7528\u6536\u7f29-\u63a9\u7801\u6269\u5c55\u7b56\u7565\uff0c\u4f46\u6536\u7f29\u64cd\u4f5c\u4f1a\u4e22\u5931\u6587\u672c\u8fb9\u7f18\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u6df7\u6dc6\u524d\u666f\u4e0e\u80cc\u666f\u5dee\u5f02\uff0c\u8fd9\u7ed9\u6587\u672c\u7279\u5f81\u8bc6\u522b\u5e26\u6765\u4e86\u56fa\u6709\u5c40\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u76f4\u63a5\u5206\u5272\u6574\u4e2a\u6587\u672c\u533a\u57df\u4e14\u80fd\u81ea\u7136\u5206\u79bb\u7c98\u8fde\u6587\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6587\u672c\u901a\u6ee4\u6ce2\u5668\uff08TPF\uff09\uff0c\u6a21\u62df\u5e26\u901a\u6ee4\u6ce2\u5668\u539f\u7406\uff0c\u4e3a\u6bcf\u4e2a\u6587\u672c\u6784\u5efa\u72ec\u7279\u7684\u7279\u5f81-\u6ee4\u6ce2\u5668\u5bf9\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u6bcf\u4e2a\u6ee4\u6ce2\u5668\u901a\u8fc7\u5176\u901a\u5e26\u7279\u5f81\u5e76\u963b\u6321\u5176\u4ed6\u7279\u5f81\u6765\u63d0\u53d6\u5339\u914d\u7684\u6587\u672c\u3002\u8bbe\u8ba1\u4e86\u5f3a\u5316\u96c6\u6210\u5355\u5143\uff08REU\uff09\u589e\u5f3a\u540c\u4e00\u6587\u672c\u7684\u7279\u5f81\u4e00\u81f4\u6027\u5e76\u6269\u5927\u6ee4\u6ce2\u5668\u7684\u8bc6\u522b\u8303\u56f4\uff0c\u4ee5\u53ca\u524d\u666f\u5148\u9a8c\u5355\u5143\uff08FPU\uff09\u6539\u5584\u524d\u666f\u4e0e\u80cc\u666f\u7684\u533a\u5206\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86REU\u548cFPU\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86TPF\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u7136\u5206\u79bb\u7c98\u8fde\u6587\u672c\uff0c\u65e0\u9700\u590d\u6742\u7684\u89e3\u7801\u6216\u540e\u5904\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6587\u672c\u68c0\u6d4b\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "TPF\u901a\u8fc7\u6a21\u62df\u5e26\u901a\u6ee4\u6ce2\u5668\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u7684\u56fa\u6709\u5c40\u9650\uff0c\u80fd\u591f\u76f4\u63a5\u5206\u5272\u6574\u4e2a\u6587\u672c\u533a\u57df\u5e76\u81ea\u7136\u5206\u79bb\u7c98\u8fde\u6587\u672c\uff0c\u4e3a\u5b9e\u65f6\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18115", "categories": ["cs.LG", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.18115", "abs": "https://arxiv.org/abs/2601.18115", "authors": ["Guyang Cao", "Shuyao Li", "Sushrut Karmalkar", "Jelena Diakonikolas"], "title": "Robust Learning of a Group DRO Neuron", "comment": null, "summary": "We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\\mathcal p_{[1]},\\dots,\\mathcal p_{[K]}$, we seek to approximate $\\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\\boldsymbol\u03bb \\in \u0394_K$, where the objective is $\\sum_{i \\in [K]}\u03bb_{[i]}\\,\\mathbb E_{(\\mathbf x,y)\\sim\\mathcal p_{[i]}}(\u03c3(\\mathbf w\\cdot\\mathbf x)-y)^2 - \u03bdd_f(\\boldsymbol\u03bb,\\frac{1}{K}\\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $\u03bd\\geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\\widehat{\\mathbf w}$ that is constant-factor competitive with $\\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5728\u4efb\u610f\u6807\u7b7e\u566a\u58f0\u548c\u7ec4\u7ea7\u5206\u5e03\u504f\u79fb\u4e0b\u5b66\u4e60\u5355\u4e2a\u795e\u7ecf\u5143\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u7fa4\u4f53\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u5bf9\u5076\u7b97\u6cd5\u83b7\u5f97\u4e0e\u6700\u4f18\u53c2\u6570\u7ade\u4e89\u7684\u89e3\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5e38\u5b58\u5728\u6807\u7b7e\u566a\u58f0\u548c\u5206\u5e03\u504f\u79fb\uff0c\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u5f00\u53d1\u9c81\u68d2\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u7ec4\u6743\u91cd\u5206\u914d\u4e2d\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7fa4\u4f53\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528f-\u6563\u5ea6\u60e9\u7f5a\u504f\u79bb\u5747\u5300\u7ec4\u6743\u91cd\u3002\u5f00\u53d1\u8ba1\u7b97\u9ad8\u6548\u7684\u539f\u5bf9\u5076\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u5916\u63a8\u66f4\u65b0\u5904\u7406\u635f\u5931\u51fd\u6570\u7684\u975e\u51f8\u6027\uff0c\u76f4\u63a5\u5bf9\u6297\u4efb\u610f\u6807\u7b7e\u635f\u574f\u548c\u7ec4\u7279\u5b9a\u5206\u5e03\u504f\u79fb\u3002", "result": "\u7b97\u6cd5\u8f93\u51fa\u5411\u91cf\u5728\u5e38\u6570\u56e0\u5b50\u8303\u56f4\u5185\u4e0e\u6700\u4f18\u53c2\u6570\u7ade\u4e89\uff0c\u5728\u6700\u574f\u60c5\u51b5\u7ec4\u6743\u91cd\u4e0b\u4fdd\u6301\u6027\u80fd\u3002\u7b97\u6cd5\u6846\u67b6\u5728LLM\u9884\u8bad\u7ec3\u57fa\u51c6\u4e0a\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u5728\u5b58\u5728\u4efb\u610f\u6807\u7b7e\u566a\u58f0\u548c\u5206\u5e03\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5b66\u4e60\u5355\u4e2a\u795e\u7ecf\u5143\uff0c\u4e3a\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u7684\u5206\u5e03\u53d8\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\u3002"}}
{"id": "2601.18099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18099", "abs": "https://arxiv.org/abs/2601.18099", "authors": ["Akbar Saadat"], "title": "Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs", "comment": "9 pages, 14 input images, 3 TikZ images. arXiv admin note: substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779", "summary": "Following the earlier verification for Gaussian model in \\cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\\%$, obtained by applying the extracted defocus filters to less blurred images.", "AI": {"tldr": "\u63d0\u51fa\u96f6\u8bad\u7ec3\u524d\u5411\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u6790\u8868\u8fbe\u5f0f\u79bb\u6563\u8ba1\u7b97\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u65af\u6a21\u578b\u53bb\u6a21\u7cca\uff0c\u5728\u771f\u5b9e\u56fe\u50cf\u4e0aMAE\u4f4e\u4e8e1.7%", "motivation": "\u5728\u5148\u524d\u9ad8\u65af\u6a21\u578b\u9a8c\u8bc1\u57fa\u7840\u4e0a\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u7684\u5feb\u901f\u8ba1\u7b97\u6846\u67b6\uff0c\u5904\u7406\u4e24\u5e45\u56fe\u50cf\u4e92\u4e3a\u90e8\u5206\u6a21\u7cca\u7248\u672c\u7684\u60c5\u51b5", "method": "\u57fa\u4e8e\u9ad8\u65af\u6838\u6807\u51c6\u5dee\u5e94\u7528\u8303\u56f4\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u79bb\u6563\u8ba1\u7b97\uff0c\u4ece\u6e05\u6670\u56fe\u50cf\u8ba1\u7b97\u6563\u7126\u56fe\u50cf\uff0c\u901a\u8fc7\u90bb\u57df\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u7b5b\u9009\u591a\u89e3", "result": "\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\uff0c\u5408\u6210\u6a21\u7cca\u503c\u4f30\u8ba1\u7684MAE\u4f4e\u4e8e1.7%\uff0c\u5b9e\u9645\u6a21\u7cca\u56fe\u50cf\u5f3a\u5ea6\u4e0e\u4f30\u8ba1\u503c\u7684\u5dee\u5f02\u4fdd\u6301\u57282%\u4ee5\u5185", "conclusion": "\u63d0\u51fa\u7684\u96f6\u8bad\u7ec3\u6846\u67b6\u80fd\u6709\u6548\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u65af\u6a21\u578b\u53bb\u6a21\u7cca\uff0c\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u7528\u6027"}}
{"id": "2601.18142", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18142", "abs": "https://arxiv.org/abs/2601.18142", "authors": ["Mingxu Zhang", "Huicheng Zhang", "Jiaming Ji", "Yaodong Yang", "Ying Sun"], "title": "Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods", "comment": null, "summary": "Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\\%, establishing superior effectiveness for Safe RL in complex environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faADRC-Lagrangian\u65b9\u6cd5\uff0c\u5c06\u4e3b\u52a8\u6297\u6270\u63a7\u5236\u5f15\u5165\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u5b89\u5168\u8fdd\u89c4\u548c\u632f\u8361\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u5347\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u5305\u62ecPID\u548c\u7ecf\u5178Lagrangian\u65b9\u6cd5\uff09\u5b58\u5728\u632f\u8361\u9891\u7e41\u3001\u5b89\u5168\u8fdd\u89c4\u591a\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u53c2\u6570\u654f\u611f\u6027\u548c\u56fa\u6709\u7684\u76f8\u4f4d\u6ede\u540e\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faADRC-Lagrangian\u65b9\u6cd5\uff0c\u5c06\u4e3b\u52a8\u6297\u6270\u63a7\u5236\uff08ADRC\uff09\u6280\u672f\u878d\u5165Lagrangian\u6846\u67b6\uff0c\u589e\u5f3a\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u632f\u8361\uff0c\u8be5\u7edf\u4e00\u6846\u67b6\u5305\u542b\u7ecf\u5178\u548cPID Lagrangian\u65b9\u6cd5\u4f5c\u4e3a\u7279\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5c06\u5b89\u5168\u8fdd\u89c4\u51cf\u5c1174%\uff0c\u7ea6\u675f\u8fdd\u89c4\u5e45\u5ea6\u964d\u4f4e89%\uff0c\u5e73\u5747\u6210\u672c\u964d\u4f4e67%\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u3002", "conclusion": "ADRC-Lagrangian\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u4e3b\u52a8\u6297\u6270\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7684\u632f\u8361\u548c\u5b89\u5168\u8fdd\u89c4\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18100", "abs": "https://arxiv.org/abs/2601.18100", "authors": ["James Tribble", "Hao Wang", "Si-En Hong", "Chaoyi Zhou", "Ashish Bastola", "Siyu Huang", "Abolfazl Razi"], "title": "Spatial-Conditioned Reasoning in Long-Egocentric Videos", "comment": null, "summary": "Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u663e\u5f0f\u7a7a\u95f4\u4fe1\u53f7\u5982\u4f55\u5f71\u54cd\u57fa\u4e8eVLM\u7684\u957f\u65f6\u5e8f\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u5f15\u5165Sanpo-D\u6570\u636e\u96c6\u548c\u878d\u5408\u6df1\u5ea6\u56fe\u6765\u8bc4\u4f30\u7a7a\u95f4\u63a8\u7406\u80fd\u529b", "motivation": "\u957f\u65f6\u5e8f\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u5b58\u5728\u89c6\u89d2\u6f02\u79fb\u548c\u7f3a\u4e4f\u6301\u4e45\u51e0\u4f55\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u65f6\u5e8f\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u7814\u7a76\u663e\u5f0f\u7a7a\u95f4\u4fe1\u53f7\u5bf9VLM\u89c6\u9891\u7406\u89e3\u7684\u5f71\u54cd", "method": "1) \u5f15\u5165Sanpo-D\u6570\u636e\u96c6\uff08Google Sanpo\u6570\u636e\u96c6\u7684\u7ec6\u7c92\u5ea6\u91cd\u65b0\u6807\u6ce8\uff09\uff1b2) \u5728\u5bfc\u822a\u5bfc\u5411\u7684\u7a7a\u95f4\u67e5\u8be2\u4e0a\u5bf9\u591a\u4e2aVLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1b3) \u878d\u5408\u6df1\u5ea6\u56fe\u4e0eRGB\u5e27\u6765\u7814\u7a76\u8f93\u5165\u7ea7\u5f52\u7eb3\u504f\u7f6e", "result": "\u7ed3\u679c\u663e\u793a\u901a\u7528\u51c6\u786e\u6027\u4e0e\u7a7a\u95f4\u4e13\u4e1a\u5316\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u6df1\u5ea6\u611f\u77e5\u548c\u7a7a\u95f4\u57fa\u7840\u8868\u793a\u53ef\u4ee5\u63d0\u5347\u884c\u4eba\u68c0\u6d4b\u548c\u969c\u788d\u7269\u68c0\u6d4b\u7b49\u5b89\u5168\u5173\u952e\u4efb\u52a1\u7684\u6027\u80fd", "conclusion": "\u663e\u5f0f\u7a7a\u95f4\u4fe1\u53f7\u80fd\u591f\u589e\u5f3aVLM\u5728\u957f\u65f6\u5e8f\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u6df1\u5ea6\u611f\u77e5\u8868\u793a\u5bf9\u5b89\u5168\u5173\u952e\u4efb\u52a1\u6709\u79ef\u6781\u5f71\u54cd"}}
{"id": "2601.18150", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18150", "abs": "https://arxiv.org/abs/2601.18150", "authors": ["Zhaopeng Qiu", "Shuang Yu", "Jingqi Zhang", "Shuai Zhang", "Xue Huang", "Jingyi Yang", "Junjie Lai"], "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684FP8 rollout\u5806\u6808\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7FP8\u91cf\u5316\u548cKV\u7f13\u5b58\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe44%\u7684\u751f\u6210\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eBF16\u57fa\u7ebf\u76f8\u5f53\u7684\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u957f\u8f93\u51fa\u5e8f\u5217\u7684\u751f\u6210\u8fc7\u7a0b\u6210\u4e3a\u74f6\u9888\uff0c\u6ce8\u610f\u529b\u673a\u5236\u548cKV\u7f13\u5b58\u5185\u5b58\u6d88\u8017\u4e3b\u5bfc\u4e86\u7aef\u5230\u7aef\u65f6\u95f4\u3002FP8\u63d0\u4f9b\u4e86\u52a0\u901fRL\u7684\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u5de5\u7a0b\u548c\u7b97\u6cd5\u6311\u6218\uff1a\u7b56\u7565\u6743\u91cd\u9891\u7e41\u53d8\u5316\u9700\u8981\u91cd\u590d\u91cf\u5316\uff0c\u4f4e\u7cbe\u5ea6\u751f\u6210\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3-\u63a8\u7406\u4e0d\u5339\u914d\u3002", "method": "1) \u4f7f\u7528\u5757\u72b6FP8\u91cf\u5316\u5b9e\u73b0FP8 W8A8\u7ebf\u6027\u5c42\u751f\u6210\uff1b2) \u901a\u8fc7\u6bcf\u6b65QKV\u5c3a\u5ea6\u91cd\u65b0\u6821\u51c6\u5c06FP8\u6269\u5c55\u5230KV\u7f13\u5b58\uff0c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5185\u5b58\u74f6\u9888\uff1b3) \u4f7f\u7528\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7684\u751f\u6210\u6821\u6b63\uff08token\u7ea7TIS/MIS\u53d8\u4f53\uff09\u7f13\u89e3\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u5bc6\u96c6\u548cMoE\u6a21\u578b\u4e0a\uff0c\u8fd9\u4e9b\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u8fbe44%\u7684\u751f\u6210\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eBF16\u57fa\u7ebf\u76f8\u5f53\u7684\u5b66\u4e60\u884c\u4e3a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684FP8\u751f\u6210\u5806\u6808\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u751f\u6210\u74f6\u9888\uff0c\u901a\u8fc7FP8\u91cf\u5316\u3001KV\u7f13\u5b58\u4f18\u5316\u548c\u6821\u6b63\u6280\u672f\uff0c\u5728\u4fdd\u6301\u5b66\u4e60\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2601.18118", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18118", "abs": "https://arxiv.org/abs/2601.18118", "authors": ["Daeyoung Kim"], "title": "LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment", "comment": null, "summary": "Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLungCRCT\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u8868\u793a\u5b66\u4e60\u5206\u6790\u80ba\u764c\u8fdb\u5c55\u7684\u7269\u7406\u56e0\u679c\u673a\u5236\uff0c\u5b9e\u73b0\u56e0\u679c\u5e72\u9884\u5206\u6790\u548c\u8f7b\u91cf\u7ea7\u80bf\u7624\u5206\u7c7b\uff0cAUC\u8fbe93.91%", "motivation": "\u80ba\u764c\u65e9\u671f\u75c7\u72b6\u4e0d\u660e\u663e\u4e14\u6613\u4e0e\u5176\u4ed6\u547c\u5438\u75be\u75c5\u6df7\u6dc6\uff0c\u5bfc\u81f4\u65e9\u671f\u8bca\u65ad\u56f0\u96be\u3002\u867d\u7136LDCT\u626b\u63cf\u548cCNN/ViT\u6a21\u578b\u5728\u80ba\u764c\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7531\u4e8e\u76f8\u5173\u4f9d\u8d56\u6027\u9650\u5236\u548c\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u6cbb\u7597\u5206\u6790\u548c\u56e0\u679c\u5e72\u9884\u6a21\u62df", "method": "\u63d0\u51faLungCRCT\u6846\u67b6\uff0c\u57fa\u4e8e\u56fe\u81ea\u7f16\u7801\u5668\u7684\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\uff0c\u7ed3\u5408\u8ddd\u79bb\u76f8\u5173\u89e3\u7ea0\u7f20\u548c\u57fa\u4e8e\u71b5\u7684\u56fe\u50cf\u91cd\u5efa\u7ec6\u5316\uff0c\u4ece\u80ba\u764c\u8fdb\u5c55\u7684\u7269\u7406\u56e0\u679c\u673a\u5236\u4e2d\u63d0\u53d6\u56e0\u679c\u8868\u793a", "result": "\u6846\u67b6\u4e0d\u4ec5\u652f\u6301\u80ba\u764c\u6cbb\u7597\u7684\u56e0\u679c\u5e72\u9884\u5206\u6790\uff0c\u8fd8\u5728\u6076\u6027\u80bf\u7624\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8693.91%\u7684AUC\u5f97\u5206\uff0c\u4e14\u4e0b\u6e38\u6a21\u578b\u6781\u5176\u8f7b\u91cf", "conclusion": "LungCRCT\u901a\u8fc7\u56e0\u679c\u8868\u793a\u5b66\u4e60\u514b\u670d\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u80ba\u764c\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u6846\u67b6\uff0c\u652f\u6301\u6cbb\u7597\u5e72\u9884\u6a21\u62df\u548c\u9ad8\u6548\u7684\u80bf\u7624\u5206\u7c7b"}}
{"id": "2601.18171", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18171", "abs": "https://arxiv.org/abs/2601.18171", "authors": ["Yuguang Zhang", "Lijun Sheng", "Jian Liang", "Ran He"], "title": "Learning Fair Domain Adaptation with Virtual Label Distribution", "comment": "ICASSP 2026", "summary": "Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.", "AI": {"tldr": "VILL\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u548cKL\u6563\u5ea6\u518d\u5e73\u8861\u7b56\u7565\uff0c\u89e3\u51b3\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4e2d\u7684\u7c7b\u522b\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6700\u5dee\u7c7b\u522b\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u9ad8\u603b\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\u5927\u591a\u5173\u6ce8\u603b\u4f53\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4f46\u5ffd\u89c6\u4e86\u4e0d\u540c\u7c7b\u522b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff08\u7c7b\u522b\u516c\u5e73\u6027\u95ee\u9898\uff09\u3002\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0UDA\u5206\u7c7b\u5668\u503e\u5411\u4e8e\u504f\u5411\u6613\u5206\u7c7b\u522b\u800c\u5ffd\u89c6\u96be\u5206\u7c7b\u522b\u3002", "method": "\u63d0\u51fa\u865a\u62df\u6807\u7b7e\u5206\u5e03\u611f\u77e5\u5b66\u4e60\uff08VILL\uff09\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u7b56\u7565\uff08\u653e\u5927\u96be\u5206\u7c7b\u522b\u5f71\u54cd\uff09\u548c\u57fa\u4e8eKL\u6563\u5ea6\u7684\u518d\u5e73\u8861\u7b56\u7565\uff08\u663e\u5f0f\u8c03\u6574\u51b3\u7b56\u8fb9\u754c\u4ee5\u589e\u5f3a\u7c7b\u522b\u516c\u5e73\u6027\uff09\u3002", "result": "\u5728\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVILL\u53ef\u4ee5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709UDA\u65b9\u6cd5\u4e2d\uff0c\u663e\u8457\u6539\u5584\u7c7b\u522b\u516c\u5e73\u6027\u3002", "conclusion": "VILL\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u89e3\u51b3UDA\u4e2d\u7684\u7c7b\u522b\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6700\u5dee\u7c7b\u522b\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u9ad8\u603b\u4f53\u51c6\u786e\u7387\u3002"}}
{"id": "2601.18135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18135", "abs": "https://arxiv.org/abs/2601.18135", "authors": ["Jiahao Lyu", "Minghua Zhao", "Xuewen Huang", "Yifei Chen", "Shuangli Du", "Jing Hu", "Cheng Shi", "Zhiyong Lv"], "title": "Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection", "comment": "It has been submitted to the KBS journal", "summary": "As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.", "AI": {"tldr": "FoGA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u524d\u5411\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u95e8\u63a7\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u4ec5\u542b\u7ea6200\u4e07\u53c2\u6570\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5927\u89c4\u6a21\u6a21\u578b\u8ffd\u6c42\u6781\u81f4\u7cbe\u5ea6\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u53ef\u884c\u6027\u3002\u540c\u65f6\u4e3b\u6d41\u9884\u6d4b\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u5e27\u672a\u6765\u9884\u6d4b\u8bef\u5dee\u68c0\u6d4b\u5f02\u5e38\uff0c\u5ffd\u7565\u4e86\u66f4\u957f\u671f\u65f6\u95f4\u524d\u5411\u4fe1\u606f\u7684\u4e30\u5bcc\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eUnet\u7684\u65b9\u6cd5\uff0c\u5bf9\u8fde\u7eed\u5e27\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u751f\u6210\u5373\u65f6\u548c\u524d\u5411\u9884\u6d4b\uff1b\u5728\u8df3\u8dc3\u8fde\u63a5\u4e2d\u5f15\u5165\u95e8\u63a7\u4e0a\u4e0b\u6587\u805a\u5408\u6a21\u5757\uff0c\u52a8\u6001\u878d\u5408\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7279\u5f81\uff1b\u4f7f\u7528\u65b0\u9896\u7684\u524d\u5411\u4e00\u81f4\u6027\u635f\u5931\u8054\u5408\u4f18\u5316\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u5f02\u5e38\u6d4b\u91cf\u7b56\u7565\u6574\u5408\u5373\u65f6\u548c\u524d\u5411\u5e27\u7684\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fd0\u884c\u901f\u5ea6\u53ef\u8fbe155 FPS\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u6307\u6807\u95f4\u5b9e\u73b0\u4e86\u4f18\u79c0\u7684\u5e73\u8861\u3002", "conclusion": "FoGA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u524d\u5411\u4e00\u81f4\u6027\u5b66\u4e60\u548c\u95e8\u63a7\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7387\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2601.18189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18189", "abs": "https://arxiv.org/abs/2601.18189", "authors": ["Rui Wu", "Yongjun Li"], "title": "Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients", "comment": "20 pages, 8 figures", "summary": "Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.", "AI": {"tldr": "\u63d0\u51faAHOC\u6df7\u5408\u9636\u65e0\u73af\u7ea6\u675f\u548cSPG-AHOC\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fd1\u7aef\u7b97\u6cd5\u7684\u6d41\u5f62\u8bc6\u522b\u7279\u6027\uff0c\u7406\u8bba\u4e0a\u4fdd\u8bc1\u6709\u9650\u65f6\u95f4\u5185\u7cbe\u786e\u6062\u590dDAG\u7ed3\u6784\uff0c\u65e0\u9700\u540e\u5904\u7406\u9608\u503c\u5316", "motivation": "\u73b0\u6709\u8fde\u7eed\u4f18\u5316\u65b9\u6cd5\uff08\u5982NOTEARS\uff09\u901a\u5e38\u53ea\u80fd\u4fdd\u8bc1\u6e10\u8fd1\u6536\u655b\u5230\u9a7b\u70b9\uff0c\u4ea7\u751f\u7a20\u5bc6\u6743\u91cd\u77e9\u9635\uff0c\u9700\u8981\u4efb\u610f\u7684\u540e\u5904\u7406\u9608\u503c\u5316\u624d\u80fd\u6062\u590dDAG\u3002\u8fde\u7eed\u4f18\u5316\u4e0e\u79bb\u6563\u56fe\u7ed3\u6784\u4e4b\u95f4\u7684\u5dee\u8ddd\u662f\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u9636\u65e0\u73af\u7ea6\u675f\uff08AHOC\uff09\uff0c\u901a\u8fc7\u5e73\u6ed1\u8fd1\u7aef\u68af\u5ea6\uff08SPG-AHOC\uff09\u8fdb\u884c\u4f18\u5316\u3002\u5229\u7528\u8fd1\u7aef\u7b97\u6cd5\u7684\u6d41\u5f62\u8bc6\u522b\u7279\u6027\uff0c\u5728\u6807\u51c6\u53ef\u8bc6\u522b\u6027\u5047\u8bbe\u4e0b\uff0c\u7b97\u6cd5\u80fd\u5728\u6709\u9650\u8fed\u4ee3\u4e2d\u7cbe\u786e\u6062\u590dDAG\u652f\u6491\uff08\u7ed3\u6784\uff09\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6709\u9650\u65f6\u95f4\u9884\u8a00\u5c5e\u6027\uff1aSPG-AHOC\u5728\u6709\u9650\u8fed\u4ee3\u4e2d\u6062\u590d\u7cbe\u786e\u7684DAG\u652f\u6491\u7ed3\u6784\uff0c\u5373\u4f7f\u4f18\u5316\u7684\u662f\u5e73\u6ed1\u8fd1\u4f3c\u3002\u7b97\u6cd5\u8fd4\u56de\u5177\u6709\u7cbe\u786e\u96f6\u6761\u76ee\u7684\u56fe\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u622a\u65ad\u3002\u5b9e\u9a8c\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u5f3a\u6709\u529b\u5730\u8bc1\u5b9e\u4e86\u6709\u9650\u65f6\u95f4\u8bc6\u522b\u7406\u8bba\u3002", "conclusion": "\u901a\u8fc7AHOC\u7ea6\u675f\u548cSPG-AHOC\u7b97\u6cd5\uff0c\u6210\u529f\u5f25\u5408\u4e86\u8fde\u7eed\u4f18\u5316\u4e0e\u79bb\u6563\u56fe\u7ed3\u6784\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u6709\u9650\u65f6\u95f4\u5185\u7cbe\u786e\u6062\u590dDAG\u7ed3\u6784\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u6d88\u9664\u4e86\u7ed3\u6784\u6a21\u7cca\u6027\u3002"}}
{"id": "2601.18168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18168", "abs": "https://arxiv.org/abs/2601.18168", "authors": ["Zehua Liu", "Shihao Zou", "Jincai Huang", "Yanfang Zhang", "Chao Tong", "Weixin Si"], "title": "TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration", "comment": "Accepted by IEEE BIBM 2025", "summary": "Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\\% lower MSE and 17.7\\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \\textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ecf\u52a8\u8109\u5316\u7597\u6813\u585e\u672f\u7684\u7c97\u5230\u7ec6\u8840\u7ba1\u914d\u51c6\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ed3\u6784\u611f\u77e5\u900f\u89c6n\u70b9\u5168\u5c40\u5bf9\u9f50\u548c\u65f6\u5e8f\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e862D-3D\u8840\u7ba1\u914d\u51c6\u7684\u51c6\u786e\u6027\u548c\u89e3\u5256\u5408\u7406\u6027\u3002", "motivation": "\u7ecf\u52a8\u8109\u5316\u7597\u6813\u585e\u672f\u662f\u809d\u764c\u7b49\u809d\u810f\u6076\u6027\u80bf\u7624\u7684\u9996\u9009\u6cbb\u7597\u65b9\u6cd5\uff0c\u4f46\u7531\u4e8e\u672f\u4e2d\u8840\u7ba1\u5bfc\u822a\u590d\u6742\u548c\u89e3\u5256\u7ed3\u6784\u53d8\u5f02\u5927\uff0c\u8be5\u624b\u672f\u6781\u5177\u6311\u6218\u6027\u3002\u7cbe\u786e\u7a33\u5065\u76842D-3D\u8840\u7ba1\u914d\u51c6\u5bf9\u4e8e\u6307\u5bfc\u5fae\u5bfc\u7ba1\u548c\u5668\u68b0\u5b9a\u4f4d\u3001\u5b9e\u73b0\u7cbe\u51c6\u6cbb\u7597\u9776\u5411\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u7c97\u5230\u7ec6\u914d\u51c6\u7b56\u7565\uff1a1\uff09\u5168\u5c40\u5bf9\u9f50\u6a21\u5757SA-PnP\uff0c\u5efa\u7acb2D-3D\u8840\u7ba1\u7ed3\u6784\u5bf9\u5e94\u5173\u7cfb\uff1b2\uff09TempDiffReg\u65f6\u5e8f\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u65f6\u5e8f\u4e0a\u4e0b\u6587\u8fed\u4ee3\u8fdb\u884c\u8840\u7ba1\u53d8\u5f62\uff0c\u6355\u6349\u590d\u6742\u89e3\u5256\u53d8\u5f02\u548c\u5c40\u90e8\u7ed3\u6784\u53d8\u5316\u3002", "result": "\u572823\u540d\u60a3\u8005626\u4e2a\u591a\u5e27\u6837\u672c\u4e0a\u8bc4\u4f30\uff0c\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u89e3\u5256\u5408\u7406\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u5177\u4f53\u6307\u6807\uff1a\u5747\u65b9\u8bef\u5dee0.63mm\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee0.51mm\uff0c\u76f8\u6bd4\u6700\u4f18\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u964d\u4f4e66.7%\u548c17.7%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u8840\u7ba1\u914d\u51c6\u7cbe\u5ea6\uff0c\u6709\u52a9\u4e8e\u7ecf\u9a8c\u4e0d\u8db3\u7684\u4e34\u5e8a\u533b\u751f\u5b89\u5168\u9ad8\u6548\u5730\u6267\u884c\u590d\u6742TACE\u624b\u672f\uff0c\u6700\u7ec8\u6539\u5584\u624b\u672f\u6548\u679c\u548c\u60a3\u8005\u62a4\u7406\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2601.18172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18172", "abs": "https://arxiv.org/abs/2601.18172", "authors": ["Lin Huang", "Yujuan Tan", "Weisheng Li", "Shitai Shan", "Liu Liu", "Bo Liu", "Linlin Shen", "Jing Yu", "Yue Niu"], "title": "YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection", "comment": null, "summary": "One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.", "AI": {"tldr": "YOLO-DS\u901a\u8fc7\u53cc\u7edf\u8ba1\u534f\u540c\u7b97\u5b50(DSO)\u89e3\u8026\u7269\u4f53\u7279\u5f81\uff0c\u7ed3\u5408\u901a\u9053\u5747\u503c\u4e0e\u5cf0\u5747\u5dee\u5efa\u6a21\uff0c\u914d\u5408DSG\u548cMSG\u95e8\u63a7\u6a21\u5757\uff0c\u5728YOLO\u7cfb\u5217\u4e0a\u5b9e\u73b01.1-1.7% AP\u63d0\u5347\uff0c\u63a8\u7406\u5ef6\u8fdf\u4ec5\u8f7b\u5fae\u589e\u52a0\u3002", "motivation": "\u73b0\u6709YOLO\u68c0\u6d4b\u5668\u7f3a\u4e4f\u5bf9\u5171\u4eab\u7279\u5f81\u901a\u9053\u4e2d\u5f02\u6784\u7269\u4f53\u54cd\u5e94\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u63d0\u51faYOLO-DS\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u53cc\u7edf\u8ba1\u534f\u540c\u7b97\u5b50(DSO)\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u901a\u9053\u5747\u503c\u548c\u5cf0\u5747\u5dee\u6765\u89e3\u8026\u7269\u4f53\u7279\u5f81\u3002\u57fa\u4e8eDSO\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u8f7b\u91cf\u95e8\u63a7\u6a21\u5757\uff1a\u53cc\u7edf\u8ba1\u534f\u540c\u95e8\u63a7(DSG)\u7528\u4e8e\u81ea\u9002\u5e94\u901a\u9053\u7279\u5f81\u9009\u62e9\uff0c\u591a\u8def\u5f84\u5206\u6bb5\u95e8\u63a7(MSG)\u7528\u4e8e\u6df1\u5ea6\u7279\u5f81\u52a0\u6743\u3002", "result": "\u5728MS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cYOLO-DS\u5728\u4e94\u4e2a\u6a21\u578b\u5c3a\u5ea6(N,S,M,L,X)\u4e0a\u5747\u4f18\u4e8eYOLOv8\uff0cAP\u63d0\u53471.1%\u81f31.7%\uff0c\u63a8\u7406\u5ef6\u8fdf\u4ec5\u8f7b\u5fae\u589e\u52a0\u3002\u53ef\u89c6\u5316\u3001\u6d88\u878d\u548c\u5bf9\u6bd4\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "YOLO-DS\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5f02\u6784\u7269\u4f53\u54cd\u5e94\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u5f02\u6784\u7269\u4f53\u5224\u522b\u65b9\u9762\u7684\u4f18\u8d8a\u80fd\u529b\u3002"}}
{"id": "2601.18231", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18231", "abs": "https://arxiv.org/abs/2601.18231", "authors": ["Trong Khiem Tran", "Manh Cuong Dao", "Phi Le Nguyen", "Thao Nguyen Truong", "Trong Nghia Hoang"], "title": "Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting", "comment": "Accepted AISTATS 20226. Preprint version", "summary": "Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u65b0\u7279\u5f81\u6a21\u6001\u65f6\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u62df\u5408\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u7279\u5f81\u6807\u7b7e\u5931\u771f\u6982\u5ff5\u5efa\u7acb\u4e86\u53ef\u8bc1\u660e\u7684\u6cdb\u5316\u8fb9\u754c\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u6539\u8fdb\u7684\u7b97\u6cd5\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u7279\u5f81\u6a21\u6001\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7279\u5f81\u5bf9\u9f50\u548c\u76ee\u6807\u62df\u5408\u4e4b\u95f4\u5173\u952e\u4ea4\u4e92\u4f5c\u7528\u7684\u7406\u8bba\u7406\u89e3\uff0c\u672a\u6821\u51c6\u7684\u7ec4\u5408\u4f1a\u52a0\u5267\u6e90\u57df\u548c\u76ee\u6807\u57df\u7279\u5f81-\u6807\u7b7e\u7ed3\u6784\u7684\u9519\u914d\uff0c\u964d\u4f4e\u76ee\u6807\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u7279\u5f81\u6807\u7b7e\u5931\u771f\u6982\u5ff5\u5efa\u7acb\u4e86\u76ee\u6807\u8bef\u5dee\u7684\u53ef\u8bc1\u660e\u6cdb\u5316\u8fb9\u754c\uff0c\u8be5\u8fb9\u754c\u89e3\u91ca\u4e86\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u62df\u5408\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u4e3a\u5b9e\u9645\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u4f18\u5316\u6307\u5bfc\u3002", "result": "\u57fa\u4e8e\u8be5\u7406\u8bba\u6846\u67b6\u8bbe\u8ba1\u7684\u65b9\u6cd5\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u7279\u5f81\u5bf9\u9f50\u4e0e\u76ee\u6807\u62df\u5408\u4ea4\u4e92\u4f5c\u7528\u7406\u8bba\u7406\u89e3\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u4e0d\u4ec5\u89e3\u91ca\u4e86\u8fd9\u4e00\u5173\u952e\u4ea4\u4e92\u4f5c\u7528\uff0c\u8fd8\u4e3a\u5b9e\u9645\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u6027\u80fd\u3002"}}
{"id": "2601.18245", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18245", "abs": "https://arxiv.org/abs/2601.18245", "authors": ["Santanu Das", "Jatin Batra"], "title": "Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity", "comment": null, "summary": "Phase retrieval is the classical problem of recovering a signal $x^* \\in \\mathbb{R}^n$ from its noisy phaseless measurements $y_i = \\langle a_i, x^* \\rangle^2 + \u03b6_i$ (where $\u03b6_i$ denotes noise, and $a_i$ is the sensing vector) for $i \\in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \\log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u91cd\u5c3e\u566a\u58f0\u548c\u5bf9\u6297\u6027\u635f\u574f\u7684\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7ebf\u6027\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "motivation": "\u76f8\u4f4d\u6062\u590d\u662f\u4e00\u4e2a\u7ecf\u5178\u95ee\u9898\uff0c\u5728\u5149\u5b66\u3001\u6676\u4f53\u5b66\u3001\u5f02\u65b9\u5dee\u56de\u5f52\u3001\u5929\u4f53\u7269\u7406\u5b66\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u4f20\u7edf\u7b97\u6cd5\u5bf9\u6d4b\u91cf\u8bef\u5dee\u7684\u9c81\u68d2\u6027\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u91cd\u5c3e\u566a\u58f0\u548c\u5bf9\u6297\u6027\u635f\u574f\u7684\u60c5\u51b5\u4e0b\u3002\u867d\u7136\u6700\u8fd1\u5728\u9c81\u68d2\u7edf\u8ba1\u7b97\u6cd5\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u73b0\u6709\u7684\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u7b97\u6cd5\u8981\u4e48\u662f\u6307\u6570\u65f6\u95f4\u590d\u6742\u5ea6\u7684\uff0c\u8981\u4e48\u7f3a\u4e4f\u6709\u6548\u7684\u9c81\u68d2\u8c31\u521d\u59cb\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u9c81\u68d2\u8c31\u521d\u59cb\u5316\u4e0e\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u6700\u65b0\u7b97\u6cd5\u8fdb\u5c55\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5f00\u53d1\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u9c81\u68d2PCA\u6280\u672f\u6765\u83b7\u5f97\u9c81\u68d2\u7684\u8c31\u521d\u59cb\u5316\uff0c\u8fd9\u662f\u76f8\u4f4d\u6062\u590d\u7b97\u6cd5\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u91cd\u5c3e\u566a\u58f0\u548c\u5bf9\u6297\u6027\u635f\u574f\u65f6\u3002", "result": "\u5b9e\u73b0\u4e86\u9996\u4e2a\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u6709\u91cd\u5c3e\u566a\u58f0\u548c\u5bf9\u6297\u6027\u635f\u574f\u7684\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u95ee\u9898\u3002\u7b97\u6cd5\u5177\u6709\u63a5\u8fd1\u7ebf\u6027\uff08\u76f8\u5bf9\u4e8en\uff09\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524dAISTATS 2025\u63d0\u51fa\u7684\u6307\u6570\u65f6\u95f4\u7b97\u6cd5\uff08\u9700\u8981O(n log n)\u6837\u672c\uff09\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u4e2d\u7684\u8c31\u521d\u59cb\u5316\u95ee\u9898\u4e0e\u9c81\u68d2PCA\u7684\u6700\u65b0\u8fdb\u5c55\u8054\u7cfb\u8d77\u6765\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u88ab\u8ba4\u4e3a\u8d85\u51fa\u5df2\u77e5\u9ad8\u6548\u7b97\u6cd5\u6280\u672f\u8303\u56f4\u7684\u95ee\u9898\uff0c\u4e3a\u9c81\u68d2\u76f8\u4f4d\u6062\u590d\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18255", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18255", "abs": "https://arxiv.org/abs/2601.18255", "authors": ["Fei Meng"], "title": "Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs", "comment": null, "summary": "Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \\textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief \"wake-up\" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded \"safety guarantee\" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u7ecf\u9a8c\u56de\u653e\u5728\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u4e8c\u5206\u73b0\u8c61\uff1a\u5bf9\u975e\u7ed3\u6784\u5316\u4efb\u52a1\u4ea7\u751f\u6b63\u5411\u8fc1\u79fb\uff0c\u4f46\u5bf9\u7ed3\u6784\u5316\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u751f\u6210\uff09\u9020\u6210\u8d1f\u5411\u8fc1\u79fb\uff0c\u5e76\u63d0\u51fa\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5524\u9192\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u56f0\u5883\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u5e73\u8861\u7a33\u5b9a\u6027\uff08\u4fdd\u7559\u65e7\u77e5\u8bc6\uff09\u548c\u53ef\u5851\u6027\uff08\u5b66\u4e60\u65b0\u4efb\u52a1\uff09\u7684\u5173\u952e\u6311\u6218\u3002\u7ecf\u9a8c\u56de\u653e\u4f5c\u4e3a\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u7684\u6807\u51c6\u5bf9\u7b56\uff0c\u5176\u5728\u4e0d\u540c\u80fd\u529b\u4e0a\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5524\u9192\u65b9\u6cd5\uff1a\u901a\u8fc7\u77ed\u6682\u7684\"\u5524\u9192\"\u9636\u6bb5\u8bc6\u522b\u5148\u524d\u4efb\u52a1\u7684\u5173\u952e\u53c2\u6570\u5b50\u7a7a\u95f4\uff0c\u5e76\u5f3a\u5236\u65b0\u4efb\u52a1\u8fdb\u884c\u6b63\u4ea4\u66f4\u65b0\uff0c\u4e3a\u5df2\u5efa\u7acb\u7684\u77e5\u8bc6\u7ed3\u6784\u63d0\u4f9b\u6570\u5b66\u57fa\u7840\u7684\"\u5b89\u5168\u4fdd\u8bc1\"\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u56db\u4efb\u52a1\u5e8f\u5217\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cOSW\u5728\u7ecf\u9a8c\u56de\u653e\u5931\u8d25\u7684\u8106\u5f31\u7f16\u7801\u80fd\u529b\u4fdd\u62a4\u65b9\u9762\u8868\u73b0\u72ec\u7279\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u65b0\u4efb\u52a1\u7684\u9ad8\u53ef\u5851\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u8bc4\u4f30\u7ed3\u6784\u5b89\u5168\u6027\u4e0e\u5e73\u5747\u4fdd\u7559\u540c\u7b49\u91cd\u8981\uff0cOSW\u65b9\u6cd5\u4e3a\u89e3\u51b3\u7ecf\u9a8c\u56de\u653e\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18192", "categories": ["cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18192", "abs": "https://arxiv.org/abs/2601.18192", "authors": ["Tian-Yi Zhou", "Xuan-Hao Liu", "Bao-Liang Lu", "Wei-Long Zheng"], "title": "MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models", "comment": null, "summary": "Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.", "AI": {"tldr": "MindCine\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff0c\u5728\u6709\u9650EEG-\u89c6\u9891\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u91cd\u5efa", "motivation": "EEG\u4fe1\u53f7\u91cd\u5efa\u52a8\u6001\u89c6\u89c9\u611f\u77e5\u5177\u6709\u91cd\u8981\u7814\u7a76\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4ec5\u4f7f\u7528\u6587\u672c\u6a21\u6001\u5bfc\u81f4\u5ffd\u7565\u5176\u4ed6\u6a21\u6001\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\uff1b2\uff09\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u8bad\u7ec3\u56f0\u96be", "method": "\u63d0\u51faMindCine\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u7b56\u7565\u6574\u5408\u6587\u672c\u5916\u5176\u4ed6\u6a21\u6001\uff1b2\uff09\u5229\u7528\u9884\u8bad\u7ec3\u5927\u578bEEG\u6a21\u578b\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u89e3\u7801\u8bed\u4e49\u4fe1\u606f\uff1b3\uff09\u8bbe\u8ba1\u5177\u6709\u56e0\u679c\u6ce8\u610f\u529b\u7684Seq2Seq\u6a21\u578b\u89e3\u7801\u611f\u77e5\u4fe1\u606f", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u6a21\u6001\u4e92\u8865\u4f18\u52bf\u7684\u6709\u6548\u6027\uff0c\u4ee5\u53ca\u5927\u89c4\u6a21EEG\u6a21\u578b\u901a\u8fc7\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u8fdb\u4e00\u6b65\u63d0\u5347\u91cd\u5efa\u6027\u80fd", "conclusion": "MindCine\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86EEG\u5230\u89c6\u9891\u91cd\u5efa\u4e2d\u7684\u5355\u6a21\u6001\u9650\u5236\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u91cd\u5efa\uff0c\u4e3a\u57fa\u4e8eEEG\u7684\u89c6\u89c9\u611f\u77e5\u89e3\u7801\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18261", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18261", "abs": "https://arxiv.org/abs/2601.18261", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Yukun Ma", "Chong Zhang", "Chong Deng", "Qinglin Zhang", "Xiangang Li", "Jieping Ye"], "title": "FGGM: Fisher-Guided Gradient Masking for Continual Learning", "comment": "Accepted by ICASSP 2026", "summary": "Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.", "AI": {"tldr": "FGGM\u6846\u67b6\u901a\u8fc7Fisher\u4fe1\u606f\u6307\u5bfc\u7684\u68af\u5ea6\u63a9\u7801\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728TRACE\u57fa\u51c6\u4e0a\u76f8\u6bd4SFT\u63d0\u53479.6%\uff0c\u76f8\u6bd4MIGU\u63d0\u53474.4%", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u53c2\u6570\u5e45\u5ea6\u7684\u65b9\u6cd5\uff08\u5982MIGU\uff09\u7f3a\u4e4f\u6570\u5b66\u539f\u7406\u57fa\u7840\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u53c2\u6570\u91cd\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\u6765\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027", "method": "\u63d0\u51faFisher-Guided Gradient Masking (FGGM)\u6846\u67b6\uff0c\u4f7f\u7528\u5bf9\u89d2Fisher\u4fe1\u606f\u6218\u7565\u6027\u5730\u9009\u62e9\u9700\u8981\u66f4\u65b0\u7684\u53c2\u6570\uff0c\u52a8\u6001\u751f\u6210\u5177\u6709\u81ea\u9002\u5e94\u9608\u503c\u7684\u4e8c\u8fdb\u5236\u63a9\u7801\uff0c\u4fdd\u62a4\u5173\u952e\u53c2\u6570\u800c\u4e0d\u9700\u8981\u5386\u53f2\u6570\u636e", "result": "\u5728TRACE\u57fa\u51c6\u4e0a\uff0cFGGM\u76f8\u6bd4\u76d1\u7763\u5fae\u8c03(SFT)\u5728\u4fdd\u7559\u901a\u7528\u80fd\u529b\u65b9\u9762\u67099.6%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u76f8\u6bd4MIGU\u5728TRACE\u4efb\u52a1\u4e0a\u67094.4%\u7684\u63d0\u5347\uff1b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u989d\u5916\u5206\u6790\u8bc1\u5b9e\u4e86FGGM\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u51cf\u5c11\u7684\u9057\u5fd8", "conclusion": "FGGM\u901a\u8fc7\u57fa\u4e8e\u6570\u5b66\u539f\u7406\u7684\u53c2\u6570\u91cd\u8981\u6027\u4f30\u8ba1\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u6301\u7eed\u5b66\u4e60\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18195", "abs": "https://arxiv.org/abs/2601.18195", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Kaiwei Zhang", "Jun Jia", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding", "comment": null, "summary": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.", "AI": {"tldr": "QualiRAG\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u611f\u77e5\u77e5\u8bc6\u8fdb\u884c\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u56db\u79cd\u4e92\u8865\u77e5\u8bc6\u6e90\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u611f\u77e5\u548c\u8f85\u52a9\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u6216\u5f3a\u5316\u5b66\u4e60\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u4e14\u5bb9\u6613\u53d7\u6570\u636e\u96c6\u7279\u5b9a\u504f\u5dee\u5f71\u54cd\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u611f\u77e5\u77e5\u8bc6\u3002", "method": "\u63d0\u51faQualiRAG\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u8bf7\u6c42\uff0c\u52a8\u6001\u751f\u6210\u56db\u79cd\u4e92\u8865\u77e5\u8bc6\u6e90\uff1a\u89c6\u89c9\u5143\u6570\u636e\u3001\u4e3b\u4f53\u5b9a\u4f4d\u3001\u5168\u5c40\u8d28\u91cf\u6458\u8981\u548c\u5c40\u90e8\u8d28\u91cf\u63cf\u8ff0\uff0c\u7136\u540e\u8fdb\u884c\u76f8\u5173\u6027\u611f\u77e5\u68c0\u7d22\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u3002", "result": "\u5728\u89c6\u89c9\u8d28\u91cf\u7406\u89e3\u4efb\u52a1\u4e0a\uff0cQualiRAG\u663e\u8457\u4f18\u4e8e\u5f00\u6e90\u901a\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u548c\u4e13\u95e8\u5fae\u8c03\u7684VQA\u6a21\u578b\uff1b\u5728\u89c6\u89c9\u8d28\u91cf\u6bd4\u8f83\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u65e0\u9700\u4efb\u4f55\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5c31\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u8d28\u91cf\u8bc4\u4f30\u80fd\u529b\u3002", "conclusion": "QualiRAG\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u6210\u529f\u5229\u7528\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u611f\u77e5\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u611f\u77e5\u548c\u8f85\u52a9\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18222", "abs": "https://arxiv.org/abs/2601.18222", "authors": ["Mengfan He", "Liangzheng Sun", "Chunyu Li", "Ziyang Meng"], "title": "HomoFM: Deep Homography Estimation with Flow Matching", "comment": null, "summary": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.", "AI": {"tldr": "HomoFM\u5c06\u6d41\u5339\u914d\u6280\u672f\u9996\u6b21\u5f15\u5165\u5355\u5e94\u6027\u4f30\u8ba1\uff0c\u901a\u8fc7\u5efa\u6a21\u8fde\u7eed\u70b9\u901f\u5ea6\u573a\u5c06\u566a\u58f0\u5206\u5e03\u53d8\u6362\u4e3a\u914d\u51c6\u5750\u6807\uff0c\u5e76\u52a0\u5165\u68af\u5ea6\u53cd\u8f6c\u5c42\u63d0\u5347\u8de8\u57df\u9c81\u68d2\u6027\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5355\u5e94\u6027\u4f30\u8ba1\u65b9\u6cd5\u901a\u5e38\u5c06\u5176\u89c6\u4e3a\u76f4\u63a5\u56de\u5f52\u6216\u8fed\u4ee3\u4f18\u5316\u95ee\u9898\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u51e0\u4f55\u53d8\u6362\u6216\u5728\u4e0d\u540c\u57df\u95f4\u6cdb\u5316\u3002\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faHomoFM\u6846\u67b6\uff1a1) \u5c06\u5355\u5e94\u6027\u4f30\u8ba1\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u901f\u5ea6\u573a\u5b66\u4e60\u95ee\u9898\uff0c\u5efa\u6a21\u8fde\u7eed\u70b9\u901f\u5ea6\u573a\u5c06\u566a\u58f0\u5206\u5e03\u53d8\u6362\u4e3a\u914d\u51c6\u5750\u6807\uff1b2) \u901a\u8fc7\u6761\u4ef6\u6d41\u8f68\u8ff9\u6062\u590d\u9ad8\u7cbe\u5ea6\u53d8\u6362\uff1b3) \u5728\u7279\u5f81\u63d0\u53d6\u4e3b\u5e72\u4e2d\u96c6\u6210\u68af\u5ea6\u53cd\u8f6c\u5c42(GRL)\u8fdb\u884c\u57df\u9002\u5e94\uff0c\u5b66\u4e60\u57df\u4e0d\u53d8\u8868\u793a\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHomoFM\u5728\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u5339\u914d\u548c\u53d8\u5316\u5149\u7167\u573a\u666f\u7b49\u57df\u504f\u79fb\u6311\u6218\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "HomoFM\u9996\u6b21\u5c06\u6d41\u5339\u914d\u6280\u672f\u5f15\u5165\u5355\u5e94\u6027\u4f30\u8ba1\u4efb\u52a1\uff0c\u901a\u8fc7\u901f\u5ea6\u573a\u5b66\u4e60\u548c\u57df\u9002\u5e94\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u5e94\u6027\u4f30\u8ba1\u7684\u7cbe\u5ea6\u548c\u8de8\u57df\u9c81\u68d2\u6027\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18278", "abs": "https://arxiv.org/abs/2601.18278", "authors": ["Indr\u0117 \u017dliobait\u0117"], "title": "What Do Learned Models Measure?", "comment": null, "summary": "In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u5b66\u4e60\u6d4b\u91cf\u51fd\u6570\"\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u65b0\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6307\u51fa\u4f20\u7edf\u9884\u6d4b\u6027\u80fd\u8bc4\u4f30\u65e0\u6cd5\u4fdd\u8bc1\u6d4b\u91cf\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5728\u79d1\u5b66\u548c\u6570\u636e\u9a71\u52a8\u5e94\u7528\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u6d4b\u91cf\u5de5\u5177\u800c\u975e\u4ec5\u4ec5\u662f\u9884\u6d4b\u5668\u3002\u5f53\u6d4b\u91cf\u51fd\u6570\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u65f6\uff0c\u4ece\u89c2\u5bdf\u5230\u6570\u91cf\u7684\u6620\u5c04\u7531\u8bad\u7ec3\u5206\u5e03\u548c\u5f52\u7eb3\u504f\u7f6e\u9690\u5f0f\u51b3\u5b9a\uff0c\u5bfc\u81f4\u591a\u4e2a\u4e0d\u7b49\u4ef7\u7684\u6620\u5c04\u90fd\u80fd\u6ee1\u8db3\u6807\u51c6\u9884\u6d4b\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u5c06\u5b66\u4e60\u6d4b\u91cf\u51fd\u6570\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u8bc4\u4f30\u7126\u70b9\uff0c\u5f15\u5165\u6d4b\u91cf\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u8be5\u5c5e\u6027\u6355\u6349\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u53ef\u5bb9\u8bb8\u5b9e\u73b0\u548c\u8de8\u4e0a\u4e0b\u6587\u4e2d\u6d4b\u91cf\u91cf\u7684\u4e0d\u53d8\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\u6807\u51c6\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u6807\u51c6\uff08\u5305\u62ec\u6cdb\u5316\u8bef\u5dee\u3001\u6821\u51c6\u548c\u9c81\u68d2\u6027\uff09\u4e0d\u80fd\u4fdd\u8bc1\u6d4b\u91cf\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u663e\u793a\uff0c\u5177\u6709\u53ef\u6bd4\u9884\u6d4b\u6027\u80fd\u7684\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u7cfb\u7edf\u4e0d\u7b49\u4ef7\u7684\u6d4b\u91cf\u51fd\u6570\uff0c\u5206\u5e03\u504f\u79fb\u5177\u4f53\u8bf4\u660e\u4e86\u8fd9\u79cd\u5931\u8d25\u3002", "conclusion": "\u5f53\u5b66\u4e60\u6a21\u578b\u8f93\u51fa\u88ab\u8bc6\u522b\u4e3a\u6d4b\u91cf\u65f6\uff0c\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u589e\u52a0\u989d\u5916\u7684\u8bc4\u4f30\u7ef4\u5ea6\u6765\u786e\u4fdd\u6d4b\u91cf\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.18292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18292", "abs": "https://arxiv.org/abs/2601.18292", "authors": ["Zhewen Tan", "Wenhan Yu", "Jianfeng Si", "Tongxin Liu", "Kaiqi Guan", "Huiyan Jin", "Jiawen Tao", "Xiaokun Yuan", "Duohe Ma", "Xiangzheng Zhang", "Tong Yang", "Lin Sun"], "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment", "comment": null, "summary": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.", "AI": {"tldr": "TriPlay-RL\uff1a\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u95ed\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u653b\u51fb\u8005\u3001\u9632\u5fa1\u8005\u548c\u8bc4\u4f30\u8005\u4e09\u4e2a\u89d2\u8272\u7684\u534f\u540c\u8fdb\u5316\uff0c\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u7684\u6301\u7eed\u6539\u8fdb\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\u7684\u751f\u6210\u3002\u73b0\u6709\u4e3b\u6d41\u5b89\u5168\u5bf9\u9f50\u8303\u5f0f\u901a\u5e38\u91c7\u7528\u653b\u51fb\u8005\u3001\u9632\u5fa1\u8005\u3001\u8bc4\u4f30\u8005\u4e09\u4e2a\u89d2\u8272\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u7684\u534f\u540c\u8fdb\u5316\u673a\u5236\u3002", "method": "\u63d0\u51faTriPlay-RL\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u4e09\u4e2a\u89d2\u8272\u80fd\u591f\u5728\u8fd1\u4e4e\u96f6\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u8fed\u4ee3\u5f0f\u534f\u540c\u6539\u8fdb\u3002\u653b\u51fb\u8005\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u9632\u5fa1\u8005\u8fdb\u884c\u5b89\u5168\u9632\u5fa1\uff0c\u8bc4\u4f30\u8005\u8bc4\u4f30\u54cd\u5e94\u8d28\u91cf\uff0c\u4e09\u8005\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76f8\u4e92\u4fc3\u8fdb\u3002", "result": "\u653b\u51fb\u8005\u5728\u4fdd\u6301\u9ad8\u8f93\u51fa\u591a\u6837\u6027\u7684\u540c\u65f6\uff0c\u5bf9\u6297\u6548\u679c\u63d0\u534720%-50%\uff1b\u9632\u5fa1\u8005\u5728\u5b89\u5168\u6027\u80fd\u4e0a\u83b7\u5f9710%-30%\u7684\u63d0\u5347\uff0c\u4e14\u4e0d\u964d\u4f4e\u901a\u7528\u63a8\u7406\u80fd\u529b\uff1b\u8bc4\u4f30\u8005\u901a\u8fc7\u8fed\u4ee3\u6301\u7eed\u4f18\u5316\u7ec6\u7c92\u5ea6\u5224\u65ad\u80fd\u529b\uff0c\u80fd\u51c6\u786e\u533a\u5206\u4e0d\u5b89\u5168\u54cd\u5e94\u3001\u7b80\u5355\u62d2\u7edd\u548c\u6709\u7528\u6307\u5bfc\u3002", "conclusion": "TriPlay-RL\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u5efa\u7acb\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u7684\u5b66\u4e60\u5faa\u73af\u4e2d\u5b9e\u73b0\u4e09\u4e2a\u89d2\u8272\u7684\u6301\u7eed\u534f\u540c\u8fdb\u5316\u3002"}}
{"id": "2601.18240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18240", "abs": "https://arxiv.org/abs/2601.18240", "authors": ["Mengyuan Jin", "Zehui Liao", "Yong Xia"], "title": "V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.", "AI": {"tldr": "V-Loop\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u63a8\u7406\u5f62\u6210\u89c6\u89c9\u903b\u8f91\u5faa\u73af\u6765\u9a8c\u8bc1\u533b\u5b66VQA\u4e2d\u7b54\u6848\u7684\u4e8b\u5b9e\u6b63\u786e\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u5bb9\u6613\u4ea7\u751f\u4e0e\u89c6\u89c9\u4e8b\u5b9e\u77db\u76fe\u7684\u5e7b\u89c9\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u533b\u7597\u573a\u666f\u4e2d\u6784\u6210\u91cd\u5927\u98ce\u9669\u3002\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u8ba1\u7b97\u9ad8\u6548\uff0c\u4f46\u672c\u8d28\u4e0a\u662f\u95f4\u63a5\u7684\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f30\u8ba1\u7684\u662f\u56fe\u50cf-\u95ee\u9898\u5bf9\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u975e\u9a8c\u8bc1\u7279\u5b9a\u7b54\u6848\u7684\u4e8b\u5b9e\u6b63\u786e\u6027\u3002", "method": "V-Loop\u5f15\u5165\u53cc\u5411\u63a8\u7406\u8fc7\u7a0b\uff0c\u5f62\u6210\u89c6\u89c9\u57fa\u7840\u7684\u903b\u8f91\u5faa\u73af\u6765\u9a8c\u8bc1\u4e8b\u5b9e\u6b63\u786e\u6027\u3002\u7ed9\u5b9a\u8f93\u5165\u540e\uff0cMLLM\u751f\u6210\u4e3b\u8981\u7b54\u6848\uff0cV-Loop\u4ece\u4e3b\u8981QA\u5bf9\u4e2d\u63d0\u53d6\u8bed\u4e49\u5355\u5143\uff0c\u57fa\u4e8e\u7b54\u6848\u5355\u5143\u751f\u6210\u9a8c\u8bc1\u95ee\u9898\u6765\u91cd\u65b0\u67e5\u8be2\u95ee\u9898\u5355\u5143\uff0c\u5e76\u901a\u8fc7\u5f3a\u5236\u89c6\u89c9\u6ce8\u610f\u529b\u4e00\u81f4\u6027\u786e\u4fdd\u56de\u7b54\u4e3b\u8981\u95ee\u9898\u548c\u9a8c\u8bc1\u95ee\u9898\u90fd\u4f9d\u8d56\u76f8\u540c\u7684\u56fe\u50cf\u8bc1\u636e\u3002\u5982\u679c\u9a8c\u8bc1\u7b54\u6848\u4e0e\u9884\u671f\u8bed\u4e49\u5185\u5bb9\u5339\u914d\uff0c\u903b\u8f91\u5faa\u73af\u95ed\u5408\uff0c\u8868\u660e\u4e8b\u5b9e\u57fa\u7840\uff1b\u5426\u5219\uff0c\u4e3b\u8981\u7b54\u6848\u88ab\u6807\u8bb0\u4e3a\u5e7b\u89c9\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66VQA\u57fa\u51c6\u6d4b\u8bd5\u548cMLLM\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cV-Loop\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u5185\u7701\u65b9\u6cd5\uff0c\u4fdd\u6301\u9ad8\u6548\u6027\uff0c\u5e76\u4e14\u5728\u4e0e\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u65f6\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "V-Loop\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u9a8c\u8bc1\u533b\u5b66VQA\u4e2d\u7b54\u6848\u4e8b\u5b9e\u6b63\u786e\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u903b\u8f91\u5faa\u73af\u68c0\u6d4b\u5e7b\u89c9\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.18242", "categories": ["cs.CV", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.18242", "abs": "https://arxiv.org/abs/2601.18242", "authors": ["Zerui Kang", "Yishen Lim", "Zhouyou Gu", "Seung-Woo Ko", "Tony Q. S. Quek", "Jihong Park"], "title": "Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation", "comment": null, "summary": "Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\\times$ faster convergence and 10-100$\\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u52a0\u901f\u548c\u7a33\u5b9a\u591a\u6750\u6599\u53c2\u6570\u4f30\u8ba1\uff0c\u901a\u8fc7\u8bed\u4e49\u5148\u9a8c\u6307\u5bfc\u7269\u7406\u4f18\u5316\uff0c\u5b9e\u73b0\u5feb\u901f\u53ef\u9760\u7684\u5c04\u9891\u6750\u6599\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u57286G\u7cfb\u7edf\u4e2d\uff0c\u7cbe\u786e\u7684\u5c04\u9891\u6750\u6599\u53c2\u6570\u5bf9\u7535\u78c1\u6570\u5b57\u5b6a\u751f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57fa\u4e8e\u68af\u5ea6\u7684\u9006\u5c04\u7ebf\u8ffd\u8e2a\u5bf9\u521d\u59cb\u5316\u654f\u611f\u4e14\u5728\u6709\u9650\u6d4b\u91cf\u4e0b\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u573a\u666f\u56fe\u50cf\u63a8\u65ad\u6750\u6599\u7c7b\u522b\uff0c\u901a\u8fc7ITU-R\u6750\u6599\u8868\u6620\u5c04\u5230\u5b9a\u91cf\u5148\u9a8c\uff0c\u63d0\u4f9b\u6709\u4fe1\u606f\u7684\u7535\u5bfc\u7387\u521d\u59cb\u5316\uff1bVLM\u8fdb\u4e00\u6b65\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u7684\u53d1\u5c04\u5668/\u63a5\u6536\u5668\u4f4d\u7f6e\u4ee5\u4fc3\u8fdb\u591a\u6837\u5316\u7684\u6750\u6599\u533a\u5206\u8def\u5f84\uff1b\u57fa\u4e8e\u8fd9\u4e9b\u5148\u9a8c\uff0c\u53ef\u5fae\u5206\u5c04\u7ebf\u8ffd\u8e2a\u5f15\u64ce\u4f7f\u7528\u6d4b\u91cf\u7684\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6\u8fdb\u884c\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u5728NVIDIA Sionna\u5ba4\u5185\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u5747\u5300\u6216\u968f\u673a\u521d\u59cb\u5316\u53ca\u968f\u673a\u653e\u7f6e\u57fa\u7ebf\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad82-4\u500d\uff0c\u6700\u7ec8\u53c2\u6570\u8bef\u5dee\u964d\u4f4e10-100\u500d\uff0c\u4ec5\u9700\u5c11\u91cf\u63a5\u6536\u5668\u5373\u53ef\u5b9e\u73b0\u4f4e\u4e8e0.1%\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\uff1b\u590d\u6742\u5ea6\u5206\u6790\u663e\u793a\u6bcf\u6b21\u8fed\u4ee3\u65f6\u95f4\u4e0e\u6750\u6599\u6570\u91cf\u548c\u6d4b\u91cf\u8bbe\u7f6e\u63a5\u8fd1\u7ebf\u6027\u5173\u7cfb\uff0cVLM\u5f15\u5bfc\u7684\u653e\u7f6e\u51cf\u5c11\u4e86\u51c6\u786e\u6062\u590d\u6240\u9700\u7684\u6d4b\u91cf\u6b21\u6570\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7684\u8bed\u4e49\u5148\u9a8c\u80fd\u6709\u6548\u6307\u5bfc\u57fa\u4e8e\u7269\u7406\u7684\u4f18\u5316\uff0c\u5b9e\u73b0\u5feb\u901f\u53ef\u9760\u7684\u5c04\u9891\u6750\u6599\u53c2\u6570\u4f30\u8ba1\uff0c\u4e3a6G\u7cfb\u7edf\u4e2d\u7684\u7535\u78c1\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18326", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18326", "abs": "https://arxiv.org/abs/2601.18326", "authors": ["Jie Li", "Jing Li", "Lu Lv", "Zhanyu Ju", "Fengkui Gong"], "title": "Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals", "comment": null, "summary": "We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eZC\u5e8f\u5217\u548c\u65f6\u9891\u56fe\u50cf\u8ba4\u77e5\u878d\u5408\u7684\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5728\u65e0\u4eba\u673a\u8fdc\u7a0b\u8bc6\u522b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u672a\u77e5\u6216\u975e\u6807\u51c6\u901a\u4fe1\u534f\u8bae\u7684\u65e0\u4eba\u673a\u4fe1\u53f7\u8bc6\u522b\u6311\u6218", "method": "\u901a\u8fc7\u5206\u6790DJI\u65e0\u4eba\u673a\u901a\u4fe1\u534f\u8bae\u63d0\u53d6ZC\u5e8f\u5217\u7279\u5f81\uff0c\u540c\u65f6\u4f7f\u7528\u65f6\u9891\u56fe\u50cf\u6355\u6349\u672a\u77e5\u534f\u8bae\u4fe1\u53f7\u7279\u5f81\uff0c\u91c7\u7528\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u3001\u4ea4\u4e92\u548c\u878d\u5408\u673a\u5236\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6743\u91cd\u8fdb\u884c\u5206\u7c7b", "result": "\u5728\u8fdc\u7a0b\u8bc6\u522b\u548c\u5206\u5e03\u5916\u68c0\u6d4b\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53471.7%\u548c7.5%\uff0c\u5728\u4e0d\u540c\u98de\u884c\u6761\u4ef6\u548c\u65e0\u4eba\u673a\u7c7b\u578b\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u878d\u5408\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u4fe1\u53f7\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u65e0\u4eba\u673a\u8fdc\u7a0b\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.18329", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18329", "abs": "https://arxiv.org/abs/2601.18329", "authors": ["Chuhan Feng", "Jing Li", "Jie Li", "Lu Lv", "Fengkui Gong"], "title": "Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection", "comment": null, "summary": "We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u533a\u5206\u6027\u9a71\u52a8\u7684\u7a7a\u95f4-\u901a\u9053\u9009\u62e9\u548c\u68af\u5ea6\u8303\u6570\u7684\u65e0\u4eba\u673a\u4fe1\u53f7OOD\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u65f6\u9891\u56fe\u50cf\u7279\u5f81\u81ea\u9002\u5e94\u52a0\u6743\u548c\u68af\u5ea6\u8303\u6570\u5ea6\u91cf\u6270\u52a8\u654f\u611f\u6027\uff0c\u5b9e\u73b0\u4f18\u5f02\u7684OOD\u68c0\u6d4b\u6027\u80fd", "motivation": "\u65e0\u4eba\u673a\u4fe1\u53f7\u68c0\u6d4b\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8bc6\u522b\u5206\u5e03\u5916\uff08OOD\uff09\u6837\u672c\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u9002\u5e94\u5229\u7528\u65f6\u9891\u7279\u5f81\u5e76\u91cf\u5316OOD\u6837\u672c\u5185\u5728\u4e0d\u7a33\u5b9a\u6027\u7684\u68c0\u6d4b\u7b97\u6cd5", "method": "1. \u57fa\u4e8e\u534f\u8bae\u7279\u5b9a\u65f6\u9891\u7279\u5f81\u91cf\u5316\u7c7b\u95f4\u76f8\u4f3c\u6027\u548c\u65b9\u5dee\uff0c\u81ea\u9002\u5e94\u52a0\u6743\u65f6\u9891\u56fe\u50cf\u7279\u5f81\u7684\u7a7a\u95f4\u548c\u901a\u9053\u7ef4\u5ea6\uff1b2. \u5f15\u5165\u68af\u5ea6\u8303\u6570\u5ea6\u91cf\u6270\u52a8\u654f\u611f\u6027\uff0c\u6355\u6349OOD\u6837\u672c\u7684\u5185\u5728\u4e0d\u7a33\u5b9a\u6027\uff1b3. \u5c06\u68af\u5ea6\u8303\u6570\u4e0e\u57fa\u4e8e\u80fd\u91cf\u7684\u5206\u6570\u878d\u5408\u8fdb\u884c\u8054\u5408\u63a8\u7406", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u548c\u5404\u79cd\u65e0\u4eba\u673a\u7c7b\u578b\u4e0b\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u533a\u5206\u80fd\u529b\u548c\u9c81\u68d2\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u53ef\u533a\u5206\u6027\u9a71\u52a8\u7684\u7a7a\u95f4-\u901a\u9053\u9009\u62e9\u548c\u68af\u5ea6\u8303\u6570\u7684OOD\u68c0\u6d4b\u7b97\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u65e0\u4eba\u673a\u4fe1\u53f7\u4e2d\u7684OOD\u6837\u672c\uff0c\u5177\u6709\u4f18\u5f02\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u9c81\u68d2\u6027"}}
{"id": "2601.18252", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18252", "abs": "https://arxiv.org/abs/2601.18252", "authors": ["Chao Wang", "Xuanying Li", "Cheng Dai", "Jinglei Feng", "Yuxiang Luo", "Yuqi Ouyang", "Hao Qin"], "title": "Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing", "comment": null, "summary": "Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.", "AI": {"tldr": "Co-PLNet\uff1a\u4e00\u79cd\u70b9\u7ebf\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u63d0\u793a\u7f16\u7801\u548c\u4ea4\u53c9\u5f15\u5bfc\u89e3\u7801\u5b9e\u73b0\u7ebf\u6846\u89e3\u6790\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u7ebf\u6846\u89e3\u6790\u65b9\u6cd5\u5206\u522b\u9884\u6d4b\u7ebf\u6bb5\u548c\u8fde\u63a5\u70b9\uff0c\u7136\u540e\u8fdb\u884c\u540e\u5904\u7406\u534f\u8c03\uff0c\u5bfc\u81f4\u4e0d\u5339\u914d\u548c\u9c81\u68d2\u6027\u964d\u4f4e\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u70b9\u7ebf\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u70b9\u7ebf\u63d0\u793a\u7f16\u7801\u5668\u5c06\u65e9\u671f\u68c0\u6d4b\u8f6c\u6362\u4e3a\u7a7a\u95f4\u63d0\u793a\uff0c\u518d\u901a\u8fc7\u4ea4\u53c9\u5f15\u5bfc\u7ebf\u89e3\u7801\u5668\u5229\u7528\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u7ec6\u5316\u9884\u6d4b", "result": "\u5728Wireframe\u548cYorkUrban\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u540c\u65f6\u5177\u6709\u826f\u597d\u7684\u5b9e\u65f6\u6548\u7387", "conclusion": "Co-PLNet\u901a\u8fc7\u70b9\u7ebf\u534f\u4f5c\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u51e0\u4f55\u611f\u77e5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u5982SLAM\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u51e0\u4f55\u8868\u793a"}}
{"id": "2601.18356", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18356", "abs": "https://arxiv.org/abs/2601.18356", "authors": ["Weiqin Yang", "Haowen Xue", "Qingyi Peng", "Hexuan Hu", "Qian Huang", "Tingbo Zhang"], "title": "Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning", "comment": null, "summary": "Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u56e0\u679c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u5c06\u56e0\u679c\u63a8\u7406\u539f\u7406\u4e0e\u591a\u6a21\u6001\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u8868\u9762\u7edf\u8ba1\u5173\u8054\u800c\u975e\u56e0\u679c\u75c5\u7406\u673a\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u63a8\u7406\u673a\u5236\u672c\u8d28\u4e0a\u662f\u76f8\u5173\u6027\u7684\uff0c\u4f9d\u8d56\u4e8e\u8868\u9762\u7684\u7edf\u8ba1\u5173\u8054\uff0c\u672a\u80fd\u6355\u6349\u4e34\u5e8a\u51b3\u7b56\u6240\u9700\u7684\u56e0\u679c\u75c5\u7406\u673a\u5236\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u8106\u5f31\u3001\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u5e76\u5bf9\u6570\u636e\u96c6\u504f\u5dee\u654f\u611f\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u56e0\u679c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u4ece\u5916\u90e8\u6e90\u68c0\u7d22\u4e34\u5e8a\u76f8\u5173\u793a\u4f8b\u548c\u56e0\u679c\u56fe\uff0c\u5c06\u6a21\u578b\u63a8\u7406\u5efa\u7acb\u5728\u53cd\u4e8b\u5b9e\u548c\u5e72\u9884\u8bc1\u636e\u800c\u975e\u5355\u7eaf\u76f8\u5173\u6027\u4e4b\u4e0a\u3002", "result": "\u5e94\u7528\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u3001\u8bca\u65ad\u9884\u6d4b\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u65f6\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u56e0\u679c\u68c0\u7d22\u4e3a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u4f7f\u5176\u80fd\u591f\u8d85\u8d8a\u6a21\u5f0f\u5339\u914d\uff0c\u5728\u9ad8\u98ce\u9669\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u4fe1\u8d56\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002"}}
{"id": "2601.18301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18301", "abs": "https://arxiv.org/abs/2601.18301", "authors": ["Seyedali Mousavi", "Seyedhamidreza Mousavi", "Masoud Daneshtalab"], "title": "Contextual Range-View Projection for 3D LiDAR Point Clouds", "comment": null, "summary": "Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \\textit{Centerness-Aware Projection (CAP)} and \\textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7684LiDAR\u70b9\u4e91\u5230\u8ddd\u79bb\u56fe\u50cf\u6295\u5f71\u673a\u5236\uff1aCAP\u548cCWAP\uff0c\u901a\u8fc7\u7ed3\u5408\u5b9e\u4f8b\u4e2d\u5fc3\u548c\u7c7b\u522b\u4fe1\u606f\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u4f18\u5148\u6295\u5f71\u5bfc\u81f4\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfLiDAR\u70b9\u4e91\u5230\u8ddd\u79bb\u56fe\u50cf\u7684\u6295\u5f71\u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u7b56\u7565\uff08\u4fdd\u7559\u6700\u8fd1\u70b9\uff09\uff0c\u5ffd\u7565\u4e86\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7269\u4f53\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u91cd\u8981\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e22\u5931\u3002\u9700\u8981\u6539\u8fdb\u6295\u5f71\u9009\u62e9\u7b56\u7565\u4ee5\u4fdd\u7559\u66f4\u591a\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6295\u5f71\u673a\u5236\uff1a1) CAP\uff08\u4e2d\u5fc3\u611f\u77e5\u6295\u5f71\uff09\uff1a\u6839\u636e\u70b9\u5230\u5b9e\u4f8b\u4e2d\u5fc3\u7684\u8ddd\u79bb\u8c03\u6574\u6df1\u5ea6\u503c\uff0c\u4f18\u5148\u4fdd\u7559\u5b9e\u4f8b\u4e2d\u5fc3\u70b9\u800c\u975e\u566a\u58f0\u8fb9\u754c\u70b9\u548c\u80cc\u666f\u70b9\uff1b2) CWAP\uff08\u7c7b\u522b\u52a0\u6743\u611f\u77e5\u6295\u5f71\uff09\uff1a\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u6743\u91cd\u5bf9\u4e0d\u540c\u7c7b\u522b\u8fdb\u884c\u4f18\u5148\u7ea7\u8c03\u6574\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u6295\u5f71\u7b56\u7565\u3002", "result": "\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCAP\u5728\u6295\u5f71\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u4e86\u66f4\u591a\u5b9e\u4f8b\u70b9\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad83.1%\u7684mIoU\u63d0\u5347\u3002CWAP\u80fd\u591f\u589e\u5f3a\u76ee\u6807\u7c7b\u522b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5bf9\u5176\u4ed6\u7c7b\u522b\u7684\u5f71\u54cd\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5b9e\u4f8b\u4e2d\u5fc3\u548c\u7c7b\u522b\u4fe1\u606f\u7684\u6295\u5f71\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3LiDAR\u70b9\u4e91\u6295\u5f71\u4e2d\u7684\u591a\u5bf9\u4e00\u51b2\u7a81\u95ee\u9898\uff0c\u4fdd\u7559\u66f4\u591a\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7269\u4f53\u7ed3\u6784\u4fe1\u606f\uff0c\u63d0\u53473D\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2601.18401", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18401", "abs": "https://arxiv.org/abs/2601.18401", "authors": ["Yufeng Huang"], "title": "Superlinear Multi-Step Attention", "comment": "30 pages, 6 figures", "summary": "In this paper, we propose \\textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \\textbf{random context access} (a.k.a.\\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.", "AI": {"tldr": "\u63d0\u51faSuperlinear attention\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u6b65\u641c\u7d22\u5b9e\u73b0\u957f\u5e8f\u5217\u7684\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u968f\u673a\u4e0a\u4e0b\u6587\u8bbf\u95ee\u80fd\u529b\uff0c\u57281M\u548c10M\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u975e\u6392\u9664\u7279\u6027\uff08\u5373\u6240\u6709\u7b26\u5408\u6761\u4ef6\u7684token\u4f4d\u7f6e\u90fd\u80fd\u88ab\u6ce8\u610f\u529b\u673a\u5236\u8bbf\u95ee\uff09\u3002", "method": "\u5c06\u6807\u51c6\u56e0\u679c\u81ea\u6ce8\u610f\u529b\u91cd\u65b0\u8868\u8ff0\u4e3aN\u6b65\u641c\u7d22\u95ee\u9898\uff0c\u63d0\u51faSuperlinear attention\u67b6\u6784\u3002\u4ee5N=2\u4e3a\u4f8b\uff0c\u7b2c\u4e00\u6b65\u8fdb\u884cO(L^{3/2})\u7684\u8de8\u5ea6\u641c\u7d22\u9009\u62e9\u76f8\u5173\u5e8f\u5217\u6bb5\uff0c\u7b2c\u4e8c\u6b65\u5728\u9009\u5b9a\u6bb5\u4e0a\u5e94\u7528O(L^{3/2})\u7684\u8de8\u5ea6\u6ce8\u610f\u529b\u3002", "result": "\u5728O(L^{1.54})\u914d\u7f6e\u4e0b\uff0c\u5728\u5355B200 GPU\u4e0a\uff0c1M\u4e0a\u4e0b\u6587\u957f\u5ea6\u5e73\u5747\u89e3\u7801\u541e\u5410\u91cf\u8fbe114 tokens/sec\uff0c10M\u4e0a\u4e0b\u6587\u957f\u5ea6\u8fbe80 tokens/sec\u3002\u5728NIAH\u4efb\u52a1\u4e0a\uff0c256K\u4e0a\u4e0b\u6587\u957f\u5ea6\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u8def\u7531\u8de8\u5ea6\u9009\u62e9\u53ef\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "conclusion": "Superlinear attention\u67b6\u6784\u5728\u4fdd\u6301\u968f\u673a\u4e0a\u4e0b\u6587\u8bbf\u95ee\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u53ef\u884c\u6027\uff0c\u4e3a\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5168\u9762\u8d28\u91cf\u8bc4\u4f30\u9700\u672a\u6765\u5de5\u4f5c\u5b8c\u6210\u3002"}}
{"id": "2601.18409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18409", "abs": "https://arxiv.org/abs/2601.18409", "authors": ["Aniket Sanyal", "Baraah A. M. Sidahmed", "Rebekka Burkholz", "Tatjana Chavdarova"], "title": "Frequency-Based Hyperparameter Selection in Games", "comment": null, "summary": "Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \\emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51faMoLA\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u4f30\u8ba1\u81ea\u9002\u5e94\u8c03\u6574LookAhead\u8d85\u53c2\u6570\uff0c\u52a0\u901f\u535a\u5f08\u73af\u5883\u4e2d\u7684\u8bad\u7ec3", "motivation": "\u5149\u6ed1\u535a\u5f08\u4e2d\u7684\u5b66\u4e60\u4e0e\u4f20\u7edf\u6700\u5c0f\u5316\u95ee\u9898\u4e0d\u540c\uff0c\u5b58\u5728\u65cb\u8f6c\u52a8\u529b\u5b66\uff0c\u4f7f\u7ecf\u5178\u8d85\u53c2\u6570\u8c03\u4f18\u7b56\u7565\u5931\u6548\u3002LookAhead\u65b9\u6cd5\u867d\u7136\u5b9e\u7528\u4f46\u5f15\u5165\u989d\u5916\u53c2\u6570\u4e14\u6027\u80fd\u654f\u611f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u535a\u5f08\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u3002", "method": "\u63d0\u51faModal LookAhead (MoLA)\u65b9\u6cd5\uff1a1) \u5206\u6790\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\u548c\u79bb\u6563\u52a8\u529b\u5b66\u9891\u8c31\u4e2d\u7684\u632f\u8361\u884c\u4e3a\uff1b2) \u5229\u7528\u9891\u7387\u4f30\u8ba1\u65cb\u8f6c\u52a8\u529b\u5b66\uff1b3) \u81ea\u9002\u5e94\u9009\u62e9\u8d85\u53c2\u6570\u4ee5\u9002\u5e94\u5177\u4f53\u95ee\u9898\u3002", "result": "MoLA\u5728\u7eaf\u65cb\u8f6c\u535a\u5f08\u548c\u6df7\u5408\u673a\u5236\u4e2d\u90fd\u80fd\u52a0\u901f\u8bad\u7ec3\uff0c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\uff0c\u5e76\u63d0\u4f9b\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u9891\u7387\u5206\u6790\u7684\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u9009\u62e9\u65b9\u6cd5MoLA\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u535a\u5f08\u73af\u5883\u4e2d\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2601.18330", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18330", "abs": "https://arxiv.org/abs/2601.18330", "authors": ["Muhammad Ali Shah", "Muhammad Mansoor Alam", "Saddam Hussain Khan"], "title": "A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification", "comment": "33 Pages, 8 Tables, Figures 16", "summary": "This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5bc6\u96c6Swin\u6df7\u5408\uff08EDSH\uff09\u6846\u67b6\u7528\u4e8e\u8111\u80bf\u7624MRI\u5206\u6790\uff0c\u901a\u8fc7\u4e24\u4e2a\u80bf\u7624\u611f\u77e5\u5b9e\u9a8c\u8bbe\u7f6e\u6765\u8054\u5408\u6355\u83b7\u7ec6\u7c92\u5ea6\u7eb9\u7406\u6a21\u5f0f\u548c\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u8111\u80bf\u7624MRI\u5206\u6790\u9700\u8981\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u7eb9\u7406\u6a21\u5f0f\u548c\u957f\u8ddd\u79bb\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u4e0d\u540c\u80bf\u7624\u7c7b\u578b\u7684\u7279\u5f02\u6027\u8bca\u65ad\u6311\u6218\uff0c\u5982\u5f25\u6f2b\u6027\u80f6\u8d28\u7624\u7684\u4e0d\u89c4\u5219\u5f62\u72b6\u3001\u8fb9\u754c\u4e0d\u6e05\u548c\u5f02\u8d28\u6027\u7eb9\u7406\uff0c\u4ee5\u53ca\u8111\u819c\u7624\u548c\u5782\u4f53\u7624\u7684\u660e\u786e\u80bf\u5757\u3001\u7279\u5b9a\u4f4d\u7f6e\u548c\u80bf\u7624\u6269\u5c55\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86EDSH\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u80bf\u7624\u611f\u77e5\u5b9e\u9a8c\u8bbe\u7f6e\uff1a1\uff09\u589e\u5f3a\u7279\u5f81\u7a7a\u95f4\uff08BFS\uff09\u8bbe\u7f6e\uff0c\u901a\u8fc7\u72ec\u7acb\u5b9a\u5236\u7684DenseNet\u548cSwin\u5206\u652f\u5b66\u4e60\u4e92\u8865\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8868\u793a\uff0c\u8fdb\u884c\u7ef4\u5ea6\u5bf9\u9f50\u3001\u878d\u5408\u548c\u589e\u5f3a\uff1b2\uff09\u5206\u5c42DenseNet-Swin\u67b6\u6784\uff0c\u5177\u6709\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u53cc\u6b8b\u5dee\u8fde\u63a5\uff08DFE\u548cDR\uff09\uff0cDenseNet\u4f5c\u4e3a\u4e3b\u5e72CNN\u5b66\u4e60\u7ed3\u6784\u5316\u5c40\u90e8\u7279\u5f81\uff0cSwin_t\u6a21\u578b\u5b66\u4e60\u5168\u5c40\u80bf\u7624\u5f62\u6001\u3002DenseNet\u5728\u8f93\u5165\u7ea7\u522b\u5b9a\u5236\u4ee5\u5339\u914dMRI\u7a7a\u95f4\u7279\u6027\uff0cSwin_t\u901a\u8fc7\u4efb\u52a1\u5bf9\u9f50\u7684\u8865\u4e01\u5d4c\u5165\u548c\u79fb\u4f4d\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u5b9a\u5236\u3002", "result": "\u5728\u5305\u542b40,260\u5f20\u56fe\u50cf\u7684\u5927\u89c4\u6a21MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.50%\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u8868\u73b0\u4f18\u4e8e\u72ec\u7acb\u7684CNN\u3001Vision Transformer\u548c\u6df7\u5408\u6a21\u578b\u3002", "conclusion": "EDSH\u6846\u67b6\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u5c40\u90e8\u7eb9\u7406\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u540c\u7c7b\u578b\u8111\u80bf\u7624\u7684\u8bca\u65ad\u6311\u6218\uff0c\u5728\u8111\u80bf\u7624MRI\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6df7\u5408\u67b6\u6784\u601d\u8def\u3002"}}
{"id": "2601.18420", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18420", "abs": "https://arxiv.org/abs/2601.18420", "authors": ["Satya Prakash Dash", "Hossein Abdi", "Wei Pan", "Samuel Kaski", "Mingfei Sun"], "title": "Gradient Regularized Natural Gradients", "comment": null, "summary": "Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.", "AI": {"tldr": "\u63d0\u51faGRNG\u7b97\u6cd5\uff0c\u5c06\u68af\u5ea6\u6b63\u5219\u5316\u4e0e\u81ea\u7136\u68af\u5ea6\u7ed3\u5408\uff0c\u63d0\u5347\u4e8c\u9636\u4f18\u5316\u5668\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u68af\u5ea6\u6b63\u5219\u5316\u5df2\u88ab\u8bc1\u660e\u80fd\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4e8c\u9636\u4f18\u5316\u5668\u5982\u4f55\u4ece\u68af\u5ea6\u6b63\u5219\u5316\u4e2d\u83b7\u76ca\u7684\u7814\u7a76\u8f83\u5c11\u3002\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u5728\u8bad\u7ec3\u521d\u671f\u80fd\u52a0\u901f\u4f18\u5316\uff0c\u4f46\u5982\u4f55\u7ed3\u5408\u68af\u5ea6\u6b63\u5219\u5316\u63d0\u5347\u8bad\u7ec3\u52a8\u6001\u503c\u5f97\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u68af\u5ea6\u6b63\u5219\u5316\u81ea\u7136\u68af\u5ea6(GRNG)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u7b97\u6cd5\uff1a1) \u9891\u7387\u6d3e\u53d8\u4f53\u901a\u8fc7\u7ed3\u6784\u5316\u8fd1\u4f3c\u907f\u514d\u663e\u5f0fFisher\u4fe1\u606f\u77e9\u9635\u6c42\u9006\uff1b2) \u8d1d\u53f6\u65af\u53d8\u4f53\u57fa\u4e8e\u6b63\u5219\u5316\u5361\u5c14\u66fc\u516c\u5f0f\u5b8c\u5168\u6d88\u9664FIM\u6c42\u9006\u9700\u6c42\u3002", "result": "GRNG\u5728\u6536\u655b\u6027\u4e0a\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u68af\u5ea6\u6b63\u5219\u5316\u63d0\u5347\u7a33\u5b9a\u6027\u5e76\u786e\u4fdd\u6536\u655b\u5230\u5168\u5c40\u6700\u5c0f\u503c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4e00\u9636\u65b9\u6cd5(SGD, AdamW)\u548c\u4e8c\u9636\u57fa\u7ebf(K-FAC, Sophia)\uff0cGRNG\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u4e0a\u6301\u7eed\u63d0\u5347\u4f18\u5316\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u68af\u5ea6\u6b63\u5219\u5316\u662f\u89e3\u9501\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u9c81\u68d2\u6027\u7684\u539f\u5219\u6027\u548c\u5b9e\u7528\u5de5\u5177\uff0cGRNG\u6846\u67b6\u4e3a\u4e8c\u9636\u4f18\u5316\u5668\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u68af\u5ea6\u6b63\u5219\u5316\u96c6\u6210\u65b9\u6848\u3002"}}
{"id": "2601.18336", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.18336", "abs": "https://arxiv.org/abs/2601.18336", "authors": ["Isaac Deutsch", "Nicolas Mo\u00ebnne-Loccoz", "Gavriel State", "Zan Gojcic"], "title": "PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction", "comment": "For more details and updates, please visit our project website: https://research.nvidia.com/labs/sil/projects/ppisp/", "summary": "Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp", "AI": {"tldr": "\u63d0\u51faPPISP\u6a21\u5757\uff0c\u901a\u8fc7\u7269\u7406\u53ef\u89e3\u91ca\u7684ISP\u6821\u6b63\u89e3\u51b3\u591a\u89c6\u89d23D\u91cd\u5efa\u4e2d\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u5347\u65b0\u89c6\u89d2\u91cd\u5efa\u8d28\u91cf", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u76f8\u673a\u5149\u5b66\u7279\u6027\u548c\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\uff08ISP\uff09\u53d8\u5316\u5f15\u8d77\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u9ad8\u5ea6\u654f\u611f\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u7f3a\u4e4f\u7269\u7406\u57fa\u7840\u4e14\u5bf9\u65b0\u89c6\u89d2\u6cdb\u5316\u80fd\u529b\u5dee", "method": "\u63d0\u51fa\u7269\u7406\u53ef\u89e3\u91ca\u7684ISP\uff08PPISP\uff09\u6821\u6b63\u6a21\u5757\uff0c\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u89e3\u91ca\u53d8\u6362\u89e3\u8026\u76f8\u673a\u56fa\u6709\u7279\u6027\u548c\u62cd\u6444\u4f9d\u8d56\u6548\u5e94\uff1b\u4e13\u95e8\u7684PPISP\u63a7\u5236\u5668\u5728\u8f93\u5165\u89c6\u56fe\u4e0a\u8bad\u7ec3\uff0c\u9884\u6d4b\u65b0\u89c6\u89d2\u7684ISP\u53c2\u6570", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u76f4\u89c2\u63a7\u5236\uff0c\u652f\u6301\u5728\u53ef\u7528\u65f6\u96c6\u6210\u5143\u6570\u636e", "conclusion": "PPISP\u6a21\u5757\u80fd\u591f\u5b9e\u73b0\u5bf9\u65b0\u89c6\u89d2\u7684\u73b0\u5b9e\u4e14\u516c\u5e73\u7684\u8bc4\u4f30\uff0c\u65e0\u9700\u8bbf\u95ee\u771f\u5b9e\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d23D\u91cd\u5efa\u4e2d\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898"}}
{"id": "2601.18340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18340", "abs": "https://arxiv.org/abs/2601.18340", "authors": ["Bingzheng Qu", "Kehai Chen", "Xuefeng Bai", "Jun Yu", "Min Zhang"], "title": "Beyond Rigid: Benchmarking Non-Rigid Video Editing", "comment": null, "summary": "Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u975e\u521a\u6027\u89c6\u9891\u7f16\u8f91\u7684\u57fa\u51c6NRVBench\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u65b0\u8bc4\u4f30\u6307\u6807\u548c\u8bad\u7ec3\u514d\u8d39\u57fa\u7ebf\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u5408\u7406\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1\u6587\u672c\u9a71\u52a8\u89c6\u9891\u7f16\u8f91\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u751f\u6210\u8fde\u8d2f\u7684\u975e\u521a\u6027\u53d8\u5f62\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7ecf\u5e38\u53d7\u5230\u7269\u7406\u5931\u771f\u548c\u65f6\u95f4\u95ea\u70c1\u7684\u56f0\u6270\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u7269\u7406\u5408\u7406\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "1) \u6784\u5efaNRVBench\u57fa\u51c6\uff1a\u5305\u542b180\u4e2a\u975e\u521a\u6027\u8fd0\u52a8\u89c6\u9891\uff086\u4e2a\u7269\u7406\u7c7b\u522b\uff09\u30012,340\u4e2a\u7ec6\u7c92\u5ea6\u4efb\u52a1\u6307\u4ee4\u548c360\u4e2a\u591a\u9879\u9009\u62e9\u9898\uff1b2) \u63d0\u51faNRVE-Acc\u8bc4\u4f30\u6307\u6807\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e25\u683c\u8bc4\u4f30\u7269\u7406\u5408\u89c4\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u5bf9\u9f50\uff1b3) \u63d0\u51faVM-Edit\u57fa\u7ebf\u65b9\u6cd5\uff1a\u91c7\u7528\u53cc\u533a\u57df\u53bb\u566a\u673a\u5236\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u63a7\u5236\uff0c\u5e73\u8861\u7ed3\u6784\u4fdd\u6301\u548c\u52a8\u6001\u53d8\u5f62\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u800c\u63d0\u51fa\u7684VM-Edit\u65b9\u6cd5\u5728\u6807\u51c6\u548c\u63d0\u51fa\u7684\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002NRVBench\u53ef\u4f5c\u4e3a\u63a8\u8fdb\u7269\u7406\u611f\u77e5\u89c6\u9891\u7f16\u8f91\u7684\u6807\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "NRVBench\u662f\u9996\u4e2a\u4e13\u95e8\u7684\u975e\u521a\u6027\u89c6\u9891\u7f16\u8f91\u57fa\u51c6\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u65b0\u8bc4\u4f30\u6307\u6807\u548c\u6709\u6548\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u7269\u7406\u611f\u77e5\u89c6\u9891\u7f16\u8f91\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.18479", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18479", "abs": "https://arxiv.org/abs/2601.18479", "authors": ["Kyoleen Kwak", "Hyoseok Hwang"], "title": "Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States", "comment": "Accepted at AAAI-26. 7 pages (excluding references), 3 figures", "summary": "Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.", "AI": {"tldr": "\u63d0\u51faASAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u6e21\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\u548c\u5bf9\u9f50\u52a8\u4f5c\u6765\u6291\u5236\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u9891\u632f\u8361\uff0c\u5b9e\u73b0\u66f4\u5e73\u6ed1\u7684\u63a7\u5236", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u56fa\u6709\u7684\u9ad8\u9891\u632f\u8361\u7279\u6027\u9650\u5236\u4e86\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u5408\u6210\u7684\u72b6\u6001\u76f8\u4f3c\u6027\u5b9a\u4e49\u6765\u4fc3\u8fdb\u52a8\u4f5c\u4e00\u81f4\u6027\uff0c\u4f46\u8fd9\u4e9b\u5b9a\u4e49\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5e95\u5c42\u7cfb\u7edf\u52a8\u6001\u3002", "method": "\u63d0\u51faASAP\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165\u8fc7\u6e21\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\uff0c\u5b9a\u4e49\u4e3a\u4ece\u524d\u4e00\u72b6\u6001\u8fc7\u6e21\u5230\u7684\u4e0b\u4e00\u72b6\u6001\u5206\u5e03\uff0c\u4ec5\u4f7f\u7528\u73af\u5883\u53cd\u9988\u548c\u5b9e\u9645\u6536\u96c6\u6570\u636e\uff1b2\uff09\u901a\u8fc7\u5c06\u52a8\u4f5c\u4e0e\u8fc7\u6e21\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\u4e2d\u7684\u52a8\u4f5c\u5bf9\u9f50\u6765\u5f3a\u5236\u52a8\u4f5c\u5e73\u6ed1\u6027\uff1b3\uff09\u60e9\u7f5a\u4e8c\u9636\u5dee\u5f02\u4ee5\u6291\u5236\u9ad8\u9891\u632f\u8361\u3002", "result": "\u5728Gymnasium\u548cIsaac-Lab\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cASAP\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u63a7\u5236\u548c\u66f4\u597d\u7684\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "ASAP\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u8fc7\u6e21\u8bf1\u5bfc\u76f8\u4f3c\u72b6\u6001\u548c\u5bf9\u9f50\u52a8\u4f5c\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u52a8\u4f5c\u632f\u8361\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18346", "abs": "https://arxiv.org/abs/2601.18346", "authors": ["Sijing Wu", "Yunhao Li", "Zicheng Zhang", "Qi Jia", "Xinyue Li", "Huiyu Duan", "Xiongkuo Min", "Guangtao Zhai"], "title": "Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.", "AI": {"tldr": "Q-Bench-Portrait\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u4eba\u50cf\u56fe\u50cf\u8d28\u91cf\u611f\u77e5\u7684\u7efc\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,765\u4e2a\u56fe\u50cf-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u50cf\u56fe\u50cf\u611f\u77e5\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u56fe\u50cf\u7684\u4f4e\u7ea7\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5177\u6709\u72ec\u7279\u7ed3\u6784\u548c\u611f\u77e5\u7279\u6027\u7684\u4eba\u50cf\u56fe\u50cf\u9886\u57df\u7684\u611f\u77e5\u548c\u8bc4\u4f30\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86Q-Bench-Portrait\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u6837\u7684\u4eba\u50cf\u56fe\u50cf\u6765\u6e90\uff08\u81ea\u7136\u3001\u5408\u6210\u5931\u771f\u3001AI\u751f\u6210\u3001\u827a\u672f\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u56fe\u50cf\uff09\u3001\u5168\u9762\u7684\u8d28\u91cf\u7ef4\u5ea6\uff08\u6280\u672f\u5931\u771f\u3001AIGC\u7279\u5b9a\u5931\u771f\u548c\u7f8e\u5b66\uff09\u4ee5\u53ca\u591a\u79cd\u95ee\u9898\u683c\u5f0f\uff08\u5355\u9009\u3001\u591a\u9009\u3001\u5224\u65ad\u548c\u5f00\u653e\u5f0f\u95ee\u9898\uff09\uff0c\u5e76\u5728\u5168\u5c40\u548c\u5c40\u90e8\u4e24\u4e2a\u5c42\u6b21\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e8620\u4e2a\u5f00\u6e90\u548c5\u4e2a\u95ed\u6e90MLLMs\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u4eba\u50cf\u56fe\u50cf\u611f\u77e5\u65b9\u9762\u5177\u6709\u4e00\u5b9a\u80fd\u529b\uff0c\u4f46\u8868\u73b0\u4ecd\u7136\u6709\u9650\u4e14\u4e0d\u7cbe\u786e\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u5c06\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4ee5\u589e\u5f3a\u901a\u7528\u548c\u9886\u57df\u7279\u5b9aMLLMs\u7684\u4eba\u50cf\u56fe\u50cf\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2601.18500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18500", "abs": "https://arxiv.org/abs/2601.18500", "authors": ["Chen Liang", "Donghua Yang", "Yutong Wang", "Tianle Zhang", "Shenghe Zhou", "Zhiyu Liang", "Hengtong Zhang", "Hongzhi Wang", "Ziqi Li", "Xiyang Zhang", "Zheng Liang", "Yifei Li"], "title": "Nearly Optimal Bayesian Inference for Structural Missingness", "comment": null, "summary": "Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8d1d\u53f6\u65af\u6846\u67b6\u5904\u7406\u7ed3\u6784\u5316\u7f3a\u5931\u6570\u636e\uff0c\u901a\u8fc7\u540e\u9a8c\u9884\u6d4b\u5206\u5e03\u6574\u5408\u4e0d\u786e\u5b9a\u6027\u800c\u975e\u5355\u70b9\u63d2\u8865\uff0c\u5728\u5206\u7c7b\u548c\u63d2\u8865\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA", "motivation": "\u7ed3\u6784\u5316\u7f3a\u5931\u6570\u636e\u5b58\u5728\u56e0\u679c\u5faa\u73af\u56f0\u5883\uff1a\u9884\u6d4b\u9700\u8981\u7f3a\u5931\u7279\u5f81\uff0c\u4f46\u63a8\u65ad\u5b83\u4eec\u53c8\u4f9d\u8d56\u4e8e\u7f3a\u5931\u673a\u5236\uff1bMNAR\u4e0b\u7f3a\u5931\u90e8\u5206\u53ef\u80fd\u6765\u81ea\u5206\u5e03\u504f\u79fb\uff1b\u5355\u70b9\u63d2\u8865\u4f1a\u9501\u5b9a\u4e0d\u786e\u5b9a\u6027\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u7684\u504f\u5dee\u51b3\u7b56", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u89c6\u89d2\uff0c\u901a\u8fc7\u540e\u9a8c\u9884\u6d4b\u5206\u5e03\u6574\u5408\u5b8c\u6574\u6a21\u578b\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u5b66\u4e60\u7f3a\u5931\u503c\u540e\u9a8c\u4e0e\u6807\u7b7e\u9884\u6d4b\u89e3\u8026\uff0c\u5b9e\u73b0\u540e\u9a8c\u79ef\u5206", "result": "\u572843\u4e2a\u5206\u7c7b\u548c15\u4e2a\u63d2\u8865\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u5728SCM\u5148\u9a8c\u4e0b\u5177\u6709\u6709\u9650\u6837\u672c\u8fd1\u8d1d\u53f6\u65af\u6700\u4f18\u6027\u4fdd\u8bc1", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\"\u51e0\u4e4e\u514d\u8d39\u5348\u9910\"\uff1a\u4e00\u65e6\u5b66\u4e60\u5230\u540e\u9a8c\uff0c\u9884\u6d4b\u5373\u63d2\u5373\u7528\u540c\u65f6\u4fdd\u6301\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u6709\u6548\u89e3\u51b3\u7ed3\u6784\u5316\u7f3a\u5931\u6570\u636e\u7684\u56e0\u679c\u5faa\u73af\u548cMNAR\u95ee\u9898"}}
{"id": "2601.18368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18368", "abs": "https://arxiv.org/abs/2601.18368", "authors": ["Caterina Fuster-Barcel\u00f3", "Claudia Castrill\u00f3n", "Laura Rodrigo-Mu\u00f1oz", "Victor Manuel Vega-Su\u00e1rez", "Nicol\u00e1s P\u00e9rez-Fern\u00e1ndez", "Gorka Bastarrika", "Arrate Mu\u00f1oz-Barrutia"], "title": "OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI", "comment": null, "summary": "We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.\n  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.\n  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.", "AI": {"tldr": "OREHAS\u662f\u9996\u4e2a\u5168\u81ea\u52a8\u91cf\u5316\u5185\u6dcb\u5df4\u79ef\u6c34\u7684\u7ba1\u9053\u7cfb\u7edf\uff0c\u901a\u8fc73D MRI\u76f4\u63a5\u8ba1\u7b97\u5185\u6dcb\u5df4\u4e0e\u8033\u8717\u4f53\u79ef\u6bd4\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u5272\u3002", "motivation": "\u5f53\u524d\u5185\u6dcb\u5df4\u79ef\u6c34\u7684MRI\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u548c\u5546\u4e1a\u8f6f\u4ef6\uff0c\u5b58\u5728\u64cd\u4f5c\u8005\u4f9d\u8d56\u6027\u5f3a\u3001\u65b9\u6cd5\u4e0d\u4e00\u81f4\u3001\u4f53\u79ef\u4f30\u8ba1\u4e0d\u51c6\u786e\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u3001\u53ef\u91cd\u590d\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "OREHAS\u96c6\u6210\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u5207\u7247\u5206\u7c7b\u3001\u5185\u8033\u5b9a\u4f4d\u548c\u5e8f\u5217\u7279\u5f02\u6027\u5206\u5272\uff0c\u5f62\u6210\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\uff0c\u4ec5\u9700\u6bcf\u4e2a\u60a3\u80053-6\u4e2a\u6807\u6ce8\u5207\u7247\u8fdb\u884c\u8bad\u7ec3\uff0c\u5373\u53ef\u5904\u7406\u5b8c\u65743D MRI\u4f53\u79ef\u3002", "result": "\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\uff0cOREHAS Dice\u5206\u6570\u8fbe0.90\uff08SPACE-MRC\uff09\u548c0.75\uff08REAL-IR\uff09\uff0c\u4e0e\u4e13\u5bb6\u6807\u6ce8\u5339\u914d\u5ea6\uff08VSI=74.3%\uff09\u663e\u8457\u4f18\u4e8e\u4e34\u5e8a\u8f6f\u4ef6syngo.via\uff08VSI=42.5%\uff09\uff0c\u63d0\u4f9b\u66f4\u751f\u7406\u771f\u5b9e\u7684\u4f53\u79ef\u6d4b\u91cf\u3002", "conclusion": "OREHAS\u8bc1\u660e\u901a\u8fc7\u6709\u9650\u76d1\u7763\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u4e0e\u4e34\u5e8a\u5bf9\u9f50\u7684\u5de5\u4f5c\u6d41\uff0c\u53ef\u5b9e\u73b0\u53ef\u9760\u3001\u53ef\u91cd\u590d\u7684\u5185\u6dcb\u5df4\u79ef\u6c34\u91cf\u5316\uff0c\u4e3a\u5927\u89c4\u6a21\u7814\u7a76\u548c\u4e34\u5e8a\u8bca\u65ad\u9608\u503c\u6821\u51c6\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2601.18509", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18509", "abs": "https://arxiv.org/abs/2601.18509", "authors": ["Andro Sabashvili"], "title": "Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark", "comment": null, "summary": "Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\u7684\u5e94\u7528\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5982\u4f55\u514b\u670d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u975e\u4ea4\u6362\u6027\u8fd9\u4e00\u6838\u5fc3\u95ee\u9898\uff0c\u5e76\u6bd4\u8f83\u4e86\u5404\u7c7b\u7b97\u6cd5\u7684\u5b9e\u9645\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9650\u5236\u6027\u5206\u5e03\u5047\u8bbe\u3002\u4fdd\u5f62\u9884\u6d4b\u4f5c\u4e3a\u4e00\u79cd\u65e0\u5206\u5e03\u6846\u67b6\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u7684\u65f6\u5e8f\u4f9d\u8d56\u6027\u8fdd\u53cd\u4e86\u4fdd\u5f62\u9884\u6d4b\u7684\u6838\u5fc3\u4ea4\u6362\u6027\u5047\u8bbe\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u51b2\u7a81\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u56db\u7c7b\u4e3b\u8981\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\uff1a1\uff09\u653e\u5bbd\u4ea4\u6362\u6027\u5047\u8bbe\u7684\u65b9\u6cd5\uff1b2\uff09\u5c06\u6570\u636e\u5355\u4f4d\u91cd\u65b0\u5b9a\u4e49\u4e3a\u72ec\u7acb\u65f6\u95f4\u5e8f\u5217\u96c6\u5408\u7684\u65b9\u6cd5\uff1b3\uff09\u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u6b8b\u5dee\u52a8\u6001\u7684\u65b9\u6cd5\uff1b4\uff09\u9002\u5e94\u5206\u5e03\u6f02\u79fb\u4ee5\u7ef4\u6301\u957f\u671f\u8986\u76d6\u7684\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u7efc\u5408\u6bd4\u8f83\u8fd9\u4e9b\u65b9\u6cd5\uff0c\u672c\u6587\u5f3a\u8c03\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u4fdd\u5f62\u9884\u6d4b\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6280\u672f\u8bc4\u4f30\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u4fdd\u5f62\u9884\u6d4b\u4e3a\u65f6\u95f4\u5e8f\u5217\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65e0\u5206\u5e03\u6846\u67b6\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5904\u7406\u65f6\u5e8f\u4f9d\u8d56\u6027\u3002\u672c\u6587\u7efc\u8ff0\u7684\u7cfb\u7edf\u5206\u7c7b\u548c\u6027\u80fd\u6bd4\u8f83\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.18385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18385", "abs": "https://arxiv.org/abs/2601.18385", "authors": ["Rinka Kawano", "Masaki Kawamura"], "title": "Estimation of geometric transformation matrices using grid-shaped pilot signals", "comment": null, "summary": "Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u72b6\u5bfc\u9891\u4fe1\u53f7\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u51e0\u4f55\u53d8\u6362\u540e\u7f51\u683c\u7684\u7578\u53d8\u6765\u4f30\u8ba1\u53d8\u6362\u77e9\u9635\uff0c\u5b9e\u73b0\u88c1\u526a\u7b49\u653b\u51fb\u4e0b\u7684\u51c6\u786e\u540c\u6b65\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5bf9\u88c1\u526a\u653b\u51fb\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u800c\u88c1\u526a\u4f1a\u6539\u53d8\u56fe\u50cf\u539f\u70b9\uff0c\u4f7f\u6c34\u5370\u540c\u6b65\u53d8\u5f97\u56f0\u96be\u3002\u9700\u8981\u4e00\u79cd\u80fd\u51c6\u786e\u4f30\u8ba1\u51e0\u4f55\u53d8\u6362\u3001\u5b9e\u73b0\u6709\u6548\u540c\u6b65\u7684\u6c34\u5370\u6280\u672f\u3002", "method": "\u5d4c\u5165\u5177\u6709\u4e0d\u540c\u6c34\u5e73\u548c\u5782\u76f4\u503c\u7684\u7f51\u683c\u72b6\u5bfc\u9891\u4fe1\u53f7\u3002\u5f53\u56fe\u50cf\u53d1\u751f\u51e0\u4f55\u53d8\u6362\u65f6\uff0c\u7f51\u683c\u4e5f\u4f1a\u76f8\u5e94\u7578\u53d8\u3002\u901a\u8fc7\u5bf9\u7578\u53d8\u56fe\u50cf\u8fdb\u884cRadon\u53d8\u6362\uff0c\u4f30\u8ba1\u7f51\u683c\u89d2\u5ea6\u548c\u95f4\u9694\u3002\u5229\u7528\u6c34\u5e73\u548c\u5782\u76f4\u7ebf\u7f16\u7801\u4e0d\u540c\u7684\u7279\u6027\u786e\u5b9a\u7f51\u683c\u65b9\u5411\uff0c\u51cf\u5c11\u6a21\u7cca\u6027\u3002", "result": "\u5728\u5404\u9879\u5f02\u6027\u7f29\u653e\u3001\u65cb\u8f6c\u3001\u526a\u5207\u548c\u88c1\u526a\u7b49\u653b\u51fb\u4e0b\u8fdb\u884c\u4eff\u771f\u6d4b\u8bd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u4f30\u8ba1\u53d8\u6362\u77e9\u9635\uff0c\u5728\u5355\u4e00\u548c\u590d\u5408\u653b\u51fb\u4e0b\u5747\u4fdd\u6301\u8f83\u4f4e\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5bfc\u9891\u4fe1\u53f7\u7684\u7f51\u683c\u5206\u6790\u65b9\u6cd5\u80fd\u6709\u6548\u4f30\u8ba1\u51e0\u4f55\u53d8\u6362\uff0c\u89e3\u51b3\u4e86\u88c1\u526a\u653b\u51fb\u4e0b\u7684\u6c34\u5370\u540c\u6b65\u95ee\u9898\uff0c\u4e3a\u51e0\u4f55\u653b\u51fb\u9c81\u68d2\u7684\u6570\u5b57\u6c34\u5370\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.18524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18524", "abs": "https://arxiv.org/abs/2601.18524", "authors": ["Yongqi Jin", "Yecheng Wang", "Jun-jie Wang", "Rong Zhu", "Guolin Ke", "Weinan E"], "title": "From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale", "comment": null, "summary": "Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.", "AI": {"tldr": "\u63d0\u51fa\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u6570\u767e\u4e07\u6587\u732e\u63d0\u53d6\u7684\u672a\u6807\u8bb0NMR\u5149\u8c31\u9884\u6d4b\u5316\u5b66\u4f4d\u79fb\uff0c\u65e0\u9700\u539f\u5b50\u7ea7\u6807\u6ce8\uff0c\u901a\u8fc7\u6392\u5e8f\u635f\u5931\u5b9e\u73b0\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u800cNMR\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u5bf9\u5149\u8c31\u5206\u6790\u548c\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6570\u636e\u5229\u7528\u65b9\u5f0f", "method": "\u63d0\u51fa\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5c06\u6587\u732e\u5149\u8c31\u7684\u5316\u5b66\u4f4d\u79fb\u9884\u6d4b\u5efa\u6a21\u4e3a\u7f6e\u6362\u4e0d\u53d8\u96c6\u5408\u76d1\u7763\u95ee\u9898\uff0c\u5728\u635f\u5931\u51fd\u6570\u6ee1\u8db3\u5e38\u89c1\u6761\u4ef6\u4e0b\uff0c\u6700\u4f18\u4e8c\u5206\u5339\u914d\u7b80\u5316\u4e3a\u57fa\u4e8e\u6392\u5e8f\u7684\u635f\u5931\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u534a\u76d1\u7763\u8bad\u7ec3", "result": "\u6a21\u578b\u5728\u663e\u8457\u66f4\u5927\u66f4\u591a\u6837\u7684\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9996\u6b21\u5728\u5e38\u89c1NMR\u6eb6\u5242\u4e2d\u6355\u6349\u5230\u7cfb\u7edf\u6027\u7684\u6eb6\u5242\u6548\u5e94", "conclusion": "\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u6587\u732e\u5149\u8c31\u53ef\u4f5c\u4e3a\u8bad\u7ec3NMR\u4f4d\u79fb\u6a21\u578b\u7684\u6709\u6548\u6570\u636e\u6e90\uff0c\u8868\u660e\u6587\u732e\u884d\u751f\u7684\u5f31\u7ed3\u6784\u5316\u6570\u636e\u5728\u6570\u636e\u4e2d\u5fc3\u7684\u79d1\u5b66AI\u4e2d\u5177\u6709\u66f4\u5e7f\u6cdb\u4f5c\u7528"}}
{"id": "2601.18407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18407", "abs": "https://arxiv.org/abs/2601.18407", "authors": ["Jon Sporring", "David Stansby"], "title": "Larger than memory image processing", "comment": "10 pages", "summary": "This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8d85\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\uff08\u5982PB\u7ea7\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\uff09\u7684\u6d41\u5f0f\u5904\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u5207\u7247\u6d41\u5f0f\u67b6\u6784\u6700\u5c0f\u5316\u78c1\u76d8I/O\uff0c\u5e76\u5f15\u5165\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u81ea\u52a8\u4f18\u5316\u5185\u5b58\u4f7f\u7528\u548c\u6d41\u6c34\u7ebf\u8c03\u5ea6\u3002", "motivation": "\u5904\u7406PB\u7ea7\u8d85\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u65f6\u9762\u4e34\u5185\u5b58\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u6027\u80fd\u4e3b\u8981\u53d7I/O\u9650\u5236\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u5904\u7406\u8d85\u51fa\u5185\u5b58\u5bb9\u91cf\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5207\u7247\u6d41\u5f0f\u67b6\u6784\uff0c\u652f\u6301\u4e24\u79cd3D\u6570\u636e\u8868\u793a\uff082D\u5207\u7247\u5806\u6808\u548c3D\u5206\u5757\u5e03\u5c40\uff09\uff1b\u5f15\u5165\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\uff0c\u901a\u8fc7\u7f16\u8bd1\u65f6\u548c\u8fd0\u884c\u65f6\u5206\u6790\u81ea\u52a8\u9009\u62e9\u7a97\u53e3\u5927\u5c0f\u3001\u878d\u5408\u5904\u7406\u9636\u6bb5\u3001\u8c03\u5ea6\u6570\u636e\u6d41\uff1b\u91c7\u7528\u57fa\u4e8e\u626b\u63cf\u7684\u6267\u884c\u3001\u7a97\u53e3\u5316\u64cd\u4f5c\u548c\u91cd\u53e0\u611f\u77e5\u5206\u5757\u6765\u6700\u5c0f\u5316\u5197\u4f59\u8bbf\u95ee\u3002", "result": "\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7ebf\u6027\u7684I/O\u626b\u63cf\u548c\u53ef\u9884\u6d4b\u7684\u5185\u5b58\u5360\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u5927\u89c4\u6a21\u56fe\u50cf\u5904\u7406\u7684\u541e\u5410\u91cf\uff0c\u65e0\u9700\u5c06\u6574\u4e2a\u6570\u636e\u5377\u5b8c\u5168\u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\u3002", "conclusion": "\u901a\u8fc7\u6d41\u5f0f\u5904\u7406\u67b6\u6784\u548c\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u8d85\u51fa\u5185\u5b58\u5bb9\u91cf\u7684\u8d85\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\uff0c\u5c06\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u91cd\u6784\u4e3a\u4f18\u5148\u987a\u5e8f\u8bfb\u5199\u6a21\u5f0f\u7684\u6d41\u6c34\u7ebf\uff0c\u4e3a\u5206\u5272\u548c\u5f62\u6001\u5b66\u7b49\u73b0\u6709\u5de5\u5177\u63d0\u4f9b\u4e86\u96c6\u6210\u65b9\u6848\u3002"}}
{"id": "2601.18525", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18525", "abs": "https://arxiv.org/abs/2601.18525", "authors": ["Eleonora Grassucci", "Giordano Cicchetti", "Emanuele Frasca", "Aurelio Uncini", "Danilo Comminiello"], "title": "Closing the Modality Gap Aligns Group-Wise Semantics", "comment": "ICLR 2026", "summary": "In multimodal learning, CLIP has been recognized as the \\textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff0c\u53d1\u73b0\u867d\u7136CLIP\u5728\u8bed\u4e49\u5c42\u9762\u6709\u6548\u5bf9\u9f50\u6a21\u6001\uff0c\u4f46\u6a21\u6001\u95f4\u9699\u5bf9\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u5f71\u54cd\u6709\u9650\uff0c\u800c\u5728\u7fa4\u4f53\u7ea7\u4efb\u52a1\u4e2d\u5f71\u54cd\u663e\u8457\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u51cf\u5c11\u6a21\u6001\u95f4\u9699\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u8fd9\u80fd\u663e\u8457\u63d0\u5347\u805a\u7c7b\u7b49\u7fa4\u4f53\u7ea7\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u867d\u7136CLIP\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5b83\u521b\u5efa\u7684\u6f5c\u5728\u7a7a\u95f4\u5f80\u5f80\u53ea\u662f\u90e8\u5206\u5171\u4eab\u7684\uff0c\u5b58\u5728\u7ed3\u6784\u4e0d\u5339\u914d\u95ee\u9898\uff08\u6a21\u6001\u95f4\u9699\uff09\u3002\u5c3d\u7ba1\u8fd9\u79cd\u95f4\u9699\u5bf9\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u5f71\u54cd\u6709\u9650\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u5b83\u5bf9\u7fa4\u4f53\u7ea7\u4efb\u52a1\u53ef\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u6301\u7eed\u51cf\u5c11\u53cc\u6a21\u6001\u8bbe\u7f6e\u4e2d\u7684\u6a21\u6001\u95f4\u9699\uff0c\u5e76\u53ef\u4ee5\u7b80\u5355\u6269\u5c55\u5230\u4e00\u822c\u7684n\u6a21\u6001\u60c5\u51b5\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u66f4\u6709\u6548\u5730\u5bf9\u9f50\u4e0d\u540c\u6a21\u6001\u7684\u8868\u793a\u7a7a\u95f4\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u4f5c\u8005\u53d1\u73b0\u51cf\u5c11\u6a21\u6001\u95f4\u9699\u5bf9\u4f20\u7edf\u5b9e\u4f8b\u7ea7\u4efb\u52a1\uff08\u5982\u68c0\u7d22\uff09\u53ea\u80fd\u5e26\u6765\u8fb9\u9645\u6216\u4e0d\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u4f46\u5bf9\u7fa4\u4f53\u7ea7\u4efb\u52a1\uff08\u5982\u805a\u7c7b\uff09\u5374\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u8fd9\u63ed\u793a\u4e86\u6a21\u6001\u95f4\u9699\u5728\u8bed\u4e49\u5206\u7ec4\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u91cd\u5851\u4e86\u5bf9\u6a21\u6001\u95f4\u9699\u7684\u7406\u89e3\uff0c\u5f3a\u8c03\u4e86\u5b83\u5728\u63d0\u5347\u9700\u8981\u8bed\u4e49\u5206\u7ec4\u7684\u4efb\u52a1\u6027\u80fd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002\u867d\u7136\u6a21\u6001\u95f4\u9699\u5bf9\u5b9e\u4f8b\u7ea7\u4efb\u52a1\u5f71\u54cd\u6709\u9650\uff0c\u4f46\u5bf9\u7fa4\u4f53\u7ea7\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.18414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18414", "abs": "https://arxiv.org/abs/2601.18414", "authors": ["Aura Loredana Dan"], "title": "Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings", "comment": "9 pages, 8 figures", "summary": "Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08MobileNet\u3001EfficientNet\u3001VGG16\uff09\u5728\u513f\u7ae5\u7ed8\u753b\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u5206\u7c7b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u3002", "motivation": "\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\u513f\u7ae5\u5728\u60c5\u611f\u8868\u8fbe\u548c\u6c9f\u901a\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u65e9\u671f\u9636\u6bb5\u3002\u4f20\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u5177\u6709\u4fb5\u5165\u6027\u3001\u4e3b\u89c2\u6027\u6216\u96be\u4ee5\u4e00\u81f4\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u5ba2\u89c2\u3001\u975e\u4fb5\u5165\u6027\u7684\u60c5\u611f\u72b6\u6001\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u7531\u5fc3\u7406\u5b66\u4e13\u5bb6\u6807\u6ce8\u60c5\u611f\u6807\u7b7e\u7684\u513f\u7ae5\u7ed8\u753b\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08MobileNet\u3001EfficientNet\u3001VGG16\uff09\u8fdb\u884c\u7edf\u4e00\u5b9e\u9a8c\u6846\u67b6\u4e0b\u7684\u6bd4\u8f83\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u8f7b\u91cf\u7ea7\u67b6\u6784\u4e0e\u6df1\u5c42\u67b6\u6784\u5728\u57fa\u4e8e\u7ed8\u753b\u7684\u60c5\u611f\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6743\u8861\uff0c\u7279\u522b\u662f\u5728\u79fb\u52a8\u548c\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u4e0b\u3002\u4e0d\u540c\u6a21\u578b\u5728\u5206\u7c7b\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u5404\u81ea\u7684\u4f18\u52bf\u548c\u5c40\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u513f\u7ae5\u60c5\u611f\u72b6\u6001\u8bc6\u522b\u63d0\u4f9b\u4e86\u57fa\u4e8e\u7ed8\u753b\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6bd4\u8f83\uff0c\u5f3a\u8c03\u4e86\u5728\u79fb\u52a8\u548c\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u4e2d\u6a21\u578b\u9009\u62e9\u65f6\u9700\u8981\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u5206\u7c7b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.18546", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18546", "abs": "https://arxiv.org/abs/2601.18546", "authors": ["Arash Jamshidi", "Katsiaryna Haitsiukevich", "Kai Puolam\u00e4ki"], "title": "Information Hidden in Gradients of Regression with Target Noise", "comment": null, "summary": "Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $\u03a3$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $\u03a9(1)$ factor. The proposed method is practical (a \"set target-noise variance to $n$\" rule) and robust (variance $\\mathcal{O}(n)$ suffices to recover $\u03a3$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u68af\u5ea6\u4fe1\u606f\u6062\u590d\u4e8c\u9636\u4fe1\u606f\uff08Hessian\u77e9\u9635\uff09\u7684\u65b9\u6cd5\uff0c\u6838\u5fc3\u662f\u901a\u8fc7\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u4f7f\u603b\u76ee\u6807\u566a\u58f0\u65b9\u5dee\u7b49\u4e8e\u6279\u91cf\u5927\u5c0f\uff0c\u4ece\u800c\u8ba9\u7ecf\u9a8c\u68af\u5ea6\u534f\u65b9\u5dee\u8fd1\u4f3c\u4e8eHessian\u77e9\u9635\u3002", "motivation": "\u5728\u8bb8\u591a\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u53ea\u80fd\u89c2\u5bdf\u5230\u68af\u5ea6\u4fe1\u606f\uff0c\u800c\u4e8c\u9636\u4fe1\u606f\uff08\u5982\u66f2\u7387\u6216\u6570\u636e\u534f\u65b9\u5dee\uff09\u5bf9\u4e8e\u4f18\u5316\u3001\u8bca\u65ad\u548c\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4ec5\u4ece\u68af\u5ea6\u6062\u590dHessian\u77e9\u9635\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b9\u5dee\u6821\u51c6\u65b9\u6cd5\uff1a\u6ce8\u5165\u9ad8\u65af\u566a\u58f0\u4f7f\u603b\u76ee\u6807\u566a\u58f0\u65b9\u5dee\u7b49\u4e8e\u6279\u91cf\u5927\u5c0f\u3002\u901a\u8fc7\u8fd9\u79cd\u6821\u51c6\uff0c\u5373\u4f7f\u8fdc\u79bb\u6700\u4f18\u89e3\uff0c\u7ecf\u9a8c\u68af\u5ea6\u534f\u65b9\u5dee\u4e5f\u80fd\u7d27\u5bc6\u8fd1\u4f3cHessian\u77e9\u9635\u3002\u5728\u4e9a\u9ad8\u65af\u8f93\u5165\u4e0b\u63d0\u4f9b\u4e86\u975e\u6e10\u8fd1\u7b97\u5b50\u8303\u6570\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a1\uff09\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u6062\u590dHessian\u77e9\u9635\uff1b2\uff09\u6ca1\u6709\u8fd9\u79cd\u6821\u51c6\u65f6\uff0c\u6062\u590d\u53ef\u80fd\u5931\u8d25\u03a9(1)\u56e0\u5b50\uff1b3\uff09\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\uff08\u65b9\u5deeO(n)\u8db3\u4ee5\u6062\u590d\u03a3\u5230\u5c3a\u5ea6\uff09\uff1b4\uff09\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4ec5\u901a\u8fc7\u68af\u5ea6\u4fe1\u606f\u53ef\u4ee5\u6062\u590d\u4e8c\u9636\u4fe1\u606f\uff0c\u63d0\u51fa\u7684\"\u5c06\u76ee\u6807\u566a\u58f0\u65b9\u5dee\u8bbe\u4e3an\"\u7684\u7b80\u5355\u89c4\u5219\u662f\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u3002\u8be5\u65b9\u6cd5\u5728\u9884\u6761\u4ef6\u52a0\u901f\u4f18\u5316\u3001\u5bf9\u6297\u98ce\u9669\u4f30\u8ba1\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u68af\u5ea6\u8bad\u7ec3\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.18448", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18448", "abs": "https://arxiv.org/abs/2601.18448", "authors": ["Lloyd Austin Courtenay"], "title": "On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics", "comment": "17 pages, 5 figures, Preprint pending review", "summary": "Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust \"diagonal\" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u51e0\u4f55\u5f62\u6001\u6d4b\u91cf\u5b66\u4e2d\uff0c\u4f7f\u7528\u5e7f\u4e49\u666e\u6c0f\u5206\u6790\u8fdb\u884c\u6570\u636e\u5bf9\u9f50\u65f6\u53ef\u80fd\u5bfc\u81f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cd\u5bf9\u9f50\u65b9\u6cd5\u6765\u89e3\u51b3\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e4b\u95f4\u7684\u4ea4\u53c9\u6837\u672c\u4f9d\u8d56\u3002", "motivation": "\u51e0\u4f55\u5f62\u6001\u6d4b\u91cf\u5b66\u5e7f\u6cdb\u7528\u4e8e\u91cf\u5316\u5f62\u72b6\u53d8\u5f02\uff0c\u5e76\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u5206\u6790\u7684\u8f93\u5165\u3002\u6807\u51c6\u505a\u6cd5\u662f\u5728\u5c06\u6570\u636e\u5206\u5272\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e4b\u524d\uff0c\u901a\u8fc7\u5e7f\u4e49\u666e\u6c0f\u5206\u6790\u5bf9\u9f50\u6240\u6709\u6807\u672c\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7edf\u8ba1\u4f9d\u8d56\u5e76\u6c61\u67d3\u4e0b\u6e38\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u63a7\u52362D\u548c3D\u6a21\u62df\u5b9e\u9a8c\uff0c\u5728\u4e0d\u540c\u6837\u672c\u5927\u5c0f\u3001\u5730\u6807\u5bc6\u5ea6\u548c\u5f02\u901f\u751f\u957f\u6a21\u5f0f\u4e0b\uff0c\u6b63\u5f0f\u8868\u5f81GPA\u5f15\u8d77\u7684\u6c61\u67d3\u6548\u5e94\u3002\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u91cd\u5bf9\u9f50\u7a0b\u5e8f\uff0c\u5c06\u6d4b\u8bd5\u6807\u672c\u4e0e\u8bad\u7ec3\u96c6\u5bf9\u9f50\u540e\u518d\u8fdb\u884c\u6a21\u578b\u62df\u5408\uff0c\u6d88\u9664\u4ea4\u53c9\u6837\u672c\u4f9d\u8d56\u3002\u4f7f\u7528\u7ebf\u6027\u548c\u5377\u79ef\u56de\u5f52\u6a21\u578b\u8fdb\u4e00\u6b65\u5c55\u793a\u5730\u6807\u95f4\u7a7a\u95f4\u81ea\u76f8\u5173\u7684\u91cd\u8981\u6027\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u63ed\u793a\u4e86\u6837\u672c\u5927\u5c0f\u4e0e\u5730\u6807\u7a7a\u95f4\u4e4b\u95f4\u7684\u7a33\u5065\"\u5bf9\u89d2\u7ebf\"\u5173\u7cfb\uff0c\u53cd\u6620\u4e86\u5404\u5411\u540c\u6027\u53d8\u5f02\u4e0bRMSE\u7684\u7f29\u653e\u89c4\u5f8b\uff0c\u5176\u659c\u7387\u53ef\u4ee5\u4ece\u666e\u6c0f\u5207\u7a7a\u95f4\u7684\u81ea\u7531\u5ea6\u4e2d\u89e3\u6790\u63a8\u5bfc\u3002\u5f53\u5ffd\u7565\u5730\u6807\u95f4\u5173\u7cfb\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u4f1a\u4e0b\u964d\uff0c\u7a81\u663e\u4e86\u7a7a\u95f4\u81ea\u76f8\u5173\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86\u5728GMM\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\u9700\u8981\u4ed4\u7ec6\u9884\u5904\u7406\u7684\u91cd\u8981\u6027\uff0c\u63d0\u4f9b\u4e86\u91cd\u5bf9\u9f50\u7684\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u9610\u660e\u4e86\u666e\u6c0f\u5f62\u72b6\u7a7a\u95f4\u56fa\u6709\u7684\u57fa\u672c\u7edf\u8ba1\u7ea6\u675f\u3002"}}
{"id": "2601.18564", "categories": ["cs.LG", "cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.18564", "abs": "https://arxiv.org/abs/2601.18564", "authors": ["Chong Hyun Lee", "Kibae Lee", "Hyun Hee Yim"], "title": "An Unsupervised Tensor-Based Domain Alignment", "comment": "5 pages, 5 figures", "summary": "We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u7684\u57df\u5bf9\u9f50\u7b97\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u77e9\u9635\u5728\u4e0d\u53d8\u5b50\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6e90\u548c\u76ee\u6807\u5f20\u91cf\uff0c\u4f7f\u7528\u659c\u6d41\u5f62\u7ea6\u675f\u63d0\u4f9b\u6bd4\u4f20\u7edfStiefel\u6d41\u5f62\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u9879\u4fdd\u6301\u65b9\u5dee\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u8f6c\u6362\u901f\u5ea6\u548c\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\u57df\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528Stiefel\u6d41\u5f62\u7ea6\u675f\uff0c\u7075\u6d3b\u6027\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u57df\u5bf9\u9f50\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u57df\u9002\u5e94\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u6e90\u548c\u76ee\u6807\u5f20\u91cf\u7684\u65b9\u5dee\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u7684\u57df\u5bf9\u9f50\u7b97\u6cd5\uff0c\u4f7f\u7528\u5bf9\u9f50\u77e9\u9635\u5728\u4e0d\u53d8\u5b50\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6e90\u548c\u76ee\u6807\u5f20\u91cf\u3002\u91c7\u7528\u659c\u6d41\u5f62\u7ea6\u675f\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u6bd4\u4f20\u7edfStiefel\u6d41\u5f62\u66f4\u7075\u6d3b\u3002\u5f15\u5165\u6b63\u5219\u5316\u9879\u6765\u4fdd\u6301\u6e90\u548c\u76ee\u6807\u5f20\u91cf\u7684\u65b9\u5dee\uff0c\u786e\u4fdd\u9c81\u68d2\u6027\u80fd\u3002\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5c06\u73b0\u6709\u5f20\u91cf\u57df\u5bf9\u9f50\u65b9\u6cd5\u4f5c\u4e3a\u7279\u4f8b\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u57df\u5bf9\u9f50\u8f6c\u6362\u901f\u5ea6\uff0c\u8fd8\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002\u5728\u590d\u6742\u57df\u9002\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5f20\u91cf\u7684\u57df\u5bf9\u9f50\u7b97\u6cd5\u4f7f\u7528\u659c\u6d41\u5f62\u7ea6\u675f\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u7075\u6d3b\uff0c\u901a\u8fc7\u65b9\u5dee\u4fdd\u6301\u6b63\u5219\u5316\u786e\u4fdd\u9c81\u68d2\u6027\uff0c\u5728\u901f\u5ea6\u548c\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u662f\u590d\u6742\u57df\u9002\u5e94\u4efb\u52a1\u7684\u4f18\u9009\u65b9\u6cd5\u3002"}}
{"id": "2601.18451", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.18451", "abs": "https://arxiv.org/abs/2601.18451", "authors": ["Xuanmeng Sha", "Liyun Zhang", "Tomohiro Mashita", "Naoya Chiba", "Yuki Uranishi"], "title": "3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control", "comment": "13 pages, 5 figures", "summary": "Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.", "AI": {"tldr": "3DGesPolicy\uff1a\u57fa\u4e8e\u6269\u6563\u7b56\u7565\u7684\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6574\u4f53\u624b\u52bf\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8fde\u7eed\u8f68\u8ff9\u63a7\u5236\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8eab\u4f53\u8fd0\u52a8\u8bed\u4e49\u534f\u8c03\u548c\u7a7a\u95f4\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u3001\u8868\u8fbe\u6027\u5f3a\u4e14\u4e0e\u8bed\u97f3\u9ad8\u5ea6\u5bf9\u9f50\u7684\u6574\u4f53\u624b\u52bf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6574\u4f53\u534f\u540c\u624b\u52bf\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u8eab\u4f53\u8fd0\u52a8\u4e0e\u9762\u90e8\u8868\u60c5\u7684\u8bed\u4e49\u4e0d\u534f\u8c03\uff1b2\uff09\u7531\u4e8e\u90e8\u5206\u5206\u89e3\u6216\u5e27\u7ea7\u56de\u5f52\u65b9\u6cd5\u5bfc\u81f4\u7684\u7a7a\u95f4\u4e0d\u7a33\u5b9a\u65e0\u610f\u4e49\u8fd0\u52a8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7a7a\u95f4\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u6574\u4f53\u624b\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa3DGesPolicy\u6846\u67b6\uff0c\u5c06\u6574\u4f53\u624b\u52bf\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u8fde\u7eed\u8f68\u8ff9\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528\u6269\u6563\u7b56\u7565\u3002\u901a\u8fc7\u5c06\u5e27\u95f4\u53d8\u5316\u5efa\u6a21\u4e3a\u7edf\u4e00\u7684\u6574\u4f53\u52a8\u4f5c\uff0c\u5b66\u4e60\u5e27\u95f4\u6574\u4f53\u624b\u52bf\u8fd0\u52a8\u6a21\u5f0f\u3002\u540c\u65f6\u63d0\u51faGesture-Audio-Phoneme\uff08GAP\uff09\u878d\u5408\u6a21\u5757\uff0c\u6df1\u5ea6\u6574\u5408\u591a\u6a21\u6001\u4fe1\u53f7\uff0c\u786e\u4fdd\u8bed\u97f3\u8bed\u4e49\u3001\u8eab\u4f53\u8fd0\u52a8\u548c\u9762\u90e8\u8868\u8fbe\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "result": "\u5728BEAT2\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c3DGesPolicy\u5728\u751f\u6210\u81ea\u7136\u3001\u8868\u8fbe\u6027\u5f3a\u4e14\u4e0e\u8bed\u97f3\u9ad8\u5ea6\u5bf9\u9f50\u7684\u6574\u4f53\u624b\u52bf\u65b9\u9762\uff0c\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "3DGesPolicy\u901a\u8fc7\u52a8\u4f5c\u5efa\u6a21\u548c\u6269\u6563\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6574\u4f53\u624b\u52bf\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u534f\u8c03\u548c\u7a7a\u95f4\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7ed3\u5408GAP\u878d\u5408\u6a21\u5757\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u4fe1\u53f7\u7684\u6df1\u5ea6\u6574\u5408\uff0c\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u534f\u540c\u624b\u52bf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18580", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18580", "abs": "https://arxiv.org/abs/2601.18580", "authors": ["Vincenzo De Paola", "Mirco Mutti", "Riccardo Zamboni", "Marcello Restelli"], "title": "K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents", "comment": null, "summary": "Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.", "AI": {"tldr": "K-Myriad\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5e76\u884c\u7b56\u7565\u7fa4\u4f53\u8bf1\u5bfc\u7684\u96c6\u4f53\u72b6\u6001\u71b5\uff0c\u57f9\u517b\u4e13\u95e8\u5316\u63a2\u7d22\u7b56\u7565\u7ec4\u5408\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u9c81\u68d2\u521d\u59cb\u5316\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u5e76\u53d1\u73b0\u5f02\u6784\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5e76\u884c\u5316\u901a\u5e38\u7528\u4e8e\u52a0\u901f\u5355\u4e2a\u7b56\u7565\u8bad\u7ec3\uff0c\u591a\u4e2a\u5de5\u4f5c\u8005\u4ece\u76f8\u540c\u7684\u91c7\u6837\u5206\u5e03\u6536\u96c6\u7ecf\u9a8c\u3002\u8fd9\u79cd\u8bbe\u8ba1\u9650\u5236\u4e86\u5e76\u884c\u5316\u7684\u6f5c\u529b\uff0c\u5ffd\u7565\u4e86\u591a\u6837\u5316\u63a2\u7d22\u7b56\u7565\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faK-Myriad\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5e76\u884c\u7b56\u7565\u7fa4\u4f53\u8bf1\u5bfc\u7684\u96c6\u4f53\u72b6\u6001\u71b5\uff0c\u57f9\u517b\u4e13\u95e8\u5316\u63a2\u7d22\u7b56\u7565\u7ec4\u5408\u3002\u8fd9\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u96c6\u4f53\u63a2\u7d22\u3002", "result": "\u5728\u9ad8\u7ef4\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u5e76\u884c\u5316\u5b9e\u9a8c\u4e2d\uff0cK-Myriad\u80fd\u591f\u5b66\u4e60\u5230\u5e7f\u6cdb\u7684\u72ec\u7279\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u96c6\u4f53\u63a2\u7d22\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "K-Myriad\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u9c81\u68d2\u521d\u59cb\u5316\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u5e76\u53d1\u73b0\u4e86\u5f02\u6784\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u65b0\u578b\u5e76\u884c\u5316\u7b56\u7565\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2601.18464", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18464", "abs": "https://arxiv.org/abs/2601.18464", "authors": ["Wenbin Wei", "Suyuan Yao", "Cheng Huang", "Xiangyu Gao"], "title": "Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System", "comment": null, "summary": "Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.", "AI": {"tldr": "Fair-Eye Net\u662f\u4e00\u4e2a\u516c\u5e73\u3001\u53ef\u9760\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u9752\u5149\u773c\u4ece\u7b5b\u67e5\u5230\u968f\u8bbf\u7684\u4e34\u5e8a\u95ed\u73af\u7ba1\u7406\uff0c\u901a\u8fc7\u878d\u5408\u773c\u5e95\u7167\u7247\u3001OCT\u3001\u89c6\u91ce\u68c0\u67e5\u7b49\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u91c7\u7528\u516c\u5e73\u6027\u7ea6\u675f\u51cf\u5c11\u5f31\u52bf\u7fa4\u4f53\u7684\u6f0f\u8bca\u7387\u3002", "motivation": "\u9752\u5149\u773c\u662f\u5168\u7403\u4e0d\u53ef\u9006\u6027\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u5f53\u524d\u7b5b\u67e5\u548c\u8fdb\u5c55\u8bc4\u4f30\u4f9d\u8d56\u5355\u4e00\u6d4b\u8bd5\u6216\u677e\u6563\u5173\u8054\u7684\u68c0\u67e5\uff0c\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u788e\u7247\u5316\u62a4\u7406\u95ee\u9898\u3002\u9ad8\u8d28\u91cf\u6210\u50cf\u5de5\u5177\u548c\u4e13\u5bb6\u8d44\u6e90\u7684\u6709\u9650\u83b7\u53d6\u8fdb\u4e00\u6b65\u5f71\u54cd\u4e86\u73b0\u5b9e\u4e16\u754c\u4f7f\u7528\u7684\u4e00\u81f4\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u5f00\u53d1\u4e86Fair-Eye Net\u7cfb\u7edf\uff0c\u6574\u5408\u773c\u5e95\u7167\u7247\u3001OCT\u7ed3\u6784\u6307\u6807\u3001\u89c6\u91ce\u529f\u80fd\u6307\u6570\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u56e0\u7d20\uff0c\u91c7\u7528\u53cc\u6d41\u5f02\u6784\u878d\u5408\u67b6\u6784\uff0c\u914d\u5907\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5206\u5c42\u95e8\u63a7\u7b56\u7565\u8fdb\u884c\u9009\u62e9\u6027\u9884\u6d4b\u548c\u5b89\u5168\u8f6c\u8bca\u3002\u901a\u8fc7\u516c\u5e73\u6027\u7ea6\u675f\u51cf\u5c11\u5f31\u52bf\u4e9a\u7ec4\u7684\u6f0f\u8bca\u7387\u3002", "result": "\u7cfb\u7edf\u8fbe\u5230AUC 0.912\uff08\u7279\u5f02\u602796.7%\uff09\uff0c\u5c06\u79cd\u65cf\u5047\u9634\u6027\u5dee\u5f02\u51cf\u5c11\u4e8673.4%\uff08\u4ece12.31%\u964d\u81f33.28%\uff09\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u8de8\u57df\u6027\u80fd\uff0c\u5e76\u80fd\u63d0\u4f9b3-12\u4e2a\u6708\u7684\u65e9\u671f\u98ce\u9669\u9884\u8b66\uff08\u654f\u611f\u602792%\uff0c\u7279\u5f02\u602788%\uff09\u3002", "conclusion": "Fair-Eye Net\u5c06\u516c\u5e73\u6027\u4f5c\u4e3a\u4e3b\u8981\u76ee\u6807\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u786e\u4fdd\u4e34\u5e8a\u53ef\u9760\u6027\uff0c\u4e3a\u4e34\u5e8a\u8f6c\u5316\u548c\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u5168\u7403\u773c\u5065\u5eb7\u516c\u5e73\u3002"}}
{"id": "2601.18586", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18586", "abs": "https://arxiv.org/abs/2601.18586", "authors": ["Miguel Costa", "Arthur Vandervoort", "Carolin Schmidt", "Morten W. Petersen", "Martin Drews", "Karyn Morrissey", "Francisco C. Pereira"], "title": "Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning", "comment": null, "summary": "Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u96c6\u6210\u8bc4\u4f30\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u5728\u6c14\u5019\u53d8\u5316\u4e0b\u7684\u591a\u5341\u5e74\u9002\u5e94\u6027\u6295\u8d44\u8def\u5f84", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u964d\u96e8\u7b49\u707e\u5bb3\uff0c\u589e\u52a0\u57ce\u5e02\u4ea4\u901a\u7cfb\u7edf\u4e2d\u65ad\u3002\u8bbe\u8ba1\u6709\u6548\u9002\u5e94\u7b56\u7565\u9762\u4e34\u957f\u671f\u57fa\u7840\u8bbe\u65bd\u6295\u8d44\u5e8f\u5217\u6027\u3001\u6df1\u5ea6\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u8de8\u90e8\u95e8\u4ea4\u4e92\u7684\u6311\u6218", "method": "\u63d0\u51fa\u901a\u7528\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u5c06\u96c6\u6210\u8bc4\u4f30\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u8026\u5408\uff0c\u7ed3\u5408\u957f\u671f\u6c14\u5019\u9884\u6d4b\u3001\u707e\u5bb3\u6982\u7387\u6620\u5c04\u3001\u57fa\u7840\u8bbe\u65bd\u5f71\u54cd\u4f20\u64ad\u548c\u6210\u672c\u8bc4\u4f30\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\u5b66\u4e60\u9002\u5e94\u6027\u6c14\u5019\u9002\u5e94\u653f\u7b56", "result": "\u5728\u54e5\u672c\u54c8\u683c\u5185\u57ce\u96e8\u6d2a\u6848\u4f8b\u4e2d\uff082024-2100\uff09\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u4ea7\u751f\u534f\u8c03\u7684\u65f6\u7a7a\u8def\u5f84\uff0c\u76f8\u6bd4\u4f20\u7edf\u4f18\u5316\u57fa\u51c6\uff08\u4e0d\u4f5c\u4e3a\u548c\u968f\u673a\u884c\u52a8\uff09\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u53ef\u8f6c\u79fb\u5230\u5176\u4ed6\u707e\u5bb3\u548c\u57ce\u5e02\u7684\u6f5c\u529b\uff0c\u4e3a\u57ce\u5e02\u6c14\u5019\u9002\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177"}}
{"id": "2601.18532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18532", "abs": "https://arxiv.org/abs/2601.18532", "authors": ["Devon Levy", "Bar Assayag", "Laura Gaspar", "Ilan Shimshoni", "Bella Specktor-Fadida"], "title": "From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation", "comment": "19 pages without references", "summary": "Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u4e0e\u805a\u7c7b\u8fdb\u884c\u51b7\u542f\u52a8\u91c7\u6837\uff0c\u518d\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u8981\u7cbe\u786e\u6807\u6ce8\uff0c\u4f46\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u8017\u529b\u3002\u4e3b\u52a8\u5b66\u4e60\u901a\u8fc7\u4f18\u5148\u6807\u6ce8\u4fe1\u606f\u91cf\u5927\u7684\u6837\u672c\u6765\u51cf\u8f7b\u6807\u6ce8\u8d1f\u62c5\uff0c\u4f46\u4f20\u7edf\u7684\u51b7\u542f\u52a8\u9636\u6bb5\u901a\u5e38\u57fa\u4e8e\u591a\u6837\u6027\u9009\u62e9\uff0c\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u51b7\u542f\u52a8\u91c7\u6837\u7b56\u7565\uff1a\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u4e0e\u805a\u7c7b\uff0c\u5305\u62ec\u81ea\u52a8\u9009\u62e9\u805a\u7c7b\u6570\u91cf\u548c\u6309\u6bd4\u4f8b\u8de8\u805a\u7c7b\u91c7\u6837\uff0c\u6784\u5efa\u591a\u6837\u4e14\u5177\u4ee3\u8868\u6027\u7684\u521d\u59cb\u8bad\u7ec3\u96c6\u3002\u968f\u540e\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u7a7a\u95f4\u591a\u6837\u6027\u6307\u5bfc\u6837\u672c\u9009\u62e9\u3002\u65b9\u6cd5\u76f4\u89c2\u53ef\u89e3\u91ca\uff0c\u652f\u6301\u53ef\u89c6\u5316\u5019\u9009\u6837\u672c\u7684\u7279\u5f81\u7a7a\u95f4\u5206\u5e03\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff1aCheXmask\u6570\u636e\u96c6\u4e0a\uff0c\u51b7\u542f\u52a8\u7b56\u7565\u4f18\u4e8e\u968f\u673a\u9009\u62e9\uff0cDice\u4ece0.918\u63d0\u5347\u81f30.929\uff0cHausdorff\u8ddd\u79bb\u4ece32.41\u964d\u81f327.66mm\uff1b\u4e3b\u52a8\u5b66\u4e60\u7ed3\u5408\u71b5\u548c\u591a\u6837\u6027\u9009\u62e9\u5c06Dice\u4ece0.919\u63d0\u5347\u81f30.939\uff0cHausdorff\u8ddd\u79bb\u4ece30.10\u964d\u81f319.16mm\u3002Montgomery\u6570\u636e\u96c6\u4e0a\u51b7\u542f\u52a8\u6548\u679c\u663e\u8457\uff0cDice\u4ece0.928\u63d0\u5347\u81f30.950\uff0cHausdorff\u8ddd\u79bb\u4ece14.22\u964d\u81f39.38mm\u3002SynthStrip\u6570\u636e\u96c6\u4e0a\u51b7\u542f\u52a8\u7565\u5fae\u5f71\u54cdDice\u4f46\u964d\u4f4eHausdorff\u8ddd\u79bb\uff0c\u4e3b\u52a8\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4f4e\u6570\u636e\u91cf\u60c5\u51b5\u4e0b\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u51c6\u786e\u6027\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6807\u6ce8\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18543", "abs": "https://arxiv.org/abs/2601.18543", "authors": ["Kaixun Jiang", "Yuzheng Wang", "Junjie Zhou", "Pandeng Li", "Zhihang Liu", "Chen-Wei Xie", "Zhaoyu Chen", "Yun Zheng", "Wenqiang Zhang"], "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning", "comment": null, "summary": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{https://github.com/deep-kaixun/GenAgent}{this url}.", "AI": {"tldr": "GenAgent\u662f\u4e00\u4e2a\u901a\u8fc7\u667a\u80fd\u4f53\u6846\u67b6\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u7684\u6a21\u578b\uff0c\u5b83\u5c06\u7406\u89e3\u4efb\u52a1\u4ea4\u7ed9\u591a\u6a21\u6001\u6a21\u578b\u5904\u7406\uff0c\u800c\u5c06\u751f\u6210\u4efb\u52a1\u59d4\u6258\u7ed9\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u652f\u6301\u81ea\u4e3b\u591a\u8f6e\u4ea4\u4e92\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u7edf\u4e00\u6a21\u578b\u9762\u4e34\u6602\u8d35\u7684\u8bad\u7ec3\u6210\u672c\u4ee5\u53ca\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u73b0\u6709\u6a21\u5757\u5316\u7cfb\u7edf\u53d7\u9650\u4e8e\u9759\u6001\u6d41\u6c34\u7ebf\uff0c\u65e0\u6cd5\u652f\u6301\u81ea\u4e3b\u7684\u591a\u8f6e\u4ea4\u4e92\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u7406\u89e3\u4e0e\u751f\u6210\u89e3\u8026\uff1a\u591a\u6a21\u6001\u6a21\u578b\u8d1f\u8d23\u7406\u89e3\uff0c\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\u3002\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1) \u76d1\u7763\u5fae\u8c03\u9ad8\u8d28\u91cf\u5de5\u5177\u8c03\u7528\u548c\u53cd\u601d\u6570\u636e\uff1b2) \u7aef\u5230\u7aef\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u70b9\u5bf9\u70b9\u5956\u52b1\uff08\u6700\u7ec8\u56fe\u50cf\u8d28\u91cf\uff09\u548c\u6210\u5bf9\u5956\u52b1\uff08\u53cd\u601d\u51c6\u786e\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u8f68\u8ff9\u91cd\u91c7\u6837\u589e\u5f3a\u591a\u8f6e\u63a2\u7d22\u3002", "result": "GenAgent\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u751f\u6210\u5668\uff08FLUX.1-dev\uff09\u5728GenEval++\uff08+23.6%\uff09\u548cWISE\uff08+14%\uff09\u4e0a\u7684\u6027\u80fd\u3002\u6846\u67b6\u5c55\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u7279\u6027\uff1a1) \u8de8\u5de5\u5177\u6cdb\u5316\u5230\u4e0d\u540c\u80fd\u529b\u7684\u751f\u6210\u5668\uff1b2) \u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\uff0c\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u6301\u7eed\u6539\u8fdb\uff1b3) \u4efb\u52a1\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u80fd\u81ea\u52a8\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u3002", "conclusion": "GenAgent\u901a\u8fc7\u667a\u80fd\u4f53\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7edf\u4e00\u6a21\u578b\u7684\u8bad\u7ec3\u6210\u672c\u548c\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u652f\u6301\u81ea\u4e3b\u591a\u8f6e\u4ea4\u4e92\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2601.18620", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18620", "abs": "https://arxiv.org/abs/2601.18620", "authors": ["Panagiotis Lymperopoulos", "Abhiramon Rajasekharan", "Ian Berlot-Attwell", "St\u00e9phane Aroca-Ouellette", "Kaheer Suleman"], "title": "CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling", "comment": "28 pages, 2 figures", "summary": "Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.", "AI": {"tldr": "CASSANDRA\uff1a\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u5efa\u6a21\u65b9\u6cd5\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u77e5\u8bc6\u5148\u9a8c\u6784\u5efa\u8f7b\u91cf\u7ea7\u8fc7\u6e21\u6a21\u578b\u7528\u4e8e\u89c4\u5212\uff0c\u5728\u5496\u5561\u5e97\u548c\u4e3b\u9898\u516c\u56ed\u4e1a\u52a1\u6a21\u62df\u5668\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf", "motivation": "\u73b0\u5b9e\u4e16\u754c\u9886\u57df\uff08\u5982\u5546\u4e1a\uff09\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\uff0c\u53ef\u4ee5\u5229\u7528\u4e16\u754c\u77e5\u8bc6\u4ece\u6709\u9650\u6570\u636e\u4e2d\u6709\u6548\u5efa\u6a21\u590d\u6742\u7684\u884c\u52a8\u6548\u679c\u548c\u56e0\u679c\u5173\u7cfb\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u590d\u6742\u6027\uff0c\u9700\u8981\u7ed3\u5408\u7b26\u53f7\u77e5\u8bc6\u548c\u6982\u7387\u5efa\u6a21\u3002", "method": "CASSANDRA\u6574\u5408\u4e24\u4e2a\u7ec4\u4ef6\uff1a1\uff09LLM\u5408\u6210\u7684\u4ee3\u7801\u7528\u4e8e\u5efa\u6a21\u786e\u5b9a\u6027\u7279\u5f81\uff1b2\uff09LLM\u5f15\u5bfc\u7684\u6982\u7387\u56fe\u6a21\u578b\u7ed3\u6784\u5b66\u4e60\uff0c\u7528\u4e8e\u6355\u6349\u968f\u673a\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002\u8fd9\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u77e5\u8bc6\u5148\u9a8c\u3002", "result": "\u5728\u5c0f\u578b\u5496\u5561\u5e97\u6a21\u62df\u5668\u548c\u590d\u6742\u7684\u4e3b\u9898\u516c\u56ed\u4e1a\u52a1\u6a21\u62df\u5668\u4e2d\u8bc4\u4f30\uff0cCASSANDRA\u5728\u8fc7\u6e21\u9884\u6d4b\u548c\u89c4\u5212\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "CASSANDRA\u901a\u8fc7\u7ed3\u5408LLM\u7684\u77e5\u8bc6\u5148\u9a8c\u548c\u6982\u7387\u5efa\u6a21\uff0c\u80fd\u591f\u6709\u6548\u6784\u5efa\u73b0\u5b9e\u4e16\u754c\u9886\u57df\u7684\u8f7b\u91cf\u7ea7\u8fc7\u6e21\u6a21\u578b\uff0c\u4e3a\u590d\u6742\u4e1a\u52a1\u73af\u5883\u4e2d\u7684\u89c4\u5212\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2601.18547", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.18547", "abs": "https://arxiv.org/abs/2601.18547", "authors": ["Qing Ding", "Mai Xu", "Shengxi Li", "Xin Deng", "Xin Zou"], "title": "REMAC: Reference-Based Martian Asymmetrical Image Compression", "comment": "Accepted for publication in IEEE Transactions on Geoscience and Remote Sensing (TGRS). 2025 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. 18 pages, 20 figures", "summary": "To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \\textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \\textit{intra-} and \\textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \\textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \\textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.", "AI": {"tldr": "\u63d0\u51faREMAC\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u56fe\u50cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u8f6c\u79fb\u7b56\u7565\uff0c\u5728\u706b\u661f\u56fe\u50cf\u538b\u7f29\u4e2d\u5b9e\u73b043.51%\u7684\u7f16\u7801\u5668\u590d\u6742\u5ea6\u964d\u4f4e\u548c0.2664 dB\u7684BD-PSNR\u589e\u76ca", "motivation": "\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u5728\u706b\u661f\u56fe\u50cf\u538b\u7f29\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u5ffd\u7565\u4e86\u706b\u661f\u4e0a\u6781\u5176\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\uff1b2\uff09\u6ca1\u6709\u5229\u7528\u706b\u661f\u56fe\u50cf\u95f4\u5f3a\u70c8\u7684\u56fe\u50cf\u95f4\u76f8\u4f3c\u6027\u6765\u63d0\u5347\u538b\u7f29\u6027\u80fd\u3002\u57fa\u4e8e\u5bf9\u706b\u661f\u56fe\u50cf\u5728\u7eb9\u7406\u3001\u989c\u8272\u548c\u8bed\u4e49\u65b9\u9762\u7684\u5f3a\u56fe\u50cf\u5185\u548c\u56fe\u50cf\u95f4\u76f8\u4f3c\u6027\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u9700\u8981\u5f00\u53d1\u9002\u5408\u706b\u661f\u73af\u5883\u7684\u9ad8\u6548\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53c2\u8003\u7684\u706b\u661f\u975e\u5bf9\u79f0\u56fe\u50cf\u538b\u7f29\uff08REMAC\uff09\u65b9\u6cd5\uff1a1\uff09\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u7f16\u7801\u5668\u8f6c\u79fb\u5230\u8d44\u6e90\u4e30\u5bcc\u7684\u89e3\u7801\u5668\uff1b2\uff09\u5229\u7528\u56fe\u50cf\u95f4\u76f8\u4f3c\u6027\uff0c\u63d0\u51fa\u53c2\u8003\u5f15\u5bfc\u7684\u71b5\u6a21\u5757\u548c\u53c2\u8003\u89e3\u7801\u5668\uff0c\u5229\u7528\u53c2\u8003\u56fe\u50cf\u7684\u6709\u7528\u4fe1\u606f\uff1b3\uff09\u5229\u7528\u56fe\u50cf\u5185\u76f8\u4f3c\u6027\uff0c\u53c2\u8003\u89e3\u7801\u5668\u91c7\u7528\u6df1\u5ea6\u591a\u5c3a\u5ea6\u67b6\u6784\u6269\u5927\u611f\u53d7\u91ce\uff1b4\uff09\u5f00\u53d1\u6f5c\u5728\u7279\u5f81\u56de\u6536\u673a\u5236\u8fdb\u4e00\u6b65\u7f13\u89e3\u706b\u661f\u8ba1\u7b97\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cREMAC\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u51cf\u5c11\u4e8643.51%\u7684\u7f16\u7801\u5668\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5b9e\u73b0\u4e860.2664 dB\u7684BD-PSNR\u589e\u76ca\uff0c\u5728\u4fdd\u6301\u538b\u7f29\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u706b\u661f\u7aef\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "conclusion": "REMAC\u901a\u8fc7\u5229\u7528\u706b\u661f\u56fe\u50cf\u7684\u5f3a\u76f8\u4f3c\u6027\u7279\u5f81\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u4ece\u8d44\u6e90\u53d7\u9650\u7684\u7f16\u7801\u5668\u5411\u8d44\u6e90\u4e30\u5bcc\u7684\u89e3\u7801\u5668\u7684\u8f6c\u79fb\uff0c\u4e3a\u706b\u661f\u63a2\u7d22\u4e2d\u7684\u56fe\u50cf\u4f20\u8f93\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18626", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.18626", "abs": "https://arxiv.org/abs/2601.18626", "authors": ["Yingxiao Huo", "Satya Prakash Dash", "Radu Stoican", "Samuel Kaski", "Mingfei Sun"], "title": "Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning", "comment": null, "summary": "Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79e9-1\u8fd1\u4f3c\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u9006Fisher\u4fe1\u606f\u77e9\u9635\u6765\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4f18\u4e8e\u6807\u51c6actor-critic\u548c\u4fe1\u8d56\u57df\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u81ea\u7136\u68af\u5ea6\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u5feb\u901f\u6536\u655b\u7279\u6027\uff0c\u4f46\u8ba1\u7b97\u81ea\u7136\u68af\u5ea6\u9700\u8981\u6bcf\u6b21\u8fed\u4ee3\u90fd\u6c42\u9006Fisher\u4fe1\u606f\u77e9\u9635(FIM)\uff0c\u8fd9\u5728\u8ba1\u7b97\u4e0a\u662f\u4e0d\u53ef\u884c\u7684\u3002\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u79e9-1\u8fd1\u4f3c\u6765\u903c\u8fd1\u5b8c\u6574\u9006FIM\u7684\u9ad8\u6548\u81ea\u7136\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u79e9-1\u8fd1\u4f3c\u9006FIM\u6bd4\u7b56\u7565\u68af\u5ea6\u6536\u655b\u66f4\u5feb\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u5177\u6709\u4e0e\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u76f8\u540c\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u73af\u5883\u4e2d\u5bf9\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6807\u51c6\u7684actor-critic\u548c\u4fe1\u8d56\u57df\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u81ea\u7136\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u79e9-1\u8fd1\u4f3c\u9006FIM\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.18555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18555", "abs": "https://arxiv.org/abs/2601.18555", "authors": ["Roberto Di Via", "Vito Paolo Pastore", "Francesca Odone", "Si\u00f4n Glyn-Jones", "Irina Voiculescu"], "title": "Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray", "comment": "Accepted at International Symposium on Biomedical Imaging (ISBI 2026)", "summary": "Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions", "AI": {"tldr": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u4f7f\u7528\u6807\u51c6\u70ed\u56fe\u56de\u5f52\u67b6\u6784\u5728MRI\u4e0a\u68c0\u6d4b\u9acb\u5173\u8282\u649e\u51fb\u7efc\u5408\u5f81(FAI)\u6807\u5fd7\u70b9\u7684\u53ef\u884c\u6027\uff0c\u8bc1\u660eMRI\u4e0eX\u5c04\u7ebf\u5728\u5b9a\u4f4d\u548c\u8bca\u65ad\u51c6\u786e\u6027\u4e0a\u5177\u6709\u4e34\u5e8a\u7b49\u6548\u6027\u3002", "motivation": "\u4e34\u5e8a\u7b5b\u67e5\u51b3\u7b56\u5e38\u57fa\u4e8e\u89d2\u5ea6\u6d4b\u91cf\uff0c\u7279\u522b\u662f\u9acb\u5173\u8282\u649e\u51fb\u7efc\u5408\u5f81(FAI)\u7b5b\u67e5\u4f20\u7edf\u4e0a\u4f9d\u8d56X\u5c04\u7ebf\u6d4b\u91cf\u89d2\u5ea6\u3002\u7136\u800c\uff0c\u8bc4\u4f30\u649e\u51fb\u533a\u57df\u7684\u9ad8\u5ea6\u548c\u8303\u56f4\u9700\u8981MRI\u626b\u63cf\u76843D\u89c6\u56fe\u3002\u4e24\u79cd\u6a21\u6001\u4e3a\u5916\u79d1\u533b\u751f\u63d0\u4f9b\u4e0d\u540c\u65b9\u9762\u7684\u4fe1\u606f\uff0c\u9700\u8981\u9a8c\u8bc1MRI\u5728FAI\u8bc4\u4f30\u4e2d\u7684\u4e34\u5e8a\u7b49\u6548\u6027\u3002", "method": "\u91c7\u7528\u5339\u914d\u961f\u5217\u9a8c\u8bc1\u7814\u7a76(89\u540d\u60a3\u8005\uff0c\u914d\u5bf9MRI/X\u5c04\u7ebf)\uff0c\u4f7f\u7528\u6807\u51c6\u70ed\u56fe\u56de\u5f52\u67b6\u6784\u8bc4\u4f30\u8de8\u6a21\u6001\u4e34\u5e8a\u7b49\u6548\u6027\u3002\u57283D MRI\u4f53\u79ef\u7684\u51a0\u72b6\u89c6\u56fe\u4e2d\u8fdb\u884c\u6807\u5fd7\u70b9\u68c0\u6d4b\uff0c\u4e3a\u540e\u7eed\u901a\u8fc7\u653e\u7f6e\u66f4\u591a\u6807\u5fd7\u70b9\u8fdb\u884c\u4f53\u79ef\u5206\u6790\u63d0\u4f9b\u53ef\u80fd\u6027\u3002", "result": "\u7814\u7a76\u8868\u660eMRI\u5728cam\u578b\u649e\u51fb\u7efc\u5408\u5f81\u7684\u6807\u5fd7\u70b9\u5b9a\u4f4d\u548c\u8bca\u65ad\u51c6\u786e\u6027\u65b9\u9762\u4e0eX\u5c04\u7ebf\u5177\u6709\u7b49\u6548\u6027\u3002\u65b9\u6cd5\u57283D MRI\u4f53\u79ef\u7684\u51a0\u72b6\u89c6\u56fe\u4e2d\u5c55\u793a\u4e86\u4e34\u5e8a\u53ef\u884c\u6027\uff0c\u652f\u6301\u5c06\u81ea\u52a8\u5316FAI\u8bc4\u4f30\u6574\u5408\u5230\u5e38\u89c4MRI\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u6807\u51c6\u70ed\u56fe\u56de\u5f52\u67b6\u6784\u5728MRI\u4e0a\u68c0\u6d4bFAI\u6807\u5fd7\u70b9\u7684\u4e34\u5e8a\u53ef\u884c\u6027\uff0c\u4e3a\u901a\u8fc7\u653e\u7f6e\u66f4\u591a\u6807\u5fd7\u70b9\u8fdb\u884c\u4f53\u79ef\u5206\u6790\u5f00\u8f9f\u4e86\u53ef\u80fd\u6027\uff0c\u652f\u6301\u5c06\u81ea\u52a8\u5316FAI\u8bc4\u4f30\u6574\u5408\u5230\u5e38\u89c4MRI\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2601.18556", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18556", "abs": "https://arxiv.org/abs/2601.18556", "authors": ["Jingsong Xia", "Siqi Wang"], "title": "Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis", "comment": null, "summary": "In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.", "AI": {"tldr": "SDA-QEC\u6846\u67b6\u7ed3\u5408\u7b80\u5316\u6269\u6563\u589e\u5f3a\u4e0e\u91cf\u5b50\u589e\u5f3a\u5206\u7c7b\uff0c\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u56fe\u50cf\u5206\u7c7b\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u771f\u5b9e\u533b\u5b66\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u9633\u6027\u6837\u672c\u8fdc\u591a\u4e8e\u9634\u6027\u6837\u672c\uff0c\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u591a\u6570\u7c7b\uff0c\u5bf9\u5c11\u6570\u7c7b\u53ec\u56de\u7387\u4f4e\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u5e76\u5e26\u6765\u4e34\u5e8a\u8bef\u8bca\u98ce\u9669\u3002", "method": "\u63d0\u51faSDA-QEC\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6269\u6563\u589e\u5f3a\u5668\u4e3a\u5c11\u6570\u7c7b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6837\u672c\uff0c\u5e73\u8861\u8bad\u7ec3\u5206\u5e03\uff1b2\uff09\u5728MobileNetV2\u67b6\u6784\u4e2d\u5d4c\u5165\u91cf\u5b50\u7279\u5f81\u5c42\uff0c\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u9ad8\u7ef4\u7279\u5f81\u6620\u5c04\u589e\u5f3a\u6a21\u578b\u5224\u522b\u80fd\u529b\u3002", "result": "\u5728\u51a0\u72b6\u52a8\u8109\u9020\u5f71\u56fe\u50cf\u5206\u7c7b\u5b9e\u9a8c\u4e2d\uff0cSDA-QEC\u8fbe\u523098.33%\u51c6\u786e\u7387\u300198.78% AUC\u548c98.33% F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8eResNet18\u3001MobileNetV2\u3001DenseNet121\u548cVGG16\u7b49\u7ecf\u5178\u57fa\u7ebf\u6a21\u578b\u3002\u540c\u65f6\u83b7\u5f9798.33%\u654f\u611f\u6027\u548c98.33%\u7279\u5f02\u6027\uff0c\u5b9e\u73b0\u4e34\u5e8a\u90e8\u7f72\u6240\u9700\u7684\u5e73\u8861\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5728\u771f\u5b9e\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u96c6\u6210\u751f\u6210\u589e\u5f3a\u4e0e\u91cf\u5b50\u589e\u5f3a\u5efa\u6a21\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u53d1\u5728\u5c0f\u6837\u672c\u3001\u9ad8\u5ea6\u4e0d\u5e73\u8861\u548c\u9ad8\u98ce\u9669\u8bca\u65ad\u573a\u666f\u4e2d\u9ad8\u5ea6\u53ef\u9760\u7684\u533b\u5b66AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u9014\u5f84\u3002"}}
{"id": "2601.18640", "categories": ["cs.LG", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2601.18640", "abs": "https://arxiv.org/abs/2601.18640", "authors": ["Zhiwei Zheng", "Kevin Bryson"], "title": "TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning", "comment": null, "summary": "Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.\n  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as \"background\" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.\n  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.", "AI": {"tldr": "TwinPurify\u662f\u4e00\u4e2a\u57fa\u4e8eBarlow Twins\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6279\u91cf\u8f6c\u5f55\u7ec4\u6570\u636e\u4e2d\u63d0\u53d6\u80bf\u7624\u7279\u5f02\u6027\u4fe1\u53f7\uff0c\u65e0\u9700\u5916\u90e8\u53c2\u8003\uff0c\u901a\u8fc7\u5229\u7528\u540c\u4e00\u961f\u5217\u4e2d\u7684\u76f8\u90bb\u6b63\u5e38\u7ec4\u7ec7\u4f5c\u4e3a\"\u80cc\u666f\"\u6307\u5bfc\u6765\u89e3\u8026\u80bf\u7624\u7279\u5f02\u6027\u4fe1\u53f7\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u60a3\u8005\u961f\u5217\u7814\u7a76\u4ecd\u4f9d\u8d56\u6279\u91cf\u8f6c\u5f55\u7ec4\u6570\u636e\uff0c\u4f46\u80bf\u7624\u7eaf\u5ea6\u53d8\u5316\u4f1a\u63a9\u76d6\u80bf\u7624\u5185\u5728\u8f6c\u5f55\u4fe1\u53f7\u5e76\u9650\u5236\u4e0b\u6e38\u53d1\u73b0\u3002\u8bb8\u591a\u53bb\u5377\u79ef\u65b9\u6cd5\u5728\u5408\u6210\u6279\u91cf\u6df7\u5408\u7269\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7531\u4e8e\u672a\u5efa\u6a21\u7684\u751f\u7269\u548c\u6280\u672f\u53d8\u5f02\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u771f\u5b9e\u60a3\u8005\u961f\u5217\u3002", "method": "TwinPurify\u91c7\u7528Barlow Twins\u81ea\u76d1\u7763\u76ee\u6807\uff0c\u5b66\u4e60\u8fde\u7eed\u7684\u9ad8\u7ef4\u80bf\u7624\u5d4c\u5165\u8868\u793a\uff0c\u800c\u4e0d\u662f\u5c06\u6279\u91cf\u6df7\u5408\u7269\u5206\u89e3\u4e3a\u79bb\u6563\u7ec6\u80de\u7c7b\u578b\u5206\u6570\u3002\u5b83\u5229\u7528\u540c\u4e00\u961f\u5217\u4e2d\u7684\u76f8\u90bb\u6b63\u5e38\u7ec4\u7ec7\u4f5c\u4e3a\"\u80cc\u666f\"\u6307\u5bfc\uff0c\u65e0\u9700\u4f9d\u8d56\u4efb\u4f55\u5916\u90e8\u53c2\u8003\u5373\u53ef\u89e3\u8026\u80bf\u7624\u7279\u5f02\u6027\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u4e2a\u5927\u578b\u764c\u75c7\u961f\u5217\uff08RNA-seq\u548c\u5fae\u9635\u5217\u5e73\u53f0\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTwinPurify\u5728\u6062\u590d\u80bf\u7624\u5185\u5728\u548c\u514d\u75ab\u4fe1\u53f7\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u81ea\u7f16\u7801\u5668\uff09\u3002\u7eaf\u5316\u7684\u5d4c\u5165\u6539\u8fdb\u4e86\u5206\u5b50\u4e9a\u578b\u548c\u5206\u7ea7\u5206\u7c7b\uff0c\u589e\u5f3a\u4e86\u751f\u5b58\u6a21\u578b\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u539f\u59cb\u6279\u91cf\u8c31\u76f8\u6bd4\u66f4\u5177\u751f\u7269\u5b66\u610f\u4e49\u7684\u901a\u8def\u6d3b\u6027\u3002", "conclusion": "TwinPurify\u901a\u8fc7\u63d0\u4f9b\u53ef\u8f6c\u79fb\u7684\u6279\u91cf\u8f6c\u5f55\u7ec4\u53bb\u6c61\u67d3\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u4e34\u5e8a\u6570\u636e\u96c6\u5728\u5206\u5b50\u53d1\u73b0\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u4ee3\u8868\u4e86\u4e0e\u53bb\u5377\u79ef\u8303\u5f0f\u7684\u6839\u672c\u6027\u8f6c\u53d8\u3002"}}
{"id": "2601.18560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18560", "abs": "https://arxiv.org/abs/2601.18560", "authors": ["Li Fang", "Tianyu Li", "Yanghong Lin", "Shudong Zhou", "Wei Yao"], "title": "AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging", "comment": null, "summary": "As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u7684\u9ad8\u6548AI\u536b\u661f\u8fb9\u7f18\u8ba1\u7b97\u8303\u5f0f\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u975e\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u5c11\u6837\u672c\u5b66\u4e60\u7b56\u7565\uff0c\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u50cf\u7d20\u7ea7\u6807\u7b7e\u4f20\u64ad\u65b9\u6848\uff0c\u65e0\u9700\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u9002\u5408\u536b\u661f\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "motivation": "\u9ad8\u5149\u8c31\u6210\u50cf\u536b\u661f\u5177\u6709\u5f3a\u5927\u7684\u5149\u8c31\u6d4b\u91cf\u80fd\u529b\uff0c\u4f46\u536b\u661f\u4e0b\u884c\u4f20\u8f93\u901f\u5ea6\u5df2\u6210\u4e3a\u707e\u5bb3\u76d1\u6d4b\u548c\u5e94\u6025\u6d4b\u7ed8\u7b49\u9700\u8981\u5feb\u901f\u54cd\u5e94\u80fd\u529b\u7684\u5e94\u7528\u7684\u4e3b\u8981\u74f6\u9888\u3002\u9700\u8981\u8ba9\u536b\u661f\u5177\u5907\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u540c\u65f6\u8981\u9002\u5e94\u536b\u661f\u5e73\u53f0\u7684\u8d44\u6e90\u9650\u5236\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u975e\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u96c6\u6210\u5c11\u6837\u672c\u5b66\u4e60\u7b56\u7565\u3002\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u50cf\u7d20\u7ea7\u6807\u7b7e\u4f20\u64ad\u65b9\u6848\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u6784\u5efa\u7684\u951a\u70b9-\u50cf\u7d20\u4eb2\u548c\u77e9\u9635\u4f20\u64ad\u9009\u5b9a\u7684\u951a\u70b9\u6807\u7b7e\u83b7\u5f97\u521d\u59cb\u50cf\u7d20\u6807\u7b7e\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u4ece\u7a00\u758f\u56fe\u63a8\u5bfc\u7684\u95ed\u5f0f\u89e3\u66ff\u4ee3\u8fed\u4ee3\u8ba1\u7b97\u3002\u8fd8\u5f00\u53d1\u4e86\u57fa\u4e8e\u79e9\u7ea6\u675f\u7684\u56fe\u805a\u7c7b\u7b97\u6cd5\u6765\u786e\u5b9a\u951a\u70b9\u6807\u7b7e\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u536b\u661f\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\uff0c\u9002\u5e94\u536b\u661f\u8d44\u6e90\u7ea6\u675f\uff0c\u5904\u7406\u4f20\u611f\u5668\u6545\u969c\u548c\u626b\u63cf\u6a21\u5f0f\u9519\u8bef\u5bfc\u81f4\u7684\u56fe\u50cf\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u3002", "conclusion": "\u63d0\u51fa\u7684AI\u536b\u661f\u8fb9\u7f18\u8ba1\u7b97\u8303\u5f0f\u4f7f\u536b\u661f\u5177\u5907\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u975e\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u548c\u4e24\u9636\u6bb5\u50cf\u7d20\u7ea7\u6807\u7b7e\u4f20\u64ad\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u536b\u661f\u8d44\u6e90\u9650\u5236\u548c\u56fe\u50cf\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u707e\u5bb3\u76d1\u6d4b\u7b49\u9700\u8981\u5feb\u901f\u54cd\u5e94\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18650", "abs": "https://arxiv.org/abs/2601.18650", "authors": ["Liheng Yu", "Zhe Zhao", "Yuxuan Wang", "Pengkun Wang", "Binwu Wang", "Yang Wang"], "title": "FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning", "comment": "camera-ready for iclr2026", "summary": "Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten\". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \\textit{Heterogeneous Unlearning Deviation} and \\textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \\textbf{Supplementary Material}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u4efb\u52a1\u4e2d\u9057\u5fd8\u96c6\u5448\u957f\u5c3e\u5206\u5e03\u7684\u73b0\u5b9e\u573a\u666f\uff0c\u63d0\u51fa\u4e86FaLW\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u52a8\u6001\u635f\u5931\u91cd\u52a0\u6743\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u5f02\u8d28\u9057\u5fd8\u504f\u5dee\u548c\u503e\u659c\u9057\u5fd8\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u7814\u7a76\u4e3b\u8981\u8bc4\u4f30\u76f8\u5bf9\u5e73\u8861\u7684\u9057\u5fd8\u96c6\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u9057\u5fd8\u6570\u636e\uff08\u5982\u7528\u6237\u6d3b\u52a8\u8bb0\u5f55\uff09\u901a\u5e38\u9075\u5faa\u957f\u5c3e\u5206\u5e03\u7684\u60c5\u51b5\u3002\u8fd9\u662f\u9996\u6b21\u7814\u7a76\u8fd9\u4e00\u5173\u952e\u7814\u7a76\u7a7a\u767d\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u8bbe\u7f6e\u4e0b\u5b58\u5728\u5f02\u8d28\u9057\u5fd8\u504f\u5dee\u548c\u503e\u659c\u9057\u5fd8\u504f\u5dee\u4e24\u5927\u95ee\u9898\u3002", "method": "\u63d0\u51faFaLW\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u5b9e\u4f8b\u7ea7\u52a8\u6001\u635f\u5931\u91cd\u52a0\u6743\u65b9\u6cd5\u3002FaLW\u521b\u65b0\u6027\u5730\u901a\u8fc7\u6bd4\u8f83\u6bcf\u4e2a\u6837\u672c\u7684\u9884\u6d4b\u6982\u7387\u4e0e\u540c\u4e00\u7c7b\u522b\u672a\u89c1\u6570\u636e\u7684\u5206\u5e03\u6765\u8bc4\u4f30\u5176\u9057\u5fd8\u72b6\u6001\uff0c\u7136\u540e\u4f7f\u7528\u7531\u5e73\u8861\u56e0\u5b50\u8c03\u5236\u7684\u9057\u5fd8\u611f\u77e5\u91cd\u52a0\u6743\u65b9\u6848\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u4e2a\u6837\u672c\u7684\u9057\u5fd8\u5f3a\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eFaLW\u5728\u957f\u5c3e\u5206\u5e03\u573a\u666f\u4e0b\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u4ee3\u7801\u5df2\u5728\u8865\u5145\u6750\u6599\u4e2d\u63d0\u4f9b\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u5728\u957f\u5c3e\u5206\u5e03\u573a\u666f\u4e0b\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u51fa\u7684FaLW\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u8bbe\u7f6e\u4e0b\u7684\u504f\u5dee\u95ee\u9898\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18577", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18577", "abs": "https://arxiv.org/abs/2601.18577", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Saining Xie", "Jaehong Yoon", "Sung Ju Hwang"], "title": "Self-Refining Video Sampling", "comment": "Project page: https://agwmon.github.io/self-refine-video/", "summary": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u7ec6\u5316\u89c6\u9891\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u89e3\u91ca\u4e3a\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u8fed\u4ee3\u5185\u5faa\u73af\u7ec6\u5316\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u989d\u5916\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u8fde\u8d2f\u6027\u548c\u7269\u7406\u5bf9\u9f50\u3002", "motivation": "\u73b0\u4ee3\u89c6\u9891\u751f\u6210\u5668\u5728\u5904\u7406\u590d\u6742\u7269\u7406\u52a8\u529b\u5b66\u65f6\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u7269\u7406\u771f\u5b9e\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5916\u90e8\u9a8c\u8bc1\u5668\u6216\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u3002", "method": "\u81ea\u7ec6\u5316\u89c6\u9891\u91c7\u6837\u65b9\u6cd5\uff1a\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u89e3\u91ca\u4e3a\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u8fed\u4ee3\u5185\u5faa\u73af\u7ec6\u5316\uff1b\u5f15\u5165\u57fa\u4e8e\u81ea\u4e00\u81f4\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u7b56\u7565\uff0c\u9009\u62e9\u6027\u7ec6\u5316\u533a\u57df\u4ee5\u9632\u6b62\u8fc7\u5ea6\u7ec6\u5316\u5bfc\u81f4\u7684\u4f2a\u5f71\u3002", "result": "\u5728\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u5668\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u8fde\u8d2f\u6027\u548c\u7269\u7406\u5bf9\u9f50\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u76f8\u6bd4\u9ed8\u8ba4\u91c7\u6837\u5668\u548c\u57fa\u4e8e\u5f15\u5bfc\u7684\u91c7\u6837\u5668\uff0c\u83b7\u5f97\u8d85\u8fc770%\u7684\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "\u81ea\u7ec6\u5316\u89c6\u9891\u91c7\u6837\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u751f\u6210\u5668\u81ea\u8eab\u4f5c\u4e3a\u7ec6\u5316\u5668\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5916\u90e8\u9a8c\u8bc1\uff0c\u5c31\u80fd\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u771f\u5b9e\u6027\u548c\u8fd0\u52a8\u8d28\u91cf\u3002"}}
{"id": "2601.18585", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.18585", "abs": "https://arxiv.org/abs/2601.18585", "authors": ["Chenxi Liu", "Selena Ling", "Alec Jacobson"], "title": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization", "comment": null, "summary": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.", "AI": {"tldr": "GimmBO\uff1a\u57fa\u4e8e\u504f\u597d\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u4ea4\u4e92\u5f0f\u9002\u914d\u5668\u5408\u5e76\u63a2\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210", "motivation": "\u5f53\u524d\u57fa\u4e8e\u624b\u52a8\u6ed1\u5757\u8c03\u6574\u7684\u9002\u914d\u5668\u5408\u5e76\u65b9\u6cd5\u5728\u63a2\u7d22\u5927\u89c4\u6a21\u8bbe\u8ba1\u7a7a\u95f4\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5904\u740620-30\u4e2a\u9002\u914d\u5668\u7684\u5019\u9009\u96c6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4ea4\u4e92\u5f0f\u63a2\u7d22\u5de5\u5177", "method": "\u63d0\u51faGimmBO\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8d1d\u53f6\u65af\u4f18\u5316\u540e\u7aef\uff0c\u7ed3\u5408\u771f\u5b9e\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u7a00\u758f\u6027\u548c\u6743\u91cd\u8303\u56f4\u7ea6\u675f\uff0c\u63d0\u9ad8\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u91c7\u6837\u6548\u7387\u548c\u6536\u655b\u6027", "result": "\u901a\u8fc7\u6a21\u62df\u7528\u6237\u548c\u7528\u6237\u7814\u7a76\u8bc4\u4f30\uff0c\u76f8\u6bd4\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u7ebf\u641c\u7d22\u57fa\u7ebf\uff0cGimmBO\u663e\u793a\u51fa\u6539\u8fdb\u7684\u6536\u655b\u6027\u3001\u9ad8\u6210\u529f\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u6846\u67b6\u7684\u7075\u6d3b\u6027\u6269\u5c55", "conclusion": "GimmBO\u4e3a\u6269\u6563\u6a21\u578b\u9002\u914d\u5668\u5408\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4ea4\u4e92\u5f0f\u63a2\u7d22\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u7684\u6743\u91cd\u9009\u62e9\u96be\u9898"}}
{"id": "2601.18675", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18675", "abs": "https://arxiv.org/abs/2601.18675", "authors": ["Aditya Kumar", "Mario A. Cypko", "Oliver Amft"], "title": "Learning temporal embeddings from electronic health records of chronic kidney disease patients", "comment": "7 pages, 3 figures, 3 tables. The paper has been submitted to IEEE EMBC 2026 and copyright might be transferred without notice", "summary": "We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u65f6\u95f4\u5d4c\u5165\u6a21\u578b\u80fd\u5426\u5728\u4e0d\u727a\u7272\u9884\u6d4b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u8868\u793a\uff0c\u4ee5\u53ca\u67b6\u6784\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u5d4c\u5165\u8d28\u91cf\u3002\u901a\u8fc7\u6bd4\u8f83\u4e09\u79cd\u5faa\u73af\u67b6\u6784\u5728CKD\u60a3\u8005\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u65f6\u95f4\u611f\u77e5LSTM\u4ea7\u751f\u66f4\u7ed3\u6784\u5316\u7684\u5d4c\u5165\uff0c\u4e14\u5d4c\u5165\u6a21\u578b\u5728\u6b7b\u4ea1\u7387\u9884\u6d4b\u4e0a\u4f18\u4e8e\u7aef\u5230\u7aef\u6a21\u578b\u3002", "motivation": "\u6a21\u578b\u5f15\u5bfc\u533b\u5b66\u9700\u8981\u80fd\u591f\u6355\u6349\u75be\u75c5\u52a8\u6001\u540c\u65f6\u4fdd\u6301\u900f\u660e\u548c\u4efb\u52a1\u65e0\u5173\u7684\u8868\u793a\uff0c\u800c\u5927\u591a\u6570\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u4ec5\u9488\u5bf9\u5355\u4e00\u4efb\u52a1\u4f18\u5316\u3002\u8868\u793a\u5b66\u4e60\u6709\u52a9\u4e8e\u5b66\u4e60\u8de8\u4e0b\u6e38\u4efb\u52a1\u6cdb\u5316\u7684\u5d4c\u5165\uff0c\u5faa\u73af\u67b6\u6784\u9002\u5408\u5efa\u6a21\u4e34\u5e8a\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u7ed3\u6784\u3002", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\u96c6\u7814\u7a76\u6162\u6027\u80be\u810f\u75c5(CKD)\u60a3\u8005\uff0c\u6bd4\u8f83\u4e09\u79cd\u5faa\u73af\u67b6\u6784\uff1a\u666e\u901aLSTM\u3001\u6ce8\u610f\u529b\u589e\u5f3aLSTM\u548c\u65f6\u95f4\u611f\u77e5LSTM(T-LSTM)\u3002\u6240\u6709\u6a21\u578b\u65e2\u4f5c\u4e3a\u5d4c\u5165\u6a21\u578b\u8bad\u7ec3\uff0c\u4e5f\u4f5c\u4e3a\u76f4\u63a5\u7aef\u5230\u7aef\u9884\u6d4b\u5668\u8bad\u7ec3\u3002\u901a\u8fc7CKD\u9636\u6bb5\u805a\u7c7b\u548cICU\u5185\u6b7b\u4ea1\u7387\u9884\u6d4b\u8bc4\u4f30\u5d4c\u5165\u8d28\u91cf\u3002", "result": "T-LSTM\u4ea7\u751f\u66f4\u7ed3\u6784\u5316\u7684\u5d4c\u5165\uff0c\u83b7\u5f97\u66f4\u4f4e\u7684Davies-Bouldin\u6307\u6570(DBI=9.91)\u548c\u66f4\u9ad8\u7684CKD\u9636\u6bb5\u5206\u7c7b\u51c6\u786e\u7387(0.74)\uff0c\u4f18\u4e8e\u666e\u901aLSTM(DBI=15.85\uff0c\u51c6\u786e\u73870.63)\u548c\u6ce8\u610f\u529b\u589e\u5f3aLSTM(DBI=20.72\uff0c\u51c6\u786e\u73870.67)\u3002\u5728ICU\u5185\u6b7b\u4ea1\u7387\u9884\u6d4b\u4e2d\uff0c\u5d4c\u5165\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u7aef\u5230\u7aef\u9884\u6d4b\u5668\uff0c\u51c6\u786e\u7387\u4ece0.72-0.75\u63d0\u5347\u52300.82-0.83\u3002", "conclusion": "\u65f6\u95f4\u611f\u77e5LSTM\u67b6\u6784\u80fd\u591f\u5b66\u4e60\u66f4\u7ed3\u6784\u5316\u7684\u4e34\u5e8a\u8868\u793a\uff0c\u4e14\u5c06\u5d4c\u5165\u5b66\u4e60\u4f5c\u4e3a\u4e2d\u95f4\u6b65\u9aa4\u6bd4\u76f4\u63a5\u7aef\u5230\u7aef\u5b66\u4e60\u66f4\u6709\u6548\uff0c\u8868\u660e\u65f6\u95f4\u5d4c\u5165\u6a21\u578b\u53ef\u4ee5\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u5b66\u4e60\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u8868\u793a\u3002"}}
{"id": "2601.18676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18676", "abs": "https://arxiv.org/abs/2601.18676", "authors": ["Miles Martinez", "Alex H. Williams"], "title": "Quasi Monte Carlo methods enable extremely low-dimensional deep generative models", "comment": null, "summary": "This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.", "AI": {"tldr": "QLVMs\u662f\u4e00\u79cd\u4f7f\u7528\u51c6\u8499\u7279\u5361\u6d1b\u79ef\u5206\u76f4\u63a5\u8fd1\u4f3c\u8fb9\u7f18\u4f3c\u7136\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u5bfb\u627e\u9ad8\u7ef4\u6570\u636e\u7684\u6781\u4f4e\u7ef4\u53ef\u89e3\u91ca\u5d4c\u5165\uff0c\u57281-3\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfVAE\u548cIWAE\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b66\u4e60\u7f16\u7801\u5668\u548c\u53d8\u5206\u4e0b\u754c\uff0c\u96be\u4ee5\u83b7\u5f97\u6781\u4f4e\u7ef4\u4e14\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u5d4c\u5165\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u5bfb\u627e\u9ad8\u7ef4\u6570\u636e\u7684\u4f4e\u7ef4\u53ef\u89e3\u91ca\u5d4c\u5165\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u51c6\u8499\u7279\u5361\u6d1b\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\uff08QLVMs\uff09\uff0c\u901a\u8fc7\u968f\u673a\u51c6\u8499\u7279\u5361\u6d1b\u79ef\u5206\u76f4\u63a5\u8fd1\u4f3c\u8fb9\u7f18\u4f3c\u7136\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u53d8\u5206\u4e0b\u754c\u548c\u5b66\u4e60\u7f16\u7801\u5668\u3002\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf91-3\u7ef4\u6f5c\u5728\u7a7a\u95f4\u8bbe\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cQLVMs\u5728\u5339\u914d\u6f5c\u5728\u7ef4\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u91cd\u8981\u6027\u52a0\u6743\u81ea\u7f16\u7801\u5668\uff08IWAE\uff09\u3002\u751f\u6210\u7684\u5d4c\u5165\u652f\u6301\u900f\u660e\u53ef\u89c6\u5316\u3001\u975e\u53c2\u6570\u5bc6\u5ea6\u4f30\u8ba1\u3001\u805a\u7c7b\u548c\u6d4b\u5730\u8def\u5f84\u8ba1\u7b97\u7b49\u540e\u5206\u6790\u3002", "conclusion": "\u867d\u7136QLVMs\u8ba1\u7b97\u5bc6\u96c6\u4e14\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u96be\u4ee5\u751f\u6210\u7cbe\u7ec6\u7ec6\u8282\uff0c\u4f46\u5b83\u4e3a\u4f18\u5148\u8003\u8651\u53ef\u89e3\u91ca\u6027\u548c\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u5438\u5f15\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u6781\u4f4e\u7ef4\u53ef\u89e3\u91ca\u5d4c\u5165\u7684\u573a\u666f\u3002"}}
{"id": "2601.18678", "categories": ["cs.LG", "cs.CV", "cs.HC", "math.DG"], "pdf": "https://arxiv.org/pdf/2601.18678", "abs": "https://arxiv.org/abs/2601.18678", "authors": ["Eslam Zaher", "Maciej Trzaskowski", "Quan Nguyen", "Fred Roosta"], "title": "Counterfactual Explanations on Robust Perceptual Geodesics", "comment": "Accepted at ICLR 2026", "summary": "Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.", "AI": {"tldr": "PCG\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u9ece\u66fc\u5ea6\u91cf\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6d4b\u5730\u7ebf\u751f\u6210\u8bed\u4e49\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\uff0c\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u79bb\u6d41\u5f62\u4f2a\u5f71\u548c\u5bf9\u6297\u6027\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u8ddd\u79bb\u5ea6\u91cf\u9009\u62e9\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u6270\u52a8\u53ef\u80fd\u662f\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\uff0c\u4e5f\u53ef\u80fd\u662f\u5bf9\u6297\u6027\u7684\u3002\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u5e73\u5766\u6216\u4e0d\u5bf9\u9f50\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5bfc\u81f4\u79bb\u6d41\u5f62\u4f2a\u5f71\u3001\u8bed\u4e49\u6f02\u79fb\u6216\u5bf9\u6297\u6027\u5d29\u6e83\u3002", "method": "PCG\u65b9\u6cd5\u5728\u9c81\u68d2\u89c6\u89c9\u7279\u5f81\u8bf1\u5bfc\u7684\u611f\u77e5\u9ece\u66fc\u5ea6\u91cf\u4e0b\uff0c\u901a\u8fc7\u8ffd\u8e2a\u6d4b\u5730\u7ebf\u6784\u5efa\u53cd\u4e8b\u5b9e\u3002\u8fd9\u79cd\u51e0\u4f55\u7ed3\u6784\u4e0e\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\uff0c\u60e9\u7f5a\u8106\u5f31\u65b9\u5411\uff0c\u5b9e\u73b0\u5e73\u6ed1\u3001\u5728\u6d41\u5f62\u4e0a\u3001\u8bed\u4e49\u6709\u6548\u7684\u8f6c\u6362\u3002", "result": "\u5728\u4e09\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPCG\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u6807\u51c6\u5ea6\u91cf\u4e0b\u9690\u85cf\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "PCG\u901a\u8fc7\u611f\u77e5\u9ece\u66fc\u5ea6\u91cf\u6784\u5efa\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u8bed\u4e49\u6709\u6548\u3001\u5728\u6d41\u5f62\u4e0a\u7684\u53cd\u4e8b\u5b9e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u51e0\u4f55\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2601.18619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18619", "abs": "https://arxiv.org/abs/2601.18619", "authors": ["Jorge Quesada", "Ghassan AlRegib"], "title": "Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c3a\u5ea6\u611f\u77e5\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5c0f\u7a97\u53e3\u88c1\u526a\u589e\u5f3a\u7b56\u7565\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u805a\u7126\u4e8e\u7ec6\u7c92\u5ea6\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u3001\u7a00\u758f\u6216\u4e0d\u89c4\u5219\u7269\u4f53\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u901a\u5e38\u9488\u5bf9\u5927\u800c\u5747\u5300\u7684\u533a\u57df\u8fdb\u884c\u4f18\u5316\uff0c\u4f46\u5728\u5904\u7406\u5c0f\u3001\u7a00\u758f\u6216\u5c40\u90e8\u4e0d\u89c4\u5219\u7269\u4f53\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u76ee\u6807\u7269\u4f53\u5c3a\u5ea6\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u5c3a\u5ea6\u611f\u77e5\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u9002\u5e94\u65b9\u6cd5\uff0c\u5c06\u5c0f\u7a97\u53e3\u88c1\u526a\u96c6\u6210\u5230\u6570\u636e\u589e\u5f3a\u6d41\u7a0b\u4e2d\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u653e\u5927\u7ec6\u7c92\u5ea6\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u4e0d\u540c\u6570\u636e\u6a21\u6001\u7684\u9886\u57df\u8fdb\u884c\u9a8c\u8bc1\uff1a\u5730\u9707\u6210\u50cf\uff08\u5206\u5272\u7a00\u758f\u65ad\u5c42\uff09\u548c\u795e\u7ecf\u6210\u50cf\uff08\u63cf\u7ed8\u5c0f\u7ec6\u80de\u7ed3\u6784\uff09\u3002", "result": "\u5728\u6807\u7b7e\u53d7\u9650\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u548c\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb\uff1a\u65ad\u5c42\u5206\u5272\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe13%\uff0c\u7ec6\u80de\u63cf\u7ed8\u51c6\u786e\u7387\u63d0\u53475%\u3002\u4f46\u5bf9\u4e8e\u5927\u89c4\u6a21\u7279\u5f81\uff08\u5982\u5730\u9707\u76f8\u6216\u7ec4\u7ec7\u533a\u57df\uff09\u6539\u5584\u6709\u9650\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u4ef7\u503c\u5173\u952e\u53d6\u51b3\u4e8e\u76ee\u6807\u7269\u4f53\u7684\u5c3a\u5ea6\uff0c\u9700\u8981\u5c06SSL\u8bbe\u8ba1\u4e0e\u7269\u4f53\u5927\u5c0f\u548c\u7a00\u758f\u6027\u5bf9\u9f50\u3002\u8fd9\u4e3a\u8de8\u79d1\u5b66\u6210\u50cf\u9886\u57df\u6784\u5efa\u66f4\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u6d41\u7a0b\u63d0\u4f9b\u4e86\u901a\u7528\u539f\u5219\u3002"}}
{"id": "2601.18681", "categories": ["cs.LG", "cs.AI", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.18681", "abs": "https://arxiv.org/abs/2601.18681", "authors": ["Yilie Huang", "Wenpin Tang", "Xunyu Zhou"], "title": "ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule", "comment": "17 pages, 7 figures", "summary": "We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fr\u00e9chet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u91cd\u53c2\u6570\u5316\u65f6\u95f4\uff08ART\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u91cd\u53c2\u6570\u5316\u65f6\u95f4\u53d8\u91cf\u7684\u65f6\u949f\u901f\u5ea6\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u6b65\u8c03\u5ea6\uff0c\u51cf\u5c11\u79bb\u6563\u5316\u8bef\u5dee\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08ART-RL\uff09\u81ea\u52a8\u5b66\u4e60\u6700\u4f18\u65f6\u95f4\u8868\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u65f6\u95f4\u7f51\u683c\u5728\u6709\u9650\u65f6\u95f4\u6b65\u9884\u7b97\u4e0b\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u5bfc\u81f4\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\u6765\u4f18\u5316\u65f6\u95f4\u6b65\u8c03\u5ea6\uff0c\u6700\u5c0f\u5316\u79bb\u6563\u5316\u8bef\u5dee\u3002", "method": "\u63d0\u51faART\u65b9\u6cd5\u63a7\u5236\u91cd\u53c2\u6570\u5316\u65f6\u95f4\u53d8\u91cf\u7684\u65f6\u949f\u901f\u5ea6\uff0c\u5b9e\u73b0\u4e0d\u5747\u5300\u65f6\u95f4\u6b65\u8c03\u5ea6\u4f46\u4fdd\u6301\u7ec8\u7aef\u65f6\u95f4\u4e0d\u53d8\u3002\u8fdb\u4e00\u6b65\u63d0\u51faART-RL\uff0c\u5c06\u65f6\u95f4\u53d8\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u9ad8\u65af\u7b56\u7565\uff0c\u901a\u8fc7actor-critic\u66f4\u65b0\u6570\u636e\u9a71\u52a8\u5730\u5b66\u4e60\u6700\u4f18\u65f6\u95f4\u8868\u3002", "result": "\u57fa\u4e8eEDM\u6846\u67b6\uff0cART-RL\u5728CIFAR-10\u4e0a\u663e\u8457\u6539\u5584\u4e86Fr\u00e9chet Inception Distance\uff08FID\uff09\uff0c\u5e76\u5728AFHQv2\u3001FFHQ\u548cImageNet\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u8fc1\u79fb\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "ART-RL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u6b65\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u91c7\u6837\u8d28\u91cf\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.18623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18623", "abs": "https://arxiv.org/abs/2601.18623", "authors": ["Zihao Wang", "Yuzhou Chen", "Shaogang Ren"], "title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation", "comment": "Paper accepted as a conference paper at ICLR 2026", "summary": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8de8\u6a21\u6001\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u53d8\u5316\u7684\u6df7\u5408\u573a\u548c\u76ee\u6807\u4e00\u81f4\u6062\u590d\u9879\u6765\u907f\u514d\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4e2d\u7684\u56fa\u5b9a\u8c03\u5ea6\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53bb\u566a\u6b65\u9aa4\u3002", "motivation": "\u4f20\u7edf\u8de8\u6a21\u6001\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\u5b58\u5728\u8106\u5f31\u6027\u548c\u4f4e\u6548\u6027\u95ee\u9898\u3002\u6807\u51c6\u6269\u6563\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u7684\u5168\u5c40\u7ebf\u6027\u57df\u8f6c\u79fb\uff0c\u8fd9\u8feb\u4f7f\u91c7\u6837\u5668\u904d\u5386\u79bb\u6d41\u5f62\u7684\u9ad8\u6210\u672c\u533a\u57df\uff0c\u589e\u52a0\u4e86\u6821\u6b63\u8d1f\u62c5\u5e76\u5bfc\u81f4\u8bed\u4e49\u6f02\u79fb\u3002\u4f5c\u8005\u5c06\u8fd9\u79cd\u5171\u4eab\u5931\u8d25\u6a21\u5f0f\u79f0\u4e3a\u56fa\u5b9a\u8c03\u5ea6\u57df\u8f6c\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u57df\u8f6c\u79fb\u52a8\u6001\u76f4\u63a5\u5d4c\u5165\u751f\u6210\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002\u6a21\u578b\u5728\u6bcf\u4e2a\u53cd\u5411\u6b65\u9aa4\u9884\u6d4b\u7a7a\u95f4\u53d8\u5316\u7684\u6df7\u5408\u573a\uff0c\u5e76\u5728\u6f02\u79fb\u4e2d\u6ce8\u5165\u663e\u5f0f\u7684\u76ee\u6807\u4e00\u81f4\u6062\u590d\u9879\u3002\u8fd9\u79cd\u6b65\u5185\u6307\u5bfc\u4f7f\u5927\u66f4\u65b0\u4fdd\u6301\u5728\u6d41\u5f62\u4e0a\uff0c\u5e76\u5c06\u6a21\u578b\u89d2\u8272\u4ece\u5168\u5c40\u5bf9\u9f50\u8f6c\u53d8\u4e3a\u5c40\u90e8\u6b8b\u5dee\u6821\u6b63\u3002\u63d0\u4f9b\u4e86\u8fde\u7eed\u65f6\u95f4\u516c\u5f0f\u548c\u7cbe\u786e\u89e3\u5f62\u5f0f\uff0c\u5e76\u63a8\u5bfc\u4e86\u4fdd\u6301\u8fb9\u7f18\u4e00\u81f4\u6027\u7684\u5b9e\u7528\u4e00\u9636\u91c7\u6837\u5668\u3002", "result": "\u5728\u533b\u5b66\u6210\u50cf\u3001\u9065\u611f\u548c\u7535\u81f4\u53d1\u5149\u8bed\u4e49\u6620\u5c04\u7b49\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5728\u66f4\u5c11\u7684\u53bb\u566a\u6b65\u9aa4\u4e2d\u6536\u655b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u57df\u8f6c\u79fb\u52a8\u6001\u76f4\u63a5\u6574\u5408\u5230\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6269\u6563\u6a21\u578b\u7684\u56fa\u5b9a\u8c03\u5ea6\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u8de8\u6a21\u6001\u56fe\u50cf\u7ffb\u8bd1\u3002"}}
{"id": "2601.18696", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18696", "abs": "https://arxiv.org/abs/2601.18696", "authors": ["Paul Whitten", "Francis Wolff", "Chris Papachristou"], "title": "Explainability Methods for Hardware Trojan Detection: A Systematic Comparison", "comment": null, "summary": "Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).\n  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like \"high fanin complexity near outputs indicates potential triggers.\" Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.\n  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.\n  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\u4e2d\u4e09\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5c5e\u6027\u7684\u7535\u8def\u5206\u6790\u3001\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u548c\u6a21\u578b\u65e0\u5173\u7279\u5f81\u5f52\u56e0\uff0c\u53d1\u73b0\u524d\u4e24\u79cd\u65b9\u6cd5\u5728\u9886\u57df\u5bf9\u9f50\u548c\u57fa\u4e8e\u5148\u4f8b\u7684\u89e3\u91ca\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u7279\u5f81\u6392\u540d\u65b9\u6cd5\u3002", "motivation": "\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\u9700\u8981\u51c6\u786e\u8bc6\u522b\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u4ee5\u4fbf\u5b89\u5168\u5de5\u7a0b\u5e08\u80fd\u591f\u9a8c\u8bc1\u5e76\u57fa\u4e8e\u7ed3\u679c\u91c7\u53d6\u884c\u52a8\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u68c0\u6d4b\u7ed3\u679c\u7684\u9886\u57df\u76f8\u5173\u89e3\u91ca\uff0c\u96be\u4ee5\u8ba9\u5de5\u7a0b\u5e08\u4fe1\u4efb\u548c\u9a8c\u8bc1\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u5c5e\u6027\u7684\u5206\u6790\uff0c\u4f7f\u752831\u4e2a\u7535\u8def\u7279\u5b9a\u7279\u5f81\uff08\u95e8\u6247\u5165\u6a21\u5f0f\u3001\u89e6\u53d1\u5668\u8ddd\u79bb\u3001I/O\u8fde\u63a5\u6027\uff09\uff1b2\uff09\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\uff0c\u4f7f\u7528k\u8fd1\u90bb\u7b97\u6cd5\u8fdb\u884c\u57fa\u4e8e\u5148\u4f8b\u7684\u89e3\u91ca\uff1b3\uff09\u6a21\u578b\u65e0\u5173\u7279\u5f81\u5f52\u56e0\uff08LIME\u3001SHAP\u3001\u68af\u5ea6\u65b9\u6cd5\uff09\u3002\u4f7f\u7528XGBoost\u5206\u7c7b\u5668\u5728Trust-Hub\u57fa\u51c6\u4e0a\u8fdb\u884c\u95e8\u7ea7\u6728\u9a6c\u68c0\u6d4b\u3002", "result": "XGBoost\u5206\u7c7b\u572811,392\u4e2a\u6d4b\u8bd5\u6837\u672c\u4e0a\u8fbe\u523046.15%\u7cbe\u5ea6\u548c52.17%\u53ec\u56de\u7387\uff0c\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\uff08Hasegawa\u7b49\uff1a5.13%\uff09\u7cbe\u5ea6\u63d0\u9ad89\u500d\uff0c\u8bef\u62a5\u7387\u4ece5.6%\u964d\u81f30.25%\u3002\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u5728\u9884\u6d4b\u548c\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u8fbe\u523097.4%\u5bf9\u5e94\u6027\u3002LIME\u548cSHAP\u7279\u5f81\u5f52\u56e0\u5177\u6709\u5f3a\u76f8\u5173\u6027\uff08r=0.94\uff0cp<0.001\uff09\u3002\u68af\u5ea6\u65b9\u6cd5\u6bd4SHAP\u5feb481\u500d\u3002", "conclusion": "\u57fa\u4e8e\u5c5e\u6027\u7684\u5206\u6790\u548c\u57fa\u4e8e\u6848\u4f8b\u7684\u63a8\u7406\u65b9\u6cd5\u5728\u9886\u57df\u5bf9\u9f50\u548c\u57fa\u4e8e\u5148\u4f8b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u901a\u7528\u7279\u5f81\u6392\u540d\u65b9\u6cd5\uff0c\u5bf9\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9a8c\u8bc1\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u3002"}}
{"id": "2601.18625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18625", "abs": "https://arxiv.org/abs/2601.18625", "authors": ["Zequn Xie"], "title": "CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search", "comment": "Accepted by ICASSP 2026", "summary": "Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.", "AI": {"tldr": "CONQUER\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u884c\u4eba\u641c\u7d22\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u65f6\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u589e\u5f3a\u548c\u63a8\u7406\u65f6\u7684\u81ea\u9002\u5e94\u67e5\u8be2\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u6587\u672c\u884c\u4eba\u641c\u7d22\u9762\u4e34\u8de8\u6a21\u6001\u5dee\u5f02\u548c\u6a21\u7cca\u67e5\u8be2\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u6216\u6a21\u7cca\u67e5\u8be2\u65f6\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u591a\u7c92\u5ea6\u7f16\u7801\u3001\u4e92\u8865\u5bf9\u6316\u6398\u548c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u4e0a\u4e0b\u6587\u5f15\u5bfc\u5339\u914d\uff1b\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u67e5\u8be2\u589e\u5f3a\u6a21\u5757\u8fdb\u884c\u951a\u70b9\u9009\u62e9\u548c\u5c5e\u6027\u9a71\u52a8\u4e30\u5bcc\u3002", "result": "\u5728CUHK-PEDES\u3001ICFG-PEDES\u548cRSTPReid\u6570\u636e\u96c6\u4e0a\uff0cCONQUER\u5728Rank-1\u51c6\u786e\u7387\u548cmAP\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u8de8\u57df\u548c\u4e0d\u5b8c\u6574\u67e5\u8be2\u573a\u666f\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "CONQUER\u4e3a\u5b9e\u9645\u90e8\u7f72\u7684\u6587\u672c\u884c\u4eba\u641c\u7d22\u63d0\u4f9b\u4e86\u5b9e\u7528\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u534f\u540c\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2601.18699", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18699", "abs": "https://arxiv.org/abs/2601.18699", "authors": ["Olaf Yunus Laitinen Imanov"], "title": "Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning", "comment": "16 pages, 16 figures (6 main + 10 supplementary)", "summary": "Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u987a\u5e8f\u5fae\u8c03\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\u8fdb\u884c\u4e86\u673a\u5236\u6027\u5206\u6790\uff0c\u53d1\u73b0\u68af\u5ea6\u5e72\u6270\u3001\u8868\u5f81\u6f02\u79fb\u548c\u635f\u5931\u666f\u89c2\u5e73\u5766\u5316\u662f\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u9057\u5fd8\u4e25\u91cd\u7a0b\u5ea6\u4e0e\u4efb\u52a1\u76f8\u4f3c\u5ea6\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u987a\u5e8f\u5fae\u8c03\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\uff0c\u65b0\u83b7\u5f97\u7684\u77e5\u8bc6\u4f1a\u5e72\u6270\u5148\u524d\u5b66\u4e60\u7684\u80fd\u529b\u3002\u867d\u7136\u8fd9\u79cd\u73b0\u8c61\u88ab\u5e7f\u6cdb\u89c2\u5bdf\u5230\uff0c\u4f46\u5176\u673a\u5236\u7406\u89e3\u4ecd\u7136\u6709\u9650\u3002", "method": "\u5bf9\u57fa\u4e8eTransformer\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u987a\u5e8f\u5fae\u8c03\u4e2d\u8fdb\u884c\u5168\u9762\u7684\u673a\u5236\u5206\u6790\uff0c\u901a\u8fc7\u8de8\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\uff08109B\u5230400B\u603b\u53c2\u6570\uff09\u548c\u4efb\u52a1\u5e8f\u5217\u7684\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u8bc6\u522b\u707e\u96be\u6027\u9057\u5fd8\u7684\u4e3b\u8981\u673a\u5236\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u79cd\u4e3b\u8981\u9057\u5fd8\u673a\u5236\uff1a\u6ce8\u610f\u529b\u6743\u91cd\u4e2d\u7684\u68af\u5ea6\u5e72\u6270\u3001\u4e2d\u95f4\u5c42\u7684\u8868\u5f81\u6f02\u79fb\u4ee5\u53ca\u635f\u5931\u666f\u89c2\u5e73\u5766\u5316\u3002\u9057\u5fd8\u4e25\u91cd\u7a0b\u5ea6\u4e0e\u4efb\u52a1\u76f8\u4f3c\u5ea6\u9ad8\u5ea6\u76f8\u5173\uff08Pearson r = 0.87\uff09\uff0c\u7ea615-23%\u7684\u6ce8\u610f\u529b\u5934\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u53d7\u5230\u4e25\u91cd\u7834\u574f\uff0c\u8f83\u4f4e\u5c42\u8868\u73b0\u51fa\u66f4\u5927\u7684\u6613\u611f\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5f00\u53d1\u6709\u9488\u5bf9\u6027\u7684\u7f13\u89e3\u7b56\u7565\u5efa\u7acb\u4e86\u673a\u5236\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5bf9\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\u7684\u6df1\u5165\u7406\u89e3\u3002"}}
{"id": "2601.18633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18633", "abs": "https://arxiv.org/abs/2601.18633", "authors": ["Tong Shi", "Melonie de Almeida", "Daniela Ivanova", "Nicolas Pugeault", "Paul Henderson"], "title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting", "comment": null, "summary": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.", "AI": {"tldr": "Splat-Portrait\uff1a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u76843D\u8bf4\u8bdd\u5934\u751f\u6210\u65b9\u6cd5\uff0c\u65e0\u97003D\u76d1\u7763\u6216\u8fd0\u52a8\u5148\u9a8c\uff0c\u4ece\u5355\u5f20\u8096\u50cf\u548c\u8bed\u97f3\u5408\u6210\u81ea\u7136\u7684\u8bf4\u8bdd\u89c6\u9891", "motivation": "\u73b0\u67093D\u8bf4\u8bdd\u5934\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u7279\u5b9a\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u5f62\u53d8\u7684\u8fd0\u52a8\u8868\u793a\u5148\u9a8c\uff09\uff0c\u5bfc\u81f43D\u5934\u50cf\u91cd\u5efa\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u52a8\u753b\u771f\u5b9e\u611f", "method": "\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u9759\u60013D\u91cd\u5efa\uff0c\u81ea\u52a8\u4ece\u5355\u5f20\u8096\u50cf\u4e2d\u89e3\u8026\u51fa\u9759\u60013D\u91cd\u5efa\u548c2D\u80cc\u666f\uff0c\u57fa\u4e8e\u8f93\u5165\u97f3\u9891\u751f\u6210\u81ea\u7136\u5507\u90e8\u8fd0\u52a8\uff0c\u65e0\u9700\u8fd0\u52a8\u9a71\u52a8\u5148\u9a8c\uff0c\u4ec5\u4f7f\u75282D\u91cd\u5efa\u548c\u5206\u6570\u84b8\u998f\u635f\u5931\u8bad\u7ec3", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSplat-Portrait\u5728\u8bf4\u8bdd\u5934\u751f\u6210\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u89c6\u89c9\u8d28\u91cf\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c", "conclusion": "Splat-Portrait\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D\u5934\u90e8\u91cd\u5efa\u548c\u5507\u90e8\u8fd0\u52a8\u5408\u6210\u7684\u6311\u6218\uff0c\u65e0\u97003D\u76d1\u7763\u6216\u5173\u952e\u70b9\u6807\u6ce8\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8bf4\u8bdd\u5934\u751f\u6210"}}
{"id": "2601.18702", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18702", "abs": "https://arxiv.org/abs/2601.18702", "authors": ["Hansheng Ren"], "title": "From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic", "comment": "8 pages, 6 figures. Submitted to UAI 2026", "summary": "Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the \"hallucinations\" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6311\u6218\u4e86\u6df1\u5ea6\u5b66\u4e60\u4f18\u5148\u8ba1\u7b97\u541e\u5410\u91cf\u800c\u975e\u6570\u503c\u7cbe\u5ea6\u7684\u73b0\u6709\u8303\u5f0f\uff0c\u63d0\u51fa\u7cbe\u786e\u6027\u5047\u8bbe\uff1a\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u9700\u8981\u652f\u6301\u4efb\u610f\u7cbe\u5ea6\u7b97\u672f\u7684\u8ba1\u7b97\u57fa\u7840\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u662f\u6d6e\u70b9\u8fd1\u4f3c\u8bef\u5dee\u7d2f\u79ef\u7684\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0f\u8fc7\u4e8e\u5f3a\u8c03\u8ba1\u7b97\u541e\u5410\u91cf\u800c\u5ffd\u89c6\u6570\u503c\u7cbe\u5ea6\uff0c\u8ba4\u4e3a\u667a\u80fd\u4ec5\u6e90\u4e8e\u5927\u89c4\u6a21\u7edf\u8ba1\u76f8\u5173\u6027\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7684\"\u5e7b\u89c9\"\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u8ba4\u4e3a\u8fd9\u4e9b\u662fIEEE 754\u6d6e\u70b9\u8fd1\u4f3c\u8bef\u5dee\u5728\u6df1\u5ea6\u7ec4\u5408\u51fd\u6570\u4e2d\u7d2f\u79ef\u7684\u4ea7\u7269\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u6570\u503c\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7cbe\u786e\u6027\u5047\u8bbe\uff0c\u5f15\u5165Halo\u67b6\u6784\u4f5c\u4e3a\u8303\u5f0f\u8f6c\u53d8\uff0c\u91c7\u7528\u6709\u7406\u6570\u7b97\u672f\uff08\u211a\uff09\u4f5c\u4e3a\u8ba1\u7b97\u57fa\u7840\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u7cbe\u786e\u63a8\u7406\u5355\u5143\uff08EIU\uff09\u3002\u901a\u8fc7Huginn-0125\u539f\u578b\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5728Huginn-0125\u539f\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53600B\u53c2\u6570\u7684BF16\u57fa\u7ebf\u5728\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u5d29\u6e83\u65f6\uff0cHalo\u67b6\u6784\u80fd\u591f\u65e0\u9650\u671f\u4fdd\u6301\u96f6\u6570\u503c\u53d1\u6563\u3002\u8fd9\u8bc1\u660e\u4e86\u7cbe\u786e\u7b97\u672f\u5728\u51cf\u5c11\u7cfb\u7edf2 AGI\u903b\u8f91\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7cbe\u786e\u7b97\u672f\u662f\u51cf\u5c11\u7cfb\u7edf2 AGI\u903b\u8f91\u4e0d\u786e\u5b9a\u6027\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u4e3a\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u57fa\u7840\u65b9\u5411\uff0c\u6311\u6218\u4e86\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u5bf9\u6570\u503c\u7cbe\u5ea6\u7684\u5ffd\u89c6\u3002"}}
{"id": "2601.18698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18698", "abs": "https://arxiv.org/abs/2601.18698", "authors": ["Xiao Liu", "Jiawei Zhang"], "title": "Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge", "comment": "Work in progress", "summary": "Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGAP\u6846\u67b6\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u5730\u7406\u516c\u5e73\u6027\uff0c\u53d1\u73b0Sora 2\u6a21\u578b\u5728\u5168\u7403\u89c6\u89c9\u77e5\u8bc6\u8868\u8fbe\u4e0a\u76f8\u5bf9\u5747\u5300\uff0c\u5730\u7406\u504f\u89c1\u6bd4\u9884\u671f\u5f31\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u4ea7\u751f\u89c6\u89c9\u4e0a\u5438\u5f15\u4eba\u7684\u7ed3\u679c\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u7f16\u7801\u4e86\u5730\u7406\u4e0a\u516c\u5e73\u7684\u89c6\u89c9\u77e5\u8bc6\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u5730\u7406\u516c\u5e73\u6027\u548c\u5730\u7406\u57fa\u7840\u89c6\u89c9\u77e5\u8bc6\u3002", "method": "\u63d0\u51faGeo-Attraction Landmark Probing (GAP)\u7cfb\u7edf\u6846\u67b6\uff0c\u6784\u5efaGEOATTRACTION-500\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u542b500\u4e2a\u5168\u7403\u5206\u5e03\u7684\u65c5\u6e38\u666f\u70b9\uff09\uff0c\u6574\u5408\u5168\u5c40\u7ed3\u6784\u5bf9\u9f50\u3001\u7ec6\u7c92\u5ea6\u5173\u952e\u70b9\u5bf9\u9f50\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u7b49\u4e92\u8865\u6307\u6807\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5bf9\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578bSora 2\u5e94\u7528GAP\u6846\u67b6\u53d1\u73b0\uff0c\u4e0e\u5e38\u89c1\u7684\u5730\u7406\u504f\u89c1\u5047\u8bbe\u76f8\u53cd\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u5730\u533a\u3001\u53d1\u5c55\u6c34\u5e73\u548c\u6587\u5316\u7fa4\u4f53\u4e4b\u95f4\u8868\u73b0\u51fa\u76f8\u5bf9\u5747\u5300\u7684\u5730\u7406\u57fa\u7840\u89c6\u89c9\u77e5\u8bc6\u6c34\u5e73\uff0c\u5bf9\u666f\u70b9\u6d41\u884c\u5ea6\u7684\u4f9d\u8d56\u8f83\u5f31\u3002", "conclusion": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u8868\u8fbe\u5168\u7403\u89c6\u89c9\u77e5\u8bc6\u6bd4\u9884\u671f\u66f4\u5747\u5300\uff0c\u8fd9\u7a81\u663e\u4e86\u5b83\u4eec\u5728\u5168\u7403\u5316\u90e8\u7f72\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u4e5f\u5f3a\u8c03\u4e86\u968f\u7740\u7cfb\u7edf\u53d1\u5c55\u9700\u8981\u6301\u7eed\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2601.18714", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.18714", "abs": "https://arxiv.org/abs/2601.18714", "authors": ["Judith Vilella-Cantos", "Mauro Martini", "Marcello Chiaberge", "M\u00f3nica Ballesta", "David Valiente"], "title": "Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning", "comment": null, "summary": "Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.", "AI": {"tldr": "\u63d0\u51faMinkUNeXt-VINE\u65b9\u6cd5\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u5730\u70b9\u8bc6\u522b\uff0c\u5728\u8461\u8404\u56ed\u73af\u5883\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u7a00\u758fLiDAR\u8f93\u5165\u548c\u5b9e\u65f6\u573a\u666f\u3002", "motivation": "\u519c\u4e1a\u73af\u5883\u7531\u4e8e\u975e\u7ed3\u6784\u5316\u7279\u6027\u548c\u7f3a\u4e4f\u663e\u8457\u5730\u6807\uff0c\u5b9a\u4f4d\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7269\u4f53\u5206\u7c7b\u548c\u5206\u5272\uff0c\u4f46\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\u5728\u5f53\u524d\u6280\u672f\u6c34\u5e73\u4e0b\u4ecd\u4e0d\u7b80\u5355\u3002", "method": "\u63d0\u51faMinkUNeXt-VINE\u65b9\u6cd5\uff0c\u91c7\u7528\u9884\u5904\u7406\u548cMatryoshka\u8868\u793a\u5b66\u4e60\u591a\u635f\u5931\u65b9\u6cd5\uff0c\u4f18\u5148\u8003\u8651\u4f4e\u6210\u672c\u7a00\u758fLiDAR\u8f93\u5165\u548c\u4f4e\u7ef4\u8f93\u51fa\uff0c\u786e\u4fdd\u5b9e\u65f6\u573a\u666f\u7684\u9ad8\u6548\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u957f\u671f\u8461\u8404\u56ed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u6d88\u878d\u7814\u7a76\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6743\u8861\u8f93\u51fa\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u4f4e\u6210\u672c\u548c\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u6570\u636e\u4e0a\u5177\u6709\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "MinkUNeXt-VINE\u65b9\u6cd5\u5728\u8461\u8404\u56ed\u73af\u5883\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u519c\u4e1a\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5730\u70b9\u8bc6\u522b\u4efb\u52a1\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u7528\u4e8e\u590d\u73b0\u3002"}}
{"id": "2601.18728", "categories": ["cs.LG", "math.DG", "math.OC", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.18728", "abs": "https://arxiv.org/abs/2601.18728", "authors": ["Willem Diepeveen", "Oscar Leong"], "title": "Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data", "comment": null, "summary": "Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.", "AI": {"tldr": "Riemannian AmbientFlow\uff1a\u4ece\u635f\u574f\u89c2\u6d4b\u4e2d\u540c\u65f6\u5b66\u4e60\u6982\u7387\u751f\u6210\u6a21\u578b\u548c\u5e95\u5c42\u975e\u7ebf\u6027\u6570\u636e\u6d41\u5f62\u7684\u6846\u67b6", "motivation": "\u5728\u8bb8\u591a\u79d1\u5b66\u548c\u6210\u50cf\u5e94\u7528\u4e2d\uff0c\u65e0\u6cd5\u83b7\u5f97\u5e72\u51c0\u6837\u672c\uff0c\u53ea\u80fd\u89c2\u5bdf\u5230\u566a\u58f0\u6216\u7ebf\u6027\u635f\u574f\u7684\u6d4b\u91cf\u503c\u3002\u6b64\u5916\uff0c\u6570\u636e\u4e2d\u5b58\u5728\u7684\u6f5c\u5728\u7ed3\u6784\uff08\u5982\u6d41\u5f62\u51e0\u4f55\uff09\u5bf9\u4e8e\u4e0b\u6e38\u79d1\u5b66\u5206\u6790\u5f88\u91cd\u8981\uff0c\u9700\u8981\u63d0\u53d6\u3002", "method": "\u57fa\u4e8eAmbientFlow\u7684\u53d8\u5206\u63a8\u65ad\u6846\u67b6\uff0c\u7ed3\u5408\u7531\u5f52\u4e00\u5316\u6d41\u8bf1\u5bfc\u7684\u6570\u636e\u9a71\u52a8\u9ece\u66fc\u51e0\u4f55\uff0c\u901a\u8fc7\u62c9\u56de\u5ea6\u91cf\u548c\u9ece\u66fc\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u6d41\u5f62\u7ed3\u6784\u3002\u5728\u9002\u5f53\u7684\u51e0\u4f55\u6b63\u5219\u5316\u548c\u6d4b\u91cf\u6761\u4ef6\u4e0b\uff0c\u5b66\u4e60\u6a21\u578b\u80fd\u6062\u590d\u5e95\u5c42\u6570\u636e\u5206\u5e03\u5e76\u4ea7\u751f\u5e73\u6ed1\u7684\u53ccLipschitz\u6d41\u5f62\u53c2\u6570\u5316\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u8868\u660e\uff0c\u5b66\u4e60\u6a21\u578b\u80fd\u6062\u590d\u5e95\u5c42\u6570\u636e\u5206\u5e03\u81f3\u53ef\u63a7\u8bef\u5dee\uff0c\u5e76\u83b7\u5f97\u5e73\u6ed1\u7684\u53ccLipschitz\u6d41\u5f62\u53c2\u6570\u5316\u3002\u5e73\u6ed1\u89e3\u7801\u5668\u53ef\u4f5c\u4e3a\u9006\u95ee\u9898\u7684\u539f\u5219\u6027\u751f\u6210\u5148\u9a8c\uff0c\u5177\u6709\u6062\u590d\u4fdd\u8bc1\u3002\u5728\u4f4e\u7ef4\u5408\u6210\u6d41\u5f62\u548cMNIST\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "conclusion": "Riemannian AmbientFlow\u4e3a\u4ece\u635f\u574f\u89c2\u6d4b\u4e2d\u540c\u65f6\u5b66\u4e60\u751f\u6210\u6a21\u578b\u548c\u5e95\u5c42\u6570\u636e\u6d41\u5f62\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u9006\u95ee\u9898\u4e2d\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\u3002"}}
{"id": "2601.18739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18739", "abs": "https://arxiv.org/abs/2601.18739", "authors": ["Ignacio Antequera-S\u00e1nchez", "Juan Luis Su\u00e1rez-D\u00edaz", "Rosana Montes", "Francisco Herrera"], "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification", "comment": "28 pages", "summary": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.", "AI": {"tldr": "\u63d0\u51faSeNeDiF-OOD\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5d4c\u5957\u4e8c\u5206\u878d\u5408\u7684\u5c42\u6b21\u7ed3\u6784\u89e3\u51b3OOD\u68c0\u6d4b\u4e2d\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5728MonuMAI\u5efa\u7b51\u98ce\u683c\u8bc6\u522b\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "OOD\u68c0\u6d4b\u5bf9\u4e8eAI\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46OOD\u6570\u636e\u7684\u5f02\u8d28\u6027\uff08\u4ece\u4f4e\u7ea7\u635f\u574f\u5230\u8bed\u4e49\u504f\u79fb\uff09\u4f7f\u5f97\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u96be\u4ee5\u6709\u6548\u5904\u7406\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faSeNeDiF-OOD\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bed\u4e49\u5d4c\u5957\u4e8c\u5206\u878d\u5408\u6846\u67b6\uff0c\u5c06\u68c0\u6d4b\u4efb\u52a1\u5206\u89e3\u4e3a\u5c42\u6b21\u5316\u7684\u4e8c\u5143\u878d\u5408\u8282\u70b9\u7ed3\u6784\uff0c\u6bcf\u5c42\u96c6\u6210\u4e0e\u7279\u5b9a\u8bed\u4e49\u62bd\u8c61\u7ea7\u522b\u5bf9\u9f50\u7684\u51b3\u7b56\u8fb9\u754c", "result": "\u5728MonuMAI\u5efa\u7b51\u98ce\u683c\u8bc6\u522b\u7cfb\u7edf\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\uff0c\u80fd\u6709\u6548\u8fc7\u6ee4\u975e\u7eaa\u5ff5\u7891\u56fe\u50cf\u3001\u672a\u77e5\u5efa\u7b51\u98ce\u683c\u548c\u5bf9\u6297\u653b\u51fb\u7b49\u591a\u6837OOD\u7c7b\u522b\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u5e03\u5185\u6027\u80fd", "conclusion": "\u8bed\u4e49\u5d4c\u5957\u4e8c\u5206\u878d\u5408\u7684\u5c42\u6b21\u6846\u67b6\u4e3a\u89e3\u51b3OOD\u68c0\u6d4b\u4e2d\u7684\u5f02\u8d28\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5728\u771f\u5b9e\u5f00\u653e\u73af\u5883\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2601.18734", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18734", "abs": "https://arxiv.org/abs/2601.18734", "authors": ["Siyan Zhao", "Zhihui Xie", "Mengchen Liu", "Jing Huang", "Guan Pang", "Feiyu Chen", "Aditya Grover"], "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models", "comment": "13 pages", "summary": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.", "AI": {"tldr": "\u63d0\u51faOn-Policy Self-Distillation (OPSD)\u6846\u67b6\uff0c\u8ba9\u5355\u4e2aLLM\u540c\u65f6\u626e\u6f14\u6559\u5e08\u548c\u5b66\u751f\u89d2\u8272\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u751f\u6210\u54cd\u5e94\uff0c\u5b9e\u73b0\u81ea\u6211\u84b8\u998f\uff0c\u63d0\u9ad8\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u5728\u7ebf\u84b8\u998f\u9700\u8981\u72ec\u7acb\u7684\u3001\u901a\u5e38\u66f4\u5927\u7684\u6559\u5e08\u6a21\u578b\uff1b2) \u6ca1\u6709\u5145\u5206\u5229\u7528\u63a8\u7406\u6570\u636e\u96c6\u4e2d\u53ef\u7528\u7684\u771f\u5b9e\u89e3\u51b3\u65b9\u6848\u3002\u53d7\u5230\"\u8db3\u591f\u5f3a\u5927\u7684LLM\u80fd\u591f\u7406\u6027\u5206\u6790\u5916\u90e8\u7279\u6743\u63a8\u7406\u8f68\u8ff9\u5e76\u6559\u5bfc\u5176\u8f83\u5f31\u81ea\u6211\u7248\u672c\"\u8fd9\u4e00\u76f4\u89c9\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u81ea\u6211\u84b8\u998f\u6846\u67b6\u3002", "method": "OPSD\u6846\u67b6\u4e2d\uff0c\u5355\u4e2a\u6a21\u578b\u540c\u65f6\u4f5c\u4e3a\u6559\u5e08\u548c\u5b66\u751f\uff0c\u4f46\u57fa\u4e8e\u4e0d\u540c\u4e0a\u4e0b\u6587\u6761\u4ef6\uff1a\u6559\u5e08\u7b56\u7565\u57fa\u4e8e\u7279\u6743\u4fe1\u606f\uff08\u5982\u5df2\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9\uff09\uff0c\u5b66\u751f\u7b56\u7565\u4ec5\u770b\u5230\u95ee\u9898\u3002\u8bad\u7ec3\u65f6\u6700\u5c0f\u5316\u8fd9\u4e24\u4e2a\u5206\u5e03\u5728\u5b66\u751f\u81ea\u8eab\u8f68\u8ff9\u4e0a\u7684\u6bcf\u4ee4\u724c\u5206\u6b67\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4GRPO\u7b49\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e864-8\u500d\u7684\u4ee4\u724c\u6548\u7387\u63d0\u5347\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8e\u79bb\u7ebf\u84b8\u998f\u65b9\u6cd5\u3002", "conclusion": "OPSD\u6846\u67b6\u901a\u8fc7\u8ba9\u5355\u4e2a\u6a21\u578b\u81ea\u6211\u84b8\u998f\uff0c\u65e2\u907f\u514d\u4e86\u9700\u8981\u72ec\u7acb\u6559\u5e08\u6a21\u578b\u7684\u5f00\u9500\uff0c\u53c8\u5145\u5206\u5229\u7528\u4e86\u6570\u636e\u96c6\u4e2d\u7684\u7279\u6743\u4fe1\u606f\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u9ad8\u6548\u4e14\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2601.18751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.18751", "abs": "https://arxiv.org/abs/2601.18751", "authors": ["Seyed Amir Hosseini", "Maryam Abdolali", "Amirhosein Tavakkoli", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback", "comment": "Equal contribution: Seyed Amir Hosseini and Maryam Abdolali. Corresponding author: Maryam Abdolali (maryam.abdolali@kntu.ac.ir)", "summary": "Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.", "AI": {"tldr": "TriTrust-PBRL (TTP) \u662f\u4e00\u4e2a\u65b0\u7684\u504f\u597d\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5305\u542b\u53ef\u9760\u3001\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u7684\u5f02\u6784\u4e13\u5bb6\u53cd\u9988\u4e2d\u5b66\u4e60\uff0c\u901a\u8fc7\u4fe1\u4efb\u53c2\u6570\u81ea\u52a8\u5904\u7406\u5bf9\u6297\u6027\u504f\u597d\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u504f\u597d\u6570\u636e\u901a\u5e38\u6765\u81ea\u5177\u6709\u4e0d\u540c\u53ef\u9760\u6027\u7684\u5f02\u6784\u6807\u6ce8\u8005\uff0c\u5305\u62ec\u51c6\u786e\u3001\u566a\u58f0\u548c\u7cfb\u7edf\u6027\u5bf9\u6297\u7684\u6807\u6ce8\u8005\u3002\u73b0\u6709\u7684PBRL\u65b9\u6cd5\u8981\u4e48\u5e73\u7b49\u5bf9\u5f85\u6240\u6709\u53cd\u9988\uff0c\u8981\u4e48\u8bd5\u56fe\u8fc7\u6ee4\u4e0d\u53ef\u9760\u6765\u6e90\uff0c\u4f46\u5728\u9762\u5bf9\u7cfb\u7edf\u6027\u63d0\u4f9b\u9519\u8bef\u504f\u597d\u7684\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u65f6\u90fd\u4f1a\u5931\u8d25\u3002", "method": "\u63d0\u51faTriTrust-PBRL (TTP)\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u5171\u4eab\u5956\u52b1\u6a21\u578b\u548c\u4e13\u5bb6\u7279\u5b9a\u7684\u4fe1\u4efb\u53c2\u6570\u3002\u5173\u952e\u6d1e\u5bdf\u662f\u4fe1\u4efb\u53c2\u6570\u5728\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u8fc7\u7a0b\u4e2d\u81ea\u7136\u6f14\u53d8\u4e3a\u6b63\u503c\uff08\u4fe1\u4efb\uff09\u3001\u63a5\u8fd1\u96f6\uff08\u5ffd\u7565\uff09\u6216\u8d1f\u503c\uff08\u7ffb\u8f6c\uff09\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u52a8\u53cd\u8f6c\u5bf9\u6297\u6027\u504f\u597d\u5e76\u6062\u590d\u6709\u7528\u4fe1\u53f7\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e22\u5f03\u635f\u574f\u7684\u53cd\u9988\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u9886\u57df\uff08MetaWorld\u64cd\u4f5c\u4efb\u52a1\u548cDM Control\u8fd0\u52a8\u4efb\u52a1\uff09\u7684\u5404\u79cd\u635f\u574f\u573a\u666f\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002TTP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u5728\u5bf9\u6297\u6027\u635f\u574f\u4e0b\u4fdd\u6301\u63a5\u8fd1oracle\u7684\u6027\u80fd\uff0c\u800c\u6807\u51c6PBRL\u65b9\u6cd5\u5219\u5b8c\u5168\u5931\u8d25\u3002TTP\u6210\u529f\u5730\u4ece\u5305\u542b\u53ef\u9760\u548c\u5bf9\u6297\u6027\u6807\u6ce8\u8005\u7684\u6df7\u5408\u4e13\u5bb6\u6c60\u4e2d\u5b66\u4e60\uff0c\u4e14\u4ec5\u9700\u4e13\u5bb6\u6807\u8bc6\u7d22\u5f15\uff0c\u65e0\u9700\u989d\u5916\u4e13\u5bb6\u7279\u5f81\u3002", "conclusion": "TriTrust-PBRL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u4e13\u5bb6\u504f\u597d\u53cd\u9988\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u6807\u6ce8\u8005\uff0c\u901a\u8fc7\u81ea\u52a8\u5b66\u4e60\u4fe1\u4efb\u53c2\u6570\u6765\u4f18\u5316\u5956\u52b1\u6a21\u578b\u5b66\u4e60\uff0c\u5728\u73b0\u6709PBRL\u6d41\u7a0b\u4e2d\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2601.18760", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18760", "abs": "https://arxiv.org/abs/2601.18760", "authors": ["Henry Bell", "Lara Neubauer da Costa Schertel", "Bochu Ding", "Brandon Fain"], "title": "Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values", "comment": null, "summary": "A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \\textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \\textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGrounded Constitutional AI (GCAI)\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u5bf9AI\u7684\u666e\u904d\u671f\u671b\uff08\u901a\u7528\u539f\u5219\uff09\u548c\u4ea4\u4e92\u65f6\u504f\u597d\uff08\u60c5\u5883\u539f\u5219\uff09\u6765\u751f\u6210\u66f4\u5177\u4ee3\u8868\u6027\u7684\u4eba\u5de5\u667a\u80fd\u5baa\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u53d7\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\uff0c\u5982\u4f55\u516c\u5e73\u786e\u5b9a\u4ee3\u8868\u5e7f\u6cdb\u5229\u76ca\u76f8\u5173\u8005\u610f\u89c1\u7684\u5baa\u6cd5\u539f\u5219\u5b58\u5728\u6311\u6218\u3002\u73b0\u6709\u5baa\u6cd5AI\u6846\u67b6\u96be\u4ee5\u5145\u5206\u6355\u6349\u7528\u6237\u7684\u5177\u4f53\u504f\u597d\u548c\u60c5\u5883\u9700\u6c42\u3002", "method": "\u6269\u5c55Inverse Constitutional AI (ICAI)\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u7c7b\u504f\u597d\u6807\u6ce8\u6570\u636e\u4e2d\u7684\"\u539f\u56e0\"\u751f\u6210\u60c5\u5883\u539f\u5219\uff0c\u540c\u65f6\u4ece\u7528\u6237\u5173\u4e8eAI\u7684\"\u4ef7\u503c\u89c2\"\u9648\u8ff0\u4e2d\u63d0\u53d6\u901a\u7528\u539f\u5219\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u5baa\u6cd5\u751f\u6210\u6846\u67b6\u3002", "result": "GCAI\u751f\u6210\u7684\u5baa\u6cd5\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4f18\u4e8eICAI\u751f\u6210\u7684\u5baa\u6cd5\uff0c\u4e0d\u4ec5\u4e2a\u4eba\u504f\u597d\u5ea6\u66f4\u9ad8\uff0c\u4e5f\u66f4\u9002\u5408\u5e7f\u6cdb\u7528\u4e8e\u89c4\u8303AI\u884c\u4e3a\u3002\u53c2\u4e0e\u8005\u8ba4\u4e3aGCAI\u5baa\u6cd5\u66f4\u5177\u9053\u5fb7\u57fa\u7840\u3001\u8fde\u8d2f\u6027\u548c\u591a\u5143\u6027\u3002", "conclusion": "GCAI\u6846\u67b6\u901a\u8fc7\u6574\u5408\u901a\u7528\u539f\u5219\u548c\u60c5\u5883\u539f\u5219\uff0c\u80fd\u591f\u751f\u6210\u66f4\u5168\u9762\u4ee3\u8868\u7528\u6237\u671f\u671b\u7684AI\u5baa\u6cd5\uff0c\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5baa\u6cd5\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2601.18777", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18777", "abs": "https://arxiv.org/abs/2601.18777", "authors": ["Abhishek Divekar", "Anirban Majumder"], "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation", "comment": "Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)", "summary": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPRECISE\u6846\u67b6\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u548cLLM\u5224\u65ad\u6765\u8bc4\u4f30\u641c\u7d22\u3001\u6392\u5e8f\u548cRAG\u7cfb\u7edf\u8d28\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u9700\u6c42", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u641c\u7d22\u3001\u6392\u5e8f\u548cRAG\u7cfb\u7edf\u9700\u8981\u5927\u91cf\u4eba\u5de5\u76f8\u5173\u6027\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u6602\u3002\u867d\u7136LLM\u53ef\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u5176\u56fa\u6709\u504f\u89c1\u963b\u788d\u4e86\u76f4\u63a5\u7528\u4e8e\u6307\u6807\u4f30\u8ba1", "method": "\u63d0\u51faPRECISE\u7edf\u8ba1\u6846\u67b6\uff0c\u6269\u5c55\u9884\u6d4b\u9a71\u52a8\u63a8\u7406(PPI)\uff0c\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\uff08100\u4e2a\u67e5\u8be2\uff09\u548c\u5927\u91cfLLM\u5224\u65ad\uff0810,000\u4e2a\u672a\u6807\u6ce8\u6837\u672c\uff09\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(2^|C|)\u964d\u4f4e\u5230O(2^K)\uff0c\u5176\u4e2d|C|\u4e3a\u8bed\u6599\u5e93\u89c4\u6a21\uff0cK\u4e3a\u5e38\u6570", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u5173\u952e\u4e1a\u52a1\u6307\u6807Precision@K\u7684\u4f30\u8ba1\u65b9\u5dee\uff0c\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u6709\u6548\u6821\u6b63LLM\u504f\u89c1\uff0c\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u9700\u6c42", "conclusion": "PRECISE\u6846\u67b6\u4e3a\u641c\u7d22\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u7edf\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u548cLLM\u5224\u65ad\uff0c\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8bc4\u4f30\u6210\u672c"}}
{"id": "2601.18779", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18779", "abs": "https://arxiv.org/abs/2601.18779", "authors": ["Yuxiao Qu", "Amrith Setlur", "Virginia Smith", "Ruslan Salakhutdinov", "Aviral Kumar"], "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration", "comment": null, "summary": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.", "AI": {"tldr": "POPE\u65b9\u6cd5\u5229\u7528\u7279\u6743\u4fe1\u606f\uff08\u5982\u4eba\u7c7b\u89e3\u51b3\u65b9\u6848\uff09\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u7684\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u5728\u786c\u95ee\u9898\u4e0a\u63a2\u7d22\u5931\u8d25\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u9762\u5bf9\u56f0\u96be\u95ee\u9898\u7ecf\u5e38\u65e0\u6cd5\u83b7\u5f97\u4efb\u4f55\u6b63\u786e\u8f68\u8ff9\uff0c\u5bfc\u81f4\u96f6\u5956\u52b1\u548c\u7f3a\u4e4f\u5b66\u4e60\u4fe1\u53f7\u3002\u4f20\u7edf\u63a2\u7d22\u65b9\u6cd5\u5982\u71b5\u5956\u52b1\u3001\u91cd\u8981\u6027\u6bd4\u7387\u8c03\u6574\u7b49\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u800c\u6df7\u5408\u96be\u6613\u95ee\u9898\u8bad\u7ec3\u53cd\u800c\u4f1a\u4ea7\u751f\"\u5c04\u7ebf\u5e72\u6270\"\u73b0\u8c61\uff0c\u963b\u788d\u786c\u95ee\u9898\u7684\u8fdb\u5c55\u3002", "method": "POPE\uff08\u7279\u6743\u5728\u7ebf\u63a2\u7d22\uff09\u65b9\u6cd5\uff1a\u5229\u7528\u4eba\u7c7b\u6216\u5176\u4ed6oracle\u89e3\u51b3\u65b9\u6848\u4f5c\u4e3a\u7279\u6743\u4fe1\u606f\u6765\u5f15\u5bfc\u786c\u95ee\u9898\u7684\u63a2\u7d22\u3002\u5177\u4f53\u505a\u6cd5\u662f\u5728\u786c\u95ee\u9898\u524d\u6dfb\u52a0oracle\u89e3\u51b3\u65b9\u6848\u7684\u524d\u7f00\uff0c\u4f7fRL\u5728\u5f15\u5bfc\u8f68\u8ff9\u4e2d\u83b7\u5f97\u975e\u96f6\u5956\u52b1\u3002\u5173\u952e\u662f\u901a\u8fc7\u6307\u4ee4\u8ddf\u968f\u548c\u63a8\u7406\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5c06\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8fc1\u79fb\u56de\u539f\u59cb\u672a\u5f15\u5bfc\u7684\u95ee\u9898\u3002", "result": "POPE\u663e\u8457\u6269\u5c55\u4e86\u53ef\u89e3\u51b3\u95ee\u9898\u7684\u96c6\u5408\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5e45\u63d0\u5347\u4e86\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "POPE\u901a\u8fc7\u7279\u6743\u4fe1\u606f\u5f15\u5bfc\u63a2\u7d22\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\u7684\u63a2\u7d22\u5931\u8d25\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06oracle\u89e3\u51b3\u65b9\u6848\u4f5c\u4e3a\u63a2\u7d22\u5f15\u5bfc\u800c\u975e\u8bad\u7ec3\u76ee\u6807\u7684\u521b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.18783", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.18783", "abs": "https://arxiv.org/abs/2601.18783", "authors": ["Deepthi Pathare", "Leo Laine", "Morteza Haghir Chehreghani"], "title": "Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic", "comment": null, "summary": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u91cd\u578b\u8f66\u8f86\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u51b3\u7b56\uff0c\u5b66\u4e60\u8fde\u7eed\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u96c6\u4ee5\u5e73\u8861\u5b89\u5168\u3001\u80fd\u8017\u548c\u65f6\u95f4\u6548\u7387\u7684\u6743\u8861\u3002", "motivation": "\u91cd\u578b\u8f66\u8f86\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u9700\u8981\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8fd0\u8425\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u590d\u6742\u7684\u6743\u8861\u51b3\u7b56\u3002\u4f20\u7edf\u7684\u6807\u91cf\u5956\u52b1\u51fd\u6570\u901a\u8fc7\u805a\u5408\u8fd9\u4e9b\u7ade\u4e89\u76ee\u6807\u5f80\u5f80\u6a21\u7cca\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u6743\u8861\u7ed3\u6784\uff0c\u9700\u8981\u66f4\u660e\u786e\u7684\u6743\u8861\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u5361\u8f66\u6218\u672f\u51b3\u7b56\u7684\u53ef\u6269\u5c55\u4eff\u771f\u5e73\u53f0\u4e0a\u5b66\u4e60\u8fde\u7eed\u7684\u653f\u7b56\u96c6\uff0c\u660e\u786e\u8868\u793a\u5b89\u5168\uff08\u78b0\u649e\u548c\u5b8c\u6210\u7387\uff09\u3001\u80fd\u6e90\u6548\u7387\uff08\u80fd\u6e90\u6210\u672c\uff09\u548c\u65f6\u95f4\u6548\u7387\uff08\u9a7e\u9a76\u5458\u6210\u672c\uff09\u4e09\u4e2a\u51b2\u7a81\u76ee\u6807\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u8be5\u65b9\u6cd5\u5b66\u4e60\u5230\u5e73\u6ed1\u4e14\u53ef\u89e3\u91ca\u7684\u8fde\u7eed\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u96c6\uff0c\u80fd\u591f\u6355\u83b7\u4e0d\u540c\u51b2\u7a81\u76ee\u6807\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u751f\u6210\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u5e73\u6ed1\u53ef\u89e3\u91ca\uff0c\u5141\u8bb8\u5728\u4e0d\u540c\u9a7e\u9a76\u884c\u4e3a\u4e4b\u95f4\u7075\u6d3b\u9009\u62e9\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u5e94\u7528\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u81ea\u9002\u5e94\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u80fd\u591f\u65e0\u7f1d\u5728\u4e0d\u540c\u9a7e\u9a76\u7b56\u7565\u4e4b\u95f4\u5207\u6362\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u80fd\u8017\u548c\u65f6\u95f4\u6548\u7387\u4e4b\u95f4\u7684\u660e\u786e\u6743\u8861\u8868\u793a\u3002"}}
{"id": "2601.18795", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.18795", "abs": "https://arxiv.org/abs/2601.18795", "authors": ["Amrith Setlur", "Zijian Wang", "Andrew Cohen", "Paria Rashidinejad", "Sang Michael Xie"], "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes", "comment": null, "summary": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.", "AI": {"tldr": "PrefixRL\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528\u65e7\u7684\u8ba1\u7b97\u8d44\u6e90\uff08\u79bb\u7ebf\u7b56\u7565\u8f68\u8ff9\uff09\u6765\u63d0\u5347LLM\u63a8\u7406\u4efb\u52a1\u7684\u8bad\u7ec3\u6548\u7387\uff0c\u907f\u514d\u4f20\u7edf\u79bb\u7ebf\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u5b9e\u73b02\u500d\u8bad\u7ec3\u52a0\u901f\u548c3\u500d\u6700\u7ec8\u5956\u52b1\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5728LLM\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u56f0\u96be\u95ee\u9898\u4e0a\u7684\u8ba1\u7b97\u6d6a\u8d39\u95ee\u9898\uff1a\u6b63\u786e\u7b56\u7565\u8f68\u8ff9\u7a00\u5c11\u3001\u7b56\u7565\u68af\u5ea6\u6d88\u5931\u3001\u5b66\u4e60\u505c\u6ede\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684RL\u65b9\u6cd5\u6765\u91cd\u7528\u5df2\u6709\u7684\u8ba1\u7b97\u8d44\u6e90\uff08\u79bb\u7ebf\u7b56\u7565\u8f68\u8ff9\uff09\u3002", "method": "\u63d0\u51faPrefixRL\u65b9\u6cd5\uff1a\u57fa\u4e8e\u6210\u529f\u79bb\u7ebf\u7b56\u7565\u8f68\u8ff9\u7684\u524d\u7f00\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u7136\u540e\u8fd0\u884c\u5728\u7ebfRL\u6765\u5b8c\u6210\u5269\u4f59\u90e8\u5206\u3002\u901a\u8fc7\u8c03\u6574\u524d\u7f00\u957f\u5ea6\u6765\u8c03\u8282\u95ee\u9898\u96be\u5ea6\uff0c\u907f\u514d\u79bb\u7ebf\u7b56\u7565\u7684\u4e0d\u7a33\u5b9a\u6027\u3002\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u83b7\u53d6\u79bb\u7ebf\u8f68\u8ff9\uff0c\u5f62\u6210\u81ea\u6211\u6539\u8fdb\u5faa\u73af\u3002", "result": "\u5728\u56f0\u96be\u63a8\u7406\u95ee\u9898\u4e0a\uff0cPrefixRL\u8fbe\u5230\u76f8\u540c\u8bad\u7ec3\u5956\u52b1\u7684\u901f\u5ea6\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff08\u79bb\u7ebf\u6570\u636eSFT\u540eRL\uff09\u5feb2\u500d\uff0c\u6700\u7ec8\u5956\u52b1\u63d0\u53473\u500d\u3002\u53d1\u73b0\u540e\u5411\u6cdb\u5316\u73b0\u8c61\uff1a\u4ec5\u5728\u524d\u7f00\u95ee\u9898\u4e0a\u8bad\u7ec3\u80fd\u6cdb\u5316\u5230\u65e0\u524d\u7f00\u6027\u80fd\u3002\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u6765\u6e90\u7684\u79bb\u7ebf\u8f68\u8ff9\u4e0a\u4ecd\u7136\u6709\u6548\u3002", "conclusion": "PrefixRL\u901a\u8fc7\u91cd\u7528\u79bb\u7ebf\u8ba1\u7b97\u8d44\u6e90\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2dRL\u8bad\u7ec3\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u79bb\u7ebf\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u5728\u56f0\u96be\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u7075\u6d3b\u6027\u3002"}}
