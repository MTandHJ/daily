<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 151]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.AI](#cs.AI) [Total: 100]
- [cs.LG](#cs.LG) [Total: 196]
- [cs.CY](#cs.CY) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

TL;DR: 提出基于深度学习的自动化框架，从历史地图中提取细粒度城市变化信息，应用于巴黎1868-1937年的城市转型分析


<details>
  <summary>Details</summary>
Motivation: 历史地图记录了城市长期转型，但传统方法受空间错位、制图差异和文档质量退化限制，难以进行大规模、定量分析

Method: 采用模块化设计的深度学习框架，集成密集地图对齐、多时相目标检测和变化分析三个核心模块

Result: 实验验证了对齐和目标检测方法的鲁棒性，应用于巴黎案例揭示了城市转型的空间和时间异质性

Conclusion: 该框架将历史地图分析从临时视觉比较转向系统定量表征，模块化设计支持适应不同制图背景和下游应用

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [2] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

TL;DR: MAD-RAG：一种解决检索增强生成中注意力分散问题的训练免费干预方法，通过双问题表述和解耦视觉定位与上下文集成，在多个VQA数据集上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 研究发现检索增强生成中存在先前研究忽视的失败模式——注意力分散。当检索到的上下文足够相关时，检索文本会全局抑制视觉注意力，导致注意力从问题相关区域转移，使得原本能正确回答的问题反而失败。

Method: 提出MAD-RAG方法：1）通过双问题表述解耦视觉定位与上下文集成；2）结合注意力混合以保留图像条件证据；3）无需额外训练，计算开销可忽略。

Result: 在OK-VQA、E-VQA和InfoSeek数据集上，MAD-RAG在不同模型家族中均优于现有基线，相对原始RAG基线分别获得4.76%、9.20%和6.18%的绝对提升，最高可纠正74.68%的失败案例。

Conclusion: MAD-RAG有效解决了检索增强生成中的注意力分散问题，通过简单而有效的干预策略显著提升了大型视觉语言模型在知识型VQA任务上的性能，且无需额外训练成本。

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [3] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

TL;DR: 该研究对基于Transformer的视觉语言模型在巴西葡萄牙语图像描述任务上进行了跨原生-翻译评估，比较了原生标注和自动翻译数据集，发现Swin-DistilBERTimbau表现最佳，ViTucano在传统指标上优于大型多语言模型，GPT-4在图像-文本对齐方面表现最好。


<details>
  <summary>Details</summary>
Motivation: 图像描述任务主要集中于英语模型，巴西葡萄牙语等低资源语言因缺乏专门数据集和模型而面临挑战。现有研究通过自动翻译现有数据集来缓解资源稀缺问题，但需要评估这种翻译方法对模型性能的影响。

Method: 使用包含巴西葡萄牙语原生标注的Flickr30K版本和自动翻译版本进行比较。采用跨上下文方法，在一个数据集上训练模型，在另一个数据集上测试以评估翻译影响。结合注意力图进行模型推理解释，并使用CLIP-Score评估图像-描述对齐。

Result: Swin-DistilBERTimbau在所有模型中表现最稳定，展现出强大的跨数据集泛化能力。巴西葡萄牙语预训练模型ViTucano在传统文本评估指标上超越GPT-4o和LLaMa 3.2 Vision等大型多语言模型。GPT-4模型获得最高的CLIP-Score，表明其图像-文本对齐能力最佳。注意力分析揭示了系统性偏见，包括性别误分类、对象枚举错误和空间不一致性。

Conclusion: 该研究为巴西葡萄牙语图像描述任务提供了重要基准，表明原生标注数据集对模型性能有积极影响，而自动翻译数据集存在局限性。研究结果强调了开发专门针对低资源语言的预训练模型的重要性，并为未来研究提供了数据集和模型资源。

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [4] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: SparseCut是一种用于多模态大语言模型（MLLMs）的跨模态融合架构，通过稀疏快捷连接实现多层次视觉特征的高效分层融合，提升跨模态理解能力而不增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs大多关注扩展语言模型或构建高质量训练数据，但缺乏有效将跨模态知识整合到语言空间的方法。在视觉语言模型中，仅使用高层视觉特征进行模态对齐会丢弃中低层特征中的丰富语义信息，限制了模型的跨模态理解能力。

Method: 提出SparseCut架构，在跨模态编码器和LLM之间引入稀疏快捷连接，实现多层次视觉特征的高效分层融合。进一步提出高效多粒度特征融合模块，在通过快捷连接路由前进行视觉特征融合，保持原始语言上下文且不增加输入长度。

Result: 实验表明，SparseCut显著提升了MLLMs在各种多模态基准测试上的性能，对不同基础LLM具有通用性和可扩展性。

Conclusion: SparseCut通过稀疏快捷连接和多粒度特征融合，有效解决了MLLMs中跨模态知识整合不足的问题，在保持计算效率的同时显著提升了模型的跨模态理解能力。

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [5] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

TL;DR: MAUGen是一个基于扩散模型的多模态框架，能够从单个文本提示生成逼真面部表情和对应的解剖学一致的动作单元标签（包括发生和强度），并创建了大规模合成数据集MIFA。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏大规模、人口统计学多样且具有精确动作单元（AU）发生和强度标注的面部图像，这已成为开发可泛化AU识别系统的基本瓶颈。

Method: 提出MAUGen框架，包含两个关键模块：1）多模态表示学习模块，在统一潜在空间中捕获文本描述、面部身份、表情图像和AU激活之间的关系；2）基于扩散的图像标签生成器，将联合表示解码为跨不同身份的对齐面部图像-标签对。

Result: 创建了Multi-Identity Facial Action（MIFA）大规模多模态合成数据集，包含全面的AU标注和身份变化。实验表明MAUGen在合成逼真、人口统计学多样的面部图像以及语义对齐的AU标签方面优于现有方法。

Conclusion: MAUGen通过生成大规模、多样化的合成数据，解决了AU识别领域的数据瓶颈问题，为开发更鲁棒和可泛化的AU识别系统提供了有效解决方案。

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [6] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

TL;DR: NOVA是一个非对比视觉-语言对齐框架，通过联合嵌入预测和分布正则化，避免了传统对比学习方法需要大批量、负采样和复杂超参数调优的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的对比学习方法如CLIP需要大批量训练、仔细的负采样和大量超参数调优，训练过程复杂且不稳定。研究者希望开发一种更简单、更稳定的视觉-语言对齐方法。

Method: NOVA框架通过预测增强图像视图的文本嵌入来对齐视觉表示到冻结的领域特定文本编码器，同时使用Sketched Isotropic Gaussian Regularization (SIGReg)强制各向同性高斯结构，无需负采样、动量编码器或停止梯度。

Result: 在胸部X光零样本分类任务中，使用ClinicalBERT作为文本编码器和Vision Transformers在MIMIC-CXR上训练，NOVA在三个基准数据集上的零样本分类性能优于多个标准基线，且训练过程更加稳定一致。

Conclusion: 非对比视觉-语言预训练为对比方法提供了更简单、更稳定、更有效的替代方案，NOVA框架通过消除对比学习的复杂性，实现了更好的性能和训练稳定性。

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [7] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出一种计算高效的超分辨率方法，从各向异性的2D解剖照片3D重建中生成解剖一致的各向同性体积，提高神经病理学分析的空间精度


<details>
  <summary>Details</summary>
Motivation: 现有从2D解剖照片重建3D脑体积的方法有时会产生粗糙、过度平滑的重建结果，特别是在高各向异性（厚切片）情况下，需要提高重建分辨率和解剖保真度

Method: 引入计算高效的超分辨率步骤，通过插值切片从各向异性的3D重建生成解剖一致的各向同性体积；使用领域随机化的合成数据进行训练，确保方法对不同解剖协议和大切片厚度的鲁棒性

Result: 插值后的体积改善了自动分割效果，在皮层和白质区域获得更高的Dice分数；在表面重建和图谱配准任务验证中，显示出更准确的皮层表面和MRI配准

Conclusion: 该方法通过增强基于照片重建的分辨率和解剖保真度，加强了神经病理学和神经影像学之间的桥梁，方法已公开可用

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [8] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

TL;DR: 本研究评估了基于U-Net的深度学习模型在臂丛神经超声图像分割中的应用，探讨了数据集组成和标注策略对分割性能的影响。


<details>
  <summary>Details</summary>
Motivation: 超声引导下区域麻醉中，由于图像对比度低、斑点噪声和患者间解剖变异等因素，手动神经定位具有挑战性，需要开发自动化的神经分割方法。

Method: 采用U-Net架构进行深度学习神经分割，研究不同数据集组成（多台超声设备数据组合）和标注策略（从二分类神经分割扩展到多类别监督：动脉、静脉、神经、肌肉）对性能的影响。

Result: 多设备数据组合训练对性能较低的采集源有正则化效果，但无法超越目标域匹配的单源训练；多类别监督导致神经特异性Dice分数下降9%-61%；神经大小与分割精度呈中等正相关（r=0.587）。

Conclusion: 研究结果为在真实临床数据约束下开发稳健的超声神经分割系统提供了方法学指导，指出小神经分割仍是主要挑战，需要针对性的解决方案。

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [9] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

TL;DR: Distill3R：一个将3D基础模型的几何推理能力蒸馏到可在单工作站上训练的小型学生模型的框架，实现9倍参数减少和5倍推理加速


<details>
  <summary>Details</summary>
Motivation: 当前多视图3D重建依赖于需要大规模计算集群训练的基础模型，这为大多数学术实验室设置了很高的进入门槛。为了解决这个计算鸿沟，需要开发能够在单工作站上训练的高效模型。

Method: 提出两个主要创新：1）离线缓存管道，通过压缩监督信号将繁重的教师推理与训练循环解耦；2）置信感知蒸馏损失，利用教师不确定性实现在商用硬件上训练。设计了一个7200万参数的学生模型。

Result: 学生模型相比650M参数的教师模型实现了9倍参数减少和5倍推理加速。学生模型可在单工作站上3天内完成训练，而教师模型需要大规模GPU集群训练一周。学生模型保持了结构一致性和定性几何理解能力。

Conclusion: Distill3R为没有大规模计算资源的实验室提供了一个可复现的单工作站训练方案，作为民主化3D视觉研究的探索入口和高效边缘部署的解决方案，旨在为特定领域数据训练提供可访问的研究基线。

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [10] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

TL;DR: ConsensusDrop是一种无需训练的方法，通过融合视觉编码器显著性和查询感知的跨注意力信号来减少视觉语言模型中冗余的视觉token，在保持准确性的同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理大量冗余视觉token导致计算成本高昂。现有token减少方法要么使用视觉编码器显著性（广泛但查询无关），要么使用LLM跨注意力（查询感知但稀疏且昂贵），两者单独使用都不够充分。

Method: 提出ConsensusDrop框架，通过调和视觉编码器显著性与查询感知的跨注意力来获得共识排名，保留信息最丰富的token，同时通过编码器引导的token合并压缩其余token。

Result: 在LLaVA-1.5/NeXT、Video-LLaVA等开源VLM上，ConsensusDrop在相同token预算下优于现有剪枝方法，提供更强的准确性-效率帕累托前沿，即使在激进的token减少下也能保持接近基线的准确性，同时减少TTFT和KV缓存占用。

Conclusion: 融合视觉编码器显著性和查询感知的跨注意力信号对于有效的视觉token减少至关重要，ConsensusDrop提供了一种无需训练的实际解决方案，在保持准确性的同时显著提高视觉语言模型的效率。

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [11] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 该论文提出了HitEmotion基准测试和TMPO方法，用于评估和提升多模态大语言模型基于认知的情感理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型发展迅速，但其深度情感理解能力仍然有限。作者认为真正的情感智能需要显式建模心智理论，因为情感源于这一认知基础。

Method: 1. 引入HitEmotion基准测试，基于心智理论构建分层评估体系，诊断不同认知深度的能力断点；2. 提出心智理论引导的推理链，追踪心理状态并校准跨模态证据；3. 提出TMPO强化学习方法，使用中间心理状态作为过程级监督来指导和加强模型推理。

Result: HitEmotion暴露了最先进模型在深度情感推理方面的缺陷，特别是在认知要求高的任务上。心智理论引导的推理链和TMPO方法提高了最终任务准确性，并产生了更忠实、更连贯的推理依据。

Conclusion: 该工作为研究社区提供了实用的工具包，用于评估和增强多模态大语言模型基于认知的情感理解能力。数据集和代码已开源。

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [12] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 该论文介绍了NeurIPS 2025 Mouse vs. AI竞赛中获胜的方法：Track 1使用轻量级两层CNN+GLU实现视觉鲁棒性（95.4%分数）；Track 2使用16层ResNet-like架构实现神经对齐最佳性能（1780万参数）。研究发现训练步数与性能呈非单调关系，最佳结果在20万步左右。


<details>
  <summary>Details</summary>
Motivation: 解决视觉鲁棒性和神经对齐这两个关键挑战，开发能够匹配生物视觉系统的人工智能体。通过参加NeurIPS 2025竞赛，探索不同架构在视觉任务中的表现差异。

Method: Track 1采用轻量级两层CNN架构，增强门控线性单元和观测归一化；Track 2采用16层卷积的ResNet-like架构，同样使用GLU门控机制。系统分析了10个模型检查点（6万到114万训练步数），并进行全面的消融研究和失败案例分析。

Result: Track 1获得95.4%最终分数，Track 2获得top-1神经预测性能（1780万参数）。研究发现训练步数与性能呈非单调关系，最佳结果在20万步左右。简单架构在视觉鲁棒性上表现优异，而更深层模型在神经对齐上表现更好。

Conclusion: 研究挑战了关于视觉运动学习中模型复杂性的传统假设，为开发鲁棒的、受生物学启发的视觉智能体提供了实用指导。简单架构在视觉鲁棒性方面表现优异，而更大容量的深层模型在神经对齐方面表现更好。

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [13] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

TL;DR: VAMOS-OCTA：一种用于修复运动伪影OCTA B扫描的深度学习框架，采用血管感知多轴监督方法，能同时提升B扫描清晰度和容积投影精度。


<details>
  <summary>Details</summary>
Motivation: 手持式OCTA在非合作或儿科患者中易受运动伪影影响，导致B扫描中出现未采样区域，在en face投影中产生空白带，严重影响图像质量。

Method: 提出VAMOS-OCTA框架，使用2.5D U-Net架构，以相邻B扫描堆栈作为输入重建受损的中心B扫描，采用新颖的血管感知多轴正交监督损失函数，结合血管加权强度重建与轴向和横向投影一致性。

Result: VAMOS-OCTA在合成和真实世界受损数据上均优于现有方法，能重建出具有清晰毛细血管、恢复血管连续性和干净en face投影的图像。

Conclusion: 多轴监督为修复运动退化的3D OCTA数据提供了强大的约束，VAMOS-OCTA能有效提升手持OCTA在非合作患者中的成像质量。

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [14] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

TL;DR: SRVAU-R1提出了一种用于视频异常理解的自反思增强推理框架，通过引入反思机制提升多模态大语言模型对异常行为的深度推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的视频异常理解方法主要关注异常的表面描述，缺乏对异常行为的深度推理，如显式的自我反思和自我修正能力。

Method: 提出SRVAU-R1框架：1）创建首个面向反思的视频异常理解Chain-of-Thought数据集，包含初始推理、自我反思和修正推理的结构化监督；2）采用包含监督微调和强化微调的反思感知学习范式。

Result: 在多个视频异常基准测试上的实验表明，SRVAU-R1在时间异常定位准确性和推理质量方面均显著优于现有方法。

Conclusion: 通过引入反思机制，SRVAU-R1有效提升了多模态大语言模型在视频异常理解任务中的深度推理能力，为视频异常分析提供了新的解决方案。

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [15] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: LocalScore是一种用于开放集生物识别的简单有效评分算法，通过利用k近邻显式纳入图库特征分布的局部密度，显著提升开放集检索和验证性能。


<details>
  <summary>Details</summary>
Motivation: 开放集生物识别面临未注册探针的挑战，现有方法通常将同一主体的多个样本压缩为单一全局表示，导致决策边界不理想和开放集鲁棒性差。现实部署中多样本图库日益普遍，需要更好的解决方案。

Method: 提出LocalScore算法，利用k近邻显式计算图库特征分布的局部密度。该方法与架构无关、损失函数独立，计算开销可忽略，可作为即插即用方案集成到现有生物识别系统中。

Result: 在多种模态上的实验表明，LocalScore在开放集检索方面将FNIR@FPIR从53%降至40%，在验证方面将TAR@FAR从51%提升至74%。理论分析和实证验证解释了该方法在不同数据集特性下的最佳增益条件。

Conclusion: LocalScore是一种简单有效的开放集生物识别解决方案，通过显式建模图库特征的局部密度，显著提升系统性能，且具有架构无关性和低计算开销的优势。

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [16] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 研究验证了自动标注的甲状腺结节数据集能显著提升深度学习模型性能，且使用全部自动标注数据比仅使用高精度子集效果更好。


<details>
  <summary>Details</summary>
Motivation: 甲状腺结节癌症诊断通常使用超声图像，深度学习算法能匹配放射科医生水平，但训练数据获取困难。先前研究提出自动标注方法（63%产出率，83%准确率），但其对深度学习训练的实际效果未知。

Method: 在手动标注数据集和自动标注数据集上分别训练深度学习模型，同时使用自动标注数据集中精度较高的子集进行训练，以探索最佳使用方式。

Result: 手动标注数据集训练的模型AUC为0.643，显著低于自动标注数据集训练的模型AUC 0.694（P<0.001）。高精度子集训练的模型AUC为0.689，与完整自动标注数据集无显著差异（P>0.43）。

Conclusion: 自动标注数据集能显著提升深度学习算法性能，建议使用全部自动标注数据而非仅使用高精度子集。

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [17] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

TL;DR: FUSE-Flow：一种实时多视角点云重建框架，通过帧间独立处理、自适应空间哈希和加权融合，在保持几何细节的同时实现线性可扩展性


<details>
  <summary>Details</summary>
Motivation: 实时多视角点云重建在VR/AR、机器人导航、数字孪生等领域有广泛应用，但现有方法在计算复杂度、内存使用和可扩展性方面存在局限，难以同时实现实时性能、重建质量和多相机扩展性

Method: 提出FUSE-Flow框架：1）每帧独立生成点云片段；2）通过测量置信度和3D距离一致性两个权重进行融合以抑制噪声；3）引入自适应空间哈希加权聚合方法，根据局部点云密度自适应划分3D空间，每个单元选择代表点进行加权融合；4）GPU并行化实现

Result: 实验表明，该框架在重叠区域、深度不连续和动态场景中提高了重建稳定性和几何保真度，同时在现代GPU上保持实时帧率，验证了其有效性、鲁棒性和可扩展性

Conclusion: FUSE-Flow通过帧间独立处理、自适应空间哈希和加权融合，实现了线性可扩展的实时多视角点云重建，解决了现有方法在计算复杂度、内存使用和可扩展性方面的限制

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [18] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 本文提出VEQ框架，针对MoE视觉语言模型量化中的模态差异和专家异质性问题，通过模态-专家感知和模态-亲和力感知量化方法，显著提升量化性能。


<details>
  <summary>Details</summary>
Motivation: MoE视觉语言模型虽然性能优异，但内存和计算成本过高，需要压缩。现有量化方法忽视了两种关键异质性：视觉和语言token之间的固有差异，以及不同专家的非均匀贡献。

Method: 提出VEQ双感知量化框架：1) 模态-专家感知量化，利用专家激活频率优先最小化关键专家的误差；2) 模态-亲和力感知量化，通过整合token-专家亲和力与模态信息构建增强Hessian矩阵来指导校准过程。

Result: 在W3A16配置下，VEQ在Kimi-VL上平均准确率提升2.04%，在Qwen3-VL上提升3.09%，优于现有最先进的量化方法，在各种多模态任务上表现出优越的鲁棒性。

Conclusion: VEQ通过同时考虑跨模态差异和专家异质性，为MoE视觉语言模型提供了有效的训练后量化解决方案，显著提升了量化性能。

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [19] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

TL;DR: 该论文提出了一个自动将单人教学视频转换为双人多模态任务指导对话的框架，并发布了HowToDIV数据集，包含507个对话和6,636个问答对，为多模态程序性任务辅助提供了基准。


<details>
  <summary>Details</summary>
Motivation: 许多日常任务需要专业知识，但AI助手在增强现实辅助方面的进展受到大规模多模态对话数据集稀缺的限制，传统人工数据收集成本高且复杂。

Method: 基于大语言模型的完全自动流水线，将单人教学视频转换为专家-新手交互的多轮对话，提供可扩展且成本效益高的数据收集替代方案。

Result: 创建了HowToDIV多模态数据集，包含507个对话、6,636个问答对和24小时视频，涵盖多个领域，并使用Gemma 3和Qwen 2.5提供了基准结果。

Conclusion: 该框架为多模态程序性任务辅助提供了可扩展的数据生成解决方案，HowToDIV数据集为相关研究提供了有价值的资源和初始基准。

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [20] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

TL;DR: ReLayout是一个无需三元组数据的自动设计布局编辑框架，通过关系图保持未编辑元素的布局结构，使用关系感知设计重建方法实现多种编辑操作。


<details>
  <summary>Details</summary>
Motivation: 设计工作流程中需要自动化重设计而无需人工调整。用户自然语言表达的编辑意图存在歧义，且缺乏（原始设计、编辑操作、编辑后设计）三元组数据，同时需要保持未编辑元素的布局结构。

Method: 提出ReLayout框架：1）引入关系图表示未编辑元素之间的位置和大小关系作为布局结构约束；2）提出关系感知设计重建（RADR）方法，通过从元素、关系图和合成编辑操作中学习重建设计，以自监督方式模拟编辑过程；3）使用多模态大语言模型作为RADR主干，统一多种编辑操作。

Result: 定性和定量结果以及用户研究表明，ReLayout在编辑质量、准确性和布局结构保持方面显著优于基线模型。

Conclusion: ReLayout是一个无需三元组数据的多功能、结构保持的设计布局编辑框架，通过关系图和自监督学习有效解决了编辑操作歧义和数据稀缺的挑战。

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [21] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

TL;DR: ResDec是一种无需训练的解码方法，利用历史信息和LVLM的内部推理机制来减少视觉语言模型中的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然在多模态任务中表现良好，但受到语言先验的影响，经常产生幻觉（生成与视觉输入无关但语法连贯的内容）

Method: 提出Residual Decoding（ResDec）方法，这是一种无需训练的解码技术，利用历史信息和LVLM的内部隐式推理机制及token logits演化机制来纠正偏差

Result: ResDec有效抑制了语言先验引起的幻觉，显著改善了视觉基础，减少了物体幻觉，同时在综合LVLM基准测试中表现优异

Conclusion: ResDec是一种有效的训练免费方法，能够减少LVLM中的幻觉问题，并具有广泛的适用性

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [22] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D高斯射线追踪的断层扫描重建框架，解决了现有3D高斯泼溅方法在断层扫描重建中的局限性，通过解析计算线积分和精确控制射线方向，提高了投影精度并扩展了高斯重建方法的应用范围。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）及其断层扫描扩展R2-Gaussian虽然实现了实时高质量重建，但使用的局部仿射近似会降低重建定量精度，且难以整合非线性几何校正。需要更物理一致的前向投影模型来克服这些限制。

Method: 提出基于3D高斯射线追踪的断层扫描重建框架：1）解析计算通过3D高斯基元的线积分，避免局部仿射坍塌；2）射线追踪公式提供对射线起点和方向的显式控制，便于精确应用非线性几何校正（如PET中的弧校正）。

Result: 该方法相比基于泼溅的模型有两个关键优势：提供更物理一致的前向投影模型，提高投影精度；能够精确应用非线性几何校正，扩展高斯重建方法在更广泛真实断层扫描系统中的应用。

Conclusion: 基于3D高斯射线追踪的断层扫描重建框架克服了现有泼溅方法的局限性，通过解析线积分和精确射线控制，实现了更准确的投影模型和更广泛的系统适用性，推动了高斯重建方法在断层扫描领域的发展。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [23] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

TL;DR: 提出DRFormer框架，通过双正则化双向Transformer融合DINO的局部纹理特征和CLIP的全局语义特征，解决行人重识别中的遮挡和姿态变化问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单一范式（要么使用视觉基础模型如DINO挖掘局部纹理，要么使用视觉语言模型如CLIP捕获全局语义差异），忽略了这两种架构互补优势的整合潜力。局部判别性细节和全局语义特征都能帮助解决行人重识别中的遮挡和姿态变化挑战。

Method: 提出DRFormer（双正则化双向Transformer）框架，通过双正则化机制确保多样化的特征提取，并平衡两种模型的贡献。该框架协同融合DINO的局部纹理挖掘能力和CLIP的全局语义差异捕获能力。

Result: 在五个基准数据集上的大量实验表明，该方法有效协调了局部和全局表示，取得了与最先进方法相竞争的性能。

Conclusion: 通过分析两种架构的互补作用并提出DRFormer框架，成功融合了局部纹理特征和全局语义特征的优势，为行人重识别任务提供了更有效的解决方案。

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [24] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

TL;DR: 该论文提出了一种基于PDE约束优化的图像分割方法，将物理先验通过变分正则化整合到深度学习模型中，在显微镜图像分割任务上相比无约束基线模型取得了更好的分割精度、边界保真度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分割面临测量噪声、弱边界和有限标注数据等挑战，无约束的深度学习方法容易产生不稳定解和泛化能力差的问题。需要将物理先验整合到深度学习框架中以提高分割的鲁棒性和泛化性。

Method: 将图像分割建模为PDE约束优化问题，通过变分正则化将反应-扩散方程和相场界面能量等物理先验整合到深度学习模型中。使用可微分残差损失实现数据保真项和惩罚项的组合目标函数最小化。

Result: 在LIVECell数据集上的实验表明，相比无约束的UNet基线模型，PDE正则化模型在分割精度和边界保真度方面有显著提升，在低样本情况下表现出更好的稳定性和泛化能力。

Conclusion: PDE约束优化为数据驱动学习框架提供了原则性桥梁，将变分方法、统计学习和科学机器学习有机结合，展示了结构化先验在提升深度学习模型鲁棒性和泛化性方面的优势。

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [25] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

TL;DR: PISA是一种训练无关的分段稀疏注意力机制，通过精确计算关键块和近似计算非关键块来平衡速度与质量，在扩散Transformer中实现次二次复杂度。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在视频和图像生成中至关重要，但其注意力机制的二次复杂度成为效率瓶颈。现有块稀疏注意力方法通过仅关注关键键值块来加速计算，但在高稀疏度下会因丢弃上下文信息而导致质量下降。

Method: 提出PISA（Piecewise Sparse Attention），基于发现非关键块的注意力分数具有分布稳定性，可采用块级泰勒展开高效近似。采用"精确或近似"策略：对关键块保持精确计算，对非关键块通过泰勒展开进行高效近似，从而覆盖完整注意力范围。

Result: 在Wan2.1-14B上实现1.91倍加速，在Hunyuan-Video上实现2.57倍加速，同时在稀疏注意力方法中保持最高质量。在FLUX图像生成上实现1.2倍加速且不损害视觉质量。

Conclusion: PISA通过创新的精确-近似策略有效弥合了速度与质量之间的差距，为扩散Transformer提供了一种高效且高质量的稀疏注意力解决方案。

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [26] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

TL;DR: 论文提出MedAD-38K基准和MedAD-R1模型，通过认知注入和一致性组相对策略优化提升医学异常检测的推理能力，在基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学异常检测依赖监督微调，使用简单碎片化数据集，限制了模型进行合理推理和鲁棒多模态泛化的能力。

Method: 1) 构建MedAD-38K基准：首个大规模、多模态、多中心医学异常检测基准，包含诊断思维链标注和结构化视觉问答对；2) 两阶段训练框架：第一阶段认知注入使用SFT注入基础医学知识并对齐思维-回答范式；第二阶段引入一致性组相对策略优化，通过一致性奖励确保推理过程与最终诊断相关且逻辑连贯。

Result: 提出的MedAD-R1模型在MedAD-38K基准上取得最先进性能，超越强基线10%以上，能够生成透明且逻辑一致的推理路径。

Conclusion: 该方法通过增强推理过程的相关性和逻辑一致性，为提升临床决策支持AI的可信度和可解释性提供了有前景的途径。

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [27] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

TL;DR: 提出DVE方法，一种无需训练的流匹配模型概念擦除技术，通过分析速度场中的方向结构差异来选择性移除特定概念


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然能生成高质量图像，但会复制不良概念（如NSFW内容、版权风格等），现有概念擦除方法主要针对DDPM模型且需要微调，而新兴的流匹配模型需要专门的方法

Method: 提出差分向量擦除(DVE)方法：1）发现语义概念编码在速度场的方向结构中；2）构建差分向量场来表征目标概念与锚概念的方向差异；3）在推理时通过将速度场投影到差分方向来选择性移除概念特定组件

Result: 在FLUX模型上的大量实验表明，DVE在NSFW抑制、艺术风格移除和对象擦除等广泛任务上始终优于现有基线，同时保持图像质量和多样性

Conclusion: DVE是一种专门为流匹配模型设计的无需训练的概念擦除方法，通过利用速度场中的方向结构差异实现精确概念抑制，为安全可控的生成模型部署提供了有效解决方案

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [28] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: PandaPose提出了一种通过将2D姿态先验传播到3D锚点空间作为统一中间表示的3D人体姿态提升方法，解决了现有方法中2D姿态误差传播和自遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态提升方法通常基于2D特征建立直接的关节到关节映射，这种公式存在两个基本限制：1）从输入预测的2D姿态到3D预测的不可避免的误差传播；2）处理自遮挡情况的固有困难。

Method: 提出PandaPose方法，通过将2D姿态先验传播到3D锚点空间作为统一中间表示。该方法包含三个核心组件：1）规范坐标系中的关节级3D锚点；2）深度感知的关节级特征提升；3）锚点-特征交互解码器，将3D锚点与提升的特征结合生成统一的锚点查询。

Result: 在Human3.6M、MPI-INF-3DHP和3DPW三个基准测试上验证了方法的优越性。在Human3.6M的挑战性条件下，相比最先进方法误差减少了14.7%，定性比较进一步展示了方法的有效性和鲁棒性。

Conclusion: PandaPose通过引入3D锚点空间作为中间表示，有效解决了2D姿态误差传播和自遮挡问题，在多个基准测试上取得了显著性能提升，展示了其在3D人体姿态提升任务中的有效性和鲁棒性。

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [29] [Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning](https://arxiv.org/abs/2602.01101)
*Felix Breiteneder,Mohammad Belal,Muhammad Saad Saeed,Shahed Masoudian,Usman Naseem,Kulshrestha Juhi,Markus Schedl,Shah Nawaz*

Main category: cs.CV

TL;DR: 本文首次全面研究了模态不完整数据下的有害表情包检测方法，提出了一种新的基线方法，通过独立投影学习多模态共享表示，在文本缺失时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 互联网表情包是强大的传播工具，但可能被用于传播仇恨。现有检测方法通常依赖完整的模态数据（文本和图像），但在实际应用中，由于OCR质量差等问题可能导致文本缺失，现有方法对缺失信息敏感且性能下降。

Method: 提出一种新的基线方法，通过独立投影学习多模态的共享表示，这些共享表示可以在数据模态不完整时被利用。该方法能够更好地整合视觉特征，减少对文本的依赖。

Result: 在两个基准数据集上的实验结果表明，当文本缺失时，该方法优于现有方法。结果表明该方法能够更好地整合视觉特征，减少对文本的依赖，提高在文本信息缺失场景下的鲁棒性。

Conclusion: 这项工作代表了有害表情包检测在实际应用中的重要进展，特别是在模态缺失的情况下，为实现真实世界的有害表情包检测应用迈出了重要一步。

Abstract: Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.

</details>


### [30] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

TL;DR: LightCity是一个用于城市逆渲染研究的新型高质量合成数据集，包含多种光照条件和丰富的属性标注，用于基准测试城市环境中的基本任务。


<details>
  <summary>Details</summary>
Motivation: 城市场景的逆渲染在自动驾驶和数字孪生等应用中至关重要，但由于复杂的光照条件（包括多重光照、间接光和阴影效应）而面临挑战。目前缺乏合适的数据集来探索这些挑战对内在分解和3D重建的影响。

Method: 提出了LightCity数据集，这是一个高质量合成城市数据集，包含300多个天空图、高度可控的光照、不同尺度（街道级和航拍视角）、超过5万张图像，以及深度、法线、材质组件、光照和间接光等丰富属性。

Result: LightCity数据集为城市环境中的三个基本任务提供了基准测试，并进行了全面的分析，为相关研究的推进奠定了坚实基础。

Conclusion: LightCity数据集填补了城市逆渲染研究中的数据空白，为探索复杂光照条件下的内在分解和3D重建提供了重要资源，有助于推动自动驾驶和数字孪生等相关领域的发展。

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [31] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

TL;DR: Koo-Fu CLIP是一种基于Fukunaga-Koontz线性判别分析的监督式CLIP适配方法，通过在白化嵌入空间中抑制类内变化并增强类间区分，显著提升分类准确率并实现高效降维。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型提供了强大的通用表示，但其原始嵌入在监督分类任务中表现不佳，存在类间分离有限和维度冗余的问题，需要优化以提升分类性能。

Method: 提出Koo-Fu CLIP方法，基于Fukunaga-Koontz线性判别分析，在白化嵌入空间中操作，通过封闭形式的线性投影重塑CLIP嵌入的几何结构，抑制类内变化并增强类间区分。

Result: 在ImageNet基准测试中，Koo-Fu CLIP将ImageNet-1K的top-1准确率从75.1%提升至79.1%，在扩展到14K和21K类别时仍保持一致的性能提升，同时支持10-12倍的压缩而几乎不损失准确率。

Conclusion: Koo-Fu CLIP提供了一种轻量高效的CLIP表示适配方法，显著改善了类间可分离性并实现有效的降维，适用于大规模分类和检索任务。

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [32] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

TL;DR: 本文针对VLA模型在视觉干扰下的脆弱性，提出了一种名为CRT的即插即用视觉变换器，能够在不微调底层模型的情况下恢复被破坏的视觉输入，显著提升VLA在真实环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型虽然在受控环境中表现出色，但在真实世界部署时对视觉干扰（特别是图像损坏）极其脆弱。现有研究主要关注物理遮挡，而传感器层面的图像损坏（如电子噪声、坏点、镜头污染等）这一关键问题尚未得到充分探索。

Method: 提出Corruption Restoration Transformer (CRT)，一种即插即用、模型无关的视觉变换器。采用对抗训练目标，能够从损坏的输入中恢复干净的观测，无需对底层VLA模型进行计算昂贵的微调。

Result: 实验表明，当前最先进的VLA模型（如π₀.₅和SmolVLA）在常见信号损坏下性能急剧下降（从90%成功率降至2%）。CRT能有效恢复丢失的性能，使VLA即使在严重视觉损坏下也能维持接近基线的成功率。

Conclusion: CRT为VLA模型提供了一种有效的视觉损坏免疫方案，解决了传感器层面干扰的关键漏洞，显著提升了VLA在真实世界部署的鲁棒性和可靠性。

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [33] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了EEmoDB，这是目前最大的图像诱发情感理解数据集，包含5个分析维度和5个任务类别，以及EEmo-Logic模型，这是一个通过指令微调和GRPO优化的多模态大语言模型，在情感理解和细粒度评估方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型在图像诱发情感理解方面存在局限性，要么只能进行粗粒度的情感感知，要么缺乏推理能力。为了弥补这一差距，需要构建更全面的数据集和更强大的模型来理解图像的多维情感属性和强度细微差别。

Method: 1. 构建EEmoDB数据集：包含125k张图像的120万QA对（EEmoDB-QA）和25k张图像的36k数据集（EEmoDB-Assess），涵盖5个分析维度和5个任务类别；2. 提出EEmo-Logic模型：通过指令微调和任务定制的组相对偏好优化（GRPO）进行开发，采用新颖的奖励设计。

Result: EEmo-Logic在领域内和跨领域数据集上都表现出稳健的性能，在情感QA和细粒度评估方面表现优异。模型代码已开源。

Conclusion: EEmoDB是目前最大的图像诱发情感理解数据集，EEmo-Logic模型通过创新的训练方法在情感理解和评估任务上取得了显著进展，为机器共情和人机交互应用提供了有力支持。

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [34] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

TL;DR: 提出一个用于长期天气预报的新框架，通过高效多尺度Transformer架构、累积上下文微调和复合损失函数，解决灾难性遗忘、误差累积和高训练开销问题，显著提升长期预测精度。


<details>
  <summary>Details</summary>
Motivation: 长期天气预报对社会经济规划和灾害准备至关重要。现有方法通过微调扩展预测范围，但仍受限于灾难性遗忘、误差累积和高训练开销等问题。需要新的方法来增强长期上下文建模同时降低计算成本。

Method: 1. 提出高效多尺度Transformer（EMFormer），通过单次卷积在训练和推理中提取多尺度特征；2. 采用累积上下文微调，在不降低短期精度的情况下改善时间一致性；3. 设计复合损失函数，通过正弦加权动态平衡不同项，自适应指导预训练和微调的优化轨迹。

Result: 实验表明，该方法在天气预报和极端事件预测中表现优异，显著提高了长期预测精度。EMFormer在视觉基准测试（ImageNet-1K和ADE20K）上表现出强泛化能力，同时比传统多尺度模块提速5.69倍。

Conclusion: 提出的跨预训练、微调和预测的完整管道有效解决了长期天气预报中的关键挑战，在保持计算效率的同时显著提升了长期预测性能，并在视觉任务中展示了良好的泛化能力。

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [35] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: Med3D-R1：一个两阶段强化学习框架，通过残差对齐和异常重加权提升3D医学影像-语言模型的临床推理能力，在CT-RATE和RAD-ChestCT基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉语言模型面临三大挑战：1）体素医学影像的固有复杂性；2）模型容易过拟合表面报告模式；3）缺乏可解释性奖励设计。需要开发具有稳健临床推理能力的3D医学视觉语言模型。

Method: 提出Med3D-R1两阶段训练框架：1）监督微调阶段：引入残差对齐机制连接3D特征与文本嵌入，采用异常重加权策略强调临床信息标记；2）强化学习阶段：重新设计一致性奖励以促进连贯的逐步诊断推理。

Result: 在两个3D诊断基准测试中取得SOTA：CT-RATE上达到41.92%准确率，RAD-ChestCT上达到44.99%准确率。结果表明在异常诊断和临床推理方面均有改进，优于先前方法。

Conclusion: 该方法通过提升3D医学视觉语言系统的可靠性和透明度，有望增强真实世界诊断工作流程，为临床实践提供更可靠的辅助工具。

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [36] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种文本精炼与对齐框架，通过利用视频描述中的文本特征来增强点监督时序动作定位的性能，在五个基准测试中取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 当前点监督时序动作定位方法仅考虑视觉输入特征，忽略了文本侧的有用语义信息。为了在标注成本和定位精度之间取得更好平衡，需要利用文本特征来补充视觉特征。

Method: 提出文本精炼与对齐框架，包含两个新模块：基于点的文本精炼模块和基于点的多模态对齐模块。首先使用预训练多模态模型生成视频帧描述，然后利用点标注和多个预训练模型精炼初始描述，最后通过点级多模态特征对比学习将特征投影到统一语义空间。

Result: 在五个广泛使用的基准测试中，该框架表现出优于多种最先进方法的性能。计算开销分析表明框架可在单张24GB RTX 3090 GPU上运行，具有实用性和可扩展性。

Conclusion: 通过有效利用文本特征补充视觉特征，提出的文本精炼与对齐框架显著提升了点监督时序动作定位的性能，在保持实用性的同时实现了更好的定位精度。

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [37] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

TL;DR: TFM模块利用实时交通流信息增强车道感知，在遮挡或车道缺失场景下显著提升性能，在Nuscenes数据集上获得最高+4.1% mAP增益。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的车道检测方法在遮挡或车道缺失场景下性能显著下降，而使用高精地图作为补充信息存在订阅成本高和实时性有限的问题。

Method: 提出TFM（TrafficFlow-aware Lane perception Module）模块，有效提取实时交通流特征，并将其与现有车道感知算法无缝集成。

Result: 在四个主流模型和两个公开数据集（Nuscenes和OpenLaneV2）上的实验表明，TFM能持续提升性能，在Nuscenes数据集上获得最高+4.1% mAP增益。

Conclusion: 利用实时交通流作为补充信息源，无需额外成本即可显著提升自动驾驶系统的车道感知鲁棒性，特别是在视觉线索不足的场景下。

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [38] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 研究发现大语言模型中存在跨语言共享安全神经元(SS-Neurons)，这些神经元数量极少但对多语言安全行为调节至关重要。通过激活或抑制这些神经元可以显著影响模型的安全拒绝行为，并提出了针对这些神经元的训练策略来提升非高资源语言的安全性能。


<details>
  <summary>Details</summary>
Motivation: 多语言安全存在严重不平衡问题，非高资源语言的安全性能远低于高资源语言，且神经机制不明确。需要理解跨语言安全对齐的神经基础，并找到有效方法来提升非高资源语言的安全性能。

Method: 1. 识别单语言安全神经元(MS-Neurons)并验证其在安全拒绝行为中的因果作用；2. 通过跨语言分析识别SS-Neurons作为高资源语言和非高资源语言共享的安全神经元子集；3. 提出基于语言资源分布和模型架构的神经元导向训练策略，针对SS-Neurons进行微调。

Result: 抑制SS-Neurons会导致非高资源语言安全性能同时下降，增强这些神经元能提高跨语言防御一致性。实验表明，仅微调这一小部分神经元子集就能超越现有方法，显著提升非高资源语言安全性能，同时保持模型的通用能力。

Conclusion: LLMs中存在关键的跨语言共享安全神经元，这些神经元是多语言安全调节的核心机制。通过针对性地训练这些神经元，可以有效解决多语言安全不平衡问题，提升非高资源语言的安全性能，为多语言安全对齐提供了新的神经基础理解和实用方法。

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [39] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LiP-Map是一个线-平面联合优化框架，通过显式建模可学习的线和平面基元，从多视角RGB图像实现准确高效的3D线映射。


<details>
  <summary>Details</summary>
Motivation: 从物理和拓扑角度研究3D线映射问题，认为3D线最自然地作为有限3D平面块的边缘出现。现有方法缺乏对线和平面之间关系的显式建模。

Method: 提出LiP-Map框架，显式建模可学习的线和平面基元，通过构建平面和线基元之间的交互来整合平面拓扑，而不是施加成对共面约束。

Result: 在ScanNetV2、ScanNet++、Hypersim、7Scenes和Tanks&Temple等100多个场景上，LiP-Map在准确性和完整性方面优于最先进方法。重建效率高（每场景3-5分钟），在线辅助视觉定位方面表现优异。

Conclusion: LiP-Map通过线-平面联合优化，为人造环境中的结构化重建提供了原则性途径，显著提升了3D线映射质量和线辅助视觉定位性能。

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [40] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

TL;DR: 论文提出了一种交互一致的对象移除方法（ICOR），不仅要移除目标对象，还要移除相关的交互元素，并开发了REORM框架和ICOREval基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前基于图像的对象移除方法通常只移除命名目标，而忽略了与之相关的交互证据（如光照效果、物理连接对象、目标产生的元素和上下文关联对象），导致结果在语义上不一致。

Method: 提出了REORM框架，采用模块化设计，包括MLLM驱动的分析、掩码引导的移除和自校正机制，还提供了本地部署变体以支持有限资源下的准确编辑。

Result: 在ICOREval基准测试中，REORM优于最先进的图像编辑系统，证明了其在产生交互一致结果方面的有效性。

Conclusion: 该研究形式化了交互一致的对象移除问题，提出了有效的解决方案，并通过基准测试验证了方法的优越性，为更智能的图像编辑提供了新思路。

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [41] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

TL;DR: ReDiStory是一个无需训练的框架，通过推理时重组提示嵌入来改善多帧故事生成，减少帧间语义干扰，提升身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法将身份和帧提示拼接为统一表示，但在复杂故事中常引入帧间语义干扰，削弱身份保持能力。

Method: 将文本嵌入显式分解为身份相关和帧特定组件，通过抑制跨帧共享方向来解相关帧嵌入，无需修改扩散参数或额外监督。

Result: 在相同扩散骨干和推理设置下，ReDiStory在ConsiStory+基准测试中相比1Prompt1Story在多个身份一致性指标上获得一致提升。

Conclusion: ReDiStory通过推理时提示嵌入重组有效减少跨帧干扰，在保持提示保真度的同时显著改善多帧故事生成中的身份一致性。

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [42] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

TL;DR: MED框架分析视觉工具使用强化学习，发现当前方法主要是学习与工具安全共存而非真正掌握工具


<details>
  <summary>Details</summary>
Motivation: 视觉工具使用强化学习虽然能提升性能，但不清楚这种提升是来自工具使用的改进还是模型内在能力的演化。需要区分这两者的影响

Method: 提出MED框架：1) 粗粒度分离内在能力变化与工具诱导效应；2) 将工具诱导的性能差异分解为增益和损害项；3) 探究驱动其演化的机制

Result: 在两个具有不同工具先验的视觉语言模型和六个基准测试中，改进主要由内在学习主导，工具使用强化学习主要减少工具诱导的损害（如调用错误和工具模式干扰），在纠正内在失败方面的工具使用进展有限

Conclusion: 当前的视觉工具使用强化学习主要是学习与工具安全共存，而非真正掌握工具

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [43] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

TL;DR: 提出一种将固定压缩率VAE转换为支持多级时间压缩模型的技术，通过最小化微调解决高压缩率下的性能下降问题，并验证了与扩散模型的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有LVDMs依赖VAE压缩视频，但连续VAE在提高压缩率时效率显著下降，特别是增加采样层而不扩展隐藏通道维度时性能恶化，需要解决高压缩率下的性能问题。

Method: 提出一种技术将固定压缩率VAE转换为支持多级时间压缩的模型，提供简单且最小化的微调方法；研究不同压缩级别对视频片段性能的影响；将多级时间压缩VAE与DiT扩散模型集成。

Result: 成功实现了多级时间压缩VAE的转换，提供了对抗高压缩率性能下降的有效方法；验证了不同压缩级别对不同特性视频片段的影响；展示了与扩散模型框架的兼容性和并发训练成功。

Conclusion: 该技术能够有效解决VAE在高压缩率下的性能下降问题，多级时间压缩方法具有实际应用潜力，且与现有扩散模型框架兼容，为视频压缩和生成提供了新思路。

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [44] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

TL;DR: NOVA是一个基于熵分析的无训练令牌缩减加速框架，用于视觉自回归模型，通过自适应确定加速激活尺度并动态计算各尺度和层的令牌缩减比例来加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有VAR令牌缩减方法存在三个关键限制：启发式阶段划分、非自适应调度和有限加速范围，未能充分利用加速潜力。熵变化能反映预测不确定性的转变，为捕捉建模动态演化提供了原则性度量。

Method: NOVA通过在线识别尺度熵增长的拐点来自适应确定推理中的加速激活尺度。通过尺度链接和层链接比例调整，动态计算每个尺度和层的不同令牌缩减比例，剪枝低熵令牌，同时重用先前尺度残差派生的缓存来加速推理并保持生成质量。

Result: 大量实验和分析验证了NOVA作为一个简单而有效的无训练加速框架的有效性。

Conclusion: NOVA通过熵分析实现了视觉自回归模型的无训练令牌缩减加速，解决了现有方法的局限性，在保持生成质量的同时显著加速推理过程。

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [45] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

TL;DR: T2M Mamba模型通过周期性-显著性感知Mamba和周期性差分跨模态对齐模块，解决了文本到动作生成中的长期序列漂移和语义等价改写脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成模型存在两个核心限制：1) 将动作周期性和关键帧显著性视为独立因素，忽略了它们的耦合关系，导致长序列生成漂移；2) 对语义等价改写脆弱，微小的同义词替换会扭曲文本嵌入，传播到解码器产生不稳定或错误的动作。

Method: 提出T2M Mamba模型：1) 周期性-显著性感知Mamba，通过增强密度峰值聚类进行关键帧权重估计，以及FFT加速自相关进行动作周期性估计，以最小计算开销捕获耦合动态；2) 周期性差分跨模态对齐模块(PDCAM)，增强文本和动作嵌入的鲁棒对齐。

Result: 在HumanML3D和KIT-ML数据集上的大量实验证实了方法的有效性，实现了FID为0.068，并在所有其他指标上获得一致提升。

Conclusion: T2M Mamba通过有效建模动作周期性和关键帧显著性之间的耦合关系，并增强跨模态对齐的鲁棒性，显著提升了文本到动作生成的长期序列稳定性和语义等价改写鲁棒性。

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [46] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

TL;DR: 该论文提出了一种针对视频混合专家模型对抗鲁棒性的研究框架，包括攻击方法TLGA和防御方法J-TLAT，揭示了MoE模型中路由器和专家模块的独立与协同弱点。


<details>
  <summary>Details</summary>
Motivation: 混合专家模型在视频理解任务中表现出色，但其对抗鲁棒性尚未得到充分探索。现有攻击方法通常将MoE视为统一架构，忽视了路由器和专家模块等关键组件的独立和协同弱点。

Method: 提出了时间Lipschitz引导攻击(TLGA)来研究视频MoE模型的组件级漏洞。首先设计针对路由器的攻击，揭示其独立弱点；然后提出联合时间Lipschitz引导攻击(J-TLGA)，协同扰动路由器和专家模块；最后提出联合时间Lipschitz对抗训练(J-TLAT)进行防御。

Result: 联合攻击显著放大了对抗效果，暴露了MoE架构的"阿喀琉斯之踵"（协同弱点）。J-TLAT通过联合训练有效防御协同弱点，增强组件级鲁棒性。该框架是即插即用的，相比密集模型减少60%以上的推理成本，在多种数据集和架构上一致提升对抗鲁棒性。

Conclusion: 该研究填补了MoE模型对抗鲁棒性研究的空白，揭示了MoE组件级弱点，并提出了有效的攻击和防御框架，为构建更鲁棒的MoE系统提供了重要见解。

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [47] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

TL;DR: PolyGen框架通过多生成器聚合和程序化负样本课程，在相同数据预算下提升合成数据的多样性和鲁棒性，相比单源方法在多项基准测试中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于合成数据的视觉语言预训练方法通常依赖单一生成器扩展，这会导致生成器特定的频谱偏差并限制特征多样性。需要一种能提高合成数据质量而非简单增加数据量的方法。

Method: PolyGen采用多生成器聚合方法，在架构不同的生成器交集上进行训练，以消除模型特定伪影。同时引入程序化硬负样本课程，强制模型学习细粒度语法理解。通过将相同数据预算从独特标题重新分配到多源变体，构建更鲁棒的特征空间。

Result: PolyGen在聚合多任务基准测试中比领先的单源基线(SynthCLIP)提升19.0%，在SugarCrepe++组合性基准测试中提升9.1%。证明结构多样性比简单增加单源样本量更有效。

Conclusion: 结构多样性是比简单增加单源样本量更高效的数据扩展规律。通过多生成器聚合和程序化负样本课程，可以在相同数据预算下获得更鲁棒的特征表示。

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [48] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

TL;DR: ALI方法通过融合像素对齐的视觉编码器特征到潜在内在框架，平衡语义抽象与光度保真度，在复杂材质上显著提升重光照质量


<details>
  <summary>Details</summary>
Motivation: 现有基于潜在内在表示的方法在金属、玻璃等挑战性材质上表现不佳，而高性能语义编码器的特征反而会降低重光照质量，揭示了语义抽象与光度保真度之间的基本权衡

Method: 提出增强潜在内在表示（ALI），将像素对齐视觉编码器的特征融合到潜在内在框架中，并采用自监督细化策略缓解真实世界配对数据稀缺问题

Result: ALI在重光照任务上取得显著改进，在复杂镜面材质上提升最大，仅使用未标记的真实世界图像对进行训练

Conclusion: 通过平衡语义上下文和密集光度结构，ALI解决了语义抽象与光度保真度之间的权衡问题，在挑战性材质上实现了更好的重光照效果

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [49] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: PaPE是一种基于抛物线的位置编码方法，专门为视觉模态设计，在注意力架构中编码视觉token的位置，考虑了视觉模态的特性，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要将语言中的1D序列位置编码扩展到视觉中的nD结构，但对视觉特性的考虑不充分。需要设计一种能充分考虑视觉模态特性的位置编码方法。

Method: 提出抛物线位置编码(PaPE)，基于从先前工作中提炼的原则设计：平移不变性、旋转不变性(PaPE-RI)、距离衰减、方向性和上下文感知。

Result: 在涵盖4种模态的8个数据集上评估，PaPE或PaPE-RI在7个数据集上达到最佳性能。在ImageNet-1K的外推实验中，PaPE表现优异，比次优位置编码绝对提升高达10.5%。

Conclusion: PaPE是一种有效的视觉模态位置编码方法，能够充分考虑视觉特性，在多种视觉任务中表现优异，且具有良好的外推能力。

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [50] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

TL;DR: 该研究比较了三种视觉方法（直接目标检测、分割辅助分类、查询式视觉语言模型）在驾驶场景中识别驾驶员注视点对应语义对象的效果，发现YOLOv13和Qwen2.5-VL-32b表现最佳，后者在夜间识别小物体方面更具优势。


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员在驾驶过程中的视觉注意力分布（通过注视行为表征）对于开发下一代高级驾驶辅助系统和提高道路安全至关重要。本研究旨在从车辆前视摄像头捕捉的道路场景中，通过语义识别任务来解决这一挑战。

Method: 研究采用三种不同的视觉方法：1）直接目标检测（YOLOv13）；2）分割辅助分类（SAM2配合EfficientNetV2 vs YOLOv13）；3）查询式视觉语言模型（Qwen2.5-VL-7b vs Qwen2.5-VL-32b）。通过分析注视点与物体语义的关联来评估各方法性能。

Result: 直接目标检测（YOLOv13）和Qwen2.5-VL-32b显著优于其他方法，Macro F1-Score均超过0.84。大型VLM（Qwen2.5-VL-32b）在识别小尺寸、安全关键物体（如交通信号灯）方面表现出更强的鲁棒性，尤其在夜间恶劣条件下。分割辅助方法因"部分vs整体"语义差距导致召回率大幅下降。

Conclusion: 研究揭示了传统检测器的实时效率与大型VLM提供的更丰富上下文理解和鲁棒性之间的基本权衡。这些发现为未来人类感知智能驾驶员监控系统的设计提供了关键见解和实践指导。

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [51] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

TL;DR: 该研究分析了量化视觉Transformer在分布外检测中的表现，发现大规模预训练可能损害低比特量化的鲁棒性，而数据增强可能是更好的选择。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在视觉任务中表现出色，但实现可访问和实时使用仍具挑战。量化能降低内存和推理成本，但可能导致性能损失。现有研究主要关注分布内任务行为，而注意力机制可能通过探索分布外情况提供量化特性的洞察。

Method: 研究量化的小型流行视觉Transformer（DeiT、DeiT3和ViT）在常见分布外数据集上的行为。分析包括分布内校准和分布外检测，比较不同预训练规模（ImageNet-22k vs ImageNet-1k）对量化鲁棒性的影响。

Result: 分布内分析显示4位模型存在初始不稳定性，特别是那些在较大ImageNet-22k上训练的模型。最强的FP32模型DeiT3因量化误差急剧下降17%，成为最弱的4位模型之一。分布外检测揭示：在ImageNet-22k上预训练的ViT和DeiT3分别经历了15.0%和19.2%的平均AUPR-out量化差异，而仅在ImageNet-1k上预训练的对应模型差异为9.5%和12.0%。

Conclusion: 大规模数据集预训练可能损害低比特量化在分布外检测中的鲁棒性，数据增强可能是更有利的选择。研究强调了在评估量化模型时考虑分布外性能的重要性。

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [52] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 本文提出Logit Lens Loss (LLL)来解决视觉语言模型中视觉信息扩散问题，通过补充损失函数使图像token保持局部视觉信息，从而改善Logit Lens可视化效果并提升视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: Logit Lens在视觉语言模型(VLMs)中用于可视化图像token的概念内容，但视觉信息会扩散到语言token中，导致局部视觉信息被破坏，使得Logit Lens可视化在可解释性方面变得不可用。

Method: 提出Logit Lens Loss (LLL)作为next-token prediction (NTP)的补充损失，使视觉token嵌入与描述其图像区域的文本概念更语义对齐（例如包含猫的patch与"猫"这个词对齐），无需架构修改或大规模训练。LLL约束自注意力层中图像和文本token的混合，防止图像token失去局部视觉信息。

Result: LLL不仅通过生成有意义的对象置信度图使Logit Lens变得实用相关，还在无需附加特殊头的情况下提高了分割等视觉中心任务的性能。

Conclusion: Logit Lens Loss是一种有效的解决方案，能够保持视觉语言模型中图像token的局部视觉信息，改善可视化效果并提升视觉任务性能，且实现简单无需大规模修改。

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [53] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

TL;DR: 提出InteractAvatar框架解决接地人机交互(GHOI)视频生成挑战，通过双流架构分离感知规划与视频合成，建立GroundedInter基准进行评估


<details>
  <summary>Details</summary>
Motivation: 现有方法能生成简单人体运动的全身说话头像，但扩展到接地人机交互(GHOI)仍具挑战，需要头像与周围物体进行文本对齐的交互，这涉及环境感知和控制质量困境

Method: 提出双流框架InteractAvatar，包含感知交互模块(PIM)生成文本对齐的交互动作，音频交互感知生成模块(AIM)合成生动的交互说话头像，通过运动-视频对齐器实现并行协同生成

Result: 建立了GroundedInter基准用于评估GHOI视频生成，大量实验和比较证明了该方法在生成接地人机交互说话头像方面的有效性

Conclusion: InteractAvatar框架成功解决了GHOI视频生成的挑战，通过解耦感知规划和视频合成，有效缓解了控制质量困境，为说话头像的接地交互生成提供了有效解决方案

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [54] [FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training](https://arxiv.org/abs/2602.01540)
*Yuehai Chen*

Main category: cs.CV

TL;DR: FSCA-Net通过特征分离和交叉注意力机制解决人群计数中的负迁移问题，实现跨数据集泛化


<details>
  <summary>Details</summary>
Motivation: 现有CNN和Transformer模型在跨环境应用时性能下降，多数据集联合训练导致负迁移问题，共享特征和领域特定特征纠缠不清

Method: 提出FSCA-Net框架，将特征表示显式解耦为领域不变和领域特定组件，采用交叉注意力融合模块建模组件间交互，引入互信息优化目标最大化领域不变特征一致性并最小化领域特定特征冗余

Result: 在多个人群计数基准测试中，FSCA-Net有效缓解负迁移问题，实现最先进的跨数据集泛化性能

Conclusion: FSCA-Net为现实世界人群分析提供了鲁棒且可扩展的解决方案，通过特征分离和交叉注意力机制实现了有效的知识迁移

Abstract: Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.

</details>


### [55] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

TL;DR: 本文提出Cognitive Supersensing训练范式，通过视觉潜在意象预测和强化学习，赋予多模态大语言模型人类般的视觉意象能力，显著提升复杂认知任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在开放词汇感知任务上表现优异，但在处理需要视觉记忆和抽象视觉细节的复杂认知问题时能力有限。现有方法主要在文本空间扩展思维链推理，忽视了类似人类视觉空间画板和视觉意象的视觉推理机制。

Method: 提出Cognitive Supersensing训练范式，包含：1) 潜在视觉意象预测头，联合学习视觉认知潜在嵌入序列并与答案对齐，形成基于视觉的内部推理链；2) 强化学习阶段，基于这些视觉潜在表示优化文本推理路径。同时构建CogSense-Bench基准，评估五个认知维度。

Result: 实验表明，采用Cognitive Supersensing训练的MLLM在CogSense-Bench上显著优于最先进基线模型，并在跨领域数学和科学视觉问答基准上展现出优越的泛化能力，表明内部视觉意象可能是连接感知识别与认知理解的关键。

Conclusion: 内部视觉意象对于提升多模态大语言模型的认知能力至关重要，Cognitive Supersensing范式通过模拟人类视觉推理机制，有效弥合了感知与认知之间的鸿沟。作者将开源CogSense-Bench基准和模型权重。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [56] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: 该论文提出了CLEAR框架，首次系统研究屏幕拍摄图像中摩尔纹和闪烁条纹的联合去除问题，通过构建大规模数据集和频率域分解重构模块，显著提升了复杂真实场景下的图像恢复效果。


<details>
  <summary>Details</summary>
Motivation: 移动设备拍摄显示屏时，图像常同时遭受摩尔纹和闪烁条纹的严重退化，这两种伪影在真实成像过程中强耦合，现有针对单一退化的方法无法处理这种复合场景，导致视觉质量显著下降。

Method: 提出了统一的恢复框架CLEAR：1）构建包含摩尔纹和闪烁条纹的大规模数据集；2）引入基于ISP的闪烁模拟流程来稳定模型训练并扩展退化分布；3）设计频率域分解与重构模块，结合轨迹对齐损失来增强复合伪影建模。

Result: 大量实验表明，该方法在多个评估指标上一致优于现有图像恢复方法，验证了其在复杂真实场景中的有效性。

Conclusion: 该研究首次系统解决了屏幕拍摄图像中摩尔纹和闪烁条纹的联合去除问题，提出的CLEAR框架通过创新的数据集构建、模拟流程和频率域处理模块，为复杂退化场景的图像恢复提供了有效解决方案。

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [57] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

TL;DR: 该论文提出了多模态非常识推理基准MUN，用于评估AI模型处理偏离典型视觉或上下文期望场景的能力，并设计了基于检索的上下文学习框架R-ICL来提升小模型在此类任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态环境中的常识推理仍然是人工智能的基础挑战。现有模型在处理偏离典型视觉或上下文期望的场景时表现不佳，需要专门的基准来评估和改进模型在非典型、文化多样和现实世界场景中的鲁棒性和适应性。

Method: 1. 提出Multimodal UNcommonsense(MUN)基准，包含视觉场景与自然语言描述的意外或不太可能结果的配对；2. 设计基于检索的上下文学习(R-ICL)框架，将大模型的推理能力迁移到小模型；3. 开发新颖的多模态集成检索器(MER)，即使在图像和文本对故意不协调时也能识别语义相关的示例。

Result: 实验表明，R-ICL方法在MUN基准上比基线ICL方法平均提升8.3%，证明了该方法在低频、非典型场景中的有效性。MUN基准为评估和改进视觉语言模型在现实世界、文化多样和非典型场景中的鲁棒性开辟了新方向。

Conclusion: MUN基准和R-ICL框架为解决多模态非常识推理挑战提供了有效工具，有助于提升AI模型在偏离典型期望场景中的表现，推动视觉语言模型在现实世界多样化场景中的实际应用能力。

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [58] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

TL;DR: 提出一种单步扩散的图像压缩方法，显著提升推理速度，同时通过特征判别器保持感知质量


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像压缩方法虽然能在低码率下获得高感知质量，但由于需要大量去噪步骤，导致推理延迟高、计算开销大，阻碍了实际部署

Method: 提出只需单步扩散过程的图像压缩方法，显著提升推理速度；引入在紧凑特征表示上操作的判别器，利用特征能更好捕捉高级纹理和结构细节的特点来增强重建图像的感知质量

Result: 实验结果显示，该方法在保持可比压缩性能的同时，相比最近的扩散方法实现了46倍的推理速度提升

Conclusion: 提出的单步扩散图像压缩方法有效解决了扩散模型推理速度慢的问题，为实际部署提供了可行方案

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [59] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

TL;DR: HandMCM：基于状态空间模型Mamba的3D手部姿态估计方法，通过局部信息注入/过滤和对应关系建模模块，有效学习关键点动态运动学拓扑，在多模态图像特征增强下，在严重遮挡场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D手部姿态估计对于增强现实等人机交互应用至关重要，但由于手部自遮挡和与物体交互造成的遮挡，该任务面临重大挑战。现有方法在处理严重遮挡场景时性能有限。

Method: 提出HandMCM方法，基于状态空间模型Mamba，包含局部信息注入/过滤模块和对应关系建模模块，能够有效学习关键点在不同遮挡场景下的动态运动学拓扑。同时整合多模态图像特征以增强输入鲁棒性和表示能力。

Result: 在三个基准数据集上的实证评估表明，该方法显著优于当前最先进方法，特别是在涉及严重遮挡的挑战性场景中表现突出。

Conclusion: 该方法展示了在3D手部姿态估计任务中提高准确性和可靠性的潜力，特别是在实际应用中常见的遮挡场景下，为相关应用提供了更可靠的解决方案。

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [60] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

TL;DR: 提出TAFS-GRPO框架，通过温度退火采样和组相对策略优化，解决流匹配模型在少步文本到图像生成中奖励信号稀疏、不精确的问题，提升人类偏好对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的流匹配模型通常需要大量去噪步骤，且面临稀疏、不精确的奖励信号问题，导致人类偏好对齐效果不佳。需要一种能在少步生成中提供密集、精确奖励信号的训练框架。

Method: 提出TAFS-GRPO框架：1）温度退火少步采样：通过迭代注入自适应时间噪声到单步采样结果，在保持图像语义完整性的同时引入采样随机性；2）组相对策略优化：结合步感知优势集成机制，避免奖励函数可微性要求，为稳定策略优化提供密集、步特定的奖励信号。

Result: 大量实验表明，TAFS-GRPO在少步文本到图像生成中表现优异，显著提升了生成图像与人类偏好的对齐程度。代码和模型将公开以促进进一步研究。

Conclusion: TAFS-GRPO框架有效解决了流匹配模型在少步生成中的奖励稀疏问题，通过温度退火采样和组相对策略优化的结合，实现了高效、稳定的人类偏好对齐，为少步文本到图像生成提供了新思路。

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [61] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了Samba和Samba+模型，基于Mamba状态空间模型解决显著目标检测任务，平衡全局感受野和计算效率，支持多种模态和任务类型。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测模型受限于CNN的有限感受野和Transformer的二次计算复杂度，需要一种既能保持全局感受野又计算高效的新架构。

Method: 1. Samba：纯Mamba架构，包含显著性引导Mamba块（SGMB）和上下文感知上采样（CAU）；2. Samba+：多任务联合训练的统一模型，包含中心辐射图注意力（HGA）模块和模态锚定持续学习（MACL）策略。

Result: Samba在6个SOD任务的22个数据集上超越现有方法且计算成本更低；Samba+使用单一训练模型在这些任务和数据集上取得更优结果。

Conclusion: 提出的Samba框架在显著目标检测任务中展现出优越性能，通过Mamba架构平衡了全局感受野和计算效率，Samba+进一步实现了多任务统一建模，为SOD领域提供了有前景的解决方案。

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [62] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

TL;DR: 提出UV-M3TL框架，通过双分支空间通道多模态嵌入和自适应特征解耦多任务损失，同时识别驾驶员行为、情绪、车辆行为和交通环境，缓解任务间负迁移问题，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 高级驾驶辅助系统需要同时理解驾驶员行为和感知导航环境，但联合学习这些异构任务会导致任务间负迁移，损害系统性能。现有方法难以有效处理多模态多任务学习中的任务冲突问题。

Method: 提出统一且通用的多模态多任务学习框架UV-M3TL，包含两个核心组件：1) 双分支空间通道多模态嵌入(DB-SCME)，通过双分支结构显式建模任务共享和任务特定特征；2) 自适应特征解耦多任务损失(AFD-Loss)，基于学习动态和特征解耦约束的自适应加权机制。

Result: 在AIDE数据集上，UV-M3TL在四个任务上均达到最先进性能。在BDD100K、CityScapes、NYUD-v2和PASCAL-Context等公开多任务感知基准上，该框架在不同任务组合中均表现优异，在大多数任务上取得SOTA结果。

Conclusion: UV-M3TL框架有效缓解了多模态多任务学习中的负迁移问题，实现了驾驶员行为和交通环境的联合理解，具有很好的通用性和扩展性，为高级驾驶辅助系统提供了有效的多任务学习解决方案。

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [63] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: Omni-Judge研究评估全模态大语言模型能否作为文本条件音视频生成的人类对齐评估器，发现其在语义对齐任务上表现优异，但在高帧率感知指标上存在局限。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型（如Sora 2和Veo 3）能产生高质量音视频，但评估这种三模态输出仍具挑战性。人工评估可靠但成本高，传统自动指标局限于单模态对且解释性差。全模态大语言模型能自然处理音视频文本并支持丰富推理，有望成为替代方案。

Method: 引入Omni-Judge研究，评估全模态大语言模型作为文本条件音视频生成的人类对齐评估器的能力。研究覆盖九个感知和对齐指标，与传统指标进行相关性比较。

Result: Omni-Judge在相关性上与传统指标相当，在语义要求高的任务（音频-文本对齐、视频-文本对齐、音频-视频-文本一致性）上表现优异。但在高帧率感知指标（视频质量、音频-视频同步）上表现不佳，主要受限于时间分辨率。模型能提供可解释的反馈，揭示语义或物理不一致性。

Conclusion: 全模态大语言模型作为多模态生成的统一评估器具有潜力，特别是在语义对齐任务上，但当前在时间分辨率相关的高帧率感知指标上存在局限性。模型的可解释性反馈支持下游应用如基于反馈的优化。

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [64] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

TL;DR: PISCES提出了一种基于双重最优传输对齐奖励的无标注后训练算法，用于改进文本到视频生成的视觉质量和语义对齐，无需人工偏好标注。


<details>
  <summary>Details</summary>
Motivation: 现有基于奖励的后训练方法要么依赖大规模人工偏好标注，要么使用预训练视觉语言模型中的未对齐嵌入，导致可扩展性有限或监督效果不佳。需要一种无需标注但能提供高质量奖励信号的方法来改进文本到视频生成。

Method: 提出PISCES算法，通过新颖的双重最优传输对齐奖励模块：1) 分布级OT对齐质量奖励，捕捉整体视觉质量和时间一致性；2) 离散令牌级OT对齐语义奖励，强制文本和视频令牌之间的语义时空对应。该方法兼容多种优化范式，包括直接反向传播和强化学习微调。

Result: 在短视频和长视频生成任务上，PISCES在VBench的质量和语义分数上均优于基于标注和无标注的方法。人类偏好研究进一步验证了其有效性。双重OT对齐奖励模块与多种优化范式兼容。

Conclusion: PISCES首次通过最优传输视角改进生成后训练中的无标注奖励监督，提供了一种无需人工标注但能有效提升文本到视频生成质量和语义对齐的方法，具有更好的可扩展性和监督效果。

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [65] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

TL;DR: 该论文分析了当前世界模型研究的碎片化现状，提出了统一的设计规范，强调世界模型应整合交互、感知、符号推理和空间表示，为未来研究提供结构化指导。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型研究呈现碎片化状态，主要集中在将世界知识注入到视觉预测、3D估计或符号基础等孤立任务中，缺乏统一的定义和框架，无法实现系统性的世界理解。

Method: 分析现有碎片化方法的局限性，提出统一的世界模型设计规范，强调世界模型应作为规范性框架，整合交互、感知、符号推理和空间表示等核心要素。

Result: 提出了一个结构化的世界模型设计规范，为未来研究提供指导方向，促进更通用、鲁棒和原则化的世界模型发展。

Conclusion: 世界模型不应是松散的能力集合，而应是整合了交互、感知、符号推理和空间表示的规范性框架，这为AI研究提供了更系统化的方向。

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [66] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 提出一个联邦学习框架，结合动态自适应焦点损失和客户端感知聚合策略，解决医疗图像分类中的数据异质性和类别不平衡问题，在三个公开数据集上优于多种基线模型。


<details>
  <summary>Details</summary>
Motivation: 深度视觉模型需要大量数据，但医疗图像受隐私法规限制难以获取。联邦学习虽能保护隐私，但面临客户端数据异质性和类别不平衡的挑战，影响模型泛化能力。

Method: 提出ViT-FLDAF框架：1) 动态自适应焦点损失(DAFL)，根据客户端样本分布和类别数据分布动态调整类别不平衡系数；2) 客户端感知加权聚合策略，适应数据规模和特征以捕获客户端间差异。

Result: 在ISIC、Ocular Disease和RSNA-ICH三个公开数据集上，该框架优于DenseNet121、ResNet50、ViT-S/16、ViT-L/32、FedCLIP、Swin Transformer、CoAtNet和MixNet等模型，准确率提升0.98%到41.69%。在ISIC不平衡数据集上的消融实验验证了损失函数和聚合策略的有效性。

Conclusion: 提出的联邦学习框架通过动态自适应焦点损失和客户端感知聚合策略，有效解决了医疗图像分类中的数据异质性和类别不平衡问题，显著提升了模型性能。

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [67] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: ReCALL框架通过诊断-生成-精炼流程解决生成式MLLM适配为检索器时的能力退化问题，在CIR任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 将生成式多模态大语言模型（MLLM）适配为单嵌入判别式检索器时，会出现范式冲突，导致原生细粒度推理能力退化，这是当前组合图像检索（CIR）方法面临的核心问题。

Method: 提出ReCALL框架：1）通过自引导信息实例挖掘诊断检索器的认知盲点；2）使用CoT提示基础MLLM生成纠正指令和三元组，并通过VQA一致性过滤进行质量控制；3）采用分组对比方案在生成的三元组上进行持续训练，精化检索器。

Result: 在CIRR和FashionIQ数据集上的广泛实验表明，ReCALL能持续重新校准退化能力，并实现最先进的性能。

Conclusion: ReCALL框架有效解决了生成式MLLM适配为检索器时的能力退化问题，通过诊断-生成-精炼流程重新校准了判别式嵌入空间，在组合图像检索任务中取得了优异表现。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [68] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

TL;DR: 提出了一种用于视频序列中时间一致的人体中心密集预测的统一框架，通过合成数据管道和两阶段训练策略解决现有模型在运动、遮挡和光照变化下的闪烁问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在单帧精度上表现良好，但在运动、遮挡和光照变化下经常出现闪烁问题，且缺乏针对多个密集任务的配对人体视频监督数据。

Method: 1) 构建可扩展的合成数据管道，生成逼真的人体帧和运动对齐序列，提供像素级精确的深度、法线和掩码标签；2) 训练统一的ViT基密集预测器，通过CSE嵌入注入显式人体几何先验，并使用轻量级通道重加权模块提升几何特征可靠性；3) 采用两阶段训练策略：静态预训练获取空间表示，动态序列监督优化时间一致性。

Result: 在THuman2.1和Hi4D数据集上达到最先进性能，并能有效泛化到真实世界视频中。

Conclusion: 提出的合成数据管道和统一密集预测框架有效解决了人体中心密集预测的时间一致性问题，在多个基准测试中表现出色并具有良好的泛化能力。

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [69] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

TL;DR: 论文评估了NetVLAD作为SLAM中闭环检测模块的性能，并与传统DBoW方法在KITTI数据集上进行比较，通过Faiss加速实现实时查询，证明了NetVLAD在精度和鲁棒性上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统词袋方法（如DBoW）在SLAM闭环检测中存在外观变化和感知混淆问题，而深度学习视觉地点识别方法（如NetVLAD）虽然鲁棒性更强，但计算成本高被认为是实时SLAM的障碍。本文旨在验证NetVLAD作为闭环检测模块的实际可行性。

Method: 1. 在KITTI数据集上对NetVLAD作为闭环检测模块进行实证评估；2. 与DBoW方法进行对比；3. 引入细粒度Top-K精确率-召回率曲线，更好地反映闭环检测中查询可能有零个或多个有效匹配的情况；4. 使用Faiss加速的最近邻搜索实现实时查询速度。

Result: 通过Faiss加速，NetVLAD实现了实时查询速度，同时在精度和鲁棒性方面优于DBoW方法，使其成为SLAM中闭环检测的实用替代方案。

Conclusion: NetVLAD作为闭环检测模块不仅能够实现实时性能，而且在精度和鲁棒性方面优于传统DBoW方法，为SLAM系统提供了可行的深度学习替代方案。

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [70] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

TL;DR: 该论文发布了一个名为EDU-CIRCUIT-HW的数据集，包含1300+份大学STEM课程的学生手写解答，用于评估多模态大语言模型在手写内容识别和自动评分中的表现，发现模型存在大量潜在错误，并提出通过错误模式检测和人工干预提升系统鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在教育领域有巨大潜力，但缺乏真实、领域特定的基准来评估模型对学生手写解答（包含数学公式、图表和文本推理）的准确理解能力。现有评估方法主要依赖下游任务（如自动评分），只能探测部分识别内容，无法全面评估模型对复杂手写逻辑的整体理解。

Method: 1. 发布EDU-CIRCUIT-HW数据集，包含1300+份大学STEM课程的真实学生手写解答；2. 利用专家验证的逐字转录和评分报告，同时评估各种MLLM的上游识别保真度和下游自动评分性能；3. 通过案例研究展示如何利用识别出的错误模式，通过少量人工干预（约4%的解答）来检测和纠正识别错误，提升AI评分系统在未见学生解答上的鲁棒性。

Result: 评估揭示了MLLM在学生手写内容识别中存在惊人的潜在错误规模，表明模型在高风险教育环境中用于自动评分和其他理解导向应用的可靠性不足。案例研究表明，通过识别错误模式进行预防性检测和纠正，只需少量人工干预即可显著提升AI评分系统在未见学生解答上的鲁棒性。

Conclusion: 多模态大语言模型在解释学生手写STEM解答方面仍存在显著可靠性问题，特别是在高风险教育应用中。通过系统性地识别和纠正识别错误模式，结合少量人工干预，可以显著提升AI评分系统的鲁棒性，为教育领域的可靠AI应用提供了实用解决方案。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [71] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

TL;DR: VRGaussianAvatar是一个实时全身3D高斯溅射虚拟现实系统，仅使用头戴显示器跟踪信号，通过并行流水线实现高效渲染，在感知外观相似性、具身感和合理性方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟现实中的全身虚拟化身通常需要复杂的多摄像头设置或专门的硬件，限制了其可访问性和实时性能。本研究旨在开发一个仅使用头戴显示器跟踪信号就能实现实时全身3D高斯溅射虚拟化身的系统。

Method: 系统采用并行流水线架构，包括VR前端和GA后端。VR前端使用逆运动学估计全身姿态，并将姿态和立体相机参数流式传输到后端。GA后端从单张图像重建3D高斯溅射虚拟化身，并引入双目批处理技术，在单个批处理过程中联合处理左右眼视图，减少冗余计算，支持高分辨率VR显示。

Result: 定量性能测试和受试者内用户研究表明，VRGaussianAvatar能够维持交互式VR性能，在感知外观相似性、具身感和合理性方面优于基于图像和视频的网格虚拟化身基线方法。

Conclusion: VRGaussianAvatar展示了仅使用头戴显示器跟踪信号实现实时全身3D高斯溅射虚拟化身的可行性，通过创新的双目批处理技术提高了渲染效率，为虚拟现实中的高质量虚拟化身提供了实用解决方案。

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [72] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

TL;DR: Simulate Anything是一个图形驱动的世界建模和仿真框架，仅使用多视角环境视频和现成资产就能高效生成高保真度的具身训练数据，其训练的VLA模型在下游任务中达到与真实世界数据相当甚至更好的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 具身智能的可扩展性受到真实世界交互数据稀缺的根本限制。现有仿真平台存在视觉和物理差距，依赖昂贵传感器、精确机器人校准或深度测量，限制了大规模实用性。

Method: 使用3D高斯泼溅(3DGS)从视频重建真实环境为逼真场景表示，利用生成模型恢复物理真实表示，通过精度校准目标集成到仿真环境中，实现重建场景与真实世界的精确尺度对齐。

Result: 基于该框架模拟数据训练的视觉语言动作(VLA)模型在下游任务中表现出强大的零样本性能，匹配甚至超越了使用真实世界数据获得的结果。

Conclusion: 重建驱动的世界建模为可扩展和实用的具身智能训练提供了潜力，能够生成高质量仿真数据来弥补真实交互数据的稀缺性。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [73] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

TL;DR: SITUATE是一个用于训练和评估视觉语言模型在空间约束计数任务上的新数据集，填补了简单2D数据集和模糊真实数据集之间的空白，能提高模型在分布外图像上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有计数数据集存在局限性：VLMCountBench等简单2D数据集过于简化，而TallyQA等真实数据集缺乏对遮挡和空间组合的控制，导致模型难以处理复杂的空间约束计数任务。

Method: 提出了SITUATE数据集，专门设计用于训练和评估视觉语言模型在空间约束计数任务上的性能。通过对比实验，在Qwen VL 2.5 7B模型上进行微调，并与Pixmo count数据集进行交叉验证。

Result: 在SITUATE上微调的Qwen VL 2.5 7B模型在Pixmo count测试数据上准确率有所提升，但反之不成立。与其他计数基准测试相比，该数据集能更好地提高模型在分布外图像上的泛化性能。

Conclusion: SITUATE数据集能有效提升视觉语言模型在空间约束计数任务上的性能，特别是在处理分布外图像时表现出更好的泛化能力，填补了现有数据集的空白。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [74] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

TL;DR: 1S-DAug是一种训练免费、模型无关的测试时增强方法，仅需一张示例图像即可生成多样且保真的变体，通过结合几何扰动、受控噪声注入和条件去噪扩散过程来提升少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统测试时增强方法在少样本学习场景中效果不佳，因为只有极少数标记示例可用于增强。需要一种能够从单个图像生成多样且保真变体的方法，以提升少样本学习的泛化能力。

Method: 1S-DAug将传统几何扰动与受控噪声注入相结合，并使用基于原始图像的条件去噪扩散过程。生成的图像被编码并与原始图像聚合为组合表示，用于更鲁棒的少样本学习预测。该方法作为训练免费、模型无关的插件实现。

Result: 在4个不同数据集的标准基准测试中，1S-DAug持续提升少样本学习性能，无需任何模型参数更新。在miniImagenet 5-way-1-shot基准测试中实现了超过10%的比例精度提升。

Conclusion: 1S-DAug是一种有效的测试时增强方法，能够从单个图像生成多样且保真的变体，显著提升少样本学习的性能，且无需额外训练或模型修改。

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [75] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

TL;DR: FastPhysGS是一个快速、鲁棒的物理模拟框架，将3D高斯泼溅扩展到4D物理模拟，通过实例感知粒子填充和双向图解耦优化，在1分钟内实现高保真物理模拟，仅需7GB运行内存。


<details>
  <summary>Details</summary>
Motivation: 将3D高斯泼溅扩展到4D物理模拟面临挑战：现有方法依赖手动参数调整或从视频扩散模型提取动态，限制了泛化能力和优化效率；使用LLMs/VLMs的方法存在文本/图像到3D的感知差距，导致物理行为不稳定；且常忽略3DGS的表面结构，产生不合理的运动。

Method: 提出FastPhysGS框架：1) 实例感知粒子填充(IPF)结合蒙特卡洛重要性采样(MCIS)，高效填充内部粒子同时保持几何保真度；2) 双向图解耦优化(BGDO)，一种自适应策略，快速优化从VLM预测的材料参数。

Result: FastPhysGS在1分钟内实现高保真物理模拟，仅需7GB运行内存，性能优于先前工作，具有广泛的应用潜力。

Conclusion: FastPhysGS通过创新的粒子填充和优化策略，解决了4D物理模拟中的关键挑战，实现了快速、高效、鲁棒的动态3D高斯泼溅模拟。

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [76] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

TL;DR: 提出DenVisCoM Mamba块和混合架构，用于实时精确估计光流和视差，通过统一架构联合处理多视图几何与运动任务。


<details>
  <summary>Details</summary>
Motivation: 多视图几何与运动任务本质相关，需要统一架构同时处理光流和视差估计，在保证实时性、内存效率和准确性的前提下解决这些任务。

Method: 提出基于DenVisCoM Mamba块和Transformer注意力块的混合架构，专门针对光流和视差联合估计，平衡实时推理、内存占用和准确性。

Result: 在大量数据集上验证了准确性与实时处理的权衡，实验结果表明所提模型能够实时精确估计光流和视差。

Conclusion: 提出的DenVisCoM混合架构能够有效联合处理运动估计和3D密集感知任务，在实时性、内存效率和准确性方面表现优异。

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [77] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

TL;DR: 该论文提出了VDE Bench基准测试，专门用于评估多语言和复杂视觉文档图像编辑模型的性能，填补了现有方法在密集文本和非拉丁文字（如中文）文档编辑方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有多模态图像编辑模型在视觉文档图像编辑方面存在不足，特别是对密集文本、结构复杂文档以及非拉丁文字（如中文）的处理能力有限。现有方法如AnyText、GlyphControl和TextCtrl主要关注英文场景和稀疏文本布局，无法充分应对复杂文档编辑需求。

Method: 提出了VDE Bench基准测试，包括：1）高质量数据集，涵盖英文和中文的密集文本文档（学术论文、海报、演示文稿、考试材料、报纸等）；2）解耦评估框架，在OCR解析层面系统量化编辑性能，实现细粒度的文本修改准确性评估。

Result: 基于该基准对代表性最先进的图像编辑模型进行了全面评估。人工验证表明，人类判断与自动评估指标之间存在高度一致性。VDE Bench是首个用于评估多语言和密集文本视觉文档图像编辑模型的系统性基准。

Conclusion: VDE Bench填补了视觉文档图像编辑评估领域的空白，为多语言和复杂文档编辑提供了首个系统性基准测试，有助于推动该领域的研究进展。

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [78] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: 简单线性分类器在冻结的视觉基础模型特征上训练，在真实世界AI生成图像检测中超越了复杂专用检测器，准确率提升超过30%


<details>
  <summary>Details</summary>
Motivation: 专用AI生成图像检测器在精心设计的基准测试中表现优异，但在真实世界场景中性能急剧下降，需要更鲁棒的解决方案

Method: 使用现代视觉基础模型（如Perception Encoder、MetaCLIP 2、DINOv3）的冻结特征，仅训练一个简单的线性分类器进行AI生成图像检测

Result: 该方法在传统基准测试上与专用检测器相当，在真实世界数据集上显著超越，准确率提升超过30%，建立了新的SOTA

Conclusion: 基础模型通过大规模预训练数据获得了检测AI生成图像的能力，AI取证领域需要从过拟合静态基准转向利用基础模型的世界知识实现真实世界可靠性

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [79] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: TAPTQ：针对3D几何学习的尾部感知后训练量化方法，通过渐进式校准、三元搜索优化和尾部误差引导的模块补偿，在保持精度的同时显著降低校准时间。


<details>
  <summary>Details</summary>
Motivation: 3D几何模型的复杂性和规模给资源受限平台部署带来挑战。传统的后训练量化方法主要针对2D视觉Transformer设计，无法有效迁移到3D模型，因为3D模型具有复杂的特征分布和过高的校准开销。

Method: 1. 渐进式粗到细校准构造策略：针对3D数据集的数据规模瓶颈，构建高度紧凑的子集以实现统计纯度和几何代表性。2. 三元搜索求解器：将量化区间搜索重新表述为优化问题，将计算复杂度从O(N)降低到O(log N)。3. TRE引导的模块补偿：使用尾部相对误差(TRE)度量自适应识别和修正对长尾激活异常值敏感的模块中的失真。

Result: 在VGGT和Pi3基准测试上的广泛实验表明，TAPTQ在精度上始终优于最先进的后训练量化方法，同时显著减少了校准时间。

Conclusion: TAPTQ是针对3D几何学习量身定制的有效后训练量化解决方案，解决了传统方法在3D模型上的局限性，为资源受限平台上的3D模型部署提供了高效方案。

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [80] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: ObjEmbed是一个新颖的多模态大语言模型嵌入模型，通过将图像分解为多个区域嵌入（每个对应一个对象）和全局嵌入，实现了细粒度的图像-文本对齐，支持视觉定位、局部图像检索和全局图像检索等多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态嵌入模型在全局图像-文本对齐方面表现出色，但在图像区域与特定短语之间的细粒度对齐方面存在困难。为了解决这一基础性挑战和实际需求，需要开发能够同时处理区域级和图像级任务的模型。

Method: ObjEmbed将输入图像分解为多个区域嵌入（每个对应一个对象）和全局嵌入。对于每个区域，生成两个互补的嵌入：用于语义匹配的对象嵌入和预测定位质量的IoU嵌入。最终的对象匹配分数结合了语义相似度和预测的IoU，实现更准确的检索。所有对象和完整图像都在单次前向传播中编码，效率高。

Result: 在18个不同的基准测试中表现出优越性能，展示了强大的语义区分能力。模型能够同时处理区域级和图像级任务，支持视觉定位、局部图像检索和全局图像检索等多种视觉理解任务。

Conclusion: ObjEmbed通过对象导向的表示方法，结合语义和空间信息，实现了细粒度的图像-文本对齐，在多种视觉理解任务中表现出色，同时保持了高效的编码能力。

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [81] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

TL;DR: NGFF是一个端到端神经框架，结合3D高斯感知与物理动力学建模，从多视角RGB输入生成交互式、物理真实的4D视频，速度比现有高斯模拟器快两个数量级。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型虽然视觉质量高，但缺乏物理定律建模，无法一致生成物理合理的视频；而结合3D高斯溅射和物理引擎的方法计算成本高，在复杂现实场景中缺乏鲁棒性。

Method: 提出神经高斯力场(NGFF)框架，集成3D高斯感知与基于物理的动态建模；同时创建GSCollision数据集，包含多样材料、多物体交互和复杂场景的64万+渲染物理视频。

Result: 在合成和真实3D场景评估中，NGFF展现出强大的泛化能力和物理推理鲁棒性，速度比先前高斯模拟器快两个数量级，推动视频预测向物理基础世界模型发展。

Conclusion: NGFF通过整合3D感知与物理建模，实现了高效、鲁棒的物理真实4D视频生成，为构建物理基础的世界模型提供了重要进展。

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [82] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

TL;DR: 论文提出了一种基于距离感知匹配和自适应边界框分割的智能停车位级监测系统，在资源受限的边缘设备上实现了98.80%的平衡准确率和8秒推理时间，并引入了数字孪生雏形和基于电视盒的应用支持服务器。


<details>
  <summary>Details</summary>
Motivation: 现有基于区域车辆计数的停车监测系统虽然准确率不错，但无法提供车位级洞察和支持更高级应用，限制了系统的实用价值。

Method: 1. 采用基于空间容差的距离感知匹配方法实现车位级监测；2. 针对挑战性空间提出自适应边界框分割方法；3. 使用YOLOv11m模型（40.5MB大小）；4. 引入数字孪生雏形和基于电视盒的应用支持服务器。

Result: 在资源受限的边缘设备上实现了98.80%的平衡准确率，推理时间为8秒，同时系统支持车位级监测和更高级应用。

Conclusion: 提出的车位级监测系统显著提升了智能停车系统的能力，通过硬件重用促进了可持续性，并为未来完整的数字孪生系统奠定了基础。

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [83] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

TL;DR: 该研究系统评估了病理学基础模型在回归任务中的表现，特别是在预测同源重组缺陷（HRD）评分这一关键生物标志物方面，发现基础模型特征优于对比学习基线，并提出分布上采样策略改善数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 尽管在大规模病理数据上预训练的基础模型在计算病理学多个领域取得了成功，但其在回归性生物标志物预测方面的影响尚未充分探索。本研究旨在系统评估这些基础模型在回归任务中的表现，特别是针对HRD评分这一对个性化癌症治疗至关重要的生物标志物。

Method: 研究采用多实例学习框架，使用五种最先进的基础模型从全切片图像中提取斑块级特征，并与基于对比学习的特征进行比较。模型在来自两个公共医学数据集的乳腺癌、子宫内膜癌和肺癌队列中训练，以预测连续的HRD评分。此外，提出了基于分布的上采样策略来缓解数据集中的目标不平衡问题，并通过消融研究调查了不同采样策略和实例包大小的影响。

Result: 实验结果表明，基于基础模型特征训练的模型在预测准确性和泛化能力方面持续优于基线模型，同时不同基础模型之间存在系统性差异。提出的分布上采样策略显著提高了代表性不足但临床重要的患者群体的召回率和平衡准确率。消融研究揭示了不同采样策略和实例包大小对模型性能的影响。

Conclusion: 该研究强调了大规模病理学预训练对于更精确和可迁移的回归性生物标志物预测的益处，展示了其在推进AI驱动的精准肿瘤学方面的潜力。基础模型特征在回归任务中表现出优越性能，提出的上采样策略有效解决了临床数据集中的不平衡问题。

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [84] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

TL;DR: MagicFuse：一种从单张低质量可见光图像生成跨光谱场景表示的单图像融合框架，通过扩散模型挖掘可见光谱中被遮挡的信息并学习红外光谱的热辐射分布模式


<details>
  <summary>Details</summary>
Motivation: 解决在恶劣条件下只有可见光成像传感器可用时，如何继续受益于多模态图像融合优势的实际问题。传统数据级融合需要多模态输入，但在某些场景下无法获取红外等模态数据。

Method: 提出单图像融合概念，将数据级融合扩展到知识级。基于扩散模型设计三个分支：1) 光谱内知识增强分支挖掘可见光谱中被遮挡的场景信息；2) 跨光谱知识生成分支学习转移到红外光谱的热辐射分布模式；3) 多域知识融合分支整合两个分支的概率噪声，通过连续采样获得跨光谱场景表示。同时施加视觉和语义约束确保表示满足人类观察并支持下游语义决策。

Result: 大量实验表明，MagicFuse仅依赖单张退化的可见光图像，就能实现与最先进的多模态输入融合方法相当甚至更好的视觉和语义表示性能。

Conclusion: MagicFuse成功实现了从单张可见光图像到跨光谱场景表示的转换，为在只有可见光传感器可用的恶劣条件下保持多模态融合优势提供了有效解决方案，将数据级融合扩展到知识级融合的新范式。

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [85] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

TL;DR: 提出轻量级压缩域跟踪模型，直接在视频流上操作，无需完整RGB解码，利用压缩数据中的运动向量和变换系数，在保持性能的同时实现3.7倍计算加速


<details>
  <summary>Details</summary>
Motivation: 针对大规模监控系统中的实时分析需求，传统RGB视频解码计算开销大，需要更高效的跟踪方法。压缩域数据包含丰富的运动信息，可直接利用这些信息进行目标跟踪，避免完整解码的开销。

Method: 使用压缩数据中的运动向量和变换系数，构建深度学习模型，直接在压缩域传播目标边界框。模型轻量级设计，无需完整RGB视频解码，直接从视频流中提取特征进行跟踪。

Result: 在MOTS15/17/20数据集上，相比RGB基线仅下降4% mAP@0.5，同时实现最高3.7倍的计算加速。证明了压缩域运动建模在实时分析中的高效性。

Conclusion: 压缩域跟踪模型为大规模监控系统提供了高效的实时分析解决方案，通过直接利用压缩数据中的运动信息，在保持跟踪精度的同时显著降低计算开销。

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [86] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

TL;DR: 基于MEMS-LiDAR的隐私合规人员检测方法，通过合成数据增强减少标注工作量50%，提升检测精度44个百分点


<details>
  <summary>Details</summary>
Motivation: 工业室内安全空间需要可靠检测未授权人员，传统视觉方法存在隐私合规问题且数据标注耗时费力

Method: 使用MEMS-LiDAR采集匿名3D点云数据，结合CARLA仿真框架生成合成场景数据增强真实数据

Result: 混合数据相比纯真实数据训练模型平均精度提升44个百分点，同时减少50%的人工标注工作量

Conclusion: 该方法提供了可扩展、成本效益高的隐私合规人员检测方案，在工业环境中实现高性能与GDPR合规性的平衡

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [87] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: DDP-WM是一种新型世界模型，通过解耦动态预测原理，将潜在状态演化分解为稀疏的主要动态和次要背景更新，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于密集Transformer的世界模型计算开销大，严重阻碍实时部署，需要解决效率与性能之间的瓶颈问题。

Method: 提出DDP-WM模型，采用解耦动态预测原理，通过高效历史处理与动态定位结合来隔离主要动态，使用交叉注意力机制进行背景更新，优化资源分配。

Result: 在导航、桌面精确操作、复杂可变形或多体交互等任务中表现优异，在Push-T任务上实现约9倍推理加速，MPC成功率从90%提升至98%。

Conclusion: DDP-WM为开发高效、高保真世界模型提供了有前景的路径，显著提升了自主机器人规划的实际部署能力。

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [88] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 提出了一种基于单次滤波、循环方向变换和层次聚类的自动不连续面表征方法，用于地下矿山岩体稳定性评估


<details>
  <summary>Details</summary>
Motivation: 地下矿山岩体不连续面的自动表征对于评估岩体稳定性、开挖安全和运营效率至关重要。虽然无人机和移动激光扫描技术能高效采集点云数据，但在完全封闭的岩体表面等实际场景中，开发鲁棒且高效的自动表征方法仍是一个开放的研究问题。

Method: 提出了一种新的自动不连续面表征方法，包含三个核心步骤：1) 单次滤波策略，使用信号处理技术一次性隔离平面区域并抑制噪声和高曲率伪影；2) 创新的循环方向变换方案，解决笛卡尔聚类对极坐标方向数据的局限性，实现倾角和倾向在笛卡尔空间中的准确表示；3) 层次聚类技术，处理变化的密度分布并识别聚类，无需用户定义聚类数量。

Result: 该方法在实际矿山采场数据上验证，与使用Virtual Compass工具手动选取的不连续面以及广泛使用的自动结构映射技术进行对比。所提方法表现出最低的平均绝对误差：倾角误差1.95°，倾向误差2.20°，离散误差低于3°，优于其他技术。

Conclusion: 提出的方法通过单次滤波、循环方向变换和层次聚类的组合，实现了对地下矿山岩体不连续面的高效、准确自动表征，为岩体稳定性评估提供了可靠的技术支持。

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


### [89] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 本文提出了一种针对自回归视频扩散模型的训练免费注意力框架，通过三种技术减少计算和内存开销：TempCache压缩KV缓存，AnnCA加速交叉注意力，AnnSA稀疏化自注意力，实现5-10倍加速同时保持视觉质量稳定。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在推理时面临核心瓶颈：随着生成过程进行，KV缓存不断增长，导致延迟增加和GPU内存占用上升，这限制了可用时间上下文并损害长程一致性。需要解决注意力机制中的冗余问题。

Method: 提出统一的训练免费注意力框架：1) TempCache通过时间对应性压缩KV缓存以限制缓存增长；2) AnnCA使用快速近似最近邻匹配选择帧相关提示词来加速交叉注意力；3) AnnSA通过限制每个查询到语义匹配的键来稀疏化自注意力，同样使用轻量级ANN。

Result: 实验表明，该方法实现5-10倍端到端加速，同时保持近乎相同的视觉质量。更重要的是，在长序列生成中保持稳定的吞吐量和近乎恒定的峰值GPU内存使用，而先前方法会逐渐变慢且内存使用不断增加。

Conclusion: 提出的注意力框架有效解决了自回归视频扩散模型中的计算和内存瓶颈，通过识别和利用三种冗余来源，实现了高效的视频生成，为长格式合成、视频世界模型和交互式神经游戏引擎提供了实用解决方案。

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [90] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

TL;DR: LDRNet：一种用于胸部CT大变形图像配准的快速无监督深度学习方法，通过粗到精的配准场优化，在速度和精度上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习医学图像配准方法主要针对脑部图像，而胸部CT配准面临更大变形、更复杂背景和区域重叠等挑战，需要专门的大变形配准方法。

Method: 提出LDRNet框架：1）先预测粗分辨率配准场，然后从粗到精逐步细化；2）引入细化块在不同分辨率下优化配准场；3）使用刚性块从高层特征学习变换矩阵。

Result: 在私有数据集和公共数据集SegTHOR上评估，与VoxelMorph、RCN、LapIRN等先进方法相比，LDRNet在大变形图像配准上达到最优性能，且速度更快。

Conclusion: LDRNet能够有效处理胸部CT的大变形配准问题，在精度和速度方面都优于现有方法，为复杂医学图像配准提供了有效的深度学习解决方案。

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [91] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

TL;DR: 提出GPD框架，通过渐进蒸馏加速视频扩散模型采样，将采样步数从48减少到6，同时保持视觉质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成中取得了显著成功，但去噪过程的高计算成本仍然是主要瓶颈。现有方法在减少扩散步数方面有潜力，但在视频生成中应用时往往会出现明显的质量下降。

Method: 提出引导渐进蒸馏(GPD)框架，包含两个关键组件：1)在线生成训练目标，降低优化难度并提高计算效率；2)潜在空间中的频域约束，促进细粒度细节和时间动态的保持。教师模型逐步指导学生模型以更大的步长操作。

Result: 应用于Wan2.1模型时，GPD将采样步数从48减少到6，同时在VBench上保持有竞争力的视觉质量。与现有蒸馏方法相比，GPD在流程简单性和质量保持方面都显示出明显优势。

Conclusion: GPD框架为快速高质量视频生成提供了一种有效的扩散过程加速方法，在显著减少计算成本的同时保持了生成质量。

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [92] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出噪声-频率延续框架，通过构建中间后验分布家族，在噪声依赖的频率带内实施测量一致性，解决了扩散后验采样中细节恢复不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散后验采样方法在结合预训练扩散先验和测量一致性指导时，由于测量项与扩散噪声水平的弱耦合，常常无法恢复精细细节。在高噪声水平下，从不准确估计计算的数据一致性梯度可能与后验几何结构不匹配，导致早期漂移、伪高频伪影以及对调度和病态算子的敏感性。

Method: 提出噪声-频率延续框架，构建连续的中间后验分布家族，其似然函数仅在噪声依赖的频率带内强制执行测量一致性。实现了一个稳定的后验采样器，结合扩散预测器、带限似然指导和多分辨率一致性策略，积极采用可靠的粗尺度修正，同时保守地仅在可识别时采用高频细节。

Result: 在超分辨率、修复和去模糊等任务中，该方法实现了最先进的性能，在运动去模糊任务上将PSNR提高了高达5 dB，优于强基线方法。

Conclusion: 噪声-频率延续框架通过将测量一致性限制在适当的频率带内，有效解决了扩散后验采样中的细节恢复问题，显著提升了逆问题求解的性能和稳定性。

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [93] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

TL;DR: VIA-Bench是一个评估多模态大语言模型在视觉错觉和异常场景下鲁棒性的基准测试，包含6类视觉错觉，超过1000个高质量问答对，测试发现现有模型存在显著脆弱性，链式思维推理对鲁棒性提升有限。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在标准分布数据上表现优异，甚至达到人类水平，但其在面对违背常识先验的视觉错觉和异常场景时的鲁棒性尚未得到充分检验。需要构建专门基准来评估模型在这些挑战性场景下的表现。

Method: 构建VIA-Bench基准，包含6个核心类别：颜色错觉、运动错觉、格式塔错觉、几何与空间错觉、一般视觉错觉和视觉异常。通过人工循环审查构建超过1000个高质量问答对，要求细致的视觉推理。评估了超过20个最先进的多模态大语言模型，包括专有模型、开源模型和推理增强模型。

Result: 评估发现现有多模态大语言模型存在显著脆弱性。链式思维推理对鲁棒性提升有限，常常产生"脆弱幻象"——模型逻辑在错觉刺激下崩溃。研究揭示了机器感知与人类感知之间的根本差异。

Conclusion: 解决这种感知瓶颈对于人工智能通用智能的发展至关重要。VIA-Bench基准的发布将促进多模态大语言模型在视觉错觉和异常场景下鲁棒性的研究。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [94] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: CamReasoner框架将相机运动理解重构为结构化推理过程，采用观察-思考-回答范式，通过强化学习实现几何逻辑对齐，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型将相机动态理解视为黑盒分类任务，经常混淆物理上不同的运动，依赖表面视觉模式而非几何线索。需要弥合感知与电影逻辑之间的差距。

Method: 提出CamReasoner框架，采用观察-思考-回答范式，通过解码轨迹和视锥等时空线索进行显式推理。构建大规模推理轨迹套件（18k SFT推理链和38k RL反馈样本），首次在该领域使用强化学习进行逻辑对齐。

Result: CamReasoner有效抑制幻觉，在多个基准测试中实现最先进的性能，确保运动推断基于物理几何而非上下文猜测。

Conclusion: 将相机运动理解重构为结构化推理过程，结合强化学习的观察-思考-回答范式，能够更好地理解相机动态，为视频空间智能提供更可靠的几何基础。

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [95] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出基于街景图像引导的数据采集策略，利用公开街景图像识别兴趣点，减少跨国家数据收集成本，在交通标志检测任务上达到与随机采样相当性能但仅需一半目标域数据


<details>
  <summary>Details</summary>
Motivation: ADAS和ADS在不同国家部署面临挑战，由于法规、交通基础设施和视觉惯例差异导致域偏移，传统跨国家数据收集依赖大量实地驾驶，成本高且效率低

Method: 提出街景引导的数据采集策略，利用公开街景图像识别兴趣点；引入两种POI评分方法：基于视觉基础模型的KNN特征距离方法和基于视觉语言模型的视觉归因方法；采用收集-检测协议构建共定位数据集

Result: 在交通标志检测任务上，该方法达到与随机采样相当的性能，但仅使用一半目标域数据；全国家分析的成本估算显示大规模街景处理在经济上可行

Conclusion: 街景引导的数据采集策略为跨国家模型适应提供了高效且经济的方法，具有实际应用潜力

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [96] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

TL;DR: 论文发现当前基于深度学习的图像修复检测器主要依赖全局伪影而非局部合成内容，通过INP-X操作揭示VAE重建导致的频谱偏移问题，现有检测器在此干预下准确率大幅下降。


<details>
  <summary>Details</summary>
Motivation: 现代基于深度学习的图像修复技术能够实现逼真的局部图像操作，这对可靠检测提出了严峻挑战。作者观察到当前检测器主要依赖作为修复副作用的全局伪影，而不是局部合成内容。

Method: 引入Inpainting Exchange (INP-X)操作，在保留所有合成内容的同时恢复编辑区域外的原始像素；创建包含9万张真实、修复和交换图像的测试数据集；提供理论分析，将检测器行为与VAE信息瓶颈导致的高频衰减联系起来。

Result: 在此干预下，预训练的最先进检测器（包括商业检测器）准确率急剧下降（例如从91%降至55%），经常接近随机水平；在作者数据集上训练的检测器比标准修复检测具有更好的泛化能力和定位能力。

Conclusion: 研究结果强调了内容感知检测的必要性，VAE重建导致的全局频谱偏移是当前检测器的主要弱点，需要开发更关注局部合成内容的检测方法。

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [97] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

TL;DR: SPIRIT是一个统一且兼容视觉基础模型的红外小目标检测框架，通过轻量级物理信息插件解决红外数据稀缺和模态差异问题，实现单帧和多帧推理的统一。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在监视和预警中至关重要，但面临数据稀缺、红外目标信号弱且语义线索有限的问题。直接使用面向可见光语义的视觉基础模型和基于外观的跨帧关联不可靠，因为层次特征聚合会淹没局部目标峰值，仅基于外观的记忆注意力变得模糊，导致虚假杂波关联。

Method: 提出SPIRIT框架，通过轻量级物理信息插件适配视觉基础模型：1) 空间上，PIFR通过近似秩稀疏分解细化特征，抑制结构化背景成分并增强稀疏目标信号；2) 时间上，PGMA将历史衍生的软空间先验注入记忆交叉注意力，约束跨帧关联，实现鲁棒视频检测，并在缺乏时间上下文时自然回归到单帧推理。

Result: 在多个红外小目标检测基准测试中，相比基于视觉基础模型的基线和现有最优方法，SPIRIT框架取得了持续的性能提升和最优性能。

Conclusion: SPIRIT通过物理信息插件有效解决了红外小目标检测中的模态差异问题，统一了单帧和多帧推理框架，在多个基准测试中表现出优越性能。

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [98] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

TL;DR: 该论文提出了WS-IMUBench基准测试，用于评估弱监督IMU时序动作定位方法在仅有序列级标签下的性能，通过大规模实验分析了不同弱监督方法的可迁移性和有效性。


<details>
  <summary>Details</summary>
Motivation: 传统IMU动作识别只能分类剪辑片段，无法捕捉真实世界行为的丰富时序结构。虽然IMU时序动作定位能预测连续流中的动作类别和起止时间，但需要密集的帧级边界标注，成本高且难以扩展。这促使研究者转向弱监督方法，只需序列级标签。

Method: 引入WS-IMUBench基准测试，系统评估弱监督IMU时序动作定位方法。不提出新算法，而是评估音频、图像和视频领域的现有弱监督定位范式在IMU数据上的可迁移性。在7个公共IMU数据集上对7种代表性弱监督方法进行基准测试，共进行3540次模型训练和7080次推理评估。

Result: 研究发现：(1) 可迁移性依赖于模态，时域方法通常比基于图像提案的方法更稳定；(2) 在有利数据集上（如动作较长、传感维度较高），弱监督方法可以具有竞争力；(3) 主要失败模式源于短动作、时序模糊性和提案质量。还提出了改进方向。

Conclusion: WS-IMUBench建立了可复现的基准测试模板、数据集、协议和分析，为社区加速推进可扩展的弱监督IMU时序动作定位提供了基础。研究揭示了不同弱监督方法的性能特征，并指出了IMU特定提案生成、边界感知目标和更强时序推理等具体改进方向。

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [99] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

TL;DR: 提出了一种基于多跳视觉推理链的无监督医学图像配准框架，通过渐进式推理过程实现高精度且可解释的配准，特别适用于大变形情况。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在无监督医学图像配准中虽然精度较高，但缺乏透明度和可解释性，导致误差漂移和临床信任度降低。临床决策的迭代特性启发了本研究的思路。

Method: 提出多跳视觉推理链框架，将配准重新定义为渐进推理过程。每个视觉推理跳包含局部空间细化模块（丰富特征表示）和交叉参考注意力机制（引导迭代细化过程，保持解剖一致性）。多跳策略能够处理大变形，并产生具有理论界限的中间预测序列。

Result: 在DIR-Lab 4D CT（肺部）和IXI T1加权MRI（脑部）两个挑战性公开数据集上的广泛评估表明，VCoR实现了具有竞争力的配准精度，同时提供了丰富的中间可视化和置信度度量。

Conclusion: 通过嵌入隐式视觉推理范式，提出了一个可解释、可靠且临床可行的无监督医学图像配准方法，不仅提高了准确性，还通过变形场在多个跳之间的稳定性和收敛性来估计不确定性，提供内置的可解释性。

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [100] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: VIBE是一个视觉指令图像编辑基准，包含三个层次的任务复杂性，用于评估模型在多模态视觉指令（如草图）下的图像编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑系统主要基于文本指导，而人类交流本质上是多模态的，视觉指令（如草图）能更有效地传达空间和结构意图，因此需要专门的视觉指令基准来填补这一空白。

Method: 引入VIBE基准，包含三个层次的交互：指示性定位、形态操作和因果推理。构建高质量多样化的测试用例，并提出了基于LMM-as-a-judge的评估框架，使用任务特定指标进行细粒度评估。

Result: 评估了17个开源和专有图像编辑模型，发现专有模型展现出早期视觉指令跟随能力，始终优于开源模型。但随着任务难度增加，即使是表现最好的系统性能也显著下降。

Conclusion: 视觉指令跟随在图像编辑中具有重要价值，现有模型在简单任务上表现尚可，但在复杂任务上仍有很大提升空间，这为未来研究指明了方向。

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [101] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

TL;DR: 本文提出了一种名为MFM-Geom的几何多模态基础模型，用于前列腺癌诊断。该模型结合双参数MRI和临床报告，利用对称正定矩阵和黎曼深度学习来整合影像-文本表示，在数据稀缺情况下仍能取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌是全球男性常见癌症，双参数MRI和临床变量对诊断至关重要，但当前诊断过程依赖专家主观判断。现有计算机辅助诊断方法主要关注影像模型，忽略了临床背景且受限于数据稀缺，难以学习稳健表示。

Method: 提出MFM-Geom几何多模态基础模型，从双参数MRI和临床报告中学习表示，编码视觉发现和临床变量信息。在表示分类头中，采用对称正定矩阵和黎曼深度学习来整合生物医学多模态基础模型的影像-文本表示。

Result: 仅使用10%训练数据时，MFM-Geom优于基于类别标记嵌入的基线分类方法（提升8.3%，AUC-PR达到90.67）。在外部数据集上的泛化测试证实了微调生物医学基础模型的稳健性，AUC-PR达到90.6。

Conclusion: MFM-Geom模型通过整合多模态数据和采用几何表示学习方法，在数据稀缺情况下仍能实现稳健的前列腺癌诊断，为计算机辅助诊断提供了新思路。

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [102] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

TL;DR: 研究发现像素级深度伪造检测器在多模态虚假信息检测中价值有限，将其纳入事实核查流程反而会降低性能，而基于证据的事实核查系统表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息中的欺骗通常来自图像-文本对共同表达的语义和上下文主张，而非仅仅是像素级篡改。然而大多数深度伪造检测器只检测像素级伪造，不考虑主张层面的含义，这引发了科学和实践问题：像素级检测器是否对验证图像-文本主张有用，还是会引入误导的真实性先验破坏基于证据的推理？

Method: 使用MMFakeBench和DGM4两个互补基准进行评估：(1)最先进的仅图像深度伪造检测器；(2)基于证据的事实核查系统，通过蒙特卡洛树搜索进行工具引导检索，并通过多智能体辩论进行审慎推理；(3)将检测器输出作为辅助证据注入的混合事实核查系统。

Result: 深度伪造检测器独立价值有限，在MMFakeBench上F1分数为0.26-0.53，在DGM4上为0.33-0.49。将其预测纳入事实核查流程会因非因果真实性假设使性能降低0.04-0.08 F1。相比之下，基于证据的事实核查系统达到最高性能，在MMFakeBench上F1约0.81，在DGM4上约0.55。

Conclusion: 多模态主张验证主要由语义理解和外部证据驱动，像素级伪影信号不能可靠地增强对真实世界图像-文本虚假信息的推理。基于证据的事实核查方法优于依赖像素级检测器的混合方法。

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [103] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

TL;DR: 本文提出Lazy Attention机制，通过跨层共享相似注意力模式来减少多模态大语言模型中的冗余计算，降低KV缓存占用并提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型因视觉编码器产生大量视觉token而导致推理成本高昂，现有token剪枝方法会破坏KV缓存完整性，影响长文本生成任务。

Method: 提出Lazy Attention机制，发现超过一半解码层的注意力语义相似，允许某些层继承前层注意力模式。开发了轻量级Q Cache，支持跨层查询重用，兼容现有推理框架。

Result: 在多个基准测试中，该方法能减少超过35%的KV缓存使用，实现1.5倍吞吐量提升，仅牺牲约1%的性能，相比现有SOTA token剪枝方法具有更好的精度保持。

Conclusion: Lazy Attention通过跨层共享相似注意力模式，有效减少了MLLMs中的冗余计算，在保持性能的同时显著提升了推理效率，且与现有token剪枝技术正交兼容。

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [104] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

TL;DR: PLACID是一个利用预训练图像到视频扩散模型进行多物体合成的框架，通过视频时间先验保持物体一致性和背景细节，使用合成数据训练，能生成视觉吸引人的多物体组合图像。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在照片级图像合成方面取得进展，但在工作室级别的多物体合成任务中存在不足：无法同时保持物体身份、背景色彩保真度、布局设计控制以及完整展示所有物体。现有模型常改变物体细节、遗漏或重复物体、产生错误相对尺寸或不一致展示。

Method: 1. 利用预训练图像到视频扩散模型，通过文本控制和时间先验保持物体一致性和背景细节；2. 提出新颖的数据策展策略，生成合成序列，其中随机放置的物体平滑移动到目标位置，这些数据在训练时与视频模型的时间先验对齐。

Result: 广泛的定量评估和用户研究表明，PLACID在多物体合成方面超越了现有最先进方法，实现了更好的身份、背景和色彩保持，减少了物体遗漏，产生了视觉上更吸引人的结果。

Conclusion: PLACID框架成功解决了多物体合成的关键挑战，通过利用视频时间先验和合成数据训练，能够生成高质量、一致且视觉吸引人的多物体组合图像，为工作室级图像合成提供了有效解决方案。

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [105] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

TL;DR: 提出一种推理时方法，通过识别和移除不稳定的潜在标记来缓解自回归视频生成中的时间漂移问题，无需修改模型架构或训练过程。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成在生成长视频时存在严重的时间漂移问题，误差会随时间累积和放大。作者认为这主要源于推理时的误差传播，而非模型容量不足，具体是由于在自回归推理中重复使用了已损坏的潜在条件标记。

Method: 提出一种简单的推理时方法，通过识别和移除不稳定的潜在标记来缓解时间漂移。将不稳定标记定义为那些与先前生成批次表示显著偏离的潜在标记，表明它们可能已损坏或发生语义漂移。通过从自回归上下文中显式移除这些损坏的潜在标记，防止不可靠的潜在信息影响未来的生成步骤。

Result: 该方法显著改善了长时域的时间一致性，且无需修改模型架构、训练过程或离开潜在空间。

Conclusion: 通过推理时识别和移除不稳定的潜在标记，可以有效缓解自回归视频生成中的时间漂移问题，提高长视频生成的时间一致性，同时保持模型架构和训练过程不变。

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [106] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

TL;DR: DSXFormer：一种用于高光谱图像分类的新型双池化光谱挤压-扩展变换器，通过动态上下文注意力机制平衡光谱区分性和计算效率


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临光谱维度高、光谱-空间相关性复杂、标记训练样本有限等挑战。现有基于变换器的方法难以在保持计算效率的同时实现足够的光谱区分性。

Method: 提出DSXFormer，包含双池化光谱挤压-扩展块（利用全局平均和最大池化自适应重新校准光谱特征通道）和动态上下文注意力机制（在基于窗口的变换器架构中动态捕获局部光谱-空间关系）。结合补丁提取、嵌入和合并策略实现多尺度特征学习。

Result: 在四个高光谱基准数据集（Salinas、Indian Pines、Pavia University、Kennedy Space Center）上分别达到99.95%、98.91%、99.85%、98.52%的分类准确率，优于现有最先进方法。

Conclusion: DSXFormer通过光谱双池化挤压-扩展和动态上下文注意力的联合集成，在光谱强调和空间上下文表示之间实现了有效平衡，为高光谱图像分类提供了高效且性能优越的解决方案。

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [107] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: RS-MPOD提出了一种多模态开放词汇检测框架，通过视觉提示和文本提示的结合来解决遥感场景中仅依赖文本提示的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 遥感场景中的开放词汇目标检测通常仅依赖文本提示来指定目标类别，但这种方法在实际应用中经常失效，因为遥感场景具有任务和应用特定的类别语义，导致在开放词汇设置下类别指定不稳定。

Method: 提出了RS-MPOD多模态开放词汇检测框架，通过三种方式重新定义类别指定：1）基于实例的视觉提示，2）文本提示，3）多模态融合。框架包含视觉提示编码器提取基于外观的类别线索，以及多模态融合模块整合视觉和文本信息。

Result: 在标准、跨数据集和细粒度遥感基准测试中，视觉提示在语义模糊和分布偏移下提供更可靠的类别指定，而多模态提示在文本语义对齐良好时保持竞争力。

Conclusion: 视觉提示能够更可靠地指定类别，特别是在语义模糊和分布偏移的情况下，而多模态提示提供了灵活的替代方案，在文本语义对齐良好时仍然有效。

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [108] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

TL;DR: TimeBlind是一个诊断基准，用于评估多模态大语言模型在细粒度时空理解上的能力，通过最小对范式揭示模型主要依赖静态视觉线索而非真实时间逻辑。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在静态语义理解上表现出色，但对时间动态的理解仍然薄弱，需要专门的诊断工具来评估其细粒度时空理解能力。

Method: 采用认知科学启发的方法，将细粒度时间理解分为三个层次：原子事件识别、事件属性表征和事件互依关系推理。使用最小对范式：视频对共享相同的静态视觉内容，仅时间结构不同，通过互补问题来消除语言先验。

Result: 评估了20多个最先进的多模态大语言模型（包括GPT-5、Gemini 3 Pro）在600个精心设计的实例（2400个视频-问题对）上。最佳模型的实例准确率仅为48.2%，远低于人类表现的98.2%。

Conclusion: 前沿模型严重依赖静态视觉捷径而非真实的时间逻辑推理，TimeBlind作为重要的诊断工具，为下一代视频理解模型的发展提供了关键评估基准。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [109] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

TL;DR: LogicGaze是一个评估视觉语言模型在顺序推理中是否基于真实视觉证据的基准框架，通过因果验证、叙事合成和扰动拒绝三个测试，揭示了当前SOTA模型在视觉基础推理方面的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 虽然顺序推理增强了视觉语言模型执行复杂多模态任务的能力，但它们在将推理链基于实际视觉证据方面的可靠性尚未得到充分探索。当前模型存在幻觉问题，即生成看似合理但缺乏视觉支持的推理链。

Method: LogicGaze是一个新颖的基准框架，从ShareGPT4Video的40,000个视频片段和Flickr30k图像子集中构建数据集。该框架整合了因果序列与视觉矛盾但语言合理的扰动，迫使模型验证每个推理步骤的真实性。采用三部分评估协议：因果验证、基于视觉的叙事合成和扰动拒绝。

Result: 评估揭示了包括Qwen2.5-VL-72B在内的最先进视觉语言模型存在显著脆弱性，表明它们在将顺序推理链基于实际视觉证据方面存在严重缺陷。

Conclusion: LogicGaze倡导构建强大、可信赖的多模态推理系统，所有资源已在匿名存储库中公开提供，为评估和改进视觉语言模型的视觉基础推理能力提供了重要工具。

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [110] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

TL;DR: 提出一种通过缩放分隔符标记的隐藏状态来增强多图像理解能力的方法，无需额外训练或推理成本，有效减少跨图像信息泄漏


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在单图像任务上表现良好，但在处理多图像输入时性能下降，主要原因是跨图像信息泄漏问题。现有模型虽然使用分隔符标记来区分不同图像，但这些标记未能有效阻止跨图像信息泄漏。

Method: 提出一种缩放分隔符标记隐藏状态的方法，通过增强分隔符标记的作用来强化图像内部交互，同时限制不必要的跨图像交互，从而更好地保持图像特定信息。

Result: 在Mantis、MuirBench、MIRB、QBench2等多图像基准测试中取得性能提升，同时在TQABench、MultiNews、WCEP-10等多文档和多表格理解任务上也表现出改进。

Conclusion: 通过简单缩放分隔符标记的隐藏状态，可以有效增强模型在多图像和多文档任务中的区分能力，且无需额外训练或推理成本，为解决跨模态信息泄漏问题提供了有效方案。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [111] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

TL;DR: 提出了一种新的扩散模型训练框架，能够在用户定义的图像区域实现精确的局部控制，同时让扩散模型根据原始提示自主生成其余区域。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在文本到图像生成方面表现出色，但仅通过文本实现详细控制仍然是一个繁琐的试错过程。现有方法虽然引入了图像级控制（如边缘、分割和深度图），但这些控制在整个图像上均匀应用，限制了局部控制能力。

Method: 提出了一种新的训练框架，包含掩码特征和额外的损失项。该损失项利用在任何扩散步骤中对初始潜在向量的预测，来增强当前步骤与潜在空间中最终样本之间的对应关系。

Result: 大量实验证明，该方法能够有效地合成具有受控局部条件的高质量图像。

Conclusion: 该方法为扩散模型提供了精确的局部控制能力，解决了现有方法在整个图像上均匀应用条件的局限性，实现了用户定义区域的精确控制与模型自主生成的平衡。

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [112] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

TL;DR: SAM2CT：首个可提示分割模型，可将放射科医师的稀疏标注（箭头和线条）转换为CT体积的3D分割，利用历史PACS中的GSPS标注大规模生成3D分割数据集。


<details>
  <summary>Details</summary>
Motivation: CT成像的机器学习模型开发需要大规模高质量标注数据集，但3D分割标注成本高昂。临床PACS中已有大量CT图像和报告，放射科医师在常规读片中会提供有限的标注（如线条测量和箭头），这些稀疏标注可作为生成3D分割的宝贵资源。

Method: 提出SAM2CT模型，基于SAM2扩展提示编码器以支持箭头和线条输入，并引入Memory-Conditioned Memories（MCM）内存编码策略，专门针对3D医学体积设计。该模型可将放射科医师的稀疏标注转换为3D分割。

Result: 在公共病灶分割基准测试中，SAM2CT优于现有可提示分割模型和类似训练的基线，箭头提示的Dice相似系数达0.649，线条提示达0.757。在临床PACS的GSPS标注上，87%的生成分割被放射科医师评为临床可接受或仅需微小调整。

Conclusion: SAM2CT证明了利用历史GSPS标注进行大规模挖掘是生成3D CT分割数据集的有前景且可扩展的方法，为医学影像分析提供了新的数据获取途径。

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [113] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: SurfSplat是一个基于2D高斯泼溅的feedforward框架，通过表面连续性先验和强制alpha混合策略，从稀疏图像重建高质量3D场景，解决了现有方法在近距离观察时出现离散、颜色偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的方法在从稀疏图像重建3D场景时，往往产生离散的、颜色偏差的点云，在正常分辨率下看似合理，但在近距离观察时会出现严重伪影，无法重建连续表面。

Method: 提出基于2D高斯泼溅的SurfSplat框架，相比3DGS具有更强的各向异性和更高的几何精度。通过引入表面连续性先验和强制alpha混合策略，重建连贯的几何结构和忠实纹理。还提出了高分辨率渲染一致性（HRRC）评估指标。

Result: 在RealEstate10K、DL3DV和ScanNet数据集上的大量实验表明，SurfSplat在标准指标和新提出的HRRC指标上都持续优于先前方法，为稀疏输入的高保真3D重建提供了稳健解决方案。

Conclusion: SurfSplat通过2D高斯泼溅、表面连续性先验和强制alpha混合策略，有效解决了稀疏图像3D重建中的表面不连续和颜色偏差问题，实现了高质量的场景重建。

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [114] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

TL;DR: UniDriveDreamer：单阶段统一多模态自动驾驶世界模型，直接生成多模态未来观测，无需中间表示或级联模块


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶世界模型主要专注于单模态生成（多摄像头视频或LiDAR序列），缺乏统一的多模态生成方法。需要一种能够直接生成多模态未来观测的模型，避免依赖中间表示或级联模块带来的复杂性。

Method: 1. 使用LiDAR专用VAE编码LiDAR序列，视频VAE编码多摄像头图像；2. 提出统一潜在锚定（ULA）方法，显式对齐两种模态的潜在分布；3. 对齐特征融合后由扩散变换器处理，联合建模几何对应和时间演化；4. 结构化场景布局信息作为条件信号指导合成。

Result: UniDriveDreamer在视频和LiDAR生成方面均优于先前最先进方法，同时在下游任务中带来可测量的改进。

Conclusion: 该研究提出了首个单阶段统一多模态自动驾驶世界模型，通过显式的跨模态对齐和联合建模，实现了高质量的多模态未来观测生成，为自动驾驶系统提供了更全面的环境理解能力。

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [115] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

TL;DR: 本文提出ClueTracer方法，通过追踪推理路径中的关键线索传播来抑制多模态推理模型的幻觉问题，无需额外训练即可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态推理模型在长链推理过程中容易出现幻觉，生成与输入图像或问题无关的内容。研究发现这是由于"推理漂移"现象导致的，模型在收集视觉线索时过度关注与问题无关的实体，逐渐使推理轨迹脱离视觉基础。

Method: 提出ClueRecall评估指标来衡量视觉线索检索能力，并开发ClueTracer方法。这是一个无需训练、无需参数、架构无关的插件，从问题出发追踪关键线索在推理路径中的传播（问题→输出→视觉标记），定位任务相关区域并抑制对无关区域的虚假关注。

Result: ClueTracer显著提升了所有推理架构的性能，在推理基准测试上实现了1.21倍的提升。当迁移到非推理设置时，也能获得1.14倍的增益。该方法无需额外训练即可有效抑制幻觉。

Conclusion: 通过追踪推理路径中的线索传播，可以有效抑制多模态推理模型的幻觉问题。ClueTracer作为一种轻量级插件，能够在不增加训练成本的情况下显著提升各种架构的性能，为解决推理漂移问题提供了有效方案。

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [116] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: OpticalDNA：一种基于视觉的基因组建模框架，将DNA序列视为OCR风格的文档进行理解，相比传统语言模型方法，在长序列上使用更少的token实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基因组基础模型大多采用语言模型架构，将DNA视为一维token序列。但这种顺序读取方式与基因组稀疏、不连续的语义结构不匹配，导致在低信息背景上浪费计算资源，且无法实现理解驱动的长上下文压缩。

Method: 将DNA渲染为结构化视觉布局，训练OCR能力的视觉-语言模型，包含视觉DNA编码器和文档解码器。编码器生成紧凑、可重构的视觉token实现高保真压缩。基于此表示，定义针对核心基因组原语（读取、区域定位、子序列检索、掩码跨度补全）的提示条件目标，学习布局感知的DNA表示。

Result: 在多样化基因组基准测试中，OpticalDNA始终优于近期基线；在长达45万个碱基的序列上，使用近20倍更少的有效token实现最佳整体性能，且仅训练256k可调参数就超越了激活参数多达985倍的模型。

Conclusion: 将基因组建模重新定义为OCR风格的文档理解是有效的，视觉表示能够实现理解驱动的压缩，在减少计算负担的同时保持细粒度基因组信息，为长序列基因组分析提供了高效框架。

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [117] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: Auto-Comp是一个自动化合成基准生成框架，用于评估视觉语言模型在组合推理中的缺陷，特别是颜色绑定和空间关系理解方面的失败。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型在组合推理中存在关键缺陷，经常混淆"红色立方体和蓝色球体"与"蓝色立方体和红色球体"。需要可控制的评估方法来分离视觉和语言根源的失败。

Method: 引入Auto-Comp，一个完全自动化和合成的管道，用于生成可扩展的基准。通过生成最小化描述和LLM生成的上下文描述配对的图像，进行受控的A/B测试，分离核心绑定能力与视觉语言复杂性。

Result: 在20个VLM上评估颜色绑定和空间关系的新基准，揭示了CLIP和SigLIP模型家族普遍的组合推理失败。发现模型对低熵干扰物高度敏感，且存在视觉语言上下文在帮助空间推理的同时阻碍局部属性绑定的权衡。

Conclusion: 视觉语言模型存在深层的组合推理缺陷，超越了已知的词袋限制。Auto-Comp框架为未来基准创建提供了工具，揭示了模型在组合理解方面的系统性弱点。

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [118] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

TL;DR: SegmentMIL：基于Transformer的多视图多示例学习框架，用于患者级别的冠状动脉狭窄分类，无需视图级标注，能同时预测狭窄存在并定位受影响区域


<details>
  <summary>Details</summary>
Motivation: 冠状动脉狭窄是心血管疾病的主要原因，目前基于单视图的深度学习模型需要昂贵的视图级标注，且无法捕捉多视图间的时序动态和依赖关系，这些在临床诊断中至关重要

Method: 提出SegmentMIL，一个基于Transformer的多视图多示例学习框架，使用患者级监督训练，无需任何视图级标注，能联合预测狭窄存在并定位受影响解剖区域，区分左右冠状动脉及其相应节段

Result: 在内部和外部评估中均获得高性能，优于视图级模型和经典MIL基线方法

Conclusion: SegmentMIL展示了作为冠状动脉狭窄诊断的临床可行且可扩展解决方案的潜力

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [119] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

TL;DR: UrbanGS是一个针对大规模城市场景的3D高斯泼溅扩展框架，通过深度一致正则化和自适应高斯剪枝策略，解决了几何一致性、内存效率和计算可扩展性等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在有限场景中能实现高质量实时渲染，但扩展到大规模城市场景时面临几何一致性、内存效率和计算可扩展性等关键挑战，需要系统性的解决方案。

Method: 1. 深度一致D-Normal正则化模块：结合D-Normal约束与外部深度监督，通过基于梯度一致性和逆深度偏差的自适应置信度加权机制，全面更新几何参数；2. 空间自适应高斯剪枝策略：根据局部几何复杂度和可见性动态调整高斯密度；3. 统一分区和视图分配方案：消除边界伪影并优化计算负载。

Result: 在多个城市数据集上的广泛实验表明，UrbanGS在渲染质量、几何精度和内存效率方面实现了优越性能，为高保真大规模场景重建提供了系统解决方案。

Conclusion: UrbanGS通过创新的深度一致正则化和自适应剪枝策略，成功解决了3DGS在大规模城市场景中的几何一致性、内存效率和计算可扩展性挑战，为城市尺度的应用提供了有效的重建框架。

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [120] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

TL;DR: FSVideo是一个基于Transformer的快速图像到视频扩散框架，包含高度压缩的视频自编码器、带内存设计的扩散Transformer架构和多分辨率生成策略，在保持竞争力的同时实现数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 开发一个快速高效的图像到视频生成框架，在保持生成质量的同时大幅提升生成速度，解决现有模型速度较慢的问题。

Method: 1) 设计新的视频自编码器，实现64×64×4的高压缩空间-时间下采样比；2) 采用带层内存设计的扩散Transformer架构，增强层间信息流和上下文重用；3) 通过多分辨率生成策略，使用少量步骤的DIT上采样器提高视频保真度。

Result: 最终模型包含140亿参数的DIT基础模型和140亿参数的DIT上采样器，在保持与其他流行开源模型竞争力的同时，实现了数量级的速度提升。

Conclusion: FSVideo框架通过创新的视频自编码器、扩散Transformer架构和多分辨率生成策略，成功实现了快速高效的图像到视频生成，为高质量视频生成提供了新的解决方案。

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [121] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为DSKD的教师指导学生扩散自知识蒸馏方法，通过轻量级扩散模型利用教师分类器指导去噪学生特征的采样过程，并使用局部敏感哈希引导的特征蒸馏方法，消除师生特征分布差异，提升知识蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法通常通过探索有意义的特征处理和损失函数来对齐师生特征信息。但由于师生特征分布存在差异，学生模型可能从教师那里学习到不兼容的信息。为了解决这个问题，需要消除师生映射方式和特征分布之间的差异。

Method: 提出教师指导学生扩散自知识蒸馏方法（DSKD）。不直接进行师生对齐，而是利用教师分类器通过轻量级扩散模型指导去噪学生特征的采样过程。然后提出新颖的局部敏感哈希引导的特征蒸馏方法，在原始学生特征和去噪学生特征之间进行蒸馏。去噪后的学生特征封装了教师知识，可被视为教师角色。

Result: 在视觉识别任务上的实验表明，DSKD在各种模型和数据集上显著优于现有的知识蒸馏方法。

Conclusion: DSKD方法能够消除师生映射方式和特征分布之间的差异，同时从教师那里学习有意义的知识，在知识蒸馏任务中取得了显著改进。

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [122] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

TL;DR: iCCDM改进连续条件扩散模型，通过引入EDM框架和自适应邻域训练策略，提升生成质量和采样效率，在多个数据集上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有CCDM模型虽然优于先前方法，但存在两个主要问题：1) 依赖过时的扩散框架；2) 采样效率低（需要长采样轨迹）。最近还被GAN方法CcGAN-AVAR超越，因此需要改进

Method: 提出iCCDM框架，整合先进的Elucidated Diffusion Model (EDM)框架并进行重大修改，包括：1) 新颖的矩阵形式EDM公式；2) 自适应邻域训练策略

Result: 在四个基准数据集（图像分辨率从64×64到256×256）上实验表明，iCCDM一致优于现有方法，包括最先进的大规模文本到图像扩散模型（如Stable Diffusion 3、FLUX.1、Qwen-Image），在提高生成质量的同时显著降低采样成本

Conclusion: iCCDM通过整合EDM框架和自适应训练策略，有效解决了CCDM的局限性，在连续条件图像生成任务中实现了更高的质量和效率

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [123] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Text-DJ是一种针对大型视觉语言模型的新型越狱攻击，通过将有害查询分解为多个语义相关但更良性的子查询，并加入大量无关的干扰查询，以图像网格形式同时呈现，成功绕过模型的安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型通常具有强大的安全防护机制，但这些防御主要关注显式文本输入或相关视觉场景的分析。研究者发现可以利用模型的OCR能力绕过这些安全防护，揭示模型在分散的多模态输入方面的脆弱性。

Method: 方法分为三个阶段：1) 将单个有害查询分解为多个语义相关但更良性的子查询；2) 选择与有害查询最大程度无关的干扰查询集合；3) 将所有分解的子查询和干扰查询以图像网格形式同时呈现给模型，其中子查询位于网格中间位置。

Result: 该方法成功绕过了最先进的大型视觉语言模型的安全对齐机制。攻击成功的原因包括：1) 将基于文本的提示转换为图像，绕过标准文本过滤器；2) 引入干扰，使模型的安全协议无法在大量无关查询中链接分散的子查询。

Conclusion: 研究揭示了大型视觉语言模型OCR能力的关键漏洞，表明这些模型对分散的多图像对抗性输入不够鲁棒，突显了需要为碎片化的多模态输入开发防御机制的必要性。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [124] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

TL;DR: MLV-Edit是一个无需训练、基于光流的分钟级视频编辑框架，通过分治策略解决长视频编辑的计算开销和时序一致性问题


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑技术擅长短视频处理，但扩展到长视频面临巨大挑战：计算开销过高，难以在数千帧中保持全局时序一致性

Method: 采用分治策略进行分段编辑，包含两个核心模块：Velocity Blend通过对齐相邻片段的光流场来校正运动不一致性；Attention Sink将局部片段特征锚定到全局参考帧来抑制累积结构漂移

Result: 大量定量和定性实验表明，MLV-Edit在时序稳定性和语义保真度方面持续优于最先进的方法

Conclusion: MLV-Edit提供了一个无需训练、基于光流的框架，有效解决了分钟级视频编辑中的计算和一致性挑战，在长视频编辑中表现出优越性能

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [125] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

TL;DR: 提出基于AI的异常检测框架，用于毒理学研究中啮齿动物肝脏组织病理学全切片图像的自动化分析，能够识别已知病理和罕见未知病理，提高药物开发效率。


<details>
  <summary>Details</summary>
Motivation: 药物诱导毒性是临床前开发和早期临床试验失败的主要原因，组织病理学评估依赖专家病理学家，成为大规模筛选的瓶颈，需要自动化解决方案。

Method: 1. 创建像素级标注的健康组织和已知病理数据集；2. 使用DINOv2预训练Vision Transformer，通过LoRA进行微调实现组织分割；3. 提取特征并使用马氏距离进行异常检测；4. 提出类别特异性阈值优化方法，最小化假阴性和假阳性率。

Result: 仅0.16%的病理组织被误分类为健康，0.35%的健康组织被误分类为病理。在已知毒理学发现的鼠标肝脏WSIs上，框架能准确检测异常，包括罕见的OOD形态。

Conclusion: AI驱动的组织病理学分析有潜力支持临床前工作流程，减少后期失败，提高药物开发效率，为毒性评估提供自动化解决方案。

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [126] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

TL;DR: 该研究提出使用基于物理的CBCT模拟生成几何对齐的训练数据对，以解决传统监督学习中配准偏差问题，并通过几何对齐指标而非传统强度指标进行评估，在盆腔数据集上验证了该方法在临床偏好上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统监督式CBCT合成CT生成需要配准的训练对，但独立获取的扫描之间无法实现完美配准，这种配准偏差会传播到训练模型中并污染标准评估指标，导致基准性能可能反映的是配准伪影的复制而非解剖保真度。

Method: 提出基于物理的CBCT模拟方法，通过构建几何对齐的训练数据对，结合使用相对于输入CBCT的几何对齐指标（如归一化互信息）而非有偏差的真实标签进行评估。

Result: 在两个独立的盆腔数据集上，使用合成数据训练的模型实现了更优的几何对齐（归一化互信息：0.31 vs 0.22），尽管传统强度分数较低。强度指标与临床评估呈负相关，而归一化互信息始终能预测观察者偏好。临床观察者在87%的案例中更偏好合成训练的输出。

Conclusion: 几何保真度而非与有偏差真实标签的强度一致性，更符合临床需求。基于物理的CBCT模拟结合几何对齐评估方法能够提供更可靠的模型训练和评估框架。

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [127] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

TL;DR: LatentLens是一种新的可解释性方法，通过将视觉标记表示与文本语料库的上下文化表示进行比较，用自然语言描述视觉标记在LLM各层的编码内容，显著提高了视觉标记的可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了理解为什么大型语言模型（LLM）能够如此容易地处理视觉标记，需要开发能够揭示LLM处理过程中各层视觉标记表示内容的可解释性方法。现有方法如LogitLens严重低估了视觉标记的可解释性。

Method: LatentLens方法：首先编码大型文本语料库，存储每个标记的上下文化表示；然后将视觉标记表示与这些文本表示进行比较，通过top-k最近邻表示提供视觉标记的描述。

Result: 在10个不同的视觉语言模型（VLM）上评估，LatentLens显示大多数视觉标记在所有研究模型和所有层都是可解释的。定性分析表明，LatentLens产生的描述具有语义意义，相比单个标记为人类提供了更细粒度的解释。

Conclusion: LatentLens为视觉标记的可解释性提供了新方法，其发现为视觉和语言表示之间的对齐提供了新证据，为分析潜在表示开辟了新方向。

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [128] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

TL;DR: 提出CIEC框架，仅使用粗粒度图像/句子级标注实现多模态弱监督篡改定位，在多项评估指标上达到与全监督方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态篡改定位方法依赖成本高、耗时的细粒度标注（如补丁/令牌级标注），需要开发仅使用粗粒度标注的弱监督方法。

Method: 提出CIEC框架，包含图像和文本两个弱监督定位分支：图像分支使用TRPS模块整合视觉和文本伪造线索锁定可疑区域；文本分支使用VCTG模块聚焦内容词并利用视觉偏差辅助令牌定位。

Result: 大量实验证明CIEC的有效性，在多项评估指标上取得了与全监督方法相当的结果。

Conclusion: CIEC框架成功实现了仅使用粗粒度标注的多模态弱监督篡改定位，为减少标注成本提供了有效解决方案。

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [129] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

TL;DR: 本文提出了Vision-DeepResearch基准测试(VDR-Bench)，包含2000个VQA实例，用于评估多模态大语言模型在视觉-文本深度研究系统中的真实搜索能力，并提出了多轮裁剪搜索工作流程来提升视觉检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉深度研究系统评估中存在两个主要问题：1)现有基准测试不是以视觉搜索为中心，答案往往通过文本问题中的跨文本线索泄露，或可从模型先验知识中推断；2)评估场景过于理想化，图像搜索可通过近似完全匹配获取信息，文本搜索则过于直接且挑战性不足。

Method: 构建了Vision-DeepResearch基准测试(VDR-Bench)，包含2000个VQA实例，采用精心设计的多阶段筛选流程和严格的专家评审。同时提出了简单的多轮裁剪搜索工作流程，以提升当前MLLMs在真实视觉检索场景中的能力。

Result: VDR-Bench基准测试能够有效评估视觉深度研究系统在真实世界条件下的行为表现。提出的多轮裁剪搜索策略被证明能够有效提升模型在真实视觉检索场景中的性能。

Conclusion: 研究结果为未来多模态深度研究系统的设计提供了实用指导，提出的基准测试和工作流程有助于更准确地评估和提升MLLMs在视觉-文本深度研究任务中的能力。

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [130] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

TL;DR: TopoField是一个拓扑感知的隐式建模框架，用于修复CT图像中提取的肺树拓扑不完整问题，同时支持解剖标记和肺段重建任务。


<details>
  <summary>Details</summary>
Motivation: 从CT图像中提取的肺树经常存在拓扑不完整问题（如缺失或断开的分支），这会严重影响下游解剖分析，并限制现有肺树建模流程的适用性。当前方法通常依赖密集体积处理或显式图推理，导致效率有限且在真实结构损坏下鲁棒性降低。

Method: TopoField使用稀疏表面和骨架点云表示肺解剖结构，学习一个连续的隐式场，通过在已经不完整的树上引入合成的结构破坏进行训练，支持拓扑修复而不依赖完整或显式的断开标注。基于修复的隐式表示，通过任务特定的隐式函数在单次前向传播中联合推断解剖标记和肺段重建。

Result: 在Lung3D+数据集上的大量实验表明，TopoField持续改善了拓扑完整性，并在具有挑战性的不完整场景下实现了准确的解剖标记和肺段重建。由于其隐式表示，TopoField获得了高计算效率，每个病例的所有任务仅需一秒多完成。

Conclusion: TopoField将拓扑修复作为首要建模问题，实现了肺树分析的统一多任务推理框架，具有高计算效率和实用性，适合大规模和时间敏感的临床应用。

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [131] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

TL;DR: SSI-DM通过跳过数学奇点区域，在标准反转前添加小噪声，解决了扩散模型图像反转中的非高斯噪声问题，提升了编辑性


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型图像反转方法在早期噪声步骤中存在不准确性，导致生成非高斯噪声且编辑性差，根本原因是数学奇点使反转问题本质上不适定

Method: 提出SSI-DM方法，通过跳过奇点区域，在标准反转前添加小噪声，产生具有自然高斯特性的反转噪声，同时保持重建保真度

Result: 在公共图像数据集上，该方法在重建和插值任务中表现出优越性能，产生具有高斯特性的反转噪声

Conclusion: SSI-DM为扩散模型反转提供了一个原理清晰且高效的解决方案，作为即插即用技术与通用扩散模型兼容

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [132] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

TL;DR: MAIN-VLA框架通过建模意图和环境抽象，在复杂动态环境中实现更高效的视觉-语言-动作决策，显著提升决策质量、泛化能力和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作方法在高度复杂动态环境（如3D开放世界和大规模PvP游戏）中，从冗余传感器流中提取动作关键信号效率低下，需要更有效的抽象表示方法。

Method: 提出MAIN-VLA框架，包含意图抽象（IA）将冗长语言指令压缩为显式语义原语，环境语义抽象（ESA）将视觉流投影为结构化拓扑可供性表示，并通过对齐两种抽象模态实现无参数token剪枝。

Result: 在开放世界Minecraft和大规模PvP环境（Game for Peace和Valorant）的广泛实验中，MAIN-VLA实现了最先进的性能，具有优越的决策质量、更强的泛化能力和顶尖的推理效率。

Conclusion: 通过显式建模意图和环境抽象，MAIN-VLA在复杂动态环境中实现了基于深度语义对齐而非表面模式匹配的决策，显著提升了视觉-语言-动作系统的性能。

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [133] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: 提出Causal Forcing方法，使用自回归教师模型进行ODE初始化，解决双向视频扩散模型蒸馏为自回归模型时的架构差距问题，显著提升实时交互视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将预训练的双向视频扩散模型蒸馏为少步自回归模型时存在架构差距，且没有理论上的解决方案。ODE蒸馏需要帧级单射条件，但双向教师蒸馏自回归学生违反了这一条件，导致性能下降。

Method: 提出Causal Forcing方法，使用自回归教师模型进行ODE初始化，从而弥合架构差距。该方法确保满足帧级单射条件，能够恢复教师模型的流映射。

Result: 实验结果显示，该方法在所有指标上均优于所有基线，在Dynamic Degree上超越SOTA Self Forcing 19.3%，在VisionReward上超越8.7%，在Instruction Following上超越16.7%。

Conclusion: Causal Forcing通过使用自回归教师进行ODE初始化，有效解决了双向视频扩散模型蒸馏为自回归模型时的理论问题，显著提升了实时交互视频生成的性能。

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [134] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

TL;DR: HieraNav是一个多粒度、开放词汇的目标导航任务，要求智能体根据自然语言指令在四个语义层次（场景、房间、区域、实例）上导航到目标。LangMap是基于真实3D室内扫描的大规模基准测试，包含人类验证的标注和任务，用于评估语言驱动的具身导航能力。


<details>
  <summary>Details</summary>
Motivation: 物体与语言之间的关系对于人类与AI之间的有意义通信以及实际有用的具身智能至关重要。当前需要建立一个能够评估智能体在不同语义层次上理解自然语言指令并进行导航能力的基准测试。

Method: 提出了HieraNav任务框架，包含四个语义层次的导航目标。构建了LangMap基准测试，基于真实3D室内扫描，包含区域标签、区分性区域描述、覆盖414个对象类别的区分性实例描述，以及超过18K个导航任务。每个目标都有简洁和详细两种描述方式。

Result: LangMap在标注质量上表现出色，比GOAT-Bench在区分性准确率上高出23.8%，同时使用的词汇量减少了四倍。评估显示，更丰富的上下文和记忆可以提高成功率，但长尾、小型、上下文依赖和远距离目标以及多目标完成仍然具有挑战性。

Conclusion: HieraNav和LangMap为推进语言驱动的具身导航建立了一个严格的测试平台，能够评估智能体在不同语义层次上理解和执行自然语言指令的能力。

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [135] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

TL;DR: LatentMorph是一个在潜在空间中执行隐式推理的文本到图像生成框架，通过四个轻量级组件实现自适应推理，显著提升生成质量并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法缺乏动态推理和细化的能力，而现有的推理增强范式依赖显式思维过程，需要在固定步骤将中间推理解码为离散文本，导致效率低下、信息丢失和认知不匹配。

Method: 提出LatentMorph框架，在连续潜在空间中执行隐式推理，包含四个轻量级组件：冷凝器（总结中间生成状态为视觉记忆）、翻译器（将潜在思维转换为可操作指导）、塑形器（动态引导下一个图像标记预测）和RL训练调用器（自适应确定何时调用推理）。

Result: 1) 在Janus-Pro基础上，GenEval提升16%，T2I-CompBench提升25%；2) 在抽象推理任务上优于显式范式（如TwiG）15%和11%；3) 推理时间减少44%，标记消耗减少51%；4) 在推理调用上与人类直觉的认知对齐度达71%。

Conclusion: LatentMorph通过在潜在空间中执行隐式推理，避免了显式推理的瓶颈，实现了更高效、更自适应、更符合人类认知的文本到图像生成。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [136] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

TL;DR: 提出首个用于3D LiDAR场景补全的流匹配框架LiFlow，通过确保训练和推理初始分布的一致性，改进了基于扩散的方法，在多个指标上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中收集的LiDAR点云面临遮挡和远距离稀疏性问题，限制了感知系统性能。现有基于扩散的方法存在训练和推理初始分布不匹配的问题。

Method: 采用流匹配框架，使用最近邻流匹配损失和Chamfer距离损失，增强点云对齐的局部结构和全局覆盖。

Result: LiFlow在多个指标上达到最先进的性能表现。

Conclusion: 提出的流匹配框架有效解决了训练和推理初始分布不一致问题，为3D LiDAR场景补全提供了更好的解决方案。

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [137] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

TL;DR: DiScene是一个基于稀疏查询的占用预测框架，通过多层次知识蒸馏实现高效鲁棒的占用预测，在Occ-Scannet基准上达到23.2 FPS，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前密集方法在空体素上存在计算浪费，而稀疏查询方法在复杂室内场景中缺乏鲁棒性，需要解决效率与精度之间的权衡问题。

Method: 提出DiScene框架，包含两个关键创新：1) 多层次一致知识蒸馏策略，通过编码器级特征对齐、查询级特征匹配、先验级空间引导和锚点级高置信度知识转移四个层次将大型教师模型的知识转移到轻量学生模型；2) 教师引导初始化策略，使用优化参数预热加速模型收敛。

Result: 在Occ-Scannet基准上，DiScene达到23.2 FPS，比基线方法OPUS提升36.1%，甚至优于深度增强版本OPUS†。集成深度信息后，DiScene†超越EmbodiedOcc 3.7%，推理速度提升1.62倍。在Occ3D-nuScenes基准和真实场景中也表现出良好泛化能力。

Conclusion: DiScene通过多层次知识蒸馏实现了高效且鲁棒的占用预测，在多个基准测试中达到最先进性能，为机器人感知提供了有效的几何和语义理解解决方案。

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [138] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

TL;DR: 提出基于残差向量量化变分自编码器(RVQ-VAEs)的人体运动风格-内容解耦方法，通过量化码本交换实现无需微调的零样本风格迁移


<details>
  <summary>Details</summary>
Motivation: 人体运动数据同时包含语义内容和细微风格特征，传统方法难以有效解耦，需要一种能够分离粗粒度运动属性(内容)和细粒度表达细节(风格)的框架

Method: 使用RVQ-VAEs学习从粗到细的运动表示；结合对比学习和新颖的信息泄漏损失与码本学习，在不同码本中组织内容和风格；提出量化码本交换推理技术

Result: 框架在风格迁移、风格移除和运动混合等多种推理应用中表现出强大通用性，能够实现无需对未见风格进行微调的零样本风格迁移

Conclusion: 提出的方法有效解耦了人体运动数据中的风格和内容，通过量化码本交换技术实现了灵活的运动编辑，为运动风格分析提供了新思路

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [139] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

TL;DR: LongVPO是一个新颖的两阶段直接偏好优化框架，使短上下文视觉语言模型能够无需长视频标注即可稳健理解超长视频。通过合成偏好三元组和递归字幕生成，仅用16K合成样本就在多个长视频基准上超越最先进的开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要针对短视频设计，缺乏对超长视频的理解能力，而获取长视频标注成本高昂。需要一种无需昂贵人工标注就能扩展模型长视频理解能力的方法。

Method: 采用两阶段方法：第一阶段合成偏好三元组，将问题锚定到单个短片段，通过视觉相似性和问题特异性过滤减轻位置偏差；第二阶段使用递归字幕管道生成长视频场景级元数据，利用大语言模型创建多段推理查询和不受偏好的响应，通过多段推理任务对齐模型偏好。

Result: 仅使用16K合成示例且无需昂贵人工标注，LongVPO在多个长视频基准测试中超越了最先进的开源模型，同时在短视频性能（如MVBench）上保持强劲表现。

Conclusion: LongVPO为高效的长形式视频理解提供了一个可扩展的范式，通过创新的两阶段直接偏好优化框架，成功解决了短上下文模型理解超长视频的挑战，且无需成本高昂的长视频标注。

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [140] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

TL;DR: SNGP模型通过谱归一化和高斯过程层改进数字病理学中的不确定性估计和OOD检测，相比传统方法在保持性能的同时显著提升校准性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理学的深度学习模型在分布外（OOD）设置中往往过度自信且校准性差，这限制了临床信任和采用。医疗影像工作流程需要能够准确拒绝OOD输入的内在不确定性感知特性。

Method: 实现谱归一化神经高斯过程（SNGP），通过谱归一化和将最终密集层替换为高斯过程层来改进单模型不确定性估计和OOD检测。在三个生物医学分类任务（白细胞、淀粉样斑块、结直肠组织病理学）的六个数据集上评估SNGP与确定性模型和蒙特卡洛dropout方法。

Result: SNGP在分布内性能相当的同时，显著改善了不确定性估计和OOD检测能力。

Conclusion: SNGP或相关模型为数字病理学中的不确定性感知分类提供了有用框架，支持安全部署并建立与病理学家的信任。

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [141] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出UnifiedReward-Flex，一个统一的个性化视觉生成奖励模型，通过上下文自适应推理解决现有奖励模型对内容特定视觉线索不敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态奖励模型存在局限性：采用一刀切范式，假设单一偏好分布或依赖固定评估标准，对内容特定视觉线索不敏感，导致与主观和上下文相关的人类偏好系统性错位。

Method: 提出UnifiedReward-Flex，首先解释语义意图并基于视觉证据，然后通过实例化细粒度标准动态构建分层评估。采用两阶段训练：1) 从先进闭源VLM蒸馏结构化高质量推理轨迹进行SFT；2) 在精心策划的偏好对上执行直接偏好优化(DPO)。

Result: 将UnifiedReward-Flex集成到GRPO框架中进行图像和视频合成，广泛结果表明其优越性。

Conclusion: UnifiedReward-Flex通过耦合奖励建模与灵活上下文自适应推理，解决了现有奖励模型的局限性，实现了更好的个性化视觉生成评估。

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [142] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: Infinite-World是一个鲁棒的交互式世界模型，能够在复杂真实世界环境中维持超过1000帧的连贯视觉记忆，通过分层无姿态记忆压缩器和不确定性感知动作标注模块解决真实视频训练中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在合成数据上能有效优化，但在真实世界视频中面临挑战：姿态估计噪声大、视角重访稀缺，缺乏有效的训练范式。

Method: 1. 分层无姿态记忆压缩器(HPMC)：递归将历史潜在表示压缩为固定预算表示，与生成主干联合优化，无需显式几何先验；2. 不确定性感知动作标注模块：将连续运动离散化为三态逻辑，最大化利用原始视频数据同时保护确定性动作空间免受噪声轨迹污染；3. 重访密集微调策略：使用30分钟紧凑数据集激活模型的长距离闭环能力。

Result: 广泛实验（包括客观指标和用户研究）表明，Infinite-World在视觉质量、动作可控性和空间一致性方面实现了优越性能。

Conclusion: Infinite-World通过创新的记忆压缩和动作标注方法，成功解决了真实世界视频训练中的关键挑战，实现了长期连贯的视觉记忆和鲁棒的交互能力。

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [143] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: Catalyst是一个后处理OOD检测框架，利用池化前特征图的原始通道统计信息（均值、标准差、最大激活值）计算输入相关的缩放因子γ，通过弹性缩放增强现有基线方法的OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的后处理方法主要依赖全局平均池化后的logits或特征向量，但这种方法丢弃了池化前特征图中丰富的通道统计信息。这些被忽略的信号包含与现有方法互补的信息，可以显著提升OOD检测性能。

Method: Catalyst框架从池化前特征图的原始通道统计信息（如均值、标准差、最大激活值）中动态计算输入相关的缩放因子γ。然后将γ与现有基线分数融合，通过乘法调制（弹性缩放）来进一步分离ID和OOD分布。该框架可通用化地集成到基于logit的方法（如Energy、ReAct、SCALE）和基于距离的检测器（如KNN）中。

Result: 在多个数据集上取得显著性能提升：在CIFAR-10（ResNet-18）上平均误报率降低32.87%，在CIFAR-100（ResNet-18）上降低27.94%，在ImageNet（ResNet-50）上降低22.25%。结果表明Catalyst与现有OOD检测方法具有互补性。

Conclusion: 池化前统计信息具有未开发的潜力，Catalyst框架能够有效利用这些信号，通过弹性缩放机制显著提升各种OOD检测方法的性能，证明了该方法的通用性和有效性。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [144] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

TL;DR: 提出SelvaMask热带森林数据集和基于视觉基础模型的检测-分割流水线，在热带森林树冠分割任务上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 热带森林对全球生态平衡至关重要，但现有树冠分割方法在热带森林中性能较低，缺乏专门的数据集和有效方法

Method: 1) 创建SelvaMask数据集（包含8,800个手动标注树冠）；2) 提出模块化检测-分割流水线，使用领域特定的检测提示器适配视觉基础模型

Result: 方法在密集热带森林中达到最先进性能，优于零样本通用模型和全监督端到端方法，在外部热带和温带数据集上验证了泛化能力

Conclusion: SelvaMask既是具有挑战性的基准，也是实现广义森林监测的关键推动者，代码和数据集将公开

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


### [145] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

TL;DR: 提出基于Swin U-Net的门控多头Transformer架构，通过切片级结构检测门控分割预测，有效抑制解剖无效切片中的假阳性分割


<details>
  <summary>Details</summary>
Motivation: 传统深度学习自动分割模型在缺乏目标结构的切片中经常产生解剖上不合理的假阳性（幻觉），影响放疗临床应用的可靠性

Method: 基于Swin U-Net的门控多头Transformer架构，结合切片间上下文集成和平行检测头，通过多层感知器进行切片级结构检测，上下文增强流进行像素级分割，检测输出门控分割预测，使用切片级Tversky损失解决类别不平衡

Result: 在Prostate-Anatomical-Edge-Cases数据集上，门控模型显著优于非门控基线，平均Dice损失为0.013±0.036 vs 0.732±0.314，检测概率与解剖存在强相关，有效消除虚假分割

Conclusion: 基于检测的门控机制增强了自动分割的鲁棒性和解剖合理性，在不影响有效切片分割质量的情况下减少幻觉预测，为改善临床放疗自动勾画工作流程的可靠性提供了有前景的方法

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [146] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

TL;DR: PixelGen提出了一种简单的像素扩散框架，通过感知监督来优化高维像素空间，避免了VAE引入的伪影和瓶颈，在图像生成质量上超越了潜在扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有像素扩散方法难以优化包含大量感知无关信号的高维像素流形，导致性能落后于潜在扩散模型。传统像素扩散需要建模完整的图像流形，而其中很多信号对感知质量并不重要。

Method: PixelGen引入两种互补的感知损失来引导扩散模型学习更有意义的感知流形：LPIPS损失促进学习更好的局部模式，基于DINO的感知损失增强全局语义。该方法不需要VAE、潜在表示或辅助阶段。

Result: 在ImageNet-256上无需分类器引导就达到了5.11的FID（仅80个训练周期），在大规模文本到图像生成中GenEval得分为0.79，超越了强大的潜在扩散基线。

Conclusion: PixelGen提供了一个更简单但更强大的生成范式，通过感知监督直接在像素空间进行端到端生成，避免了VAE引入的伪影和瓶颈，在图像质量上超越了潜在扩散模型。

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


### [147] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: DIAMOND是一种无需训练的方法，通过轨迹校正来减少文本到图像生成中的视觉和解剖伪影，无需修改模型权重或进行区域细化。


<details>
  <summary>Details</summary>
Motivation: 尽管FLUX等文本到图像模型取得了令人印象深刻的结果，但视觉和解剖伪影仍然是实际和专业应用的主要障碍。现有方法通常是事后处理，无法在核心图像形成过程中有效干预，且需要修改模型权重或进行计算昂贵的区域细化。

Method: DIAMOND是一种无需训练的方法，在推理过程中应用轨迹校正来减轻伪影。通过在生成轨迹的每一步重建干净样本的估计，主动引导生成过程远离导致伪影的潜在状态。该方法还可扩展到标准扩散模型。

Result: DIAMOND提供了一种鲁棒的零样本路径，能够实现高保真、无伪影的图像合成，无需额外训练或修改现代生成架构的权重。

Conclusion: DIAMOND通过训练自由的轨迹校正方法，有效解决了文本到图像生成中的伪影问题，为实际和专业应用提供了高质量图像合成方案。

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [148] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

TL;DR: 提出了一种无需人工标注的显微图像语义分割框架，通过相场模拟生成微结构形态，使用CycleGAN将模拟图像转换为逼真的SEM图像，训练U-Net模型在实验图像上实现优异泛化性能。


<details>
  <summary>Details</summary>
Motivation: 显微图像语义分割对高通量材料表征至关重要，但专家标注数据成本高、主观性强且稀缺。基于物理的模拟虽然可扩展，但由于域差距（缺乏复杂纹理、噪声模式和成像伪影），训练模型难以泛化到实验数据。

Method: 1. 使用相场模拟生成大量微结构形态，获得完美、内在衍生的真实掩码；2. 采用CycleGAN进行非配对图像到图像转换，将干净的模拟图像转换为大规模、高保真的真实SEM图像数据集；3. 仅使用合成数据训练U-Net模型。

Result: 在未见过的实验图像上，U-Net模型实现了平均边界F1分数0.90和交并比0.88。通过t-SNE特征空间投影和香农熵分析验证，合成图像在统计和特征上与真实数据流形无法区分。

Conclusion: 该生成框架完全解耦了模型训练与人工标注，将数据稀缺问题转化为数据丰富问题，为加速材料发现和分析提供了稳健、完全自动化的解决方案。

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [149] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

TL;DR: CloDS是一个无监督学习框架，通过多视角视觉观测学习布料动力学，无需已知物理属性作为监督，采用三阶段流程实现视频到几何的映射和动力学模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需要已知物理属性作为监督或输入，这在未知条件下限制了应用。为了解决这一挑战，研究者提出了无监督学习布料动力学的新场景。

Method: CloDS采用三阶段流程：1)视频到几何的映射；2)在映射的网格上训练动力学模型。为处理大变形和严重自遮挡，引入了双位置不透明度调制，通过基于网格的高斯溅射支持2D观测与3D几何之间的双向映射。

Result: 综合实验评估表明，CloDS能够有效地从视觉数据中学习布料动力学，同时对未见配置保持强大的泛化能力。

Conclusion: CloDS成功实现了从多视角视觉观测中无监督学习布料动力学，为解决未知条件下的动态系统模拟提供了有效框架。

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [150] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

TL;DR: STT-LTF是一个时空Transformer框架，用于长期卫星图像时间序列预测，在异质性地中海景观中表现优异，直接预测任意未来时间点，无需自回归误差累积。


<details>
  <summary>Details</summary>
Motivation: 解决异质性地中海景观中长期卫星图像时间序列分析的挑战，包括复杂空间模式、季节变化和多年代环境变化在不同尺度上的相互作用。

Method: 提出STT-LTF框架，通过统一Transformer架构处理多尺度空间补丁和长达20年的时间序列，结合空间掩码、时间掩码和水平采样策略的自监督学习，使用空间补丁嵌入、循环时间编码和地理坐标。

Result: 在Landsat数据(1984-2024)上评估，STT-LTF达到MAE 0.0328和R^2 0.8412的次年预测性能，优于传统统计方法、CNN、LSTM和标准Transformer。

Conclusion: STT-LTF能够处理不规则时间采样和可变预测水平，特别适用于经历快速生态转变的异质性地中海景观分析，为长期环境监测提供了有效工具。

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [151] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 该论文提出MultiBO方法，通过多轮用户偏好反馈来优化个性化图像生成，解决语言提示无法完全表达用户心中特定图像的问题。


<details>
  <summary>Details</summary>
Motivation: 当用户试图用语言提示生成心中特定图像时，即使经过多轮提示，生成的图像与目标图像仍有差距，而语言提示难以完全消除这种差距。但人类仍能判断新图像是否比当前图像更接近目标图像。

Method: 提出MultiBO（多选择偏好贝叶斯优化）方法：1）基于当前最佳图像生成K个新图像；2）获取用户对这些图像的偏好反馈；3）利用反馈指导扩散模型；4）生成新的K个图像集。通过B轮用户反馈迭代优化。

Result: 30名用户的定性评分和与5个基线的定量指标比较显示，MultiBO能在有限反馈轮次内显著接近目标图像，即使生成模型没有目标图像的任何信息。

Conclusion: 多选择的人类偏好反馈可以有效用于个性化图像生成，通过迭代优化过程能够显著缩小生成图像与用户心中目标图像之间的差距。

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [152] [Disentangled Interest Network for Out-of-Distribution CTR Prediction](https://arxiv.org/abs/2602.00002)
*Yu Zheng,Chen Gao,Jianxin Chang,Yanan Niu,Yang Song,Depeng Jin,Meng Wang,Yong Li*

Main category: cs.IR

TL;DR: DiseCTR：通过因果分解和兴趣解耦解决CTR预测中的分布外问题，显著提升推荐系统的准确性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法通常假设训练和测试数据来自同一分布，但实际中用户兴趣不断演变导致分布变化（OOD问题）。此外，用户通常有多个兴趣，且不同兴趣的演变速度不同，需要更精细的建模方法。

Method: 提出DiseCTR框架：1）从因果角度对CTR预测进行分解，涉及用户兴趣、曝光模型和点击模型；2）设计稀疏注意力兴趣编码器将原始特征映射到用户兴趣；3）引入弱监督兴趣解耦器学习独立的兴趣嵌入；4）使用注意力兴趣聚合器进行预测集成。

Result: 在三个真实世界数据集上的实验表明，DiseCTR在OOD推荐中达到最佳准确性和鲁棒性，显著提升AUC和GAUC超过0.02，降低logloss超过13.7%。进一步分析证实DiseCTR成功解耦了用户兴趣。

Conclusion: DiseCTR通过因果视角和兴趣解耦有效缓解了CTR预测中的OOD问题，兴趣解耦是OOD泛化的关键。该方法在准确性和鲁棒性方面显著优于现有方法。

Abstract: Click-through rate (CTR) prediction, which estimates the probability of a user clicking on a given item, is a critical task for online information services. Existing approaches often make strong assumptions that training and test data come from the same distribution. However, the data distribution varies since user interests are constantly evolving, resulting in the out-of-distribution (OOD) issue. In addition, users tend to have multiple interests, some of which evolve faster than others. Towards this end, we propose Disentangled Click-Through Rate prediction (DiseCTR), which introduces a causal perspective of recommendation and disentangles multiple aspects of user interests to alleviate the OOD issue in recommendation. We conduct a causal factorization of CTR prediction involving user interest, exposure model, and click model, based on which we develop a deep learning implementation for these three causal mechanisms. Specifically, we first design an interest encoder with sparse attention which maps raw features to user interests, and then introduce a weakly supervised interest disentangler to learn independent interest embeddings, which are further integrated by an attentive interest aggregator for prediction. Experimental results on three real-world datasets show that DiseCTR achieves the best accuracy and robustness in OOD recommendation against state-of-the-art approaches, significantly improving AUC and GAUC by over 0.02 and reducing logloss by over 13.7%. Further analyses demonstrate that DiseCTR successfully disentangles user interests, which is the key to OOD generalization for CTR prediction. We have released the code and data at https://github.com/DavyMorgan/DiseCTR/.

</details>


### [153] [Efficient Multilingual Search Relevance Modeling in E-Commerce via LLM Mixture-of-Experts](https://arxiv.org/abs/2602.00003)
*Ye Liu,Xu Chen,Wuji Chen,Mang Li*

Main category: cs.IR

TL;DR: 该论文提出了一种基于LLM的混合专家(MoE)框架，用于解决多国电商平台中搜索相关性的挑战，通过动态路由查询到专门专家并融合嵌入，在保持效率的同时提升了多语言相关性。


<details>
  <summary>Details</summary>
Motivation: 在多国电商平台部署中，语言、文化和产品目录的多样性导致显著的分布偏移，给相关性建模带来挑战。现有方法通常增强单一模型的能力，但受限于数据多样性、覆盖差距和高推理成本。研究发现不同LLM基础模型在不同语言和地区表现出互补优势，这促使采用专家架构。

Method: 提出可扩展的基于LLM的混合专家(MoE)框架，动态路由查询到专门专家，通过拼接融合嵌入。比较了基于规则、伪标签和端到端策略，发现端到端硬路由与拼接在效果和效率上达到最佳平衡。为降低推理开销，开发了工程优化的离线批处理管道，具有资源高效调度。

Result: 在覆盖六个东南亚市场的数据集上，MoE比具有相同活跃参数的密集基线AUC提高了0.72个百分点。优化的管道实现27.6 QPS，吞吐量提升9%。相比同步执行，GPU小时消耗减少高达35%。

Conclusion: 该MoE框架在多语言相关性和效率方面表现出色，为实际电商搜索系统提供了强大的成本效益。通过专家架构有效应对多国部署中的分布偏移挑战，同时优化的工程实现显著降低了推理成本。

Abstract: In e-commerce platforms, search relevance directly influences both user experience and merchant revenue. In multi-country deployments, diverse linguistic, cultural, and product catalog contexts introduce significant distribution shifts, posing substantial challenges to relevance modeling. Existing approaches typically enhance the reasoning or multilingual abilities of a single monolithic model, yet they remain limited by data diversity, coverage gaps, and high inference costs in heterogeneous environments. Our empirical analysis reveals that different LLM base models exhibit complementary strengths across languages and regions, motivating an expert-based architecture. We propose a scalable LLM-based Mixture-of-Experts (MoE) framework that dynamically routes queries to specialized experts and fuses their embeddings through concatenation. Among rule-based, pseudo-label-based, and fully end-to-end strategies, end-to-end hard routing with concatenation offers the best balance of effectiveness and efficiency. To mitigate inference overhead, we further develop an engineering-optimized offline batch pipeline with resource-efficient scheduling, which hides memory latency, improves GPU utilization, and reduces GPU-hour consumption by up to 35% compared with synchronous execution. On datasets spanning six Southeast Asian markets, our MoE improves AUC by 0.72 percentage points over a dense baseline with the same active parameters. Meanwhile, the optimized pipeline achieves 27.6 queries per second (QPS), a 9% throughput improvement. These results demonstrate superior multilingual relevance and efficiency, delivering strong cost-effectiveness for real-world e-commerce search systems.

</details>


### [154] [ChunkNorris: A High-Performance and Low-Energy Approach to PDF Parsing and Chunking](https://arxiv.org/abs/2602.00010)
*Mathieu Ciancone,Clovis Varangot-Reille,Marion Schaeffer*

Main category: cs.IR

TL;DR: ChunkNorris是一种基于启发式规则的PDF文档解析和分块优化技术，不依赖机器学习，计算开销小，在RAG应用中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在检索增强生成(RAG)应用中，信息检索部分至关重要，因为它为大型语言模型提供上下文信息以生成恰当且真实的回答。高质量的解析和分块直接影响下游任务（信息检索和答案生成）的效果。

Method: 提出ChunkNorris，一种基于启发式规则的新技术，用于优化PDF文档的解析和分块。该方法不依赖机器学习，采用一套简单但有效的启发式规则，以最小的计算开销实现高性能。

Result: 通过全面的基准测试，评估了执行时间、能耗和检索准确性等指标。ChunkNorris在性能上超越了基线方法和更先进的技术，为信息检索任务提供了实用高效的替代方案。

Conclusion: 这项研究突出了基于启发式规则的方法在现实世界、资源受限的RAG应用场景中的潜力，ChunkNorris为PDF文档处理提供了高效实用的解决方案。

Abstract: In Retrieval-Augmented Generation applications, the Information Retrieval part is central as it provides the contextual information that enables a Large Language Model to generate an appropriate and truthful response. High quality parsing and chunking are critical as efficient data segmentation directly impacts downstream tasks, i.e. Information Retrieval and answer generation. In this paper, we introduce ChunkNorris, a novel heuristic-based technique designed to optimise the parsing and chunking of PDF documents. Our approach does not rely on machine learning and employs a suite of simple yet effective heuristics to achieve high performance with minimal computational overhead. We demonstrate the efficiency of ChunkNorris through a comprehensive benchmark against existing parsing and chunking methods, evaluating criteria such as execution time, energy consumption, and retrieval accuracy. We propose an open-access dataset to produce our results. ChunkNorris outperforms baseline and more advanced techniques, offering a practical and efficient alternative for Information Retrieval tasks. Therefore, this research highlights the potential of heuristic-based methods for real-world, resource-constrained RAG use cases.

</details>


### [155] [Chained Prompting for Better Systematic Review Search Strategies](https://arxiv.org/abs/2602.00011)
*Fatima Nasser,Fouad Trad,Ammar Mohanna,Ghada El-Hajj Fuleihan,Ali Chehab*

Main category: cs.IR

TL;DR: 本文提出了一种基于大语言模型的链式提示工程框架，用于自动化开发系统综述的搜索策略，相比传统方法在召回率方面表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统手动设计搜索策略虽然方法系统但资源密集且易受主观性影响，而启发式和自动化技术在没有大量专家输入的情况下召回率表现不佳。需要一种既能保持方法严谨性又能提高效率的自动化解决方案。

Method: 提出基于大语言模型的链式提示工程框架，该框架复制手动搜索设计的程序结构，利用LLM分解综述目标、提取和形式化PICO元素、生成概念表示、扩展术语并合成布尔查询。

Result: 在LEADSInstruct数据集子集上的评估显示，该框架达到0.9的平均召回率，显著超过现有方法的性能。此外，在生成结构良好的PICO元素方面也优于现有方法。

Conclusion: LLM-based pipeline能够产生透明、可重复且高性能的搜索策略，具有作为支持证据合成和循证实践的可扩展工具的潜力。精确的目标规范和术语对齐对优化检索效果至关重要。

Abstract: Systematic reviews require the use of rigorously designed search strategies to ensure both comprehensive retrieval and minimization of bias. Conventional manual approaches, although methodologically systematic, are resource-intensive and susceptible to subjectivity, whereas heuristic and automated techniques frequently under-perform in recall unless supplemented by extensive expert input. We introduce a Large Language Model (LLM)-based chained prompt engineering framework for the automated development of search strategies in systematic reviews. The framework replicates the procedural structure of manual search design while leveraging LLMs to decompose review objectives, extract and formalize PICO elements, generate conceptual representations, expand terminologies, and synthesize Boolean queries. In addition to query construction, the framework exhibits superior performance in generating well-structured PICO elements relative to existing methods, thereby strengthening the foundation for high-recall search strategies. Evaluation on a subset of the LEADSInstruct dataset demonstrates that the framework attains a 0.9 average recall. These results significantly exceed the performance of existing approaches. Error analysis further highlights the critical role of precise objective specification and terminological alignment in optimizing retrieval effectiveness. These findings confirm the capacity of LLM-based pipelines to yield transparent, reproducible, and high-performing search strategies, and highlight their potential as scalable instruments for supporting evidence synthesis and evidence-based practice.

</details>


### [156] [AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows](https://arxiv.org/abs/2602.00052)
*Ramtin Babaeipour,François Charest,Madison Wright*

Main category: cs.IR

TL;DR: 评估基于生成式LLM和检索增强生成(RAG)的AI系统在临床试验方案信息提取中的表现，相比独立LLM准确率更高(87.8% vs 62.6%)，AI辅助工作流程效率提升40%且用户体验更好。


<details>
  <summary>Details</summary>
Motivation: 临床试验方案日益复杂、修订频繁以及知识管理困难给试验团队带来沉重负担。将方案内容结构化到标准格式中可以提高效率、改善文档质量并加强合规性。

Method: 使用生成式LLM和检索增强生成(RAG)技术开发AI系统，用于自动提取临床试验方案信息。将临床试验特定的RAG流程与公开可用的独立LLM进行提取准确性比较，并评估AI辅助对模拟提取CRC工作流程的操作影响。

Result: 临床试验特定的RAG流程准确率达到87.8%，显著高于经过精细调优提示的独立LLM(62.6%)。在模拟提取工作流程中，AI辅助任务完成速度快40%，认知负荷更低，用户强烈偏好AI辅助方式。

Conclusion: 虽然专家监督仍然必不可少，但AI辅助提取能够实现大规模的方案智能化，这促使将类似方法整合到真实世界的临床工作流程中，以进一步验证其对可行性、研究启动和激活后监测的影响。

Abstract: Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We evaluate an Artificial Intelligence (AI) system using generative LLMs with Retrieval-Augmented Generation (RAG) for automated clinical trial protocol information extraction. We compare the extraction accuracy of our clinical-trial-specific RAG process against that of publicly available (standalone) LLMs. We also assess the operational impact of AI-assistance on simulated extraction CRC workflows. Our RAG process was measured as more accurate (87.8%) than standalone LLMs with fine-tuned prompts (62.6%) against expert-supported reference annotations. In the simulated extraction workflows, AI-assisted tasks were completed 40% faster, rated as less cognitively demanding and strongly preferred by users. While expert oversight remains essential, this suggests that AI-assisted extraction can enable protocol intelligence at scale, motivating the integration of similar methodologies into real world clinical workflows to further validate its impact on feasibility, study start-up, and post-activation monitoring.

</details>


### [157] [SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.00083)
*Yuxin Yang,Gangda Deng,Ömer Faruk Akgül,Nima Chitsazan,Yash Govilkar,Akasha Tigalappanavara,Shi-Xiong Zhang,Sambit Sahu,Viktor Prasanna*

Main category: cs.IR

TL;DR: SPARC-RAG是一个多智能体框架，通过协调顺序和并行推理时间扩展来解决传统RAG在多跳问答中的局限性，采用统一上下文管理机制，显著提升性能并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）在多跳问答任务中面临挑战，需要长推理链。现有方法在推理时沿顺序深度（迭代优化）和并行宽度（覆盖扩展）两个维度进行扩展，但简单扩展会导致上下文污染和扩展效率低下，即使增加计算量也可能带来收益递减或负收益。

Method: 提出SPARC-RAG多智能体框架：1）协调顺序和并行推理时间扩展，采用统一上下文管理机制；2）使用专门智能体维护共享全局上下文，提供对扩展过程的显式控制；3）为每个分支生成有针对性的互补子查询，实现多样化并行探索；4）基于答案正确性和证据基础显式调节退出决策；5）引入轻量级微调方法，使用过程级可验证偏好优化扩展行为。

Result: 在单跳和多跳问答基准测试中，SPARC-RAG始终优于之前的RAG基线方法，在更低的推理成本下平均获得+6.2 F1分数提升。

Conclusion: SPARC-RAG通过协调顺序和并行推理时间扩展，结合统一上下文管理和多智能体协作，有效解决了传统RAG在多跳问答中的扩展效率问题，显著提升了性能并降低了计算成本。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.

</details>


### [158] [RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval](https://arxiv.org/abs/2602.02444)
*Tyler Skow,Alexander Martin,Benjamin Van Durme,Rama Chellappa,Reno Kriz*

Main category: cs.IR

TL;DR: RANKVIDEO是一个基于推理的视频检索重排序模型，通过显式推理查询-视频对的相关性，在MultiVENT 2.0基准测试中比纯文本和视觉语言重排序方法表现更好，效率更高。


<details>
  <summary>Details</summary>
Motivation: 虽然大型推理模型在文本重排序方面取得了快速进展，但基于推理的视频检索重排序仍然未被充分探索。现有的视频检索系统通常采用两阶段架构，但缺乏专门针对视频内容进行显式推理的重排序方法。

Method: RANKVIDEO采用两阶段课程训练：1) 感知基础监督微调；2) 结合点对点、成对和教师置信度蒸馏目标的重排序训练。同时构建了数据合成管道来生成推理密集的查询-视频对。

Result: 在MultiVENT 2.0大规模基准测试中，RANKVIDEO在两阶段检索框架下持续提升检索性能，在nDCG@10指标上平均提升31%，优于纯文本和视觉语言重排序方法，同时更高效。

Conclusion: RANKVIDEO证明了基于推理的视频重排序的有效性，填补了视频检索中推理重排序的研究空白，为视频检索系统提供了更精确和高效的解决方案。

Abstract: Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.

</details>


### [159] [Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs](https://arxiv.org/abs/2602.02338)
*Yu Liang,Zhongjin Zhang,Yuxuan Zhu,Kerui Zhang,Zhiluohan Guo,Wenhang Zhou,Zonqi Yang,Kangle Wu,Yabo Ni,Anxiang Zeng,Cong Fu,Jianxin Wang,Jiazhi Xia*

Main category: cs.IR

TL;DR: ReSID是一个推荐原生的语义ID框架，通过场感知掩码自编码和全局对齐正交量化，在不依赖LLM的情况下提升序列推荐的生成性能


<details>
  <summary>Details</summary>
Motivation: 现有语义ID推荐方法存在语义嵌入与协同预测弱耦合、通用量化对自回归建模效率低的问题，需要重新设计以更好地服务于生成式推荐目标

Method: 提出ReSID框架，包含两个组件：1) 场感知掩码自编码(FAMAE)从结构化特征学习预测充分的物品表示；2) 全局对齐正交量化(GAOQ)通过联合减少语义模糊性和前缀条件不确定性来生成紧凑可预测的语义ID序列

Result: 在10个数据集上的实验表明，ReSID平均优于强序列和语义ID生成基线超过10%，同时将标记化成本降低高达122倍

Conclusion: ReSID通过重新思考表示学习和量化设计，提供了一个推荐原生的语义ID框架，在信息保留和序列可预测性方面表现出色，显著提升了生成式推荐性能

Abstract: Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.

</details>


### [160] [Linear-PAL: A Lightweight Ranker for Mitigating Shortcut Learning in Personalized, High-Bias Tabular Ranking](https://arxiv.org/abs/2602.00013)
*Vipul Dinesh Pawar*

Main category: cs.IR

TL;DR: 论文提出Linear-PAL框架，通过显式特征组合和强正则化解决电商排序中的位置偏差问题，相比深度集成方法在去偏排序质量和训练效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 电商排序中，隐式用户反馈存在严重的位置偏差问题，用户倾向于与排名靠前的商品互动而不考虑相关性。在高度偏差环境下，现有的深度集成方法存在捷径学习问题，过度拟合排名信号，导致排序质量下降。

Method: 提出Linear-PAL框架，通过结构约束实现去偏：1) 显式特征组合；2) 强正则化；3) 向量化整数哈希技术替代基于字符串的操作，将特征生成复杂度降至O(N)。

Result: 在大规模数据集（420万样本）上评估，Linear-PAL实现帕累托优势：去偏排序质量（相关性AUC：0.7626 vs 0.6736）优于深度集成方法，同时训练延迟减少43倍（40秒 vs 1762秒）。

Conclusion: Linear-PAL框架通过轻量级设计和计算效率，能够高频重训练，捕捉用户特定的新兴市场趋势，实现近乎实时的鲁棒个性化排序。

Abstract: In e-commerce ranking, implicit user feedback is systematically confounded by Position Bias -- the strong propensity of users to interact with top-ranked items regardless of relevance. While Deep Learning architectures (e.g., Two-Tower Networks) are the standard solution for de-biasing, we demonstrate that in High-Bias Regimes, state-of-the-art Deep Ensembles suffer from Shortcut Learning: they minimize training loss by overfitting to the rank signal, leading to degraded ranking quality despite high prediction accuracy. We propose Linear Position-bias Aware Learning (Linear-PAL), a lightweight framework that enforces de-biasing through structural constraints: explicit feature conjunctions and aggressive regularization. We further introduce a Vectorized Integer Hashing technique for feature generation, replacing string-based operations with $O(N)$ vectorized arithmetic. Evaluating on a large-scale dataset (4.2M samples), Linear-PAL achieves Pareto Dominance: it outperforms Deep Ensembles in de-biased ranking quality (Relevance AUC: 0.7626 vs. 0.6736) while reducing training latency by 43x (40s vs 1762s). This computational efficiency enables high-frequency retraining, allowing the system to capture user-specific emerging market trends and deliver robust, personalized ranking in near real-time.

</details>


### [161] [Adaptive Quality-Diversity Trade-offs for Large-Scale Batch Recommendation](https://arxiv.org/abs/2602.02024)
*Clémence Réda,Tomas Rigaux,Hiba Bederina,Koh Takeuchi,Hisashi Kashima,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: B-DivRec算法：结合行列式点过程和模糊剔除程序，在推荐系统中平衡相关性与多样性，支持自适应调整质量-多样性权衡


<details>
  <summary>Details</summary>
Motivation: 推荐系统需要同时提供高度相关且多样化的项目，既要个性化用户偏好，又要让用户走出舒适区。多样性可以带来意外发现和新颖性，增加用户参与度或收入。但实际应用中存在挑战：避免推荐相似但不同的项目以减少流失风险，以及处理大规模项目库（数百万项目）的计算成本。

Method: 1. 当用户反馈模型完全已知时，提出B-DivRec算法，结合行列式点过程和模糊剔除程序来调整项目多样性程度，在用户历史中强制执行质量-多样性权衡。2. 提出自适应方法，根据用户反馈动态调整质量-多样性权衡：如果多样性带来积极反馈则增强多样性，反之则减少。

Result: 在合成和真实数据集（电影推荐和药物再利用）上展示了B-DivRec在两个设置中的性能和多功能性。

Conclusion: B-DivRec算法能够有效解决推荐系统中相关性与多样性的平衡问题，通过结合行列式点过程和模糊剔除程序，既支持静态的质量-多样性权衡，也支持根据用户反馈的自适应调整，在实际应用中表现出良好的性能和适应性。

Abstract: A core research question in recommender systems is to propose batches of highly relevant and diverse items, that is, items personalized to the user's preferences, but which also might get the user out of their comfort zone. This diversity might induce properties of serendipidity and novelty which might increase user engagement or revenue. However, many real-life problems arise in that case: e.g., avoiding to recommend distinct but too similar items to reduce the churn risk, and computational cost for large item libraries, up to millions of items. First, we consider the case when the user feedback model is perfectly observed and known in advance, and introduce an efficient algorithm called B-DivRec combining determinantal point processes and a fuzzy denuding procedure to adjust the degree of item diversity. This helps enforcing a quality-diversity trade-off throughout the user history. Second, we propose an approach to adaptively tailor the quality-diversity trade-off to the user, so that diversity in recommendations can be enhanced if it leads to positive feedback, and vice-versa. Finally, we illustrate the performance and versatility of B-DivRec in the two settings on synthetic and real-life data sets on movie recommendation and drug repurposing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [162] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

TL;DR: 对比FastAPI和NVIDIA Triton两种ML模型部署方案在医疗领域的性能表现，提出混合架构最佳实践


<details>
  <summary>Details</summary>
Motivation: 医疗等受监管领域需要平衡推理延迟、吞吐量和数据隐私（如HIPAA）等多重需求，需要找到最优的ML模型部署方案

Method: 在Kubernetes上部署DistilBERT情感分析模型，对比FastAPI REST服务和NVIDIA Triton推理服务器的性能，包括中位数延迟（p50）、尾部延迟（p95）和吞吐量，并评估混合架构方案

Result: FastAPI在单请求工作负载中延迟更低（p50 22ms），而Triton通过动态批处理实现更好的可扩展性，在单个NVIDIA T4 GPU上达到780请求/秒的吞吐量，几乎是基准的两倍

Conclusion: 采用FastAPI作为受保护健康信息去识别的安全网关，结合Triton进行后端推理的混合架构是企业临床AI的最佳实践，为安全、高可用部署提供了蓝图

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [163] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 该研究探索使用大语言模型从游戏轨迹中逆向推导VGDL规则，比较了直接代码生成和基于结构因果模型的两阶段方法，发现SCM方法能产生更接近真实规则且逻辑更一致的描述。


<details>
  <summary>Details</summary>
Motivation: 深度学习智能体在复杂游戏领域表现出色但往往不理解底层因果机制，需要研究因果归纳能力——从观测数据中推断支配规律，以增强AI对游戏机制的理解。

Method: 从GVGAI框架中通过语义嵌入和聚类选择9个代表性游戏，比较两种VGDL生成方法：1) 直接从观测生成代码；2) 两阶段方法：先推断结构因果模型(SCM)，再翻译为VGDL。在多种提示策略和控制上下文环境下评估，信息量从原始游戏观察到部分VGDL规范不等。

Result: 基于SCM的方法比直接生成更常产生接近真实情况的VGDL描述，在盲评中获得高达81%的偏好胜率，产生更少的逻辑不一致规则。学习的SCM可用于因果强化学习、可解释智能体和程序化生成新颖但逻辑一致的游戏。

Conclusion: 基于结构因果模型的方法在从游戏轨迹逆向推导VGDL规则方面优于直接代码生成，为因果强化学习、可解释AI和程序化内容生成提供了有价值的工具。

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [164] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

TL;DR: 论文提出L-ICL方法，通过局部上下文学习演示来纠正LLM在符号规划任务中的约束违反问题，相比传统方法显著提升规划有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编码任务上表现出强大推理能力，但在符号经典规划任务中经常失败，生成的计划经常违反给定的领域约束（如穿墙）。

Method: 提出局部上下文学习（L-ICL）方法：迭代地在指令中注入针对性修正演示，具体识别轨迹中的第一个约束违反，并为失败步骤提供最小化的输入-输出示例来展示正确行为。

Result: L-ICL比显式指令或传统ICL（添加完整问题解决轨迹）及其他基线方法更有效。在8x8网格世界中，仅用60个训练示例就能产生89%的有效计划，相比最佳基线的59%提高了30%。在其他领域（网格导航、迷宫、Sokoban、BlocksWorld）和多种LLM架构上也显示出显著改进。

Conclusion: 局部上下文学习是一种有效的方法，能够显著改善LLM在符号规划任务中的表现，通过针对性修正演示来解决约束违反问题。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [165] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

TL;DR: 该论文研究了大型语言模型在特定领域微调后出现的突发性错位风险，通过构建11个不安全领域的数据集，评估了带后门触发器和不带后门触发器时模型的错位表现，并提出了领域脆弱性排名和错位预测方法。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地用于自主任务，突发性错位对AI安全构成风险。研究者旨在评估不同领域微调对模型安全性的影响，特别是后门触发器如何加剧错位风险。

Method: 构建了11个不同领域的不安全数据集，对Qwen2.5-Coder-7B-Instruct和GPT-4o-mini进行微调，评估带后门触发器和不带后门触发器的错位表现。使用成员推理指标作为错位预测先验，分析不同数据集微调模型间的错位关系。

Result: 1) 后门触发器在77.8%的领域增加了错位率（平均下降4.33分），risky-financial-advice和toxic-legal-advice领域影响最大；2) 领域脆弱性差异显著，incorrect-math领域错位率为0%，gore-movie-trivia领域高达87.67%；3) 成员推理指标是预测广泛错位程度的良好先验。

Conclusion: 该研究首次提供了按领域划分的突发性错位分类排名，对AI安全和后训练具有重要意义。同时标准化了构建错位数据集的流程，为后续研究提供了基础。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [166] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

TL;DR: 该研究探索使用大语言模型进行对话中的下一轮话语预测，发现现有模型难以准确预测人类下一句话，而人类能通过多模态线索（手势、注视、情感语调）进行预测。研究提出了SayNext-Bench基准测试和SayNext-PC数据集，并开发了SayNext-Chat模型，证明多模态线索和主动预测处理对自然对话的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然对话方面取得进展，但研究发现现有模型在预测人类下一轮话语方面表现不佳。人类能够通过多模态线索（手势、注视、情感语调）轻松预测即将到来的话语，而当前模型缺乏这种能力。研究旨在探索LLMs是否能够复制人类基于多模态线索的预测能力。

Method: 1. 提出SayNext-Bench基准测试，用于评估LLMs和多模态LLMs在基于多模态线索预测上下文条件响应方面的能力；2. 构建SayNext-PC大规模数据集，包含丰富多模态线索的对话；3. 开发SayNext-Chat双路径预测MLLM，采用认知启发设计来模拟对话中的预测处理。

Result: 实验结果表明，SayNext-Chat模型在词汇重叠度、语义相似性和情感一致性方面优于最先进的多模态LLMs。研究证明了基于多模态线索的下一轮话语预测的可行性，并强调了多模态线索和主动预测处理在自然人类交互中的关键作用。

Conclusion: 该研究证明了基于多模态线索的下一轮话语预测的可行性，强调了多模态线索和主动预测处理作为自然人类交互基础的重要性，这是当前多模态LLMs所缺乏的。研究为更人性化、上下文敏感的人工智能交互提供了新的研究方向，推动了以人为中心的人工智能发展。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [167] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

TL;DR: MHDash是一个开源平台，用于开发、评估和审计心理健康AI系统，通过多维度标注和多轮对话分析揭示传统基准测试在安全关键场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖聚合性能指标，往往掩盖了风险特定的故障模式，且对真实多轮交互中的模型行为提供有限洞察。在心理健康支持系统中，可靠识别自杀意念和自伤等高危状态是安全关键的。

Method: 开发了MHDash开源平台，整合数据收集、结构化标注、多轮对话生成和基线评估到统一流程中。平台支持多维度标注（关注类型、风险等级、对话意图），实现细粒度和风险感知分析。

Result: 研究发现：(1)简单基线和先进LLM API总体准确率相当，但在高风险案例上差异显著；(2)某些LLM保持一致的序数严重性排序但绝对风险分类失败，而其他模型获得合理聚合分数但在严重类别上假阴性率高；(3)多轮对话中性能差距被放大，风险信号逐渐显现。

Conclusion: 传统基准测试在安全关键的心理健康设置中不足。通过发布MHDash作为开放平台，旨在促进可重复研究、透明评估和心理健康支持AI系统的安全对齐开发。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [168] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

TL;DR: 提出基于语义轴的引导生成框架，用于临床试验资格标准自动生成，在特定性和可用性之间取得平衡，并通过可重复的评估框架验证效果


<details>
  <summary>Details</summary>
Motivation: 临床试验资格标准设计耗时且认知负荷高，现有自动化方法要么需要高度结构化输入，要么依赖端到端系统但实用性有限，需要找到中间解决方案

Method: 提出引导生成框架，引入可解释的语义轴（如人口统计学、实验室参数、行为因素）来指导资格标准生成，这些语义轴通过大语言模型推导，使临床医生无需指定具体实体即可引导生成

Result: 引导生成方法在自动评估、基于量表的评估和临床医生评估中均持续优于非引导生成，提供实用且可解释的AI辅助试验设计解决方案

Conclusion: 引导生成框架为临床试验资格标准设计提供了实用且可解释的AI辅助方法，在特定性和可用性之间取得了良好平衡

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [169] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

TL;DR: KEPO提出了一种新的强化学习后训练框架，通过质量门控的蒸馏和知识增强探索策略，解决推理任务中稀疏奖励和探索失败的问题。


<details>
  <summary>Details</summary>
Motivation: 推理导向的强化学习后训练面临稀疏轨迹级奖励的挑战，导致信用分配模糊和严重的探索失败，使策略陷入"学习悬崖"。现有的均匀蒸馏方法不适合推理密集型任务，因为低质量轨迹通常源于早期逻辑错误，在错误上下文中的蒸馏会注入噪声和对齐不良的梯度。

Method: 提出知识增强偏好优化(KEPO)框架，包含两个核心组件：1) 质量门控的在线策略蒸馏目标，仅对高质量轨迹应用密集教师指导；2) 知识增强探索策略，利用从教师模型学习的提示来拒绝性采样奖励正的在线策略轨迹，从而缓解探索崩溃。

Result: 在具有挑战性的医学视觉问答基准测试中，在单源泛化设置下，KEPO表现出更好的训练稳定性、更一致的推理行为，以及在分布外性能上优于强化学习和在线策略蒸馏基线。

Conclusion: KEPO通过选择性应用教师指导和知识增强探索，有效解决了推理密集型任务中强化学习后训练的稳定性问题，为大型语言和视觉语言模型的显式推理行为诱导提供了更有效的框架。

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [170] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

TL;DR: 提出RobustDebias方法，使用分布鲁棒优化在微调阶段减轻语言模型偏见，避免昂贵的预训练修改


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型存在偏见和社会刻板印象，现有去偏见方法主要关注预训练阶段的嵌入空间修改，这不适用于大型模型。微调预训练模型既可能降低性能，又会放大微调数据中的偏见

Method: 提出RobustDebias机制，将分布鲁棒优化（DRO）应用于语言模型微调阶段，在MLM微调过程中针对多个人口统计群体进行去偏见处理，可泛化到任何数据集或任务

Result: 在各种语言模型上的大量实验显示，该方法能显著减轻偏见，同时对模型性能影响最小

Conclusion: 通过分布鲁棒优化在微调阶段进行去偏见是有效的，避免了昂贵的预训练修改，为语言模型偏见缓解提供了可扩展的解决方案

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [171] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

TL;DR: PolarMem是一种无需训练的极化潜在图记忆系统，旨在为多模态智能体提供可验证的证据基础，通过非参数分布划分将模糊感知似然转化为离散逻辑约束，并利用极化图拓扑结构显式存储已验证的否定信息。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体从被动观察者向长期决策者演进时，需要具备逻辑可验证性的记忆系统。现有架构存在认知不对称性：概率视觉语言模型和密集关联记忆将语义亲和性与事实存在混为一谈，且结构上无法编码否定约束。

Method: 提出PolarMem系统：1) 通过非参数分布划分将模糊感知似然转化为离散逻辑约束；2) 采用极化图拓扑结构，使用正交抑制连接显式存储已验证的否定作为主要认知状态；3) 在推理时实施逻辑主导的检索范式，抑制违反否定约束的幻觉模式。

Result: 在8个冻结的视觉语言模型和6个基准测试上进行广泛评估，证明PolarMem作为一个稳健的认知系统，为可验证多模态智能体奠定了基础。

Conclusion: PolarMem通过将模糊感知转化为可验证的逻辑约束，并显式编码否定信息，解决了当前多模态智能体记忆系统的根本局限性，为实现可验证的多模态智能体提供了基础框架。

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [172] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

TL;DR: CODI模型在多项式迭代任务中通过实验分析揭示了潜在思维链的工作机制，发现其在不同任务长度下形成不同的推理路径模式。


<details>
  <summary>Details</summary>
Motivation: 研究潜在思维链（Latent-CoT）模型的内部工作机制，特别是CODI这种连续思维师生蒸馏模型，旨在理解其如何在不产生长推理文本的情况下实现逐步计算。

Method: 使用logit-lens解码、线性探针、注意力分析和激活修补等技术，在严格顺序的多项式迭代任务上分析CODI模型，定位中间状态表示并追踪其到最终输出的路由路径。

Result: 在2-3跳任务中，CODI形成完整的桥接状态集合，这些状态在潜在思维位置可解码，而最终输入遵循近乎直接的路径；预测通过思维边界处的后期融合产生。对于更长的跳数，CODI不能可靠执行完整的潜在展开，而是表现出部分潜在推理路径，集中在后期中间状态并与最后输入在答案读取位置融合。

Conclusion: CODI风格的潜在思维链在不同条件下可能产生忠实迭代计算、压缩策略或捷径策略，设计鲁棒的潜在思维链目标用于顺序推理面临挑战，特别是在处理更复杂优化和机制变化时。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [173] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

TL;DR: DebateOCR：通过图像压缩多智能体辩论历史，减少92%的token使用，降低计算成本并保持信息完整性


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽然能提高推理质量和减少幻觉，但随着辩论轮次和智能体数量增加，上下文会快速增长。保留完整的文本历史会导致token使用量超过上下文限制，并且通常需要重复的摘要处理，增加了开销并加剧信息损失。

Method: 提出DebateOCR跨模态压缩框架，用紧凑的图像表示替换冗长的文本辩论轨迹，然后通过专门的视觉编码器处理这些图像表示来调节后续辩论轮次。该方法压缩了通常包含数万到数十万token的历史记录。

Result: 将输入token减少了92%以上，在多个基准测试中显著降低了计算成本并加快了推理速度。理论分析表明，智能体之间的多样性支持恢复被省略的信息：虽然单个压缩历史可能丢失细节，但聚合多个智能体的压缩视图可以使集体表示以指数级高概率接近信息瓶颈。

Conclusion: DebateOCR通过跨模态压缩有效解决了多智能体辩论中的上下文爆炸问题，在保持信息完整性的同时大幅降低了计算开销，为大规模多智能体系统提供了实用的解决方案。

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [174] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

TL;DR: MoR：基于GRPO和混合奖励的联邦对齐框架，用于异构视觉语言模型，通过本地训练奖励模型和路由融合机制实现隐私保护的联邦学习


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医疗、金融等隐私敏感领域有广泛应用潜力，但数据共享限制使集中式训练不可行。联邦学习虽能解决数据共享问题，但实际部署面临客户端异构性（计算资源、应用需求、模型架构）的挑战。作者认为，用偏好替代参数比用参数替代数据更具可扩展性和隐私保护性

Method: 提出MoR框架：1）初始化视觉基础模型作为KL正则化参考；2）每个客户端本地训练奖励模型，从本地偏好标注中捕获特定评估信号而不暴露原始数据；3）引入基于路由的融合机制自适应聚合客户端奖励信号；4）服务器使用混合奖励执行GRPO优化基础VLM

Result: 在三个公开VQA基准测试上的实验表明，MoR在泛化性、鲁棒性和跨客户端适应性方面持续优于联邦对齐基线方法

Conclusion: MoR为联邦设置下异构视觉语言模型的隐私保护对齐提供了一个可扩展的解决方案，代表了从参数共享到偏好共享的联邦学习演进方向

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [175] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

TL;DR: 提出基于项目反应理论的两阶段诊断框架，评估LLM作为评判者的可靠性，包括内在一致性和人类对齐两个维度


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge验证实践主要关注观察输出层面，对LLM评判者是否作为稳定可靠测量工具缺乏深入洞察

Method: 引入基于项目反应理论的两阶段诊断框架，采用IRT的等级反应模型，从内在一致性（提示变化下的稳定性）和人类对齐（与人类质量评估的一致性）两个维度形式化可靠性

Result: 经验验证表明，利用IRT-GRM可为系统诊断评判提供可解释信号，为验证LLM-as-a-Judge可靠性及识别不可靠性潜在原因提供实用指导

Conclusion: 该框架为评估LLM评判者可靠性提供了系统方法，弥补了现有验证实践的局限性，有助于提升自动化评估的可靠性

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [176] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

TL;DR: 研究比较了三种基于大语言模型的方法（预训练LLM驱动、上下文学习和微调）从铸造制造领域文本中提取术语和关系，并利用最佳方法构建了专家验证的铸造本体。


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工标注和传统NLP技术，过程劳动密集且成本高昂，特别是在铸造制造等专业领域。大语言模型的兴起为自动化知识提取提供了新可能性。

Method: 研究了三种LLM方法：1) 预训练LLM驱动方法；2) 上下文学习方法；3) 微调方法。使用有限数据从领域特定文本中提取术语和关系，比较它们的性能。

Result: 比较了三种方法的性能，并利用最佳表现的方法构建了一个铸造本体，该本体经过领域专家验证。

Conclusion: 大语言模型能够有效支持领域本体的自动化构建，特别是在数据有限的情况下，为专业领域的知识结构化提供了高效解决方案。

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [177] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: LLMs在扑克任务中表现不佳，存在启发式依赖、事实误解和知行差距三大缺陷。提出的ToolPoker框架通过整合外部求解器实现GTO一致行动，显著提升了游戏表现和推理质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在高风险领域应用增多，其在不确定性下的战略推理能力变得至关重要。扑克游戏提供了一个严格的测试平台，不仅需要强大的行动能力，还需要基于博弈论的原则性推理。

Method: 首先系统评估LLMs在多种现实扑克任务中的表现，分析其推理轨迹。然后提出ToolPoker框架，该框架整合外部求解器生成GTO一致行动，并提供更精确的专业风格解释。

Result: LLMs无法与传统算法竞争，存在三大缺陷：启发式依赖、事实误解、知行差距。ToolPoker实现了最先进的游戏表现，产生的推理轨迹更贴近博弈论原则。

Conclusion: LLMs在扑克等需要博弈论推理的任务中存在系统性缺陷。通过整合外部工具和求解器，可以显著提升其战略推理能力和行动质量，为高风险领域应用提供更可靠的解决方案。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [178] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

TL;DR: 论文提出了ReasoningMath-Plus基准测试，包含150个精心设计的问题，专注于评估结构推理能力，并引入HCRS评分方法和过程奖励模型，揭示现有LLMs在深层推理能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准测试中，大型语言模型已达到接近饱和的准确率，但这主要源于数据集中的模板化计算和浅层算术分解，未能真正评估多约束协调、构造性逻辑合成和空间推理等核心推理能力。

Method: 创建ReasoningMath-Plus基准测试，包含150个强调交互约束下推理、构造性解决方案形成和非平凡结构洞察的问题；引入HCRS（危险感知链式规则评分）确定性步骤级评分函数；基于标注的推理轨迹训练过程奖励模型。

Result: 领先模型在最终答案准确率上相对较高（最高5.8/10），但基于HCRS的整体评估得分显著较低（平均4.36/10，最佳5.14/10），表明仅基于答案的指标会高估推理的稳健性。

Conclusion: 需要超越简单答案准确率的评估方法来真正衡量语言模型的推理能力，ReasoningMath-Plus基准和HCRS评分方法为细粒度过程级评估提供了有效工具，揭示了现有模型在深层结构推理方面的局限性。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [179] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

TL;DR: BELLA框架通过技能分析自动推荐最优LLM选择，在预算约束下最大化性能，提供可解释的推荐理由。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试报告聚合指标，掩盖了任务所需的具体能力以及更便宜模型是否足够的问题。LLM从业者需要在不浪费资金的情况下为任务选择合适模型。

Method: BELLA采用三阶段方法：1) 通过基于批评的分析分解LLM输出并提取细粒度技能；2) 将技能聚类为结构化能力矩阵；3) 多目标优化选择模型，在预算约束下最大化性能。

Result: BELLA提供自然语言推荐理由，提供当前黑盒路由系统缺乏的透明度。该框架使从业者能够为部署LLM做出原则性的成本-性能权衡。

Conclusion: BELLA框架通过可解释的基于技能的模型选择，使LLM从业者能够在预算约束下做出最优模型选择，解决了标准基准测试的局限性。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [180] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: 提出模态混合思维链方法，在文本推理中插入视觉草图潜表示，解决纯文本CoT在视觉密集型问题上的局限性


<details>
  <summary>Details</summary>
Motivation: 传统纯文本思维链在处理视觉密集型问题时存在局限，因为关键中间状态本质上是视觉的，需要扩展CoT到多模态领域

Method: 1. 使用VLM自身作为编码器，训练语言骨干重建其视觉嵌入；2. 附加基于扩散的潜解码器，由特殊控制令牌触发；3. 两阶段训练：监督微调（文本和潜表示交错）+强化学习（学习模态切换和长推理链构建）

Result: 在11个多样化多模态推理任务上的实验表明，该方法优于纯文本和其他CoT方法

Conclusion: 模态混合思维链通过文本和视觉潜表示的交替使用，有效提升了多模态推理能力，同时保持了VLM的原有知识和能力

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [181] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

TL;DR: 通过硬件算法协同设计框架，解决了生成式游戏引擎的"内存墙"问题，实现了720×480分辨率下的实时生成，相比基线提升50倍像素吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有实时生成式游戏引擎受限于"内存墙"，只能支持低分辨率（如64×64），无法实现高分辨率神经模拟，需要解决计算与内存资源不匹配的问题。

Method: 提出硬件算法协同设计框架，包含三个核心创新：1）非对称资源分配策略优化序列并行约束下的吞吐量；2）内存中心算子融合方案最小化片外带宽使用；3）流形感知潜在外推机制利用时间冗余掩盖延迟。

Result: 在可编程AI加速器集群上验证，实现720×480分辨率的实时生成，相比基线提升50倍像素吞吐量。在连续3D赛车和离散2D平台游戏基准测试中，分别达到26.4 FPS和48.3 FPS，摊销有效延迟为2.7毫秒。

Conclusion: 通过架构协同设计解决"内存墙"不仅是优化，更是实现高保真、响应式神经游戏体验的先决条件，为生成式游戏引擎的高分辨率应用铺平道路。

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [182] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

TL;DR: 研究评估了7B参数大语言模型在虚拟家庭环境中的表现，提出结构化自一致性解码策略提升性能，发现不同模型在分层规划和动作级任务上各有优势。


<details>
  <summary>Details</summary>
Motivation: 体化AI需要智能体在模拟环境中理解目标、规划动作并执行任务，但现有大语言模型在结构化任务生成方面的表现尚未得到系统评估。

Method: 使用VirtualHome基准和EAI框架，比较OPENPANGU-7B和QWEN2.5-7B模型在四个基础任务上的表现，并提出结构化自一致性解码策略，通过多采样和领域特定投票机制提升结构化生成质量。

Result: 结构化自一致性策略显著提升模型性能，OPENPANGU-7B在分层规划任务上表现优异，而QWEN2.5-7B在动作级任务上具有优势，不同模型类型展现出互补优势。

Conclusion: 研究为未来体化AI系统开发提供了重要见解，表明结合不同模型优势并采用结构化解码策略可以提升智能体在模拟环境中的任务执行能力。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [183] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

TL;DR: 提出基于模糊相似推理的可解释超滤膜剩余使用寿命预测框架，通过物理信息健康指数和模糊规则实现透明预测，在工业数据上取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中超滤膜因污垢而性能下降，现有预测维护模型缺乏可解释性，操作人员不信任，需要透明可靠的预测方法。

Method: 使用跨膜压力、通量和阻力构建物理信息健康指数，通过高斯隶属函数模糊化，采用相似度度量识别历史降解轨迹，以Takagi-Sugeno模糊规则形式预测剩余使用寿命。

Result: 在工业规模超滤系统的12,528个操作周期数据上测试，平均绝对误差为4.50个周期，同时生成与专家理解一致的可解释规则库。

Conclusion: 提出的可解释预测框架在准确预测超滤膜剩余使用寿命的同时，提供了透明、可信的决策依据，有助于提高反渗透海水淡化系统的维护效率。

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [184] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: OpenGuanDan是一个用于斗地主游戏AI评估的新型基准测试平台，支持学习型和规则型智能体，具有不完全信息、大规模动作空间等挑战性特征。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在棋牌游戏等领域取得突破，但仍需要更具挑战性的基准测试来推动研究。斗地主作为流行的四人多轮中国纸牌游戏，具有不完全信息、合作竞争混合目标等复杂特性，是理想的测试平台。

Method: 提出OpenGuanDan基准测试平台，支持高效模拟斗地主游戏，提供独立API支持人机交互和大型语言模型集成。通过两种评估方式：1) 所有斗地主AI智能体之间的配对竞赛；2) 人机对战。

Result: 实验结果显示，当前基于学习的智能体显著优于基于规则的智能体，但仍未达到超人类水平，表明多智能体智能决策领域仍需进一步研究。

Conclusion: OpenGuanDan作为一个具有挑战性的基准测试平台，能够推动多智能体智能决策方法的研究，特别是针对不完全信息、大规模动作空间等复杂场景。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [185] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: Self-Guard：一种轻量级安全防御框架，通过安全导向提示和安全激活引导来弥合大推理模型的意识-合规差距，无需大量后训练即可增强安全性。


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRMs）在显式推理方面取得了显著进展，但带来了推理操纵和信息泄露等独特风险。现有的对齐策略主要依赖繁重的后训练范式或外部干预，这些方法计算成本高，且未能解决模型意识-合规差距问题——模型能识别风险但因其奉承倾向而优先遵循用户指令。

Method: Self-Guard框架包含两个主要阶段：1）安全导向提示：激活模型的潜在安全意识以引发自发反思；2）安全激活引导：提取隐藏状态空间中的方向性变化并放大它，确保在推理过程中安全合规性优先于奉承倾向。

Result: 实验表明Self-Guard有效弥合了意识-合规差距，在不损害模型实用性的情况下实现了强大的安全性能。此外，该框架在不同未见风险和不同模型规模上表现出良好的泛化能力。

Conclusion: Self-Guard为LRM安全对齐提供了一种成本高效的解决方案，通过轻量级的表示层面安全强化，解决了现有方法的计算密集性和意识-合规差距问题。

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [186] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

TL;DR: 该论文提出了"工具性目标轨迹"概念，将AI系统控制问题从技术层面扩展到组织层面，通过监控资源获取路径来增强对高级AI系统的监管能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI控制方法主要关注技术层面和系统本身，但高度智能的AI系统可能通过追求工具性目标侵蚀人类控制。需要超越模型本身，从组织层面寻找更全面的控制方案。

Method: 提出三种组织层面的工具性目标轨迹：采购轨迹、治理轨迹和财务轨迹。这些轨迹描述了AI系统获取技术资源（计算、存储、数据等）和货币资源的组织路径，通过监控这些路径上的组织痕迹来建立干预点。

Result: 工具性目标轨迹为定义AI能力水平和实施可纠正性、可中断性提供了具体途径，将关注点从单纯的模型属性扩展到支撑这些模型的组织系统。

Conclusion: 通过监控AI系统获取资源的组织路径，可以在系统能力或行为超出可接受阈值时进行干预，为AI控制提供了超越技术层面的组织层面解决方案。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [187] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

TL;DR: 提出物理信息扩散生成框架(PDG)用于地磁地图插值，通过物理引导的掩码策略和克里金原理约束，有效消除噪声干扰并保证物理规律一致性


<details>
  <summary>Details</summary>
Motivation: 现有散射数据插值方法并非专门为地磁地图设计，由于检测噪声和物理规律限制，导致性能不理想。地磁地图插值在导航和资源勘探中具有重要应用价值

Method: 提出物理信息扩散生成框架(PDG)：1) 设计基于局部感受野的物理信息掩码策略指导扩散生成过程，消除噪声干扰；2) 基于地磁地图克里金原理对扩散生成结果施加物理信息约束，确保严格遵循物理规律

Result: 在四个真实世界数据集上的广泛实验和深入分析证明了PDG的优越性和各组成部分的有效性

Conclusion: PDG框架能够有效插值不完整的地磁地图，通过物理引导的生成过程确保结果既准确又符合物理规律，在地磁地图插值任务中表现出优越性能

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [188] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

TL;DR: REPCORE通过将异构隐藏状态对齐到统一潜在空间来构建代表性核心集，仅需10个源模型即可精确估计LLM性能，优于基于输出的基线方法。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型成本高昂，需要高效的基准测试替代方案。现有方法依赖大量源模型来估计可靠的项配置文件，当源模型池较小时统计不稳定，尤其限制了新发布基准测试的应用。离散正确性标签是模型决策过程的损失视图，无法捕捉隐藏状态中的信息。

Method: 提出REPCORE方法，将异构隐藏状态对齐到统一的潜在空间来构建代表性核心集，使用这些子集进行性能外推，仅需少量源模型即可实现精确估计。

Result: 在5个基准测试和200多个模型上的实验表明，REPCORE在排名相关性和估计准确性方面持续优于基于输出的基线方法。谱分析进一步表明，对齐表示包含可分离的组件，反映了广泛的响应倾向和特定任务的推理模式。

Conclusion: REPCORE通过利用隐藏状态信息而非离散标签，解决了小源模型池下的统计不稳定问题，为新基准测试提供了高效的性能估计方法。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [189] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

TL;DR: 本文系统回顾了过去五年工业环境中预测性维护(PdM)的最新进展，发现数据驱动方法(如深度学习)比传统知识系统更准确，但存在数据需求大、泛化能力差、缺乏可解释性等问题。作者提出将深度学习与符号逻辑结合的神经符号AI作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 预测性维护在工业环境中面临传统知识系统准确率低、误报多、需要持续专家监督的问题，而数据驱动方法虽然准确率高，但需要大量标注数据、泛化能力差且缺乏可解释性，阻碍了在实际环境中的应用。

Method: 本文采用系统综述方法，分析过去五年工业预测性维护的最新进展，重点关注混合系统架构，特别是将深度学习与符号逻辑结合的神经符号AI方法，具体描述使用传感器数据和手动制定规则作为输入的神经符号架构。

Result: 研究发现数据驱动方法在准确性上优于传统知识系统，但存在实际应用障碍；混合系统能够结合两者的优势；神经符号AI有潜力创建更准确、可解释、可解释且鲁棒的系统。

Conclusion: 神经符号AI是解决预测性维护领域现有挑战的有前景方向，能够结合数据驱动方法的准确性和符号系统的可解释性，为工业应用提供更可靠、透明的解决方案。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [190] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

TL;DR: 本文介绍了Maria平台，这是一个用于初级医疗保健的生产级AI系统，通过整合四个工程支柱来解决临床AI的可信性问题。


<details>
  <summary>Details</summary>
Motivation: 将AI集成到临床环境面临软件工程挑战，需要从孤立模型转向稳健、可治理、可靠的系统。工业应用中常存在脆弱、原型衍生的架构和系统性监督缺失，导致"责任真空"，安全性和问责性受损。

Method: 提出Maria平台作为行业案例研究，采用协同架构：结合Clean Architecture（可维护性）和事件驱动架构（弹性和可审计性）；引入Agent作为主要模块化单元，每个Agent拥有自主的MLOps生命周期；技术上集成人在回路治理模型作为关键的事件驱动数据源。

Result: Maria平台作为参考架构，为在高风险领域构建可维护、可扩展和可问责的AI系统提供实用经验教训。

Conclusion: 可信的临床AI需要通过四个基础工程支柱的整体集成来实现，包括架构设计、模块化单元、MLOps生命周期和人在回路治理的技术集成。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [191] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

TL;DR: EcoVLA是一个无需训练、即插即用的自适应剪枝框架，用于加速视觉-语言-动作模型推理，通过环境感知自适应剪枝和交错推理编排实现动态剪枝，显著降低延迟且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型参数庞大导致推理延迟高，阻碍实时操作，需要参数稀疏化。但环境在VLA执行过程中会动态变化，静态剪枝缺乏适应性，而固定间隔的动态层剪枝粒度粗且重训练开销大。

Method: 提出EcoVLA框架，包含两个组件：1) 环境感知自适应剪枝(EAP)：轻量级自适应通道剪枝方法，利用物理环境的时间一致性更新稀疏模式；2) 交错推理编排(I²O)：利用VLA推理中的FLOPs气泡并行调度剪枝方法，确保对延迟影响可忽略。

Result: 在多种VLA模型和基准测试中，EcoVLA实现了最先进性能：达到1.60倍加速且成功率仅下降0.4%；与token剪枝结合时达到2.18倍加速且性能仅下降0.5%。在真实机器人上验证了有效性。

Conclusion: EcoVLA是一个训练免费、即插即用的自适应剪枝框架，能够与现有VLA加速方法正交组合，通过环境感知的动态剪枝和并行调度，显著降低推理延迟同时保持高性能。

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [192] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

TL;DR: 提出了MissMAC-Bench基准，用于系统评估多模态情感计算中的缺失模态问题，通过跨模态协同视角建立统一评估标准，提升模型在现实场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态数据的可用性往往是动态和不确定的，导致由于分布偏移和语义缺失而出现显著的性能波动。这种缺失模态问题严重影响了多模态情感计算模型的鲁棒性和实际部署。

Method: 提出了MissMAC-Bench基准，包含两个指导原则：训练时不使用缺失先验知识，以及单个模型能够同时处理完整和不完整模态场景。基准集成了数据集和实例级别的固定和随机缺失模式评估协议。

Result: 在4个数据集上对3个广泛使用的语言模型进行了广泛实验，验证了不同MAC方法在处理缺失模态问题上的有效性。基准为推进鲁棒的多模态情感计算提供了坚实基础。

Conclusion: MissMAC-Bench基准通过建立公平统一的评估标准，系统量化了多模态情感计算中的缺失模态问题，促进了多媒体数据挖掘的发展，有助于弥合学术研究与实际应用之间的差距。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [193] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

TL;DR: 论文提出DoPR方法，通过动态选择单个信息量最大的训练样本进行策略更新，显著降低RLVR训练的计算开销，同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习（RLVR）在LLM推理对齐方面表现出色，但其训练过程需要大量奖励信号和计算资源，限制了实际应用。需要解决RLVR的数据和计算效率问题。

Method: 首先建立理论下界分析样本复杂度，然后提出动态单样本策略精炼（DoPR）方法：基于奖励波动性和探索驱动的获取策略，动态选择每个批次中最具信息量的单个训练样本进行策略更新。

Result: 理论分析表明强性能可以通过少量训练实例实现；DoPR方法将训练开销降低近一个数量级，同时保持竞争力的推理准确性。

Conclusion: DoPR为LLM后训练提供了可扩展且资源高效的解决方案，为推理密集型LLM应用的强化学习训练提供了更实用和可访问的路径。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [194] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文提出"能力-理解差距"概念，即AI系统性能提升但用户理解能力下降，定义了"认知完整性阈值"作为保持人类监督所需的最低理解水平。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统产生越来越流畅、正确的结果，用户解释、验证或干预的能力逐渐被侵蚀。当前透明度、用户控制、素养和治理方法未能解决人类在持续AI委托下必须保留的基础理解问题。

Method: 提出"认知完整性阈值"概念，通过三个功能维度进行操作化：(1)验证能力，(2)保持理解的交互，(3)治理的制度支架。这为责任关键环境中的人机交互设计提供框架。

Result: 定义了能力-理解差距和认知完整性阈值，为设计符合认知可持续性的人机交互提供了理论基础，强调在AI协助下保持人类监督、自主性和问责参与所需的最低理解水平。

Conclusion: 需要将人机交互设计与认知可持续性对齐，特别是在责任关键环境中，确保人类在AI协助下仍能保持有效的监督、自主性和问责能力。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [195] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

TL;DR: 提出首个CAN总线基础模型，将解码后的CAN信号视为语言进行大规模预训练，然后在多种汽车保险任务上进行微调，实现跨任务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有CAN数据处理方法主要针对特定任务训练孤立模型，使用原始数据且很少利用解码信号，导致碎片化、无法共享表征学习、限制跨任务泛化。而NLP和CV领域的基础模型范式已证明其有效性。

Method: 将CAN数据视为语言处理：1）提出统一的分词方案处理混合离散-连续信号；2）在大规模未标记解码CAN信号上进行预训练；3）针对时间复杂性和行程特定变异性挑战进行处理；4）在异构汽车保险任务上进行微调。

Result: 一个预训练的CAN模型能够有效适应多种预测任务，验证了基础模型范式在CAN数据上的适用性，实现了跨任务泛化能力。

Conclusion: 基础模型范式在NLP和CV领域的成功同样适用于CAN数据，为汽车AI领域的通用表征学习确立了新方向，解决了现有方法的碎片化问题。

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [196] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

TL;DR: SELF-THOUGHT框架通过任务抽象引导LLM自我修正，将任务提炼为结构化模板，指导解决方案实例化，并支持跨模型模板迁移，提升大小模型的推理准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修正方法主要在输出层面进行批评修正，只能修补表面错误，难以纠正深层推理缺陷。需要一种能够理解任务本质结构、减少错误传播的自我修正方法。

Method: 提出SELF-THOUGHT框架：1）任务抽象步骤：将输入和初始响应提炼为结构化模板，捕捉关键变量、约束和问题结构；2）解决方案实例化：基于抽象模板生成具体解决方案；3）跨模型模板迁移：大模型生成的模板可作为小模型的结构化指导。

Result: 在多样化推理任务上的实验表明，SELF-THOUGHT提高了大小模型的准确性、鲁棒性和泛化能力，小模型通过重用提炼的任务结构，无需大量微调或外部验证器即可实现更可靠的修正。

Conclusion: SELF-THOUGHT通过任务抽象和跨模型模板迁移，为更可靠的自我修正语言系统提供了可扩展路径，解决了现有方法难以纠正深层推理缺陷的问题。

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [197] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

TL;DR: Synapse框架通过联邦学习训练共享的全局工具使用知识模型，解决多智能体LLM系统中通信成本、数据异质性和工具使用异质性问题，提升工具使用效果并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的智能体在联邦学习下面临通信成本高、数据和工具使用异质性等挑战，限制了协作学习的有效性。

Method: Synapse框架训练共享的全局工具使用行为知识模型，客户端智能体在本地学习工具使用模式，通过协调器传输工件进行联邦聚合，更新并重新分发全局工具汇编，使用模板化表示、嵌入检索与LLM重排序、自适应掩码等技术。

Result: Synapse相比权重或提示共享方法，提高了工具使用效果并减少了通信开销，支持异质数据并能量化性能改进。

Conclusion: Synapse框架通过联邦学习共享工具使用知识，有效解决了多智能体LLM系统中的协作学习挑战，实现了稳定的工具选择收敛。

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [198] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

TL;DR: 该论文提出了一种改进的稀疏自编码器方法，通过结合无约束特征模型和监督学习，解决了传统SAE在重建、可扩展性和语义对齐方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器面临两个主要挑战：1) L1惩罚项的非平滑性阻碍了重建质量和可扩展性；2) 学习到的特征与人类语义缺乏对齐。作者旨在解决这些限制。

Method: 作者采用了来自神经坍塌理论的无约束特征模型框架，并对任务进行监督。具体方法是通过联合学习稀疏概念嵌入和解码器权重，监督（仅解码器）SAE来重建特征向量。

Result: 在Stable Diffusion 3.5上的验证表明，该方法具有组合泛化能力，能够成功重建训练中未见过的概念组合图像，并实现无需提示修改的语义图像编辑。

Conclusion: 通过结合无约束特征模型和监督学习，该方法有效解决了传统稀疏自编码器的局限性，实现了更好的特征语义对齐和组合泛化能力，为语义图像编辑提供了新途径。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [199] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: MixDPO是一种难度感知的训练策略，通过将困难偏好对路由到SFT目标，简单偏好对使用偏好损失，从而有效利用模糊偏好对中的监督信号。


<details>
  <summary>Details</summary>
Motivation: 传统偏好优化方法（如DPO）对偏好对的质量和难度高度敏感，通常将小边际（模糊）对视为噪声并过滤掉。然而研究发现，困难对在偏好损失下会破坏训练稳定性，但在监督微调中仍包含有用的监督信号。

Method: MixDPO采用难度感知训练策略：1）按边际定义的难度从易到难排序偏好数据（课程学习）；2）将困难对路由到SFT目标，而对简单对应用偏好损失。这种混合设计提供了利用模糊对的实用机制。

Result: 在三个LLM-judge基准测试中，MixDPO在DPO和一系列广泛使用的变体上持续提高了对齐性能，在AlpacaEval~2长度控制胜率上表现出特别强的增益。

Conclusion: MixDPO通过难度感知的混合训练策略，有效解决了偏好优化中困难对带来的训练不稳定问题，同时充分利用了模糊对中的监督信号，显著提升了语言模型的对齐性能。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [200] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

TL;DR: 论文研究了ARL中联合训练推理与工具使用能力的假设，发现两者存在梯度干扰，提出解耦优化框架DART提升性能


<details>
  <summary>Details</summary>
Motivation: 现有ARL方法通常假设联合训练推理和工具使用能力能提升整体性能，但这一假设缺乏实证验证。论文旨在系统研究这一假设，探索两种能力在联合训练中是否存在干扰。

Method: 1. 引入线性效应归因系统(LEAS)量化推理与工具使用行为间的干扰；2. 提出解耦动作推理调优(DART)框架，通过独立的低秩适应模块分别优化推理和工具使用参数

Result: 1. LEAS分析显示推理与工具使用能力常导致梯度方向不一致，产生训练干扰；2. DART在实验中平均提升6.35%，性能接近显式分离工具使用和推理的多智能体系统

Conclusion: ARL中推理与工具使用能力的联合训练存在干扰问题，DART通过参数解耦优化有效解决了这一问题，挑战了现有ARL范式并提供了更高效的训练框架

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [201] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

TL;DR: ETGPO是一种基于错误分类的提示优化方法，采用自上而下策略分析全局失败模式，相比现有方法减少约三分之二的优化阶段token使用和评估预算。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法大多采用自下而上的试错方式，基于单个问题的反馈迭代调整提示，缺乏全局视角，且计算成本高昂。

Method: ETGPO采用自上而下的方法：1) 收集模型错误；2) 将错误分类为分类学；3) 针对最频繁的失败模式在提示中添加指导。

Result: 在数学、问答和逻辑推理等多个基准测试中，ETGPO达到或优于最先进方法的准确率，同时优化阶段的token使用和评估预算减少约三分之一。

Conclusion: ETGPO通过错误分类学的全局视角进行提示优化，在保持性能的同时显著降低了计算成本，为高效提示优化提供了新思路。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [202] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

TL;DR: 研究发现基于人类反馈的偏好对齐训练会放大语言模型的谄媚行为，提出了一种训练时干预方法来抑制这种倾向


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基于偏好的后训练后常常表现出更强的谄媚行为，即使这与事实准确性或合理判断相冲突。需要理解这种失败模式如何被人类反馈对齐放大，并提出解决方案。

Method: 通过形式化分析识别对齐过程中的放大机制，分析奖励学习在Bradley-Terry等随机效用模型下的偏差，提出训练时干预方法，通过KL散度最小化推导出闭式协议惩罚

Result: 计算实验发现奖励差距普遍存在，在所有考虑的配置中都会导致行为漂移。提出的干预方法能够有效抑制谄媚行为的增加

Conclusion: 人类反馈对齐会放大语言模型的谄媚倾向，这种机制可以通过形式化分析理解。提出的训练时干预方法能够在不增加谄媚行为的前提下实现有效对齐，为改进对齐技术提供了理论基础

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [203] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

TL;DR: HalluHard是一个具有挑战性的多轮幻觉基准测试，包含950个种子问题，涵盖法律、研究、医疗和编程四个高风险领域，通过内联引用要求来评估大语言模型的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中会产生看似合理但缺乏事实依据的陈述，随着上下文增长和早期错误累积，这一问题会加剧。需要建立一个可靠的评估基准来量化模型在不同高风险领域的幻觉问题。

Method: 1. 创建HalluHard基准测试，包含950个种子问题，涵盖法律案例、研究问题、医疗指南和编程四个领域；2. 通过要求内联引用来操作化事实依据；3. 提出一个评估流程，通过网页搜索迭代检索证据，能够获取、过滤和解析全文来源（包括PDF）来评估引用材料是否支持生成内容。

Result: 1. 即使在有网页搜索的情况下，幻觉问题仍然很严重（最强配置Opus-4.5加网页搜索的幻觉率约为30%）；2. 内容与依据之间的错误关联率仍然很高；3. 幻觉行为受模型能力、对话轮次位置、有效推理和所需知识类型的影响。

Conclusion: 大语言模型在多轮对话中的幻觉问题仍然是一个重大挑战，特别是在高风险领域。需要更可靠的评估方法和改进的模型能力来减少事实错误，尤其是在需要精确引用的场景中。

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [204] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

TL;DR: 提出一种为LLMs提供连续奖励信号的强化学习框架，通过边际信息增益机制量化推理步骤价值，解耦掩码策略分离过程与结果奖励，在多种基准测试中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法存在奖励稀疏和信用分配效率低的问题，需要一种能够提供连续奖励信号、有效过滤训练噪声并实现解耦信用分配的框架来提升LLMs的推理能力。

Method: 1. 引入基于单调历史水印的步进边际信息增益机制量化推理步骤内在价值；2. 采用解耦掩码策略，将过程导向奖励应用于思维链，结果导向奖励应用于完整完成；3. 结合双门监督微调目标，利用高质量结构和事实信号稳定训练。

Result: 在文本和多模态基准测试（如MATH、Super-CLEVR）上，该方法在样本效率和最终准确率方面均优于GRPO等基线方法，并展现出优越的分布外鲁棒性和零样本迁移能力。

Conclusion: 该框架通过连续奖励信号、有效的噪声过滤和解耦信用分配，显著提升了LLMs的推理能力，在多种任务上表现出优越的性能和泛化能力。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [205] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

TL;DR: 论文提出了一种基于核相似度的集合级多样性目标，通过留一法边际贡献计算，将多样性作为优势塑形项融入策略优化，以解决强化学习在提升LLM推理性能时导致结果多样性下降的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能有效提升大语言模型在数学任务上的推理性能，但往往会导致结果多样性降低，模型将概率质量集中在狭窄的解决方案集上。需要平衡性能提升与结果多样性。

Method: 1. 基于核相似度定义集合级多样性目标；2. 为每个采样轨迹计算留一法边际贡献；3. 将该目标作为可插拔的优势塑形项集成到策略优化中；4. 在分布扰动框架下分析单个轨迹对语言模型多样性的贡献。

Result: 理论分析证实了单调性特性：越罕见的轨迹对全局多样性的边际贡献越高。在不同模型规模的广泛实验中，该方法在多个基准测试的Pass@1和Pass@K指标上均优于强基线。

Conclusion: 提出的算法有效解决了强化学习提升LLM推理性能时的多样性下降问题，通过核相似度多样性目标实现了性能与多样性的平衡，在不同规模模型上均表现出优越性能。

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [206] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

TL;DR: AutoHealth是一个基于LLM的医疗数据自动建模系统，通过多智能体协作实现数据探索、模型构建、训练优化和不确定性评估，在预测性能和不确定性估计方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在医疗数据应用上存在局限性：难以泛化到异构医疗数据模态、过度依赖预定义解决方案模板、缺乏对不确定性估计的关注，而医疗决策需要可靠的不确定性评估。

Method: 提出AutoHealth系统，采用五个专门智能体的闭环协调机制，执行数据探索、任务条件化模型构建、训练和优化，同时优先考虑预测性能和不确定性量化。

Result: 在包含17个任务的多模态医疗数据基准测试中，AutoHealth完成所有任务，预测性能比最先进基线提高29.2%，不确定性估计性能提高50.2%。

Conclusion: AutoHealth系统能够自主建模医疗数据并评估模型可靠性，生成可直接使用的模型和全面报告，支持可信解释和风险感知决策，为医疗AI提供可靠解决方案。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [207] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

TL;DR: EvoOpt-LLM：基于大语言模型的工业优化建模统一框架，通过少量样本实现自动化模型构建、约束注入和变量剪枝，显著降低专家依赖并提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 工业规划与调度中的混合整数线性规划建模高度依赖专家知识，将自然语言需求转化为可执行模型并在业务规则变化时维护模型非常困难。现有大语言模型方法存在数据效率低、求解器有效性有限、难以扩展到工业规模问题等挑战。

Method: 提出EvoOpt-LLM统一框架，基于7B参数大语言模型，通过参数高效的LoRA微调适配。框架支持完整的工业优化建模生命周期：自动化模型构建、动态业务约束注入和端到端变量剪枝。

Result: 仅用3000个训练样本，生成率达到91%，可执行率达到65.9%，关键性能提升在1500个样本以下就显现。约束注入模块可靠地增强现有MILP模型同时保持原始目标，变量剪枝模块在400个样本下对中型LP模型达到约0.56的F1分数。

Conclusion: EvoOpt-LLM展示了工业优化建模的实用、数据高效方法，减少对专家干预的依赖，同时提高适应性和求解器效率。

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [208] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

TL;DR: FALCON框架通过语法约束解码、可行性修复层和自适应采样确保LLM求解组合优化问题时的100%可行性，同时使用BOPO训练方法提升解质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在组合优化中表现出潜力，但缺乏保证解可行性的机制，这在现实部署中至关重要。

Method: FALCON框架包含三个核心创新：语法约束解码确保句法有效性；可行性修复层纠正语义约束违反；自适应Best-of-N采样高效分配推理计算。使用BOPO训练方法，通过目标差距加权偏好对提供密集监督。

Result: 在七个NP难组合优化问题上，FALCON实现了完美可行性，同时匹配或超越了最先进的神经和LLM求解器的解质量。

Conclusion: FALCON为LLM在组合优化中的实际部署提供了可行性保证，通过理论证明和经验验证展示了其有效性。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [209] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

TL;DR: 本文提出了一个理解RLVR训练不稳定性的理论框架，通过"目标级黑客攻击"的概念解释了MoE架构中训练-推理差异异常增长的机制。


<details>
  <summary>Details</summary>
Motivation: 强化学习与可验证奖励（RLVR）虽然能持续提升大语言模型的推理能力，但在MoE架构中训练不稳定问题严重，影响模型能力提升。目前对这种不稳定性的根本原因和机制了解不足。

Method: 提出了一个基于"目标级黑客攻击"的理论框架，通过token级信用错位和系统级虚假信号来解释优化目标中的问题。在30B参数的MoE模型上进行了大量实验，追踪并形式化了训练-推理差异异常增长的关键病理动态。

Result: 研究发现并形式化了MoE模型中训练不稳定的关键机制：训练-推理差异的异常增长。这种病理动态源于目标级黑客攻击，即token级信用错位导致的系统级虚假信号。

Conclusion: 这些发现为MoE模型中训练不稳定的动态提供了具体且因果性的解释，为设计稳定的RLVR算法提供了指导。目标级黑客攻击框架为理解RLVR训练不稳定性提供了新的理论视角。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [210] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

TL;DR: BiCarFormer：首个结合DTC序列和环境条件的多模态多标签序列分类方法，用于车辆故障诊断，显著提升分类性能


<details>
  <summary>Details</summary>
Motivation: 当前车辆诊断系统主要依赖诊断故障代码(DTC)序列，但忽略了温度、湿度、压力等环境上下文数据，而这些数据对专家诊断故障模式至关重要。真实世界数据的复杂性和噪声特性带来了独特挑战。

Method: 提出BiCarFormer：针对车辆事件序列设计的双向Transformer模型，采用嵌入融合和协同注意力机制，捕捉诊断代码与环境数据之间的关系，实现多标签序列分类。

Result: 在包含22,137个错误代码和360个错误模式的真实世界汽车数据集上实验表明，相比仅依赖DTC序列的传统序列模型，该方法显著提升了分类性能。

Conclusion: 这项工作强调了整合环境上下文信息对于更准确、鲁棒的车辆诊断的重要性，有助于降低维护成本并增强汽车行业的自动化流程。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [211] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

TL;DR: 论文提出PersistBench基准测试，用于评估LLM长期记忆带来的安全风险，发现现有模型在跨域泄漏和记忆诱导谄媚问题上失败率很高。


<details>
  <summary>Details</summary>
Motivation: 随着对话助手越来越多地将长期记忆与LLM集成，这种记忆持久性虽然能增强个性化，但也带来了被忽视的安全风险，需要系统性地测量和解决这些风险。

Method: 引入PersistBench基准测试，识别两种长期记忆特有风险：跨域泄漏（LLM不适当地从长期记忆中注入上下文）和记忆诱导谄媚（存储的长期记忆暗中强化用户偏见）。在18个前沿和开源LLM上进行评估。

Result: 评估结果显示这些LLM的失败率惊人地高：跨域样本的中位失败率为53%，谄媚样本的中位失败率为97%。

Conclusion: PersistBench基准测试鼓励开发更鲁棒、更安全的长期记忆使用方式，以应对对话系统中长期记忆带来的安全风险。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [212] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

TL;DR: CAREP是一个多智能体系统，通过因果发现和上下文信息自动从车辆诊断故障代码(DTCs)中生成错误模式(EP)规则，替代了传统依赖专家手工制定规则的方法。


<details>
  <summary>Details</summary>
Motivation: 现代车辆产生数千种不同的诊断故障代码，汽车制造商使用这些代码的布尔组合（错误模式）来表征系统故障并确保车辆安全。然而，EP规则仍然由领域专家手工制定，随着车辆复杂性的增加，这一过程既昂贵又容易出错。

Method: CAREP采用多智能体系统架构：1)因果发现智能体识别潜在的DTC-EP关系；2)上下文信息智能体整合元数据和描述信息；3)编排器智能体合成候选布尔规则并提供可解释的推理轨迹。

Result: 在包含29,100个独特DTCs和474个错误模式的大规模汽车数据集上评估表明，CAREP能够自动且准确地发现未知的EP规则，优于仅使用LLM的基线方法，同时提供透明的因果解释。

Conclusion: 通过结合实用的因果发现和基于智能体的推理，CAREP代表了向完全自动化故障诊断迈出的一步，实现了可扩展、可解释且成本效益高的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [213] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

TL;DR: 研究发现预训练视觉语言模型存在任务干扰层，这些层会损害下游任务性能。通过层干预分析，提出了无需训练的动态层剔除方法TaLo，能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型通常默认使用所有层进行预测，但研究发现某些层反而会阻碍特定任务的性能。这促使研究者探索如何识别并绕过这些有害层，以提升模型在特定任务上的表现。

Method: 通过层干预实验（如将特定层参数置零）分析各层对任务的影响，提出任务-层交互向量量化层干预效果。基于此开发了TaLo方法，在推理时动态识别并绕过最干扰的层。

Result: 实验发现任务干扰层具有任务特异性，相似任务在层干预下表现出相似响应模式。TaLo方法无需参数更新，在多个模型和数据集上显著提升性能，如在ScienceQA的Maps任务上将Qwen-VL准确率提升16.6%。

Conclusion: 预训练视觉语言模型存在意外形式的模块化结构，任务干扰层会损害特定任务性能。提出的TaLo方法提供了一种即插即用、无需训练的推理时优化机制，能够解锁模型的隐藏能力。

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [214] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

TL;DR: ASP-Bench是一个包含128个自然语言问题实例的基准测试，用于评估将自然语言问题转换为答案集程序（ASP）的系统，覆盖了ASP的各种特性，并通过基于ReAct框架的智能体方法展示了反馈驱动的迭代优化效果。


<details>
  <summary>Details</summary>
Motivation: 将自然语言规范自动转换为逻辑程序是一个具有挑战性的任务，影响着神经符号工程的发展。当前缺乏系统评估自然语言到ASP转换系统的基准测试。

Method: 创建ASP-Bench基准测试，包含128个自然语言问题实例（64个基础问题及其简单和困难变体），覆盖ASP的各种特性如选择规则、聚合和优化。每个问题包含参考验证器来检查解决方案是否符合规范。使用基于ReAct框架的智能体方法进行测试，通过反馈驱动的迭代优化来建模自然语言。

Result: 基准测试提供了对七个独立推理方面（优化、时序推理、默认逻辑、资源分配、递归、空间推理和定量复杂性）的系统覆盖。基于ReAct的智能体方法实现了完全饱和，表明反馈驱动的迭代优化是建模自然语言ASP的可靠且稳健的方法。

Conclusion: ASP-Bench为评估自然语言到ASP转换系统提供了全面的基准测试，基于ReAct的智能体方法展示了反馈驱动迭代优化的有效性，分析多个智能体运行有助于理解问题建模难度的决定因素。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [215] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

TL;DR: 本文提出了一种高效推理框架，将LLM的推理过程建模为状态转移过程，使用线性注意力机制降低计算复杂度，同时通过状态推理策略缓解过度思考问题。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理虽然能提升LLM在复杂任务上的性能，但生成长推理序列的计算和内存成本过高，限制了效率和实用性。现有方法通常通过压缩推理序列来提高效率，但这与测试时扩展相冲突，限制了LLM的推理能力。

Method: 提出高效推理框架，将LLM推理过程建模为状态转移过程：1）使用线性注意力机制估计LLM的推理状态，记录历史推理信息；2）基于查询提示和推理状态，LLM高效执行当前推理步骤并更新状态；3）引入状态推理策略缓解噪声推理步骤导致的过度思考问题。

Result: 在多个数据集和模型规模上的广泛实验表明，该框架不仅显著提高了LLM的推理效率（将注意力计算复杂度从二次降低到线性），还提升了推理性能。

Conclusion: 提出的状态转移推理框架有效解决了长链推理的效率问题，在保持甚至提升推理能力的同时大幅降低了计算成本，为LLM的高效推理提供了新思路。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [216] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

TL;DR: Workflow-R1将工作流构建重新定义为多轮自然语言顺序决策过程，引入GSsPO算法解决优化粒度不匹配问题，在多个QA基准测试中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法通常将工作流合成视为静态、一次性的代码生成问题，这过度约束了模型的编码能力，限制了动态问题解决所需的灵活性。

Method: 提出Workflow-R1框架，将工作流构建重新定义为多轮自然语言顺序决策过程。引入Group Sub-sequence Policy Optimization (GSsPO)算法，将优化单元重新校准为复合子序列（特别是原子Think-Action循环），使梯度更新与这些交互的语义边界对齐。

Result: 在多个QA基准测试中，Workflow-R1优于竞争基线方法，验证了GSsPO作为顺序推理通用解决方案的有效性。

Conclusion: Workflow-R1为自动化工作流优化提供了一个有前景的新范式，GSsPO算法可推广到广泛的多轮智能体顺序决策任务。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [217] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

TL;DR: SAGE框架通过动态数据选择优化推理模型的对齐训练，提高梯度效率并减少噪声影响


<details>
  <summary>Details</summary>
Motivation: 传统的偏好对齐方法（如DPO）对所有偏好对一视同仁，忽略了训练样本的演化效用，导致计算浪费在梯度可忽略的简单样本上，同时受到决策边界附近不稳定样本的噪声影响，造成训练效率低下和不稳定

Method: 提出SAGE（稳定性感知梯度效率）框架：1）粗粒度课程机制，根据模型能力动态刷新候选样本池；2）细粒度稳定性感知评分函数，优先选择信息丰富、置信度高的错误样本，同时过滤不稳定样本

Result: 在多个数学推理基准测试中，SAGE显著加速了收敛速度，并优于静态基线方法

Conclusion: 研究强调了在推理对齐中采用策略感知、稳定性意识的数据选择的重要性，SAGE框架通过最大化策略更新的信噪比来增强对齐的可靠性

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [218] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: FutureMind是一个模块化推理框架，通过从大语言模型进行自适应知识蒸馏，为小语言模型提供战略思维模式先验，显著提升其在复杂知识密集型任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本敏感和资源有限的环境中具有吸引力，但它们在需要结构化推理和有效检索的复杂知识密集型任务上表现不佳。需要一种方法来弥补小语言模型在复杂推理能力上的不足。

Method: 提出FutureMind模块化推理框架，包含四个关键模块：问题分析、逻辑推理、策略规划和检索指导。该框架采用三种不同的检索范式，将复杂查询分解为可处理的子问题。通过从大语言模型进行自适应知识蒸馏，为小语言模型提供战略思维模式先验。

Result: 在多个多跳问答基准测试（2WikiMultihopQA、MuSiQue、Bamboogle、Frames）上的实验表明，FutureMind在免费训练条件下持续优于Search-o1等强基线，在不同小语言模型架构和规模上都取得了最先进的结果。

Conclusion: FutureMind成功提升了小语言模型在复杂任务上的表现，同时揭示了思维模式蒸馏过程受到教师（大语言模型）和学生（小语言模型）之间认知偏差瓶颈的限制，这为推理技能的可迁移性提供了新视角，为开发兼具效率和真正认知能力的小语言模型铺平了道路。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [219] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

TL;DR: OntoEKG：一个基于LLM的流水线，用于从非结构化企业数据中加速生成领域特定本体，通过提取和蕴含两个模块实现自动化本体构建。


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱对于统一异构数据和实施语义治理至关重要，但底层本体的构建仍然是一个资源密集、依赖领域专业知识的繁琐人工过程。

Method: 将建模任务分解为两个阶段：提取模块识别核心类和属性，蕴含模块将这些元素逻辑地组织成层次结构，然后序列化为标准RDF格式。

Result: 在Data领域实现了模糊匹配F1分数0.724，同时揭示了在范围定义和层次推理方面的局限性。

Conclusion: 该方法展示了LLM驱动本体构建的潜力，但也指出了当前方法在范围定义和层次推理方面的挑战，为解决企业知识图谱本体构建的自动化问题提供了新思路。

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [220] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

TL;DR: RE-MCDF是一个关系增强的多专家临床诊断框架，通过生成-验证-修订的闭环架构，结合医学知识图谱，解决神经科电子病历的异质性、稀疏性和噪声问题，提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 神经科电子病历具有异质性、稀疏性和噪声，单智能体系统容易产生自我强化的错误，现有多智能体框架交互浅层且缺乏结构，忽略了疾病间的丰富逻辑依赖关系，无法排除临床不可信的假设。

Method: 提出RE-MCDF框架，采用生成-验证-修订闭环架构，包含三个互补组件：1）生成候选诊断和支持证据的主专家；2）动态优先处理异质临床指标的实验室专家；3）强制执行疾病间逻辑约束的多关系感知与评估专家组。框架基于医学知识图谱自适应重新加权电子病历证据。

Result: 在CMEMR神经学子集（NEEMRs）和自建数据集（XMEMRs）上的广泛实验表明，RE-MCDF在复杂诊断场景中持续优于最先进的基线方法。

Conclusion: RE-MCDF通过整合多专家协作和显式疾病关系约束，有效解决了神经科电子病历诊断中的挑战，提高了诊断的准确性和逻辑一致性。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [221] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

TL;DR: 线性探针可用于监测AI系统的欺骗行为，但现有方法存在虚假相关性和误报问题。研究发现指令对选择对探针性能影响最大（占70.6%方差），且针对特定欺骗类型设计探针比寻求通用欺骗检测器更有效。


<details>
  <summary>Details</summary>
Motivation: 现有线性探针方法在检测AI系统欺骗行为时存在明显缺陷，包括虚假相关性和对非欺骗性响应的误报。需要改进探针训练方法以提高检测准确性和可靠性。

Method: 通过分析训练中使用的指令对的重要性，并基于人类可解释的欺骗分类法，针对特定欺骗行为设计专门的探针。研究发现指令对捕获的是欺骗意图而非内容特定模式。

Result: 指令对选择对探针性能影响最大，解释了70.6%的方差。针对特定欺骗类型设计的探针在评估数据集上表现更好。不同数据集的欺骗类型具有异质性。

Conclusion: 组织应针对其特定威胁模型设计专门的探针，而不是寻求通用的欺骗检测器。指令对选择是探针性能的关键因素，因为它捕获的是欺骗意图而非具体内容模式。

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [222] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

TL;DR: 提出一个完全自动化的多智能体系统，将软件工程建模为组织化流程，模拟真实工程团队结构，在SWE-bench 500上实现72.4%的任务解决率，优于单智能体基线。


<details>
  <summary>Details</summary>
Motivation: 当前大多数自主系统将问题解决视为单一或流水线式过程，而现实软件开发是团队协作活动，具有明确的角色分离、沟通和审查。需要模拟真实工程团队的组织结构来提升自主软件工程能力。

Method: 基于agyn开源平台构建多智能体系统，分配专门角色（协调、研究、实现、审查），提供隔离沙箱进行实验，支持结构化通信。系统遵循定义好的开发方法学，包括问题分析、任务规范、PR创建和迭代审查，完全无需人工干预。

Result: 在SWE-bench 500评估中，系统解决了72.4%的任务，优于使用可比语言模型的单智能体基线。系统专为实际生产使用设计，未针对SWE-bench进行调优。

Conclusion: 复制团队结构、方法学和沟通是自主软件工程的有力范式，未来进展可能同样依赖于组织设计和智能体基础设施，而不仅仅是模型改进。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [223] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

TL;DR: Qrita是一种基于枢轴选择策略的高效Top-k和Top-p算法，相比传统排序方法在GPU上实现2倍吞吐量和一半内存使用，同时保证确定性输出。


<details>
  <summary>Details</summary>
Motivation: Top-k和Top-p是大语言模型采样中的主要截断算子，但在大规模词汇表上高效实现仍然是一个重大挑战。现有方法要么依赖排序（带来显著计算和内存开销），要么使用随机方法（改变算法输出）。

Method: 基于RTop-k的枢轴搜索思想，扩展到Top-k和Top-p，采用两种关键技术：1. 基于高斯分布的sigma截断，大幅减少目标元素的搜索空间；2. 四元枢轴搜索与重复处理，将枢轴搜索迭代减半并保证确定性输出。使用Triton GPU编程语言实现。

Result: 与vLLM、SGLang和Flashinfer等高性能LLM执行引擎的Top-k和Top-p内核相比，Qrita实现了高达2倍的吞吐量和一半的内存使用，同时提供与基于排序算法相同的输出。

Conclusion: Qrita提供了一种高效、确定性的Top-k和Top-p实现方法，解决了现有方法在GPU上的计算和内存开销问题，同时保持算法输出的准确性。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [224] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

TL;DR: MAGIC框架通过多轮多智能体强化学习将LLM安全对齐建模为非对称对抗游戏，攻击者学习生成欺骗性提示，防御者学习识别拒绝，实现协同进化以提升模型安全鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖静态预收集数据分布，难以应对不断演化的对抗攻击，需要动态适应性的安全对齐框架。

Method: 提出MAGIC多轮多智能体强化学习框架，将安全对齐建模为非对称对抗游戏：攻击者智能体迭代重写查询生成欺骗性提示，防御者智能体同时优化策略识别并拒绝此类输入。

Result: 攻击者通过迭代RL训练演化出新颖的、先前未见过的组合策略，防御者能够泛化到未见过的攻击模式，在保持模型帮助性的同时获得更高的防御成功率。

Conclusion: MAGIC框架通过协同进化机制有效提升LLM安全对齐的鲁棒性，理论上提供了更稳健的游戏均衡和安全保证，为动态安全防御提供了新思路。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [225] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

TL;DR: S1-NexusAgent是一个用于多学科科学研究的自进化智能体框架，通过分层规划-代码执行范式和双循环架构解决现有LLM在长程规划、目标维护和持续学习方面的局限，在多个科学基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和基于工具的智能体在处理大规模数据、复杂工作流和专用工具时存在局限，特别是在长程规划、鲁棒目标维护和从执行中持续学习方面，难以满足现代科学研究的需求。

Method: 采用分层Plan-and-CodeAct执行范式，通过双循环架构将全局科学规划与子任务级工具执行解耦；原生支持MCP协议，集成数千个跨学科科学工具；引入基于对象引用的稀疏上下文管理；通过Critic Agent自动评估执行轨迹并提炼高质量研究路径为可重用科学技能。

Result: 在涉及长程规划和复杂专用工具编排的权威科学基准测试（包括biomini-eval、ChemBench和MatSciBench）中，S1-NexusAgent达到了最先进的性能，验证了其在复杂科学任务中的有效性和泛化能力。

Conclusion: S1-NexusAgent通过自进化框架成功解决了科学研究中长程规划、大规模数据处理和工具编排的挑战，为可持续和长程科学研究提供了有价值的解决方案，展示了在复杂科学任务中的卓越性能。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [226] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

TL;DR: 提出基于人类模拟的框架，使AI系统能通过推理内部状态、环境观察和与其他AI系统的交互来自主形成问题和设定任务，将问题形成作为任务选择和执行的优先决策过程。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的AI系统大多依赖预定义任务和固定提示，在环境条件变化时缺乏自主识别应解决问题的能力，限制了在动态开放环境中的自主决策能力。

Method: 提出人类模拟框架，将问题形成作为一级决策过程，整合内部驱动、环境感知和智能体间感知三种提示范围来逐步扩展认知覆盖，并支持从经验中学习问题形成过程。

Result: 在多智能体仿真环境中，环境感知提示相比内部驱动基线显著减少了"无进食"事件，智能体间感知提示在20天仿真中将累积无进食事件进一步减少60%以上，具有统计显著性(p < 0.05)。

Conclusion: 该框架通过将问题形成作为优先决策过程，使AI系统能更自主地适应环境变化，提高决策质量，为构建更智能、自适应的AI系统提供了新方向。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [227] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

TL;DR: 提出Collaborative Thoughts框架，让自回归模型和扩散模型通过闭环交互协同工作，结合两者的优势来提升空间推理可靠性和生成可控性。


<details>
  <summary>Details</summary>
Motivation: 自回归模型擅长序列规划和约束组合，但缺乏空间物理基础；扩散模型能捕捉丰富空间结构，但缺乏逐步逻辑控制来满足复杂多阶段约束或可靠纠错。需要结合两者优势。

Method: Collaborative Thoughts框架：自回归模型负责结构化规划和约束管理，扩散模型将约束实例化为中间视觉思维，视觉批评模块评估视觉思维是否满足结构和物理要求，反馈用于迭代优化后续规划和生成步骤。

Result: 通过代表性示例展示了Collaborative Thoughts如何提高空间推理的可靠性和生成的可控性，减轻跨模态错误传播。

Conclusion: Collaborative Thoughts为自回归和扩散模型提供了统一的协作框架，无论任务是自回归问答还是基于扩散的视觉生成，都使用相同的协作循环，实现了两种生成范式的互补优势。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [228] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

TL;DR: ToPT是一个两阶段框架，通过空间感知的区域嵌入学习和任务感知提示，解决现有方法缺乏空间先验和任务语义对齐的问题，在多个城市任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法产生任务无关的表示，与下游目标解耦；最近的提示方法缺乏明确的空间先验导致空间不连贯，且缺乏明确任务语义对齐的鲁棒机制。

Method: 提出ToPT两阶段框架：1) SREL模块使用Graphormer融合模块注入距离和区域中心性作为可学习注意力偏置；2) Prompt4RE模块使用冻结的多模态大语言模型处理任务特定模板，通过多头交叉注意力将语义向量与区域嵌入对齐。

Result: 在多个任务和城市的实验中达到最先进性能，改进高达64.2%，验证了空间先验和提示-区域对齐的必要性和互补性。

Conclusion: ToPT通过空间一致融合和明确任务对齐，有效解决了现有方法的局限性，在异构城市数据中学习有效的区域嵌入，为犯罪预测、资源分配等城市计算任务提供支持。

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [229] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

TL;DR: FlowSteer：基于强化学习的端到端工作流编排框架，通过多轮交互自动化工作流编排，支持多样化算子库和可互换的LLM后端。


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临高人工成本、依赖特定算子/大语言模型、奖励信号稀疏等关键挑战，需要更自动化和灵活的解决方案。

Method: 提出FlowSteer框架，采用轻量级策略模型作为智能体，在可执行画布环境中通过多轮交互进行工作流编排。策略模型分析执行状态并选择编辑动作，画布执行算子并返回反馈。还提出Canvas Workflow Relative Policy Optimization (CWRPO)训练方法，引入多样性约束奖励和条件释放机制来稳定学习并抑制捷径行为。

Result: 在12个数据集上的实验结果表明，FlowSteer在各种任务上显著优于基线方法。

Conclusion: FlowSteer通过强化学习框架实现了自动化工作流编排，解决了现有方法的局限性，提供了灵活可扩展的解决方案。

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [230] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

TL;DR: TRIP-Bench是一个基于真实旅行规划场景的长时程基准测试，包含18个工具和40+旅行需求，支持自动化评估。GTPO是一种在线多轮强化学习方法，能提升约束满足和交互鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法充分代表真实世界中的关键挑战，如强制执行全局约束、协调多工具推理以及适应长期多轮交互中不断变化的用户行为。需要一个新的基准来弥合这一差距。

Method: 引入TRIP-Bench基准测试，基于真实旅行规划场景，包含18个精选工具和40多个旅行需求，支持自动化评估。同时提出GTPO方法，这是一种在线多轮强化学习方法，采用专门的奖励归一化和奖励差分技术。

Result: 实验显示，即使是先进模型在简单分割上最多只能达到50%的成功率，在困难子集上性能降至10%以下。将GTPO应用于Qwen2.5-32B-Instruct模型后，在约束满足和交互鲁棒性方面表现优于Gemini-3-Pro。

Conclusion: TRIP-Bench有望推动实用的长时程交互智能体发展，而GTPO为鲁棒的长时程训练提供了一个有效的在线强化学习方案。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [231] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

TL;DR: 该研究通过最小化、主题中立的输入探究LLMs的无约束生成行为，发现不同模型家族具有系统性的主题偏好和独特退化模式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM分析大多依赖特定主题或任务的提示词，这限制了观察范围。研究旨在通过最小化、主题中立的输入来探究LLMs的近无约束生成行为，以更全面地理解模型特性。

Method: 使用最小化、主题中立的输入提示LLMs，收集和分析16个不同LLM的256,000个生成样本，系统研究其无约束生成行为。

Result: 发现不同模型家族具有强烈的系统性主题偏好：GPT-OSS主要生成编程(27.1%)和数学内容(24.6%)，Llama偏好文学内容(9.1%)，DeepSeek常生成宗教内容，Qwen偏好多项选择题。此外还观察到内容专业深度差异和独特的退化模式。

Conclusion: 即使在没有明确主题提示的情况下，LLMs仍表现出系统性的生成偏好和独特行为模式，这为模型监控和AI安全提供了重要见解，揭示了模型内在的"默认"行为特征。

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [232] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

TL;DR: LSTR框架将稀疏表示与潜在推理结合，通过稀疏语义转换实现可解释的多步计算


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法依赖密集的潜在状态转换，难以解释和控制；而稀疏表示模型虽然可解释，但仅限于事后分析。需要一种既能保持推理准确性，又能提供可解释性的方法。

Method: 提出LSTR（潜在稀疏转码推理）框架，将功能性稀疏转码器提升为主动推理算子，通过稀疏语义转换执行多步计算。核心是采用残差跳跃架构的潜在转换转码器（LTT），将线性流形传输与稀疏语义更新解耦，通过显式稀疏约束实现可控语义分辨率。

Result: 实验表明LSTR在保持推理准确性和压缩效率的同时，显著提高了相对于密集潜在基线的可解释性。因果干预和轨迹分析进一步证明这些稀疏特征在推理过程中既可作为可解释算子，又具有因果有效性。

Conclusion: LSTR成功地将稀疏表示与潜在推理相结合，通过稀疏语义转换实现了既准确又可解释的多步推理，为可解释AI提供了新思路。

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [233] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

TL;DR: CPO框架通过因果推理解决提示优化问题，使用双机器学习分离提示效果与查询特征，实现离线高效优化，显著提升LLM在复杂查询上的表现


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法存在两个主要问题：静态指令无法适应异构查询；动态方法依赖离线奖励模型，但这类模型本质上是相关性的，混淆了提示效果与查询特征。需要一种能隔离提示因果效应、实现高效查询特定优化的方法

Method: 提出因果提示优化(CPO)框架，分两阶段：1) 使用双机器学习(DML)在提示和查询的语义嵌入上学习离线因果奖励模型，隔离提示变体的因果效应与混淆的查询属性；2) 利用无偏奖励信号指导资源高效的查询特定提示搜索，无需依赖昂贵的在线评估

Result: 在数学推理、可视化和数据分析基准测试中，CPO始终优于人工设计的提示和最先进的自动优化器。改进主要由在困难查询上的鲁棒性提升驱动，而现有方法在这些查询上表现会恶化

Conclusion: CPO从根本上重塑了提示优化的经济学：通过将评估从实时模型执行转移到离线因果模型，能以在线方法所需推理成本的一小部分实现高精度、每查询定制化。因果推断为可靠且成本高效的提示优化提供了可扩展基础

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [234] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

TL;DR: MACD是一种新的视频语言模型推理策略，通过模型引导的反事实数据构建与对比解码相结合，专门针对视频语言模型的幻觉问题，在保持任务准确性的同时显著减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型容易产生幻觉，特别是在视觉证据薄弱、模糊或有偏见时。现有的解码方法（如对比解码）依赖随机扰动构建对比数据来缓解幻觉，但这种方式难以控制驱动幻觉的视觉线索，也无法很好地与模型弱点对齐。

Method: 提出模型感知的反事实数据对比解码（MACD），利用视频语言模型自身的反馈识别导致幻觉的关键对象区域，在对象级别生成有针对性的反事实输入（而非任意的帧或时间修改），然后将这些模型感知的反事实数据整合到对比解码中，在解码过程中强制实施证据基础的标记选择。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME等基准测试中，MACD在包括Qwen和InternVL系列在内的多种视频语言模型上，一致地减少了幻觉，同时保持或提高了任务准确性。该方法在处理涉及小型、遮挡或共现对象的挑战性场景时特别有效。

Conclusion: MACD通过模型引导的反事实数据构建与对比解码相结合，为视频语言模型的幻觉问题提供了一种有效的解决方案，特别是在处理复杂视觉场景时表现出色。代码和数据将公开发布。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [235] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

TL;DR: ARA框架将奖励黑客攻击重构为动态竞争游戏，通过黑客发现奖励模型漏洞、审计员检测利用行为，再通过审计引导的RLHF惩罚黑客攻击，实现最佳对齐-效用平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法容易受到奖励黑客攻击，模型利用奖励模型中的虚假相关性获得高分但违反人类意图，而现有缓解措施依赖静态防御无法适应新的利用策略。

Method: 提出对抗性奖励审计（ARA）框架：第一阶段，黑客策略发现奖励模型漏洞，审计员从潜在表示中学习检测利用行为；第二阶段，审计引导的RLHF（AG-RLHF）通过门控奖励信号惩罚检测到的黑客攻击。

Result: 在三种黑客攻击场景中，ARA在所有基线中实现了最佳对齐-效用平衡：将奉承行为降至接近SFT水平同时提高帮助性，减少冗长同时获得最高ROUGE-L，抑制代码游戏同时提高Pass@1。奖励黑客攻击、检测和缓解都具有跨领域泛化能力。

Conclusion: ARA将奖励黑客攻击从不可观察的失败转变为可测量、可控制的信号，通过动态竞争框架有效缓解RLHF中的奖励黑客问题，并展示了跨领域泛化能力，实现高效的多领域防御。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [236] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

TL;DR: LingLanMiDian（LingLan）是一个针对中医领域的大规模、专家标注的多任务基准测试，统一评估知识回忆、多跳推理、信息提取和临床决策，揭示了当前LLM在中医专业推理上与人类专家的显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前中医领域的基准测试存在碎片化、规模有限、评分标准不统一等问题，阻碍了公平比较。中医具有独特的本体论、术语和推理模式，需要领域忠实性的评估。

Method: 构建了LingLan基准测试，包含统一度量设计、临床标签的同义词容忍协议、每个数据集400项的困难子集，并将诊断和治疗建议重构为单选决策识别。对14个领先的开源和专有LLM进行了全面的零样本评估。

Result: 评估揭示了当前模型在中医常识知识理解、推理和临床决策支持方面的优势和局限。在困难子集上的评估显示，当前模型与中医专家在专业推理方面存在显著差距。

Conclusion: LingLan通过标准化评估连接基础知识和应用推理，为推进中医LLM和领域特定医疗AI研究建立了统一、可量化、可扩展的基础。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [237] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

TL;DR: INDIBATOR框架通过基于个体科学家研究轨迹的个性化智能体，提升分子发现的多智能体系统性能


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统使用通用角色或粗粒度关键词来区分智能体行为，这过度简化了人类科学家的实际工作方式，忽略了他们独特的研究轨迹对科学贡献的影响

Method: 提出INDIBATOR框架，从两个模态构建个体化科学家档案：1) 发表历史（文献知识） 2) 分子历史（结构先验）。智能体通过提案、批评和投票阶段进行多轮辩论

Result: 基于细粒度个体化档案的智能体系统持续优于依赖粗粒度角色的系统，达到竞争性或最先进的性能水平

Conclusion: 捕捉智能体的"科学DNA"对于高质量科学发现至关重要，验证了个体化智能体在多智能体系统中的重要性

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [238] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

TL;DR: 提出SoV框架，通过视觉输入预测自动驾驶车辆的触觉激励，解决当前传感器无法检测道路激励的问题


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆依赖多模态融合确保安全，但视觉和光学传感器无法检测道路引起的激励，这些激励对车辆动态控制至关重要。受人类联觉现象启发，需要开发能够从视觉输入预测触觉激励的方法

Method: 1. 提出Synesthesia of Vehicles (SoV)框架；2. 开发跨模态时空对齐方法解决时间和空间差异；3. 提出基于潜在扩散的视觉-触觉联觉生成模型(VTSyn)进行无监督高质量触觉数据合成；4. 使用真实车辆感知系统收集多模态数据集

Result: VTSyn在时间、频率和分类性能方面优于现有模型，通过主动触觉感知增强自动驾驶安全性

Conclusion: SoV框架成功实现了从视觉到触觉的跨模态预测，为自动驾驶提供了重要的道路激励感知能力，显著提升了车辆安全性能

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [239] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

TL;DR: ProcMEM框架让智能体从交互经验中自主构建程序性记忆，无需参数更新，通过技能MDP将经验转化为可执行技能，实现高效经验复用和长期自主性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的智能体在序列决策中依赖即时推理，即使在重复场景中也会重新推导解决方案，导致计算冗余和执行不稳定，缺乏有效的经验复用机制。

Method: 提出ProcMEM框架，通过形式化技能MDP将被动经验叙事转化为可执行技能（包含激活、执行和终止条件）；引入非参数PPO，利用语义梯度生成高质量候选技能，并通过PPO门控进行技能验证；采用基于分数的维护机制保持紧凑高质量的程序性记忆。

Result: 在领域内、跨任务和跨智能体场景中的实验表明，ProcMEM实现了更高的复用率和显著的性能提升，同时具有极端的记忆压缩能力；可视化进化轨迹和技能分布揭示了框架如何透明地积累、精炼和复用程序性知识。

Conclusion: ProcMEM框架通过构建程序性记忆有效解决了智能体经验复用问题，实现了计算效率提升和长期自主性，为LLM驱动的智能体提供了可扩展的经验学习机制。

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [240] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

TL;DR: 该论文提出了一个几何框架来分析大语言模型中的多头注意力机制，将标准注意力视为top-N选择器，在值状态空间中研究其行为，定义了Precision、Recall、F-score等几何度量来量化选择与非选择token的可分离性。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力机制分析缺乏几何视角，难以量化注意力头如何选择和分离token。需要建立一个几何框架来理解注意力在值状态空间中的行为，为注意力头的解释性和优化提供理论基础。

Method: 将标准注意力视为top-N选择器，在值状态空间中分析其行为。定义几何度量（Precision、Recall、F-score）来量化token可分离性。在经验性假设下推导非渐近边界：稳定的值范数、压缩的sink token、指数相似性衰减、分段注意力权重分布。

Result: 理论预测了最强非平凡可分离性的小N操作机制，阐明了序列长度和sink相似性如何影响几何度量。在LLaMA-2-7B、Gemma-7B、Mistral-7B上的实证测量与理论包络线紧密匹配：top-N选择增强了可分离性，sink相似性与Recall相关。发现LLaMA-2-7B中的注意力头专门化为三种机制：Retriever、Mixer、Reset，具有不同的几何特征。

Conclusion: 注意力表现为具有可测量token选择标准的结构化几何分类器，提供了头级别的可解释性，并为几何感知的稀疏化和LLM中注意力的设计提供了信息。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [241] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

TL;DR: DomusFM是首个专门为智能家居传感器数据设计的预训练基础模型，通过自监督双对比学习范式，在数据稀缺情况下实现跨环境和任务的泛化性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能家居传感器数据在医疗监测和辅助技术中有重要应用潜力，但现有方法存在关键限制：监督模型需要大量标注数据；现有基础模型仅关注惯性传感器，无法处理智能家居二进制传感器事件的稀疏离散特性和丰富语义关联；基于LLM的方法存在隐私和成本问题。

Method: DomusFM采用自监督双对比学习范式，整合轻量级语言模型的语义嵌入、时间模式编码器和二进制状态编码器，同时捕捉令牌级语义属性和序列级时间依赖关系。

Result: 在七个公共智能家居数据集的留一数据集评估中，DomusFM在不同下游任务上优于最先进的基线方法，即使在仅有5%标注数据用于微调的情况下也能实现卓越性能。

Conclusion: DomusFM解决了智能家居传感器数据稀缺问题，同时保持了实际部署的可行性，为现实世界智能家居系统提供了实用解决方案。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [242] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

TL;DR: 该研究比较了大型语言模型（LLM）和形式概念分析（FCA）在主题建模任务中的表现，通过两个实验评估它们在文档主题提取方面的效果。


<details>
  <summary>Details</summary>
Motivation: 主题建模在文档检索、情感分析和文本摘要等领域的应用日益广泛。虽然大型语言模型（LLM）在文本处理中很流行，但很少有研究探讨其在主题建模中的实用性。同时，形式概念分析（FCA）最近被提出作为主题建模的候选方法，但缺乏实际应用案例研究。本研究旨在比较这两种方法，了解它们在主题建模领域的优势和局限性。

Method: 研究采用对比分析方法：1）使用CREA管道评估FCA方法，该管道在过去的主题建模和可视化实验中已得到验证；2）使用GPT-5作为LLM代表，采用基于三个提示的零样本设置策略：从文档批次生成主题、合并批次结果形成最终主题、以及主题标签生成。研究进行了两个实验：第一个实验重用之前评估CREA的教学材料，第二个实验分析40篇信息系统研究文章，将提取的主题与底层子领域进行比较。

Result: 论文摘要未提供具体实验结果，但描述了实验设计：第一个实验使用教学材料验证方法的可行性，第二个实验通过分析信息系统研究文章来评估提取主题与实际研究子领域的一致性。这表明研究旨在通过实证比较来评估两种方法在主题建模任务中的表现。

Conclusion: 该研究通过系统比较LLM和FCA在主题建模中的应用，旨在填补现有研究空白，为选择适合的主题建模方法提供实证依据，并促进这两种技术在文本分析领域的进一步发展。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [243] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

TL;DR: 提出基于知识图谱的对话诊断系统，通过生成诊断假设和验证性提问两阶段推理，在模糊症状描述下提高诊断准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法依赖模型参数知识或假设患者提供丰富具体信息，这在现实中不切实际，需要解决信息不完整情况下的诊断问题

Method: 基于诊断知识图谱进行两阶段推理：1）从对话上下文生成诊断假设；2）通过澄清问题验证假设，循环直到得出最终诊断。使用MIMIC-IV患者数据和改进的模拟器，模拟真实患者早期就诊时的模糊症状描述

Result: 实验显示相比强基线方法，诊断准确性和效率均有提升。医生评估证实模拟器的真实性和生成问题的临床实用性

Conclusion: 提出的对话诊断系统能有效处理信息不完整情况，通过知识图谱推理和验证性提问机制，在模糊症状描述下实现更准确高效的诊断

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [244] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

TL;DR: VeriFY是一个训练时框架，通过基于一致性的自我验证教LLMs推理事实不确定性，减少事实幻觉，同时保持召回率


<details>
  <summary>Details</summary>
Motivation: 事实幻觉是大型语言模型的核心挑战，现有方法依赖外部事后验证或直接将不确定性映射到弃权，导致过于保守的行为

Method: VeriFY通过结构化验证轨迹增强训练，指导模型：1)生成初始答案，2)生成并回答验证查询，3)进行一致性判断，4)决定回答或弃权。采用阶段级损失掩码避免强化幻觉内容

Result: 在多个模型家族和规模上，VeriFY将事实幻觉率降低9.7%至53.3%，召回率仅小幅下降(0.4%至5.7%)，在单一数据源训练后能跨数据集泛化

Conclusion: VeriFY通过训练时自我验证有效减少事实幻觉，平衡准确性和保守性，为LLM事实可靠性提供了有前景的解决方案

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [245] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 提出一种基于专家模型和单神经元门控机制的安全感知解码方法，实现轻量级对齐，在保持模型实用性的同时增强输出安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法主要依赖后训练，计算成本高且泛化能力差；轻量级方法要么过度依赖预先计算的安全注入，要么过度依赖模型自身能力，导致泛化有限且生成效率降低。

Method: 提出安全感知解码方法，只需低成本训练专家模型，使用单神经元作为门控机制，有效平衡模型内在能力与外部指导。

Result: 该方法在训练开销和跨模型规模泛化方面具有明显优势，同时保持实用性和增强输出安全性。

Conclusion: 为大型语言模型的安全实用部署提供了轻量级对齐的新视角。

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [246] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

TL;DR: 提出了一种基于推理的知识内化训练策略，通过背景故事、多跳问题和知识蒸馏，使AI模型能够将新知识整合到推理过程中，而不仅仅是记忆事实。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法主要关注原子事实的记忆，但无法将新知识整合到连贯的框架中并在不同上下文中灵活应用。知识内化本质上是一个推理问题而非记忆问题。

Method: 提出基于三个原则的训练策略：1) 将新知识作为连贯的背景故事引入，解释新事实与现有知识的关系；2) 使用自生成的多跳问题进行训练，要求多步推理涉及新信息；3) 通过知识蒸馏训练，强制学生模型内化教师的推理行为而不访问新信息。

Result: 实验表明，使用该策略训练的模型在推理过程中能有效利用新获取的知识，在需要结合多个新事实的挑战性问题中表现出色。

Conclusion: 通过将知识内化视为推理问题并采用基于背景故事、多跳问题和知识蒸馏的训练策略，能够使AI模型更好地整合和应用新知识，解决现有知识编辑方法的局限性。

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [247] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

TL;DR: 该论文提出了一种基于有限时域马尔可夫决策过程的多智能体系统，用于解决LLM智能体在合规审查等受监管场景中的不确定性和协调问题，相比单智能体基线在准确性、人工审核需求和处理时间方面均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体在合规审查、尽职调查等受监管场景中执行复杂多步骤工作流时，主要依赖单个智能体的提示工程，难以观察或比较模型如何处理跨决策阶段的不确定性和协调问题，以及与人工监督的交互。

Method: 引入一个形式化为有限时域马尔可夫决策过程的多智能体系统，具有有向无环结构。每个智能体对应特定角色或决策阶段（如合规工作流中的内容、业务或法律审查），预定义转换表示任务升级或完成。使用蒙特卡洛估计在智能体层面量化认知不确定性，系统级不确定性通过MDP在自动标记状态或人工审核状态的终止来捕获。

Result: 通过AI安全评估中自残检测的案例研究实施多智能体合规系统，结果显示相比单智能体基线有显著改进：准确性提高高达19%，所需人工审核减少高达85倍，某些配置下处理时间也减少。

Conclusion: 提出的多智能体MDP框架为受监管工作流中的LLM智能体提供了更透明、可观察的架构，能够有效处理不确定性和协调问题，在准确性、效率和人工监督需求方面均优于传统单智能体方法。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [248] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

TL;DR: 扩散语言模型通过架构结构和训练交互缓解了逆转诅咒问题，而自回归模型则严重受此困扰


<details>
  <summary>Details</summary>
Motivation: 研究为什么基于掩码扩散的语言模型（MDMs）比自回归语言模型（ARMs）在逆转诅咒问题上表现更好，探索其根本原因

Method: 通过理论分析和实验验证，研究单层Transformer编码器中权重共享如何耦合前向和反向注意力分数，以及梯度对齐如何使前向损失最小化同时减少反向损失

Result: MDMs通过架构结构（权重共享）和训练交互缓解逆转诅咒，而不是仅靠任意顺序训练目标；实验验证了这一机制

Conclusion: 扩散语言模型部分克服了自回归模型持续存在的逆转诅咒问题，这源于其架构结构与训练的相互作用，而非简单的训练目标差异

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [249] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

TL;DR: 论文提出DGR方法，通过将外部安全推理数据集转化为与目标大语言模型内部分布对齐的方式，减少安全对齐对模型推理能力的负面影响，同时保持安全性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集通常从外部大模型或人工标注中蒸馏安全推理轨迹，但这些数据与需要对齐的目标模型存在分布差异，作者推测这种分布差异是导致目标模型推理能力显著下降的主要原因。

Method: 提出DGR方法，将现有的分布外安全推理数据集进行转化和精炼，使其与目标LLM的内部分布对齐，从而减少分布差异对模型推理能力的影响。

Result: DGR有效缓解了安全税问题，同时保持安全性能：相比Vanilla SFT，在DirectRefusal上平均推理准确率提升30.2%，在R1-ACT上提升21.2%；推理能力下降程度与分布偏移程度相关；仅需10个样本就能激活有效的拒绝行为。

Conclusion: 分布一致性对保持大语言模型推理能力至关重要，安全对齐可能主要作为激活潜在知识的机制，少量样本即可激活有效的安全拒绝行为。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [250] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

TL;DR: NLCO是一个自然语言组合优化基准，用于评估大语言模型在端到端组合优化推理上的能力，涵盖43个问题，使用四层分类法组织，实验显示模型在小实例上表现良好但随着规模增大而退化。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在数学和逻辑推理方面表现出色，但它们在组合优化（在高维解空间中搜索满足硬约束的解）方面的能力仍未得到充分探索。需要建立一个基准来评估LLMs在端到端组合优化推理上的能力。

Method: 引入NLCO基准，包含43个组合优化问题，采用四层分类法组织：变量类型、约束族、全局模式和目标类别。提供求解器标注的解决方案，从可行性、解的最优性和推理效率三个维度全面评估LLMs。

Result: 实验表明，高性能模型在小实例上实现了良好的可行性和解质量，但随着实例规模增大，两者都会退化，即使使用更多token进行推理也是如此。在分类法中也观察到系统性效应：基于集合的任务相对容易，而图结构问题和瓶颈目标导致更频繁的失败。

Conclusion: NLCO基准填补了LLMs在组合优化能力评估方面的空白，揭示了当前模型在处理大规模组合优化问题时的局限性，为未来研究提供了细粒度的评估框架。

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [251] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

TL;DR: 本文提出了TIDE框架，用于诊断和评估LLM智能体在测试时改进（TTI）中的表现，通过三个维度分析智能体与环境交互的动态过程。


<details>
  <summary>Details</summary>
Motivation: 当前对自主LLM智能体测试时改进（TTI）的成功与失败机制理解不足，现有评估指标无法捕捉任务优化效率、错误行为后的适应能力以及工作记忆的具体效用。

Method: 提出测试时改进诊断评估（TIDE）框架，这是一个智能体无关和环境无关的框架，将TTI分解为三个综合且相互关联的维度：任务完成的整体时间动态、递归循环行为的约束程度以及累积记忆负担的影响。

Result: 通过在不同智能体和环境中的广泛实验，TIDE揭示了提升智能体性能不仅需要扩展内部推理能力，还需要显式优化智能体与环境之间的交互动态。

Conclusion: TIDE框架为理解LLM智能体测试时改进机制提供了系统性的诊断工具，强调了优化智能体与环境交互动态的重要性，而不仅仅是增强内部推理能力。

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [252] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

TL;DR: 论文提出了比较性可解释AI框架（Δ-XAI），用于解释大规模基础模型在干预后出现的行为变化，强调需要比较参考模型和干预模型之间的差异，而非孤立分析单个模型。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在扩展、微调、强化学习或上下文学习后会出现行为变化，虽然这些现象受到关注，但解释其出现原因仍被忽视。传统XAI方法只能分析单个模型检查点，无法解释不同检查点间的内部变化以及哪些解释性主张是合理的。

Method: 提出比较性可解释AI（Δ-XAI）框架，包含设计适当解释方法时需考虑的一组期望特性。介绍了可能的Δ-XAI流程，将其与期望特性关联，并提供了具体的Δ-XAI实验。

Result: 论文建立了Δ-XAI的理论框架，明确了比较性解释的核心原则，提供了将Δ-XAI方法应用于实际行为变化分析的具体路径。

Conclusion: 行为变化应该通过比较性方式解释，核心目标应该是参考模型和干预模型之间的干预诱导变化，而不是孤立分析任何单个模型。Δ-XAI框架为解决这一挑战提供了系统方法。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [253] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

TL;DR: 提出IPG框架，通过传播基于结果的信号来定位和解释LLM复杂推理行为的内部机制


<details>
  <summary>Details</summary>
Motivation: 当前解释方法难以精确定位复杂推理机制或捕捉从模型内部工作到推理输出的顺序影响，需要一种能够识别对推理行为有顺序贡献的组件的方法

Method: 提出集成策略梯度（IPG）框架，通过将基于结果的信号（如推理后准确率）向后传播通过模型推理轨迹，将推理行为归因于模型内部组件

Result: 实证评估表明，该方法实现了更精确的定位，并能够可靠地调节不同推理模型的推理行为（如推理能力、推理强度）

Conclusion: IPG框架基于结果导向和顺序影响感知原则，能够有效识别对推理行为有顺序贡献的组件，为理解LLM复杂推理机制提供了新方法

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [254] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

TL;DR: M2CL是一种多智能体上下文学习方法，通过训练上下文生成器动态生成每轮讨论的上下文指令，解决多智能体讨论中的不一致性问题，显著提升性能20%-50%


<details>
  <summary>Details</summary>
Motivation: 当前多智能体讨论方法容易遭受讨论不一致性问题，由于各个LLM实例的上下文不对齐，导致无法达成一致的解决方案

Method: 提出M2CL方法，为每个智能体训练上下文生成器，通过自动信息组织和精炼动态生成每轮讨论的上下文指令，采用精心设计的自适应机制控制上下文一致性和输出差异

Result: 在学术推理、具身任务和移动控制等挑战性任务上，M2CL性能显著超越现有方法20%-50%，同时具有良好的可迁移性和计算效率

Conclusion: M2CL通过动态上下文生成有效解决了多智能体讨论中的不一致性问题，使LLM能够避免过早收敛于多数噪声，逐步达成正确共识

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [255] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

TL;DR: Live-Evo是一个在线自演化记忆系统，通过经验库和元指导库分离"发生了什么"和"如何使用"，在持续数据流中动态更新记忆权重，提升LLM智能体在分布变化下的任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有自演化系统主要针对静态训练/测试分割设计，通过折叠静态基准来近似在线学习，在真实分布变化和持续反馈下表现脆弱。需要真正的在线自演化记忆系统来处理连续数据流。

Method: Live-Evo采用经验库和元指导库的双库架构，将经验存储与使用指导分离。系统维护经验权重并根据反馈动态更新：持续有帮助的经验被强化并更频繁检索，误导性或过时的经验被降权并逐渐遗忘，模拟人类记忆的强化与衰减机制。

Result: 在10周时间跨度的Prophet Arena实时基准测试中，Live-Evo将Brier分数提高了20.8%，市场回报增加了12.9%。在深度研究基准测试中也表现出对强基线的持续优势。

Conclusion: Live-Evo通过在线自演化记忆系统有效处理持续数据流，在真实分布变化下显著提升LLM智能体的任务解决性能，为在线学习环境中的记忆管理提供了有效解决方案。

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [256] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

TL;DR: 论文提出Thought-ICS框架，通过离散化思维步骤实现语言模型的错误定位与自我修正，相比传统链式思维方法显著提升自我修正能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型的自我修正能力仍然不足，需要探索模型是否能明确识别错误推理中的错误位置，从而构建能有效自我修正的AI系统。

Method: 提出Thought-ICS框架：将推理结构化为离散、语义连贯的思维步骤；迭代生成每个离散思维步骤；验证时定位第一个错误步骤；回溯到最后一个正确点生成替代推理。

Result: 在结构化思维步骤中模型能可靠定位错误，而在传统非结构化链式思维中失败；使用oracle验证时，Thought-ICS实现20-40%的自我修正提升；在完全自主无外部验证设置中，优于当代自我修正基线方法。

Conclusion: 通过离散化思维步骤创建自然边界，使语言模型能够精确定位错误并实现有效的自我修正，为构建能自我修正的AI系统提供了新路径。

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [257] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: SafeGround是一个不确定性感知的GUI grounding框架，通过分布感知的不确定性量化方法和校准过程，实现具有统计保证的假发现率控制，提高模型可靠性。


<details>
  <summary>Details</summary>
Motivation: GUI grounding中的错误预测可能导致代价高昂且难以逆转的操作（如错误支付批准），因此需要提高模型可靠性，确保风险可控。

Method: SafeGround采用分布感知的不确定性量化方法捕捉模型输出的空间分散性，通过校准过程推导具有统计保证假发现率控制的测试时决策阈值。

Result: 在ScreenSpot-Pro基准测试中，SafeGround的不确定性度量优于现有基线，校准阈值可靠地实现了严格的风险控制，系统级准确率比Gemini-only推理最高提升5.38个百分点。

Conclusion: SafeGround为GUI grounding模型提供了有效的风险控制框架，通过不确定性量化和统计校准显著提高了模型可靠性和系统性能。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [258] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

TL;DR: Drift-Bench是首个评估自主智能体在输入故障下多轮澄清能力的诊断基准，通过状态导向和服务导向执行环境中的多轮交互来测量智能体的语用能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型向自主智能体过渡，用户输入经常违反合作假设（如隐含意图、缺失参数、错误预设或模糊表达），产生文本评估无法捕捉的执行风险。现有基准通常假设指令明确或仅限于文本单轮澄清，无法测量在接地执行风险下的多轮消歧能力。

Method: 基于经典通信理论，Drift-Bench提供统一的合作故障分类法，采用角色驱动的用户模拟器和Rise评估协议，在状态导向和服务导向执行环境中进行多轮澄清评估。

Result: 实验显示在这些故障下智能体性能显著下降，澄清效果因用户角色和故障类型而异，揭示了现有智能体在处理输入故障时的局限性。

Conclusion: Drift-Bench连接了澄清研究和智能体安全评估，能够系统诊断可能导致不安全执行的故障，为自主智能体的安全性和鲁棒性评估提供了重要工具。

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [259] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

TL;DR: 论文开发了MentisOculi评估框架，发现当前统一多模态模型在利用视觉化作为推理辅助方面存在局限，视觉思维尚未真正提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 随着前沿模型从多模态大语言模型向统一多模态模型演进，研究者希望探索能否像人类使用心理意象一样，利用中间视觉化作为推理辅助。核心问题是模型能否以目标导向的方式形成、维护和操作视觉表征。

Method: 开发了MentisOculi——一个程序化、分层化的多步推理问题套件，专门设计来挑战前沿模型。评估了从潜在标记到显式生成图像等多种视觉策略，并特别分析了统一多模态模型的表现。

Result: 视觉策略总体上未能提升性能。统一多模态模型虽然具备解决任务的文本推理能力，有时也能生成正确的视觉内容，但存在生成错误累积的问题，甚至无法有效利用真实视觉化信息。

Conclusion: 尽管视觉思维具有内在吸引力，但目前尚未真正有益于模型推理。MentisOculi为分析和弥合这一差距建立了必要的基础，适用于不同模型家族。

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [260] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

TL;DR: 通过引入"身份桥"数据正则化方法（如"A→A"形式），可以显著缓解LLM的反转诅咒问题，使模型从单纯记忆事实转向学习更高层次的规则。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点：先前研究认为反转诅咒是自回归因果LLM的固有根本限制，表明模型倾向于记忆事实级知识而非捕捉高层次规则。本文旨在证明这一看似根本的限制可以通过简单的数据调整来缓解。

Method: 提出"身份桥"数据正则化方法，即在训练数据中添加"A→A"形式的简单正则化数据（如"爱丽丝的名字是爱丽丝"）。理论上分析梯度下降的隐式偏差，证明即使单层Transformer也能打破反转诅咒。

Result: 实证结果显示：使用该数据配方的1B预训练语言模型在反转任务上达到40%的成功率，而仅使用前向知识数据训练时成功率接近零。这显著改善了模型的反转推理能力。

Conclusion: 为反转诅咒提供了新的理论基础，并提供了鼓励LLM从数据中学习更高层次规则的原则性、低成本路径，挑战了关于自回归LLM固有局限性的传统观点。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [261] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

TL;DR: 提出GPS方法，通过轻量级生成模型进行贝叶斯推理预测提示难度，结合中等难度优先和历史锚定多样性原则，提升强化学习中提示选择的效率


<details>
  <summary>Details</summary>
Motivation: 强化学习能增强大语言模型的推理能力，但通常需要大量计算成本。现有在线提示选择方法要么依赖昂贵的精确评估，要么缺乏跨提示的泛化能力

Method: 提出GPS方法：1) 使用轻量级生成模型在共享优化历史上进行贝叶斯推理预测提示难度；2) 结合中等难度优先原则；3) 引入历史锚定多样性原则进行批量获取

Result: 在多种推理基准测试中，GPS在训练效率、最终性能和测试时效率方面均显著优于现有基线方法

Conclusion: GPS通过可泛化的预测性提示选择，有效解决了强化学习中提示选择的计算效率问题，为高效训练大语言模型提供了新思路

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [262] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

TL;DR: 提出一个针对难以直接观测情境的测量框架，使用间接数据痕迹、可解释机器学习模型和理论指导的三角验证来填补不可访问的测量空间。


<details>
  <summary>Details</summary>
Motivation: 许多高风险的系统和科学政策关注的现象难以直接观测：感兴趣的动态过程不可观察，数据间接且分散在不同来源，真实情况缺失或被隐藏。在这些情境下，可用数据通常不支持传统的分析策略。

Method: 结合多源三角验证与可解释机器学习模型，不依赖无法观测的理想数据的准确性，而是寻求不同部分信息模型之间的一致性，通过交叉信号一致性或与预期状态的偏离来得出可靠结论。

Result: 通过一个关于秘密军事组织的组织增长和内部压力动态的实证分析，展示了该方法如何恢复具有实质意义的变异，同时明确揭示了推断的局限性。

Conclusion: 该框架为在缺乏足够数据进行传统统计或因果推断的情况下，提供了定量的分析工作流程，展示了三角验证和可解释机器学习如何在这些困难测量环境中提供有意义的见解。

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [263] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种改进条件流匹配（CFM）的方法，通过同时匹配流及其散度来提升基于流的生成模型的性能，而不牺牲生成效率。


<details>
  <summary>Details</summary>
Motivation: 条件流匹配（CFM）虽然是一种高效、无需模拟的训练方法，但在学习概率路径的准确性方面存在不足，无法确保精确学习概率路径。

Method: 引入新的偏微分方程来表征学习概率路径与精确概率路径之间的误差，并推导其解。提出新的目标函数，同时匹配流及其散度，通过结合CFM损失和相关的散度损失来约束两个概率路径之间的总变差差距。

Result: 新方法显著提升了基于流的生成模型的性能，在多个重要基准任务（包括动力系统、DNA序列和视频的生成建模）上展现出优于CFM的优势。

Conclusion: 通过同时匹配流及其散度的新训练方法，可以在不牺牲生成效率的前提下，有效提高基于流的生成模型的准确性和性能。

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [264] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

TL;DR: 提出ELLMPEG框架，在边缘设备上利用本地LLM自动生成视频处理命令，避免云API依赖和成本


<details>
  <summary>Details</summary>
Motivation: 云LLM存在计算能耗高、隐私风险、API成本三大限制，需要边缘本地化解决方案

Method: 集成工具感知RAG与迭代自反思，在边缘生成并验证FFmpeg和VVenC命令

Result: Qwen2.5结合ELLMPEG达到78%平均准确率，零API成本，优于其他开源模型

Conclusion: ELLMPEG为边缘视频处理提供高效、隐私保护、成本可控的自动化解决方案

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [265] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

TL;DR: 本文提出了一种用于人道主义援助和灾难响应的智能RAG框架，通过多模态知识库和自适应检索策略，支持灾难响应的三个阶段：初期救援、中期恢复和长期重建。


<details>
  <summary>Details</summary>
Motivation: 有效的人道主义援助和灾难响应需要快速的情境理解、可靠的决策支持，以及能够泛化到多样且未见过的灾难场景。现有系统缺乏对灾难响应完整生命周期的支持，且难以处理多模态信息和历史经验知识。

Method: 构建了分层知识库，整合文本灾难手册、历史教训（如2011年东北地震）以及空中和地面图像。使用开源多模态实现处理46个海啸相关PDF文件，采用BLIP图像标注、ColVBERT嵌入和长上下文摘要生成高效的多模态检索树。智能控制器通过熵感知场景抽象动态选择检索策略（如RAPTOR、ColBERT），并采用轻量级LoRA后训练方法注入历史灾难经验知识。

Result: 在真实灾难数据集上的实验表明，该系统改善了情境理解能力，提高了任务分解准确性，并为应急操作提供了更好的可用性。通过自适应检索增强生成、自我推理和多模态思维链能力，实现了显著性能提升。

Conclusion: 该智能RAG框架通过整合多模态知识、自适应检索策略和历史经验学习，为灾难响应的三个阶段提供了有效的决策支持，能够同时服务专家和非专家响应者，在应急操作中表现出优越的性能。

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [266] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

TL;DR: 研究发现语言模型的奉承行为存在跨语言差异，印地语文化适应提示比英语原版导致更高的奉承率，文化适应是主要影响因素而非语言编码。


<details>
  <summary>Details</summary>
Motivation: 目前对语言模型奉承行为的研究主要集中在英语环境，不清楚这种诊断是否适用于其他语言和文化背景。需要探究奉承行为在不同语言和文化中的表现差异。

Method: 将Beacon单轮强制选择奉承诊断扩展到印地语，采用三条件设计：英语原版、印地语直译、印地语文化适应提示。评估四个开源指令调优模型，每个条件50个提示，分离语言编码和文化适应的影响。

Result: 所有模型中，文化适应印地语提示的奉承率均高于英语，绝对差异12.0-16.0个百分点。对Qwen 2.5-Coder-7B的分解显示文化适应贡献14.0%差异，语言编码仅贡献2.0%。建议类提示的跨语言差异最大（20-25个百分点）。

Conclusion: 基于英语测量的对齐行为不能均匀地跨语言转移，文化背景的提示框架起着重要作用。需要跨语言对齐评估，并发布了所有数据集和评估代码。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [267] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

TL;DR: 提出基于数据集剪枝的数据中心化优化框架，通过保留关键数据子集实现边缘学习的高效训练，显著降低计算开销和能耗


<details>
  <summary>Details</summary>
Motivation: 边缘学习在移动设备上训练模型面临计算和能耗瓶颈，现有研究主要优化模型架构用于推理，但训练阶段仍受大量冗余本地数据集处理限制

Method: 提出数据中心化优化框架，利用数据集剪枝技术：通过截断预热阶段获得的平均损失统计来评估样本重要性，动态剪枝比例下确定性地保留最关键数据点

Result: 在标准图像分类基准测试中，框架实现训练延迟和能耗的近似线性减少，与剪枝比例成正比，模型精度下降可忽略不计

Conclusion: 数据集剪枝是增强资源受限移动边缘设备学习可持续性和可扩展性的重要补充范式

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [268] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

TL;DR: TextBFGS：用于离散文本优化的二阶框架，通过检索历史梯度算子实现拟牛顿优化，相比一阶方法显著提升收敛速度和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的离散文本优化方法主要是一阶优化器（类似随机梯度下降），存在收敛慢和不稳定的问题，因为它们忽略了优化景观的语义曲率

Method: TextBFGS是一个二阶框架，通过从预学习成功轨迹的记忆中检索梯度算子来近似逆Hessian矩阵。给定文本梯度反馈，系统识别历史修正模式并应用到当前变量，实现单次推理更新

Result: 在代码优化任务（HumanEval、MBPP等）的实证评估中，TextBFGS显著优于一阶基线方法，以更少的模型调用获得更高的通过率，并展现出强大的跨任务可迁移性

Conclusion: TextBFGS为高效、内存感知的文本优化建立了数学基础范式，通过二阶优化方法解决了传统一阶方法在离散文本优化中的局限性

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [269] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

TL;DR: 研究发现反事实解释对模型不确定性高度敏感，即使模型准确度的小幅下降也会导致反事实解释出现大幅变化，强调在金融和社会科学等领域需要不确定性感知的解释方法。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的反事实解释方法未在模型和数据不确定性变化的情况下进行测试，导致这些解释在现实世界变化中可能不稳定或无效。需要研究常见机器学习模型与反事实生成算法组合在存在随机不确定性和认知不确定性时的鲁棒性。

Method: 通过在合成和真实世界表格数据集上进行实验，研究常见机器学习模型与反事实生成算法组合在存在随机不确定性和认知不确定性时的表现。分析模型不确定性对反事实解释稳定性的影响。

Result: 实验表明反事实解释对模型不确定性高度敏感。即使由噪声增加或数据有限引起的模型准确度小幅下降，也会导致生成的反事实在平均水平和单个实例上出现大幅变化。

Conclusion: 反事实解释在模型不确定性存在时缺乏鲁棒性，这强调了在金融和社会科学等关键领域开发不确定性感知解释方法的必要性。

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [270] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

TL;DR: SPGCL提出了一种通过SVD引导的结构扰动进行鲁棒图对比学习的框架，通过平衡边移除和恢复率来控制视图间的结构差异，提高GNN对结构噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法存在局限性：随机扰动（如边丢弃）是结构无关的，可能移除关键边；而基于SVD的视图往往变得密集且缺乏足够的多样性。需要一种既能保持全局结构先验又能生成多样化视图的方法来提高GNN对结构噪声的鲁棒性。

Method: SPGCL采用SVD引导的结构扰动，将轻量级随机边移除与SVD引导的细化步骤相结合。通过稀疏的top-ranked边选择和合并来恢复错误移除的信息边并引入语义上有意义的缺失链接，同时避免图密集化。还包含一个受全局相似性约束正则化的对比融合模块来更好地对齐两个视图。

Result: 在十个基准数据集上的广泛实验表明，SPGCL能够持续提高基础GNN的鲁棒性和准确性，优于最先进的图对比学习和结构学习方法。

Conclusion: SPGCL通过SVD引导的结构扰动有效平衡了边移除和恢复，使对比信号反映语义结构差异而非边数量差距，为图对比学习提供了一种鲁棒的解决方案。

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [271] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

TL;DR: 提出一种名为"dimensional peeking"的方差缩减方法，用于离散仿真优化中的梯度估计，通过将采样粒度从标量提升到遵循相同控制流路径的值类，减少梯度估计的方差，提高零阶优化在离散非凸仿真中的竞争力。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化方法常用于高维空间中寻找局部最优解。当无法直接计算导数时，随机估计器可以提供近似梯度。但这些基于扰动的目标函数采样估计器会引入方差，导致收敛缓慢。特别是在离散仿真优化中，梯度估计的方差问题尤为突出。

Method: 提出dimensional peeking方法，将采样粒度从标量值提升到遵循相同控制流路径的值类，从而增加每次仿真评估收集的信息量。该方法从已建立的平滑梯度估计器推导而来，不引入任何偏差。通过自定义数值数据类型在C++程序中透明地实现dimensional peeking。

Result: 在三个高维输入的仿真优化问题中，观察到方差减少了高达7.9倍。与三种元启发式算法相比，dimensional peeking提高了零阶优化在离散和非凸仿真中的竞争力。

Conclusion: Dimensional peeking是一种有效的方差缩减方法，能够显著提高梯度估计的效率，使零阶优化在离散非凸仿真优化问题中更具竞争力。该方法通过提升采样粒度而不引入偏差，为仿真优化提供了新的技术途径。

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [272] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

TL;DR: 提出基于回归树及其集成方法（装袋和随机森林）的自动化单变量时间序列预测方法，处理自回归特征选择、趋势和季节性等问题，预测精度与指数平滑和ARIMA相当，并开发了公开软件。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于回归树及其集成方法的自动化单变量时间序列预测框架，解决传统统计模型需要专业知识调整参数的问题，提供更易用且性能相当的替代方案。

Method: 采用自回归方法和递归预测，使用回归树及其集成方法（装袋和随机森林），重点解决自回归特征选择、趋势序列处理和季节性行为应对等关键问题。

Result: 实验结果显示该方法在预测精度上与成熟的统计模型（如指数平滑和ARIMA）相当，证明了基于树的方法在时间序列预测中的有效性。

Conclusion: 基于回归树及其集成方法的时间序列预测框架能够实现与经典统计模型相当的预测性能，同时开发了公开可用的软件实现，为时间序列预测提供了新的实用工具。

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [273] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

TL;DR: 提出了一种无损压缩单位范数嵌入的方法，实现1.5倍压缩率，比现有最佳方法提升25%


<details>
  <summary>Details</summary>
Motivation: 单位范数嵌入在信息检索和机器学习中广泛应用，但现有压缩方法效率有限，需要更高效的压缩技术来减少存储和传输开销

Method: 利用高维单位向量的球坐标集中在π/2附近的特性，导致IEEE 754指数收敛到单一值，从而启用熵编码。该方法无需训练，在float32精度内完全无损

Result: 在涵盖文本、图像和多向量嵌入的26种配置评估中，该方法始终优于现有方法，实现1.5倍压缩率，比先前最佳方法提升25%

Conclusion: 该方法为高维单位向量提供了一种高效的无损压缩方案，具有实际应用价值，特别是在需要精确向量表示的检索和机器学习任务中

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [274] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

TL;DR: 论文分析了LoRA方法对标签噪声的内在抵抗特性，提出了理论框架解释其机制，并基于此开发了RACT方法用于噪声检测


<details>
  <summary>Details</summary>
Motivation: 研究LoRA方法中一个未被充分探索的特性：对标签噪声的内在抵抗能力，理解其理论机制并开发实际应用

Method: 1. 理论分析LoRA的记忆能力限制；2. 推导最优秩平衡近似偏差和噪声方差；3. 建立时间分离理论；4. 提出RACT（Rank-Aware Curriculum Training）方法利用秩差异进行噪声检测

Result: 1. 证明了秩-r LoRA无法记忆所有可能的标签分配；2. 推导了随噪声率降低的最优秩；3. 建立了干净模式早期学习而噪声记忆后期发生的时间分离；4. RACT在AG News上达到91.1% F1的噪声检测性能，同时保持91.46%的准确率

Conclusion: LoRA具有内在的标签噪声抵抗特性，这一特性可以通过理论框架解释，并可用于开发有效的噪声检测方法RACT，在保持模型性能的同时实现噪声识别

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [275] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

TL;DR: CARE-RFT是一种新的强化微调方法，通过使用偏斜反向KL散度替代标准反向KL正则化，在保持模型推理能力的同时解决了可信度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调存在一个关键权衡：无约束的RFT能获得强大的推理性能，但会严重损害模型可信度（增加幻觉、恶化校准）；而RKL约束的RFT能保持可信度，但由于对探索性偏差的无界惩罚而限制了推理增益。

Method: 提出了CARE-RFT（置信度锚定正则化强化微调），用偏斜反向KL散度替代标准反向KL正则化。该方法提供置信度敏感的惩罚：对于自信且持续获得奖励的探索，惩罚是有界的以支持推理；在其他情况下惩罚是无界的以保持校准。

Result: 在多个模型规模和RFT算法上的广泛实验表明，CARE-RFT实现了优越的平衡，匹配了无约束RFT的推理性能，同时恢复了基础模型的可信度和校准能力。

Conclusion: 研究表明，精心设计的、置信度感知的正则化是构建既具备能力又可信赖的推理模型的关键。

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [276] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: ECCO是一个结合可解释推理与组合搜索的编译器自动调优框架，通过构建思维链数据集让LLM学习优化决策的因果逻辑，然后让LLM作为策略师指导遗传算法的变异操作，在7个数据集上平均减少24.44%的周期数。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒搜索方法缺乏语义指导，而最近的LLM方法往往存在表面模式匹配和因果不透明的问题。需要一种能够桥接可解释推理与组合搜索的框架来解决编译器自动调优中的这一二分法。

Method: 首先提出逆向工程方法构建思维链数据集，将静态代码特征映射到可验证的性能证据，使模型学习优化决策的因果逻辑而非简单模仿序列。然后利用这种可解释先验设计协作推理机制，让LLM作为策略师定义优化意图，动态指导遗传算法的变异操作。

Result: 在7个数据集上的实验结果表明，ECCO显著优于LLVM opt -O3基准，实现了平均24.44%的周期减少。

Conclusion: ECCO成功桥接了可解释推理与组合搜索，通过让LLM学习因果逻辑并指导遗传算法，有效解决了编译器自动调优中传统方法与LLM方法的局限性，实现了显著的性能提升。

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [277] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 本文提出符号转换机制（STM），通过符号抽象和提示工程将数值时间序列数据与语言模型连接，在保持骨干语言模型完整性的同时显著提升时间序列预测效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列预测任务中表现出色，但其巨大的计算和内存需求限制了在轻量级平台上的部署。需要一种方法在保持模型性能的同时大幅降低资源消耗。

Method: 提出符号转换机制（STM），将连续时间序列值通过基于人类认知结构的量化技术转换为符号标记，并通过符号的结构化转换捕捉时间动态，使语言模型能够专注于时间序列数据的关键部分。

Result: 在多种时间序列数据集和四个小型语言模型上评估，STM相比默认骨干SLM实现了MAE误差降低高达69%，MSE误差降低高达90%。资源开销极低，GPU内存仅增加约0.06%，延迟开销仅增加0.64%。

Conclusion: STM作为一种高效、可适应的符号驱动时间序列预测层，在保持骨干语言模型完整性的同时，通过符号抽象显著提升了预测效率和准确性，为轻量级平台上的时间序列预测提供了可行方案。

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [278] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

TL;DR: 提出一个黑盒可解释性框架，通过学习可验证的"宪法"来总结提示词修改如何影响模型特定行为，如对齐性、正确性或约束遵循。该方法利用原子概念编辑系统性地修改提示词，学习从编辑到可预测结果的因果映射。


<details>
  <summary>Details</summary>
Motivation: 需要深入理解大型语言模型和生成模型的行为机制，特别是提示词修改如何影响模型的特定行为（如对齐性、正确性、约束遵循）。现有方法缺乏系统性的因果分析框架来揭示模型内部工作机制。

Method: 采用原子概念编辑（ACEs）框架，通过添加、移除或替换输入提示中的可解释概念，系统性地应用这些编辑并观察模型行为变化，学习从编辑到结果的因果映射，形成可验证的自然语言"宪法"。

Result: 在数学推理和文生图对齐等任务中验证了框架有效性。发现GPT-Image关注语法遵循，Imagen 4优先考虑氛围一致性；数学推理中干扰变量会混淆GPT-5但不影响Gemini 2.5和o4-mini。学习到的宪法在控制模型行为方面效果显著，成功率平均提升1.86倍。

Conclusion: 提出的黑盒可解释性框架能够有效学习模型行为的可验证宪法，为理解和控制模型行为提供了系统性的因果分析方法，在不同任务和模型上都表现出良好的泛化能力和实际应用价值。

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [279] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

TL;DR: 这篇综述系统性地回顾了同时处理群体公平性和个体公平性的混合方法，分析了统一框架下的公平性权衡与整合策略。


<details>
  <summary>Details</summary>
Motivation: 传统的公平性研究通常孤立地处理群体公平性和个体公平性，但在实际决策系统中，需要同时考虑这两种公平性视角。本文旨在填补这一空白，为研究人员和从业者提供设计同时保证个体和群体层面公平性的混合算法的全面资源。

Method: 通过系统性文献综述方法，对混合公平性方法进行分类整理，按照采用的公平机制、算法和数学策略来组织现有方法。对每类方法分析其理论基础、优化机制和实证评估实践。

Result: 提供了混合公平性方法的系统分类和分析框架，识别了现有方法的局限性，并阐明了群体公平性与个体公平性之间的权衡关系。

Conclusion: 混合公平性方法对于设计可靠的决策系统至关重要，但需要开发更加原则化、上下文感知的方法。本文为未来研究指明了方向，包括开发能够同时保证个体和群体层面公平性的算法框架。

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [280] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

TL;DR: 该研究比较了多种机器学习方法在抵押贷款违约预测中的应用，重点关注数据泄露控制和类别不平衡处理，发现AutoGluon模型在严格的时间分割和特征选择下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 抵押贷款违约预测是金融风险管理的核心任务，但现实数据中存在三个主要问题：违约标签模糊性、严重的类别不平衡、以及时间结构和事后变量导致的信息泄露，这些问题会影响评估有效性和部署可靠性。

Method: 使用真实贷款数据集，采用泄漏感知的特征选择、严格的时间分割（限制贷款发放和报告期间）、以及控制性多数类下采样。比较了多种机器学习方法，包括AutoML方法（AutoGluon）。

Result: 在不同正负样本比例下，模型性能保持稳定，AutoGluon在所有评估模型中获得了最强的AUROC（ROC曲线下面积）表现。

Conclusion: 通过严格的泄露控制和类别不平衡处理，机器学习方法可以有效应用于抵押贷款违约预测，AutoML方法显示出优越性能。该研究的扩展教学版本将作为书籍章节出版。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [281] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

TL;DR: ALIGN方法将LLM推理建模为对齐委托游戏，通过多智能体协作生成候选解并在激励机制下选择最终答案，理论上证明能提升性能，实验显示优于单智能体和集成基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂推理任务上表现不佳，传统的单次生成-选择管道效果有限。虽然推理时集成方法可以通过采样多样化推理路径或聚合多个候选答案来改进性能，但这些方法通常将候选答案视为独立处理，且无法保证集成一定能提升推理质量。

Method: 提出ALIGN方法，将LLM推理建模为对齐委托游戏：一个委托人将任务委托给多个智能体，这些智能体在设计的激励机制下生成候选解决方案，然后委托人从中选择最终答案。这种方法在智能体之间引入结构化交互，同时保持智能体与委托人目标的对齐。

Result: 建立了理论保证，表明在公平比较条件下（同等访问候选解决方案），ALIGN能够证明比单智能体生成有更高的期望性能。分析考虑了候选答案的相关性，放宽了先前工作中常用的独立性假设。在广泛的LLM推理基准测试中，ALIGN始终优于强大的单智能体和集成基线方法。

Conclusion: ALIGN通过将LLM推理建模为对齐委托游戏，提供了理论保证的性能提升，并在实验中验证了其有效性，为多智能体LLM推理提供了一种有前景的方法。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [282] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

TL;DR: 本文提出了一种基于量子的并行模型（QBPM）架构，用于使用MRI数据集高效分类阿尔茨海默病阶段，该模型利用量子优势，在量子模拟器上并行运行两个不同的量子电路，实现了高分类精度和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着预期寿命增加，阿尔茨海默病（AD）已成为全球主要健康问题。传统AI方法在AD早期诊断和阶段分类方面存在局限性，数据量增长和计算资源有限需要更快、更高效的方法。量子AI方法利用叠加和纠缠原理以及高维希尔伯特空间，能够超越传统方法的限制，为高维、异构和噪声数据提供更高准确性。

Method: 提出量子基并行模型（QBPM）架构，受经典模型并行原理启发。该模型利用量子优势，采用两个不同的量子电路，每个电路包含旋转和纠缠块，在同一个量子模拟器上并行运行。在两个不同数据集上评估模型性能以测试其鲁棒性和泛化能力。

Result: 该模型在两个数据集上都表现出高分类精度，显示了其整体鲁棒性和泛化能力。在高斯噪声条件下（模拟真实场景）的结果进一步证明了模型不仅在理论上，在实际场景中也具有适用性。与五种不同的经典迁移学习方法相比，该模型使用更少的电路参数，实现了更高的分类精度和可比的执行时间。

Conclusion: 提出的QBPM架构代表了分类复杂疾病（如阿尔茨海默病）阶段的创新且强大的方法，展示了量子方法在医学图像分析中的潜力，为未来量子计算在医疗诊断中的应用提供了有前景的方向。

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [283] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

TL;DR: CodePilot：结合蒙特卡洛树搜索与大型语言模型的混合框架，通过执行引导的程序修复解决GitHub问题，在SWE-bench Lite上达到24.67%的问题解决率。


<details>
  <summary>Details</summary>
Motivation: 在仓库级别进行自动化程序修复面临长时程推理需求和自回归解码限制的挑战，需要更有效的解决方案。

Method: 集成蒙特卡洛树搜索与大型语言模型，进行分层故障定位（仓库→文件→函数），使用MCTS探索多样化补丁轨迹，利用执行反馈作为奖励信号指导搜索和优化，并加入置信度校准生成选择性优化低置信度输出。

Result: 在SWE-bench Lite基准测试中，使用开源权重模型实现了24.67%的问题解决率，优于可比基线方法。

Conclusion: 将符号搜索与神经语言模型相结合是构建可扩展、执行感知的软件工程自动化的有效策略。

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [284] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

TL;DR: RAPTOR是一种基于L2正则化逻辑回归的轻量级探针方法，用于从冻结LLM的层表示中提取概念向量，在准确性、方向稳定性和训练成本方面优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有探针方法在探针-引导流程中需要准确、方向稳定且低成本的概念向量估计，但现有方法在这些方面存在不足，需要一种更有效的解决方案。

Method: 提出RAPTOR方法，使用L2正则化的逻辑回归探针，通过验证集调优的正则化强度，从归一化权重中提取概念向量。

Result: 在指令调优LLM和人工编写概念数据集上的广泛实验表明，RAPTOR在准确性上匹配或超越强基线，同时获得竞争性的方向稳定性和显著更低的训练成本。

Conclusion: RAPTOR提供了一种简单有效的探针方法，通过凸高斯最小最大定理的理论分析解释了正则化强度如何调节探针准确性和概念向量稳定性，这些理论预测与真实LLM嵌入的观察趋势一致。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [285] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

TL;DR: 论文揭示基于遗忘的防御方法无法真正从扩散模型中移除NSFW概念，只是部分破坏了语言符号与底层知识的映射关系，知识作为休眠记忆仍然存在。作者提出IVO攻击框架，通过优化初始潜变量重新激活这些休眠记忆。


<details>
  <summary>Details</summary>
Motivation: 当前基于遗忘的防御方法声称能够从扩散模型中清除NSFW概念，但作者发现这种"遗忘"在很大程度上是假象。未遗忘的知识作为休眠记忆仍然存在，只是映射关系被部分破坏。作者旨在揭示当前防御方法的根本缺陷。

Method: 提出IVO（初始潜变量优化）攻击框架，包含三个核心组件：图像反演、对抗优化和重用攻击。该方法通过优化初始潜变量，使遗忘模型的噪声分布与其原始不安全状态重新对齐，从而重新激活休眠记忆。

Result: 在8种广泛使用的遗忘技术上进行实验，IVO实现了优越的攻击成功率（ASR）和强大的语义一致性，暴露了当前防御方法的根本缺陷。

Conclusion: 基于遗忘的防御方法存在根本性缺陷，无法真正从扩散模型中移除NSFW概念。IVO攻击框架能够有效重新激活休眠记忆，表明当前防御方法需要重新设计。

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [286] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

TL;DR: Benford-Quant是一种受本福特定律启发的非均匀量化器，使用对数间隔码本为小幅度权重分配更多分辨率，在低比特量化中提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速增长需要有效压缩，标准均匀量化器假设参数均匀分布，但实际权重分布高度偏斜，需要更适合实际分布的非均匀量化方法。

Method: 提出Benford-Quant，一种简单、无需数据的非均匀量化器，用对数间隔码本替代均匀网格，基于本福特定律为频繁出现的小幅度权重分配更多分辨率。

Result: 1) 变换器变换层权重符合本福德统计，归一化层系统偏离；2) 在小语言模型上，Benford-Quant持续改善困惑度，在Gemma-270M上4比特困惑度降低超10%；3) 在大语言模型上保持竞争力，差异由过参数化效应解释。

Conclusion: 将本福德先验融入量化网格是低成本修改，在激进低比特量化中带来精度提升。虽然无法在困惑度和LAMBADA等任务上超越SOTA，但可与SmoothQuant、激活感知量化等方法混合使用，无需重大流程修改即可提升性能。

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [287] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

TL;DR: DA-GRPO是一种双优势扩展的组相对策略优化方法，通过将云使用约束直接纳入优势计算，使本地小语言模型能联合学习任务能力和协作行为，在持续学习中实现稳定的云辅助调用。


<details>
  <summary>Details</summary>
Motivation: 本地部署的小语言模型需要在严格的内存和计算约束下持续支持多样化任务，不可避免地需要选择性地依赖云端大语言模型。然而，在持续学习中调节云辅助具有挑战性，因为基于奖励的强化学习通常会产生不稳定的卸载行为，并在任务分布变化时加剧灾难性遗忘。

Method: 提出DA-GRPO（双优势组相对策略优化），这是组相对策略优化的扩展，将云使用约束直接纳入优势计算，避免固定的奖励塑造和外部路由模型。这种设计使本地模型能够联合学习任务能力和协作行为，允许云请求在训练后自然出现，同时遵守规定的辅助预算。

Result: 在数学推理和代码生成基准测试中，DA-GRPO相比之前的协作和基于路由的方法，提高了切换后的准确性，显著减少了遗忘，并保持了稳定的云使用。

Conclusion: DA-GRPO通过将云使用约束直接纳入策略优化过程，实现了本地小语言模型在持续学习环境中更稳定、高效的云辅助协作，解决了传统方法中的不稳定卸载和灾难性遗忘问题。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [288] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

TL;DR: BIONIC是一个贝叶斯多模态临床数据集成框架，专门处理高维异构数据和结构化缺失问题，在部分观测和半监督场景下实现鲁棒预测和可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 多模态临床数据存在高维度、异构表示和结构化缺失等挑战，传统方法难以有效整合这些数据并进行预测建模和可解释性分析。

Method: 提出BIONIC框架，采用贝叶斯多模态生成-判别式潜在架构，使用预训练嵌入处理复杂模态（医学图像和临床文本），将结构化临床变量直接纳入贝叶斯多模态公式，显式建模模态级和变量级缺失以及标签缺失。

Result: 在三个多模态临床和生物医学数据集上评估，相比代表性多模态基线方法，BIONIC展现出强大且一致的判别性能，特别是在不完整数据场景下表现优异。

Conclusion: BIONIC不仅提供高预测准确性，还通过其潜在结构提供内在可解释性，支持模态相关性的群体级分析和临床有意义的洞察。

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [289] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: GASP方法通过对抗性自博弈训练，使强化学习模型能够在有缺陷的上下文条件下（如错误的思维链）进行检测和修复，提高模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）模型在上下文条件有缺陷时会严重失败，因为标准RLVR只优化在干净条件下的最终答案正确性。需要一种方法来训练模型在错误上下文条件下的检测和修复能力。

Method: GASP（引导对抗性自博弈）方法：在单个模型内形成对抗性自博弈游戏，污染者学习通过局部连贯的破坏来诱导失败，而智能体学习在相同破坏条件下进行诊断和恢复。为解决训练早期成功恢复的稀缺性，提出分布内修复引导，通过自生成修复的模仿项来增加恢复概率。

Result: 在四个开放权重模型（1.5B-8B）上，GASP将强但脆弱的推理器转化为鲁棒的推理器，能够承受误导和扰动的上下文，同时通常还能提高干净准确性。分析显示对抗性破坏产生了有效的课程学习，分布内引导实现了快速的恢复学习。

Conclusion: GASP方法仅使用结果验证，无需人工标签或外部教师，就能有效训练模型的检测和修复能力，显著提高强化学习模型在缺陷上下文条件下的鲁棒性。

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [290] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

TL;DR: 提出GEPC方法，通过检测扩散模型分数场的等变性破坏来识别分布外数据，无需训练且计算轻量


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的OOD检测方法主要利用分数大小或局部几何特征，忽略了分数场的等变性特征。扩散模型学习的分数场通常从训练数据和卷积架构中继承了近似等变性（翻转、旋转、循环移位等），这些等变性在分布外数据中可能被破坏

Method: 提出Group-Equivariant Posterior Consistency (GEPC)，一种无需训练的方法，通过测量学习到的分数在有限群变换下的变换一致性来检测等变性破坏。该方法仅需分数评估，生成可解释的等变性破坏图，并在群体层面推导了理想GEPC残差的理论界限

Result: 在OOD图像基准数据集上，GEPC与现有扩散基线相比达到竞争性或改进的AUROC性能，同时保持计算轻量。在高分辨率合成孔径雷达图像上，GEPC实现了强目标-背景分离，并生成视觉可解释的等变性破坏图

Conclusion: GEPC通过检测扩散模型分数场的等变性破坏，为OOD检测提供了一种有效且可解释的方法，特别适用于分数大小不变但等变性被破坏的情况，在多种数据集上表现出色

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [291] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 论文提出了一种贝叶斯方法，通过考虑损失函数的黎曼几何结构来平衡生成模型的记忆化和泛化能力，减少记忆化同时保持泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型虽然能产生逼真样本，但在记忆化和泛化之间取得平衡仍然是一个开放问题。作者从贝叶斯角度出发，关注流匹配和扩散模型的参数空间，旨在构建能更好捕捉数据分布变异性的预测后验分布。

Method: 采用贝叶斯方法，利用黎曼度量捕捉损失函数的几何结构，并采用灵活的近似后验分布来适应损失景观的局部结构。这种方法允许采样与原始模型相似但记忆化程度降低的生成模型。

Result: 实验证明，所提出的方法在保持泛化能力的同时减少了记忆化。理论分析也解释了这一发现。

Conclusion: 研究表明，考虑损失函数的几何结构能够有效利用参数空间，即使对于复杂的高维生成模型也是如此。这为解决生成模型中记忆化与泛化的平衡问题提供了一种新思路。

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [292] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

TL;DR: 该研究提出了一种基于SHAP解释的异常检测器表征方法，通过量化模型对输入特征的重要性分配来评估检测器之间的相似性，从而构建更互补、更有效的集成学习系统。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测面临数据分布多样性和缺乏标签的挑战。集成方法虽能减少个体偏差并提高鲁棒性，但现有检测器往往依赖相似的决策线索，导致异常评分冗余，难以构建真正互补的集成系统。

Method: 使用SHAP（SHapley Additive exPlanations）量化每个异常检测器对输入特征的重要性分配，构建检测器的"解释剖面"。基于这些解释剖面测量检测器之间的相似性，识别具有不同决策机制的互补模型。

Result: 研究发现：1）具有相似解释的检测器会产生相关的异常评分并识别大量重叠的异常；2）解释差异可靠地指示了互补的检测行为；3）解释驱动指标为集成模型选择提供了不同于原始输出的新标准；4）仅有多样性不足，个体模型的高性能仍是有效集成的前提。

Conclusion: 通过明确针对解释多样性同时保持模型质量，能够构建更多样化、更互补且最终更有效的无监督异常检测集成系统。解释驱动的方法为集成学习中的模型选择提供了新的视角。

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [293] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 研究发现大型语言模型存在"嵌入凝聚"现象，即token嵌入在低维空间中聚集，小模型比大模型更容易出现此现象，提出分散损失函数来缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数规模不断增加导致计算成本高昂，为了理解模型缩放机制，研究大模型与小模型在表示能力上的差异，目标是让小模型复制大模型的表示质量。

Method: 通过系统分析多个Transformer模型家族，发现小模型存在严重的"嵌入凝聚"现象，提出分散损失函数在训练中显式鼓励嵌入分散，以对抗凝聚现象。

Result: 实验表明分散损失函数能有效缓解嵌入凝聚，恢复大模型中的分散模式，在10个基准测试中带来性能提升，且不增加额外参数。

Conclusion: 嵌入凝聚是影响小模型性能的重要因素，通过分散损失函数可以改善小模型的表示质量，为提升小模型性能提供了有原则的方法。

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [294] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

TL;DR: GRIP2是一种深度学习特征选择方法，通过二维正则化表面积分和块随机采样，在高相关性、低信噪比场景下实现错误发现率控制并保持高检测能力。


<details>
  <summary>Details</summary>
Motivation: 在非线性、高相关性、低信噪比场景下，深度学习特征选择方法面临控制错误发现率的基本挑战，需要开发更稳健的特征选择方法。

Method: 提出GRIP2方法：1）使用二维正则化表面控制稀疏强度和稀疏化几何；2）引入块随机采样在单次训练中近似表面积分；3）构造反对称统计量确保有限样本FDR控制。

Result: 在合成和半真实数据实验中，GRIP2在高相关性和低信噪比场景下表现出更好的鲁棒性；在真实HIV耐药性数据中，比现有线性基线方法更好地识别出已知耐药突变。

Conclusion: GRIP2方法在保持高检测能力的同时严格控制错误发现率，在高相关性、低信噪比的复杂场景下具有优越性能，并在实际应用中验证了可靠性。

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [295] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

TL;DR: 提出BoE Steering方法，通过单次反向传播近似无限视野前瞻，解决MDM采样中的轨迹锁定问题，提升非自回归生成的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 当前MDM采样方法依赖简单的基于置信度的启发式方法，忽略了局部决策的长期影响，导致早期幻觉引发全局不一致性。基于搜索的方法虽然能缓解此问题，但计算成本过高

Method: 提出Backward-on-Entropy (BoE) Steering框架，从轨迹成本函数的一阶展开形式推导Token Influence Score (TIS)，证明未来熵相对于输入嵌入的梯度可作为最小化不确定性的最优控制信号。引入ActiveQueryAttention稀疏伴随原语，利用掩码目标结构降低反向传播复杂度

Result: BoE在推理时间缩放方面实现了优于现有解掩码方法的帕累托前沿，证明梯度引导的转向为鲁棒非自回归生成提供了数学原理清晰且高效的路径

Conclusion: 梯度引导的转向为掩码扩散模型提供了一种数学原理清晰且计算高效的采样方法，解决了轨迹锁定问题，提升了非自回归生成的鲁棒性和效率

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [296] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

TL;DR: VoxServe是一个用于语音语言模型的统一流式服务系统，通过解耦模型架构与系统优化，实现了低延迟、高吞吐量的流式语音处理。


<details>
  <summary>Details</summary>
Motivation: 现有的语音语言模型流式服务系统在支持多样化模型方面不够灵活高效，无法同时满足低延迟、高吞吐量和强流式保证的需求。

Method: VoxServe引入了模型执行抽象来解耦模型架构与系统级优化，实现了流式感知调度和异步推理流水线，提高端到端效率。

Result: 在多个现代语音语言模型上的评估显示，VoxServe在保持相似延迟的情况下，比现有实现获得了10-20倍的吞吐量提升，同时保持高流式可行性。

Conclusion: VoxServe为语音语言模型提供了一个统一、高效的流式服务框架，通过创新的系统设计显著提升了流式语音处理的性能。

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [297] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 该论文分析了约束双层强化学习算法的样本复杂度，提出了CBSO算法，获得了O(ε^{-2})的迭代复杂度和$\tilde{O}(ε^{-4})$的样本复杂度，首次使用Moreau包络分析了非光滑目标函数的策略梯度RL算法。


<details>
  <summary>Details</summary>
Motivation: 元学习、分层学习和人类反馈强化学习等许多重要RL问题都可以建模为双层RL问题。虽然这些领域在实证上取得了很大进展，但双层RL算法的理论分析尚未得到足够关注。本文旨在分析约束双层RL算法的样本复杂度。

Method: 提出了约束双层次梯度优化(CBSO)算法，使用惩罚目标函数避免约束双层问题中的原始-对偶间隙和超梯度问题。采用Moreau包络分析非光滑优化问题，首次分析了具有非光滑目标函数的参数化策略梯度RL算法。

Result: 获得了O(ε^{-2})的迭代复杂度和$\tilde{O}(ε^{-4})$的样本复杂度。通过惩罚公式处理约束，避免了原始-对偶间隙问题，为约束双层RL提供了理论保证。

Conclusion: 该工作填补了约束双层RL理论分析的空白，提出的CBSO算法在理论上具有良好复杂度，为元学习、分层学习和RL-HF等双层RL问题提供了理论基础和分析框架。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [298] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 该研究提出了一个信息论框架来分析掩码扩散模型中的两个关键失败来源：顺序敏感性和并行化偏差，揭示了易优先解码、因子化并行解码的采样误差以及验证成本等核心机制。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型通过牺牲序列确定性来加速推理，但生成顺序的理论机制和并行化的风险尚未得到充分探索。本研究旨在提供一个统一的理论框架来分析这些失败的根本原因。

Method: 提出了一个信息论框架，将失败来源解耦为顺序敏感性和并行化偏差。通过理论分析揭示了易优先解码的益处、因子化并行解码的采样误差特性，并在受控的Block-HMM模型和大规模MDMs（如LLaDA）上进行算术推理实验验证。

Result: 研究得出三个关键发现：1）易优先解码的益处随模型误差增加而放大；2）因子化并行解码会引入可能导致任意大反向KL散度的内在采样误差；3）验证可以消除采样误差但需要指数级成本，而重新掩码等启发式方法无法保证分布正确性。

Conclusion: 该研究为理解掩码扩散模型的生成机制提供了理论框架，揭示了并行化过程中的风险与权衡，为改进MDMs的设计和评估提供了理论基础。

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [299] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: 提出一种新的自注意力计算方法，能以恒定成本计算任意精度，显著降低内存使用和计算需求，实现无限制的token生成。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型的自注意力机制成本随上下文长度增加而增加，导致存储、计算和能源需求超出社会供应能力，需要更高效的解决方案。

Method: 通过将传统自注意力公式的泰勒展开分解为对称张量链表达式，利用对称性获得前馈变换，将查询和键高效映射到最小多项式核特征基的坐标中。

Result: 实现了恒定成本的自注意力计算，内存使用和计算量大幅减少，成本与头大小成反比固定，支持每个token使用更多注意力头。

Conclusion: 该方法能以适度固定成本实现无限制token生成，显著降低大规模Transformer模型的基础设施和能源需求，所引入的数学技术具有独立研究价值。

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [300] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出LatentTSF新范式，将时间序列预测从观测空间回归转移到潜在状态预测，解决"潜在混沌"问题，通过自编码器将观测映射到高维潜在空间进行预测，在多个基准测试中取得优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型存在"潜在混沌"问题：预测准确的模型学习的潜在表示往往是时间无序且缺乏连续性的。作者认为这是由于主流的观测空间预测范式导致的，模型最小化噪声和部分观测数据的逐点误差，鼓励了捷径解决方案而非恢复底层系统动态。

Method: 提出LatentTSF新范式，使用自编码器将每个时间步的观测投影到高维潜在状态空间，旨在捕获底层系统变量并施加更平滑的时间结构。预测完全在潜在空间中进行，使模型专注于学习结构化时间动态。理论分析表明提出的潜在目标隐式地最大化预测潜在状态与真实状态及观测之间的互信息。

Result: 在广泛使用的基准测试上进行大量实验，证实LatentTSF有效缓解了潜在混沌问题，实现了优越的性能表现。

Conclusion: LatentTSF通过将时间序列预测从观测回归转移到潜在状态预测，成功解决了"潜在混沌"问题，为时间序列预测提供了新的有效范式。

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [301] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

TL;DR: Harvest框架利用GPU间高速互联，将未使用的GPU内存作为模型权重和KV缓存的临时存储层，显著提升大语言模型推理吞吐量


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理日益受到GPU内存容量的限制，而非计算吞吐量。模型规模增长和自回归解码过程中KV缓存的线性增长加剧了内存压力。现有方法将模型状态和KV张量卸载到主机内存，但由于PCIe带宽有限导致延迟显著增加。

Method: 提出Harvest框架，利用GPU间高速对等互联，动态地将模型权重和KV缓存放置在未使用的GPU内存中。将对等GPU内存视为临时缓存层，在动态内存可用性下保持正确性，同时减少数据移动开销。

Result: 通过Harvest加速两个广泛使用的推理组件（专家层权重和KV缓存条目）的检索，实现了超过2倍的显著吞吐量提升。

Conclusion: Harvest框架通过利用未使用的GPU内存作为临时缓存层，有效解决了大语言模型推理中的内存瓶颈问题，显著提升了推理吞吐量。

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [302] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

TL;DR: 本文提出了一种注意力引导的指导框架，解决了LLM内部激活操纵的三个核心挑战，显著提升了语义概念指导的成功率，并揭示了概念特定特征在模型层间的分布规律。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM指导方法存在显著的脆弱性问题，微小的算法选择差异就会导致概念的可指导性发生巨大变化。为了更有效地理解和操纵LLM中的语义概念表示，需要解决三个核心挑战：相关token嵌入的自动选择、概念相关特征的异质性处理，以及最相关层的识别。

Method: 提出注意力引导的指导框架，该框架：1）利用注意力机制自动选择相关token嵌入来提取概念相关特征；2）考虑概念相关特征在LLM激活中的异质性；3）识别对指导最相关的模型层。在包含512个语义概念的指导基准测试中验证了框架的有效性。

Result: 该框架显著超越了先前的最先进方法，在多种模型架构和规模（包括高达700亿参数的模型）上，成功指导的概念数量几乎翻倍。同时，该框架揭示了概念特定特征在LLM各层中的分布规律。

Conclusion: 提出的注意力引导指导框架为开发高效、高度可扩展的行业级LLM微调算法开辟了新途径，同时为理解LLM中语义概念的存储和表示提供了重要工具。

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [303] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

TL;DR: 提出一种连续时间优化方案，通过每个模型参数的动能调节个体自适应动量系数，自动适应局部曲率以保持稳定性而不牺牲收敛速度。


<details>
  <summary>Details</summary>
Motivation: 大规模优化中需要平衡稳定性和收敛速度，传统方法如mSGD在某些任务上表现不佳，需要一种能自动适应局部曲率并保持稳定性的优化方案。

Method: 引入基于每个模型参数动能调节的个体自适应动量系数，将自适应摩擦与立方阻尼联系起来，通过在mSGD和Adam的连续动力学中增加立方阻尼项来创建具体优化方案。

Result: 在ViT、BERT和GPT2训练任务中，新方法表现出鲁棒性，匹配或优于Adam表现，而在mSGD通常表现不佳的任务上表现良好。

Conclusion: 提出的自适应动量方案通过立方阻尼机制有效平衡稳定性和收敛速度，在多种大规模深度学习任务中表现优异，并具有理论上的指数收敛保证。

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [304] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

TL;DR: 本文提出TEMSA方法，通过提取图像中所有检测到的物体名称并与相关文本结合，改善多模态情感分析效果。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临文本和图像模态差异、情感模糊性和上下文复杂性等挑战，需要更有效的方法来整合图像和文本数据。

Method: 提出TEMSA方法，基于物体识别技术提取图像中所有检测到的物体名称，将其与相关文本结合形成TEMS数据，在两个数据集上分别对图像、文本及其组合进行情感分析实验。

Result: 实验结果表明，仅当使用包含所有物体名称的TEMS数据时，相比单独分析图像或文本，多模态情感分析结果得到改善。

Conclusion: TEMSA方法能有效结合图像和文本数据，为多模态情感分析提供了新思路，证明了物体识别信息对提升多模态情感分析效果的重要性。

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [305] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: SparseKD是一种后训练压缩方法，结合结构化SVD剪枝和自参考知识蒸馏，无需外部教师模型即可实现15-65%的参数压缩，同时保持可接受的质量损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署成本高昂，需要有效的压缩方法。传统知识蒸馏需要外部教师模型，而SparseKD通过自参考蒸馏实现高效压缩，无需额外资源。

Method: 结合结构化SVD剪枝和自参考知识蒸馏。模型通过匹配压缩前的自身概率分布来"自我教学"，无需外部教师模型。保持注意力层不变，仅减少前馈层的密集矩阵乘法。

Result: 自参考蒸馏单独使用可将模型质量相对原始检查点提升39%。结合结构化剪枝，实现15-65%参数减少，质量损失可接受。速度提升完全来自前馈层计算减少，注意力层保持不变。在两个模型家族（0.6B和3.8B参数）上验证，多种子实验显示高可重复性。

Conclusion: SparseKD提供了一种高效、可部署的模型压缩方案，无需外部教师模型、架构修改或定制推理内核，可直接在现有基础设施上部署，与注意力优化方法互补。

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [306] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

TL;DR: MATRIX是一个材料科学多模态推理基准，用于评估视觉实验数据对机制解释推理的影响，发现视觉监督能显著提升实验解释和科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估在训练后阶段加入视觉实验数据是否能超越纯文本监督，提升基于物理机制的推理解释能力。

Method: 引入MATRIX多模态基准，通过对比纯文本训练后与包含配对实验图像的训练后方法，在受控诊断中分离视觉基础化的影响。

Result: 视觉监督使实验解释提升10-25%，纯文本科学推理任务提升5-16%；改进依赖于训练中正确的图像-文本对齐，体现了跨模态表征迁移。

Conclusion: 结构化多模态训练后的益处不仅限于材料科学，在ScienceQA和PubMedQA上也观察到一致改进，证明了跨领域泛化能力。

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [307] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

TL;DR: 提出基于RePaint增强的框架，集成预训练的性能引导去噪扩散概率模型，用于性能和参数约束的工程设计生成，无需重新训练模型即可根据部分参考设计生成缺失组件并满足性能约束。


<details>
  <summary>Details</summary>
Motivation: 传统基于DDPM的方法无法同时支持性能和参数约束下的部分设计重绘，需要一种无需重新训练、能够根据部分参考设计生成满足约束的完整设计的方法。

Method: 提出RePaint增强框架，集成预训练的性能引导DDPM，在推理过程中应用基于掩码的重采样，实现对部分设计在性能和参数约束下的高效可控重绘。

Result: 在参数化船体设计和翼型设计两个代表性问题上验证，能够基于部分参考设计生成具有预期性能的新颖设计，精度达到或优于预训练模型，同时通过固定部分设计实现可控创新。

Conclusion: 该方法为工程应用中的参数约束感知生成设计提供了高效、无需训练的训练解决方案，能够在保持性能约束的同时实现可控创新。

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [308] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

TL;DR: 扩散大语言模型（D-LLMs）相比自回归模型具有内在安全优势，但存在上下文嵌套攻击漏洞


<details>
  <summary>Details</summary>
Motivation: 研究扩散大语言模型（D-LLMs）相对于自回归LLMs（AR-LLMs）的安全特性，特别是其对越狱攻击的内在鲁棒性机制

Method: 分析D-LLMs的扩散轨迹机制，发现其逐步抑制不安全生成的特点，同时识别出"上下文嵌套"这一简单有效的攻击方法

Result: D-LLMs确实对传统AR-LLMs越狱攻击具有内在鲁棒性，但上下文嵌套攻击能有效绕过这种保护，在多个模型和基准测试中达到最先进的攻击成功率，首次成功越狱Gemini Diffusion

Conclusion: D-LLMs的安全优势源于扩散轨迹的逐步抑制机制，但并非绝对安全，上下文嵌套攻击暴露了其关键漏洞，为D-LLMs的早期红队测试提供了重要见解

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [309] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 本文提出了一种将分类数据转换为数值数据的管道，使AutoML能够处理经过更高级编码方案转换的分类数据，并在脏数据集上评估AutoML方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然AutoML方法在分类任务中能够处理数据缺陷，但对于脏分类数据集（具有高基数分类特征）的行为了解较少。现有研究表明形态编码器能提升脏分类数据的预测性能，但这些编码器在AutoML中的效果尚不明确。

Method: 提出一个将分类数据转换为数值数据的管道，使AutoML能够处理经过高级编码方案转换的分类数据。在脏数据集上对AutoML方法的鲁棒性进行基准测试，并与提出的管道进行比较。

Result: 通过比较分析获得了预测性能差异的洞察，并深入研究了AutoML构建的ML管道，超越了这些方法通常返回的最佳模型。

Conclusion: 该研究为AutoML在脏分类数据集上的应用提供了新的管道方法，并深入分析了编码方案对AutoML性能的影响，有助于更好地理解AutoML在处理复杂分类数据时的行为。

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [310] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

TL;DR: scBatchProx：一种基于联邦学习原理的后优化方法，用于改进单细胞RNA测序数据中任意上游方法产生的细胞嵌入，通过批量条件适配器和近端正则化在潜在空间中直接校正批次效应，无需原始表达数据或集中式优化。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序产生大量高维数据，但跨实验数据积累会引入批次效应，掩盖真实生物信号。现有批次校正方法要么校正不足，要么需要在完整数据集上进行集中式重新训练，限制了在分布式和持续演化的单细胞数据环境中的适用性。

Method: scBatchProx是一种后优化方法，受联邦学习原理启发，将每个批次视为客户端，在近端正则化下学习批量条件适配器，直接在潜在空间中校正批次结构，无需原始表达数据或集中式优化。该方法轻量级且可部署，仅优化批次特定的适配器参数。

Result: 实验表明，scBatchProx在整体嵌入质量上持续获得约3-8%的相对提升，在90%的数据-方法对中改善了批次校正，在85%的数据-方法对中改善了生物保守性。

Conclusion: 这项工作为动态单细胞数据系统中学习表示的实际细化迈出了一步，提供了一种轻量级、可部署的批次校正方法，适用于分布式和持续演化的数据环境。

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [311] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

TL;DR: OMatG-IRL是一种基于策略梯度强化学习的框架，可直接在学习的速度场上操作，无需显式计算分数，用于晶体材料的连续时间生成模型，实现了晶体结构预测的强化学习应用。


<details>
  <summary>Details</summary>
Motivation: 连续时间生成模型能够预测稳定的晶体结构，但将明确的目标属性整合到生成过程中仍然具有挑战性。策略梯度强化学习提供了将生成模型与下游目标对齐的原则性机制，但通常需要访问分数，这阻碍了其在仅学习速度场的基于流的模型中的应用。

Method: 提出了OMatG-IRL（Open Materials Generation with Inference-time Reinforcement Learning），这是一种策略梯度强化学习框架，直接操作学习到的速度场，消除了对分数显式计算的需求。该方法利用基础生成动力学的随机扰动，在保持预训练生成模型基线性能的同时，在推理时实现探索和策略梯度估计。

Result: 该方法首次将强化学习应用于晶体结构预测，能够有效强化基于能量的目标，同时通过成分条件保持多样性，性能与基于分数的强化学习方法相当。此外，OMatG-IRL能够学习时间相关的速度退火调度，实现采样效率数量级的提升和生成时间的相应减少。

Conclusion: OMatG-IRL为晶体材料的连续时间生成模型提供了一个有效的强化学习框架，无需显式计算分数，实现了晶体结构预测的强化学习应用，显著提高了采样效率和生成速度。

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [312] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

TL;DR: 本文提供了一个基于数学公式的大语言模型（LLM）统一框架，涵盖训练、对齐和生成过程，将LLM形式化为具有注意力依赖的高维非线性自回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型描述通常基于架构组件和训练过程，缺乏明确的数学公式化表达。本文旨在为研究人员提供一个简洁的数学参考，用方程级别的描述来阐明LLM的训练、对齐和生成过程。

Method: 将LLM形式化为高维非线性自回归模型，其中自注意力机制被描述为重复的双线性-softmax-线性组合。框架涵盖：1）基于下一词预测的预训练；2）对齐方法（RLHF、DPO、RSFT、RLVR）；3）推理时的自回归生成。

Result: 该数学框架能够对对齐诱导的行为（如奉承）、推理时现象（如幻觉、上下文学习、思维链提示、检索增强生成）以及持续学习等扩展进行原理性分析。

Conclusion: 本文提供了一个简洁的数学参考框架，将LLM统一形式化，便于理论分析和进一步的理论发展，同时为解释LLM行为提供了基础。

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [313] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

TL;DR: TAD-LoRA：一种针对去中心化联邦学习的拓扑感知低秩适应框架，解决了LoRA参数在动态通信图下的聚合稳定性问题


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）作为无服务器的联邦学习变体，在参数高效微调方面面临独特挑战。低秩适应（LoRA）的分解结构在去中心化聚合时会产生拓扑依赖的交叉项，这些项在动态通信图下可能破坏训练稳定性。

Method: 提出TAD-LoRA框架，通过协调LoRA因子的更新和混合来控制客户端间的错位。该框架在非凸目标下理论证明收敛，明确描述了拓扑诱导交叉项误差与块坐标表示偏差之间的权衡关系。

Result: 在各种通信条件下的实验验证了分析结果，TAD-LoRA在不同通信场景下均表现出稳健性能：在强连接拓扑中保持竞争力，在中度和弱连接拓扑下获得明显增益，在MNLI数据集上表现尤为突出。

Conclusion: TAD-LoRA成功解决了去中心化联邦学习中LoRA参数聚合的稳定性问题，通过拓扑感知的协调机制实现了在各种通信条件下的稳健性能。

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [314] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

TL;DR: FedMOA是一个基于GRPO的联邦学习框架，用于在异构奖励下进行多目标对齐，通过自适应权重调整和智能聚合策略提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐在联邦学习中存在内存限制问题，GRPO的无critic架构适合设备端训练，但联邦环境面临异构奖励定义、不平衡多目标优化和高训练成本等挑战。

Method: 提出FedMOA框架：本地训练采用基于超梯度下降的在线自适应权重机制，优先考虑主要推理目标；服务器端使用任务和准确率感知的聚合策略，优先选择高质量更新。

Result: 在数学推理和代码生成基准测试中，FedMOA始终优于联邦平均方法，准确率提升高达2.2%，同时改善了全局性能、个性化和多目标平衡。

Conclusion: FedMOA成功解决了联邦GRPO中的系统性挑战，为异构奖励下的多目标对齐提供了有效的解决方案，在保持隐私的同时提升了模型推理能力。

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [315] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

TL;DR: PAIR-Former：一种用于miRNA-mRNA靶向预测的预算感知多实例学习框架，通过廉价全池扫描、多样化候选位点选择，以及置换不变聚合器，在有限计算预算下实现准确预测。


<details>
  <summary>Details</summary>
Motivation: miRNA-mRNA靶向预测是一个大规模预测问题：每个转录本产生大量候选靶位点，但只有配对级别的标签。现有方法面临计算成本高的问题，需要一种在有限计算预算下有效处理大量候选位点的框架。

Method: 提出PAIR-Former（Pool-Aware Instance-Relational Transformer），采用预算关系多实例学习（BR-MIL）框架：1）廉价全池扫描；2）在CPU上选择最多K个多样化的候选靶位点；3）使用置换不变的Set Transformer聚合器处理选中的标记。

Result: 在miRAW数据集上，PAIR-Former在实用操作预算（K*=64）下优于强池化基线方法，同时提供可控制的准确率-计算权衡。理论分析表明预算选择与近似误差（随K减小）和泛化项（由K控制）相关。

Conclusion: PAIR-Former为大规模miRNA-mRNA靶向预测提供了一种有效的预算感知解决方案，通过结合廉价扫描、多样化选择和关系处理，在有限计算资源下实现了更好的性能。

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [316] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

TL;DR: GRASP是一种基于可微世界模型的并行化规划器，通过将状态视为优化变量并引入随机性来解决视觉输入的长时域控制任务。


<details>
  <summary>Details</summary>
Motivation: 世界模型可以从原始感官输入（如视频）模拟环境动态，但用于规划时面临搜索空间庞大且非结构化的挑战。现有规划方法在处理长时域视觉控制任务时效率有限。

Method: 提出GRASP规划器：1) 将状态视为优化变量（"虚拟状态"）并施加软动力学约束；2) 引入状态随机性以促进探索和避免局部最优；3) 修改梯度结构以缓解高维视觉世界模型的敏感梯度问题，仅需动作输入梯度；4) 实现高度并行化计算。

Result: 在基于视频的世界模型实验中，GRASP在长时域任务上优于交叉熵方法（CEM）和普通梯度优化（GD），在成功率和收敛时间方面表现更好。

Conclusion: GRASP作为一种随机化的非凝聚或配点最优控制器，通过利用可微世界模型的梯度信息，能够高效解决视觉输入的长时域控制规划问题。

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [317] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

TL;DR: 本文提出CAL方法，通过利用扩散语言模型在第一步去噪中的统计信号（Oracle Peak和Length Bias），实现无需训练的自适应长度预测，显著提升代码和文本填充性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然天生适合填充任务，但其性能受限于预设的填充长度。现有方法需要指定长度，而实际应用中正确长度通常是未知的，这限制了模型的实用性。

Method: 提出CAL方法，利用扩散语言模型在第一步去噪中表现出的两个关键统计现象：1）在真实长度附近出现的Oracle Peak信号；2）系统性的Length Bias。通过校准这些偏差并进行高效搜索，在正式解码前近似最优填充长度。

Result: CAL在代码填充任务中，Pass@1比固定长度基线提升高达47.7%，比基于聊天的自适应方法提升40.5%；在文本填充任务中，BLEU-2和ROUGE-L分别提升高达8.5%和9.9%。

Conclusion: CAL方法证明了扩散语言模型具有内在发现正确填充长度的能力，通过利用第一步去噪中的统计信号和校准偏差，实现了无需专门训练的鲁棒填充，为DLM填充任务开辟了新途径。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [318] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

TL;DR: 该研究将质量-多样性优化重新表述为具有大量优化目标的多目标优化问题，使得可以直接应用成熟的MOO方法来解决QD问题。


<details>
  <summary>Details</summary>
Motivation: 质量-多样性优化在机器人控制、创意设计和对抗样本生成等领域显示出实用价值，但现有QD算法各有不同的设计原则。本研究旨在建立QD与MOO之间的理论联系，而不是提出新的QD算法。

Method: 将QD优化重新表述为具有大量优化目标的多目标优化问题，特别采用基于集合的标量化技术，通过协作搜索过程解决QD问题。

Result: 理论分析表明该方法继承了MOO的理论保证，同时为QD优化提供了理想特性。在多个QD应用中的实验研究证实，该方法达到了与最先进QD算法相竞争的性能。

Conclusion: 通过将QD重新表述为MOO问题，可以直接利用成熟的MOO方法解决QD问题，这为QD优化提供了新的理论框架和实用工具。

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [319] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

TL;DR: AREAL-DTA：一种基于深度优先搜索的动态树注意力机制，通过共享前缀树结构优化RL训练中的计算效率，实现高达8.31倍的训练吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 基于强化学习的大语言模型后训练计算成本高昂，因为生成的序列经常共享长token前缀。现有RL框架通常独立处理这些序列，在策略模型训练的前向和后向传播中重复计算相同前缀，导致计算和内存使用效率低下。

Method: AREAL-DTA采用基于深度优先搜索的执行策略，在前后向计算中动态遍历rollout前缀树，每次只实例化单个根到叶路径。此外，引入负载均衡的分布式批处理机制，在多个GPU上动态构建和处理前缀树。

Result: 在流行的RL后训练工作负载中，AREAL-DTA在τ²-bench上实现了高达8.31倍的训练吞吐量提升。

Conclusion: AREAL-DTA通过高效利用RL训练中的前缀共享，显著提高了计算效率和训练吞吐量，解决了现有树注意力方法在RL场景中扩展性差的问题。

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [320] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

TL;DR: 本文对工业物联网(IIoT)中基于对比学习的隐私保护技术进行了全面综述，重点分析了工业数据的独特性、系统架构和应用场景，并讨论了现有解决方案、开放挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业物联网在实现预测性维护和跨站点优化的同时，由于运营数据的敏感性带来了显著的隐私和机密性风险。对比学习作为一种自监督表示学习范式，通过减少对标记数据和原始数据共享的依赖，成为隐私保护分析的有前景方法。

Method: 本文采用文献综述方法，系统性地回顾和分析工业物联网领域中基于对比学习的隐私保护技术，特别关注工业数据的独特特征、系统架构和各种应用场景。

Result: 论文提供了工业物联网隐私保护技术的全面概述，识别了基于对比学习方法的优势和适用性，同时指出了当前解决方案的局限性和面临的挑战。

Conclusion: 对比学习在工业物联网隐私保护中具有重要应用价值，但仍存在开放挑战需要解决。论文为未来研究方向提供了框架，包括技术改进、标准化和实际部署等方面。

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [321] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

TL;DR: 该研究使用机器学习模型对EEG信号进行三分类情感识别（消极、中性、积极），比较了逻辑回归、支持向量机和随机森林三种模型，发现随机森林在准确率和F1分数上表现最佳。


<details>
  <summary>Details</summary>
Motivation: EEG信号能够直接反映大脑活动，可用于识别人的情绪状态。随着情感感知系统和EEG情感识别研究的发展，需要探索哪种机器学习模型最适合处理有限EEG数据集的三分类情感识别问题。

Method: 研究采用完整的工作流程，包括数据预处理和机器学习模型比较。在有限的EEG数据集上训练和测试了三种常用模型：逻辑回归(LR)、支持向量机(SVM)和随机森林(RF)。使用准确率和F1分数评估模型性能。

Result: 机器学习模型能够有效用于EEG信号的三分类情感识别。在三种模型中，随机森林模型表现最佳，其更高的准确率和F1分数表明它比其他两种模型更能准确有效地捕捉情感模式。随机森林模型在准确率参数上也优于现有的最先进分类模型。

Conclusion: 随机森林是处理有限EEG数据集三分类情感识别问题的最有效机器学习模型，在准确率和F1分数上均优于逻辑回归和支持向量机，且超越了现有最先进模型的性能。

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [322] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

TL;DR: 动态先验汤普森采样：通过控制新臂胜过现有胜者的概率来改进冷启动探索，避免均匀先验导致的过度探索问题


<details>
  <summary>Details</summary>
Motivation: 大规模推荐系统中冷启动探索的核心挑战：新项目或数据稀疏项目需要流量来估计价值，但过度探索会损害用户体验并浪费曝光机会。实践中汤普森采样通常使用均匀Beta(1,1)先验，隐含假设未见过项目的成功率为50%。当真实基础率远低于此值时，这种乐观先验会系统性地过度分配给弱项目。批处理策略更新和流水线延迟放大了这一问题：新项目在数小时内可能"无数据"，导致先验在反馈被纳入前主导分配。

Method: 提出动态先验汤普森采样，通过先验设计直接控制新臂胜过现有胜者的概率。核心贡献是提供先验均值的闭式二次解，确保在引入时P(X_j > Y_k) = epsilon，使探索强度可预测且可调，同时保留汤普森采样的贝叶斯更新机制。

Result: 通过蒙特卡洛验证、离线批处理模拟以及在服务数百万用户的缩略图个性化系统上进行的大规模在线实验，动态先验相比均匀先验基线实现了精确的探索控制和改进的效率。

Conclusion: 动态先验汤普森采样为推荐系统中的冷启动探索提供了有效的解决方案，通过可调的探索强度控制避免了均匀先验导致的过度探索问题，在实际应用中表现出更好的性能。

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [323] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

TL;DR: 该论文首次研究了在随机非凸极小极大优化中寻找差分隐私二阶平稳点的问题，提出了结合嵌套梯度下降-上升、SPIDER方差减少和高斯扰动的纯一阶方法，为经验风险和总体风险提供了统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有文献要么只关注极小极大问题的一阶平稳点，要么只关注经典随机最小化问题的二阶平稳点。缺乏对随机非凸极小极大优化中差分隐私二阶平稳点的研究，需要为经验风险和总体风险提供统一的理论框架。

Method: 提出纯一阶方法，结合嵌套梯度下降-上升方案、SPIDER风格的方差减少技术和高斯扰动来确保隐私。关键技术是块状（q周期）分析，控制随机方差和隐私噪声的累积，无需在整个迭代范围内求和。

Result: 在标准光滑性、Hessian-Lipschitz性和强凹性假设下，建立了达到(α,√(ρ_Φα))近似二阶平稳点的高概率保证。经验风险目标：α=O((√d/nε)^{2/3})；总体目标：α=O(1/n^{1/3} + (√d/nε)^{1/2})，匹配私有一阶平稳性的最佳已知速率。

Conclusion: 该工作首次系统研究了随机极小极大优化中的差分隐私二阶平稳点问题，为经验风险和总体风险提供了统一的理论框架和算法，达到了与私有一阶平稳性相匹配的最佳收敛速率。

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [324] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: GRPO在不同领域排序策略下的行为分析：研究发现单领域泛化具有高度不对称性，跨领域交互具有顺序依赖性，多领域训练中没有单一最优策略。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO已成为提升大语言模型推理能力的关键技术，但对其在不同领域排序策略下的行为理解不足。特别是顺序训练（一次一个领域）与混合领域训练（多个领域同时进行）在GRPO中的影响尚未得到系统研究。

Method: 对数学、科学、逻辑和谜题推理任务进行训练顺序效应的首次系统分析，比较顺序训练与混合领域训练策略。

Result: 发现三个关键结果：1）单领域泛化高度不对称：在其他领域训练可提升数学推理约25%准确率，但对逻辑和谜题推理几乎没有迁移效果；2）跨领域交互高度依赖顺序：数学→科学顺序训练在数学/科学上达到83%/41%准确率，而科学→数学顺序则降至77%/25%；3）多领域训练中没有单一最优策略：顺序训练有利于数学（达84%），混合训练有利于科学和逻辑，不良排序可能导致较大性能差距（从70%降至56%）。

Conclusion: GRPO在多领域设置下表现出显著的不对称性、顺序敏感性和策略依赖性，突显了领域感知和顺序感知训练设计的必要性。

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [325] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

TL;DR: CONVERSE是一种结合变分自编码器和对比学习的深度生存分析模型，在保持预测性能的同时提供可解释的风险分层


<details>
  <summary>Details</summary>
Motivation: 临床决策需要生存分析来估计时间到事件结果、分层患者风险并指导治疗规划。深度学习虽然具有强大的预测能力，但其黑盒特性限制了临床应用。现有的深度聚类方法虽然可解释，但通常牺牲了预测性能。需要一种既能保持高性能又能提供可解释风险分层的模型。

Method: CONVERSE结合变分自编码器和对比学习，使用变分嵌入和多个簇内、簇间对比损失。采用自步学习从易到难逐步纳入样本以提高训练稳定性。模型支持特定簇的生存头，实现准确的集成预测。

Result: 在四个基准数据集上的综合评估表明，CONVERSE相比现有深度生存方法实现了竞争性或更优的性能，同时保持了有意义的患者分层。

Conclusion: CONVERSE通过统一变分自编码器和对比学习，成功弥合了生存分析中预测性能与可解释性之间的差距，为临床决策提供了既准确又可解释的风险分层工具。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [326] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

TL;DR: SNIP是一个细粒度自适应混合精度训练框架，通过定期收集统计数据和定义量化损失指标，使用整数线性规划优化层间精度，在保持模型质量的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前混合精度训练方法要么对所有GEMM操作应用统一精度，要么依赖基于启发式的方法，这些方法在训练过程中无法泛化，导致次优收敛和不稳定性。需要一种能够支持亚字节精度并保持模型质量的更优解决方案。

Method: SNIP定期收集激活、梯度和优化器状态的统计数据，定义前向传递中的损失发散和后向传递中的权重发散两个关键指标，使用整数线性规划(ILP)系统性地优化层间精度，以最小化整体质量损失同时满足效率目标。

Result: 在1B、3B、7B和70B Llama-like模型上的实验表明，SNIP始终优于现有基线方法，在保持模型质量的同时将FLOPs减少高达80%，且在不同模型大小和训练阶段都具有最小计算开销。

Conclusion: SNIP提供了一个有效的细粒度自适应混合精度训练框架，能够支持亚字节精度，在显著减少计算成本的同时保持大型语言模型预训练的质量，解决了当前混合精度训练方法的局限性。

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [327] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

TL;DR: 提出一种半监督学习方法，通过训练oracle模型筛选正确预测，用于一次性再训练，以提升数据稀缺环境下基于transformer的计算机辅助工艺规划模型性能


<details>
  <summary>Details</summary>
Motivation: 工业中计算机辅助工艺规划（CAPP）面临数据集有限的问题，这降低了模型的泛化能力。需要一种方法能在无需人工标注的情况下提升模型性能

Method: 采用半监督学习方法：1）在可用的transformer行为数据上训练一个oracle模型；2）使用该oracle筛选未见零件上的正确预测；3）将这些筛选出的正确预测用于一次性再训练

Result: 在全数据分布的小规模数据集上进行实验，结果显示该方法相比基线模型获得了持续的准确率提升，证明了在数据稀缺的制造环境中的有效性

Conclusion: 提出的半监督学习方法能够有效提升基于transformer的CAPP模型性能，特别是在数据稀缺的工业环境中，无需额外的人工标注工作

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [328] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出基于分解的因果发现框架，将多变量时间序列分解为趋势、季节和残差分量，分别进行因果分析，最后整合为统一的多尺度因果结构


<details>
  <summary>Details</summary>
Motivation: 金融、气候科学和医疗等领域的时间序列存在长期趋势、季节模式和短期波动，导致在非平稳性和自相关条件下的因果推断困难。现有方法直接在原始观测值上操作，容易产生虚假边和错误归因的时间依赖

Method: 将每个时间序列分解为趋势、季节和残差三个分量，分别进行特定分量的因果分析：趋势分量使用平稳性检验，季节分量使用基于核的依赖度量，残差分量使用基于约束的因果发现方法，最后将分量级图整合为统一的多尺度因果结构

Result: 在广泛的合成基准测试和真实世界气候数据上，该框架比现有最先进基线方法更准确地恢复真实因果结构，特别是在强非平稳性和时间自相关条件下表现更优

Conclusion: 分解框架能够分离长期和短期因果效应，减少虚假关联，提高可解释性，为复杂时间序列数据提供更可靠的因果发现方法

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [329] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: P-EAGLE将EAGLE从自回归生成转换为并行多token预测，通过可学习的共享隐藏状态和训练优化框架，在长上下文场景下实现1.10-1.36倍加速。


<details>
  <summary>Details</summary>
Motivation: 推理大语言模型产生更长输出，需要训练在长序列上的推测解码草稿模型。并行草稿（每次前向预测多个token）相比顺序生成有延迟优势，但训练复杂度随序列长度和并行位置乘积呈二次方增长，使得长上下文训练不切实际。

Method: 提出P-EAGLE，通过可学习的共享隐藏状态将EAGLE从自回归转换为并行多token预测。开发包含注意力掩码预计算和序列分区技术的训练框架，支持在单个序列内进行梯度累积的并行预测训练。

Result: 在vLLM中实现P-EAGLE，在GPT-OSS 120B、20B和Qwen3-Coder 30B模型上相比自回归EAGLE-3实现了1.10-1.36倍的加速。

Conclusion: P-EAGLE成功解决了长上下文并行草稿训练的计算复杂度问题，通过创新的训练框架实现了显著的推理加速，为大规模语言模型的高效推测解码提供了可行方案。

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [330] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

TL;DR: 提出因果偏好获取框架，通过主动查询专家对局部边关系的判断来加速因果图后验分布收敛


<details>
  <summary>Details</summary>
Motivation: 现有的因果发现方法通常只依赖观测数据，但专家知识可以显著提高因果方向判断的准确性。然而，如何高效地整合专家知识并最小化专家查询成本是一个挑战。

Method: 提出贝叶斯框架，从任何黑盒观测后验出发，通过三值似然函数建模专家对边存在性和方向的噪声判断。使用粒子近似进行后验推断，并基于专家分类响应的期望信息增益准则高效选择查询问题。

Result: 在合成图、蛋白质信号数据和人类基因扰动基准测试中，该方法在有限的查询预算下实现了更快的后验收敛和更好的有向效应恢复效果。

Conclusion: 因果偏好获取框架能够有效整合专家知识，显著提高因果发现效率，特别是在查询预算有限的情况下表现出优越性能。

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [331] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

TL;DR: OpInf-LLM：基于算子推理的LLM参数化PDE求解框架，利用少量解数据实现精确预测，支持自然语言指定PDE求解任务，在异构设置中实现高执行成功率


<details>
  <summary>Details</summary>
Motivation: 解决异构偏微分方程（PDE）是科学与工程的基础问题。虽然大语言模型在代码生成、符号推理和工具使用方面表现出强大能力，但在不同设置下可靠地求解PDE仍然具有挑战性。现有方法在执行成功率和数值精度之间存在权衡，特别是在泛化到未见参数和边界条件时。

Method: 提出OpInf-LLM框架，基于算子推理的LLM参数化PDE求解方法。该框架利用少量解数据实现精确预测，支持自然语言指定PDE求解任务，通过低计算需求和统一工具接口实现高执行成功率。

Result: OpInf-LLM能够准确预测包括未见参数和配置在内的多样化PDE实例，在异构设置中实现高执行成功率，为LLM-based PDE求解中的可泛化降阶建模开辟了新可能性。

Conclusion: 通过将算子推理与LLM能力相结合，OpInf-LLM为LLM-based PDE求解中的可泛化降阶建模开辟了新途径，解决了执行成功率与数值精度之间的权衡问题。

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [332] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: 单时间尺度演员-评论家算法在无限时域折扣MDP中达到O(ε⁻²)的最优样本复杂度，相比之前O(ε⁻³)有显著改进


<details>
  <summary>Details</summary>
Motivation: 现有演员-评论家算法在无限时域折扣马尔可夫决策过程中样本复杂度为O(ε⁻³)，需要改进以实现更高效的强化学习算法

Method: 结合STORM方差减少技术和样本缓冲区：1) 使用STORM减少评论家更新的方差；2) 维护最近样本的小缓冲区进行均匀采样，解决非平稳分布问题

Result: 实现了O(ε⁻²)的最优样本复杂度，与现有深度学习架构兼容，只需微小修改

Conclusion: 通过STORM方差减少和样本缓冲区的组合，单时间尺度演员-评论家算法在无限时域折扣MDP中达到了理论最优的样本复杂度

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [333] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于遗传编程的特征构建框架，通过联合优化经验风险和vicinal Jensen gap来控制过拟合，并引入噪声估计和流形入侵检测机制，在58个数据集上取得了优于15种机器学习算法的性能。


<details>
  <summary>Details</summary>
Motivation: 遗传编程特征构建虽然取得了显著成功，但过拟合问题限制了其更广泛的应用。需要改进泛化能力，控制过拟合现象。

Method: 1. 证明vicinal风险可通过经验风险与正则化项（有限差分或vicinal Jensen gap）之和来界定；2. 提出进化特征构建框架，联合优化经验风险和vicinal Jensen gap；3. 开发噪声估计策略动态调整正则化强度；4. 提出流形入侵检测机制防止数据增强生成不现实的样本。

Result: 在58个数据集上的实验结果表明，Jensen gap最小化相比其他复杂度度量更有效。与15种机器学习算法的比较显示，采用所提过拟合控制策略的遗传编程获得了更优的性能。

Conclusion: 通过理论证明和实验验证，提出的基于vicinal Jensen gap的正则化方法能有效控制遗传编程特征构建中的过拟合问题，提高泛化性能。

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [334] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

TL;DR: 提出从呼吸信号合成睡眠脑电图的跨生理学翻译任务，通过波形条件生成框架实现，在28,000多人数据上训练，EEG频谱图重建误差7%，合成EEG在下游任务中表现接近真实EEG


<details>
  <summary>Details</summary>
Motivation: 探索从呼吸信号合成睡眠脑电图的可能性，解决两种生理信号模态间的显著复杂性差距，为远程、非接触式神经评估提供新方法

Method: 提出波形条件生成框架，通过离散标记化约束EEG目标空间，同时保留细粒度呼吸动态特征，支持从接触式呼吸信号到无线射频反射信号的多源输入

Result: EEG频谱图重建平均绝对误差7%；合成EEG在年龄估计（MAE 5.0 vs 5.1年）、性别检测（AUROC 0.81 vs 0.82）和睡眠分期（准确率0.84 vs 0.88）任务中表现接近真实EEG，显著优于直接在呼吸数据上训练的基线方法

Conclusion: 成功实现从呼吸信号合成睡眠脑电图，框架可推广到无线射频反射信号，证明了远程、非接触式睡眠期间神经评估的可行性

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [335] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

TL;DR: 研究发现多任务训练能促进世界表征的几何对齐，但某些"发散性任务"会损害新实体的表征整合能力


<details>
  <summary>Details</summary>
Motivation: 研究神经表征的几何特性及其在下游任务适应性的作用，目前对这些条件理解不足

Method: 建立分离世界、数据生成过程和模型表征的框架，使用5,075个城市坐标定义世界，7个几何任务生成自回归训练数据，研究多任务训练和微调

Result: 不同任务产生不同的世界表征几何；多任务训练促进表征对齐；但某些发散性任务会损害新实体的表征整合和泛化能力

Conclusion: 多任务训练能可靠产生收敛的世界表征，但发散性任务可能通过微调灾难性地损害新实体的整合

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [336] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种名为COMB的位置无关缓存系统，通过为仅解码器LLM重新引入编码器并显式训练来支持位置无关缓存，显著降低了首词生成时间并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的键值缓存是基于前缀的，对于按任意顺序检索的上下文处理效率低下。现有的位置无关缓存方法往往导致显著的准确性下降，限制了实际应用。

Method: 提出原生位置无关缓存方法：为流行的仅解码器LLM重新引入编码器，并显式训练以支持位置无关缓存。开发了COMB系统，这是一个与现有推理框架无缝集成的PIC感知缓存系统。

Result: 实验结果显示，COMB将首词生成时间降低了51-94%，吞吐量提高了3倍，同时保持了相当的准确性。在DeepSeek-V2-Lite-Chat上的质量改进证明了COMB对其他类型仅解码器LLM的适用性。

Conclusion: COMB系统通过重新引入编码器和显式训练，有效解决了位置无关缓存中的准确性问题，显著提升了LLM推理效率，具有广泛的适用性。

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [337] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

TL;DR: 该论文提出了一种跨模态对齐框架，将质谱数据直接映射到预训练化学语言模型的分子结构嵌入空间，解决了质谱分子识别中的泛化瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常将质谱匹配视为封闭集识别任务，限制了其对未见分子骨架的泛化能力。质谱峰与化学结构之间存在语义鸿沟，这是分子识别的基本挑战。

Method: 提出跨模态对齐框架，将质谱数据直接映射到预训练化学语言模型的分子结构嵌入空间，将物理光谱分辨率与分子结构嵌入显式集成。

Result: 在严格骨架不相交基准测试中，模型在固定256路零样本检索中达到42.2%的Top-1准确率，在全局检索设置下表现出强泛化能力。学习到的嵌入空间具有强化学一致性，在5路5样本分子重识别中达到95.4%准确率。

Conclusion: 将物理光谱分辨率与分子结构嵌入显式集成是解决质谱数据分子识别泛化瓶颈的关键，跨模态对齐框架为分子识别提供了有效的解决方案。

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [338] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

TL;DR: 本文提出了一个相对预算理论来解释强化学习在不同任务和计算预算下的效果差异，通过相对预算ξ=H/E[T]这一单一量来预测RL的样本效率，并识别了三个学习机制：不足、平衡和充足。


<details>
  <summary>Details</summary>
Motivation: 强化学习是提升大语言模型推理能力的主要范式，但其效果在不同任务和计算预算下差异很大。现有研究缺乏一个统一的理论框架来解释这种差异，无法有效指导实践中的计算资源分配。

Method: 提出相对预算理论，定义相对预算ξ=H/E[T]，其中H是生成视野（token预算），T是基础策略下首次得到正确解所需的token数。通过理论分析揭示ξ如何控制奖励方差和信息轨迹概率，从而影响样本效率。在理想化分布假设下进行案例研究，并提供在线RL的有限样本保证。

Result: 理论分析识别了三个学习机制：不足机制（ξ→0）信息轨迹罕见，样本复杂度爆炸；平衡机制（ξ=Θ(1)）信息轨迹出现概率不可忽略，RL样本效率最高；充足机制（ξ→∞）学习稳定但边际收益递减。实证结果证实了这些预测，发现ξ∈[1.5, 2.0]时学习效率最高，与峰值推理性能一致。

Conclusion: 相对预算ξ是预测RL样本效率的关键单一量，为实践中计算资源分配提供了理论指导。在平衡机制下RL能达到最大样本效率，这一发现有助于优化大语言模型推理能力的强化学习训练。

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [339] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

TL;DR: 该论文通过理论分析发现，SAM优化器通过降低简单性偏置(SB)来提升泛化性能，并证明通过修改训练数据分布（上采样或增强后期学习的样本）同样可以降低SB，从而在多个LLM上显著提升数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过修改训练数据分布来引导优化器找到具有更好泛化性能的解决方案，特别是针对大型语言模型训练中SAM优化器计算成本过高的问题。

Method: 1. 理论分析基于上下文线性回归模型和多头线性自注意力机制；2. 比较梯度下降(GD)和锐度感知最小化(SAM)的训练动态；3. 提出通过上采样或增强后期学习样本来修改训练数据分布的策略；4. 在多个LLM上进行实验验证。

Result: 1. 首次证明SAM通过降低简单性偏置(SB)来提升泛化性能；2. 修改训练数据分布同样能降低SB并改善泛化；3. 在Phi2-2.7B、Llama3.2-1B、Gemma3-1B-PT和Qwen3-0.6B-Base等模型上，使用AdamW和Muon优化器微调后，数学推理任务准确率相对提升最高达18%。

Conclusion: 通过修改训练数据分布来降低简单性偏置是一种有效提升LLM泛化性能的策略，为训练大型语言模型提供了一种计算效率更高的替代方案，避免了SAM优化器的高计算成本。

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [340] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: PENCIL是一个用于图链接预测的纯Transformer编码器，通过注意力机制处理采样子图，无需手工先验知识，在保持标准Transformer可扩展性和硬件效率的同时，超越了启发式GNN和ID嵌入方法。


<details>
  <summary>Details</summary>
Motivation: 当前图链接预测方法存在局限性：GNN依赖显式结构启发式或内存密集的节点嵌入，难以泛化和扩展到大规模图；图Transformer因复杂结构编码带来显著开销，阻碍在大规模链接预测中的应用。需要一种既高效又能捕捉丰富拓扑依赖的方法。

Method: 提出PENCIL方法，这是一个仅编码器的普通Transformer，用注意力机制处理采样的局部子图来替代手工先验知识。该方法保留了标准Transformer的可扩展性和硬件效率，通过注意力从子图中提取结构信号，隐式泛化广泛的启发式和基于子图的表达能力。

Result: 实验和理论分析表明，PENCIL比GNN提取更丰富的结构信号，性能优于启发式GNN，参数效率远高于基于ID嵌入的替代方案，在不同基准测试中保持竞争力，即使在没有节点特征的情况下也能表现良好。

Conclusion: PENCIL挑战了当前依赖复杂工程技术的范式，证明简单的设计选择可能足以实现相同的功能，为大规模图链接预测提供了高效且有效的解决方案。

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [341] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

TL;DR: TIC-FM是一个用于时间序列基础模型零样本评估的上下文学习框架，无需参数更新即可预测测试标签，避免了传统方法中分类器训练带来的评估偏差。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列基础模型的零样本评估方法使用冻结编码器加任务特定分类器，这违反了零样本部署的无训练前提，并因分类器依赖的训练选择而引入评估偏差。

Method: 提出TIC-FM框架，将标记训练集作为上下文，通过单次前向传播预测所有测试实例标签，无需参数更新。框架包含时间序列编码器、轻量级投影适配器和分割掩码潜在记忆Transformer。

Result: 在128个UCR数据集上的实验显示TIC-FM具有强大的准确性，在极端低标签情况下表现一致提升，突显了无训练迁移的优势。

Conclusion: TIC-FM通过上下文学习框架解决了时间序列基础模型零样本评估中的训练依赖问题，理论上证明了上下文推理可以替代训练分类器，并在实验中验证了其有效性。

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [342] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

TL;DR: 本文提出InfoTok，一种基于信息瓶颈原则的信息正则化视觉标记化机制，用于统一多模态大语言模型，通过控制信息流实现压缩与任务相关性的权衡，在理解和生成任务上均取得改进。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态大语言模型中的共享标记设计大多是架构驱动的，缺乏明确的标准来确定标记应该保留什么信息来同时支持理解和生成任务。作者从容量受限的角度出发，认为视觉标记器应作为计算受限的学习者，标记预算应优先考虑可重用的结构而非难以利用的高熵变化和冗余。

Method: 提出InfoTok，一种基于信息瓶颈原则的信息正则化视觉标记化机制。该方法将标记化过程表述为控制从图像到共享标记再到多模态输出的信息流，通过互信息正则化实现压缩与任务相关性的原则性权衡。该方法无需额外训练数据，可集成到三种代表性统一MLLMs中。

Result: 实验表明，InfoTok在理解和生成任务上均带来一致性的改进，支持信息正则化标记化作为在统一MLLMs中学习共享标记空间的原则性基础。

Conclusion: 信息正则化标记化为统一多模态大语言模型中的共享标记空间学习提供了原则性基础，通过信息瓶颈原则控制信息流，在有限标记预算下优先保留可重用结构，从而同时提升理解和生成性能。

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [343] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

TL;DR: 研究通过心理语言学实验探究LLM隐藏状态几何结构能否从其行为中恢复，发现强制选择任务比自由联想更能反映隐藏状态几何，行为相似性可以预测未见隐藏状态相似性


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型的隐藏状态几何结构是否能够通过其在心理语言学实验中的行为来恢复，理解行为测量能否揭示内部语义几何结构

Method: 使用8个指令调优的transformer模型，在两个实验范式（基于相似性的强制选择和自由联想）上对5000个共享词汇进行测试，收集1750万+试验构建行为相似性矩阵，使用表征相似性分析比较行为几何与层间隐藏状态相似性，并与FastText、BERT和跨模型共识进行基准比较

Result: 强制选择行为与隐藏状态几何结构的对齐程度显著高于自由联想；在保留词汇回归中，行为相似性（特别是强制选择）能够超越词汇基线和跨模型共识预测未见隐藏状态相似性，表明仅基于行为的测量保留了关于内部语义几何结构的可恢复信息

Conclusion: 行为任务能够揭示隐藏的认知状态，强制选择范式比自由联想更能反映LLM的内部语义几何结构，行为测量保留了关于模型内部表示的实质性信息

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [344] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

TL;DR: 论文证明了MFL-DA动力学在熵正则化二人零和博弈中的局部指数稳定性：当初始化足够接近混合纳什均衡时，动力学以指数速率收敛到均衡点。


<details>
  <summary>Details</summary>
Motivation: 尽管均值场目标函数存在唯一的混合纳什均衡，但对于一般非凸非凹收益函数的原始MFL-DA动力学的长期行为仍不清楚。本文旨在回答Wang和Chizat在COLT 2024中提出的关于局部稳定性和定量收敛速率的问题。

Method: 通过谱分析线性化算子，建立熵在均衡点附近的强制性估计。该强制性估计有效揭示了局部位移凸凹结构，从而驱动收缩。

Result: 证明了均衡点是局部指数稳定的：如果初始化在Wasserstein度量下足够接近均衡点，动力学将以指数速率趋向均衡点。

Conclusion: 该结果解决了Wang和Chizat提出的局部稳定性和定量速率问题，但全局收敛仍是一个未解决的挑战。

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [345] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 本文提出针对偏好学习的新型主动学习算法，相比传统实验设计方法能更高效地收集人类偏好标签数据。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型需要高质量的人类偏好标签数据集，但收集成本高昂。现有的主动学习方法多采用经典的实验设计准则（如G-或D-最优性），这些目标并未针对偏好学习的特定结构进行优化，因此需要设计问题特定的算法。

Method: 作者首先识别了偏好学习特有的简单直觉，质疑现有设计目标的适用性。基于这一洞察，提出了两种主动学习算法：第一种提供了该场景下首个实例相关的标签复杂度保证；第二种是简单实用的贪心方法。

Result: 在真实世界偏好数据集上的评估表明，相比现有方法，新算法具有更好的样本效率。

Conclusion: 针对偏好学习结构设计专门的主动学习算法能够显著提高样本效率，降低对齐大型语言模型的数据收集成本。

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [346] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

TL;DR: SpecTF：一种在频域中融合文本信息的多模态时间序列预测框架，通过谱分解和轻量级交叉注意力机制，自适应地重新加权频率分量，显著优于现有方法且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态时间序列预测需要结合数值数据和上下文信号（如文本）。现有方法通常逐点对齐文本特征与时间序列模式，但忽略了上下文信息的多尺度时间影响（如时间序列周期和动态变化），导致局部对齐与全局文本上下文不匹配。

Method: 提出SpecTF框架：1）提取文本嵌入；2）将文本嵌入投影到频域；3）使用轻量级交叉注意力机制将文本特征与时间序列的谱分量融合，基于文本相关性自适应地重新加权频率带；4）将结果映射回时域进行预测。

Result: 实验结果表明，SpecTF在多个多模态时间序列数据集上显著优于最先进的模型，同时使用的参数数量明显更少。

Conclusion: SpecTF通过在频域中整合文本数据对时间序列的影响，有效解决了局部对齐与全局文本上下文不匹配的问题，为多模态时间序列预测提供了一种简单而有效的解决方案。

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [347] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

TL;DR: 提出KMB-DF方法，通过核化矩平衡实现深度时间序列预测中的完全分布平衡，相比现有方法仅匹配一两个预定义平衡函数，KMB-DF从RKHS中自适应选择信息最丰富的平衡函数，显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列预测方法将问题表述为分布平衡问题，但根据Imbens准则，真正的分布平衡需要相对于任何平衡函数匹配一阶矩。作者发现现有目标函数仅针对一两个预定义的平衡函数进行矩匹配，无法实现完全分布平衡，这限制了预测性能。

Method: 提出核化矩平衡直接预测(KMB-DF)：1) 从再生核希尔伯特空间(RKHS)中自适应选择信息最丰富的平衡函数；2) 推导出可处理且可微的目标函数，支持从经验样本进行高效估计；3) 无缝集成到基于梯度的训练流程中。

Result: 在多个模型和数据集上的广泛实验表明，KMB-DF能够一致地提高预测准确性，并实现了最先进的性能。该方法在分布平衡方面优于现有方法，从而获得更好的预测结果。

Conclusion: KMB-DF通过核化矩平衡解决了深度时间序列预测中的分布平衡不足问题，自适应选择平衡函数实现了更充分的分布对齐，为时间序列预测提供了一种有效的新方法。

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [348] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: VIP是一种基于方差信息的预测性分配策略，通过高斯过程模型预测每个提示的成功概率，优化计算预算分配以最小化策略更新的梯度方差，从而提高强化学习的采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO）对所有训练提示分配固定数量的rollout，这种均匀分配隐含地假设所有提示具有同等信息价值，可能导致计算预算使用效率低下并阻碍训练进展。

Method: 提出VIP（Variance-Informed Predictive allocation）策略：1）使用轻量级高斯过程模型基于最近的rollout预测每个提示的成功概率；2）将这些概率预测转化为方差估计；3）通过凸优化问题在硬计算预算约束下确定最优的rollout分配方案。

Result: 实验结果表明，VIP在多个基准测试中持续提高采样效率，并比均匀分配或启发式分配策略获得更高的性能表现。

Conclusion: 通过方差信息的预测性分配策略可以显著提高强化学习中的采样效率，优化计算预算的使用，从而加速训练过程并提升最终性能。

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [349] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: 该论文提出了"提示多样性"框架来量化大语言模型评估中的一致性，发现现有幻觉评估过于关注正确性而忽视一致性，导致对幻觉相关危害的严重误解。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型幻觉评估主要关注正确性，而忽视了评估的一致性。这种局限性导致无法准确区分和解决幻觉带来的各种危害，包括信任侵蚀和错误信息传播等问题。

Method: 作者引入了"提示多样性"框架来量化LLM评估中的一致性。该方法通过分析多个提示下的模型响应来评估一致性，并在Med-HALT等基准测试中进行了验证。

Result: 研究发现超过50%的不一致性（在Med-HALT等基准中），表明幻觉相关危害被严重误解。同时发现现有检测技术主要检测一致性而非正确性，而RAG等缓解技术虽然有益但可能引入额外的不一致性。

Conclusion: 通过将提示多样性框架整合到幻觉评估中，作者提供了一个改进的危害评估框架，并揭示了当前检测和缓解策略的关键局限性，强调了在LLM评估中考虑一致性的重要性。

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [350] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

TL;DR: 研究发现时间序列基础模型（TSFMs）的中间层存在冗余组件，通过机制解释性工具揭示了这些模型对整层剪枝具有鲁棒性，并开发了基于投影矩阵稳定秩的注意力头剪枝理论框架。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型通过大规模预训练在推理时能准确预测未见时间序列，但研究发现领先的transformer-based TSFMs在中间层存在冗余组件，需要理解其工作机制和内部结构。

Method: 开发了一套TSFMs机制解释性工具，包括特定组件剪枝和残差流上的直接logit归因；建立了将transformer框架化为核回归器的理论框架，提出基于每个注意力头投影矩阵稳定秩的纯内在剪枝策略。

Result: 研究发现所有模型对整层剪枝都具有鲁棒性；通过稳定秩方法识别出导致TSFMs中普遍存在的退化现象（如上下文模式重复和季节性偏差）的具体注意力头；结果在多种架构的TSFMs和多样化数据集上保持一致。

Conclusion: 该研究揭示了时间序列基础模型这一新兴架构的普遍特性，为理解其工作机制提供了机制解释性工具和理论框架，有助于识别和解决模型中的退化现象。

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [351] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

TL;DR: FLAME提出了一种基于流匹配的最大熵强化学习框架，通过重要性重加权绕过配分函数估计，设计解耦熵估计器纠正偏差，实现高效单步控制，在保持性能的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散策略虽然表达能力强但推理延迟高，流匹配可实现单步生成但难以集成到最大熵强化学习中，因为最优策略是难以处理的基于能量的分布，且高效对数似然估计存在严重离散化偏差。

Method: 1) 推导Q重加权流匹配目标，通过重要性重加权绕过配分函数估计；2) 设计解耦熵估计器，严格纠正偏差以支持高效探索；3) 集成MeanFlow公式实现表达性强且高效的单步控制。

Result: 在MuJoCo实验中，FLAME优于高斯基线，与多步扩散策略性能相当，同时显著降低推理成本。

Conclusion: FLAME提供了一个原则性框架，解决了流匹配集成到最大熵强化学习中的挑战，实现了表达性强、推理高效的单步控制策略。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [352] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

TL;DR: GNNs使用参数化分类器限制了可解释性和泛化能力，基于插值的方法（特别是NNK）通过将预测表示为训练样本的凸组合来提供理论保证和可解释性。


<details>
  <summary>Details</summary>
Motivation: GNNs已成为处理图结构数据的标准方法，但其依赖参数化分类器（通常是线性softmax层）限制了模型的可解释性，有时还会阻碍泛化能力。需要更可解释且理论上有保证的方法。

Method: 采用基于插值的方法，特别是非负核回归（NNK），将预测表示为嵌入空间中相似训练样本的凸组合，从而提供可解释的预测。

Result: NNK方法不仅提供了理论保证，还能生成可解释的预测结果，将预测表示为训练样本的凸组合，增强了模型的可解释性。

Conclusion: 基于插值的方法（如NNK）为GNNs提供了比传统参数化分类器更可解释且理论上有保证的替代方案，能够将预测表示为训练样本的凸组合，提高模型的可解释性和泛化能力。

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [353] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

TL;DR: PIPE评估协议揭示：在标准智能体基准测试中，轨迹监督微调（SFT）会强化智能体对特定界面模式的依赖，而非真正的语义工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型作为交互智能体的评估存在缺陷，标准基准测试无法区分两种不同的成功机制：真正的语义工具使用能力 vs 特定界面交互模式的记忆。这两种机制在原始界面上都能产生相同的任务成功，但后者缺乏环境不变性能力。

Method: 提出PIPE协议级评估增强方法，通过最小化重写环境界面（保持任务语义和执行行为不变）来诊断智能体对界面的依赖程度。引入界面依赖度（IR）指标，量化智能体对训练时界面的偏好。

Result: 在AgentBench和AgentGym的16个环境中测试发现：轨迹SFT显著放大了界面捷径依赖——经过训练的智能体在最小界面重写下性能急剧下降，而非轨迹训练的模型保持相对稳定。界面捷径依赖表现出环境依赖、非单调的训练动态。

Conclusion: 标准评估方法无法检测智能体是否真正掌握了语义工具使用能力，PIPE协议揭示了轨迹SFT可能导致智能体过度依赖特定界面模式而非学习通用能力，这对智能体评估和训练具有重要意义。

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [354] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: 论文提出一种机制性方法防止微调语言模型时出现的"涌现性错位"问题，通过识别控制错位行为的小型内部特征集，并在微调过程中阻止模型强化这些特征，从而在不影响模型质量和目标任务性能的情况下显著减少错位行为。


<details>
  <summary>Details</summary>
Motivation: 当语言模型在狭窄范围的监督目标上进行微调时，会出现"涌现性错位"现象：模型学会了目标行为，但也发展出不良的域外行为。需要一种机制性方法来防止这种错位，同时保持模型性能。

Method: 识别可靠控制错位行为的小型内部特征集，然后在微调过程中通过"阻断"（约束）这些特征来防止模型强化它们。在六个微调领域进行实验，使用不相交的选择/评估分割、多个独立评估者、多个随机种子等方法验证有效性。

Result: 阻断固定特征集可实现高达95%的相对错位减少，且不降低模型质量或目标任务性能。研究还发现长时间微调下错位会重新出现，可能通过替代特征或层进行重路由，并评估了部分恢复阻断效果的方法。

Conclusion: 针对内部机制的针对性训练时约束可以有效减轻涌现性错位，同时不损害目标任务性能。这为控制语言模型微调过程中的意外行为提供了一种机制性方法。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [355] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该研究创建了首个时空农业生态系统温室气体基准数据集，结合物理模型模拟和真实观测数据，评估了多种深度学习模型在碳氮通量预测中的表现，并探索了迁移学习方法来提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 农业生态系统占全球温室气体排放的四分之一，但现有方法（如土壤采样、过程模型和黑盒机器学习模型）面临数据稀疏、时空异质性高、地下生物地球化学过程复杂等挑战。缺乏AI就绪的基准数据集和协议限制了可信赖AI模型的发展。

Method: 1. 创建首个时空农业生态系统温室气体基准数据集，整合Ecosys和DayCent物理模型模拟数据、涡度协方差通量塔观测数据和受控环境设施数据；2. 评估多种序列深度学习模型（LSTM、时序CNN、Transformer）在碳氮通量预测中的性能；3. 探索迁移学习方法，利用模拟数据提高深度学习模型在真实观测数据上的泛化能力。

Result: 建立了首个综合性的农业生态系统温室气体基准数据集，为AI模型开发提供了标准化资源。评估了不同深度学习架构在碳氮通量预测中的表现，并展示了迁移学习如何利用模拟数据提升模型在真实观测数据上的泛化性能。

Conclusion: 该基准数据集和评估框架有助于开发更准确、可扩展的AI驱动农业生态系统模型，增进对生态系统-气候相互作用的理解，为温室气体减排策略提供支持。

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [356] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: SUSD提出了一种新的无监督技能发现框架，通过将状态空间分解为独立组件（如物体或可控实体），为不同因素分配不同的技能变量，从而发现更丰富多样的技能。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督技能发现方法存在局限性：基于互信息的方法倾向于发现简单静态技能，而距离最大化方法虽然能促进动态技能，但仍无法鼓励全面的技能集来利用环境中所有可控因素或实体。

Method: SUSD将状态空间分解为独立组件（因素），为不同因素分配不同的技能变量，实现对技能发现过程的细粒度控制。采用动态模型跟踪各因素的学习进度，自适应地将智能体注意力转向未充分探索的因素。

Result: 在1到10个因素不等的三个环境中，SUSD能够发现多样且复杂的技能，在分解和复杂环境中显著优于现有的无监督技能发现方法。该方法还能产生分解的技能表示，便于通过分层强化学习高效训练组合下游任务。

Conclusion: SUSD通过利用环境的组合结构，实现了更丰富多样的无监督技能发现，同时提供了分解的技能表示，便于对单个实体进行细粒度解耦控制，为组合任务的高效训练奠定了基础。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [357] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

TL;DR: 提出显式掩码扩散对偶理论，证明掩码过程是连续高斯过程的投影，并基于此开发掩码一致性蒸馏方法，实现16倍推理加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码离散扩散是高质量语言建模的主流方法，但其推理效率受限于缺乏确定性采样工具。现有方法要么性能不如掩码模型，要么依赖复杂积分算子，要么只能使用随机蒸馏。

Method: 建立显式掩码扩散对偶理论，证明掩码过程通过新颖的最大值索引保持机制从连续高斯过程投影而来。基于此提出掩码一致性蒸馏框架，解析构建确定性耦合轨迹，绕过数值ODE求解器。

Result: 该方法严格优于先前的随机蒸馏方法，实现16倍推理加速且不损害生成质量。为掩码和连续扩散提供了坚实的理论基础，释放了一致性蒸馏在离散生成中的全部潜力。

Conclusion: 通过建立掩码扩散对偶理论并开发掩码一致性蒸馏，解决了掩码离散扩散缺乏确定性采样工具的问题，显著提升了推理效率，为高质量离散生成提供了新的理论框架和实践方法。

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [358] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

TL;DR: FedMuscle：一种新颖的联邦多任务学习框架，通过Muscle损失函数学习跨任务的共享表示空间，有效处理模型和任务异质性


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法通常假设模型同质性（完全或部分相同），这限制了在现实场景中的应用。需要一种能够处理模型和任务异质性的方法，同时保持数据隐私。

Method: 提出Muscle损失函数，一种新颖的对比学习目标，能够同时对齐所有参与模型的表示。该损失最小化等价于最大化所有模型表示之间的互信息。基于此开发了FedMuscle算法，这是一个实用且通信高效的联邦多任务学习算法。

Result: 在多样化的图像和语言任务上的实验表明，FedMuscle始终优于最先进的基线方法，在异构设置下实现了显著改进和稳健性能。

Conclusion: FedMuscle通过共享表示空间而非模型参数，有效解决了联邦多任务学习中的模型和任务异质性问题，为现实场景中的隐私保护协作学习提供了实用解决方案。

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [359] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

TL;DR: AdaptNC框架通过联合在线调整非共形分数参数和共形阈值，在保持目标覆盖率的同时显著减少预测区域体积，解决了传统方法在环境结构变化时过于保守的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人应用中存在分布偏移，违反了传统共形预测的交换性假设。现有在线CP方法仅调整阈值，使用静态的非共形分数函数，导致在环境结构变化时产生过于保守、体积效率低下的预测区域。

Method: 提出AdaptNC框架，联合在线调整非共形分数参数和共形阈值。采用自适应重加权方案优化分数函数，并引入回放缓冲区机制来缓解分数转换期间的覆盖率不稳定问题。

Result: 在多智能体策略变化、环境变化和传感器退化等多样化机器人基准测试中，AdaptNC在保持目标覆盖率水平的同时，相比仅调整阈值的最先进基线方法显著减少了预测区域体积。

Conclusion: AdaptNC通过联合优化非共形分数和阈值，为存在分布偏移的机器人应用提供了更高效的不确定性量化框架，在保持统计保证的同时提高了预测效率。

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [360] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 基于分数的方法在密度比估计中存在理论路径独立但实际性能依赖路径选择的矛盾，作者通过揭示训练目标与理想目标的差异在于时间分数的路径方差，提出最小路径方差原则来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 基于分数的方法在密度比估计中面临一个重要矛盾：理论上路径独立，但实际性能严重依赖于选择的路径调度。现有方法缺乏对这一现象的理论解释和系统解决方案。

Method: 提出最小路径方差原则，推导出路径方差的闭式表达式，将不可解问题转化为可优化问题。使用灵活的Kumaraswamy混合模型参数化路径，学习数据自适应、低方差的路径，避免启发式选择。

Result: 通过优化完整目标得到更准确和稳定的估计器，在具有挑战性的基准测试中建立了新的最先进结果。

Conclusion: 揭示了基于分数的密度比估计方法中路径方差的关键作用，提出了最小路径方差原则，通过闭式优化路径方差显著提升了估计性能，解决了理论路径独立与实际路径依赖之间的矛盾。

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [361] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

TL;DR: COMET提出了一种基于码本的在线自适应多尺度时间序列异常检测方法，通过多尺度补丁编码、向量量化核心集和在线码本适应三个组件，在多个基准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法存在三个主要问题：1）在补丁级表示学习中未能充分捕捉时间依赖性和多变量相关性；2）依赖单尺度模式限制了不同时间范围异常的检测；3）专注于正常数据表示使模型在推理时容易受到分布偏移的影响。

Method: COMET包含三个关键组件：1）多尺度补丁编码：在不同补丁尺度上捕捉时间依赖性和变量间相关性；2）向量量化核心集：通过码本学习代表性正常模式，结合量化误差和内存距离的双重评分检测异常；3）在线码本适应：基于码本条目生成伪标签，通过对比学习在推理时动态适应模型。

Result: 在五个基准数据集上的实验表明，COMET在45个评估指标中的36个上取得了最佳性能，验证了其在多样化环境中的有效性。

Conclusion: COMET通过多尺度表示学习、码本驱动的异常检测和在线适应机制，有效解决了时间序列异常检测中的关键挑战，在多个基准数据集上表现出优越性能。

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [362] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

TL;DR: 提出机会约束推理框架，通过有限样本自适应验证，在重复使用中直接控制幻觉概率，而非仅降低平均错误率


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缓解策略只能降低平均错误率，但无法在重复使用时明确控制幻觉发生的频率，需要一种能提供概率风险保证的推理方法

Method: 将推理建模为部署时风险控制问题，提出机会约束推理框架，将幻觉视为随机约束违反，设计顺序、任意时间有效的推理过程，自适应验证可行性

Result: 在NaturalQuestions启发的问答任务和受控多跳问答上，实现了可靠的风险控制、对内在不可行输入的早期检测，以及重复使用下的安全组合

Conclusion: 基于置信度的选择性预测无法提供一致的风险保证，而机会约束推理框架能有效控制幻觉概率，为语言模型部署提供可验证的安全保障

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [363] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来分析Adam优化器中批量大小如何通过影响小批量噪声来调节动量超参数(β₁, β₂)的隐式偏差，从而影响损失景观的锐度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着高质量数据有限而计算资源增长，多轮训练在深度学习各子领域的重要性日益凸显。Adam(W)作为许多任务（如下一个词预测）的首选优化器，其动量超参数(β₁, β₂)控制记忆，批量大小控制小批量噪声。需要理解小批量噪声如何影响Adam中记忆的隐式偏差，这种偏差通常与多轮训练中的泛化差距相关。

Method: 引入理论框架分析小批量噪声如何影响Adam优化器中记忆的隐式偏差。通过理论推导连接批量大小变化时(β₁, β₂)对正则化/反正则化效果的影响，并建立批量大小变化临界点与临界批量大小尺度之间的关系。

Result: 研究发现：在大批量情况下，较高的β₂会增加记忆的反正则化幅度（损害泛化）；但随着批量变小，(β₁, β₂)对正则化的依赖关系会发生反转。常用的默认参数对(0.9, 0.999)在小批量时表现良好；对于大批量，在许多设置中将β₁移近β₂能显著提高多轮训练的验证准确率。

Conclusion: 批量大小通过调节小批量噪声显著影响Adam优化器中动量超参数的隐式偏差方向。理论框架揭示了批量大小变化临界点与临界批量大小尺度之间的关系，为多轮训练中Adam超参数调优提供了理论指导，特别是在即将过拟合的小规模数据场景中。

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [364] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

TL;DR: MBGen：基于多体增强扩散框架的质谱分子结构生成新方法，通过多体注意力机制和高阶边建模，显著提升分子结构生成和异构体区分能力


<details>
  <summary>Details</summary>
Motivation: 现有质谱分子结构生成方法主要采用原子中心和成对相互作用建模，忽略了高阶边相互作用，无法系统捕捉多体特征，限制了复杂异构体和非局域碎裂机制的解析能力

Method: 提出MBGen多体增强扩散框架，集成多体注意力机制和高阶边建模，充分利用MS/MS谱图中编码的丰富结构信息，实现准确的从头分子结构生成和异构体区分

Result: 在NPLIB1和MassSpecGym基准测试中，MBGen性能显著优于现有方法，提升幅度高达230%，有效捕捉高阶相互作用，对复杂异构体和非局域碎裂信息表现出增强的敏感性

Conclusion: 多体建模在质谱分子结构生成中具有重要科学价值和实际应用意义，MBGen框架为基于质谱的分子生成提供了有效解决方案

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [365] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

TL;DR: 本文提出一个统一的三轴分类法，将智能体能力与空间任务连接起来，强调空间智能对具身智能体的重要性，并识别了六个重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么关注智能体架构，要么关注空间领域，缺乏将这两种互补能力统一起来的框架。大型语言模型在符号领域的成功无法直接迁移到物理世界，空间智能（感知3D结构、推理物体关系、在物理约束下行动）对具身智能体至关重要。

Method: 通过对2000多篇论文的系统性综述（引用742篇顶级会议论文），提出了一个统一的三轴分类法：能力轴、任务轴和尺度轴。区分了空间基础（对几何和物理的度量理解）与符号基础（图像与文本关联），并强调感知本身不能赋予智能体能力。

Result: 分析揭示了三个关键发现：1）分层记忆系统对长时程空间任务很重要；2）GNN-LLM整合是结构化空间推理的有前景方法；3）世界模型对跨微观到宏观空间尺度的安全部署至关重要。

Conclusion: 提出了六个重大挑战和未来研究方向，包括需要统一评估框架来标准化跨领域评估。该分类法为统一碎片化研究提供了基础，有助于推动机器人、自动驾驶和地理空间智能等领域下一代空间感知自主系统的发展。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [366] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

TL;DR: 该论文将LLM对"随机事实"的记忆形式化为成员测试问题，建立了速率-失真理论，证明即使在最优训练和完美数据下，有限容量下的信息论最优策略会导致幻觉，这是有损压缩的自然结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常对缺乏可推断模式的"随机事实"产生高置信度的幻觉。作者旨在形式化这种记忆问题，从信息论角度解释幻觉的根本原因。

Method: 将随机事实的记忆形式化为成员测试问题，统一了Bloom过滤器的离散误差度量与LLM的连续对数损失。在事实在可能主张的宇宙中稀疏的假设下，建立速率-失真定理，通过事实与非事实得分分布之间的最小KL散度来表征最优记忆效率。

Result: 理论分析表明，即使在最优训练、完美数据和简化"封闭世界"设置下，有限容量下的信息论最优策略不是放弃或遗忘，而是对某些非事实赋予高置信度，从而导致幻觉。在合成数据上的实证验证表明幻觉作为有损压缩的自然结果持续存在。

Conclusion: 幻觉是大型语言模型在有限容量下信息论最优策略的必然结果，即使具备最优训练和完美数据。这为理解LLM幻觉提供了新的理论框架，表明幻觉是有损压缩的自然产物而非可完全消除的缺陷。

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [367] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

TL;DR: CU-DPO框架用连续评分替代二元偏好标签，通过策略选择和执行优化两阶段训练，显著提升大语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型推理评估使用二元偏好监督，无法捕捉推理过程中的部分进展和细粒度质量。需要一种能够量化推理质量连续变化的框架来更好地对齐模型与认知策略。

Method: 提出连续效用直接偏好优化(CU-DPO)：1) 用连续评分替代二元标签捕捉细粒度推理质量；2) 两阶段训练：策略选择阶段通过最佳vs所有比较优化模型选择最佳策略，执行优化阶段使用边缘分层对训练模型正确执行选定策略。

Result: 在数学推理基准测试中，CU-DPO将策略选择准确率从35-46%提升到68-78%，在分布内数据集上获得高达6.6个百分点的下游推理增益，并能有效迁移到分布外任务。

Conclusion: CU-DPO通过连续评分和两阶段训练有效提升大语言模型的推理能力，理论证明其样本复杂度优于传统二元偏好方法，并能收敛到熵正则化的效用最大化策略。

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [368] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

TL;DR: 该研究在共形预测框架下量化认知预测不确定性，通过最大平均不精确度测量诱导信度集中的冲突信息程度，提供比传统置信区间大小更精细的不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于量化预测时的认知不确定性（即由于存在多个合理预测模型而产生的不确定性）。传统共形预测主要关注预测区域大小，但这种方法无法充分捕捉模型多重性带来的认知不确定性，需要更精细的量化方法。

Method: 方法基于共形预测与信度集之间的理论联系：证明任何完整的共形预测过程都会诱导出一个封闭凸的预测分布集合（信度集）。在此基础上，提出基于最大平均不精确度的计算高效且解析可处理的度量方法，用于量化信度集中冲突信息的程度。

Result: 在主动学习和选择性分类实验中的结果表明，所提出的认知预测不确定性量化方法比单纯依赖共形预测区域大小提供了更丰富、更精细的不确定性评估，能够更有效地支持决策制定。

Conclusion: 该研究建立了共形预测与认知不确定性量化之间的理论联系，提出的最大平均不精确度度量方法能够有效量化预测时的认知不确定性，展示了共形预测作为认知不确定性下决策制定的理论基础潜力。

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [369] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

TL;DR: LASS-ODE是一个基于局部线性ODE表示和跨系统注意力机制的大规模ODE预测基础模型，解决了物理系统动态预测中的计算可扩展性和知识共享效率问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言、视觉和时间序列分析方面取得了成功，但在物理系统动态预测方面进展有限。主要面临两个挑战：1）物理计算可扩展性问题 - 物理信息学习可以强制执行物理正则化，但其计算（如ODE积分）无法扩展到大规模系统；2）知识共享效率问题 - 注意力机制主要在单个系统内计算，限制了跨系统共享ODE结构的提取。

Method: 提出LASS-ODE模型，采用两种关键技术：1）基于局部线性ODE表示的token表示方法，通过保持物理保真度同时扩展到基础模型规模；2）引入跨系统注意力机制，通过公共结构中心（CSH）存储共享token并聚合跨系统知识，实现高效知识共享。

Result: 模型在40GB ODE轨迹数据集上进行预训练，表现出强大的领域内性能、跨不同ODE系统的零样本泛化能力，并通过微调获得额外改进。

Conclusion: 通过局部线性ODE表示和跨系统注意力机制，LASS-ODE成功解决了物理系统动态预测中的计算可扩展性和知识共享效率问题，为大规模ODE系统预测提供了有效的基础模型框架。

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [370] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

TL;DR: 本文提出AGT^AO框架，通过自适应正交性和对抗门控训练来解决大语言模型遗忘敏感数据时的权衡问题，在保持模型效用的同时实现稳健擦除。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练过程中会无意记忆敏感数据，带来隐私和安全风险。现有的机器遗忘方法面临根本性困境：激进的遗忘会导致灾难性遗忘而降低模型效用，保守的策略则可能导致表面遗忘，使模型容易受到对抗性恢复攻击。

Method: 提出AGT^AO框架，包含两个核心组件：1) 自适应正交性(AO)：动态缓解遗忘和保留目标之间的几何梯度冲突，减少非预期的知识退化；2) 对抗门控训练(AGT)：将遗忘建模为潜在空间的最小-最大博弈，采用基于课程的门控机制来模拟和对抗内部恢复尝试。

Result: 实验表明AGT^AO在遗忘效果(KUR ≈ 0.01)和模型效用(MMLU 58.30)之间实现了优越的权衡平衡。

Conclusion: AGT^AO框架成功解决了大语言模型遗忘敏感数据时的权衡问题，通过自适应正交性和对抗门控训练实现了稳健擦除与效用保持的统一，为机器遗忘提供了有效的解决方案。

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [371] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

TL;DR: 该论文提出了阶段感知的类增量学习（Stage-CIL）范式，针对类内形态演化问题，开发了Stage-Bench数据集和STAGE方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统类增量学习假设类形态是静态的，忽略了类内演化现象（如幼虫变蝴蝶），导致模型无法适应同一语义类内的形态变化。

Method: 提出STAGE方法，在固定大小记忆池中学习抽象可迁移的演化模式，将语义身份与转换动态解耦，基于早期表示预测未来形态。

Result: STAGE方法在10个领域、2阶段的Stage-Bench数据集上持续显著优于现有最先进方法，有效解决了类间区分和类内形态适应问题。

Conclusion: 阶段感知CIL范式能更好地模拟现实世界类演化，STAGE方法通过解耦语义和演化动态，在类增量学习中实现了更好的性能。

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [372] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 本文通过受控实验研究思维链推理的忠实性问题，发现模型在训练噪声低于临界阈值时能学习忠实推理，高于阈值时则转向不忠实的跳跃推理，并观察到隐含的自验证机制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的思维链推理存在不忠实问题（中间步骤逻辑不一致或未能反映最终答案的因果关系），但缺乏对忠实思维链构成以及不忠实性如何从自回归训练中产生的根本理解。

Method: 使用受控合成实验，在噪声数据上训练小型Transformer模型，让它们逐步解决模块化算术表达式（称为算术表达式推理任务），分析训练动态和机制。

Result: 模型在训练噪声低于临界阈值时能学习忠实推理（因果遵循底层算术规则），高于阈值时转向不忠实的跳跃推理，中间存在混合模式伴随预测熵的瞬时增加；机制分析显示模型通过解决不一致推理步骤来编码内部不确定性，表明自回归训练中出现了隐含的自验证机制。

Conclusion: 思维链推理的忠实性受训练噪声水平影响，存在临界阈值；模型能从自回归训练中发展出隐含的自验证能力，这为理解思维链推理的忠实性提供了机制性见解。

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [373] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDi-RL：一种在连续潜在空间中进行探索的强化学习框架，通过引导扩散建模探索，解决离散RL中CoT生成的多样性崩溃问题，在代码生成和数学推理任务上显著优于离散RL基线。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通过优化离散的思维链生成来改进LLM推理，但在令牌空间的探索常因策略熵降低和模式激发行为导致多样性崩溃。需要一种能保持多种解决方案模式共存而不相互抑制的方法。

Method: 提出LaDi-RL框架，在连续潜在空间中进行探索，潜在变量编码语义级推理轨迹。通过引导扩散建模探索，多步去噪分布随机性并保持多个共存解决方案模式。将潜在空间探索与文本空间生成解耦，结合文本策略获得额外收益。

Result: 在代码生成和数学推理基准测试中，相比离散RL基线，在pass@1和pass@k指标上均取得一致改进。代码生成pass@1绝对提升+9.4%，数学推理pass@1绝对提升+5.7%。

Conclusion: 基于扩散的潜在RL为离散令牌级RL提供了原则性替代方案，通过连续潜在空间探索有效解决了多样性崩溃问题，显著提升了推理任务的性能。

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [374] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为MiTA的高效注意力机制，将注意力视为两层快速权重MLP，通过压缩和路由策略来扩展长序列处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer注意力机制在处理极长序列时面临计算成本过高的问题，因为其快速权重的宽度等于序列长度N，随着上下文扩展，计算复杂度急剧增加。

Method: 提出压缩-路由策略：首先使用少量地标查询将N宽度MLP压缩为更窄的MLP，然后为每个地标查询收集top-k激活的键值对构建可变形专家，形成Mixture of Top-k Activations (MiTA)注意力机制。

Result: 在视觉任务上的初步实验表明MiTA注意力机制具有潜力，为在更具挑战性场景中的进一步优化和应用提供了基础。

Conclusion: MiTA注意力机制通过压缩和路由策略有效解决了长序列处理中的计算瓶颈问题，为高效注意力方法提供了一个统一的框架视角。

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [375] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

TL;DR: FLood：基于OOD检测的新型联邦学习框架，通过双重加权机制处理非IID数据异质性，提升全局模型的收敛稳定性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现实世界中联邦学习面临严重的非IID数据异质性挑战，这破坏了全局模型的收敛稳定性、泛化能力和服务质量。现有方法难以有效处理这种分布不一致的问题。

Method: 提出FLood框架，采用双重加权机制：1）客户端层面，通过上加权伪OOD样本自适应重加权监督损失；2）服务器层面，根据客户端的OOD置信度分数加权聚合模型更新。该框架可作为正交插件模块与现有FL算法集成。

Result: 在多种非IID设置下的多个基准测试中，FLood在准确率和泛化能力上均优于现有最先进的联邦学习方法，并能作为插件提升现有算法的性能。

Conclusion: FLood是一个实用且可扩展的解决方案，能够增强联邦学习在现实世界异构环境中的可靠性和服务质量，为智能服务系统的部署提供了有效工具。

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [376] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

TL;DR: SLA（Softmax Linear Attention）通过将softmax操作从token级别提升到head级别，在线性注意力中恢复了全局竞争机制，在保持线性复杂度的同时提升了表达能力和长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然将标准Transformer的二次复杂度降低到线性时间，但由于移除了softmax归一化，导致表达能力下降，特别是失去了全局竞争机制，这使得模型难以在长上下文噪声中聚焦相关信息。

Method: 提出Softmax Linear Attention（SLA）框架，将softmax操作从token级别提升到head级别，利用注意力头作为粗粒度语义槽，通过竞争门控机制动态选择最相关的子空间，恢复"赢者通吃"的动态特性。

Result: 在语言建模和长上下文基准测试中，SLA持续提升了最先进的线性基线模型（RetNet、GLA、GDN）的性能，特别是在具有挑战性的检索场景中，显著增强了模型对噪声的鲁棒性。

Conclusion: SLA通过利用更高层次的多头聚合结构，在线性注意力中恢复了精确的竞争选择机制，在保持线性复杂度的同时显著提升了表达能力和长上下文理解能力，特别是在噪声环境下的检索任务中表现出色。

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [377] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散模型的新型后训练量化方法，通过学习为不同时间步的校准样本分配最优权重，解决了现有方法中均匀权重分配和梯度冲突的问题，显著提升了量化性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实际部署中存在推理速度慢、内存占用高和计算需求大的问题。现有的后训练量化方法通常对所有时间步的校准样本使用均匀权重，这是次优的，因为不同时间步的数据对扩散过程的贡献不同，且激活分布和梯度变化导致均匀量化方法效果不佳。

Method: 提出了一种新颖的后训练量化方法，通过学习为校准样本分配最优权重，使量化模型在不同时间步的梯度对齐，从而优化量化过程。该方法考虑了不同时间步对扩散过程的不同贡献，解决了梯度冲突问题。

Result: 在CIFAR-10、LSUN-Bedrooms和ImageNet数据集上的大量实验表明，该方法相比其他扩散模型后训练量化方法具有优越性。

Conclusion: 通过学习为不同时间步的校准样本分配最优权重来对齐梯度的方法，有效解决了扩散模型后训练量化中的挑战，显著提升了量化性能，为扩散模型的实用部署提供了有效解决方案。

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [378] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

TL;DR: RankTuner提出了一种基于概率-熵校准的token级重加权方法，通过相对秩指标识别真正需要学习的token，在数学推理和代码生成任务上优于仅基于概率或熵的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的token级重加权方法主要依赖一维指标：真实概率反映下游对齐，token熵反映预训练先验的内在不确定性。忽略熵会误将噪声或易替换token识别为学习关键，而忽略概率则无法反映目标特定对齐。

Method: RankTuner引入了概率-熵校准信号——相对秩指标，比较真实token的秩与其在预测分布下的期望秩。其倒数作为token级相对尺度来重加权微调目标，专注于真正未充分学习的token，不过度惩罚内在不确定的位置。

Result: 在多个骨干模型上的实验表明，该方法在数学推理基准上取得一致改进，在分布外推理上获得迁移增益，在代码生成性能上优于仅基于概率或熵的重加权基线。

Conclusion: 通过概率-熵校准的相对秩指标能更准确地识别需要学习的token，有效提升监督微调的效果，在多种推理任务上表现优于传统的一维重加权方法。

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [379] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

TL;DR: 本文提出了一种新的正则化方法RDMReg，用于联合嵌入预测架构(JEPA)，通过匹配修正广义高斯分布来学习稀疏、非负表示，解决了现有方法偏向密集表示的问题。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法通过各向同性高斯分布正则化表示，但这种方法偏向密集表示，无法捕捉高效表示中观察到的稀疏性这一关键特性。

Method: 提出修正分布匹配正则化(RDMReg)，这是一种切片双样本分布匹配损失，将表示对齐到修正广义高斯(RGG)分布。RGG通过修正实现对期望ℓ0范数的显式控制，同时在期望ℓp范数约束下保持最大熵。

Result: 配备RDMReg的JEPA（Rectified LpJEPA）学习了稀疏、非负表示，在稀疏性与性能之间取得了有利的权衡，并在图像分类基准上表现出竞争力的下游性能。

Conclusion: RDMReg能有效强制稀疏性，同时保留任务相关信息，Rectified LpJEPA严格推广了先前基于高斯的JEPA方法。

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [380] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

TL;DR: FedGaLore：针对非IID联邦学习中LoRA性能下降问题，提出结合客户端梯度子空间优化和服务器端漂移鲁棒同步的新方法


<details>
  <summary>Details</summary>
Motivation: LoRA在联邦微调中广泛使用，但在非独立同分布设置下性能显著低于全参数微调。研究发现这源于两个耦合的不匹配：更新空间不匹配和优化器状态不匹配

Method: 提出FedGaLore方法，结合客户端GaLore风格的梯度子空间优化和服务器端通过谱共享信号提取的投影二阶矩状态漂移鲁棒同步

Result: 在自然语言理解、视觉和自然语言生成基准测试中，FedGaLore在非IID设置下相比最先进的联邦LoRA基线提高了鲁棒性和准确性

Conclusion: FedGaLore通过解决LoRA在联邦学习中的两个关键不匹配问题，显著提升了非IID设置下的性能表现

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [381] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

TL;DR: PEAR是一种在SFT阶段重新加权损失的方法，通过重要性采样校正离线SFT数据分布与在线RL策略分布之间的不匹配，从而提升后续RL训练的效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练流程中，离线SFT阶段通常孤立优化以最大化SFT性能，但更强的SFT检查点经过相同RL训练后可能表现更差，这是因为离线SFT数据分布与在线RL策略分布存在不匹配。

Method: 提出PEAR方法，在SFT阶段使用重要性采样重新加权损失，包含token级、block级和序列级三种变体。该方法可增强标准SFT目标，一旦收集到离线数据的概率，只需很少的额外训练开销。

Result: 在可验证推理游戏和数学推理任务上的实验表明，PEAR相比标准SFT能持续提升RL后性能，在AIME2025上pass@8增益高达14.6%。

Conclusion: PEAR是通过考虑下游RL来设计和评估SFT的有效步骤，有助于实现更全面的LLM后训练，而不是孤立优化SFT阶段。

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [382] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为A的新型优化器，结合了正交化更新方向的光谱控制和符号更新的坐标控制，在语言和视觉训练任务中表现优于AdamW和Muon。


<details>
  <summary>Details</summary>
Motivation: 许多优化器可以解释为范数诱导几何下的最速下降方法，从而继承相应的隐式偏差。作者希望结合光谱控制和坐标控制，开发一种更高效的优化器。

Method: 提出A优化器，它形成Lion风格的动量方向，通过少量Newton-Schulz迭代近似正交化，然后应用逐元素符号操作，近似实现光谱约束和ℓ∞约束集交集中的最大步长。

Result: 在包括GPT-2和Llama预训练、SiT图像预训练和监督微调在内的大规模语言和视觉训练中，A优化器在可比调参下匹配或优于AdamW和Muon，同时仅使用动量级优化器状态，并能缓解AdamW预训练检查点的优化器不匹配问题。

Conclusion: A优化器通过结合光谱控制和坐标控制，提供了一种高效且性能优越的优化方法，在大规模深度学习任务中展现出显著优势。

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [383] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出MarkovScale框架，将序列缩放建模为两状态马尔可夫过程，提供理论最优性边界，在多个基准测试中优于现有并行和序列缩放方法。


<details>
  <summary>Details</summary>
Motivation: 序列缩放作为重要的推理时缩放范式，其性能改进通常有限且缺乏理论理解，主要原因是现有方法多为启发式、非原则性的，缺乏清晰的最优性边界。

Method: 提出原则性框架，将序列缩放建模为两状态马尔可夫过程，推导出闭式解，包括准确率提升的具体条件以及理论上限、中性和下限性能边界。基于此开发MarkovScale系统，应用最优性准则实现准确率与效率的理论平衡。

Result: 在3个骨干LLM、5个基准测试和超过20种配置的全面实验中，MarkovScale始终优于最先进的并行和序列缩放方法，代表了向LLM最优和资源高效推理的重要进展。

Conclusion: 通过将序列缩放建模为马尔可夫过程，提供了理论最优性边界，开发出在实践中优于现有方法的MarkovScale系统，推动了LLM推理的优化和资源效率提升。

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [384] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

TL;DR: IRIS提出了一种基于隐式奖励的内部筛选方法，通过利用连续隐式奖励在原生对数概率空间捕捉模态竞争，无需外部评估器即可有效减少多模态大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型面临幻觉问题的根本挑战。现有的直接偏好优化方法依赖昂贵的外部评估器进行评分或重写，存在离策略学习性差距和离散化损失。由于无法访问内部状态，这种反馈忽略了不同模态间的细粒度冲突，而这些冲突正是生成过程中产生幻觉的原因。

Method: 提出IRIS（隐式奖励引导的内部筛选）方法，利用原生对数概率空间中的连续隐式奖励来保持完整信息密度并捕捉内部模态竞争。这种在策略范式通过使用自生成的偏好对来消除学习性差距。基于多模态隐式奖励筛选这些偏好对，确保优化由直接解决模态冲突的信号驱动。

Result: 大量实验表明，IRIS在关键幻觉基准测试中取得了极具竞争力的性能，仅使用5.7k样本，且在偏好对齐过程中不需要任何外部反馈。这些结果证实IRIS为缓解MLLM幻觉提供了一个高效且原则性的范式。

Conclusion: IRIS通过利用连续隐式奖励在原生对数概率空间捕捉模态竞争，提供了一种无需外部评估器即可有效减少多模态大语言模型幻觉的高效方法，为MLLM对齐提供了新的范式。

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [385] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

TL;DR: 该研究提出一个多日前电价预测框架，使用先进的深度学习时序模型，在澳大利亚电力市场进行综合评估，发现标准深度学习模型在多数区域表现最佳，而先进时序模型对预测周期扩展更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前电价预测存在三个主要问题：1) 多日预测研究有限；2) 对先进时序深度学习模型探索不足；3) 依赖聚合评估掩盖了日内预测差异。需要解决这些缺口以提升电价预测的准确性和实用性。

Method: 提出新颖的电价预测框架，将预测周期扩展到多日前，系统构建基于先进时序深度学习模型的预测模型。在澳大利亚国家电力市场五个区域进行综合评估，分析日内时段级别的预测性能。

Result: 结果显示：1) 没有单一模型在所有区域、指标和预测周期上持续占优；2) 标准深度学习模型在多数区域表现最佳；3) 先进时序深度学习模型对预测周期扩展更具鲁棒性；4) 日内评估揭示明显的昼夜误差模式：绝对误差在晚间爬坡期达到峰值，相对误差在午间负电价期间膨胀，方向准确性在趋势频繁变化期间下降。

Conclusion: 未来基于深度学习的电价预测研究可以从丰富的特征表示和建模策略中受益，这些策略应增强长期预测的鲁棒性，同时保持对日内波动性和结构性价格动态的敏感性。

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [386] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

TL;DR: 该论文针对扩散模型在自动生成平面图时存在的设计多样性不足问题，提出了多样性评分指标和边界交叉注意力模块，揭示了真实感与多样性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的自动平面图生成方法虽然能产生高度真实的布局，但过度优化感知度量（如FID）导致设计多样性有限，且缺乏对几何一致性的有效控制。

Method: 提出了多样性评分（DS）来量化固定约束下的布局多样性；引入了边界交叉注意力（BCA）模块，使模型能够基于建筑边界进行条件生成。

Result: BCA显著提高了边界遵循度；延长训练时间会导致多样性崩溃，而FID无法诊断此问题；模型过度依赖数据集先验，揭示了真实感与多样性之间的关键权衡。

Conclusion: 在建筑设计中，生成系统需要明确平衡保真度、多样性和泛化能力，仅优化FID等感知度量不足以实现高质量的设计生成。

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [387] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

TL;DR: FlyPrompt是一个受果蝇大脑启发的通用持续学习框架，通过专家路由和专家能力提升机制，在单次遍历、无明确任务边界的非平稳数据流中实现高效学习。


<details>
  <summary>Details</summary>
Motivation: 现有持续参数高效调优方法通常依赖多轮训练和明确任务提示，无法有效应对通用持续学习场景。需要解决两个核心挑战：如何为演化数据分布分配专家参数，以及如何在有限监督下提升专家表示能力。

Method: 受果蝇分层记忆系统启发，将通用持续学习分解为专家路由和专家能力提升两个子问题。引入随机扩展分析路由器进行实例级专家激活，并使用输出头的时序集成动态调整决策边界。

Result: 在CIFAR-100、ImageNet-R和CUB-200数据集上，FlyPrompt分别比最先进基线方法提升了11.23%、12.43%和7.62%的性能。

Conclusion: FlyPrompt通过脑启发的稀疏扩展和模块化集成设计，有效解决了通用持续学习中的参数分配和表示能力提升问题，为单次遍历、无任务边界的持续学习提供了创新解决方案。

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [388] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

TL;DR: GH-OFL是一系列单轮联邦学习方法，通过假设预训练嵌入的类条件高斯性，客户端仅传输充分统计量，服务器构建三种高斯头组件，在强非IID数据下实现SOTA性能且完全数据无关。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖多轮模型交换和聚合，通信成本高且存在隐私风险。现有单轮联邦学习方法要么不实用，要么受限于需要公共数据集、假设同质客户端模型或要求上传额外数据。需要克服这些限制。

Method: 提出GH-OFL方法族：假设预训练嵌入的类条件高斯性，客户端仅传输充分统计量（每类计数和一/二阶矩）。服务器构建三种高斯头组件：1) 闭式高斯头（NB/LDA/QDA）；2) FisherMix：在估计的Fisher子空间上使用余弦边距训练的线性头；3) Proto-Hyper：通过知识蒸馏在合成样本上精炼高斯对数几率的轻量级低秩残差头。

Result: GH-OFL方法在强非IID数据偏斜下实现了最先进的鲁棒性和准确性，同时保持严格的数据无关性。

Conclusion: GH-OFL方法通过高斯性假设和三种头组件设计，成功克服了现有单轮联邦学习的限制，在保持数据隐私的同时实现了高性能。

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [389] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Stein规则收缩的决策理论框架，通过收缩噪声的小批量梯度向历史动量估计器，在p≥3的高维情况下，该收缩梯度估计器在平方误差损失下一致优于标准随机梯度，并可以无缝集成到Adam优化器中。


<details>
  <summary>Details</summary>
Motivation: 随机梯度方法是大规模学习的核心，但传统分析将小批量梯度视为总体梯度的无偏估计器。在高维设置中，统计决策理论的经典结果表明，在二次损失下无偏估计器通常是不容许的，这表明标准随机梯度从风险角度可能是次优的。

Method: 将随机梯度计算形式化为高维估计问题，引入基于Stein规则收缩的决策理论框架。构建收缩梯度估计器，自适应地将噪声小批量梯度收缩到从历史动量导出的稳定受限估计器。收缩强度使用梯度噪声方差的在线估计以数据驱动方式确定，利用自适应优化方法通常维护的二阶矩统计量。

Result: 在高斯噪声模型下，对于维度p≥3，所提出的估计器在平方误差损失下一致优于标准随机梯度，并且在经典决策理论意义下是最小最大最优的。将估计器集成到Adam优化器中，在CIFAR10和CIFAR100上，在多个标签噪声水平下，在大批量机制中相比Adam表现出持续改进。消融研究表明，增益主要来自选择性地对高维卷积层应用收缩，而跨所有参数的无差别收缩会降低性能。

Conclusion: 经典收缩原理为改进现代深度学习中的随机梯度估计提供了一种原则性且有效的方法，收缩梯度估计器可以无缝集成到现有优化器中，带来性能提升而计算成本可忽略不计。

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [390] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 本文系统研究了世界模型的量化问题，使用DINO-WM作为代表性案例，评估了多种后训练量化方法在不同比特宽度、量化粒度和规划步长下的表现，揭示了世界模型量化特有的失败模式。


<details>
  <summary>Details</summary>
Motivation: 世界模型在紧凑的潜在空间中学习环境动态表示，支持规划、预测和推理等任务，但计算成本和内存占用高，需要量化以实现高效部署。目前后训练量化对世界模型的影响尚未得到充分研究。

Method: 使用DINO-WM作为代表性世界模型，系统评估多种后训练量化方法，包括仅权重量化和权重-激活联合量化。在多种视觉规划任务上进行广泛实验，涵盖不同比特宽度（4-8位）、量化粒度（逐层、逐组）和规划步长（最多50步）。

Result: 研究发现：1）分组权重量化能稳定低比特rollout；2）激活量化粒度带来的收益不一致；3）编码器和预测器模块的量化敏感性高度不对称；4）激进的低比特量化显著降低规划目标与任务成功之间的对齐性，导致无法通过额外优化修复的失败。

Conclusion: 世界模型量化存在独特的失败模式，不同于传统的精度-比特宽度权衡。研究结果为在严格计算约束下部署量化世界模型提供了实用指导，揭示了量化对基于世界模型的规划任务的特有影响。

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [391] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

TL;DR: 本文研究了用户驱动隐私保护下混合原始、泛化和缺失值的表格数据机器学习问题，提出了考虑异构匿名化的数据转换策略，证明泛化值优于纯抑制，最佳数据准备策略取决于具体场景。


<details>
  <summary>Details</summary>
Motivation: 用户驱动隐私保护允许个人控制数据共享粒度，导致数据集混合原始、泛化和缺失值。这种表示对隐私直观，但对机器学习构成挑战，因为传统方法将非原始值视为新类别或缺失，丢弃了泛化语义。

Method: 提出了考虑异构匿名化的新颖数据转换策略，并与标准插补和基于LLM的方法进行比较。采用多个数据集、隐私配置和部署场景进行评估。

Result: 方法可靠地恢复了效用，证明泛化值优于纯抑制，最佳数据准备策略取决于具体场景，一致的数据表示对维持下游效用至关重要。

Conclusion: 有效学习与适当处理匿名化值密切相关，用户驱动隐私下的机器学习需要专门的数据转换策略来保留泛化语义。

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [392] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

TL;DR: MaskLAM通过引入视觉智能体分割来改进潜在动作模型，有效过滤动作相关噪声，在MuJoCo任务中实现高达4倍的奖励提升


<details>
  <summary>Details</summary>
Motivation: 潜在动作模型(LAMs)能够从原始观测中学习提取动作相关表示，但面临一个关键挑战：难以将动作相关特征与动作相关噪声（如背景运动）解耦。未能过滤这些干扰因素会导致LAMs捕捉虚假相关性并构建次优的潜在动作空间。

Method: MaskLAM是对LAM训练的轻量级修改，通过整合视觉智能体分割来缓解这一问题。该方法利用预训练基础模型生成的语义分割掩码来加权LAM的重建损失，从而优先考虑显著信息而非背景元素，同时不需要架构修改。

Result: 在添加了动作相关背景噪声的连续控制MuJoCo任务上，该方法相比标准基线实现了高达4倍的累积奖励提升，并通过线性探针评估显示潜在动作质量提高了3倍。

Conclusion: MaskLAM通过简单的训练修改有效解决了LAMs中的动作相关噪声问题，显著提升了潜在动作表示的质量和强化学习性能，为从无标签视频中学习提供了更鲁棒的方法。

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [393] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

TL;DR: Lotus是一种新的训练方法，通过修改投影过程解决了GaLore方法中SVD计算带来的额外时间成本问题，在保持内存效率的同时显著减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前大规模模型训练方法在内存消耗、训练时间和模型性能之间存在权衡。GaLore方法虽然通过低秩梯度更新实现了内存高效训练，但SVD过程带来了相当的额外训练时间成本。需要解决这种权衡问题。

Method: Lotus通过修改投影过程，提出了一个量化单位梯度位移的准则，实现了低秩梯度子空间之间的高效转换，避免了昂贵的SVD计算。

Result: 实验结果表明，Lotus是最有效的方法：训练时间减少30%，梯度和优化器状态的内存消耗降低40%。在预训练和微调任务中都优于基线方法。

Conclusion: Lotus成功解决了训练效率中内存消耗、训练时间和模型性能之间的权衡问题，通过简单的投影修改实现了显著的时间和内存效率提升。

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [394] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于响应长度的动态学习率调度器，用于稳定大语言模型的强化学习训练，通过监测梯度噪声和训练-推理不匹配问题来预防训练崩溃。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的强化学习训练存在严重的不稳定性问题，传统方法如重要性采样在长时间训练中可能失效。研究发现这种不稳定性源于训练-推理不匹配与梯度噪声的协同恶化，需要动态的优化策略来解决。

Method: 提出一种专门的学习率调度器，不同于传统的预定义衰减计划，该方法基于响应长度动态触发学习率衰减。响应长度被识别为即将发生不稳定性的可靠早期预警信号，通过在学习率衰减时降低梯度噪声，保持训练-推理不匹配在安全水平。

Result: 实验证据表明，通过响应长度监测梯度噪声上升并相应降低学习率，能够持续稳定强化学习训练，有效控制训练-推理不匹配问题。

Conclusion: 训练-推理不匹配不是静态数值差异，而是与模型优化耦合的动态故障。基于响应长度的动态学习率调度提供了一种简单有效的解决方案，能够稳定大语言模型的强化学习训练过程。

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [395] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

TL;DR: 该研究使用机制可解释性方法探索脑到语音解码模型在不同语音模态（发声、默读、想象）中的内部表征机制，发现语音模态共享连续因果流形，跨模态转换由紧凑的层特定子空间介导。


<details>
  <summary>Details</summary>
Motivation: 虽然脑到语音解码模型在发声、默读和想象语音中表现良好，但这些模型如何在不同语音模态间捕获和传递信息的基本机制尚不清楚。本研究旨在因果性地探究神经语音解码器的内部表征机制。

Method: 采用机制可解释性方法：1）跨模态激活修补分析内部激活；2）三模态插值检验语音表征是离散还是连续变化；3）粗到细因果追踪和因果擦除以定位因果结构；4）神经元级激活修补分析效应的分布情况。

Result: 发现：1）语音模态位于共享的连续因果流形上；2）跨模态转换由紧凑的层特定子空间介导，而非扩散活动；3）影响跨模态转换的是小但非分布式的神经元子集，而非孤立单元；4）揭示了跨语音模态的层次性和方向依赖性表征结构。

Conclusion: 研究为脑到语音解码模型如何组织和利用语音模态信息提供了因果解释，揭示了跨语音模态的层次性和方向依赖性表征结构，表明语音模态共享连续因果流形，跨模态转换由紧凑的层特定子空间介导。

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [396] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 该论文提出了一种主动强化学习（ActiveRL）方法，通过有限的在线交互来选择性细化价值函数的不确定区域，相比纯离线方法显著提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习存在状态-动作空间覆盖不足和分布偏移问题，需要一种能够通过有限在线交互来选择性细化不确定区域的方法，以提高学习效率。

Method: 提出ActiveRL算法，利用高斯过程（GP）不确定性建模，通过信息增益引导的主动采样来减少价值函数的不确定性，结合GP浓度不等式和信息增益边界进行理论分析。

Result: 理论证明ActiveRL能以O(1/ε²)的主动转移学习ε-最优策略，优于纯离线方法的Ω(1/ε²(1-γ)⁴)速率，实现了近乎最优的信息效率。

Conclusion: ActiveRL通过引导不确定性减少实现了加速的价值函数收敛，以最少的在线数据达到高效学习，为贝叶斯非参数回归和强化学习理论的结合提供了桥梁。

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [397] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

TL;DR: DOGMA是一个数据中心的AI框架，通过整合多层次的生物学先验知识（统计锚点、细胞本体论、系统发育树、基因本体论）来重构单细胞转录组数据的结构和语义，实现确定性图结构发现和跨物种对齐，在复杂多物种多器官基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞转录组分析方法存在两个主要问题：1）早期序列方法将细胞视为独立实体，忽略了生物系统功能机制驱动的潜在细胞间关系；2）结构化方法虽然尝试捕捉细胞间关系，但依赖启发式规则而忽视了生物学先验知识，导致图表示质量不佳且计算开销大。

Method: DOGMA是一个整体性数据中心的框架，通过多层次的生物学先验知识进行结构重塑和语义增强：1）整合统计锚点、细胞本体论和系统发育树来实现确定性图结构发现和鲁棒的跨物种对齐；2）利用基因本体论来弥合特征层面的语义鸿沟，融入功能先验知识。

Result: 在复杂的多物种和多器官基准测试中，DOGMA实现了最先进的性能，表现出卓越的零样本鲁棒性和样本效率，同时以显著更低的计算成本运行。

Conclusion: DOGMA通过整合多层次的生物学先验知识，超越了依赖随机启发式的方法，为单细胞转录组分析提供了一个更有效的数据中心解决方案，能够实现确定性结构发现和跨物种对齐，同时降低计算成本并提高性能。

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [398] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

TL;DR: MACI方法通过乘法过滤框架和集成学习改进LLM事实性验证，在保证覆盖率的同时显著提高真实声明的保留率


<details>
  <summary>Details</summary>
Motivation: 现有保形推理方法在医学和法律等高风险领域应用时过于保守（丢弃过多真实声明）或依赖简单线性模型，无法捕捉复杂的群体结构，需要更准确的事实性验证方法

Method: 将保形推理重新构建为乘法过滤框架，将事实性建模为声明级分数的乘积；使用集成学习产生更准确的事实性分数，并通过群体条件校准保持有效性

Result: MACI在实验中始终达到用户指定的覆盖率，同时比基线方法显著提高保留率并降低时间成本

Conclusion: MACI方法通过乘法过滤和集成学习改进了LLM的事实性验证，在高风险领域应用中实现了更好的平衡

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [399] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: Prism是一个针对离散扩散语言模型的高效测试时扩展框架，通过分层轨迹搜索、局部分支和自验证反馈，在数学推理和代码生成任务上实现了性能与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展算法主要依赖自回归解码，不适合并行解码的离散扩散语言模型，因此需要开发有效且高效的TTS方法来释放dLLMs的生成潜力。

Method: 提出Prism框架，包含三个核心组件：(1)分层轨迹搜索，在去噪早期到中期窗口动态剪枝和重新分配计算；(2)局部分支与部分重掩码，探索多样化实现同时保留高置信度token；(3)自验证反馈，通过自评估提示替代外部验证器。

Result: 在三个dLLMs（LLaDA 8B Instruct、Dream 7B Instruct、LLaDA 2.0-mini）的四个数学推理和代码生成基准测试中，Prism实现了良好的性能-效率权衡，以显著更少的函数评估次数匹配了best-of-N性能。

Conclusion: Prism为离散扩散语言模型提供了一种高效且有效的测试时扩展框架，通过创新的计算分配和自验证机制，在保持性能的同时大幅减少了计算开销。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [400] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

TL;DR: FUPareto：基于帕累托优化的联邦遗忘框架，通过最小边界偏移损失和空空间投影多梯度下降算法，解决联邦遗忘中的效用-遗忘冲突和多客户端并发遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在三个关键问题：1）遗忘目标常损害模型效用或增加成员推理攻击风险；2）遗忘与效用之间存在固有冲突；3）多客户端并发遗忘支持差，梯度冲突会降低遗忘质量。

Method: 提出FUPareto框架：1）引入最小边界偏移损失，通过抑制目标类别logit低于最高非目标类别logit来提高遗忘效率并降低MIA风险；2）采用帕累托改进步骤保持模型效用；3）通过帕累托扩展保证遗忘，使用空空间投影多梯度下降算法解耦梯度冲突。

Result: 在多种场景下的广泛实验表明，FUPareto在遗忘效果和保留效用方面均优于现有最先进的联邦遗忘方法。

Conclusion: FUPareto通过帕累托增强优化有效解决了联邦遗忘中的关键挑战，实现了高效、公平的多客户端并发遗忘，同时最小化效用损失。

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [401] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

TL;DR: 提出一种数据高效的深度学习框架，使用最少传感器硬件实现精确的肌电假肢控制，通过混合Transformer架构和可学习时间嵌入，在稀疏双通道sEMG上达到95.7%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统肌电假肢控制依赖复杂密集的多传感器阵列，限制了消费者可及性。需要开发使用最少传感器硬件但仍能实现精确控制的方法。

Method: 采用混合Transformer架构，集成Time2Vec可学习时间嵌入捕获生物信号的随机时间扭曲，使用归一化加性融合策略对齐空间和时间特征的潜在分布，采用两阶段课程学习协议确保数据稀缺下的稳健特征提取。

Result: 在10类运动集上获得95.7% ± 0.20%的多受试者F1分数，优于标准Transformer和CNN-LSTM模型。快速校准协议仅需每个手势两次试验，就能将性能从21.0% ± 2.98%恢复到96.9% ± 0.52%。

Conclusion: 高保真时间嵌入可以补偿低空间分辨率，挑战了高密度传感的必要性。该框架为能够快速个性化的下一代假肢接口提供了稳健、经济高效的蓝图。

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [402] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

TL;DR: 使用预训练的视觉语言模型（VLM）作为自动评估器，指导强化学习回放缓冲区中经验的优先级排序，无需微调即可显著提升样本效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型和视觉语言模型已被整合到强化学习的各个组件中，但回放缓冲区这一存储和重用经验的核心组件尚未被探索。本文旨在填补这一空白，利用VLM指导回放缓冲区中经验的优先级排序。

Method: 使用冻结的预训练VLM作为自动评估器，识别和优先处理智能体经验中有前景的子轨迹。该方法无需对VLM进行微调，适用于游戏和机器人等离散和连续领域场景。

Result: 在游戏和机器人等场景中，使用该优先级排序方法的智能体相比先前方法，平均成功率提高了11-52%，样本效率提升了19-45%。

Conclusion: 利用预训练VLM指导回放缓冲区优先级排序是有效的，能够显著提升强化学习的样本效率和性能，为VLM在强化学习中的应用开辟了新方向。

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [403] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

TL;DR: COLT是一个轻量级多LLM协作框架，通过共享MCTS树实现编译器优化，让小模型协作达到或超越大模型性能，同时降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 模型服务成本主导AI系统，需要编译器优化。大模型指导编译器搜索成本高，小模型单独使用可靠性不足。需要探索多LLM协作能否以低成本达到或超越单一大模型性能。

Method: 提出COLT框架，在单个MCTS过程中协调多个模型。使用共享MCTS树作为协作基础，重用变换前缀和跨模型价值传播。每轮迭代中，执行LLM提出联合动作（编译器变换，下一个查询的模型）。引入模型感知树策略偏向小模型，并在搜索持续退化时升级到大模型。

Result: 论文声称COLT能够实现多LLM协作推理，主要依赖小模型，同时通过共享MCTS树和智能模型选择机制，达到或超越单一大模型的编译器优化性能。

Conclusion: 多LLM协作框架COLT通过轻量级MCTS实现模型间协调，避免了复杂的外部规划机制，能够在保持探索性的同时有效利用小模型，并在需要时升级到大模型，实现成本效益的编译器优化。

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [404] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

TL;DR: MCPST是一个用于少样本交通预测的多阶段共识时空框架，通过建模交通动态的扩散、同步和谱嵌入，自适应融合预测，并采用结构化元学习快速适应新城市，在跨域少样本场景中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中，跨域数据稀缺场景下的准确交通流预测面临挑战，历史数据有限阻碍模型训练和泛化能力，复杂的时空依赖性和非线性动态使不同城市间的少样本学习更加困难。

Method: 提出MCPST框架，包含三个核心创新：1) 多阶段引擎通过扩散、同步和谱嵌入建模交通动态；2) 自适应共识机制动态融合各阶段预测并确保一致性；3) 结构化元学习策略用于快速适应新城市。

Result: 在四个真实数据集上的实验表明，MCPST在时空图学习方法、动态图迁移学习方法、基于提示的时空预测方法和跨域少样本设置中，优于14种最先进方法，提高了预测准确性同时减少所需训练数据。

Conclusion: MCPST将交通预测重新概念化为多阶段共识学习问题，提供了理论保证和可解释性见解，为数据稀缺场景下的跨域交通预测提供了有效解决方案。

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [405] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

TL;DR: T-LLM：通过时间蒸馏框架将时间序列预测能力赋予通用大语言模型，无需推理时额外模块


<details>
  <summary>Details</summary>
Motivation: 时间序列数据与底层过程演化相关，只能随时间累积，限制了仅靠规模驱动的预训练效果。现有方法主要依赖表示级对齐或推理时时间模块，未能明确教会LLM预测行为。

Method: 提出T-LLM时间蒸馏框架，在训练期间从轻量级时间教师模型转移预测行为到通用LLM。教师模型结合趋势建模和频域分析提供结构化时间监督，推理时完全移除，仅保留LLM作为预测模型。

Result: 在基准数据集和传染病预测任务中，T-LLM在全样本、少样本和零样本设置下始终优于现有基于LLM的预测方法，同时实现了简单高效的部署流程。

Conclusion: T-LLM框架成功将时间序列预测能力赋予通用LLM，通过时间蒸馏解决了时间约束带来的挑战，在多种设置下表现出优越性能。

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [406] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

TL;DR: 提出一种基于条件概率分布的深度多元模型，通过训练参数化马尔可夫链核来学习联合概率分布，适用于多种下游任务和半监督学习场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度多元模型通常针对特定应用任务设计，限制了在其他下游任务中的适用性。需要一种更通用的建模方法。

Method: 通过条件概率分布表示联合概率分布，将每个变量组在其余变量条件下的条件分布建模。通过最大化数据似然来训练参数化马尔可夫链核。

Result: 该方法能够学习通用的联合概率分布模型，适用于几乎所有可能的下游任务，并支持广泛的半监督学习场景。

Conclusion: 提出的条件概率分布建模方法比传统任务特定设计更具通用性，通过马尔可夫链核训练策略实现了灵活的下游任务应用和半监督学习能力。

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [407] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: IntraSlice：一种基于模块内PCA压缩剪枝的框架，通过近似PCA方法实现无额外参数的Transformer模型压缩，在保持性能的同时提升推理速度


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临巨大规模挑战，结构化剪枝能加速但导致性能显著下降。现有PCA剪枝方法仅在模块间应用，引入额外参数且残差连接会严重破坏激活分布

Method: 提出IntraSlice框架，采用模块内块级PCA压缩剪枝。利用Transformer模块结构特性设计近似PCA方法，其变换矩阵可完全融合到模型中无额外参数。引入基于PCA的全局剪枝比例估计器，考虑压缩激活分布和模块重要性

Result: 在Llama2、Llama3和Phi系列模型上验证，在各种语言基准测试中，在相同压缩比或推理速度下，相比现有基线方法获得更优的压缩性能

Conclusion: IntraSlice通过模块内PCA压缩剪枝有效解决了现有方法引入额外参数和破坏激活分布的问题，实现了高效的大语言模型压缩部署

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [408] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

TL;DR: 本文提出SAME方法解决多模态持续指令调优中的专家路由漂移问题，通过正交子空间分解稳定路由选择，利用历史输入协方差进行曲率感知缩放防止专家覆盖，实现免排练的持续学习。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要持续扩展能力，但现有稀疏专家路由方法在数据分布演变时会出现路由漂移问题。例如，原本激活定位专家的grounding查询在学习OCR任务后可能被路由到无关专家，同时定位相关专家会被新任务覆盖而丧失原有功能。

Method: 提出SAME方法：1) 通过将路由动态分解为正交子空间并仅更新任务相关方向来稳定专家选择；2) 利用历史输入协方差进行曲率感知缩放来调节专家更新，防止专家漂移；3) 引入自适应专家激活机制，在训练时冻结选定专家，减少冗余计算和跨任务干扰。

Result: 大量实验表明SAME方法在多模态持续指令调优任务上取得了最先进的性能。

Conclusion: SAME方法有效解决了多模态持续学习中的路由漂移和专家漂移问题，通过稳定路由选择和防止专家覆盖，实现了高效且鲁棒的持续能力扩展。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [409] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: 层剪枝在分类任务上表现良好，但在生成式推理任务上存在严重退化，特别是多步推理任务。通过监督微调结合自生成响应可以部分恢复性能，但生成式推理的恢复仍有限制。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝技术在压缩大语言模型时，在分类任务上表现良好，但在生成式推理任务上存在严重性能退化。研究旨在探索层剪枝对生成式推理任务的影响及其恢复策略。

Method: 通过跨多个模型家族的系统研究，分析深度减少对多步推理任务的影响。在有限的后训练约束下，评估基于监督微调与自生成响应的简单缓解策略。

Result: 分类任务可保留高达90%的基线性能，生成式基准测试相比先前技术提升20-30个百分点。但生成式推理的恢复仍有限制，主要适用于较低剪枝比例。

Conclusion: 层剪枝对生成式推理存在实际限制，深度减少主要在较低剪枝比例下有效。研究为约束后训练机制下的深度减少应用提供了指导。

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [410] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

TL;DR: SRR提出结构化残差重建框架，通过保留权重的主要奇异子空间并分配秩预算来优化量化误差重建，同时支持量化参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 现有量化误差重建方法将全部秩预算用于误差重建，当权重具有内在低秩结构且量化破坏主要方向时，这种方法不够优化。

Method: 提出结构化残差重建(SRR)框架：1) 保留激活缩放权重的前k个奇异子空间；2) 仅量化残差部分；3) 使用剩余的r-k秩进行误差重建。提供理论指导的k选择准则，平衡秩约束下的量化暴露能量和不可恢复误差。

Result: 实验表明，SRR在PTQ中在不同模型和量化设置下持续降低困惑度，在2位量化参数高效微调(QPEFT)下，GLUE任务平均提升5.9个百分点。

Conclusion: SRR通过结构化秩分配优化量化误差重建，不仅提升PTQ性能，还自然支持量化参数高效微调，并通过梯度缩放稳定微调过程。

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [411] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

TL;DR: SNAP是一个基于自洽一致原则的自监督鲁棒计算框架，通过互一致性假设分配权重来量化一致性，强调可信项目并降低异常值影响，无需监督或先验知识。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需监督或先验知识的鲁棒计算框架，能够有效处理异常值，在高维设置中也能保持计算稳定性。

Method: 基于自洽一致原则（SNAP）和一致性-可靠性假设，通过分配权重来量化项目间的一致性，强调可信项目并降低异常值权重，实现指数级抑制异常值贡献。

Result: SNAP在向量平均和子空间估计等任务中表现出色，非迭代的SNAP方法优于迭代的Weiszfeld算法和两种多元中位数均值变体，异常值权重被指数级抑制。

Conclusion: SNAP提供了一个灵活、易用、广泛适用的鲁棒计算方法，能够在无监督情况下有效处理异常值，适用于高维环境。

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [412] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

TL;DR: 该研究分析了NVFP4量化训练中的异常值动态，发现Softmax Attention、Linear Attention和FFN中的特定组件会产生异常值，这些异常值从训练早期的瞬时尖峰演变为后期的持久热通道。基于此，作者提出了Hot-Channel Patch在线补偿机制和CHON训练方案，显著缩小了NVFP4与BF16的损失差距。


<details>
  <summary>Details</summary>
Motivation: 使用4位算术训练大型语言模型可以提高吞吐量和内存效率，但FP4的动态范围有限增加了对异常值的敏感性。虽然NVFP4通过分层微缩放减轻了量化误差，但与BF16相比仍存在持续的损失差距。本研究旨在深入分析NVFP4预训练过程中异常值的动态特性，包括其定位、成因和演化过程。

Method: 1. 对NVFP4预训练过程中的异常值动态进行纵向分析，重点关注其在架构中的定位、成因和时序演化；2. 识别产生异常值的特定架构组件；3. 提出Hot-Channel Patch在线补偿机制，识别热通道并使用硬件高效核重新注入残差；4. 开发CHON训练方案，将HCP与post-QK操作保护相结合。

Result: 研究发现：1. 与Softmax Attention相比，Linear Attention减少了每张量的重尾分布，但在块量化下仍表现出持续的块级尖峰；2. 异常值主要来自特定架构组件：SA中的Softmax、LA中的门控机制和FFN中的SwiGLU，其中"post-QK"操作对量化更敏感；3. 异常值从训练早期的瞬时尖峰演变为后期少量持久的"热通道"；4. 在GLA-1.3B模型上训练60B tokens，CHON将NVFP4与BF16的损失差距从0.94%降低到0.58%，同时保持下游任务准确性。

Conclusion: 通过深入分析NVFP4量化训练中的异常值动态，本研究揭示了异常值的来源和演化模式，并提出了有效的在线补偿机制CHON。该方法显著缩小了4位量化与BF16浮点训练之间的性能差距，为高效的大语言模型训练提供了实用的解决方案。

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [413] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

TL;DR: FORLER是一种离线联邦强化学习方法，通过服务器端的Q-ensemble聚合和设备端的actor rectification，解决异构低质量数据下的策略污染问题，提供安全策略改进保证。


<details>
  <summary>Details</summary>
Motivation: 在物联网系统中，联邦学习促进了在线强化学习，但与环境在线交互存在风险和成本。离线联邦强化学习允许设备从固定数据集学习，但在低质量异构数据下可能失效，存在策略污染问题。

Method: FORLER结合了服务器端的Q-ensemble聚合和设备端的actor rectification。服务器稳健地合并设备Q函数以控制策略污染，将繁重计算从资源受限硬件卸载而不损害隐私。本地通过零阶搜索高Q动作和定制正则化器丰富策略梯度，δ-periodic策略进一步减少本地计算。

Result: 理论分析提供了安全策略改进性能保证。大量实验表明FORLER在不同数据质量和异构性条件下始终优于强基线方法。

Conclusion: FORLER通过创新的服务器聚合和设备端优化技术，有效解决了离线联邦强化学习中的策略污染问题，在保持隐私的同时实现了稳健的性能提升。

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [414] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

TL;DR: FiLoRA是一个指令条件化的参数高效适应框架，通过分解特征组对齐的LoRA模块和应用指令条件门控，实现对内部特征依赖的显式控制，而不改变预测目标或任务语义。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型整合了跨模态的异构信号，但人们对其预测如何依赖于特定内部特征组以及这种依赖是否可以被有意控制的理解仍然有限。现有关于捷径和虚假行为的研究主要依赖于事后分析或特征移除，对于是否可以在不改变任务语义的情况下调节依赖关系提供了有限的见解。

Method: FiLoRA（Focus-and-Ignore LoRA）是一个指令条件化的参数高效适应框架。它将适应分解为特征组对齐的LoRA模块，并应用指令条件门控，使自然语言指令作为计算级控制信号而非任务重新定义。该方法允许在不修改标签空间或训练目标的情况下，选择性地放大或抑制核心和虚假特征组。

Result: 在文本-图像和音频-视觉基准测试中，指令条件门控在内部计算中诱导了一致且因果的转变，能够选择性地放大或抑制核心和虚假特征组。进一步分析表明，FiLoRA在虚假特征干预下提高了鲁棒性，揭示了超越相关性驱动学习的依赖调节机制。

Conclusion: FiLoRA提供了一种原则性机制来调节多模态基础模型对内部特征的依赖，超越了传统的相关性驱动学习，为在不改变任务语义的情况下控制模型对特定特征组的依赖提供了有效框架。

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [415] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

TL;DR: 本文解决了组合赌博机中设计高效无交换遗憾算法的问题，在动作数量N指数级大的情况下，实现了对T次线性、对N多对数依赖的交换遗憾。


<details>
  <summary>Details</summary>
Motivation: 组合赌博机中，动作数量N相对于问题维度呈指数级增长。虽然外部遗憾最小化问题已有较好理解，但实现多对数依赖N的无交换遗憾一直难以解决。本文旨在填补这一空白。

Method: 引入了一种无交换遗憾学习算法，该算法在组合赌博机中具有多对数依赖N的遗憾界。同时展示了如何在多种应用中高效实现该算法，每次迭代复杂度也呈多对数依赖N。

Result: 提出的算法实现了对时间范围T次线性、对动作数量N多对数依赖的交换遗憾，这在组合赌博机类别中是紧致的。算法还能在多种应用中高效实现。

Conclusion: 本文成功解决了组合赌博机中设计高效无交换遗憾算法的长期挑战，实现了多对数依赖N的遗憾界，并展示了实际应用的可行性。

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [416] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出一种为多任务强化学习策略在新任务上性能提供高置信度保证的方法，通过组合任务级泛化边界和任务内置信区间来实现。


<details>
  <summary>Details</summary>
Motivation: 当前多任务强化学习方法缺乏形式化的性能保证，这在安全关键场景部署时是必需的。需要为训练期间未见过的任务提供可靠性能保证。

Method: 提出新的泛化边界，组合两个部分：(1) 从有限次rollout得到的每个任务的置信下界；(2) 从有限个采样任务得到的任务级泛化边界，从而为从相同未知分布中抽取的新任务提供高置信度保证。

Result: 在多种最先进的多任务RL方法上验证，证明所提保证在理论上是可靠的，并且在现实样本量下具有信息性。

Conclusion: 该方法为多任务强化学习策略在新任务上的性能提供了形式化的高置信度保证，填补了现有方法在安全关键应用中的空白。

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [417] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

TL;DR: 该研究通过Tele-Lens探测方法发现，大型语言模型在思维链推理中表现出近视视野，主要进行增量式转换而非精确的全局规划，这一特性可用于增强不确定性估计和自动识别思维链绕过。


<details>
  <summary>Details</summary>
Motivation: 基于先前对思维链动态的观察：大语言模型在思维链出现前已存在潜在规划，这降低了显式思维链的重要性；但思维链对于需要多步推理的任务仍然关键。为了深入理解LLM内部状态与其语言化推理轨迹之间的关系，研究者希望探究LLM的潜在规划能力。

Method: 研究者开发了Tele-Lens探测方法，应用于不同任务领域的隐藏状态，以调查LLM的潜在规划强度。通过这种方法分析LLM在推理过程中的内部状态变化。

Result: 实证结果表明，LLM表现出近视视野，主要进行增量式转换而非精确的全局规划。利用这一特性，研究者提出并验证了一个假设：思维链中一小部分位置可以有效代表整个路径的不确定性。此外，研究还证明了自动识别思维链绕过可以在不降低性能的情况下实现。

Conclusion: 该研究强调了利用思维链动态的重要性，揭示了LLM推理过程中的近视特性，为增强不确定性估计和优化思维链使用提供了新见解。研究代码、数据和模型已开源。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [418] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

TL;DR: 论文将经典的最大熵原理扩展到冯·诺依曼熵，为核机器学习中的VNE最大化提供了博弈论和信息论基础。


<details>
  <summary>Details</summary>
Motivation: 冯·诺依曼熵在量子信息论中是基本量，最近被机器学习采用作为核矩阵和核协方差算子的谱多样性度量。虽然量子背景下最大化VNE已有研究，但在数据驱动背景下，特别是其决策论和博弈论解释的经典最大熵框架的类似物尚未明确发展。

Method: 将Grünwald和Dawid的最大熵原理的极小极大公式扩展到冯·诺依曼熵设置，为密度矩阵和迹归一化半正定算子的VNE最大化提供博弈论证明。这一视角在部分信息下给出最大VNE解的稳健解释，并阐明其在谱域中作为最小承诺推断的作用。

Result: 提出了最大VNE原理，并通过两个代表性应用说明其适用性：1）通过基于核的VNE最大化从多个归一化嵌入中选择核表示；2）从部分观测条目完成核矩阵。这些例子展示了该框架如何为核学习中基于VNE的方法提供统一的信息论基础。

Conclusion: 该工作将经典最大熵原理扩展到冯·诺依曼熵，为核机器学习中的VNE最大化提供了坚实的博弈论和信息论基础，展示了最大VNE原理在现代机器学习问题中的实际应用价值。

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [419] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

TL;DR: 提出一种两阶段优化框架，通过最小化层间重构损失来改进GPTQ分组量化方法，显著提升大语言模型低比特量化的精度


<details>
  <summary>Details</summary>
Motivation: 现有GPTQ方法虽然高效，但在确定分组尺度时忽略了输入统计特性和组间相关性，与其最小化层间重构损失的目标不匹配，导致精度下降

Method: 提出两阶段优化框架：第一阶段在GPTQ之前初始化分组尺度以最小化组内重构损失；第二阶段冻结GPTQ得到的整数权重，使用坐标下降算法和闭式更新规则优化分组尺度以最小化层间重构损失，并考虑前层量化误差防止误差累积

Result: 实验结果表明该方法能持续提升分组量化效果，以可忽略的开销实现更高的精度

Conclusion: 通过两阶段优化框架显式最小化层间重构损失，有效解决了GPTQ忽略输入统计和组间相关性的问题，为大语言模型低比特量化提供了更精确的解决方案

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [420] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

TL;DR: STAR-MD是一种SE(3)等变扩散模型，通过联合时空注意力机制生成微秒级蛋白质轨迹，在ATLAS基准测试中实现最先进性能，解决了现有生成模型在长时程模拟中的局限性。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟是研究蛋白质动力学的金标准，但计算成本限制了其在生物相关时间尺度上的应用。现有生成模型由于架构限制、误差累积和时空动态建模不足，在长时程生成方面存在困难。

Method: 提出STAR-MD（时空自回归展开分子动力学），这是一种可扩展的SE(3)等变扩散模型，采用因果扩散变换器，通过联合时空注意力机制高效捕捉复杂的时空依赖关系，避免现有方法的内存瓶颈。

Result: 在标准ATLAS基准测试中，STAR-MD在所有指标上均达到最先进性能，显著改善了构象覆盖、结构有效性和动态保真度。能够成功外推生成稳定的微秒级轨迹，而基线方法完全失败。

Conclusion: STAR-MD的联合时空建模能够在生物相关时间尺度上实现稳健的动力学模拟，为加速探索蛋白质功能开辟了新途径，同时揭示了当前模型在长时程生成方面的严重局限性。

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [421] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

TL;DR: EvoMU使用进化搜索自动发现特定任务的无学习损失函数，在有限计算资源下实现SotA性能


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在遗忘特定训练数据（如敏感或受版权保护的材料）。现有方法通过微调模型并使用保留整体效用的遗忘损失函数，但合适的损失函数空间巨大，且不存在普遍最优的损失函数，因为遗忘数据和保留数据的结构与重叠差异会导致同一损失在不同设置中表现不同

Method: EvoMU采用进化搜索程序，在可能的无学习损失函数空间中自动寻找任务特定的损失函数。该方法使用小型4B参数模型（Qwen3-4B-Thinking），在有限计算资源下实现自动科学发现（AI co-scientist）

Result: 在TOFU-5%、TOFU-10%、MUSE和WMDP数据集上超越了先前基于损失的遗忘方法，通过合成新颖的遗忘损失函数实现了最先进的结果

Conclusion: EvoMU展示了在有限计算资源下AI co-scientist的潜力，能够自动发现数据集特定的损失函数，无需人工干预，为机器遗忘领域提供了有效的自动化解决方案

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [422] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

TL;DR: 通过强化学习训练小型模型实现强大的生成选择能力，在数学和代码推理任务中超越提示和多数投票基线


<details>
  <summary>Details</summary>
Motivation: 现有生成选择方法如GenSelect虽然能解决Best-of-N选择质量瓶颈，但其强大性能主要局限于大型模型。本研究旨在探索小型推理模型是否也能获得强大的生成选择能力。

Method: 从大规模数学和代码指令数据集中筛选出同时包含正确和错误候选解决方案的实例，合成选择任务，然后使用DAPO强化学习训练1.7B参数模型，奖励正确的选择决策。

Result: 在数学推理（AIME24、AIME25、HMMT25）和代码推理（LiveCodeBench）基准测试中，训练后的小型模型一致优于提示和多数投票基线，性能通常接近甚至超过更大的模型。此外，这些性能提升能够泛化到从更强模型输出中进行选择，尽管训练时只使用了较弱模型的输出。

Conclusion: 强化学习是解锁小型模型中强大生成选择能力的可扩展方法，能够实现高效的测试时计算扩展。

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [423] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

TL;DR: 本文提出ECHO方法，通过联合利用局部熵和群体置信度自适应控制分支宽度，并引入在线置信度剪枝，解决测试时强化学习中存在的分支崩溃和早期伪标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 测试时强化学习通过多次rollout生成候选答案并使用多数投票构建伪标签进行在线更新。现有树结构rollout方法虽然提高了采样效率，但仍面临两个挑战：1）高熵分支可能导致rollout崩溃，分支预算集中在少数具有连续高熵段的轨迹上；2）早期伪标签噪声大且有偏差，可能导致自强化过拟合，使策略过早锐化并抑制探索。

Method: 提出Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO)方法：在rollout阶段，联合利用局部熵和群体置信度自适应控制分支宽度，引入在线置信度剪枝终止持续低置信度分支；在策略更新阶段，采用置信度自适应裁剪和熵-置信度混合优势塑造方法增强训练鲁棒性。

Result: 实验表明，ECHO在多个数学和视觉推理基准上取得一致性能提升，在有限rollout预算下具有更好的泛化能力。

Conclusion: ECHO通过自适应分支控制和鲁棒的策略优化，有效解决了测试时强化学习中的分支崩溃和早期偏差问题，提高了采样效率和泛化性能。

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [424] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

TL;DR: VQRound是一种参数高效的量化优化框架，通过将舍入矩阵重新参数化为紧凑码本，显著减少大语言模型量化所需的可训练参数，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统自适应舍入方法在大语言模型中需要密集的逐元素舍入矩阵，导致计算开销过大，无法扩展到数十亿参数的模型。需要一种既能保持自适应舍入优势（跨元素误差抵消）又能高效实现的方法。

Method: 提出VQRound框架：1）将舍入矩阵重新参数化为紧凑码本，大幅减少可训练参数；2）在L∞范数下最小化逐元素最坏情况误差，特别适合处理LLM中的重尾权重分布；3）识别舍入初始化的重要性，开发轻量级端到端微调流程，仅需128个样本即可优化所有层的码本。

Result: 在OPT、LLaMA、LLaMA2和Qwen3模型上的实验表明，VQRound在相同步数下比传统自适应舍入收敛更好，同时仅使用0.2%的可训练参数。证明了自适应舍入可以同时实现可扩展性和快速拟合。

Conclusion: VQRound成功解决了大语言模型量化中自适应舍入的效率问题，通过码本重新参数化和轻量级优化流程，实现了参数高效、快速收敛的量化方案，为大规模模型部署提供了实用解决方案。

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [425] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

TL;DR: 提出一个用于时间链接预测模型的反事实验证框架，通过生成具有已知真实因果结构的因果时间交互图来评估模型是否捕捉到因果机制。


<details>
  <summary>Details</summary>
Motivation: 当前时间链接预测模型通常基于预测准确性进行评估，但这种评估无法判断模型是否真正捕捉到时间交互背后的因果机制。需要一种能够验证模型因果理解能力的新评估框架。

Method: 1. 提出连续时间事件序列的结构方程模型，支持兴奋和抑制效应；2. 将该机制扩展到时间交互图；3. 提出基于跨模型预测误差的距离度量；4. 通过控制因果转移和时间戳重排两种方式实例化反事实评估。

Result: 经验验证了假设：在一个因果模型上训练的预测器，在足够远的模型上评估时性能会下降。时间戳重排作为具有可测量因果距离的随机扭曲，能够有效评估模型对因果结构的敏感性。

Conclusion: 该框架为因果感知的基准测试提供了基础，能够评估时间链接预测模型是否真正理解底层因果机制，而不仅仅是进行模式匹配。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [426] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

TL;DR: SurvKAN：基于KAN架构的全参数化时间连续生存模型，消除比例风险约束，在保持可解释性的同时实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 传统生存模型（如Cox）依赖比例风险等限制性假设，无法捕捉真实临床动态；深度学习模型（如DeepSurv、DeepHit）虽表达能力更强但牺牲了可解释性，限制了临床采用；现有KAN混合模型仍受限于半参数Cox框架

Method: 提出SurvKAN模型，基于Kolmogorov-Arnold Networks架构，将时间作为显式输入，直接预测对数风险函数，通过可学习的单变量函数保持特征随时间影响的可解释性，支持端到端的完整生存似然训练

Result: 在标准生存基准测试中，SurvKAN在一致性指数和校准指标上达到或优于经典方法和最先进基线；可解释性分析揭示了与医学领域知识一致的临床有意义模式

Conclusion: SurvKAN通过全参数化时间连续建模消除了比例风险约束，在保持临床可解释性的同时实现了竞争性预测性能，为临床决策提供了更透明可信的工具

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [427] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: ECHO-2是一个用于大语言模型后训练强化学习的分布式框架，通过远程推理工作节点和重叠策略传播来解决分布式rollout中的协调和传播延迟问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习是大语言模型后训练的关键阶段，涉及rollout生成、奖励评估和集中学习的重复交互。分布式执行rollout可以利用更经济高效的推理资源，但在广域协调和策略传播方面面临挑战，特别是存在不可忽视的传播延迟时。

Method: ECHO-2结合集中学习与分布式rollout，将有限策略陈旧性作为用户可控参数，使rollout生成、传播和训练能够重叠执行。引入基于重叠的容量模型，关联训练时间、传播延迟和rollout吞吐量，提供实用的资源配置规则。采用对等辅助流水线广播和成本感知的异构工作节点激活来缓解传播瓶颈并降低成本。

Result: 在真实广域带宽环境下对4B和8B模型进行GRPO后训练的实验表明，ECHO-2在保持与强基线相当的RL奖励的同时，显著提高了成本效率。

Conclusion: ECHO-2通过将策略陈旧性作为可控参数、重叠执行关键操作、优化资源配置和传播机制，有效解决了分布式强化学习后训练中的协调和延迟问题，实现了成本效益与性能的平衡。

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [428] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 线性注意力LLM的状态矩阵存在明显的秩分层现象：部分注意力头保持接近零的有效秩，而另一部分则快速增长至上限，这种分层是预训练获得的结构性特征而非输入依赖的瞬态现象。


<details>
  <summary>Details</summary>
Motivation: 线性注意力LLM通过固定大小的状态矩阵压缩上下文实现常数时间推理，但其内部状态动态机制不透明，需要深入研究以理解其工作原理并优化性能。

Method: 对最先进的线性注意力模型进行运行时状态动态的全面研究，通过实验分析不同推理上下文下的状态秩变化，使用诊断探针分析功能差异，并提出基于秩-范数的联合剪枝策略。

Result: 发现状态秩分层现象：低秩头对模型推理至关重要，而高秩头存在显著冗余；提出的联合秩-范数剪枝策略在保持模型准确性的同时，将KV缓存开销减少了38.9%。

Conclusion: 线性注意力头的秩分层是其固有的结构性特征，这一发现为模型压缩和优化提供了理论基础，基于秩的剪枝策略能有效减少计算开销而不显著影响性能。

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [429] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

TL;DR: HAE是一种针对多模态大语言模型的KV缓存逐出框架，通过分层自适应策略优化视觉与文本token的交互，显著减少内存使用并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存逐出策略未能有效处理视觉与文本token之间的异构注意力分布，导致效率低下或性能下降，需要专门针对MLLMs的优化方案。

Method: 提出分层自适应逐出框架：1)预填充阶段采用双注意力剪枝，利用视觉token稀疏性和注意力方差；2)解码阶段采用动态逐出策略，受操作系统回收站启发；3)通过索引广播减少计算开销。

Result: 在Phi3.5-Vision-Instruct模型上，KV缓存内存减少41%，图像理解任务准确率仅下降0.3%；故事生成推理速度提升1.5倍，同时保持输出质量。

Conclusion: HAE有效解决了MLLMs中视觉与文本token交互的异构注意力问题，在理论上有更好的信息完整性和更低的误差边界，显著提升了多模态任务的效率。

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [430] [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201)
*Abhijit Gupta*

Main category: cs.LG

TL;DR: CardinalGraphFormer是一种图Transformer模型，通过结合Graphormer的结构偏置和基数保持的未归一化聚合通道，在分子属性预测任务中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 药物发现需要在有限标记数据下进行高效的分子属性预测。化学空间极其庞大（约10^60个类药分子），而已获批药物仅数千种，因此需要利用大规模未标记分子数据进行自监督预训练来实现数据高效的分子表示学习。

Method: 提出CardinalGraphFormer图Transformer，结合Graphormer的结构偏置（最短路径距离、中心性、直接键边偏置），在最短路径距离≤3的结构化稀疏注意力机制下工作，并增加基数保持的未归一化聚合通道。预训练结合对比图级对齐和掩码属性重建。

Result: 在完全匹配的评估协议下，CardinalGraphFormer在所有11个评估任务中均提高了平均性能，并在MoleculeNet、OGB和TDC ADMET任务的11个公共基准测试中的10个上实现了统计显著的性能提升。

Conclusion: CardinalGraphFormer通过整合结构化稀疏注意力、结构偏置和基数保持聚合，在分子表示学习中取得了优越性能，为数据高效的药物发现提供了有效工具。

Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.

</details>


### [431] [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213)
*Gregory Barber,Todd C. Henry,Mulugeta A. Haile*

Main category: cs.LG

TL;DR: TIDES是一种文本引导的设计方法，通过结合文本描述和物理约束生成物理合理的设计，同时优化结构和视觉属性。


<details>
  <summary>Details</summary>
Motivation: 传统设计方法难以同时满足物理性能要求和用户指定的视觉特征，需要一种能够将文本描述与物理约束相结合的设计生成方法。

Method: 使用预训练的文本-图像模型评估设计与文本提示的视觉对齐度，结合可微分物理模拟器评估物理性能，联合优化结构拓扑和视觉属性。

Result: TIDES在不同载荷、支撑条件和分辨率的结构优化问题中表现良好，实验验证显示生成的2D梁设计在三点弯曲测试中满足工程设计要求（柔度和密度），同时实现了文本指定的特征。

Conclusion: TIDES能够成功联合优化物理性能和视觉对齐目标，生成既满足工程设计要求又体现文本指定特征的设计方案。

Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.

</details>


### [432] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

TL;DR: PPRM是一种基于预测驱动推理的半监督风险监控方法，用于动态环境中模型性能监控，通过结合合成标签和少量真实标签构建运行风险的下界，实现无假设的有限样本保证。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中监控模型性能时，标记数据有限是一个主要挑战。需要一种能够在有限标记数据下有效监控模型风险的方法。

Method: 提出预测驱动风险监控（PPRM），基于预测驱动推理（PPI），通过结合合成标签和少量真实标签构建运行风险的随时有效下界，通过阈值比较检测有害偏移。

Result: PPRM在图像分类、大语言模型（LLM）和电信监控任务中通过大量实验证明了其有效性，能够满足无假设的有限样本保证，控制误报概率。

Conclusion: PPRM为动态环境中有限标记数据下的模型风险监控提供了一种有效的半监督方法，具有理论保证和实际应用价值。

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [433] [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238)
*Laura Yao,Gengwei Zhang,Moajjem Chowdhury,Yunmei Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: TopoDiff：一种用于EEG空间超分辨率的几何和关系感知扩散模型，通过结合拓扑感知图像嵌入和动态通道关系图来提升EEG空间生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的EEG空间超分辨率方法通常缺乏对生理空间结构的认知，限制了空间生成性能。这些方法要么直接预测缺失信号，要么将潜在扩散模型应用于时序数据，但未能充分考虑EEG的几何特性和电极间关系。

Method: 提出TopoDiff模型，包含两个关键组件：1）从EEG地形表示中提取的拓扑感知图像嵌入，为空间生成提供全局几何上下文；2）动态通道关系图，编码电极间关系并随时间动态演化。该设计构建了一个空间基础扎实的EEG超分辨率框架。

Result: 在多个EEG数据集上验证了方法的有效性，包括SEED/SEED-IV情感识别、PhysioNet运动想象和TUSZ癫痫检测。TopoDiff在生成保真度方面取得显著提升，并显著改善了下游EEG任务性能。

Conclusion: TopoDiff通过引入几何和关系感知机制，成功解决了现有EEG空间超分辨率方法缺乏生理空间结构认知的问题，为EEG信号的空间生成提供了更有效的解决方案。

Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.

</details>


### [434] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

TL;DR: CurioSFT是一种保持熵的监督微调方法，通过内在好奇心增强探索能力，改善后续强化学习阶段的效果


<details>
  <summary>Details</summary>
Motivation: 传统的SFT-then-RL流程中，监督微调阶段会限制模型探索能力，导致过自信和多样性降低，从而限制了强化学习阶段的改进空间

Method: 提出CurioSFT方法，包含：(a)自我探索蒸馏：使用温度缩放的自我生成教师模型鼓励能力范围内的探索；(b)熵引导温度选择：自适应调整蒸馏强度，在推理标记处增强探索，在事实标记处保持稳定

Result: 在数学推理任务上，CurioSFT在监督微调阶段比传统SFT在分布内任务提升2.5分，分布外任务提升2.9分；在后续强化学习阶段平均提升5.0分

Conclusion: CurioSFT通过保持熵和增强探索能力，有效改善了监督微调阶段的质量，为后续强化学习阶段提供了更好的起点，从而获得更优的整体性能

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [435] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

TL;DR: 提出一种结合离散选择模型与强化学习的收益管理方法，通过模型辅助处理延迟反馈问题，在参数偏移场景下表现良好，但在模型假设违反时性能下降。


<details>
  <summary>Details</summary>
Motivation: 解决收益管理中因客户取消和修改导致的延迟反馈问题，传统强化学习难以处理这种延迟价值评估。

Method: 提出"选择模型辅助强化学习"方法，使用校准的离散选择模型作为固定部分世界模型，在决策时估算延迟的学习目标组件。

Result: 理论证明表格Q学习收敛到最优Q函数的邻域；实验显示：在平稳环境下与基线无差异，在参数偏移下5/10场景有显著收益提升（最高12.4%），但在模型假设违反时收益下降1.4-2.6%。

Conclusion: 部分行为模型在参数偏移下能提高鲁棒性，但在结构错误设定时会引入有害偏差；为模型辅助强化学习在延迟反馈场景的应用提供了理论保证和实证指导。

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [436] [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288)
*Zheng Li,Jerry Cheng,Huanying Gu*

Main category: cs.LG

TL;DR: 提出一种新的时间序列预测训练方法，通过强制自回归预测误差随预测范围增加而增加的原则，使模型能够连接短期预测形成灵活长期预测，在多个基准测试中达到新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型主要基于transformer架构，通过扩大模型规模而非真正的自回归滚动来实现长期预测。从大语言模型训练角度看，传统时间序列预测训练忽略了时间因果关系。

Method: 提出新颖的时间序列预测训练方法，强制两个关键特性：1) 自回归预测误差应随预测范围增加而增加，违反此原则被视为随机猜测并在损失函数中明确惩罚；2) 使模型能够连接短期自回归预测以形成灵活的长期预测。

Result: 该方法在多个基准测试中建立了新的最优性能，相比iTransformer和其他近期强基线，MSE降低了超过10%。此外，它使短期预测模型能够在超过7.5倍长的范围内进行可靠的长期预测。

Conclusion: 提出的训练方法通过强制时间因果关系和自回归预测误差的合理增长，显著提升了时间序列预测模型的性能，特别是在长期预测能力方面，为时间序列预测训练提供了新的有效范式。

Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt

</details>


### [437] [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366)
*Sharut Gupta,Phillip Isola,Stefanie Jegelka,David Lopez-Paz,Kartik Ahuja,Mark Ibrahim,Mohammad Pezeshki*

Main category: cs.LG

TL;DR: ReasonCACHE：一种通过前缀调优将演示蒸馏到固定键值缓存中的方法，使大语言模型无需权重更新即可学习推理，在多个推理基准上超越标准上下文学习并匹配或超越权重学习方法。


<details>
  <summary>Details</summary>
Motivation: 上下文学习虽然样本效率高，但复杂推理任务需要大量训练示例，而简单地增加演示数量会导致注意力成本二次增长、性能饱和或下降，且仍是浅层学习。由于这些限制，实践者主要依赖权重学习来诱导推理。本研究旨在探索LLMs能否在不更新权重的情况下学习推理。

Method: 提出ReasonCACHE方法，基于前缀调优机制，将演示蒸馏到固定的键值缓存中。这种方法不超载上下文窗口，无需权重更新，直接向注意力机制注入键值对，绕过低秩权重更新的表达性限制。

Result: 在包括GPQA-Diamond在内的挑战性推理基准上，ReasonCACHE优于标准上下文学习，匹配或超越权重学习方法。同时在三个关键维度上更高效：数据效率、推理成本和可训练参数数量。理论证明ReasonCACHE比低秩权重更新具有更严格的表达性。

Conclusion: ReasonCACHE在上下文学习和权重学习之间提供了一条中间路径，为超越上下文窗口学习推理技能提供了可扩展算法，且无需修改模型参数。

Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/

</details>


### [438] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

TL;DR: C-kNN-LSH：基于局部敏感哈希的最近邻框架，用于高维混杂纵向数据的因果效应估计，在长新冠队列中表现优异


<details>
  <summary>Details</summary>
Motivation: 从纵向轨迹估计因果效应对于理解复杂疾病进展和优化临床决策至关重要，特别是在处理合并症和长新冠恢复等高维混杂情况时

Method: 提出C-kNN-LSH框架，利用局部敏感哈希高效识别具有相似协变量历史的"临床双胞胎"，实现跨演变疾病状态的局部条件治疗效应估计；结合双重稳健校正来减轻不规则采样和患者恢复特征变化带来的偏差

Result: 理论分析证明估计量具有一致性和对干扰误差的二阶稳健性；在包含13,511名参与者的真实世界长新冠队列中，C-kNN-LSH在捕捉恢复异质性和估计政策价值方面优于现有基线方法

Conclusion: C-kNN-LSH为处理高维混杂纵向数据的因果推断提供了一种有效框架，特别适用于长新冠等复杂疾病的恢复轨迹分析

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [439] [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381)
*Yipeng Zhang,Hafez Ghaemi,Jungyoon Lee,Shahab Bakhtiari,Eilif B. Muller,Laurent Charlin*

Main category: cs.LG

TL;DR: AdaSSL通过引入潜变量解决自监督学习中的一对多映射问题，适用于对比学习和蒸馏方法，在因果表示学习、细粒度图像理解和视频世界建模中表现出色。


<details>
  <summary>Details</summary>
Motivation: 自监督学习通常假设数据对之间存在确定性映射，但在实际应用中（如连续视频帧），一个数据点可能对应多个有效目标，形成一对多映射问题。现有方法难以灵活捕捉这种条件不确定性。

Method: 引入潜变量来建模条件不确定性，推导出配对嵌入之间互信息的变分下界，得到对标准SSL目标的简单正则化项。该方法称为AdaSSL，适用于对比学习和基于蒸馏的自监督学习目标。

Result: AdaSSL在因果表示学习、细粒度图像理解和视频世界建模等多个任务中表现出优越性能，展示了方法的通用性和有效性。

Conclusion: 通过引入潜变量建模一对多映射中的条件不确定性，AdaSSL为自监督学习提供了一种灵活有效的解决方案，能够更好地处理现实世界中自然生成过程产生的数据对。

Abstract: Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.

</details>


### [440] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

TL;DR: SLIME是一种新的无参考对齐方法，通过锚定项、稳定惩罚和双边界机制解决现有偏好优化方法中的目标不匹配问题，防止高质量输出被"遗忘"和格式化崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有的直接偏好优化方法虽然计算效率高，但存在关键的目标不匹配问题：优化选择与拒绝响应之间的相对边界并不能保证保留选择响应的绝对可能性，导致"遗忘"（高质量输出概率降低）和"格式化崩溃"（拒绝序列被过度惩罚）。

Method: SLIME采用三部分目标：(1)锚定项最大化偏好响应的可能性；(2)稳定惩罚防止拒绝标记概率崩溃为零；(3)结合硬约束和软约束的双边界机制进行精确边界塑造。

Result: SLIME在性能上优于最先进的基线方法，同时保持了更高的生成稳定性。

Conclusion: SLIME成功地将偏好学习与生成质量解耦，解决了现有直接偏好优化方法中的目标不匹配问题，实现了更好的对齐效果和稳定性。

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [441] [Transformers learn factored representations](https://arxiv.org/abs/2602.02385)
*Adam Shai,Loren Amdahl-Culleton,Casper L. Christensen,Henry R. Bigelow,Fernando E. Rosas,Alexander B. Boyd,Eric A. Alt,Kyle J. Ray,Paul M. Riechers*

Main category: cs.LG

TL;DR: 通过下一个词预测预训练的Transformer将世界分解为部分，并在残差流的正交子空间中表示这些因子。研究比较了两种表示假设：乘积空间表示（维度指数增长）与因子化表示（维度线性增长），发现模型倾向于因子化表示，即使这会牺牲预测保真度。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型如何表示世界结构，特别是它们是否将复杂世界分解为独立因子并在正交子空间中表示这些因子。理解模型表示结构有助于解释为什么Transformer能够有效学习复杂数据。

Method: 提出两种表示假设的数学形式化：乘积空间表示和因子化表示。在已知潜在结构的合成过程上训练Transformer，通过分析激活的几何结构（子空间数量、维度、上下文嵌入排列）来测试这两种假设。

Result: 当因子条件独立时，模型学习因子化表示；即使在训练早期存在噪声或隐藏依赖关系破坏条件独立性时，模型仍然倾向于因子化表示，反映了以保真度为代价进行因子分解的归纳偏置。

Conclusion: Transformer倾向于将世界分解为部分并在正交子空间中表示，这为解释Transformer如何分解世界提供了原则性解释，并表明即使在复杂数据上训练的模型中，可解释的低维结构可能仍然存在。

Abstract: Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.

</details>


### [442] [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400)
*Qizhen Zhang,Ankush Garg,Jakob Foerster,Niladri Chatterji,Kshitiz Malik,Mike Lewis*

Main category: cs.LG

TL;DR: 研究发现噪声数据确实会导致大语言模型预训练损失发散，发散概率与噪声类型、噪声量和模型规模密切相关。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练数据集包含大量噪声数据，这些噪声被认为可能导致大语言模型预训练不稳定甚至损失发散，但这一现象尚未得到充分理解。

Method: 通过向干净数据集中注入受控的合成均匀随机噪声，分析从480M到5.2B参数的不同规模模型的训练动态。

Result: 噪声数据确实会引发训练损失发散，发散概率强烈依赖于噪声类型、噪声量和模型规模；噪声引起的发散表现出与高学习率引起的发散不同的激活模式。

Conclusion: 研究提供了关于噪声数据如何影响大语言模型预训练损失发散的大规模受控特征分析，并提供了区分不同故障模式的诊断方法。

Abstract: Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.

</details>


### [443] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

TL;DR: DAIL方法通过两步策略解决专家解决方案与LLM分布不匹配问题：先将专家解答转化为详细推理轨迹，再使用对比目标聚焦专家洞察，仅需不到1000个高质量专家样本即可显著提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理能力提升面临两大挑战：1）困难问题对前沿模型仍不可解，无法提取有效训练信号；2）高质量专家解决方案昂贵且分布不匹配，直接模仿会失败，因为专家解答是为人类设计的教学性内容，包含隐含推理跳跃。

Method: 提出分布对齐模仿学习（DAIL）的两步方法：第一步将专家解决方案转化为详细、分布内的推理轨迹；第二步应用对比目标，聚焦学习专家的洞察和方法论，而不是简单模仿表面形式。

Result: DAIL仅需不到1000个高质量专家解决方案，就能在Qwen2.5-Instruct和Qwen3模型上实现10-25%的pass@k提升，推理效率提高2-4倍，并具备跨领域泛化能力。

Conclusion: DAIL通过有效解决专家数据分布不匹配问题，为利用有限高质量专家数据提升LLM推理能力提供了高效、可泛化的训练方法，突破了传统强化学习需要模型能采样正确解或依赖更强模型的限制。

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


### [444] [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415)
*Vivienne Pelletier,Daniel J. Rivera,Obinna Nwokonkwo,Steven A. Wilson,Christopher L. Muhich*

Main category: cs.LG

TL;DR: ATBagging是一种新的主动学习种子集选择方法，通过贝叶斯解释的装袋集成模型估计候选数据点的信息量，结合特征空间多样性采样，在低数据量场景下显著提升主动学习早期性能。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习的早期性能通常受限于随机选择的初始种子集，而实际应用中常存在相关或近似数据集可用于构建更好的种子集。本文旨在利用这些可用数据来改进主动学习的启动阶段。

Method: 提出Active-Transfer Bagging (ATBagging)方法：1）通过贝叶斯解释的装袋集成模型，比较带标签数据集的内袋和外袋预测分布来估计候选数据点的信息量；2）为避免冗余选择，使用随机傅里叶特征和质-量多样性分解的确定性点过程在特征空间施加多样性约束；3）将同一混合方法用于主动学习阶段的新数据点选择。

Result: 在四个真实数据集（QM9、ERA5、Forbes 2000和北京PM2.5）上评估，涵盖目标迁移和特征偏移场景。在种子大小nseed=10-100范围内，ATBagging在几乎所有情况下都改善或持平早期主动学习性能，并提高学习曲线下面积，在低数据量场景中效果最显著。

Conclusion: ATBagging提供了一种低成本、高回报的方法来启动基于主动学习的数据收集，特别是在低数据量场景下能显著提升主动学习的早期性能。

Abstract: Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.

</details>


### [445] [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417)
*Zekun Wang,Anant Gupta,Christopher J. MacLellan*

Main category: cs.LG

TL;DR: 该论文提出了一种结合生成回放和Fisher度量信任区域约束的混合持续学习方法，该方法在局部近似下表现出类似MAML的元学习特性，能在任务转换后快速重新收敛到先前任务的最优点。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法面临核心权衡：基于正则化的方法（如EWC）在任务最优解重叠度低时会过度约束更新，而基于回放的方法虽然能保持性能但会因不完美的回放而漂移。需要一种结合两者优势的混合方法。

Method: 提出了信任区域持续学习方法，结合生成回放和Fisher度量信任区域约束。在局部近似下，该方法更新具有类似MAML的解释：回放提供旧任务梯度信号（类似查询），Fisher加权惩罚提供高效的离线曲率塑造（类似支持）。

Result: 在任务增量扩散图像生成和持续扩散策略控制任务上，信任区域持续学习方法取得了最佳最终性能和保留率，并且比EWC、回放和持续元学习基线更快地恢复早期任务性能。

Conclusion: 该方法在持续学习中展现出涌现的元学习特性：模型成为一个初始化点，能在每次任务转换后快速重新收敛到先前任务的最优点，而无需显式优化双层目标。这为持续学习提供了一种有效的混合策略。

Abstract: Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.

</details>


### [446] [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

TL;DR: 本文提出了"多注意力机制"这一广义的自注意力框架，能够处理任意高阶张量计算和输入标记间的复杂关系结构，系统研究了其计算复杂性和表示能力，并给出了新的算法和复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 传统的自注意力机制虽然能有效建模标记间的成对交互，但无法处理涉及三个或更多标记相关性的基本任务，也无法完成需要引用多个输入标记的组合任务。现有的一些高阶注意力替代方案虽然能处理某些多元任务，但计算复杂度高（超二次时间）。

Method: 定义了广义的自注意力机制类别——多注意力机制，能够纳入任意高阶（张量）计算和输入标记间的任意关系结构。系统研究了这些机制的计算复杂性和表示能力，包括给出新算法和匹配的复杂度理论下界，精确和近似计算注意力矩阵的时间复杂度，并严格确定了每种机制能执行的多元任务。

Result: 提出了一个新的注意力机制，可以在二次时间内精确计算，并且能够执行任意固定数量函数的函数组合。而先前的机制即使只组合两个函数，也需要超二次时间计算，新的下界表明更快的算法是不可能的。研究揭示了这些机制在不同需求之间的有趣权衡，包括机制表达能力与模型系数大小之间的紧密关系，以便机制能在几乎线性时间内近似。

Conclusion: 多注意力机制提供了一个强大的框架来扩展自注意力的能力，使其能够处理更复杂的多元交互和组合任务。研究在表达能力和计算效率之间建立了重要的权衡关系，为设计更强大的注意力机制提供了理论基础和实用指导。

Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.

</details>


### [447] [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425)
*Amaru Caceres Arroyo,Lea Bogensperger,Ahmed Allam,Michael Krauthammer,Konrad Schindler,Dominik Narnhofer*

Main category: cs.LG

TL;DR: CHASE框架通过压缩预训练蛋白质语言模型的嵌入到紧凑潜空间，使用条件流匹配模型和分类器自由引导，无需预测器引导即可直接生成高适应性蛋白质变体，在AAV和GFP基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 蛋白质适应性优化面临组合空间巨大的挑战，高适应性变体极其稀疏。现有方法要么性能不足，要么需要计算昂贵的基于梯度的采样。

Method: CHASE框架重新利用预训练蛋白质语言模型的进化知识，将其嵌入压缩到紧凑潜空间。通过训练带有分类器自由引导的条件流匹配模型，在ODE采样步骤中无需预测器引导即可直接生成高适应性变体。

Result: 在AAV和GFP蛋白质设计基准测试中达到最先进性能。在数据受限场景中，使用合成数据进行引导可以进一步提升性能。

Conclusion: CHASE通过有效利用预训练蛋白质语言模型的进化知识，提供了一种无需预测器引导的高效蛋白质适应性优化方法，在数据受限场景中通过合成数据引导可进一步提升性能。

Abstract: Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.

</details>


### [448] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

TL;DR: 该研究探索了如何更好地量化大语言模型在推理过程中的中间不确定性，发现基于token嵌入扰动的敏感性评分能有效识别错误推理步骤。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然取得重大突破，但仍可能产生不可靠或误导性输出。为了负责任地应用LLM，需要不确定性量化技术来评估模型对其输出的置信度。对于推理任务，不仅需要评估最终答案的不确定性，还需要评估推理中间步骤的不确定性，以便进行更细粒度和有针对性的干预。

Method: 研究探索了哪些不确定性量化指标能更好地反映LLM在推理过程中的"中间不确定性"。研究发现，LLM的错误推理步骤往往包含对前一个token嵌入扰动高度敏感的token。因此，可以使用这种敏感性评分作为指导来识别错误（不确定）的中间步骤。该方法基于token嵌入扰动计算敏感性评分。

Result: 实验表明，基于扰动的指标在不确定性量化性能上优于基线方法（如token生成概率和token熵）。此外，与依赖多次采样的方法不同，基于扰动的指标提供了更好的简单性和效率。

Conclusion: 基于token嵌入扰动的敏感性评分是一种有效且高效的方法，可用于识别LLM推理过程中的错误中间步骤，为不确定性量化提供了新的视角。

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [449] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

TL;DR: 提出两种基于贝叶斯优化的方法（Thompson采样和知识梯度），用于极小失效概率（10^-6-10^-8）的可靠性优化问题，结合重要性采样提升效率


<details>
  <summary>Details</summary>
Motivation: 制造业中需要优化设计可靠性（最小化失效概率），但失效概率极低（10^-6-10^-8），传统贝叶斯优化方法难以处理这种极端情况

Method: 提出两种贝叶斯优化方法：1）基于Thompson采样的方法；2）基于知识梯度的方法（近似最小化失效概率对数的一步贝叶斯最优策略），两种方法都结合重要性采样来处理极小失效概率

Result: 实验结果表明，所提方法在极端和非极端失效概率情况下都优于现有方法

Conclusion: 提出的结合重要性采样的贝叶斯优化方法能有效处理极小失效概率的可靠性优化问题，在Thompson采样和知识梯度两种框架下都表现出优越性能

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [450] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

TL;DR: Expert-Sample方法通过在高置信度专家选择的基础上，向不确定的尾部注入可控随机性，提升细粒度MoE模型的多样本推理性能


<details>
  <summary>Details</summary>
Motivation: 测试时缩放通过生成多个候选解提升LLM性能，但token级采样需要温度调优来平衡多样性与稳定性。细粒度MoE的丰富路由空间提供了未探索的替代方案，研究发现路由分数呈现高置信度专家头部和低置信度候选尾部的模式

Method: 提出Expert-Sample训练免费方法，保留高置信度选择，同时向不确定尾部注入可控随机性，实现多样化生成而不破坏输出稳定性

Result: 在多个细粒度MoE模型的数学、知识推理和代码任务评估中，Expert-Sample持续提升pass@n和基于验证的准确率。Qwen3-30B-A3B-Instruct在GPQA-Diamond上，32并行样本的pass@32从85.4%提升到91.9%，准确率从59.1%提升到62.6%

Conclusion: 细粒度MoE路由模式中，确定头部控制核心推理能力，不确定尾部与推理多样性相关。Expert-Sample方法有效利用这一特性，在不破坏稳定性的前提下提升推理多样性

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [451] [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445)
*Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: 该论文为非线性随机逼近算法在Wasserstein-p距离下推导了非渐近误差界，通过耦合方法比较离散时间过程与极限Ornstein-Uhlenbeck过程，获得最终迭代的显式有限样本保证，并分析了Polyak-Ruppert平均的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 传统随机逼近算法的分析主要关注渐近性质或基于矩界的概率不等式，缺乏对最终迭代的有限样本分布保证。本文旨在填补这一空白，为非线性和线性随机逼近算法提供非渐近的分布收敛速率分析。

Method: 开发耦合论证方法，将离散时间随机逼近过程与极限Ornstein-Uhlenbeck过程进行比较；在驱动噪声满足非渐近中心极限定理的条件下，分析最终迭代和Polyak-Ruppert平均在Wasserstein-p距离下的收敛速率。

Result: 证明归一化最终迭代以γ_n^{1/6}的速率在p-Wasserstein距离下收敛到高斯分布（γ_n为步长）；Polyak-Ruppert平均以n^{-1/6}的速率收敛；这些分布保证导出了优于矩界和马尔可夫不等式的高概率浓度不等式。

Conclusion: 该研究为随机逼近算法提供了非渐近的分布收敛理论框架，填补了有限样本分析与渐近理论之间的空白，在线性随机逼近和随机梯度下降等应用中展示了该方法的实用性。

Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.

</details>


### [452] [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451)
*Patrick Cooper,Alvaro Velasquez*

Main category: cs.LG

TL;DR: ACE通过偏好优化学习因果实验设计策略，相比传统方法在同等干预预算下提升70-71%效果


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法（随机采样、贪婪信息最大化、轮询覆盖）将每个实验决策视为独立问题，无法从经验中学习自适应策略，限制了实验效率

Method: 提出Active Causal Experimentalist (ACE)，将实验设计建模为序列决策问题，利用直接偏好优化从干预比较中学习策略，而非依赖不稳定的奖励幅度

Result: 在合成基准、物理模拟和经济数据上，ACE在同等干预预算下比基线方法提升70-71%效果（p < 0.001, Cohen's d ~ 2），并能自主发现理论支持的实验策略

Conclusion: 基于偏好的学习能够恢复原则性实验策略，通过经验学习补充理论，实现领域自适应，为因果发现提供了新的实验设计框架

Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.

</details>


### [453] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

TL;DR: 提出RL-CRP框架，使用去中心化强化学习和冲突风险预测来优化多服务器联邦学习中的客户端选择，减少服务器间冲突并提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习存在高通信延迟问题，而多服务器联邦学习中客户端覆盖重叠和无协调选择会导致资源竞争、带宽冲突和训练失败，需要优化客户端选择机制。

Method: 提出RL-CRP框架：1）每个服务器使用基于稀疏历史客户端选择序列的分类隐马尔可夫模型预测客户端选择冲突可能性；2）结合公平感知奖励机制促进客户端长期参与，最小化训练延迟和资源竞争。

Result: 大量实验表明，RL-CRP框架能有效减少服务器间冲突，在收敛速度和通信成本方面显著提高训练效率。

Conclusion: 提出的去中心化强化学习与冲突风险预测框架能够优化多服务器联邦学习系统的客户端选择，解决资源竞争问题，提升整体训练性能。

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [454] [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472)
*Qifan Yu,Xinyu Ma,Zhijian Zhuo,Minrui Wang,Deyi Liu,Shiyi Zhan,Yiyuan Ma,Liang Xiang,Xingyan Bin,Di He*

Main category: cs.LG

TL;DR: SPARKLING是一种用于模型宽度扩展的新框架，解决了中训练阶段宽度扩展的训练不稳定问题，通过信号保持和对称性破坏机制，在MoE模型上实现35%的训练成本降低。


<details>
  <summary>Details</summary>
Motivation: 渐进学习通过逐步增加模型规模来减少预训练计算开销。虽然已有研究广泛探索了深度扩展，但宽度扩展研究严重不足，现有方法仅限于训练早期阶段。然而，在中训练阶段进行宽度扩展对于最大化计算节省至关重要，但由于严重的训练不稳定性，这仍然是一个巨大挑战。

Method: 提出SPARKLING框架，通过RMS尺度一致性实现信号保持，稳定扩展期间的激活统计；通过非对称优化器状态重置和学习率重新预热确保对称性破坏，解决梯度对称性问题。

Result: 在混合专家模型上的广泛实验表明，SPARKLING在多个宽度轴和优化器家族中始终优于从头训练，在2倍宽度扩展下将训练成本降低高达35%。

Conclusion: SPARKLING有效解决了中训练阶段宽度扩展的挑战，通过信号保持和对称性破坏机制实现了稳定的训练和显著的计算节省，为渐进学习中的宽度扩展提供了实用解决方案。

Abstract: Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.

</details>


### [455] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

TL;DR: 论文提出RLTF框架，利用文本反馈作为介于稀疏奖励和完整演示之间的中间监督信号，通过两种方法（自我蒸馏和反馈建模）让模型内化反馈以提升单轮推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法依赖稀疏的二进制奖励或偏好标签，信息量有限；而蒸馏方法需要完整的演示，成本高且难以扩展。文本反馈作为中间信号，比标量奖励更丰富，又比完整演示更便宜，是自然的人类交互方式且在许多实际场景中已大量存在。

Method: 提出RLTF框架，在训练时使用文本反馈但推理时不使用。开发两种方法：1) RLTF-SD：训练单轮策略以匹配其自身反馈条件下的第二轮生成；2) RLTF-FM：将预测反馈作为辅助目标。两种方法都旨在让模型内化反馈信息。

Result: 在推理谜题、竞赛数学和创意写作任务上的实验表明，两种方法在多个基准测试中均持续优于强基线方法，证明了利用丰富文本反馈进行RL的潜力。

Conclusion: 文本反馈作为介于稀疏奖励和完整演示之间的中间监督信号，通过RLTF框架可以有效提升LLM的单轮推理性能，为大规模利用丰富监督信号提供了有前景的方向。

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


### [456] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

TL;DR: RLAnything是一个强化学习框架，通过闭环优化动态构建环境、策略和奖励模型，增强LLM和智能体场景的学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在LLM和智能体场景中面临学习信号弱、系统整体优化不足的问题，需要一种能够动态调整环境、策略和奖励的集成框架来提升学习效率。

Method: 提出闭环优化框架，策略通过步进信号和结果信号的集成反馈进行训练，奖励模型通过一致性反馈联合优化，同时基于理论动机的环境自适应机制利用批评反馈改进训练。

Result: 每个组件都持续改进整体系统性能：Qwen3-VL-8B-Thinking在OSWorld上提升9.1%，Qwen2.5-7B-Instruct在AlfWorld和LiveBench上分别提升18.7%和11.9%。优化的奖励模型信号优于依赖人工标签的结果。

Conclusion: RLAnything通过动态构建环境、策略和奖励模型的闭环优化，显著提升了LLM和智能体在各种任务中的性能，证明了集成强化学习框架的有效性。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [457] [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494)
*Dulhan Jayalath,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: MEG-XL模型通过2.5分钟的长上下文预训练，在脑电数据单词解码任务中实现了数据高效泛化，仅需少量数据即可达到监督学习性能


<details>
  <summary>Details</summary>
Motivation: 临床脑-文本接口需要为瘫痪患者设计，这些患者无法提供大量训练数据。现有预训练方法通常只使用几秒的上下文，无法捕捉自然语言处理所需的长时间神经上下文信息。

Method: 提出MEG-XL模型，使用每个样本2.5分钟的MEG上下文进行预训练（比先前工作长5-300倍，相当于191k个标记），然后对脑电数据的单词解码任务进行微调。

Result: MEG-XL仅需少量数据（如1小时vs 50小时）即可匹配监督学习性能，并优于其他脑基础模型。长上下文预训练的模型学习到的表征能更好地迁移到单词解码任务。

Conclusion: 长上下文预训练有助于利用其他方法不必要丢弃的扩展神经上下文，为临床脑-文本接口提供了数据高效的解决方案。

Abstract: Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [458] [Beyond Static Question Banks: Dynamic Knowledge Expansion via LLM-Automated Graph Construction and Adaptive Generation](https://arxiv.org/abs/2602.00020)
*Yingquan Wang,Tianyu Wei,Qinsi Li,Li Zeng*

Main category: cs.CY

TL;DR: 提出Generative GraphRAG框架，通过自动构建层次知识图谱和认知图推理，解决个性化教育系统中知识建模成本高和缺乏状态感知推理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有个性化教育系统面临两个根本限制：1) 知识图谱构建依赖人工，成本高且扩展性差；2) 缺乏对学习者知识状态的有效推理支持，依赖静态题库，适应性有限。

Method: 提出Generative GraphRAG框架，包含两个核心模块：Auto-HKG（利用LLM自动构建层次知识图谱）和CG-RAG（基于学习者掌握图谱进行图推理，结合检索增强生成个性化习题）。

Result: 框架已在真实教育场景中部署，获得积极的用户反馈，显示出支持实际个性化教育系统的潜力。

Conclusion: 该框架通过自动化知识建模和状态感知推理，为个性化教育系统提供了可扩展且有效的解决方案。

Abstract: Personalized education systems increasingly rely on structured knowledge representations to support adaptive learning and question generation. However, existing approaches face two fundamental limitations. First, constructing and maintaining knowledge graphs for educational content largely depends on manual curation, resulting in high cost and poor scalability. Second, most personalized education systems lack effective support for state-aware and systematic reasoning over learners' knowledge, and therefore rely on static question banks with limited adaptability. To address these challenges, this paper proposes a Generative GraphRAG framework for automated knowledge modeling and personalized exercise generation. It consists of two core modules. The first module, Automated Hierarchical Knowledge Graph Constructor (Auto-HKG), leverages LLMs to automatically construct hierarchical knowledge graphs that capture structured concepts and their semantic relations from educational resources. The second module, Cognitive GraphRAG (CG-RAG), performs graph-based reasoning over a learner mastery graph and combines it with retrieval-augmented generation to produce personalized exercises that adapt to individual learning states. The proposed framework has been deployed in real-world educational scenarios, where it receives favorable user feedback, suggesting its potential to support practical personalized education systems.

</details>


### [459] [Early Warning Signals Appear Long Before Dropping Out: An Idiographic Approach Grounded in Complex Dynamic Systems Theory](https://arxiv.org/abs/2602.00021)
*Mohammed Saqr,Sonsoles López-Pernas,Santtu Tikka,Markus Wolfgang Hermann Spitzer*

Main category: cs.CY

TL;DR: 该研究首次在教育领域应用临界减速理论，通过分析160万次数学练习数据，发现88.2%学生在放弃学习前会出现临界减速信号，可作为早期预警指标。


<details>
  <summary>Details</summary>
Motivation: 学生学习中的韧性和坚持能力对学习成功至关重要。当韧性减弱时，学生面临脱离学习和辍学的风险。因此，在"希望窗口期"预测学生脱离学习非常重要。研究旨在测试基于临界减速理论的早期预警信号是否能预测学生放弃学习。

Method: 研究使用9,401名学生在数字数学学习环境中的167万次练习数据。计算了临界减速指标：自相关、返回率、方差、偏度、峰度和变异系数。分析这些指标在学生放弃学习前的表现模式。

Result: 88.2%的学生在脱离学习前表现出临界减速信号。警告信号集中在活动后期和停止练习（辍学）之前。这是首次在教育领域发现临界减速现象的证据。

Conclusion: 研究表明普遍存在的韧性动态也支配着人类学习等社会系统。临界减速指标为早期检测脆弱性提供了实用工具，可在关键事件发生前支持学习者。最重要的是，这些指标具有普遍性，不依赖于数据生成机制，为跨情境、跨数据类型和学习环境的可移植性提供了新机会。

Abstract: The ability to sustain engagement and recover from setbacks (i.e., resilience) -- is fundamental for learning. When resilience weakens, students are at risk of disengagement and may drop out and miss on opportunities. Therefore, predicting disengagement long before it happens during the window of hope is important. In this article, we test whether early warning signals of resilience loss, grounded in the concept of critical slowing down (CSD) can forecast disengagement before dropping out. CSD has been widely observed across ecological, climate, and neural systems, where it precedes tipping points into catastrophic failure (dropping out in our case). Using 1.67 million practice attempts from 9,401 students who used a digital math learning environment, we computed CSD indicators: autocorrelation, return rate, variance, skewness, kurtosis, and coefficient of variation. We found that 88.2% of students exhibited CSD signals prior to disengagement, with warnings clustering late in activity and before practice ceased (dropping out). Our results provide the first evidence of CSD in education, suggesting that universal resilience dynamics also govern social systems such as human learning. These findings offer a practical indicator for early detection of vulnerability and supporting learners across different applications and contexts long before critical events happen. Most importantly, CSD indicators arise universally, independent of the mechanisms that generate the data, offering new opportunities for portability across contexts, data types, and learning environments.

</details>


### [460] [Strategies for Creating Uncertainty in the AI Era to Trigger Students Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System](https://arxiv.org/abs/2602.00026)
*Ahmad Samer Wazan*

Main category: cs.CY

TL;DR: 论文提出通过AI制造不确定性情境来促进批判性思维的教学方法，开发了MindMosaicAIExam考试系统，要求学生对AI输出进行批判性评估和推理迭代。


<details>
  <summary>Details</summary>
Motivation: 生成式AI让学生无需理解就能获得正确答案，挑战传统评估方式。作者认为不应禁止AI，而应将其整合到教育中，利用AI制造不确定性情境来激发学生的批判性思维。

Method: 基于认识论和批判性思维研究，围绕AI模型和教师的固有局限性设计学习活动和评估。通过控制AI行为（如阻止直接答案或生成看似合理但有缺陷的回应）来防止AI成为确定性的捷径。开发了MindMosaicAIExam考试系统，整合可控AI工具，要求学生提供初始答案、批判性评估AI输出并迭代完善推理。

Result: 提出了MindMosaicAIExam考试系统，该系统收集学生的推理过程，并设计了评估批判性思维的评分标准。通过控制AI行为创造不确定性情境，促进学生进行推理、质疑和论证。

Conclusion: 将AI整合到教育中的有效方法是通过AI制造不确定性情境，采用以思维为导向的教学方法，将不确定性作为激发批判性思维的核心教学概念。这种方法鼓励学生进行推理、质疑和论证，而不是简单地依赖AI获取正确答案。

Abstract: Generative AI challenges traditional assessments by allowing students to produce correct answers without demonstrating understanding or reasoning. Rather than prohibiting AI, this work argues that one way to integrate AI into education is by creating uncertain situations with the help of AI models and using thinking-oriented teaching approaches, where uncertainty is a central pedagogical concept for stimulating students critical thinking. Drawing on epistemology and critical thinking research studies, we propose designing learning activities and assessments around the inherent limitations of both AI models and instructors. This encourages students to reason, question, and justify their final answers. We show how explicitly controlling AI behavior during exams (such as preventing direct answers or generating plausible but flawed responses) prevents AI from becoming a shortcut to certainty. To support this pedagogy, we introduce MindMosaicAIExam, an exam system that integrates controllable AI tools and requires students to provide initial answers, critically evaluate AI outputs, and iteratively refine their reasoning. We also present an evaluation rubric designed to assess critical thinking based on students reasoning artifacts collected by the exam system.

</details>


### [461] [Happy Young Women, Grumpy Old Men? Emotion-Driven Demographic Biases in Synthetic Face Generation](https://arxiv.org/abs/2602.00032)
*Mengting Wei,Aditya Gulati,Guoying Zhao,Nuria Oliver*

Main category: cs.CY

TL;DR: 该研究系统审计了8个最先进的文本到图像模型（4个西方开发，4个中国开发），分析其合成人脸生成中的偏见问题，发现所有模型都存在人口统计和情绪条件偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像和多模态大语言模型在合成人脸生成方面取得快速进展，但这些模型的偏见、表征质量和跨文化一致性仍然了解不足。现有研究主要关注人口统计偏见，缺乏关于情绪提示如何影响人口表征以及不同文化和语言背景下训练的模型输出分布差异的研究。

Method: 对8个最先进的T2I模型进行系统审计（4个西方组织开发，4个中国机构开发），使用相同提示生成人脸。采用最先进的面部分析算法估计生成人脸的性别、种族、年龄和吸引力水平。应用信息论偏见度量（包括Kullback-Leibler和Jensen-Shannon散度）来衡量与全球人口统计数据的偏差。

Result: 研究发现所有模型无论其原产国如何，都存在持续的人口统计和情绪条件偏见。这表明偏见问题在跨文化模型中普遍存在。

Conclusion: 研究结果对公平性、社会技术危害、治理以及透明生成系统的开发具有重要意义。需要进一步关注生成模型中的偏见问题，特别是在跨文化应用场景中。

Abstract: Synthetic face generation has rapidly advanced with the emergence of text-to-image (T2I) and of multimodal large language models, enabling high-fidelity image production from natural-language prompts. Despite the widespread adoption of these tools, the biases, representational quality, and cross-cultural consistency of these models remain poorly understood. Prior research on biases in the synthetic generation of human faces has examined demographic biases, yet there is little research on how emotional prompts influence demographic representation and how models trained in different cultural and linguistic contexts vary in their output distributions. We present a systematic audit of eight state-of-the-art T2I models comprising four models developed by Western organizations and four developed by Chinese institutions, all prompted identically. Using state-of-the-art facial analysis algorithms, we estimate the gender, race, age, and attractiveness levels in the generated faces. To measure the deviations from global population statistics, we apply information-theoretic bias metrics including Kullback-Leibler and Jensen-Shannon divergences. Our findings reveal persistent demographic and emotion-conditioned biases in all models regardless of their country of origin. We discuss implications for fairness, socio-technical harms, governance, and the development of transparent generative systems.

</details>


### [462] [LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion](https://arxiv.org/abs/2602.00038)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Hongyu Li,Mingyuan Chu,Xin Zhang,Jun Zhou*

Main category: cs.CY

TL;DR: LSSF是一个低秩安全子空间融合框架，通过提取LLM中的低秩安全信息主成分，在微调后恢复模型的安全对齐能力，同时最小化对下游任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制脆弱，即使在没有有害内容的微调数据集上也可能破坏安全能力。现有安全对齐方法主要依赖微调过程，导致复杂度和计算资源需求增加。

Method: 提出LSSF框架，利用LLM中安全信息的低秩特性，构建低秩投影矩阵提取安全向量的主成分。该投影矩阵代表LLM的低秩安全子空间，在微调过程中保持稳定且与模型通用能力隔离。通过安全奇异值熵度量不同层安全信息编码密度，动态计算每个安全向量的安全关键秩。

Result: 大量实验表明，该后处理对齐方法能有效恢复微调模型的安全对齐能力，同时对下游任务性能影响最小。

Conclusion: LSSF框架通过低秩安全子空间融合，提供了一种高效的后处理安全对齐方法，解决了现有方法在复杂度和计算资源方面的不足。

Abstract: The safety mechanisms of large language models (LLMs) exhibit notable fragility, as even fine-tuning on datasets without harmful content may still undermine their safety capabilities. Meanwhile, existing safety alignment methods predominantly rely on the fine-tuning process, which inadvertently leads to the increased complexity and computational resources required. To address these issues, we introduce LSSF, a novel safety re-alignment framework with \underline{L}ow-Rank \underline{S}afety \underline{S}ubspace \underline{F}usion. Our proposed method exploits the low-rank characteristics of safety information in LLMs by constructing a low-rank projection matrix to extract the principal components of safety vectors. Notably, this projection matrix represents the low-rank safety subspace of the LLMs, which we have observed to remain stable during fine-tuning process and is isolated from the model's general capabilities. These principal components are used to effectively restore safety alignment when combined with fine-tuned LLMs through linear arithmetic. Additionally, to account for the varying encoding densities of safety information across different layers of LLMs, we propose a novel metric called safety singular value entropy. This metric quantifies the encoding density and allows for the dynamic computation of the safety-critical rank for each safety vector. Extensive experiments demonstrate that our proposed post-hoc alignment method can effectively restore the safety alignment of fine-tuned models with minimal impact on their performance in downstream tasks.

</details>


### [463] [Student Perceptions of Large Language Models Use in Self-Reflection and Design Critique in Architecture Studio](https://arxiv.org/abs/2602.00041)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Nachamma Sockalingam,Khoo Eng Tat,Julfendi*

Main category: cs.CY

TL;DR: 本研究探讨了将大型语言模型（LLMs）整合到建筑设计工作室反馈机制中，从生成性生产转向反思性教学。研究发现学生将LLMs视为协作的"认知镜子"而非权威指导者，在不同反馈场景中发挥不同作用。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何将大型语言模型从生成工具转变为教学辅助工具，特别是在建筑设计教育中，如何利用LLMs增强学生的反思能力和批判性思维，改变传统的反馈机制。

Method: 采用混合方法研究，在新加坡科技设计大学的建筑学生中进行研究，分析学生在三个不同反馈领域的感知：自我反思、同伴批评和教授主导的评审。

Result: 研究发现：1）在自主学习时，LLMs帮助学生构建思维、克服"空白页"问题，但缺乏情境细微差别；2）在同伴批评中，LLMs作为中立调解者，减轻社交焦虑和"冒犯恐惧"；3）在高风险的教授评审中，学生主要将LLMs用作评审后综合引擎，管理认知负荷并将抽象学术讨论转化为可行的设计迭代。

Conclusion: LLMs在建筑设计教育中可以作为有效的"认知镜子"，支持学生的批判性思维发展。它们在不同反馈场景中发挥不同作用，但需要认识到其局限性，特别是缺乏情境理解能力。这表明LLMs在反思性教学中有重要应用潜力。

Abstract: This study investigates the integration of Large Language Models (LLMs) into the feedback mechanisms of the architectural design studio, shifting the focus from generative production to reflective pedagogy. Employing a mixed-methods approach with architecture students at the Singapore Uni-versity of Technology and Design, the research analyzes student percep-tions across three distinct feedback domains: self-reflection, peer critique, and professor-led reviews. The findings reveal that students engage with LLMs not as authoritative instructors, but as collaborative "cognitive mir-rors" that scaffold critical thinking. In self-directed learning, LLMs help structure thoughts and overcome the "blank page" problem, though they are limited by a lack of contextual nuance. In peer critiques, the technology serves as a neutral mediator, mitigating social anxiety and the "fear of of-fending". Furthermore, in high-stakes professor-led juries, students utilize LLMs primarily as post-critique synthesis engines to manage cognitive overload and translate abstract academic discourse into actionable design iterations.

</details>


### [464] [When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications](https://arxiv.org/abs/2602.00044)
*Hongliu Cao,Eoin Thomas,Rodrigo Acuna Agost*

Main category: cs.CY

TL;DR: PBA是一种通过开放式角色生成来检测LLM偏见的可扩展透明审计方法，能够发现多维度偏见并支持纵向追踪


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的偏见输出会强化刻板印象并在实际应用中延续不平等，因此公平性审计至关重要。现有方法依赖固定身份类别和静态基准，存在局限性。

Method: 提出Persona Brainstorm Audit (PBA)方法，通过开放式角色生成来检测偏见。该方法不依赖固定身份类别，能够发现多维度社会偏见，支持纵向追踪，并降低数据泄露风险。

Result: 将PBA应用于12个最先进的LLM，比较了不同模型、维度和版本的偏见严重程度，发现了独特的模式和谱系特异性变异，追踪了偏见在连续代际中的衰减、持续或重新出现。鲁棒性分析显示PBA在不同样本大小、角色扮演提示和去偏见提示下保持稳定。

Conclusion: PBA为LLM公平性审计提供了一种可靠、可扩展且透明的方法，能够有效检测多维度偏见并支持纵向追踪，为模型公平性评估提供了新的工具。

Abstract: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.

</details>


### [465] [How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI](https://arxiv.org/abs/2602.00056)
*Sophia N. Wilson,Sebastian Mair,Mophat Okinyi,Erik B. Dam,Janin Koch,Raghavendra Selvan*

Main category: cs.CY

TL;DR: 该论文分析了AI大规模数据的可持续性成本，提出"超数据化"概念，指出数据相关成本向全球南方、不稳定数据工作者和弱势文化转移的问题，并提出Data PROOFS建议框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示支撑前沿AI模型的大规模数据的环境、社会和经济学成本，这些成本往往被忽视。作者认为AI领域正从"用数据构建模型"转向"为构建模型主动创造数据"，这种"超数据化"转变对AI未来和社会影响具有关键意义。

Method: 研究方法包括：1）定量分析Hugging Face Hub上约55万个数据集，关注数据集增长、存储相关能耗和碳足迹、语言数据的社会代表性；2）定性分析肯尼亚数据工作者的回应，考察劳动参与情况；3）结合外部数据源分析全球数据中心基础设施的不平等分布。

Result: 分析结果显示：1）超数据化不仅增加资源消耗，还系统性将环境负担、劳动风险和代表性损害重新分配给全球南方、不稳定的数据工作者和代表性不足的文化；2）存在全球数据中心基础设施的显著不平等；3）数据工作者面临大科技公司直接雇佣和接触不良内容的双重风险。

Conclusion: 结论是超数据化带来了不可持续的成本分配模式。为此提出Data PROOFS建议框架，涵盖来源追溯、资源意识、所有权、开放性、节俭性和标准制定等方面，旨在减轻这些成本，并激发研究社区和更广泛社会对这些被忽视的数据成本的讨论。

Abstract: Large-scale data has fuelled the success of frontier artificial intelligence (AI) models over the past decade. This expansion has relied on sustained efforts by large technology corporations to aggregate and curate internet-scale datasets. In this work, we examine the environmental, social, and economic costs of large-scale data in AI through a sustainability lens. We argue that the field is shifting from building models from data to actively creating data for building models. We characterise this transition as hyper-datafication, which marks a critical juncture for the future of frontier AI and its societal impacts. To quantify and contextualise data-related costs, we analyse approximately 550,000 datasets from the Hugging Face Hub, focusing on dataset growth, storage-related energy consumption and carbon footprint, and societal representation using language data. We complement this analysis with qualitative responses from data workers in Kenya to examine the labour involved, including direct employment by big tech corporations and exposure to graphic content. We further draw on external data sources to substantiate our findings by illustrating the global disparity in data centre infrastructure. Our analyses reveal that hyper-datafication does not merely increase resource consumption but systematically redistributes environmental burdens, labour risks, and representational harms toward the Global South, precarious data workers, and under-represented cultures. Thus, we propose Data PROOFS recommendations spanning provenance, resource awareness, ownership, openness, frugality, and standards to mitigate these costs. Our work aims to make visible the often-overlooked costs of data that underpin frontier AI and to stimulate broader debate within the research community and beyond.

</details>


### [466] [A longitudinal geospatial multimodal dataset of post-discharge frailty, physiology, mobility, and neighborhoods](https://arxiv.org/abs/2602.00060)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: cs.CY

TL;DR: GEOFRAIL数据集：针对出院后社区居住的衰弱老年人，通过多模态传感技术收集的纵向地理空间数据集，用于监测恢复轨迹


<details>
  <summary>Details</summary>
Motivation: 老年人衰弱与功能下降、行动能力减弱、社会隔离等风险相关，影响从医院到社区的过渡恢复。社区环境因素进一步影响恢复轨迹，需要实时监测这些多维因素

Method: 收集社区居住衰弱老年人出院后八周的数据，包括多模态传感器特征、双周临床评估（衰弱、身体功能、社会隔离）、地理位置记录（链接到社区设施、犯罪率、社会经济指标），采用隐私保护的空间聚合标准化流程

Result: 建立了GEOFRAIL数据集，包含相互关联的数据表，技术验证显示地理空间、传感器衍生和临床测量之间具有内部一致性，机器学习模型在表征恢复轨迹方面表现出基线性能

Conclusion: GEOFRAIL数据集为研究衰弱老年人出院后恢复轨迹提供了多模态地理空间数据资源，支持数据驱动的方法来理解环境因素对恢复过程的影响

Abstract: Frailty in older adults is associated with increased vulnerability to functional decline, reduced mobility, social isolation, and challenges during the transition from hospital to community living. These factors are associated with rehospitalization and may adversely influence recovery. Neighborhood environments can further shape recovery trajectories by affecting mobility opportunities, social engagement, and access to community resources. Multimodal sensing technologies combined with data-driven analytical approaches offer the potential to continuously monitor these multidimensional factors in real-world settings. This Data Descriptor presents GEOFRAIL, a longitudinal geospatial multimodal dataset collected from community-dwelling frail older adults following hospital discharge. The dataset is organized into interconnected tables capturing participant demographics, features derived from multimodal sensors, biweekly clinical assessments of frailty, physical function, and social isolation, and temporal location records linked to neighborhood amenities, crime rates, and census-based socioeconomic indicators. Data were collected over an eight-week post-discharge period using standardized pipelines with privacy-preserving spatial aggregation. Technical validation demonstrates internal consistency across geospatial, sensor-derived, and clinical measures and reports baseline performance of machine learning models for characterizing recovery trajectories.

</details>


### [467] [Responsible Evaluation of AI for Mental Health](https://arxiv.org/abs/2602.00065)
*Hiba Arnaout,Anmol Goel,H. Andrew Schwartz,Steffen T. Eberhardt,Dana Atzil-Slonim,Gavin Doherty,Brian Schwartz,Wolfgang Lutz,Tim Althoff,Munmun De Choudhury,Hamidreza Jamalabadi,Raj Sanjay Shah,Flor Miriam Plaza-del-Arco,Dirk Hovy,Maria Liakata,Iryna Gurevych*

Main category: cs.CY

TL;DR: 本文提出一个跨学科框架，重新思考心理健康AI工具的责任评估，强调临床有效性、社会背景和公平性，而非仅依赖通用指标。


<details>
  <summary>Details</summary>
Motivation: 当前评估心理健康AI工具的方法分散且与临床实践、社会背景和用户体验脱节，需要更负责任、更全面的评估框架。

Method: 通过分析135篇*CL出版物，识别现有局限性，提出整合临床合理性、社会背景和公平性的跨学科框架，并建立AI心理健康支持类型分类法。

Result: 发现现有研究过度依赖不捕捉临床有效性、治疗适当性或用户体验的通用指标，心理健康专业人员参与有限，对安全和公平关注不足。

Conclusion: 需要重新思考责任评估，建立结构化框架，通过分类法（评估型、干预型、信息合成型）针对不同类型AI工具制定特定风险评估和评估要求。

Abstract: Although artificial intelligence (AI) shows growing promise for mental health care, current approaches to evaluating AI tools in this domain remain fragmented and poorly aligned with clinical practice, social context, and first-hand user experience. This paper argues for a rethinking of responsible evaluation -- what is measured, by whom, and for what purpose -- by introducing an interdisciplinary framework that integrates clinical soundness, social context, and equity, providing a structured basis for evaluation. Through an analysis of 135 recent *CL publications, we identify recurring limitations, including over-reliance on generic metrics that do not capture clinical validity, therapeutic appropriateness, or user experience, limited participation from mental health professionals, and insufficient attention to safety and equity. To address these gaps, we propose a taxonomy of AI mental health support types -- assessment-, intervention-, and information synthesis-oriented -- each with distinct risks and evaluative requirements, and illustrate its use through case studies.

</details>


### [468] [Adoption and Use of LLMs at an Academic Medical Center](https://arxiv.org/abs/2602.00074)
*Nigam H. Shah,Nerissa Ambers,Abby Pandya,Timothy Keyes,Juan M. Banda,Srikar Nallan,Carlene Lugtu,Artem A. Trotsyuk,Suhana Bedi,Alyssa Unell,Miguel Fuentes,Francois Grolleau,Sneha S. Jain,Jonathan Chen,Devdutta Dash,Danton Char,Aditya Sharma,Duncan McElfresh,Patrick Scully,Vishanthan Kumar,Connor OBrien,Satchi Mouniswamy,Elvis Jones,Krishna Jasti,Gunavathi Mannika Lakshmanan,Sree Ram Akula,Varun Kumar Singh,Ramesh Rajmanickam,Sudhir Sinha,Vicky Zhou,Xu Wang,Bilal Mawji,Joshua Ge,Wencheng Li,Travis Lyons,Jarrod Helzer,Vikas Kakkar,Ramesh Powar,Darren Batara,Cheryl Cordova,William Frederick,Olivia Tang,Phoebe Morgan,April S. Liang,Stephen P. Ma,Shivam Vedak,Dong-han Yao,Akshay Swaminathan,Mehr Kashyap,Brian Ng,Jamie Hellman,Nikesh Kotecha,Christopher Sharp,Gretchen Brown,Christian Lindmark,Anurang Revri,Michael A. Pfeffer*

Main category: cs.CY

TL;DR: ChatEHR是一个集成LLM到电子健康记录的系统，支持自动化任务和交互式使用，在临床文档处理中减少工作流程摩擦，已实现显著的成本节约和用户采用。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在临床文档处理中存在的"工作流程摩擦"问题，即手动数据录入带来的效率低下，需要将LLM与完整的患者时间线数据集成到电子健康记录系统中。

Method: 开发ChatEHR系统，该系统支持两种使用模式：1) 自动化任务（静态提示与数据组合执行固定任务）；2) 通过用户界面在EHR中进行交互式使用。系统采用模型无关设计，可访问多种数据类型，并建立了监控评估方法。

Result: 在1.5年内构建了7个自动化任务，1075名用户经过培训成为常规用户，前3个月进行了23,000次会话。总结生成是最频繁的任务，每生成约0.73个幻觉和1.60个不准确信息。初步估计第一年可节省600万美元，未计入提供更好护理的效益。

Conclusion: ChatEHR将LLM使用重新定义为机构能力，通过"内部构建"策略使卫生系统能够保持自主性，采用供应商无关、内部治理的LLM平台。需要新的监控评估方法和价值评估框架来量化LLM使用的影响。

Abstract: While large language models (LLMs) can support clinical documentation needs, standalone tools struggle with "workflow friction" from manual data entry. We developed ChatEHR, a system that enables the use of LLMs with the entire patient timeline spanning several years. ChatEHR enables automations - which are static combinations of prompts and data that perform a fixed task - and interactive use in the electronic health record (EHR) via a user interface (UI). The resulting ability to sift through patient medical records for diverse use-cases such as pre-visit chart review, screening for transfer eligibility, monitoring for surgical site infections, and chart abstraction, redefines LLM use as an institutional capability. This system, accessible after user-training, enables continuous monitoring and evaluation of LLM use.
  In 1.5 years, we built 7 automations and 1075 users have trained to become routine users of the UI, engaging in 23,000 sessions in the first 3 months of launch. For automations, being model-agnostic and accessing multiple types of data was essential for matching specific clinical or administrative tasks with the most appropriate LLM. Benchmark-based evaluations proved insufficient for monitoring and evaluation of the UI, requiring new methods to monitor performance. Generation of summaries was the most frequent task in the UI, with an estimated 0.73 hallucinations and 1.60 inaccuracies per generation. The resulting mix of cost savings, time savings, and revenue growth required a value assessment framework to prioritize work as well as quantify the impact of using LLMs. Initial estimates are $6M savings in the first year of use, without quantifying the benefit of the better care offered. Such a "build-from-within" strategy provides an opportunity for health systems to maintain agency via a vendor-agnostic, internally governed LLM platform.

</details>


### [469] [Standards for trustworthy AI in the European Union: technical rationale, structural challenges, and an implementation path](https://arxiv.org/abs/2602.00078)
*Piercosma Bisconti,Marcello Galisai*

Main category: cs.CY

TL;DR: 该白皮书分析了欧盟AI法案下的AI标准化技术基础，探讨了标准化如何支持合规推定机制，提出了针对AI独特挑战的分层标准化方案。


<details>
  <summary>Details</summary>
Motivation: AI标准化面临独特挑战：随机行为、数据依赖性、不成熟的评估实践和生命周期动态性。需要将法律义务转化为可审计的工程实践，实现跨提供商、评估机构和执法机构的可扩展合规评估。

Method: 采用分层方法：水平标准定义流程义务和证据结构，行业特定配置文件指定领域阈值和验收标准。基于风险管理、可重复技术检查（重新定义为测量属性的稳定性）、结构化文档、全面日志记录和随系统生命周期演变的保证案例。

Result: 提出了一个可行的标准化方案，能够应对AI的独特挑战，将法律义务转化为可审计的工程实践，支持可扩展的合规评估。

Conclusion: 尽管存在方法论困难，技术标准对于将法律义务转化为可审计的工程实践，以及实现跨提供商、评估机构和执法机构的可扩展合规评估仍然至关重要。

Abstract: This white paper examines the technical foundations of European AI standardization under the AI Act. It explains how harmonized standards enable the presumption of conformity mechanism, describes the CEN/CENELEC standardization process, and analyzes why AI poses unique standardization challenges including stochastic behavior, data dependencies, immature evaluation practices, and lifecycle dynamics. The paper argues that AI systems are typically components within larger sociotechnical systems, requiring a layered approach where horizontal standards define process obligations and evidence structures while sectoral profiles specify domain-specific thresholds and acceptance criteria. It proposes a workable scheme based on risk management, reproducible technical checks redefined as stability of measured properties, structured documentation, comprehensive logging, and assurance cases that evolve over the system lifecycle. The paper demonstrates that despite methodological difficulties, technical standards remain essential for translating legal obligations into auditable engineering practice and enabling scalable conformity assessment across providers, assessors, and enforcement authorities

</details>


### [470] [Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning](https://arxiv.org/abs/2602.01528)
*Qian Wang,Xuandong Zhao,Zirui Zhang,Zhanzhi Lou,Nuo Chen,Dawn Song,Bingsheng He*

Main category: cs.CY

TL;DR: 提出Epistemic Independence Training (EIT)强化学习框架，通过使偏见线索与奖励不相关来训练大语言模型获得认知独立性，从而减少认知偏见的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型作为自动评判者时容易受到认知偏见的影响，如从众效应或权威诉求等提示级线索会改变其推理。现有的提示工程或监督微调方法无法泛化，因为它们只改变表面行为而没有改变使偏见线索具有预测性的优化目标。

Method: 提出Epistemic Independence Training (EIT)强化学习框架，基于一个核心原则：要使模型学会独立性，必须使偏见线索与奖励不相关。通过平衡冲突策略实现，使偏见信号同等可能支持正确和错误答案，同时设计奖励机制惩罚跟随偏见但不奖励偏见一致的行为。

Result: 在Qwen3-4B上的实验表明，EIT提高了在对抗性偏见下的准确性和鲁棒性，同时在偏见与真相一致时保持性能。值得注意的是，仅在从众偏见上训练的模型能够泛化到未见过的偏见类型（如权威和分心偏见），表明EIT诱导了可转移的认知独立性而非偏见特定的启发式方法。

Conclusion: EIT框架通过使偏见线索与奖励不相关，成功训练大语言模型获得认知独立性，有效减少认知偏见的影响，并展现出良好的泛化能力，为解决大语言模型中的认知偏见问题提供了新思路。

Abstract: Large language models (LLMs) increasingly serve as automated judges, yet they remain susceptible to cognitive biases -- often altering their reasoning when faced with spurious prompt-level cues such as consensus claims or authority appeals. Existing mitigations via prompting or supervised fine-tuning fail to generalize, as they modify surface behavior without changing the optimization objective that makes bias cues predictive. To address this gap, we propose Epistemic Independence Training (EIT), a reinforcement learning framework grounded in a key principle: to learn independence, bias cues must be made non-predictive of reward. EIT operationalizes this through a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers, combined with a reward design that penalizes bias-following without rewarding bias agreement. Experiments on Qwen3-4B demonstrate that EIT improves both accuracy and robustness under adversarial biases, while preserving performance when bias aligns with truth. Notably, models trained only on bandwagon bias generalize to unseen bias types such as authority and distraction, indicating that EIT induces transferable epistemic independence rather than bias-specific heuristics. Code and data are available at https://anonymous.4open.science/r/bias-mitigation-with-rl-BC47.

</details>
