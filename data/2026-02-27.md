<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 5]
- [cs.LG](#cs.LG) [Total: 29]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.AI](#cs.AI) [Total: 18]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [EmpiRE-Compass: A Neuro-Symbolic Dashboard for Sustainable and Dynamic Knowledge Exploration, Synthesis, and Reuse](https://arxiv.org/abs/2602.22276)
*Oliver Karras,Amirreza Alasti,Lena John,Sushant Aggarwal,Yücel Celik*

Main category: cs.SE

TL;DR: EmpiRE-Compass是一个神经符号仪表板，旨在降低文献综述数据的访问、复制和重用门槛，通过研究知识图谱和LLM技术促进可持续的文献综述。


<details>
  <summary>Details</summary>
Motivation: 软件工程和需求工程领域文献综述数量激增，但生成式AI快速生成的综述往往牺牲质量、严谨性和透明度，同时现有研究缺乏数据共享，限制了复制和重用。

Method: 开发EmpiRE-Compass神经符号仪表板，采用模块化系统设计，基于两个需求工程用例，将文献综述数据语义结构化到研究知识图谱中，并利用LLM实现动态访问。

Result: EmpiRE-Compass提供三个核心功能：1) 精选能力问题的探索性可视化分析；2) 自定义能力问题的神经符号合成；3) 所有查询、分析和结果的开放可用性。系统已在线免费提供，附带演示视频和开源代码。

Conclusion: 通过统一研究知识图谱和LLM，EmpiRE-Compass推动了RE、SE及其他领域可持续文献综述的发展，降低了技术门槛，促进了透明度和可重复性，实现了协作、持续更新和可重用的文献综述。

Abstract: Software engineering (SE) and requirements engineering (RE) face a significant increase in secondary studies, particularly literature reviews (LRs), due to the ever-growing number of scientific publications. Generative artificial intelligence (GenAI) exacerbates this trend by producing LRs rapidly but often at the expense of quality, rigor, and transparency. At the same time, secondary studies often fail to share underlying data and artifacts, limiting replication and reuse. This paper introduces EmpiRE-Compass, a neuro-symbolic dashboard designed to lower barriers for accessing, replicating, and reusing LR data. Its overarching goal is to demonstrate how LRs can become more sustainable by semantically structuring their underlying data in research knowledge graphs (RKGs) and by leveraging large language models (LLMs) for easy and dynamic access, replication, and reuse. Building on two RE use cases, we developed EmpiRE-Compass with a modular system design and workflows for curated and custom competency questions. The dashboard is freely available online, accompanied by a demonstration video. To manage operational costs, a limit of 25 requests per IP address per day applies to the default LLM (GPT-4o mini). All source code and documentation are released as an open-source project to foster reuse, adoption, and extension. EmpiRE-Compass provides three core capabilities: (1) Exploratory visual analytics for curated competency questions; (2) Neuro-symbolic synthesis for custom competency questions; and (3) Reusable knowledge with all queries, analyses, and results openly available. By unifying RKGs and LLMs in a neuro-symbolic dashboard, EmpiRE-Compass advances sustainable LRs in RE, SE, and beyond. It lowers technical barriers, fosters transparency and reproducibility, and enables collaborative, continuously updated, and reusable LRs

</details>


### [2] [The Ethos of the PEERfect REVIEWer: Scientific Care and Collegial Welfare](https://arxiv.org/abs/2602.22292)
*Oliver Karras*

Main category: cs.SE

TL;DR: 本文提出"完美审稿人"理念，强调同行评审应兼顾科学严谨与人文关怀，包含16条实践建议，旨在提升评审质量与学术共同体福祉。


<details>
  <summary>Details</summary>
Motivation: 当前同行评审过于注重科学严谨性，缺乏对评审过程中所有参与者（作者、共同审稿人、会议组织者、期刊编辑）的人文关怀和同理心，影响了学术共同体的共同进步和福祉。

Method: 基于作者十年学术经验，结合与同事的专业交流、文献回顾、以及对自己撰写和收到的评审意见的分析，提出了"完美审稿人"理念及其16条实践指南。

Result: 提出了以科学关怀和同事情谊为核心价值的"完美审稿人"理念框架，并制定了具体的实践指南，帮助审稿人平衡科学严谨性与人文关怀。

Conclusion: 科学严谨与同理心是互补的力量，同行评审应同时注重科学质量和学术共同体的福祉，审稿人不仅是质量把关者，更是学术旅程中的合作伙伴。

Abstract: Peer review remains a cornerstone in academia, yet it frequently falls short in fostering joint progress and well-being. While peer review primarily emphasizes scientific rigor, it often lacks the empathy essential for supporting and encouraging all peers involved. In this experience report, I aim to highlight that peer review is a practice that demands both scientific care for quality and collegial welfare for the joint progress and well-being of all peers involved, including authors, co-reviewers, workshop or conference organizers, and journal editors. Drawing on my ten years of experience in academia, I propose the ethos of the PEERfect REVIEWer, grounded in the two core values: Scientific care and collegial welfare. Through reflection shaped by professional exchanges with colleagues, consideration of literature, and an examination of both self-authored and received reviews, I formulated an accompanying guideline with 16 practical recommendations to guide reviewers in their actions to achieve these two values. The ethos of the PEERfect REVIEWer and its accompanying guideline help reviewers in upholding high scientific standards and conducting peer review in a constructive, supportive, respectful, and timely manner. They demonstrate that scientific rigor and empathy are complementary forces that promote impactful peer review practice. By placing scientific care and collegial welfare at the core of peer review, this experience report reaffirms the importance of scientific rigor while also advocating for greater attention to empathy. It invites reviewers to reconsider their role not merely as gatekeepers but as partners in the academic journey of each peer involved. The PEERfect REVIEWer is both a caretaker of quality and a steward of joint progress and well-being - as truly impactful peer review practice requires scientific rigor and empathy in equal measure.

</details>


### [3] [Contextual Memory Virtualisation: DAG-Based State Management and Structurally Lossless Trimming for LLM Agents](https://arxiv.org/abs/2602.22402)
*Cosmo Santoni*

Main category: cs.SE

TL;DR: 提出上下文记忆虚拟化系统，将LLM推理过程中积累的状态视为版本控制状态，通过有向无环图管理会话历史，实现无损压缩和跨会话上下文重用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在进行扩展推理任务时会积累大量状态信息，但当会话达到上下文限制时，这些理解会因有损压缩而丢失，需要一种方法来保留和重用这些积累的知识。

Method: 提出上下文记忆虚拟化系统，借鉴操作系统虚拟内存概念，将会话历史建模为有向无环图，定义快照、分支和修剪原语。引入三遍结构无损修剪算法，保留所有用户消息和助手响应，同时去除机械性冗余内容。

Result: 平均减少20%的token数量，最高可达86%的压缩率。在76个真实世界编码会话的案例研究中，混合工具使用会话平均减少39%，在10轮对话内达到盈亏平衡点。

Conclusion: 上下文记忆虚拟化系统能够有效管理LLM会话状态，实现无损压缩和跨会话上下文重用，特别适用于工具使用场景，具有实际应用价值。

Abstract: As large language models engage in extended reasoning tasks, they accumulate significant state -- architectural mappings, trade-off decisions, codebase conventions -- within the context window. This understanding is lost when sessions reach context limits and undergo lossy compaction. We propose Contextual Memory Virtualisation (CMV), a system that treats accumulated LLM understanding as version-controlled state. Borrowing from operating system virtual memory, CMV models session history as a Directed Acyclic Graph (DAG) with formally defined snapshot, branch, and trim primitives that enable context reuse across independent parallel sessions. We introduce a three-pass structurally lossless trimming algorithm that preserves every user message and assistant response verbatim while reducing token counts by a mean of 20% and up to 86% for sessions with significant overhead by stripping mechanical bloat such as raw tool outputs, base64 images, and metadata. A single-user case-study evaluation across 76 real-world coding sessions demonstrates that trimming remains economically viable under prompt caching, with the strongest gains in mixed tool-use sessions, which average 39% reduction and reach break-even within 10 turns. A reference implementation is available at https://github.com/CosmoNaught/claude-code-cmv.

</details>


### [4] [XMENTOR: A Rank-Aware Aggregation Approach for Human-Centered Explainable AI in Just-in-Time Software Defect Prediction](https://arxiv.org/abs/2602.22403)
*Saumendu Roy,Banani Roy,Chanchal Roy,Richard Bassey*

Main category: cs.SE

TL;DR: XMENTOR是一个VS Code插件，通过聚合多个XAI解释方法（如LIME、SHAP、BreakDown）来解决冲突解释问题，提供统一视图，提高开发者对机器学习缺陷预测模型的信任和使用体验。


<details>
  <summary>Details</summary>
Motivation: 机器学习缺陷预测模型虽然能提高软件质量，但其不透明的推理过程使开发者难以信任。现有的可解释AI方法（如LIME、SHAP、BreakDown）在同时使用时经常产生冲突的解释，增加了开发者的困惑、沮丧和认知负担。

Method: 提出XMENTOR方法，这是一个以人为中心、考虑排序的聚合方法，实现为VS Code插件。它通过自适应阈值、排序和符号一致性以及回退策略，将多个后验解释统一为单一、连贯的视图，保持清晰度而不让用户感到信息过载。

Result: 在用户研究中，近90%的参与者更喜欢聚合后的解释，认为这减少了困惑，并更好地支持日常的调试和缺陷审查任务。研究结果表明，将解释方法组合并嵌入到开发者工作流程中可以增强可解释性、可用性和信任度。

Conclusion: XMENTOR通过聚合多个XAI解释方法，解决了冲突解释带来的可用性问题，显著提高了开发者对机器学习缺陷预测模型的理解和信任，为将可解释AI有效集成到软件开发工作流程中提供了实用解决方案。

Abstract: Machine learning (ML)-based defect prediction models can improve software quality. However, their opaque reasoning creates an HCI challenge because developers struggle to trust models they cannot interpret. Explainable AI (XAI) methods such as LIME, SHAP, and BreakDown aim to provide transparency, but when used together, they often produce conflicting explanations that increase confusion, frustration, and cognitive load. To address this usability challenge, we introduce XMENTOR, a human-centered, rank-aware aggregation method implemented as a VS Code plugin. XMENTOR unifies multiple post-hoc explanations into a single, coherent view by applying adaptive thresholding, rank and sign agreement, and fallback strategies to preserve clarity without overwhelming users. In a user study, nearly 90% of the participants preferred aggregated explanations, citing reduced confusion and stronger support for daily tasks of debugging and review of defects. Our findings show how combining explanations and embedding them into developer workflows can enhance interpretability, usability, and trust.

</details>


### [5] [Utilizing LLMs for Industrial Process Automation](https://arxiv.org/abs/2602.23331)
*Salim Fares*

Main category: cs.SE

TL;DR: 该研究探索将大语言模型应用于工业过程自动化领域，解决专业编程语言环境下的实际编程任务，以加速制造系统开发周期。


<details>
  <summary>Details</summary>
Motivation: 目前大多数关于LLM在软件工程中应用的研究集中在Python等通用编程语言上，而工业过程自动化领域使用高度专业化的专有语言，这方面的应用探索不足。研究旨在填补这一空白，将LLM整合到工业开发流程中。

Method: 研究利用和整合大语言模型来解决工业自动化领域的实际编程任务，例如生成机器人手臂的运动例程，以加速制造系统的开发周期。

Result: 摘要中未提供具体实验结果，但研究目标是展示LLM在工业过程自动化领域的应用潜力，特别是在专业编程语言环境下的有效性。

Conclusion: 该研究旨在证明大语言模型可以成功应用于工业过程自动化领域，解决专业编程任务，从而加速制造系统的开发流程，填补了LLM在专有工业编程语言应用方面的研究空白。

Abstract: A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [To Deceive is to Teach? Forging Perceptual Robustness via Adversarial Reinforcement Learning](https://arxiv.org/abs/2602.22227)
*Yicheng Bao,Xuhong Wang,Xin Tan*

Main category: cs.LG

TL;DR: AOT-SFT是一个用于提升多模态大语言模型感知鲁棒性的大规模对抗数据集，AOT框架通过攻击者与防御者的自我博弈机制，让模型自己生成训练数据来增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在面对视觉复杂场景时表现出感知脆弱性，这源于对有限训练数据的依赖，而扩大数据集成本高昂且存在性能上限，需要新的方法来提升模型鲁棒性。

Method: 提出AOT-SFT对抗数据集和AOT自我博弈框架，其中图像编辑攻击者生成多样化的图像操作，迫使防御者MLLM适应和改进，形成协同进化机制。

Result: 实验表明AOT能显著提升防御者的感知鲁棒性并减少幻觉现象，为训练更可靠的MLLM建立了可扩展的范式。

Conclusion: AOT框架通过自我博弈机制为多模态大语言模型的鲁棒性训练提供了有效且可扩展的解决方案，能够克服有限训练数据的限制。

Abstract: Despite their impressive capabilities, Multimodal Large Language Models (MLLMs) exhibit perceptual fragility when confronted with visually complex scenes. This weakness stems from a reliance on finite training datasets, which are prohibitively expensive to scale and impose a ceiling on model robustness. We introduce \textbf{AOT-SFT}, a large-scale adversarial dataset for bootstrapping MLLM robustness. Building on this, we propose \textbf{AOT (Adversarial Opponent Training)}, a self-play framework that forges MLLM robustness by creating its own training data. Our method orchestrates a co-evolution between an image-editing Attacker and a Defender MLLM, where the Attacker generates a diverse and dynamic curriculum of image manipulations, forcing the Defender to adapt and improve. Extensive experiments demonstrate that AOT enhances the Defender's perceptual robustness and reduces hallucinations, establishing a scalable paradigm for training more reliable MLLMs.

</details>


### [7] [Learning geometry-dependent lead-field operators for forward ECG modeling](https://arxiv.org/abs/2602.22367)
*Arsenii Dokuchaev,Francesca Bonizzoni,Stefano Pagani,Francesco Regazzoni,Simone Pezzuto*

Main category: cs.LG

TL;DR: 提出了一种基于形状信息的替代模型，用于高效计算心电图前向模拟中的导联场算子，解决了传统方法在解剖精度、数据需求和计算效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统心电图前向计算模型面临三个主要挑战：1）临床实践中难以获得完整躯干解剖结构；2）导联场方法计算成本随电极数量线性增长；3）现有方法无法同时实现高解剖保真度、低数据需求和计算效率。

Method: 提出一个两组件框架：1）几何编码模块将解剖形状映射到低维潜在空间；2）几何条件神经替代模型根据空间坐标、电极位置和潜在编码预测导联场梯度。该模型可作为完整模型的直接替代品。

Result: 在躯干内部导联场近似中平均角度误差为5°，心脏内部也表现良好，心电图模拟相对均方误差小于2.5%。模型性能优于广泛使用的伪导联场近似，同时保持可忽略的推理成本。

Conclusion: 该方法通过紧凑的潜在表示，无需完整的躯干分割，可在数据有限的环境中部署，同时保持高保真的心电图模拟，解决了传统方法在解剖精度、数据需求和计算效率之间的权衡问题。

Abstract: Modern forward electrocardiogram (ECG) computational models rely on an accurate representation of the torso domain. The lead-field method enables fast ECG simulations while preserving full geometric fidelity. Achieving high anatomical accuracy in torso representation is, however, challenging in clinical practice, as imaging protocols are typically focused on the heart and often do not include the entire torso. In addition, the computational cost of the lead-field method scales linearly with the number of electrodes, limiting its applicability in high-density recording settings. To date, no existing approach simultaneously achieves high anatomical fidelity, low data requirements and computational efficiency. In this work, we propose a shape-informed surrogate model of the lead-field operator that serves as a drop-in replacement for the full-order model in forward ECG simulations. The proposed framework consists of two components: a geometry-encoding module that maps anatomical shapes into a low-dimensional latent space, and a geometry-conditioned neural surrogate that predicts lead-field gradients from spatial coordinates, electrode positions and latent codes. The proposed method achieves high accuracy in approximating lead fields both within the torso (mean angular error 5°) and inside the heart, resulting in highly accurate ECG simulations (relative mean squared error <2.5%. The surrogate consistently outperforms the widely used pseudo lead-field approximation while preserving negligible inference cost. Owing to its compact latent representation, the method does not require a fully detailed torso segmentation and can therefore be deployed in data-limited settings while preserving high-fidelity ECG simulations.

</details>


### [8] [Zatom-1: A Multimodal Flow Foundation Model for 3D Molecules and Materials](https://arxiv.org/abs/2602.22251)
*Alex Morehead,Miruna Cretu,Antonia Panescu,Rishabh Anand,Maurice Weiler,Tynan Perez,Samuel Blau,Steven Farrell,Wahid Bhimji,Anubhav Jain,Hrushikesh Sahasrabuddhe,Pietro Lio,Tommi Jaakkola,Rafael Gomez-Bombarelli,Rex Ying,N. Benjamin Erichson,Michael W. Mahoney*

Main category: cs.LG

TL;DR: Zatom-1是首个统一3D分子和材料生成与预测的基础模型，通过多模态流匹配训练，在保持快速稳定采样的同时，实现了跨化学领域的正向预测迁移。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法大多针对单一领域（分子或材料）和单一任务（生成或预测），限制了表示共享和迁移学习。需要开发一个统一的基础模型来处理3D化学建模的生成和预测任务。

Method: Zatom-1采用Transformer架构，通过多模态流匹配目标联合建模离散原子类型和连续3D几何结构。使用联合生成预训练作为下游多任务预测的通用初始化，支持性质、能量和力的预测。

Result: Zatom-1在生成和预测基准测试中匹配或超越专业基线模型，同时将生成推理时间减少一个数量级以上。实验显示联合生成预训练实现了化学领域间的正向预测迁移：预训练中建模材料提高了分子性质预测的准确性。

Conclusion: Zatom-1成功统一了3D分子和材料的生成与预测学习，证明了联合生成预训练作为通用初始化的有效性，并为跨化学领域的表示共享和迁移学习提供了新途径。

Abstract: General-purpose 3D chemical modeling encompasses molecules and materials, requiring both generative and predictive capabilities. However, most existing AI approaches are optimized for a single domain (molecules or materials) and a single task (generation or prediction), which limits representation sharing and transfer. We introduce Zatom-1, the first foundation model that unifies generative and predictive learning of 3D molecules and materials. Zatom-1 is a Transformer trained with a multimodal flow matching objective that jointly models discrete atom types and continuous 3D geometries. This approach supports scalable pretraining with predictable gains as model capacity increases, while enabling fast and stable sampling. We use joint generative pretraining as a universal initialization for downstream multi-task prediction of properties, energies, and forces. Empirically, Zatom-1 matches or outperforms specialized baselines on both generative and predictive benchmarks, while reducing the generative inference time by more than an order of magnitude. Our experiments demonstrate positive predictive transfer between chemical domains from joint generative pretraining: modeling materials during pretraining improves molecular property prediction accuracy.

</details>


### [9] [Deep Sequence Modeling with Quantum Dynamics: Language as a Wave Function](https://arxiv.org/abs/2602.22255)
*Ahmed Nebli,Hadi Saadatdoorabi,Kevin Yam*

Main category: cs.LG

TL;DR: 提出一种基于量子力学原理的序列建模框架，使用复值波函数作为隐状态，在有限维希尔伯特空间中通过学习的时变哈密顿量演化，利用量子干涉而非门控机制处理序列信息。


<details>
  <summary>Details</summary>
Motivation: 传统循环架构依赖门控机制抑制竞争假设，本文提出利用量子干涉原理，通过哈密顿量调控复振幅相位，让冲突解释相互抵消、兼容解释相互增强，提供更高效的序列建模方法。

Method: 使用复值波函数作为隐状态，在有限维希尔伯特空间中通过学习的时变哈密顿量进行演化，演化过程严格保持幺正性（通过Cayley离散化确保状态范数守恒），采用玻恩规则提取标记概率。

Result: 理论贡献包括分离定理：证明复值幺正模型在维度N下能精确解决的任务，任何配备标准仿射-softmax读取的实值正交模型需要Ω(N²)维度；玻恩规则通过隐式提升到秩一厄米矩阵空间，访问了线性投影无法访问的成对相位相关性。

Conclusion: 该框架利用量子干涉原理为序列建模提供了新范式，理论分析揭示了复值模型相对于实值模型的二次维度优势，并通过连续性方程和守恒的成对流为信息流追踪提供了内置诊断工具。

Abstract: We introduce a sequence modeling framework in which the latent state is a complex-valued wave function evolving on a finite-dimensional Hilbert space under a learned, time-dependent Hamiltonian. Unlike standard recurrent architectures that rely on gating mechanisms to suppress competing hypotheses, our framework utilizes quantum interference: the Hamiltonian steers the phases of complex amplitudes so that conflicting interpretations cancel while compatible ones reinforce. The dynamics are strictly unitary, ensuring that the state norm is preserved exactly at every time step via a Cayley (Crank--Nicolson) discretization. Token probabilities are extracted using the Born rule, a quadratic measurement operator that couples magnitudes and relative phases. Our primary theoretical contribution is a separation theorem characterizing the representational advantage of this readout: we define a family of disambiguation tasks that a complex unitary model of dimension $N$ solves exactly, but which requires a state dimension of $Ω(N^2)$ for any real-valued orthogonal model equipped with a standard affine-softmax readout. This quadratic gap arises because the Born rule implicitly lifts the $N$-dimensional state into the space of rank-one Hermitian matrices, accessing pairwise phase correlations that are inaccessible to linear projections. Finally, we derive a continuity equation for the latent probability mass, yielding conserved pairwise currents that serve as a built-in diagnostic for tracing information flow between dimensions.

</details>


### [10] [Code World Models for Parameter Control in Evolutionary Algorithms](https://arxiv.org/abs/2602.22260)
*Camilo Chacón Sartori,Guillem Rodríguez Corominas*

Main category: cs.LG

TL;DR: LLM通过合成代码世界模型来学习和控制优化器行为，在组合优化问题上超越传统自适应方法和深度强化学习


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能够学习优化器的动态行为，并利用这种知识来控制优化过程，特别是在随机组合优化环境中

Method: 基于次优轨迹，LLM合成优化器动态的Python模拟器，然后通过贪婪规划选择每一步的变异强度k

Result: 在LO和OneMax问题上达到理论最优策略的94%；在Jump_k问题上实现100%成功率（基线为0%）；在NK-Landscape上超越所有基线；比DQN样本效率更高

Conclusion: 代码世界模型能够有效学习优化器动态并实现控制，在多种组合优化问题上表现出色，具有高样本效率和泛化能力

Abstract: Can an LLM learn how an optimizer behaves -- and use that knowledge to control it? We extend Code World Models (CWMs), LLM-synthesized Python programs that predict environment dynamics, from deterministic games to stochastic combinatorial optimization. Given suboptimal trajectories of $(1{+}1)$-$\text{RLS}_k$, the LLM synthesizes a simulator of the optimizer's dynamics; greedy planning over this simulator then selects the mutation strength $k$ at each step. On \lo{} and \onemax{}, CWM-greedy performs within 6\% of the theoretically optimal policy -- without ever seeing optimal-policy trajectories. On \jump{$_k$}, where a deceptive valley causes all adaptive baselines to fail (0\% success rate), CWM-greedy achieves 100\% success rate -- without any collection policy using oracle knowledge of the gap parameter. On the NK-Landscape, where no closed-form model exists, CWM-greedy outperforms all baselines across fifteen independently generated instances ($36.94$ vs.\ $36.32$; $p<0.001$) when the prompt includes empirical transition statistics. The CWM also outperforms DQN in sample efficiency (200 offline trajectories vs.\ 500 online episodes), success rate (100\% vs.\ 58\%), and generalization ($k{=}3$: 78\% vs.\ 0\%). Robustness experiments confirm stable synthesis across 5 independent runs.

</details>


### [11] [Sustainable LLM Inference using Context-Aware Model Switching](https://arxiv.org/abs/2602.22261)
*Yuvarani,Akashdeep Singh,Zahra Fathanah,Salsabila Harlen,Syeikha Syafura Al-Zahra binti Zahari,Hema Subramaniam*

Main category: cs.LG

TL;DR: 提出一种上下文感知的模型切换方法，根据查询复杂度动态选择合适的大语言模型，相比始终使用最大模型可减少67.5%能耗，同时保持93.6%的响应质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型能耗日益增长引发可持续性担忧，当前AI部署依赖"一刀切"推理策略，无论任务复杂度都使用相同大模型，导致大量不必要的能源浪费。

Method: 提出上下文感知模型切换方法，结合重复查询缓存、基于规则的复杂度评分、机器学习分类捕获语义意图，以及从交互模式中学习的用户自适应组件。

Result: 使用真实对话工作负载和三个开源模型评估，模型切换方法相比始终使用最大模型可减少67.5%能耗，简单查询响应时间提升约68%，同时保持93.6%的响应质量。

Conclusion: 模型切换推理为实现更节能、可持续的AI系统提供了实用且可扩展的路径，证明无需牺牲响应质量即可实现显著的效率提升。

Abstract: Large language models have become central to many AI applications, but their growing energy consumption raises serious sustainability concerns. A key limitation in current AI deployments is the reliance on a one-size-fits-all inference strategy where most systems route every request to the same large model, regardless of task complexity, leading to substantial and unnecessary energy waste. To address this issue, we propose a context-aware model switching approach that dynamically selects an appropriate language model based on query complexity. The proposed system uses a Context-Aware Model Switching for Energy-Efficient LLM Inference that combines caching for repeated queries, rulebased complexity scoring for fast and explainable decisions, machine learning classification to capture semantic intent, and a user-adaptive component that learns from interaction patterns over time. The proposed architecture was evaluated using real conversation workloads and three open-source language models (Gemma3 1B, Gemma3 4B and Qwen3 4B) with different computational costs, measuring energy consumption (via NVML GPU power telemetry), response latency, routing accuracy, and output quality (BERTScore F1) to reflect real-world usage conditions. Experimental results show that the model switching approach can reduce energy consumption by up to 67.5% compared to always using the largest model while maintaining a response quality of 93.6%. In addition, the response time for simple queries also improved significantly by approximately 68%. These results show that model switching inference offers a practical and scalable path toward more energy-efficient and sustainable AI systems, demonstrating that significant efficiency gains can be achieved without major sacrifices in response quality.

</details>


### [12] [Entropy-Controlled Flow Matching](https://arxiv.org/abs/2602.22265)
*Chika Maduabuchi*

Main category: cs.LG

TL;DR: 提出熵控制流匹配(ECFM)方法，通过约束变分原理强制轨迹的熵率预算，解决传统流匹配中低熵瓶颈导致语义模式丢失的问题


<details>
  <summary>Details</summary>
Motivation: 传统流匹配方法虽然经验性能强，但其目标函数不直接控制轨迹的信息几何，允许低熵瓶颈存在，这些瓶颈会暂时耗尽语义模式，影响生成质量

Method: 提出熵控制流匹配(ECFM)：基于连续性方程路径的约束变分原理，强制全局熵率预算d/dt H(mu_t) >= -lambda。这是Wasserstein空间中的凸优化问题，具有KKT/Pontryagin系统，并等价于具有显式熵乘子的Schrödinger桥

Result: 在纯传输机制下，ECFM恢复熵最优传输测地线，并在lambda->0时Gamma收敛到经典最优传输。获得具有Lipschitz稳定性的模式覆盖和密度下限保证，并构建了无约束流匹配的近似最优崩溃反例

Conclusion: ECFM通过熵率约束有效控制生成轨迹的信息几何，避免模式崩溃，提供理论保证，是传统流匹配方法的改进

Abstract: Modern vision generators transport a base distribution to data through time-indexed measures, implemented as deterministic flows (ODEs) or stochastic diffusions (SDEs). Despite strong empirical performance, standard flow-matching objectives do not directly control the information geometry of the trajectory, allowing low-entropy bottlenecks that can transiently deplete semantic modes. We propose Entropy-Controlled Flow Matching (ECFM): a constrained variational principle over continuity-equation paths enforcing a global entropy-rate budget d/dt H(mu_t) >= -lambda. ECFM is a convex optimization in Wasserstein space with a KKT/Pontryagin system, and admits a stochastic-control representation equivalent to a Schrodinger bridge with an explicit entropy multiplier. In the pure transport regime, ECFM recovers entropic OT geodesics and Gamma-converges to classical OT as lambda -> 0. We further obtain certificate-style mode-coverage and density-floor guarantees with Lipschitz stability, and construct near-optimal collapse counterexamples for unconstrained flow matching.

</details>


### [13] [Data-Driven Supervision of a Thermal-Hydraulic Process Towards a Physics-Based Digital Twin](https://arxiv.org/abs/2602.22267)
*Osimone Imhogiemhe,Yoann Jus,Hubert Lejeune,Saïd Moussaoui*

Main category: cs.LG

TL;DR: 本文开发了一种用于热工水力过程故障检测与诊断的数字孪生系统，结合数值模拟和机器学习方法，实现了过程参数变化的在线检测和估计。


<details>
  <summary>Details</summary>
Motivation: 工业生产过程的实时监控是多个行业面临的共同挑战，需要确保安全、连续生产和保持高效率。数字孪生概念为解决这些挑战提供了合适的框架。

Method: 基于系统数值模拟和机器学习方法，开发了专门用于热工水力过程监控的数字孪生系统，包含过程参数变化检测和在线估计的不同模块。

Result: 在特定测试场景中验证了故障检测与诊断算法，针对系统中单次参数变化情况，数值结果显示在参数变化定位和值更新方面具有良好的准确性。

Conclusion: 提出的数字孪生系统能够有效实现热工水力过程的故障检测与诊断，为工业生产过程的实时监控提供了可行的解决方案。

Abstract: The real-time supervision of production processes is a common challenge across several industries. It targets process component monitoring and its predictive maintenance in order to ensure safety, uninterrupted production and maintain high efficiency level. The rise of advanced tools for the simulation of physical systems in addition to data-driven machine learning models offers the possibility to design numerical tools dedicated to efficient system monitoring. In that respect, the digital twin concept presents an adequate framework that proffers solution to these challenges. The main purpose of this paper is to develop such a digital twin dedicated to fault detection and diagnosis in the context of a thermal-hydraulic process supervision. Based on a numerical simulation of the system, in addition to machine learning methods, we propose different modules dedicated to process parameter change detection and their on-line estimation. The proposed fault detection and diagnosis algorithm is validated on a specific test scenario, with single one-off parameter change occurrences in the system. The numerical results show good accuracy in terms of parameter variation localization and the update of their values.

</details>


### [14] [AutoQRA: Joint Optimization of Mixed-Precision Quantization and Low-rank Adapters for Efficient LLM Fine-Tuning](https://arxiv.org/abs/2602.22268)
*Changhai Zhou,Shiyang Zhang,Yuhua Zhou,Qian Qiao,Jun Gao,Cheng Jin,Kaizhou Qin,Weizhong Zhang*

Main category: cs.LG

TL;DR: AutoQRA提出了一种联合优化量化位宽和LoRA秩的框架，通过两阶段搜索在内存约束下实现接近全精度微调的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的量化后参数高效微调方法忽略了量化位宽与LoRA秩之间的复杂交互关系。精心优化的量化分配不一定能转化为良好的微调性能，不同位宽和秩配置在相同内存预算下会产生显著差异的结果。

Method: AutoQRA采用两阶段优化：1) 全局多保真度进化搜索，通过注入层重要性先验初始化种群，使用特定算子和性能模型高效筛选候选配置；2) 信任域贝叶斯优化，在搜索空间的有希望区域进行局部细化，在给定内存预算下找到最优配置。

Result: 实验表明，AutoQRA在内存占用与均匀4位方法相当的情况下，实现了接近全精度微调的性能。

Conclusion: AutoQRA通过联合优化量化位宽和LoRA秩，有效解决了量化与微调之间的交互问题，在有限GPU内存约束下实现了高效的模型适应。

Abstract: Quantization followed by parameter-efficient fine-tuning has emerged as a promising paradigm for downstream adaptation under tight GPU memory constraints. However, this sequential pipeline fails to leverage the intricate interaction between quantization bit-width and LoRA rank. Specifically, a carefully optimized quantization allocation with low quantization error does not always translate to strong fine-tuning performance, and different bit-width and rank configurations can lead to significantly varying outcomes under the same memory budget. To address this limitation, we propose AutoQRA, a joint optimization framework that simultaneously optimizes the bit-width and LoRA rank configuration for each layer during the mixed quantized fine-tuning process. To tackle the challenges posed by the large discrete search space and the high evaluation cost associated with frequent fine-tuning iterations, AutoQRA decomposes the optimization process into two stages. First, it first conducts a global multi-fidelity evolutionary search, where the initial population is warm-started by injecting layer-wise importance priors. This stage employs specific operators and a performance model to efficiently screen candidate configurations. Second, trust-region Bayesian optimization is applied to locally refine promising regions of the search space and identify optimal configurations under the given memory budget. This approach enables active compensation for quantization noise in specific layers during training. Experiments show that AutoQRA achieves performance close to full-precision fine-tuning with a memory footprint comparable to uniform 4-bit methods.

</details>


### [15] [CQSA: Byzantine-robust Clustered Quantum Secure Aggregation in Federated Learning](https://arxiv.org/abs/2602.22269)
*Arnab Nath,Harsh Kasyap*

Main category: cs.LG

TL;DR: 提出CQSA框架，通过聚类量子安全聚合解决大规模量子联邦学习中GHZ态保真度下降和拜占庭攻击检测问题


<details>
  <summary>Details</summary>
Motivation: 现有量子安全聚合协议依赖单一全局GHZ态，面临两个根本挑战：1) 随着客户端数量增加，大规模GHZ态保真度急剧下降；2) 全局聚合无法检测拜占庭客户端

Method: 提出聚类量子安全聚合框架：将客户端随机划分为小集群，每个集群使用高保真度、低量子比特的GHZ态进行本地量子聚合；服务器通过余弦相似度和欧氏距离等统计度量分析集群级聚合结果，识别恶意贡献

Result: 通过理论分析和去极化噪声下的仿真表明，CQSA确保模型稳定收敛，相比全局QSA实现更优的态保真度

Conclusion: CQSA框架调和了近量子硬件的物理约束与联邦学习中拜占庭鲁棒性的需求，为量子辅助联邦学习提供了可行的安全聚合解决方案

Abstract: Federated Learning (FL) enables collaborative model training without sharing raw data. However, shared local model updates remain vulnerable to inference and poisoning attacks. Secure aggregation schemes have been proposed to mitigate these attacks. In this work, we aim to understand how these techniques are implemented in quantum-assisted FL. Quantum Secure Aggregation (QSA) has been proposed, offering information-theoretic privacy by encoding client updates into the global phase of multipartite entangled states. Existing QSA protocols, however, rely on a single global Greenberger-Horne-Zeilinger (GHZ) state shared among all participating clients. This design poses fundamental challenges: fidelity of large-scale GHZ states deteriorates rapidly with the increasing number of clients; and (ii) the global aggregation prevents the detection of Byzantine clients. We propose Clustered Quantum Secure Aggregation (CQSA), a modular aggregation framework that reconciles the physical constraints of near-term quantum hardware along with the need for Byzantine-robustness in FL. CQSA randomly partitions the clients into small clusters, each performing local quantum aggregation using high-fidelity, low-qubit GHZ states. The server analyzes statistical relationships between cluster-level aggregates employing common statistical measures such as cosine similarity and Euclidean distance to identify malicious contributions. Through theoretical analysis and simulations under depolarizing noise, we demonstrate that CQSA ensures stable model convergence, achieves superior state fidelity over global QSA.

</details>


### [16] [A 1/R Law for Kurtosis Contrast in Balanced Mixtures](https://arxiv.org/abs/2602.22334)
*Yuda Bi,Wenjun Xiao,Linhao Bai,Vince D Calhoun*

Main category: cs.LG

TL;DR: 论文证明了基于峰度的独立成分分析在宽平衡混合中效果减弱，提出了尖锐的冗余定律：标准化投影的峰度与有效宽度成反比，并展示了通过选择少量符号一致源可以恢复对比度。


<details>
  <summary>Details</summary>
Motivation: 传统基于峰度的ICA方法在处理宽平衡混合时效果显著减弱，需要理解这种性能下降的根本原因并找到解决方案。

Method: 通过理论分析证明标准化投影的峰度与有效宽度之间的反比关系，提出基于参与比的有效宽度概念，并设计数据驱动的启发式方法来选择少量符号一致源进行"净化"。

Result: 证明了峰度随有效宽度衰减的尖锐冗余定律，确定了在有限矩条件下样本峰度估计需要满足的条件，展示了通过选择少量符号一致源可以恢复独立于宽度的对比度。

Conclusion: 基于峰度的ICA在宽平衡混合中存在固有局限性，但通过"净化"方法选择少量符号一致源可以有效恢复对比度，为实际应用提供了理论指导和实用解决方案。

Abstract: Kurtosis-based Independent Component Analysis (ICA) weakens in wide, balanced mixtures. We prove a sharp redundancy law: for a standardized projection with effective width $R_{\mathrm{eff}}$ (participation ratio), the population excess kurtosis obeys $|κ(y)|=O(κ_{\max}/R_{\mathrm{eff}})$, yielding the order-tight $O(c_bκ_{\max}/R)$ under balance (typically $c_b=O(\log R)$). As an impossibility screen, under standard finite-moment conditions for sample kurtosis estimation, surpassing the $O(1/\sqrt{T})$ estimation scale requires $R\lesssim κ_{\max}\sqrt{T}$. We also show that \emph{purification} -- selecting $m\!\ll\!R$ sign-consistent sources -- restores $R$-independent contrast $Ω(1/m)$, with a simple data-driven heuristic. Synthetic experiments validate the predicted decay, the $\sqrt{T}$ crossover, and contrast recovery.

</details>


### [17] [A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection](https://arxiv.org/abs/2602.22412)
*Ruiqi Zhou,Donghao Zhu,Houcai Shen*

Main category: cs.LG

TL;DR: 提出基于学习的混合匹配框架，自适应结合即时匹配和延迟匹配，通过动态调整策略在等待时间和匹配效率之间取得平衡


<details>
  <summary>Details</summary>
Motivation: 在肾脏交换和货运交换等匹配市场中，延迟匹配可以提高整体市场效率，但延迟匹配会带来等待时间增加和市场拥堵等成本。固定匹配策略在动态环境中缺乏灵活性，需要一种自适应的方法来平衡即时匹配和延迟匹配的利弊。

Method: 提出基于学习的混合框架，持续收集用户离开时间数据，通过回归估计潜在的离开分布，基于控制系统对匹配效率损失容忍度的决策阈值，决定在后续期间是否延迟匹配。

Result: 该框架能够显著减少等待时间和拥堵，同时仅牺牲有限的匹配效率。通过动态调整匹配策略，系统性能可以在纯贪婪策略和纯耐心策略之间灵活插值。

Conclusion: 混合框架为静态匹配机制提供了稳健且自适应的替代方案，能够在动态环境中灵活平衡即时匹配和延迟匹配的利弊，优化市场效率。

Abstract: In matching markets such as kidney exchanges and freight exchanges, delayed matching has been shown to improve overall market efficiency. The benefits of delay are highly sensitive to participants' sojourn times and departure behavior, and delaying matches can impose significant costs, including longer waiting times and increased market congestion. These competing effects make fixed matching policies inherently inflexible in dynamic environments. We propose a learning-based Hybrid framework that adaptively combines immediate and delayed matching. The framework continuously collects data on user departures over time, estimates the underlying departure distribution via regression, and determines whether to delay matching in the subsequent period based on a decision threshold that governs the system's tolerance for matching efficiency loss. The proposed framework can substantially reduce waiting times and congestion while sacrificing only a limited amount of matching efficiency. By dynamically adjusting its matching strategy, the Hybrid framework enables system performance to flexibly interpolate between purely greedy and purely patient policies, offering a robust and adaptive alternative to static matching mechanisms.

</details>


### [18] [Calibrated Test-Time Guidance for Bayesian Inference](https://arxiv.org/abs/2602.22428)
*Daniel Geyfman,Felix Draxler,Jan Groeneveld,Hyunsoo Lee,Theofanis Karaletsos,Stephan Mandt*

Main category: cs.LG

TL;DR: 本文指出现有测试时引导方法无法正确恢复贝叶斯后验分布，提出了更一致的替代估计器来实现校准采样。


<details>
  <summary>Details</summary>
Motivation: 现有测试时引导方法主要关注最大化奖励而非从真实贝叶斯后验中采样，导致推理校准不准确，需要解决这一结构性问题。

Method: 分析现有测试时引导方法的结构性近似问题，提出一致的替代估计器，能够在扩散模型中实现从贝叶斯后验的校准采样。

Result: 在贝叶斯推理任务上显著优于先前方法，在黑洞图像重建任务上达到最先进水平。

Conclusion: 通过识别和修正现有测试时引导方法的结构性缺陷，提出了能够实现校准贝叶斯后验采样的新方法，在多个任务上表现出优越性能。

Abstract: Test-time guidance is a widely used mechanism for steering pretrained diffusion models toward outcomes specified by a reward function. Existing approaches, however, focus on maximizing reward rather than sampling from the true Bayesian posterior, leading to miscalibrated inference. In this work, we show that common test-time guidance methods do not recover the correct posterior distribution and identify the structural approximations responsible for this failure. We then propose consistent alternative estimators that enable calibrated sampling from the Bayesian posterior. We significantly outperform previous methods on a set of Bayesian inference tasks, and match state-of-the-art in black hole image reconstruction.

</details>


### [19] [From Bias to Balance: Fairness-Aware Paper Recommendation for Equitable Peer Review](https://arxiv.org/abs/2602.22438)
*Uttamasha Anjally Oyshi,Susan Gauch*

Main category: cs.LG

TL;DR: Fair-PaperRec是一个在双盲评审后使用公平性正则化的MLP模型，通过重新排序论文来减少系统性偏见，提高代表性不足群体的参与度，同时保持论文质量稳定。


<details>
  <summary>Details</summary>
Motivation: 尽管采用双盲评审，作者人口统计学相关的系统性偏见仍然使代表性不足群体处于劣势。研究假设：如果在评审后推荐系统中加入明确的公平性正则化，可以在不降低质量的情况下提高包容性。

Method: 提出Fair-PaperRec，这是一个多层感知机(MLP)模型，具有基于交叉属性（如种族、国家）的可微分公平性损失函数，用于在双盲评审后重新排序论文。先在合成数据集上进行测试，然后在ACM SIGCHI、DIS和IUI会议的真实数据上进行验证。

Result: 在合成数据上，增加公平性权重能增强宏观/微观多样性，同时保持效用稳定。在真实会议数据中，适当调整的Fair-PaperRec配置使代表性不足群体的参与度提高达42.03%，而整体效用变化最多仅3.16%。

Conclusion: 公平性正则化既可以作为公平机制，也可以作为温和的质量正则化器，特别是在高度偏见的体系中。Fair-PaperRec提供了一个实用的、以公平为中心的评审后论文选择框架，能够保持甚至在某些情况下提高学术质量。

Abstract: Despite frequent double-blind review, systemic biases related to author demographics still disadvantage underrepresented groups. We start from a simple hypothesis: if a post-review recommender is trained with an explicit fairness regularizer, it should increase inclusion without degrading quality. To test this, we introduce Fair-PaperRec, a Multi-Layer Perceptron (MLP) with a differentiable fairness loss over intersectional attributes (e.g., race, country) that re-ranks papers after double-blind review. We first probe the hypothesis on synthetic datasets spanning high, moderate, and near-fair biases. Across multiple randomized runs, these controlled studies map where increasing the fairness weight strengthens macro/micro diversity while keeping utility approximately stable, demonstrating robustness and adaptability under varying disparity levels. We then carry the hypothesis into the original setting, conference data from ACM Special Interest Group on Computer-Human Interaction (SIGCHI), Designing Interactive Systems (DIS), and Intelligent User Interfaces (IUI). In this real-world scenario, an appropriately tuned configuration of Fair-PaperRec achieves up to a 42.03% increase in underrepresented-group participation with at most a 3.16% change in overall utility relative to the historical selection. Taken together, the synthetic-to-original progression shows that fairness regularization can act as both an equity mechanism and a mild quality regularizer, especially in highly biased regimes. By first analyzing the behavior of the fairness parameters under controlled conditions and then validating them on real submissions, Fair-PaperRec offers a practical, equity-focused framework for post-review paper selection that preserves, and in some settings can even enhance, measured scholarly quality.

</details>


### [20] [Beyond performance-wise Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2602.22470)
*Balazs Pejo*

Main category: cs.LG

TL;DR: 该论文提出了一种新的联邦学习客户端评估框架，不仅关注模型性能（如准确率），还评估客户端对模型可信度的贡献，包括可靠性、鲁棒性和公平性三个维度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习客户端评估方法主要关注模型性能指标（如准确率、损失），但这只是机器学习模型整体效用的一个维度。当前方法忽略了客户端对模型可信度（可靠性、鲁棒性、公平性）的贡献，而单一指标无法全面评估和公平分配奖励。

Method: 采用最先进的Shapley值近似方法来量化客户端在多维度上的贡献，包括：1）可靠性（对噪声数据的容忍度）；2）鲁棒性（对抗样本的抵抗能力）；3）公平性（通过人口统计学平等性衡量）。

Result: 研究发现：1）没有一个客户端在所有维度上都表现出色；2）不同维度之间基本相互独立；3）当前评估方案存在关键缺陷，即单一指标无法进行全面评估和公平奖励分配。

Conclusion: 联邦学习需要多维度的客户端评估框架，考虑模型可信度的多个方面（可靠性、鲁棒性、公平性），而不仅仅是性能指标。Shapley值方法能够有效量化这些多维度贡献，为更公平的奖励分配提供基础。

Abstract: Federated learning offers a privacy-friendly collaborative learning framework, yet its success, like any joint venture, hinges on the contributions of its participants. Existing client evaluation methods predominantly focus on model performance, such as accuracy or loss, which represents only one dimension of a machine learning model's overall utility. In contrast, this work investigates the critical, yet overlooked, issue of client contributions towards a model's trustworthiness -- specifically, its reliability (tolerance to noisy data), resilience (resistance to adversarial examples), and fairness (measured via demographic parity). To quantify these multifaceted contributions, we employ the state-of-the-art approximation of the Shapley value, a principled method for value attribution. Our results reveal that no single client excels across all dimensions, which are largely independent from each other, highlighting a critical flaw in current evaluation scheme: no single metric is adequate for comprehensive evaluation and equitable rewarding allocation.

</details>


### [21] [Efficient Continual Learning in Language Models via Thalamically Routed Cortical Columns](https://arxiv.org/abs/2602.22479)
*Afshin Khadangi*

Main category: cs.LG

TL;DR: TRC²是一种解码器架构，通过丘脑路由的皮层柱结构解决语言模型持续学习中的灾难性遗忘问题，在保持计算效率的同时改善稳定性-可塑性权衡。


<details>
  <summary>Details</summary>
Motivation: 标准语言模型训练和微调在非平稳数据下表现脆弱，在线更新常导致灾难性遗忘，而现有方法要么增加延迟、内存占用，要么计算密集，难以扩展到长上下文场景。

Method: 提出TRC²架构，结合稀疏丘脑路由的皮层柱结构，包含调制、预测、记忆和反馈机制，以及支持快速适应而不破坏慢速参数的快速校正通路。该模块稀疏且支持分块并行，便于高效训练和推理。

Result: 在语言建模和持续学习基准测试中，TRC²在相同计算量下改善了稳定性-可塑性权衡，支持快速流式适应同时保持先前习得的行为。

Conclusion: TRC²在架构层面解决了持续学习问题，通过稀疏路由和快速校正通路实现了高效训练和推理，为语言模型的持续学习提供了有效的架构解决方案。

Abstract: Continual learning is a core requirement for deployed language models, yet standard training and fine-tuning pipelines remain brittle under non-stationary data. Online updates often induce catastrophic forgetting, while methods that improve stability frequently increase latency, memory footprint, or dense computation in ways that do not scale well to long contexts. We introduce TRC$^{2}$ (Thalamically Routed Cortical Columns), a decoder-only backbone that addresses continual learning at the architectural level. TRC$^{2}$ combines sparse thalamic routing over cortical columns with mechanisms for modulation, prediction, memory, and feedback, together with a fast corrective pathway that supports rapid adaptation without destabilizing slower parameters. The resulting block is sparse and chunk-parallel, enabling efficient training and inference while preserving clean ablations of each subsystem. We instantiate a reproducible training and evaluation stack and a continual-learning harness that measures proxy forgetting under streaming domain shifts. Across language modeling and continual learning benchmarks, TRC$^{2}$ improves the stability-plasticity tradeoff at comparable compute, enabling rapid on-stream adaptation while preserving previously acquired behavior.

</details>


### [22] [DP-aware AdaLN-Zero: Taming Conditioning-Induced Heavy-Tailed Gradients in Differentially Private Diffusion](https://arxiv.org/abs/2602.22610)
*Tao Huang,Jiayang Meng,Xu Yang,Chen Hou,Hong Chen*

Main category: cs.LG

TL;DR: 本文提出DP-aware AdaLN-Zero，一种用于条件扩散变换器的敏感度感知条件机制，通过限制条件表示幅度和调制参数来抑制极端梯度尾部事件，从而改善差分隐私条件下的时间序列生成性能。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私随机梯度下降（DP-SGD）中，异构条件上下文（如观测历史、缺失模式或异常协变量）会导致重尾的每样本梯度。这些罕见但重尾的梯度会过度触发全局裁剪，导致异常值主导的更新、更大的裁剪偏差，以及在固定隐私预算下降低的效用。

Method: 提出DP-aware AdaLN-Zero，这是一种即插即用的敏感度感知条件机制，通过有界重新参数化联合约束条件表示幅度和AdaLN调制参数，在梯度裁剪和噪声注入之前抑制极端梯度尾部事件。

Result: 在匹配隐私设置下，配备DP-aware AdaLN-Zero的DP-SGD在插值/填补和预测任务上表现更好。在真实世界电力数据集和两个公共ETT基准测试中观察到一致的性能提升。梯度诊断表明这些改进归因于条件特定的尾部重塑和减少的裁剪失真，同时在非私有训练中保持表达能力。

Conclusion: 敏感度感知条件机制可以显著改善私有条件扩散训练，而不会牺牲标准性能。这表明在差分隐私框架下，通过设计适当的条件机制来管理梯度特性是提高时间序列生成模型效用的有效途径。

Abstract: Condition injection enables diffusion models to generate context-aware outputs, which is essential for many time-series tasks. However, heterogeneous conditional contexts (e.g., observed history, missingness patterns or outlier covariates) can induce heavy-tailed per-example gradients. Under Differentially Private Stochastic Gradient Descent (DP-SGD), these rare conditioning-driven heavy-tailed gradients disproportionately trigger global clipping, resulting in outlier-dominated updates, larger clipping bias, and degraded utility under a fixed privacy budget. In this paper, we propose DP-aware AdaLN-Zero, a drop-in sensitivity-aware conditioning mechanism for conditional diffusion transformers that limits conditioning-induced gain without modifying the DP-SGD mechanism. DP-aware AdaLN-Zero jointly constrains conditioning representation magnitude and AdaLN modulation parameters via bounded re-parameterization, suppressing extreme gradient tail events before gradient clipping and noise injection. Empirically, DP-SGD equipped with DP-aware AdaLN-Zero improves interpolation/imputation and forecasting under matched privacy settings. We observe consistent gains on a real-world power dataset and two public ETT benchmarks over vanilla DP-SGD. Moreover, gradient diagnostics attribute these improvements to conditioning-specific tail reshaping and reduced clipping distortion, while preserving expressiveness in non-private training. Overall, these results show that sensitivity-aware conditioning can substantially improve private conditional diffusion training without sacrificing standard performance.

</details>


### [23] [Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support](https://arxiv.org/abs/2602.22673)
*Md Tanvir Hasan Turja*

Main category: cs.LG

TL;DR: 本文提出了一个两组件框架，用于抗菌素耐药性趋势预测和政策决策支持，在WHO GLASS数据上评估了六种机器学习模型，XGBoost表现最佳，并实现了基于RAG的政策问答系统。


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性（AMR）是全球性危机，预计到205年每年导致1000万人死亡。虽然WHO GLASS系统提供了44个国家的标准化监测数据，但很少有研究应用机器学习从这些数据中预测人口水平的耐药性趋势。需要开发预测模型和政策决策支持工具来应对这一挑战。

Method: 提出了一个两组件框架：1）AMR趋势预测：在5,909个WHO GLASS观测数据上（2021-2023年，覆盖六个WHO区域）评估了六种模型（朴素模型、线性回归、岭回归、XGBoost、LightGBM、LSTM）；2）政策决策支持：实现了检索增强生成（RAG）管道，结合ChromaDB向量存储的WHO政策文档和本地部署的Phi-3 Mini语言模型，生成有来源引用、幻觉受限的政策答案。

Result: XGBoost模型表现最佳，测试MAE为7.07%，R平方为0.854，比朴素基线提高了83.1%。特征重要性分析显示前一年的耐药率是最重要的预测因子（50.5%重要性）。区域MAE范围从4.16%（欧洲区域）到10.14%（东南亚区域）。RAG系统能够生成基于WHO政策文档的可靠政策建议。

Conclusion: 该研究证明了机器学习方法在预测AMR趋势方面的有效性，XGBoost是最佳模型。结合RAG的政策决策支持系统为公共卫生决策者提供了基于证据的工具。代码和数据已开源，有助于进一步研究和应用。

Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja

</details>


### [24] [Accelerating LLM Pre-Training through Flat-Direction Dynamics Enhancement](https://arxiv.org/abs/2602.22681)
*Shuchen Zhu,Rizhen Hu,Mingze Wang,Mou Sun,Xue Wang,Kun Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: LITE是一种广义加速策略，通过沿平坦方向应用更大的Hessian阻尼系数和学习率来增强训练动态，显著加速Muon和SOAP等优化器在LLM预训练中的收敛。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型需要巨大的计算资源，因此优化器效率至关重要。当前基于矩阵的优化器（如Muon和SOAP）虽然利用细粒度曲率信息优于AdamW，但其更新趋于各向同性——在平坦方向上相对保守，在陡峭方向上可能过于激进，限制了训练效率。

Method: 首先建立统一的黎曼常微分方程框架，阐明自适应算法如何协同工作：预处理器诱导黎曼几何缓解病态条件，动量作为黎曼阻尼项促进收敛。基于这些见解，提出LITE策略，通过沿平坦轨迹应用更大的Hessian阻尼系数和学习率来增强训练动态。

Result: 大量实验表明，LITE显著加速了Muon和SOAP在不同架构（密集、MoE）、参数规模（130M-1.3B）、数据集（C4、Pile）和学习率调度（余弦、warmup-stable-decay）下的训练。理论分析证实LITE在各向异性景观中沿平坦方向促进更快收敛。

Conclusion: LITE为高效LLM预训练提供了原则性方法，通过针对性增强平坦方向的训练动态，解决了现有矩阵优化器的各向同性限制，显著提升训练效率。

Abstract: Pre-training Large Language Models requires immense computational resources, making optimizer efficiency essential. The optimization landscape is highly anisotropic, with loss reduction driven predominantly by progress along flat directions. While matrix-based optimizers such as Muon and SOAP leverage fine-grained curvature information to outperform AdamW, their updates tend toward isotropy -- relatively conservative along flat directions yet potentially aggressive along sharp ones. To address this limitation, we first establish a unified Riemannian Ordinary Differential Equation (ODE) framework that elucidates how common adaptive algorithms operate synergistically: the preconditioner induces a Riemannian geometry that mitigates ill-conditioning, while momentum serves as a Riemannian damping term that promotes convergence. Guided by these insights, we propose LITE, a generalized acceleration strategy that enhances training dynamics by applying larger Hessian damping coefficients and learning rates along flat trajectories. Extensive experiments demonstrate that LITE significantly accelerates both Muon and SOAP across diverse architectures (Dense, MoE), parameter scales (130M--1.3B), datasets (C4, Pile), and learning-rate schedules (cosine, warmup-stable-decay). Theoretical analysis confirms that LITE facilitates faster convergence along flat directions in anisotropic landscapes, providing a principled approach to efficient LLM pre-training. The code is available at https://github.com/SHUCHENZHU/LITE.

</details>


### [25] [Duel-Evolve: Reward-Free Test-Time Scaling via LLM Self-Preferences](https://arxiv.org/abs/2602.21585)
*Sweta Karlekar,Carolina Zheng,Magnus Saebo,Nicolas Beltran-Velez,Shuyang Yu,John Bowlan,Michal Kucer,David Blei*

Main category: cs.LG

TL;DR: Duel-Evolve：一种基于LLM自身偏好比较的进化优化算法，无需外部奖励模型或人工评分函数，通过贝叶斯Bradley-Terry模型聚合噪声比较，在数学和编程任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖校准的标量评估器来指导搜索，但许多任务中这种评分不可用、过于稀疏或不可靠。相比之下，成对比较更容易获取，仍能提供改进方向的信号，且可以从LLM自身获得而无需外部监督。

Method: 提出Duel-Evolve进化优化算法，用从同一LLM获取的成对偏好替代外部标量奖励。通过贝叶斯Bradley-Terry模型聚合噪声比较，得到候选质量的不确定性感知估计。使用Double Thompson Sampling分配比较预算，选择高质量父代生成改进候选。

Result: 在MathBench上比现有方法和基线准确率提高20个百分点；在LiveCodeBench上比可比迭代方法提高超过12个百分点。方法无需奖励模型、搜索期间的真实标签或人工设计的评分函数。

Conclusion: 成对自我偏好为在大型离散输出空间上进行测试时改进提供了强大的优化信号，展示了仅通过LLM自身比较就能有效指导优化的潜力。

Abstract: Many applications seek to optimize LLM outputs at test time by iteratively proposing, scoring, and refining candidates over a discrete output space. Existing methods use a calibrated scalar evaluator for the target objective to guide search, but for many tasks such scores are unavailable, too sparse, or unreliable. Pairwise comparisons, by contrast, are often easier to elicit, still provide useful signal on improvement directions, and can be obtained from the LLM itself without external supervision. Building on this observation, we introduce Duel-Evolve, an evolutionary optimization algorithm that replaces external scalar rewards with pairwise preferences elicited from the same LLM used to generate candidates. Duel-Evolve aggregates these noisy candidate comparisons via a Bayesian Bradley-Terry model, yielding uncertainty-aware estimates of candidate quality. These quality estimates guide allocation of the comparison budget toward plausible optima using Double Thompson Sampling, as well as selection of high-quality parents to generate improved candidates. We evaluate Duel-Evolve on MathBench, where it achieves 20 percentage points higher accuracy over existing methods and baselines, and on LiveCodeBench, where it improves over comparable iterative methods by over 12 percentage points. Notably, the method requires no reward model, no ground-truth labels during search, and no hand-crafted scoring function. Results show that pairwise self-preferences provide strong optimization signal for test-time improvement over large, discrete output spaces.

</details>


### [26] [Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization](https://arxiv.org/abs/2602.23008)
*Zeyuan Liu,Jeonghye Kim,Xufang Luo,Dongsheng Li,Yuqing Yang*

Main category: cs.LG

TL;DR: EMPO²是一种混合强化学习框架，通过记忆增强探索能力，结合在线和离线策略优化，显著提升LLM智能体在需要发现新状态环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索是强化学习训练的大型语言模型智能体的关键瓶颈。现有方法虽然利用预训练知识，但在需要发现新状态的环境中表现不佳。

Method: 提出EMPO²（探索性记忆增强在线和离线策略优化），这是一个混合RL框架，利用记忆进行探索，并结合在线和离线策略更新，使LLM在有记忆时表现良好，同时确保无记忆时的鲁棒性。

Result: 在ScienceWorld和WebShop环境中，EMPO²相比GRPO分别实现了128.6%和11.3%的性能提升。在分布外测试中，EMPO²对新任务表现出优越的适应性，仅需少量记忆试验且无需参数更新。

Conclusion: EMPO²是构建更具探索性和泛化能力的LLM智能体的有前景框架。

Abstract: Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.

</details>


### [27] [Regularized Online RLHF with Generalized Bilinear Preferences](https://arxiv.org/abs/2602.23116)
*Junghyun Lee,Minju Hong,Kwang-Sung Jun,Chulhee Yun,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文研究具有一般偏好的上下文在线RLHF问题，目标是识别纳什均衡。采用广义双线性偏好模型处理潜在不可传递的偏好，提出两种算法获得不同复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF研究主要局限于反向KL正则化，且在高维环境中缺乏统计效率保证。本文旨在解决具有一般偏好（包括不可传递偏好）的在线RLHF问题，提供更通用的理论框架。

Method: 采用广义双线性偏好模型（GBPM）通过低秩斜对称矩阵捕捉偏好结构。基于强凸正则化器分析，证明贪婪策略的对偶间隙受估计误差平方的约束。提出两种算法：贪婪采样算法和探索-提交算法。

Result: 贪婪采样算法获得与e^{O(η)}无关的多对数遗憾界$\tilde{O}(ηd^4 (\log T)^2)$；探索-提交算法利用低秩结构获得与poly(d)无关的遗憾界$\tilde{O}(\sqrt{ηr T})$，这是高维在线RLHF的首个统计效率保证。

Conclusion: 本文为具有一般偏好的在线RLHF提供了通用理论框架，通过两种简单算法实现了不同复杂度保证，特别是探索-提交算法在高维环境中实现了首个统计效率保证，扩展了RLHF的理论边界。

Abstract: We consider the problem of contextual online RLHF with general preferences, where the goal is to identify the Nash Equilibrium. We adopt the Generalized Bilinear Preference Model (GBPM) to capture potentially intransitive preferences via low-rank, skew-symmetric matrices. We investigate general preference learning with any strongly convex regularizer (where $η^{-1}$ is the regularization strength), generalizing beyond prior works limited to reverse KL-regularization. Central to our analysis is proving that the dual gap of the greedy policy is bounded by the square of the estimation error - a result derived solely from strong convexity and the skew-symmetricity of GBPM.Building on this insight and a feature diversity assumption, we establish two regret bounds via two simple algorithms: (1) Greedy Sampling achieves polylogarithmic, $e^{O(η)}$-free regret $\tilde{O}(ηd^4 (\log T)^2)$. (2) Explore-Then-Commit achieves $\mathrm{poly}(d)$-free regret $\tilde{O}(\sqrt{ηr T})$ by exploiting the low-rank structure; this is the first statistically efficient guarantee for online RLHF in high-dimensions.

</details>


### [28] [Inferential Mechanics Part 1: Causal Mechanistic Theories of Machine Learning in Chemical Biology with Implications](https://arxiv.org/abs/2602.23303)
*Ilya Balabin,Thomas M. Kaiser*

Main category: cs.LG

TL;DR: 该论文系列探讨机器学习在自然科学中的因果缺陷，第一部分提出了化学生物学现象的基础因果结构形式框架，并引入"聚焦"概念，使机器学习算法能在大型数据集中缩小到隐藏的底层机制。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在自然科学中的应用常将预测器视为黑箱，缺乏对数据集因果结构的详细考虑。虽然已有尝试将因果关系引入自然现象机器学习模型的讨论，但缺乏坚实统一的理论处理。

Method: 结合化学理论、生物理论、概率理论和因果关系，建立化学生物学现象的基础因果结构形式框架。引入"聚焦"概念，定义为机器学习算法在大型数据集中缩小到隐藏底层机制的能力。在Akt抑制剂家族上提供初步原理验证。

Result: 建立了化学生物学的新数学框架，无需还原论工具即可建模自然机制。提供了Akt抑制剂家族上因果结构框架的初步证明。

Conclusion: 该系列论文旨在纠正当前机器学习在自然科学中的因果缺陷，第一部分为化学生物学建立了基础因果结构框架，为后续研究奠定理论基础。

Abstract: Machine learning techniques are now routinely encountered in research laboratories across the globe. Impressive progress has been made through ML and AI techniques with regards to large data set processing. This progress has increased the ability of the experimenter to digest data and make novel predictions regarding phenomena of interest. However, machine learning predictors generated from data sets taken from the natural sciences are often treated as black boxes which are used broadly and generally without detailed consideration of the causal structure of the data set of interest. Work has been attempted to bring causality into discussions of machine learning models of natural phenomena; however, a firm and unified theoretical treatment is lacking. This series of three papers explores the union of chemical theory, biological theory, probability theory and causality that will correct current causal flaws of machine learning in the natural sciences. This paper, Part 1 of the series, provides the formal framework of the foundational causal structure of phenomena in chemical biology and is extended to machine learning through the novel concept of focus, defined here as the ability of a machine learning algorithm to narrow down to a hidden underpinning mechanism in large data sets. Initial proof of these principles on a family of Akt inhibitors is also provided. The second paper containing Part 2 will provide a formal exploration of chemical similarity, and Part 3 will present extensive experimental evidence of how hidden causal structures weaken all machine learning in chemical biology. This series serves to establish for chemical biology a new kind of mathematical framework for modeling mechanisms in Nature without the need for the tools of reductionism: inferential mechanics.

</details>


### [29] [ParamMem: Augmenting Language Agents with Parametric Reflective Memory](https://arxiv.org/abs/2602.23320)
*Tianjun Yao,Yongqiang Chen,Yujia Zheng,Pan Li,Zhiqiang Shen,Kun Zhang*

Main category: cs.LG

TL;DR: ParamAgent：基于参数化记忆模块的反思型智能体框架，通过编码跨样本反思模式提升反思多样性，在代码生成、数学推理等任务上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体的自我反思机制往往产生重复输出，限制了推理性能。研究发现反思多样性与任务成功之间存在强正相关，因此需要多样化的反思信号来提升智能体性能

Method: 提出ParamMem参数化记忆模块，将跨样本反思模式编码到模型参数中，通过温度控制采样实现多样化反思生成。基于此构建ParamAgent框架，整合参数化记忆、情景记忆和跨样本记忆

Result: 在代码生成、数学推理和多跳问答任务上的实验显示，ParamAgent持续超越现有最先进基线。ParamMem具有样本效率高、支持跨模型规模的弱到强迁移、无需依赖更强外部模型即可实现自我改进等优势

Conclusion: ParamMem作为增强语言智能体的有效组件，通过参数化记忆提升反思多样性，显著改善智能体性能，展示了参数化记忆在语言智能体中的潜力

Abstract: Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.

</details>


### [30] [Differentiable Zero-One Loss via Hypersimplex Projections](https://arxiv.org/abs/2602.23336)
*Camilo Gomez,Pengyang Wang,Liansheng Tang*

Main category: cs.LG

TL;DR: 提出一种可微分的0-1损失近似方法Soft-Binary-Argmax，通过约束优化框架实现n,k维超单纯形上的平滑投影，解决传统0-1损失不可微问题，提升大批量训练下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 0-1损失被认为是分类性能的黄金标准，但由于其不可微性无法与基于梯度的优化兼容。需要开发一种可微分的近似方法，将结构化优化组件集成到端到端可微模型中，以提供更强的归纳偏置和与任务目标更紧密的对齐。

Method: 提出Soft-Binary-Argmax算子，通过约束优化框架构建n,k维超单纯形上的平滑、保序投影。推导其数学性质，展示如何高效计算其雅可比矩阵，并将其集成到二分类和多分类学习系统中。

Result: 该方法通过施加输出logits的几何一致性约束，显著提升了大批量训练下的泛化性能，缩小了传统大批量训练中观察到的性能差距。

Conclusion: 提出的Soft-Binary-Argmax算子为0-1损失提供了有效的可微分近似，能够将结构化优化组件集成到端到端可微模型中，改善大批量训练下的学习性能。

Abstract: Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.

</details>


### [31] [Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms](https://arxiv.org/abs/2602.23341)
*Alkis Kalavasis,Anay Mehrotra,Manolis Zampetakis,Felix Zhou,Ziyu Zhu*

Main category: cs.LG

TL;DR: 该论文研究从粗数据中进行高斯均值估计，解决了两个关键问题：凸划分下均值可识别的条件，以及在可识别性和凸划分下的计算高效估计可行性。


<details>
  <summary>Details</summary>
Motivation: 粗数据在实际应用中普遍存在（如测量舍入、传感器限制、经济系统滞后），但现有研究仅解决了凸划分下的样本高效估计问题，对于均值可识别的具体条件以及计算高效估计的可行性仍不清楚。

Method: 研究高斯均值估计问题，其中每个真实样本来自d维高斯分布（单位协方差），但只能通过包含样本的划分集合观察到。聚焦于凸划分情况，分析均值可识别的条件。

Result: 解决了两个开放性问题：确定了凸划分下均值可识别的条件，并证明了在可识别性和凸划分下计算高效估计是可行的。

Conclusion: 该工作完全解决了粗数据高斯均值估计中的两个基本问题，为凸划分下的均值可识别性和计算高效估计提供了理论保证。

Abstract: Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]

</details>


### [32] [FlashOptim: Optimizers for Memory Efficient Training](https://arxiv.org/abs/2602.23349)
*Jose Javier Gonzalez Ortiz,Abhay Gupta,Chris Renard,Davis Blalock*

Main category: cs.LG

TL;DR: FlashOptim通过量化优化技术将混合精度训练中的内存占用降低50%以上，从每个参数16字节减少到7字节（或5字节），同时保持模型质量不变。


<details>
  <summary>Details</summary>
Motivation: 标准混合精度训练需要大量内存（每个参数约16字节），使得训练大型模型（如70亿参数）对内存小于100GB的研究者来说不切实际。需要减少内存占用以降低训练门槛。

Method: 提出两种关键技术：1）改进的主权重分割技术，通过寻找和利用量化误差的紧致边界；2）设计压缩扩展函数，大幅减少8位优化器状态量化的误差。结合16位梯度，实现内存优化。

Result: 将AdamW内存从每个参数16字节减少到7字节（或5字节），模型检查点大小减少一半以上。在SGD、AdamW和Lion优化器上应用FlashOptim，在标准视觉和语言基准测试（包括Llama-3.1-8B微调）中未观察到质量下降。

Conclusion: FlashOptim提供了一套有效的优化技术，显著降低训练内存需求（超过50%），同时保持模型质量和API兼容性，使大型模型训练对资源有限的研究者更加可行。

Abstract: Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.
  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.
  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.

</details>


### [33] [SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport](https://arxiv.org/abs/2602.23353)
*Simon Roschmann,Paul Krzakala,Sonia Mazelet,Quentin Bouniot,Zeynep Akata*

Main category: cs.LG

TL;DR: SOTAlign：一种两阶段半监督对齐框架，使用少量配对数据和大量非配对数据对齐预训练的单模态编码器，显著优于现有监督和半监督基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖对比损失和数百万配对样本对齐预训练视觉和语言模型，但本研究探索是否能用更少的监督实现有意义的对齐。

Method: 提出SOTAlign两阶段框架：1) 使用线性教师从有限配对数据中恢复粗略共享几何；2) 通过基于最优传输的散度在非配对样本上细化对齐，转移关系结构而不过度约束目标空间。

Result: SOTAlign有效利用非配对图像和文本，学习跨数据集和编码器对的鲁棒联合嵌入，显著优于监督和半监督基线方法。

Conclusion: 该工作表明通过半监督方法，使用少量配对数据和大量非配对数据可以有效对齐预训练的单模态编码器，验证了Platonic Representation Hypothesis的实用性。

Abstract: The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.

</details>


### [34] [A Dataset is Worth 1 MB](https://arxiv.org/abs/2602.23358)
*Elad Kimchi Shoshani,Leeyam Gabay,Yedid Hoshen*

Main category: cs.LG

TL;DR: PLADA方法通过仅传输类别标签而非像素数据，在客户端已有大型无标签参考数据集的情况下，实现高效的任务知识传输，传输负载小于1MB。


<details>
  <summary>Details</summary>
Motivation: 当前数据集分发面临大规模通信成本问题，客户端硬件和软件框架多样导致无法直接传输预训练模型，而现有数据集蒸馏方法难以扩展到高分辨率数据且无法达到足够小的文件大小。

Method: PLADA方法假设客户端已预加载大型通用无标签参考数据集，通过仅传输特定图像的类别标签来传递新任务。引入剪枝机制过滤参考数据集，保留与目标任务最语义相关的图像标签，同时最大化训练效率和最小化传输负载。

Result: 在10个不同数据集上的实验表明，该方法能以小于1MB的传输负载传递任务知识，同时保持高分类准确率。

Conclusion: PLADA提供了一种高效的数据集服务解决方案，完全消除像素传输，通过仅传输标签实现任务知识的高效传递。

Abstract: A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [35] [Enriching Taxonomies Using Large Language Models](https://arxiv.org/abs/2602.22213)
*Zeinab Ghamlouch,Mehwish Alam*

Main category: cs.IR

TL;DR: Taxoria：基于大语言模型的分类法增强管道，利用现有分类法作为种子，通过LLM生成候选节点并验证后整合，解决现有分类法覆盖不足和节点过时模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类法存在覆盖范围有限、节点过时或模糊的问题，这降低了知识检索的有效性。需要一种方法来增强和更新分类法，提高其在实际应用中的实用性。

Method: Taxoria采用基于大语言模型的分类法增强管道：1）以现有分类法作为种子；2）提示LLM生成候选节点；3）验证候选节点以减少幻觉并确保语义相关性；4）整合验证后的节点形成增强后的分类法。

Result: 最终输出包括：增强后的分类法、来源追踪信息，以及用于分析的最后合并分类法的可视化展示。

Conclusion: Taxoria提供了一种新颖的分类法增强方法，利用LLM的能力来扩展和更新现有分类法，同时通过验证机制确保增强的质量和可靠性。

Abstract: Taxonomies play a vital role in structuring and categorizing information across domains. However, many existing taxonomies suffer from limited coverage and outdated or ambiguous nodes, reducing their effectiveness in knowledge retrieval. To address this, we present Taxoria, a novel taxonomy enrichment pipeline that leverages Large Language Models (LLMs) to enhance a given taxonomy. Unlike approaches that extract internal LLM taxonomies, Taxoria uses an existing taxonomy as a seed and prompts an LLM to propose candidate nodes for enrichment. These candidates are then validated to mitigate hallucinations and ensure semantic relevance before integration. The final output includes an enriched taxonomy with provenance tracking and visualization of the final merged taxonomy for analysis.

</details>


### [36] [Adaptive Prefiltering for High-Dimensional Similarity Search: A Frequency-Aware Approach](https://arxiv.org/abs/2602.22214)
*Teodor-Ioan Calin*

Main category: cs.IR

TL;DR: 提出自适应预过滤框架，利用查询频率模式和聚类一致性指标动态分配计算预算，在保持召回率的同时减少20.4%的距离计算


<details>
  <summary>Details</summary>
Motivation: 现有统一搜索策略未能利用真实世界查询分布的异构特性，导致计算资源分配效率低下

Method: 基于Zipf分布将查询空间划分为频率层级，根据历史访问模式和局部密度特征分配差异化搜索策略，结合轻量级频率跟踪和基于一致性的回退策略

Result: 在ImageNet-1k使用CLIP嵌入的实验中，相比静态nprobe选择，在保持相同召回率的同时减少了20.4%的距离计算，GPU加速FAISS索引上保持亚毫秒级延迟

Conclusion: 自适应预过滤框架通过频率感知的预算分配显著提升搜索效率，引入最小开销，并通过一致性回退策略优雅处理未见查询

Abstract: High-dimensional similarity search underpins modern retrieval systems, yet uniform search strategies fail to exploit the heterogeneous nature of real-world query distributions. We present an adaptive prefiltering framework that leverages query frequency patterns and cluster coherence metrics to dynamically allocate computational budgets. Our approach partitions the query space into frequency tiers following Zipfian distributions and assigns differentiated search policies based on historical access patterns and local density characteristics. Experiments on ImageNet-1k using CLIP embeddings demonstrate that frequency-aware budget allocation achieves equivalent recall with 20.4% fewer distance computations compared to static nprobe selection, while maintaining sub-millisecond latency on GPU-accelerated FAISS indices. The framework introduces minimal overhead through lightweight frequency tracking and provides graceful degradation for unseen queries through coherence-based fallback policies.

</details>


### [37] [Retrieval-Augmented Generation Assistant for Anatomical Pathology Laboratories](https://arxiv.org/abs/2602.22216)
*Diogo Pires,Yuriy Perezhohin,Mauro Castelli*

Main category: cs.IR

TL;DR: 本研究开发并评估了一个针对解剖病理学实验室的检索增强生成（RAG）助手，通过优化文本分块、检索方法和嵌入模型，显著提高了协议查询的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解剖病理学中70%的医疗决策依赖实验室诊断，但静态文档（如打印手册或PDF）往往过时、分散且难以搜索，导致工作流程错误和诊断延迟的风险。需要动态、可靠的知识助手来改善实验室工作效率和患者安全。

Method: 研究构建了包含99个葡萄牙医疗机构AP协议的新语料库和323个问答对。进行了10个实验，比较不同分块策略（递归分块）、检索方法（混合检索）和嵌入模型（包括生物医学专用模型MedEmbed）。使用RAGAS框架（忠实度、答案相关性、上下文召回率）和top-k检索指标进行评估。

Result: 递归分块和混合检索提供了最强的基线性能。加入生物医学专用嵌入模型MedEmbed进一步提高了答案相关性（0.74）、忠实度（0.70）和上下文召回率（0.77）。top-k分析显示，检索单个最高排名分块（k=1）能最大化效率和准确性，反映了AP协议的模块化结构。

Conclusion: 研究强调了在医疗保健领域部署RAG系统的关键设计考虑，展示了将静态文档转化为动态、可靠知识助手的潜力，从而改善实验室工作流程效率并支持患者安全。领域专用嵌入模型对性能提升至关重要。

Abstract: Accurate and efficient access to laboratory protocols is essential in Anatomical Pathology (AP), where up to 70% of medical decisions depend on laboratory diagnoses. However, static documentation such as printed manuals or PDFs is often outdated, fragmented, and difficult to search, creating risks of workflow errors and diagnostic delays. This study proposes and evaluates a Retrieval-Augmented Generation (RAG) assistant tailored to AP laboratories, designed to provide technicians with context-grounded answers to protocol-related queries. We curated a novel corpus of 99 AP protocols from a Portuguese healthcare institution and constructed 323 question-answer pairs for systematic evaluation. Ten experiments were conducted, varying chunking strategies, retrieval methods, and embedding models. Performance was assessed using the RAGAS framework (faithfulness, answer relevance, context recall) alongside top-k retrieval metrics. Results show that recursive chunking and hybrid retrieval delivered the strongest baseline performance. Incorporating a biomedical-specific embedding model (MedEmbed) further improved answer relevance (0.74), faithfulness (0.70), and context recall (0.77), showing the importance of domain-specialised embeddings. Top-k analysis revealed that retrieving a single top-ranked chunk (k=1) maximized efficiency and accuracy, reflecting the modular structure of AP protocols. These findings highlight critical design considerations for deploying RAG systems in healthcare and demonstrate their potential to transform static documentation into dynamic, reliable knowledge assistants, thus improving laboratory workflow efficiency and supporting patient safety.

</details>


### [38] [RAGdb: A Zero-Dependency, Embeddable Architecture for Multimodal Retrieval-Augmented Generation on the Edge](https://arxiv.org/abs/2602.22217)
*Ahmed Bin Khalid*

Main category: cs.IR

TL;DR: RAGdb：一种单文件知识容器架构，将多模态数据摄入、ONNX提取和混合向量检索整合到SQLite中，无需GPU推理，显著降低资源占用，适合边缘计算和隐私敏感场景。


<details>
  <summary>Details</summary>
Motivation: 传统RAG架构复杂，依赖云向量数据库、深度学习框架和高延迟嵌入推理服务器，导致"基础设施膨胀"，阻碍了边缘计算、隔离环境和隐私敏感应用的发展。

Method: 提出RAGdb单文件架构，整合多模态摄入、ONNX提取和混合向量检索到SQLite容器；设计确定性混合评分函数（HSF），结合亚线性TF-IDF向量化和精确子串增强，无需GPU推理。

Result: 在Intel i7-1165G7笔记本上测试：实体检索Recall@1达100%；增量更新效率比冷启动提升31.6倍；磁盘占用比标准Docker RAG栈减少约99.5%。

Conclusion: RAGdb证明了"单文件知识容器"作为去中心化、本地优先AI可行原语，解决了传统RAG基础设施膨胀问题，适用于边缘计算和隐私敏感场景。

Abstract: Retrieval-Augmented Generation (RAG) has established itself as the standard paradigm for grounding Large Language Models (LLMs) in domain-specific, up-to-date data. However, the prevailing architecture for RAG has evolved into a complex, distributed stack requiring cloud-hosted vector databases, heavy deep learning frameworks (e.g., PyTorch, CUDA), and high-latency embedding inference servers. This ``infrastructure bloat'' creates a significant barrier to entry for edge computing, air-gapped environments, and privacy-constrained applications where data sovereignty is paramount.
  This paper introduces RAGdb, a novel monolithic architecture that consolidates automated multimodal ingestion, ONNX-based extraction, and hybrid vector retrieval into a single, portable SQLite container. We propose a deterministic Hybrid Scoring Function (HSF) that combines sublinear TF-IDF vectorization with exact substring boosting, eliminating the need for GPU inference at query time. Experimental evaluation on an Intel i7-1165G7 consumer laptop demonstrates that RAGdb achieves 100\% Recall@1 for entity retrieval and an ingestion efficiency gain of 31.6x during incremental updates compared to cold starts. Furthermore, the system reduces disk footprint by approximately 99.5\% compared to standard Docker-based RAG stacks, establishing the ``Single-File Knowledge Container'' as a viable primitive for decentralized, local-first AI.
  Keywords: Edge AI, Retrieval-Augmented Generation, Vector Search, Green AI, Serverless Architecture, Knowledge Graphs, Efficient Computing.

</details>


### [39] [Comparative Analysis of Neural Retriever-Reranker Pipelines for Retrieval-Augmented Generation over Knowledge Graphs in E-commerce Applications](https://arxiv.org/abs/2602.22219)
*Teri Rumble,Zbyněk Gazdík,Javad Zarrin,Jagdeep Ahluwalia*

Main category: cs.IR

TL;DR: 该研究提出并评估了针对知识图谱自然语言查询的多重检索-重排RAG管道，在电子商务场景中显著提升了检索性能，相比基准提高了20.4%的Hit@1和14.5%的MRR。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在非结构化文本上表现良好，但在结构化知识图谱应用中面临挑战：跨连接图谱的检索扩展和在响应生成中保持上下文关系。交叉编码器提高了检索精度，但其与结构化数据的整合尚未充分探索。解决这些挑战对于开发生产环境中的领域特定助手至关重要。

Method: 研究设计了多个检索器-重排器管道，用于知识图谱自然语言查询，在电子商务环境中进行评估。使用STaRK半结构化知识库（SKB）这一生产规模的电子商务数据集，评估了针对语言查询优化的多种RAG管道配置。

Result: 实验结果表明，相比已发布的基准，取得了显著改进：Hit@1提高了20.4%，平均倒数排名（MRR）提高了14.5%。这些发现为将领域特定SKB集成到生成系统中建立了实用框架。

Conclusion: 研究为部署生产就绪的RAG系统提供了可行的见解，其影响不仅限于电子商务，还扩展到其他需要从结构化知识库中检索信息的领域。该框架为知识图谱与生成模型的集成提供了实用解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have transformed Natural Language Processing (NLP), enabling complex information retrieval and generation tasks. Retrieval-Augmented Generation (RAG) has emerged as a key innovation, enhancing factual accuracy and contextual grounding by integrating external knowledge sources with generative models. Although RAG demonstrates strong performance on unstructured text, its application to structured knowledge graphs presents challenges: scaling retrieval across connected graphs and preserving contextual relationships during response generation. Cross-encoders refine retrieval precision, yet their integration with structured data remains underexplored. Addressing these challenges is crucial for developing domain-specific assistants that operate in production environments. This study presents the design and comparative evaluation of multiple Retriever-Reranker pipelines for knowledge graph natural language queries in e-Commerce contexts. Using the STaRK Semi-structured Knowledge Base (SKB), a production-scale e-Commerce dataset, we evaluate multiple RAG pipeline configurations optimized for language queries. Experimental results demonstrate substantial improvements over published benchmarks, achieving 20.4% higher Hit@1 and 14.5% higher Mean Reciprocal Rank (MRR). These findings establish a practical framework for integrating domain-specific SKBs into generative systems. Our contributions provide actionable insights for the deployment of production-ready RAG systems, with implications that extend beyond e-Commerce to other domains that require information retrieval from structured knowledge bases.

</details>


### [40] [SmartChunk Retrieval: Query-Aware Chunk Compression with Planning for Efficient Document RAG](https://arxiv.org/abs/2602.22225)
*Xuechen Zhang,Koustava Goswami,Samet Oymak,Jiasi Chen,Nedim Lipka*

Main category: cs.IR

TL;DR: SmartChunk是一种查询自适应的检索增强生成框架，通过动态调整文档块抽象级别来提升长文档问答的效率和准确性，避免了传统固定分块策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）系统存在静态分块和平面检索的局限性：文档被分割为固定大小的短块，检索质量对分块大小高度敏感，容易引入不相关或误导性块，且在大规模语料库上扩展性差。

Method: SmartChunk包含两个核心组件：(1) 规划器预测每个查询的最优块抽象级别；(2) 轻量级压缩模块生成高级块嵌入而无需重复摘要。采用新颖的强化学习方案STITCH来推理块抽象，实现动态调整检索粒度。

Result: 在五个QA基准测试和一个域外数据集上，SmartChunk均优于最先进的RAG基线方法，同时降低了成本。分析表明其在大规模语料库上具有良好的扩展性，在域外数据集上获得一致增益。

Conclusion: SmartChunk通过查询自适应框架有效平衡了准确性和效率，避免了固定策略的缺点，为自适应检索提供了一个通用且有效的解决方案。

Abstract: Retrieval-augmented generation (RAG) has strong potential for producing accurate and factual outputs by combining language models (LMs) with evidence retrieved from large text corpora. However, current pipelines are limited by static chunking and flat retrieval: documents are split into short, predetermined, fixed-size chunks, embeddings are retrieved uniformly, and generation relies on whatever chunks are returned. This design brings challenges, as retrieval quality is highly sensitive to chunk size, often introduces noise from irrelevant or misleading chunks, and scales poorly to large corpora. We present SmartChunk retrieval, a query-adaptive framework for efficient and robust long-document question answering (QA). SmartChunk uses (i) a planner that predicts the optimal chunk abstraction level for each query, and (ii) a lightweight compression module that produces high-level chunk embeddings without repeated summarization. By adapting retrieval granularity on the fly, SmartChunk balances accuracy with efficiency and avoids the drawbacks of fixed strategies. Notably, our planner can reason about chunk abstractions through a novel reinforcement learning scheme, STITCH, which boosts accuracy and generalization. To reflect real-world applications, where users face diverse document types and query styles, we evaluate SmartChunk on five QA benchmarks plus one out-of-domain dataset. Across these evaluations, SmartChunk outperforms state-of-the-art RAG baselines, while reducing cost. Further analysis demonstrates strong scalability with larger corpora and consistent gains on out-of-domain datasets, highlighting its effectiveness as a general framework for adaptive retrieval.

</details>


### [41] [SQaLe: A Large Text-to-SQL Corpus Grounded in Real Schemas](https://arxiv.org/abs/2602.22223)
*Cornelius Wolff,Daniel Gomm,Madelon Hulsebos*

Main category: cs.IR

TL;DR: SQaLe是一个基于135,875个真实关系数据库模式构建的大规模半合成文本到SQL数据集，包含517,676个高质量(问题、模式、查询)三元组，旨在解决现有数据集在模式复杂性、领域覆盖和任务多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 开发通用文本到SQL模型的关键瓶颈是缺乏大规模数据集，这些数据集需要具备足够的模式复杂性、查询复杂性、领域覆盖和任务多样性。现有数据集在这些方面存在不足。

Method: 基于真实世界模式集合SchemaPile扩展出135,875个关系数据库模式，建立原则性生成管道，结合模式采样、问题合成和SQL构建，生成高质量的三元组数据。

Result: 创建了包含517,676个高质量(问题、模式、查询)三元组的SQaLe数据集，该数据集捕捉了真实的模式大小变异性、多样化的查询模式和自然语言歧义，同时保持执行有效性。

Conclusion: SQaLe是目前最真实的大规模文本到SQL数据集，相比现有基准和数据集更具优势，能够支持数据扩展和模型泛化的研究愿景，数据集已公开可用。

Abstract: Advances in large language models have accelerated progress in text-to-SQL, methods for converting natural language queries into valid SQL queries. A key bottleneck for developing generalizable text-to-SQL models is the lack of large-scale datasets with sufficient schema and query complexity, domain coverage, and task diversity. We introduce SQaLe: a large-scale semi-synthetic text-to-SQL dataset built on 135,875 relational database schemas expanded from a collection of real-world schemas, SchemaPile. We establish a principled generation pipeline which combines schema sampling, question synthesis, and SQL construction, and produce 517,676 high-quality (question, schema, query) triples. The SQaLe dataset captures realistic schema size variability, diverse query patterns, and natural language ambiguity while maintaining execution validity. We provide an analysis of its contents and characteristics, and find that SQaLe introduces the most realistic large-scale text-to-SQL dataset to date in comparison with existing benchmarks and datasets. We discuss how SQaLe enables our vision for data scaling and model generalization in text-to-SQL research. The dataset is accessible at: https://huggingface.co/datasets/trl-lab/SQaLe-text-to-SQL-dataset.

</details>


### [42] [PSQE: A Theoretical-Practical Approach to Pseudo Seed Quality Enhancement for Unsupervised MMEA](https://arxiv.org/abs/2602.22903)
*Yunpeng Hong,Chenyang Bu,Jie Zhang,Yi He,Di Wu,Xindong Wu*

Main category: cs.IR

TL;DR: 本文提出PSQE方法，通过多模态信息和聚类重采样提升无监督多模态实体对齐中伪种子的质量和图覆盖平衡，改善对比学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态实体对齐（MMEA）对大型语言模型应用很重要，但标注种子对难以获取。现有无监督方法使用伪对齐种子，但在多模态设置下，多模态信息的引入常导致知识图中伪种子覆盖不平衡，影响对齐效果。

Method: 提出PSQE（伪种子质量增强）方法，通过多模态信息和聚类重采样来提高伪种子的精度和图覆盖平衡。该方法可作为即插即用模块，提升现有对比学习基线的性能。

Result: 理论分析表明伪种子同时影响对比学习中的吸引项和排斥项，而不平衡的图覆盖会导致模型偏向高密度区域，削弱对稀疏区域实体的学习能力。实验验证了理论发现，PSQE显著提升了基线模型的性能。

Conclusion: PSQE通过改善伪种子质量和图覆盖平衡，有效解决了无监督多模态实体对齐中的关键问题，为现有对比学习方法提供了可插拔的增强模块。

Abstract: Multimodal Entity Alignment (MMEA) aims to identify equivalent entities across different data modalities, enabling structural data integration that in turn improves the performance of various large language model applications. To lift the requirement of labeled seed pairs that are difficult to obtain, recent methods shifted to an unsupervised paradigm using pseudo-alignment seeds. However, unsupervised entity alignment in multimodal settings remains underexplored, mainly because the incorporation of multimodal information often results in imbalanced coverage of pseudo-seeds within the knowledge graph. To overcome this, we propose PSQE (Pseudo-Seed Quality Enhancement) to improve the precision and graph coverage balance of pseudo seeds via multimodal information and clustering-resampling. Theoretical analysis reveals the impact of pseudo seeds on existing contrastive learning-based MMEA models. In particular, pseudo seeds can influence the attraction and the repulsion terms in contrastive learning at once, whereas imbalanced graph coverage causes models to prioritize high-density regions, thereby weakening their learning capability for entities in sparse regions. Experimental results validate our theoretical findings and show that PSQE as a plug-and-play module can improve the performance of baselines by considerable margins.

</details>


### [43] [SIGMA: A Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress](https://arxiv.org/abs/2602.22913)
*Yang Yu,Lei Kou,Huaikuan Yi,Bin Chen,Yayu Cao,Lei Shen,Chao Zhang,Bing Wang,Xiaoyi Zeng*

Main category: cs.IR

TL;DR: SIGMA是一个基于语义的指令驱动多任务生成推荐系统，通过统一的潜在空间捕捉语义和协同关系，采用混合项目标记化方法，构建大规模多任务SFT数据集，实现通过指令跟随满足多样化推荐需求。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐方法大多局限于交互驱动的下一项预测范式，无法快速适应不断变化的趋势，也无法满足现实场景中多样化的推荐任务和业务特定需求。

Method: 1) 通过统一潜在空间将项目实体与通用语义关联；2) 开发混合项目标记化方法进行精确建模和高效生成；3) 构建大规模多任务SFT数据集；4) 设计三步项目生成流程，集成自适应概率融合机制来校准输出分布。

Result: 广泛的离线实验和在线A/B测试证明了SIGMA的有效性。

Conclusion: SIGMA通过语义基础和指令驱动的方法，成功解决了现有生成推荐系统在适应性和任务多样性方面的局限性，为现实世界推荐场景提供了有效的解决方案。

Abstract: With the rapid evolution of Large Language Models, generative recommendation is gradually reshaping the paradigm of recommender systems. However, most existing methods are still confined to the interaction-driven next-item prediction paradigm, failing to rapidly adapt to evolving trends or address diverse recommendation tasks along with business-specific requirements in real-world scenarios. To this end, we present SIGMA, a Semantic-Grounded Instruction-Driven Generative Multi-Task Recommender at AliExpress. Specifically, we first ground item entities in general semantics via a unified latent space capturing both semantic and collaborative relations. Building upon this, we develop a hybrid item tokenization method for precise modeling and efficient generation. Moreover, we construct a large-scale multi-task SFT dataset to empower SIGMA to fulfill various recommendation demands via instruction-following. Finally, we design a three-step item generation procedure integrated with an adaptive probabilistic fusion mechanism to calibrate the output distributions based on task-specific requirements for recommendation accuracy and diversity. Extensive offline experiments and online A/B tests demonstrate the effectiveness of SIGMA.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: 通过人机协作发现Hermite求积规则的新误差表示和边界，展示了AI在数学研究中的潜力与局限


<details>
  <summary>Details</summary>
Motivation: 探索人工智能是否真正能促进创造性数学研究，还是仅仅自动化常规计算并引入错误风险

Method: 通过详细案例研究，使用多个人工智能助手进行系统化的人机协作，扩展手动工作无法达到的结果

Result: AI在代数操作、系统化证明探索、文献综合和LaTeX准备方面表现出色，但每个步骤都需要严格的人类验证、数学直觉和战略指导

Conclusion: 当配合适当的怀疑态度和验证协议使用时，AI工具能够有意义地加速数学发现，但需要仔细的人类监督和深厚的领域专业知识

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [45] [Graph Your Way to Inspiration: Integrating Co-Author Graphs with Retrieval-Augmented Generation for Large Language Model Based Scientific Idea Generation](https://arxiv.org/abs/2602.22215)
*Pengzhen Xie,Huizhi Liang*

Main category: cs.AI

TL;DR: 本文提出GYWI系统，通过作者知识图谱与检索增强生成结合，为LLMs提供可控学术背景和可追溯灵感路径，以生成更优质的科学创意。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在科学创意生成方面存在不足：生成的成果缺乏可控的学术背景和可追溯的灵感路径。需要一种能够提供结构化知识支持和灵感溯源机制的系统来提升科学创意的质量。

Method: 1) 提出以作者为中心的知识图谱构建方法和灵感源采样算法构建外部知识库；2) 设计混合检索机制（RAG+GraphRAG）获取深度和广度知识；3) 提出基于强化学习原理的Prompt优化策略，自动引导LLMs基于混合上下文优化结果。

Result: 基于arXiv（2018-2023）构建评估数据集，采用多维度评估方法（经验性自动评估、LLM评分、人工评估、语义空间可视化分析）。在GPT-4o、DeepSeek-V3、Qwen3-8B、Gemini 2.5等模型上实验，GYWI在创新性、可靠性、相关性等多个指标上显著优于主流LLMs。

Conclusion: GYWI系统通过整合作者知识图谱和混合检索增强生成，有效解决了LLMs科学创意生成中背景不可控和灵感路径不可追溯的问题，显著提升了生成创意的质量。

Abstract: Large Language Models (LLMs) demonstrate potential in the field of scientific idea generation. However, the generated results often lack controllable academic context and traceable inspiration pathways. To bridge this gap, this paper proposes a scientific idea generation system called GYWI, which combines author knowledge graphs with retrieval-augmented generation (RAG) to form an external knowledge base to provide controllable context and trace of inspiration path for LLMs to generate new scientific ideas. We first propose an author-centered knowledge graph construction method and inspiration source sampling algorithms to construct external knowledge base. Then, we propose a hybrid retrieval mechanism that is composed of both RAG and GraphRAG to retrieve content with both depth and breadth knowledge. It forms a hybrid context. Thirdly, we propose a Prompt optimization strategy incorporating reinforcement learning principles to automatically guide LLMs optimizing the results based on the hybrid context. To evaluate the proposed approaches, we constructed an evaluation dataset based on arXiv (2018-2023). This paper also develops a comprehensive evaluation method including empirical automatic assessment in multiple-choice question task, LLM-based scoring, human evaluation, and semantic space visualization analysis. The generated ideas are evaluated from the following five dimensions: novelty, feasibility, clarity, relevance, and significance. We conducted experiments on different LLMs including GPT-4o, DeepSeek-V3, Qwen3-8B, and Gemini 2.5. Experimental results show that GYWI significantly outperforms mainstream LLMs in multiple metrics such as novelty, reliability, and relevance.

</details>


### [46] [FIRE: A Comprehensive Benchmark for Financial Intelligence and Reasoning Evaluation](https://arxiv.org/abs/2602.22273)
*Xiyuan Zhang,Huihang Wu,Jiayu Guo,Zhenlin Zhang,Yiwei Zhang,Liangyu Huo,Xiaoxiao Ma,Jiansong Wan,Xuewei Jiao,Yi Jing,Jian Xie*

Main category: cs.AI

TL;DR: FIRE是一个综合性的金融基准测试，用于评估LLMs的理论金融知识和实际业务场景处理能力，包含理论考试题和3000个金融场景问题，并发布了基准问题和评估代码。


<details>
  <summary>Details</summary>
Motivation: 当前需要全面评估大型语言模型在金融领域的实际应用价值，不仅要测试其理论知识掌握程度，还要评估其在真实金融业务场景中的处理能力。

Method: 1) 理论评估：从广泛认可的金融资格考试中收集多样化考题；2) 实践评估：提出系统性评估矩阵，分类复杂金融领域并确保覆盖关键子领域和业务活动；3) 收集3000个金融场景问题，包括有参考答案的封闭式决策问题和按预定评分标准评估的开放式问题。

Result: 对包括轩辕4.0在内的最先进LLMs进行了全面评估，轩辕4.0作为金融领域的最新模型提供了强大的领域内基准，结果能够系统分析当前LLMs在金融应用中的能力边界。

Conclusion: FIRE基准为评估LLMs的金融能力提供了全面框架，通过公开基准问题和评估代码促进未来研究，有助于系统理解当前LLMs在金融应用中的能力边界。

Abstract: We introduce FIRE, a comprehensive benchmark designed to evaluate both the theoretical financial knowledge of LLMs and their ability to handle practical business scenarios. For theoretical assessment, we curate a diverse set of examination questions drawn from widely recognized financial qualification exams, enabling evaluation of LLMs deep understanding and application of financial knowledge. In addition, to assess the practical value of LLMs in real-world financial tasks, we propose a systematic evaluation matrix that categorizes complex financial domains and ensures coverage of essential subdomains and business activities. Based on this evaluation matrix, we collect 3,000 financial scenario questions, consisting of closed-form decision questions with reference answers and open-ended questions evaluated by predefined rubrics. We conduct comprehensive evaluations of state-of-the-art LLMs on the FIRE benchmark, including XuanYuan 4.0, our latest financial-domain model, as a strong in-domain baseline. These results enable a systematic analysis of the capability boundaries of current LLMs in financial applications. We publicly release the benchmark questions and evaluation code to facilitate future research.

</details>


### [47] [Multi-Level Causal Embeddings](https://arxiv.org/abs/2602.22287)
*Willem Schooltink,Fabio Massimo Zennaro*

Main category: cs.AI

TL;DR: 本文提出因果嵌入框架，将多个详细模型映射到粗粒度因果模型的子系统中，作为抽象化的推广，并应用于统计和因果边际问题。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象研究主要关注两个模型之间的关系，但实际应用中需要将多个详细模型整合到统一的粗粒度模型中。需要一种能够处理多个模型嵌入到同一粗粒度模型中的框架。

Method: 定义因果嵌入作为抽象化的推广，提出广义一致性概念，建立多分辨率边际问题框架，将统计边际问题和因果边际问题统一起来。

Result: 展示了因果嵌入框架在统计边际问题和因果边际问题中的相关性，并证明了其在合并来自不同表示模型的多个数据集方面的实际应用价值。

Conclusion: 因果嵌入框架为处理多个详细模型到粗粒度模型的映射提供了理论基础，能够有效解决多模型整合和数据集合并的实际问题，扩展了因果抽象的应用范围。

Abstract: Abstractions of causal models allow for the coarsening of models such that relations of cause and effect are preserved. Whereas abstractions focus on the relation between two models, in this paper we study a framework for causal embeddings which enable multiple detailed models to be mapped into sub-systems of a coarser causal model. We define causal embeddings as a generalization of abstraction, and present a generalized notion of consistency. By defining a multi-resolution marginal problem, we showcase the relevance of causal embeddings for both the statistical marginal problem and the causal marginal problem; furthermore, we illustrate its practical use in merging datasets coming from models with different representations.

</details>


### [48] [Agent Behavioral Contracts: Formal Specification and Runtime Enforcement for Reliable Autonomous AI Agents](https://arxiv.org/abs/2602.22302)
*Varun Pratap Bhardwaj*

Main category: cs.AI

TL;DR: 论文提出Agent Behavioral Contracts (ABC)框架，将设计契约原则引入AI智能体，通过形式化规范约束AI行为，解决智能体部署中的漂移和治理问题。


<details>
  <summary>Details</summary>
Motivation: 传统软件依赖API、类型系统等契约来确保正确行为，而AI智能体仅依赖提示和自然语言指令，缺乏形式化行为规范，这导致了智能体部署中的漂移、治理失败和项目失败问题。

Method: 提出ABC框架，契约C=(P,I,G,R)包含前置条件、不变量、治理策略和恢复机制作为一等运行时可执行组件。定义了(p,δ,k)-满足度概念，考虑LLM非确定性和恢复机制，并证明了漂移边界定理。

Result: 在AgentContract-Bench基准测试中，契约化智能体在1980个会话中检测到5.2-6.8个软违规（基线完全未检测），实现88-100%硬约束合规，将行为漂移限制在D*<0.27，恢复率100%（前沿模型）和17-100%（所有模型），开销<10ms/动作。

Conclusion: ABC框架为AI智能体提供了形式化行为规范机制，能有效检测违规、约束行为漂移、实现高恢复率，为AI智能体的可靠部署提供了理论基础和实践工具。

Abstract: Traditional software relies on contracts -- APIs, type systems, assertions -- to specify and enforce correct behavior. AI agents, by contrast, operate on prompts and natural language instructions with no formal behavioral specification. This gap is the root cause of drift, governance failures, and frequent project failures in agentic AI deployments. We introduce Agent Behavioral Contracts (ABC), a formal framework that brings Design-by-Contract principles to autonomous AI agents. An ABC contract C = (P, I, G, R) specifies Preconditions, Invariants, Governance policies, and Recovery mechanisms as first-class, runtime-enforceable components. We define (p, delta, k)-satisfaction -- a probabilistic notion of contract compliance that accounts for LLM non-determinism and recovery -- and prove a Drift Bounds Theorem showing that contracts with recovery rate gamma > alpha (the natural drift rate) bound behavioral drift to D* = alpha/gamma in expectation, with Gaussian concentration in the stochastic setting. We establish sufficient conditions for safe contract composition in multi-agent chains and derive probabilistic degradation bounds. We implement ABC in AgentAssert, a runtime enforcement library, and evaluate on AgentContract-Bench, a benchmark of 200 scenarios across 7 models from 6 vendors. Results across 1,980 sessions show that contracted agents detect 5.2-6.8 soft violations per session that uncontracted baselines miss entirely (p < 0.0001, Cohen's d = 6.7-33.8), achieve 88-100% hard constraint compliance, and bound behavioral drift to D* < 0.27 across extended sessions, with 100% recovery for frontier models and 17-100% across all models, at overhead < 10 ms per action.

</details>


### [49] [Exploring Human Behavior During Abstract Rule Inference and Problem Solving with the Cognitive Abstraction and Reasoning Corpus](https://arxiv.org/abs/2602.22408)
*Caroline Ahn,Quan Do,Leah Bakst,Michael P. Pascale,Joseph T. McGuire,Michael E. Hasselmo,Chantal E. Stern*

Main category: cs.AI

TL;DR: 研究人员开发了CogARC测试集来研究人类抽象推理策略，通过两个实验让260名参与者解决75个视觉推理问题，记录了高时间分辨率的行为数据，发现人类在抽象推理中表现出高效但策略多样的特点。


<details>
  <summary>Details</summary>
Motivation: 研究人类抽象推理的认知策略，了解人类如何从稀疏示例中快速学习和应用规则，为此创建了专门的人类适应版抽象推理测试集CogARC。

Method: 从ARC中选取适合人类测试的子集CogARC，在两个实验中让260名参与者解决75个抽象视觉推理问题，记录高时间分辨率的行为数据（包括示例查看、编辑序列和多尝试提交）。

Result: 参与者总体成功率较高（实验1约90%，实验2约80%），但不同问题和参与者间差异大。难题引发更长的思考时间和更多样的解决策略。随着任务进行，响应速度加快但准确率略有下降。即使是错误解决方案也常常高度收敛。

Conclusion: CogARC为研究人类抽象推理提供了丰富的实验环境，揭示了人类在不确定性下如何泛化、错误泛化和调整策略，展示了人类抽象推理的高效性和策略多样性。

Abstract: Humans exhibit remarkable flexibility in abstract reasoning, and can rapidly learn and apply rules from sparse examples. To investigate the cognitive strategies underlying this ability, we introduce the Cognitive Abstraction and Reasoning Corpus (CogARC), a diverse human-adapted subset of the Abstraction and Reasoning Corpus (ARC) which was originally developed to benchmark abstract reasoning in artificial intelligence. Across two experiments, CogARC was administered to a total of 260 human participants who freely generated solutions to 75 abstract visual reasoning problems. Success required inferring input-output rules from a small number of examples to transform the test input into one correct test output. Participants' behavior was recorded at high temporal resolution, including example viewing, edit sequences, and multi-attempt submissions. Participants were generally successful (mean accuracy ~90% for experiment 1 (n=40), ~80% for experiment 2 (n=220) across problems), but performance varied widely across problems and participants. Harder problems elicited longer deliberation times and greater divergence in solution strategies. Over the course of the task, participants initiated responses more quickly but showed a slight decline in accuracy, suggesting increased familiarity with the task structure rather than improved rule-learning ability. Importantly, even incorrect solutions were often highly convergent, even when the problem-solving trajectories differed in length and smoothness. Some trajectories progressed directly and efficiently toward a stable outcome, whereas others involved extended exploration or partial restarts before converging. Together, these findings highlight CogARC as a rich behavioral environment for studying human abstract reasoning, providing insight into how people generalize, misgeneralize, and adapt their strategies under uncertainty.

</details>


### [50] [Epistemic Filtering and Collective Hallucination: A Jury Theorem for Confidence-Calibrated Agents](https://arxiv.org/abs/2602.22413)
*Jonas Karge*

Main category: cs.AI

TL;DR: 研究异构智能体通过校准自身可靠性并选择性弃权投票的集体准确性，将孔多塞陪审团定理扩展到置信度门控的序列设置中。


<details>
  <summary>Details</summary>
Motivation: 传统孔多塞陪审团定理假设固定参与，但现实世界聚合通常允许智能体说"我不知道"。研究智能体学习自身可靠性并选择性弃权如何影响集体决策准确性。

Method: 提出概率框架：智能体先进行校准阶段更新对自身固定能力的信念，然后面对最终置信度门控决定是否投票或弃权。推导群体成功概率的非渐近下界。

Result: 证明选择性参与将孔多塞陪审团定理的渐近保证推广到序列置信度门控设置。通过蒙特卡洛模拟验证边界，并讨论在AI安全中缓解LLM集体决策幻觉的潜在应用。

Conclusion: 选择性弃权投票框架扩展了传统集体决策理论，为异构智能体通过校准和置信度门控提高集体准确性提供了理论基础，在AI安全等领域有应用前景。

Abstract: We investigate the collective accuracy of heterogeneous agents who learn to estimate their own reliability over time and selectively abstain from voting. While classical epistemic voting results, such as the \textit{Condorcet Jury Theorem} (CJT), assume fixed participation, real-world aggregation often benefits from allowing agents to say ``I don't know.'' We propose a probabilistic framework where agents engage in a \textit{calibration} phase, updating beliefs about their own fixed competence, before facing a final confidence gate that determines whether to vote or abstain. We derive a non-asymptotic lower bound on the group's success probability and prove that this \textit{selective participation} generalizes the asymptotic guarantees of the CJT to a sequential, confidence-gated setting. Empirically, we validate these bounds via Monte Carlo simulations. While our results are general, we discuss their potential application to AI safety, outlining how this framework can mitigate \textit{hallucinations} in collective LLM decision-making.

</details>


### [51] [How Do Latent Reasoning Methods Perform Under Weak and Strong Supervision?](https://arxiv.org/abs/2602.22441)
*Yingqian Cui,Zhenwei Dai,Bing He,Zhan Shi,Hui Liu,Rui Sun,Zhiji Liu,Yue Xing,Jiliang Tang,Benoit Dumoulin*

Main category: cs.AI

TL;DR: 本文对潜在推理方法进行了全面分析，揭示了其内部机制中的两个关键问题：普遍存在的捷径行为和潜在空间搜索的不忠实性，并发现了监督强度与表示多样性之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 潜在推理作为一种新兴的推理范式，通过在潜在空间而非文本空间生成步骤进行多步推理，但尽管已有许多研究关注提升其性能，其内部机制仍未得到充分研究。本文旨在深入理解潜在表示在推理过程中的角色和行为。

Method: 对具有不同监督水平的潜在推理方法进行综合分析，识别关键问题：1）观察普遍的捷径行为；2）检验潜在推理是否支持类似广度优先搜索的探索；3）分析监督强度对潜在表示能力的影响。

Result: 发现两个关键问题：1）潜在推理方法普遍存在捷径行为，能够在不需要潜在推理的情况下获得高准确率；2）潜在表示虽然能够编码多种可能性，但推理过程并未忠实实现结构化搜索，而是表现出隐式剪枝和压缩。此外，揭示了监督强度与表示能力之间的权衡：强监督能缓解捷径行为但限制表示多样性，弱监督允许更丰富的潜在表示但增加捷径行为。

Conclusion: 潜在推理方法存在内部机制上的局限性，包括捷径行为和搜索过程的不忠实性。监督强度在缓解捷径行为与保持表示多样性之间存在权衡，这为未来改进潜在推理方法提供了重要见解。

Abstract: Latent reasoning has been recently proposed as a reasoning paradigm and performs multi-step reasoning through generating steps in the latent space instead of the textual space. This paradigm enables reasoning beyond discrete language tokens by performing multi-step computation in continuous latent spaces. Although there have been numerous studies focusing on improving the performance of latent reasoning, its internal mechanisms remain not fully investigated. In this work, we conduct a comprehensive analysis of latent reasoning methods to better understand the role and behavior of latent representation in the process. We identify two key issues across latent reasoning methods with different levels of supervision. First, we observe pervasive shortcut behavior, where they achieve high accuracy without relying on latent reasoning. Second, we examine the hypothesis that latent reasoning supports BFS-like exploration in latent space, and find that while latent representations can encode multiple possibilities, the reasoning process does not faithfully implement structured search, but instead exhibits implicit pruning and compression. Finally, our findings reveal a trade-off associated with supervision strength: stronger supervision mitigates shortcut behavior but restricts the ability of latent representations to maintain diverse hypotheses, whereas weaker supervision allows richer latent representations at the cost of increased shortcut behavior.

</details>


### [52] [CWM: Contrastive World Models for Action Feasibility Learning in Embodied Agent Pipelines](https://arxiv.org/abs/2602.22452)
*Chayan Banerjee*

Main category: cs.AI

TL;DR: 论文提出对比世界模型（CWM），使用对比学习训练动作可行性评分器，通过挖掘困难负样本来区分物理上可行与不可行动作，相比传统监督微调在ScienceWorld基准上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用监督微调训练动作评分器，但这种方法独立处理每个候选动作，没有明确教导模型区分物理上正确和微妙错误的动作。需要一个能更好识别动作物理可行性的方法。

Method: 提出对比世界模型（CWM），使用InfoNCE对比目标对大型语言模型进行微调，特别强调挖掘困难负样本（语义相似但物理不兼容的候选动作），在评分空间中推动有效动作远离无效动作。

Result: 在ScienceWorld基准测试中：1）内在可用性评估显示，CWM在最小编辑负样本上的Precision@1比SFT高6.76个百分点，AUC-ROC更高（0.929 vs 0.906）；2）实时过滤特性研究显示，在分布外压力条件下，CWM保持更好的安全边界（-2.39 vs -3.96）。

Conclusion: 对比训练比单独使用监督微调能更准确地捕捉物理可行性，对比世界模型通过强调困难负样本学习到的表示能更好地区分物理上可行与不可行的动作。

Abstract: A reliable action feasibility scorer is a critical bottleneck in embodied agent pipelines: before any planning or reasoning occurs, the agent must identify which candidate actions are physically executable in the current state. Existing approaches use supervised fine-tuning (SFT) to train action scorers, but SFT treats each candidate independently and does not explicitly teach the model to discriminate between actions that are physically correct and those that are subtly wrong. We propose the Contrastive World Model (CWM), which fine-tunes a large language model (LLM) as an action scorer using an InfoNCE contrastive objective with hard-mined negative examples. The key idea is to push valid actions away from invalid ones in scoring space, with special emphasis on hard negatives: semantically similar but physically incompatible candidates. We evaluate CWM on the ScienceWorld benchmark through two studies. First, an intrinsic affordance evaluation on 605 hard-negative test pairs shows that CWM outperforms SFT by +6.76 percentage points on Precision@1 for minimal-edit negatives -- cases where a single word changes the physical outcome -- and achieves a higher AUC-ROC (0.929 vs. 0.906). Second, a live filter characterisation study measures how well CWM ranks gold-path actions against all valid environment actions during task execution. Under out-of-distribution stress conditions, CWM maintains a significantly better safety margin (-2.39) than SFT (-3.96), indicating that the gold action is ranked closer to the top. These results support the hypothesis that contrastive training induces representations that capture physical feasibility more faithfully than SFT alone.

</details>


### [53] [ConstraintBench: Benchmarking LLM Constraint Reasoning on Direct Optimization](https://arxiv.org/abs/2602.22465)
*Joseph Tso,Preston Schmittou,Quan Huynh,Jibran Hutchins*

Main category: cs.AI

TL;DR: LLM在完全指定的约束优化问题上直接生成正确解的能力评估，发现可行性是主要瓶颈而非最优性


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估LLM能否将优化问题公式化为求解器代码，但缺乏评估LLM能否在没有求解器的情况下直接为完全指定的约束优化问题生成正确解的能力

Method: 引入ConstraintBench基准，涵盖10个运筹学领域，每个任务包含自然语言场景描述，模型必须返回结构化解决方案，由确定性验证器检查所有约束和求解器证明的最优解

Result: 评估6个前沿模型在200个任务上的表现：最佳模型仅达到65.0%约束满足率，可行解平均达到Gurobi最优目标的89-96%，但没有模型在联合可行性和最优性（与求解器参考值相差0.1%以内）上超过30.5%

Conclusion: 可行性是LLM在约束优化中的主要瓶颈，不同领域难度差异大，存在系统性失败模式，ConstraintBench基准将公开发布

Abstract: Large language models are increasingly applied to operational decision-making where the underlying structure is constrained optimization. Existing benchmarks evaluate whether LLMs can formulate optimization problems as solver code, but leave open a complementary question. Can LLMs directly produce correct solutions to fully specified constrained optimization problems without access to a solver? We introduce ConstraintBench, a benchmark for evaluating LLMs on direct constrained optimization across 10 operations research domains, with all ground-truth solutions verified by the Gurobi solver. Each task presents a natural-language scenario with entities, constraints, and an optimization objective; the model must return a structured solution that a deterministic verifier checks against every constraint and the solver-proven optimum. We evaluate six frontier models on 200 tasks and find that feasibility, not optimality, is the primary bottleneck. The best model achieves only 65.0% constraint satisfaction, yet feasible solutions average 89 to 96% of the Gurobi-optimal objective. No model exceeds 30.5% on joint feasibility and optimality within 0.1% of the solver reference. Per-domain analysis shows large variation in difficulty, with average feasibility spanning from 83.3% in the production mix domain to 0.8% in the crew assignment domain. Further, systematic failure modes include duration constraint misunderstanding, entity hallucination, and a feasibility-optimality decoupling in facility location and vehicle routing where models achieve high feasibility but 0% optimality. ConstraintBench and all evaluation infrastructure will be publicly released.

</details>


### [54] [VeRO: An Evaluation Harness for Agents to Optimize Agents](https://arxiv.org/abs/2602.22480)
*Varun Ursekar,Apaar Shanker,Veronica Chatrath,Yuan,Xue,Sam Denton*

Main category: cs.AI

TL;DR: VERO是一个用于评估编码智能体优化性能的框架，包含版本控制、奖励机制和观察系统，支持对智能体进行迭代改进的评估


<details>
  <summary>Details</summary>
Motivation: 编码智能体的一个重要应用是智能体优化：通过编辑-执行-评估循环迭代改进目标智能体。然而，社区缺乏对此任务的系统性理解，且智能体优化与传统软件工程有根本区别，需要同时捕获确定性代码和随机LLM生成的中间推理及下游执行结果

Method: 引入VERO框架，提供：(1) 可复现的评估工具链，包含版本化智能体快照、预算控制评估和结构化执行轨迹；(2) 包含目标智能体和任务的基准套件，以及参考评估程序。使用VERO进行实证研究，比较不同优化器配置并分析哪些修改能可靠提升智能体性能

Result: 通过VERO框架，研究人员能够系统评估编码智能体优化性能，比较不同优化策略，识别可靠的改进方法。该框架支持智能体优化作为编码智能体核心能力的研究

Conclusion: VERO为编码智能体优化研究提供了系统化的评估框架，填补了该领域的空白，支持对智能体迭代改进过程的科学研究和分析，有助于推动编码智能体优化能力的发展

Abstract: An important emerging application of coding agents is agent optimization: the iterative improvement of a target agent through edit-execute-evaluate cycles. Despite its relevance, the community lacks a systematic understanding of coding agent performance on this task. Agent optimization differs fundamentally from conventional software engineering: the target agent interleaves deterministic code with stochastic LLM completions, requiring structured capture of both intermediate reasoning and downstream execution outcomes. To address these challenges, we introduce VERO (Versioning, Rewards, and Observations), which provides (1) a reproducible evaluation harness with versioned agent snapshots, budget-controlled evaluation, and structured execution traces, and (2) a benchmark suite of target agents and tasks with reference evaluation procedures. Using VERO, we conduct an empirical study comparing optimizer configurations across tasks and analyzing which modifications reliably improve target agent performance. We release VERO to support research on agent optimization as a core capability for coding agents.

</details>


### [55] [Mapping the Landscape of Artificial Intelligence in Life Cycle Assessment Using Large Language Models](https://arxiv.org/abs/2602.22500)
*Anastasija Mensikova,Donna M. Rizzo,Kathryn Hinkelman*

Main category: cs.AI

TL;DR: 该研究利用大语言模型对AI与生命周期评估交叉领域的研究进行系统性综述，揭示AI技术在LCA中应用的增长趋势，特别是LLM驱动方法的兴起，并提出了一个结合传统文献综述与LLM文本挖掘的动态分析框架。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在生命周期评估中的应用近年来加速发展，但对该领域研究的全面综合仍然有限。本研究旨在填补这一空白，通过系统综述AI-LCA交叉领域的研究，识别当前趋势、新兴主题和未来方向。

Method: 采用大语言模型（LLMs）进行文本挖掘，结合传统文献综述技术，开发了一个动态有效的分析框架。该方法能够捕捉高层次研究趋势和细微的概念模式，支持大规模、可重复的领域综述。

Result: 分析显示：1）随着LCA研究的扩展，AI技术采用显著增长；2）研究趋势明显向LLM驱动方法转变；3）机器学习应用持续增加；4）AI方法与相应LCA阶段存在统计显著相关性。

Conclusion: LLM辅助方法学在支持大规模、可重复的领域综述方面具有潜力，同时为在AI技术快速发展背景下实现计算高效的LCA提供了评估路径。这项工作有助于LCA从业者将前沿工具和及时洞察融入环境评估，提升可持续发展决策的严谨性和质量。

Abstract: Integration of artificial intelligence (AI) into life cycle assessment (LCA) has accelerated in recent years, with numerous studies successfully adapting machine learning algorithms to support various stages of LCA. Despite this rapid development, comprehensive and broad synthesis of AI-LCA research remains limited. To address this gap, this study presents a detailed review of published work at the intersection of AI and LCA, leveraging large language models (LLMs) to identify current trends, emerging themes, and future directions. Our analyses reveal that as LCA research continues to expand, the adoption of AI technologies has grown dramatically, with a noticeable shift toward LLM-driven approaches, continued increases in ML applications, and statistically significant correlations between AI approaches and corresponding LCA stages. By integrating LLM-based text-mining methods with traditional literature review techniques, this study introduces a dynamic and effective framework capable of capturing both high-level research trends and nuanced conceptual patterns (themes) across the field. Collectively, these findings demonstrate the potential of LLM-assisted methodologies to support large-scale, reproducible reviews across broad research domains, while also evaluating pathways for computationally-efficient LCA in the context of rapidly developing AI technologies. In doing so, this work helps LCA practitioners incorporate state-of-the-art tools and timely insights into environmental assessments that can enhance the rigor and quality of sustainability-driven decisions and decision-making processes.

</details>


### [56] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Shijian Wang,Guanting Dong,Jiajie Jin,Hao Wang,Yinuo Wang,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: OmniGAIA是一个评估全模态智能体在视频、音频和图像多模态任务中深度推理和多轮工具执行能力的基准，OmniAtlas是基于工具集成推理范式的原生全模态基础智能体。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要局限于双模态交互（如视觉-语言），缺乏统一认知能力，无法满足通用AI助手的需求。需要构建能够处理全模态感知、深度推理和工具使用的智能体。

Method: 1. 提出OmniGAIA基准：通过全模态事件图方法构建复杂多跳查询，需要跨模态推理和外部工具集成。2. 提出OmniAtlas智能体：基于工具集成推理范式，具有主动全模态感知能力，使用后见之明引导的树探索策略合成训练轨迹，并通过OmniDPO进行细粒度错误校正。

Result: OmniGAIA基准能够评估全模态智能体的深度推理和工具执行能力。OmniAtlas智能体有效增强了现有开源模型的工具使用能力，为下一代原生全模态AI助手奠定了基础。

Conclusion: 这项工作向面向真实世界场景的下一代原生全模态AI助手迈出了一步，通过OmniGAIA基准和OmniAtlas智能体，解决了当前多模态模型在全模态感知、深度推理和工具使用方面的局限性。

Abstract: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world. However, current multi-modal LLMs are primarily confined to bi-modal interactions (e.g., vision-language), lacking the unified cognitive capabilities required for general AI assistants. To bridge this gap, we introduce OmniGAIA, a comprehensive benchmark designed to evaluate omni-modal agents on tasks necessitating deep reasoning and multi-turn tool execution across video, audio, and image modalities. Constructed via a novel omni-modal event graph approach, OmniGAIA synthesizes complex, multi-hop queries derived from real-world data that require cross-modal reasoning and external tool integration. Furthermore, we propose OmniAtlas, a native omni-modal foundation agent under tool-integrated reasoning paradigm with active omni-modal perception. Trained on trajectories synthesized via a hindsight-guided tree exploration strategy and OmniDPO for fine-grained error correction, OmniAtlas effectively enhances the tool-use capabilities of existing open-source models. This work marks a step towards next-generation native omni-modal AI assistants for real-world scenarios.

</details>


### [57] [General Agent Evaluation](https://arxiv.org/abs/2602.22953)
*Elron Bandel,Asaf Yehudai,Lilach Eden,Yehoshua Sagron,Yotam Perlitz,Elad Venezian,Natalia Razinkov,Natan Ergas,Shlomit Shachor Ifergan,Segev Shlomov,Michal Jacovi,Leshem Choshen,Liat Ein-Dor,Yoav Katz,Michal Shmueli-Scheuer*

Main category: cs.AI

TL;DR: 该论文提出了通用智能体评估的系统框架，建立了首个开放通用智能体排行榜，展示了通用智能体在不同环境中无需特定调优即可达到与专用智能体相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体主要面向特定领域，缺乏对通用智能体（能在陌生环境中执行任务而无需领域特定工程）性能的系统评估。现有基准测试假设领域特定集成，无法公平评估通用智能体。

Method: 提出了通用智能体评估的概念原则、实现智能体-基准集成的统一协议，以及用于通用智能体评估的实践框架Exgentic。在六个环境中对五个主流智能体实现进行了基准测试，建立了首个开放通用智能体排行榜。

Result: 实验表明通用智能体能够泛化到多样化环境，在没有任何环境特定调优的情况下，达到与领域特定智能体相当的性能水平。

Conclusion: 该研究为通用智能体评估建立了系统基础，通过发布评估协议、框架和排行榜，为通用智能体的系统研究提供了重要基础，展示了通用智能体在实际应用中的潜力。

Abstract: The promise of general-purpose agents - systems that perform tasks in unfamiliar environments without domain-specific engineering - remains largely unrealized. Existing agents are predominantly specialized, and while emerging implementations like OpenAI SDK Agent and Claude Code hint at broader capabilities, no systematic evaluation of their general performance has been pursued. Current agentic benchmarks assume domain-specific integration, encoding task information in ways that preclude fair evaluation of general agents. This paper frames general-agent evaluation as a first-class research objective. We propose conceptual principles for such evaluation, a Unified Protocol enabling agent-benchmark integration, and Exgentic - a practical framework for general agent evaluation. We benchmark five prominent agent implementations across six environments as the first Open General Agent Leaderboard. Our experiments show that general agents generalize across diverse environments, achieving performance comparable to domain-specific agents without any environment-specific tuning. We release our evaluation protocol, framework, and leaderboard to establish a foundation for systematic research on general-purpose agents.

</details>


### [58] [The Trinity of Consistency as a Defining Principle for General World Models](https://arxiv.org/abs/2602.23152)
*Jingxuan Wei,Siyuan Li,Yuhang Xu,Zheng Sun,Junjie Jiang,Hexuan Jin,Caijun Jia,Honghao He,Xinglong Xu,Xi bai,Chang Yu,Yumou Liu,Junnan Zhu,Xuanhe Zhou,Jintao Chen,Xiaobin Hu,Shancheng Pang,Bihui Yu,Ran He,Zhen Lei,Stan Z. Li,Conghui He,Shuicheng Yan,Cheng Tan*

Main category: cs.AI

TL;DR: 该论文提出了构建通用世界模型的三位一体一致性理论框架（模态、空间、时间一致性），并引入了CoW-Bench基准来评估视频生成模型和统一多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前领域缺乏定义通用世界模型必备特性的原则性理论框架。虽然Sora等视频生成模型展示了数据驱动缩放定律的潜力，统一多模态模型提供了有前景的架构范式，但仍需要系统化的理论基础来指导世界模型的发展。

Method: 提出了三位一体一致性理论框架：模态一致性作为语义接口、空间一致性作为几何基础、时间一致性作为因果引擎。通过这个三重视角系统回顾了多模态学习的演进。同时引入了CoW-Bench基准，专注于多帧推理和生成场景，为视频生成模型和UMMs提供统一评估协议。

Result: 建立了一个原则性的世界模型发展路径，阐明了当前系统的局限性以及未来进展的架构要求。通过三位一体一致性框架为通用世界模型提供了理论定义，并通过CoW-Bench基准实现了对现有模型的系统性评估。

Conclusion: 该论文为通用世界模型的发展提供了重要的理论框架和评估工具，通过三位一体一致性原则定义了世界模型的核心特性，为未来研究指明了方向，有助于推动人工智能向更接近通用智能的方向发展。

Abstract: The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.

</details>


### [59] [AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning](https://arxiv.org/abs/2602.23258)
*Yutong Wang,Siyuan Xiong,Xuebo Liu,Wenkang Zhou,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentDropoutV2是一个测试时修正或拒绝的剪枝框架，用于动态优化多智能体系统的信息流，无需重新训练，通过检索增强的修正器和故障驱动指示器池来纠正错误，不可修复的输出被剪枝以防止错误传播。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂推理方面表现出色，但存在个体参与者生成错误信息的级联影响问题。当前解决方案通常采用刚性结构工程或昂贵的微调，限制了系统的可部署性和适应性。

Method: 提出AgentDropoutV2框架，作为主动防火墙拦截智能体输出，使用检索增强的修正器基于故障驱动指示器池迭代纠正错误。该机制利用提炼的故障模式作为先验知识精确识别潜在错误，不可修复的输出被剪枝以防止错误传播，同时使用回退策略保持系统完整性。

Result: 在广泛的数学基准测试中，AgentDropoutV2显著提升了多智能体系统的任务性能，在数学基准测试上平均准确率提高了6.3个百分点。系统表现出强大的泛化能力和适应性，能够根据任务难度动态调整修正努力，并利用上下文感知指示器解决广泛的错误模式。

Conclusion: AgentDropoutV2是一个有效的测试时框架，能够动态优化多智能体系统的信息流，无需重新训练即可显著提升系统性能，同时保持强大的泛化能力和适应性。

Abstract: While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.

</details>


### [60] [LLM Novice Uplift on Dual-Use, In Silico Biology Tasks](https://arxiv.org/abs/2602.23329)
*Chen Bo Calvin Zhang,Christina Q. Knight,Nicholas Kruus,Jason Hausenloy,Pedro Medeiros,Nathaniel Li,Aiden Kim,Yury Orlovskiy,Coleman Breen,Bryce Cai,Jasper Götting,Andrew Bo Liu,Samira Nedungadi,Paula Rodriguez,Yannis Yiming He,Mohamed Shaaban,Zifan Wang,Seth Donoughe,Julian Michael*

Main category: cs.AI

TL;DR: LLM访问使生物安全相关任务的新手准确率提升4.16倍，甚至在某些任务上超越专家，但用户未能充分利用LLM的全部能力，且大多数参与者能轻易获取双重用途信息。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在生物学基准测试中表现良好，但尚不清楚它们是否能真正提升新手用户的能力（相比仅使用互联网资源）。这种不确定性对于理解科学加速和双重用途风险至关重要。

Method: 进行多模型、多基准的人类提升研究，比较有LLM访问权限的新手与仅使用互联网资源的新手在八个生物安全相关任务集上的表现。参与者有充足时间（最复杂的任务长达13小时）处理复杂问题。

Result: LLM访问提供了显著提升：有LLM的新手准确率比对照组高4.16倍（95% CI [2.63, 6.87]）。在四个有专家基准的任务中，有LLM的新手在三个任务上超越了专家。独立LLM通常超过LLM辅助的新手，表明用户未能充分利用LLM的全部能力。89.6%的参与者报告能轻松获取双重用途相关信息。

Conclusion: LLM显著提升了新手在原本需要专业训练的生物任务上的表现，强调需要持续、交互式的提升评估与传统基准测试并行，以应对双重用途风险。

Abstract: Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.

</details>


### [61] [Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks](https://arxiv.org/abs/2602.23330)
*Kunihiro Miyazaki,Takanobu Kawahara,Stephen Roberts,Stefan Zohren*

Main category: cs.AI

TL;DR: 本文提出了一种细粒度任务分解的多智能体LLM交易框架，相比传统粗粒度指令方法，在风险调整后收益方面表现更优，并通过投资分析与决策偏好的对齐提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前主流的多智能体金融交易系统通常采用模仿分析师和经理角色的抽象指令，但忽视了现实工作流程的复杂性，导致推理性能下降和决策透明度降低。

Method: 提出了一个多智能体LLM交易框架，将投资分析明确分解为细粒度任务而非粗粒度指令，使用日本股票数据（价格、财务报表、新闻、宏观信息）在泄漏控制的回测设置下进行评估。

Result: 实验结果显示，细粒度任务分解相比传统粗粒度设计显著提高了风险调整后收益；进一步分析表明，分析输出与下游决策偏好的对齐是系统性能的关键驱动因素；通过标准投资组合优化利用与股票指数的低相关性和系统输出的方差，获得了优越性能。

Conclusion: 这些发现为在实际交易系统中应用LLM智能体时的智能体结构和任务配置设计提供了重要参考，强调了细粒度任务分解和分析-决策对齐的重要性。

Abstract: The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.

</details>
