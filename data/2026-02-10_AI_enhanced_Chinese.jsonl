{"id": "2602.07125", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07125", "abs": "https://arxiv.org/abs/2602.07125", "authors": ["Jianrui Zhang", "Anirudh Sundara Rajan", "Brandon Han", "Soochahn Lee", "Sukanta Ganguly", "Yong Jae Lee"], "title": "Reasoning-Augmented Representations for Multimodal Retrieval", "comment": null, "summary": "Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry \"silent\" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.", "AI": {"tldr": "UMR\u63d0\u51fa\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u5316\u63a8\u7406\u63d0\u5347\u591a\u6a21\u6001\u68c0\u7d22\u6027\u80fd\uff0c\u5728M-BEIR\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u6f5c\u5728\u63a8\u7406\u7684\u67e5\u8be2\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u5982\u56fe\u50cf\u5305\u542b\"\u6c89\u9ed8\"\u8bc1\u636e\u6216\u67e5\u8be2\u8bed\u4e49\u4e0d\u660e\u786e\u65f6\uff0c\u5355\u6b21\u5d4c\u5165\u9700\u8981\u540c\u65f6\u5b8c\u6210\u63a8\u7406\u548c\u538b\u7f29\uff0c\u5bb9\u6613\u5bfc\u81f4\u865a\u5047\u7279\u5f81\u5339\u914d", "method": "\u63d0\u51fa\u6570\u636e\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u5728\u68c0\u7d22\u524d\u5916\u90e8\u5316\u63a8\u7406\uff1a1) \u4f7f\u7528\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bc6\u96c6\u6807\u6ce8\u8bed\u6599\u5e93\u4e2d\u7684\u89c6\u89c9\u8bc1\u636e\uff1b2) \u89e3\u6790\u67e5\u8be2\u4e2d\u7684\u6a21\u7cca\u591a\u6a21\u6001\u5f15\u7528\uff1b3) \u5c06\u5197\u957f\u6307\u4ee4\u91cd\u5199\u4e3a\u7b80\u6d01\u68c0\u7d22\u7ea6\u675f\uff1b4) \u5728\u589e\u5f3a\u7684\u8bed\u4e49\u5bc6\u96c6\u8868\u793a\u4e0a\u8bad\u7ec3\u68c0\u7d22\u5668", "result": "\u5728M-BEIR\u57fa\u51c6\u4e0a\uff0c\u63a8\u7406\u589e\u5f3a\u8bad\u7ec3\u65b9\u6cd5\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff1a\u8bed\u6599\u589e\u5f3a\u4e3b\u8981\u53d7\u76ca\u4e8e\u77e5\u8bc6\u5bc6\u96c6\u578b\u67e5\u8be2\uff0c\u800c\u67e5\u8be2\u589e\u5f3a\u5bf9\u7ec4\u5408\u4fee\u6539\u8bf7\u6c42\u81f3\u5173\u91cd\u8981", "conclusion": "\u901a\u8fc7\u5916\u90e8\u5316\u63a8\u7406\u5e76\u5c06\u68c0\u7d22\u5668\u8bad\u7ec3\u5728\u8bed\u4e49\u5bc6\u96c6\u8868\u793a\u4e0a\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u7684\u6f5c\u5728\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u5347\u68c0\u7d22\u6027\u80fd"}}
{"id": "2602.07207", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07207", "abs": "https://arxiv.org/abs/2602.07207", "authors": ["Bucher Sahyouni", "Matthew Vowels", "Liqun Chen", "Simon Hadfield"], "title": "Multimodal Enhancement of Sequential Recommendation", "comment": null, "summary": "We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted text and visual features. A frequency-based self-attention module additionally captures the short- and long-term user preferences. Across multiple Amazon datasets, MuSTRec demonstrates superior performance (up to 33.5% improvement) over multimodal and sequential state-of-the-art baselines. Finally, we detail some interesting facets of this new recommendation paradigm. These include the need for a new data partitioning regime, and a demonstration of how integrating user embeddings into sequential recommendation leads to drastically increased short-term metrics (up to 200% improvement) on smaller datasets. Our code is availabe at https://anonymous.4open.science/r/MuSTRec-D32B/ and will be made publicly available.", "AI": {"tldr": "MuSTRec\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u6a21\u6001\u5e8f\u5217\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u7684\u7269\u54c1\u56fe\u6765\u6355\u6349\u8de8\u7269\u54c1\u76f8\u4f3c\u6027\u548c\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\uff0c\u5e76\u5728\u591a\u4e2a\u4e9a\u9a6c\u900a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u5355\u72ec\u5904\u7406\u591a\u6a21\u6001\u7279\u5f81\u6216\u5e8f\u5217\u4fe1\u606f\uff0c\u7f3a\u4e4f\u5c06\u4e24\u8005\u7edf\u4e00\u8d77\u6765\u7684\u6846\u67b6\u3002MuSTRec\u65e8\u5728\u6574\u5408\u591a\u6a21\u6001\u548c\u5e8f\u5217\u63a8\u8350\u8303\u5f0f\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7528\u6237\u504f\u597d\u3002", "method": "MuSTRec\u901a\u8fc7\u63d0\u53d6\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u6784\u5efa\u7269\u54c1-\u7269\u54c1\u56fe\u6765\u6355\u6349\u8de8\u7269\u54c1\u76f8\u4f3c\u6027\u548c\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u9891\u7387\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u6765\u6355\u6349\u7528\u6237\u7684\u77ed\u671f\u548c\u957f\u671f\u504f\u597d\u3002", "result": "\u5728\u591a\u4e2a\u4e9a\u9a6c\u900a\u6570\u636e\u96c6\u4e0a\uff0cMuSTRec\u76f8\u6bd4\u73b0\u6709\u7684\u591a\u6a21\u6001\u548c\u5e8f\u5217\u63a8\u8350\u57fa\u51c6\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff08\u6700\u9ad833.5%\u7684\u6539\u8fdb\uff09\uff0c\u7279\u522b\u662f\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u6574\u5408\u7528\u6237\u5d4c\u5165\u540e\uff0c\u77ed\u671f\u6307\u6807\u63d0\u5347\u9ad8\u8fbe200%\u3002", "conclusion": "MuSTRec\u6210\u529f\u7edf\u4e00\u4e86\u591a\u6a21\u6001\u548c\u5e8f\u5217\u63a8\u8350\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u8fd9\u4e00\u65b0\u63a8\u8350\u8303\u5f0f\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u6570\u636e\u5212\u5206\u673a\u5236\u9700\u6c42\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.07208", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07208", "abs": "https://arxiv.org/abs/2602.07208", "authors": ["Bucher Sahyouni", "Matthew Vowels", "Liqun Chen", "Simon Hadfield"], "title": "Sequences as Nodes for Contrastive Multimodal Graph Recommendation", "comment": null, "summary": "To tackle cold-start and data sparsity issues in recommender systems, numerous multimodal, sequential, and contrastive techniques have been proposed. While these augmentations can boost recommendation performance, they tend to add noise and disrupt useful semantics. To address this, we propose MuSICRec (Multimodal Sequence-Item Contrastive Recommender), a multi-view graph-based recommender that combines collaborative, sequential, and multimodal signals. We build a sequence-item (SI) view by attention pooling over the user's interacted items to form sequence nodes. We propagate over the SI graph, obtaining a second view organically as an alternative to artificial data augmentation, while simultaneously injecting sequential context signals. Additionally, to mitigate modality noise and align the multimodal information, the contribution of text and visual features is modulated according to an ID-guided gate.\n  We evaluate under a strict leave-two-out split against a broad range of sequential, multimodal, and contrastive baselines. On the Amazon Baby, Sports, and Electronics datasets, MuSICRec outperforms state-of-the-art baselines across all model types. We observe the largest gains for short-history users, mitigating sparsity and cold-start challenges. Our code is available at https://anonymous.4open.science/r/MuSICRec-3CEE/ and will be made publicly available.", "AI": {"tldr": "MuSICRec\u662f\u4e00\u4e2a\u591a\u89c6\u56fe\u56fe\u63a8\u8350\u7cfb\u7edf\uff0c\u7ed3\u5408\u534f\u540c\u8fc7\u6ee4\u3001\u5e8f\u5217\u548c\u591a\u6a21\u6001\u4fe1\u53f7\uff0c\u901a\u8fc7\u5e8f\u5217-\u9879\u76ee\u89c6\u56fe\u548cID\u5f15\u5bfc\u95e8\u63a7\u7f13\u89e3\u51b7\u542f\u52a8\u548c\u6570\u636e\u7a00\u758f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u867d\u7136\u4f7f\u7528\u591a\u6a21\u6001\u3001\u5e8f\u5217\u548c\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\u6765\u7f13\u89e3\u51b7\u542f\u52a8\u548c\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u4f46\u8fd9\u4e9b\u589e\u5f3a\u65b9\u6cd5\u5f80\u5f80\u4f1a\u5f15\u5165\u566a\u58f0\u5e76\u7834\u574f\u6709\u7528\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u6784\u5efa\u5e8f\u5217-\u9879\u76ee\u89c6\u56fe\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6c60\u5316\u7528\u6237\u4ea4\u4e92\u9879\u76ee\u5f62\u6210\u5e8f\u5217\u8282\u70b9\uff1b\u5728SI\u56fe\u4e0a\u4f20\u64ad\uff0c\u83b7\u5f97\u7b2c\u4e8c\u89c6\u56fe\u4f5c\u4e3a\u4eba\u5de5\u6570\u636e\u589e\u5f3a\u7684\u66ff\u4ee3\u65b9\u6848\uff1b\u4f7f\u7528ID\u5f15\u5bfc\u95e8\u63a7\u8c03\u8282\u6587\u672c\u548c\u89c6\u89c9\u7279\u5f81\u7684\u8d21\u732e\uff0c\u51cf\u8f7b\u6a21\u6001\u566a\u58f0\u5e76\u5bf9\u9f50\u591a\u6a21\u6001\u4fe1\u606f\u3002", "result": "\u5728Amazon Baby\u3001Sports\u548cElectronics\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e25\u683c\u7684leave-two-out\u5206\u5272\uff0cMuSICRec\u5728\u6240\u6709\u6a21\u578b\u7c7b\u578b\u4e2d\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u5386\u53f2\u8bb0\u5f55\u8f83\u77ed\u7684\u7528\u6237\u7684\u63d0\u5347\u6548\u679c\u6700\u660e\u663e\u3002", "conclusion": "MuSICRec\u901a\u8fc7\u591a\u89c6\u56fe\u56fe\u7ed3\u6784\u548cID\u5f15\u5bfc\u95e8\u63a7\u673a\u5236\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u51b7\u542f\u52a8\u548c\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u7279\u522b\u9002\u5408\u5386\u53f2\u4ea4\u4e92\u8f83\u5c11\u7684\u7528\u6237\u3002"}}
{"id": "2602.07297", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07297", "abs": "https://arxiv.org/abs/2602.07297", "authors": ["Taehee Jeong", "Xingzhe Zhao", "Peizu Li", "Markus Valvur", "Weihua Zhao"], "title": "Progressive Searching for Retrieval in RAG", "comment": null, "summary": "Retrieval Augmented Generation (RAG) is a promising technique for mitigating two key limitations of large language models (LLMs): outdated information and hallucinations. RAG system stores documents as embedding vectors in a database. Given a query, search is executed to find the most related documents. Then, the topmost matching documents are inserted into LLMs' prompt to generate a response. Efficient and accurate searching is critical for RAG to get relevant information. We propose a cost-effective searching algorithm for retrieval process. Our progressive searching algorithm incrementally refines the candidate set through a hierarchy of searches, starting from low-dimensional embeddings and progressing into a higher, target-dimensionality. This multi-stage approach reduces retrieval time while preserving the desired accuracy. Our findings demonstrate that progressive search in RAG systems achieves a balance between dimensionality, speed, and accuracy, enabling scalable and high-performance retrieval even for large databases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eRAG\u7cfb\u7edf\u7684\u6e10\u8fdb\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ece\u4f4e\u7ef4\u5d4c\u5165\u9010\u6b65\u7ec6\u5316\u5230\u9ad8\u7ef4\u7684\u591a\u9636\u6bb5\u641c\u7d22\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u68c0\u7d22\u65f6\u95f4", "motivation": "RAG\u7cfb\u7edf\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u641c\u7d22\u6765\u83b7\u53d6\u76f8\u5173\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u578b\u6570\u636e\u5e93\u4e2d\u9762\u4e34\u68c0\u7d22\u6548\u7387\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0f\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u641c\u7d22\u9010\u6b65\u7ec6\u5316\u5019\u9009\u96c6\uff1a\u4ece\u4f4e\u7ef4\u5d4c\u5165\u5f00\u59cb\uff0c\u9010\u6b65\u8fc7\u6e21\u5230\u76ee\u6807\u9ad8\u7ef4\u7a7a\u95f4\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u65b9\u6cd5\u51cf\u5c11\u68c0\u7d22\u65f6\u95f4", "result": "\u6e10\u8fdb\u5f0f\u641c\u7d22\u5728RAG\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u7ef4\u5ea6\u3001\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u5373\u4f7f\u5728\u5927\u578b\u6570\u636e\u5e93\u4e2d\u4e5f\u80fd\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u68c0\u7d22", "conclusion": "\u6e10\u8fdb\u5f0f\u641c\u7d22\u7b97\u6cd5\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3LLMs\u7684\u8fc7\u65f6\u4fe1\u606f\u548c\u5e7b\u89c9\u95ee\u9898"}}
{"id": "2602.06993", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.06993", "abs": "https://arxiv.org/abs/2602.06993", "authors": ["Shashank"], "title": "Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts", "comment": "9 pages. Code (APN implementation in nanoGPT transformer): https://github.com/shankch/nanoGPT-apn (baseline: https://github.com/karpathy/nanoGPT) Data prep: https://github.com/karpathy/nanoGPT/tree/master/data/shakespeare_char and https://github.com/karpathy/nanoGPT/tree/master/data/shakespeare", "summary": "Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights.\n  We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface.\n  This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.", "AI": {"tldr": "APN\uff08Attractor Patch Networks\uff09\u4f5c\u4e3aTransformer FFN\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u8def\u7531\u9009\u62e9\u5c11\u91cf\u4e13\u5bb6\u8865\u4e01\uff0c\u5b9e\u73b0\u6761\u4ef6\u5316\u3001\u4e0a\u4e0b\u6587\u7279\u5316\u7684\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfTransformer\u7684FFN\u5b58\u5728\u4e24\u4e2a\u5b9e\u9645\u95ee\u9898\uff1a1\uff09\u5bf9\u6240\u6709token\u4f7f\u7528\u76f8\u540c\u8ba1\u7b97\u91cf\uff0c\u65e0\u6cd5\u6839\u636e\u4e0a\u4e0b\u6587\u7ed3\u6784\u7075\u6d3b\u5206\u914d\u5bb9\u91cf\uff1b2\uff09\u6301\u7eed\u5b66\u4e60\u65f6\u5e7f\u6cdb\u5171\u4eab\u7684\u6743\u91cd\u66f4\u65b0\u4f1a\u5bfc\u81f4\u5e72\u6270\u95ee\u9898\u3002", "method": "\u63d0\u51faAPN\u4f5c\u4e3aTransformer FFN\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\u3002\u5b83\u5305\u542b\u4e00\u7ec4\u8865\u4e01\u4e13\u5bb6\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u8def\u7531\u5668\u4e3a\u6bcf\u4e2atoken\u9009\u62e9top-k\u4e2a\u8865\u4e01\uff0c\u6bcf\u4e2a\u9009\u4e2d\u7684\u8865\u4e01\u57fa\u4e8e\u7d27\u51d1\u4ee3\u7801\u751f\u6210\u4f4e\u79e9\u6b8b\u5dee\u66f4\u65b0\uff0c\u5b9e\u73b0\u6761\u4ef6\u5316\u3001\u4e0a\u4e0b\u6587\u7279\u5316\u7684\u975e\u7ebf\u6027\u53d8\u6362\u3002", "result": "\u5728\u5b57\u7b26\u7ea7\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0cAPN\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u56f0\u60d1\u5ea6\uff084.57 vs 4.32 PPL\uff09\u3002\u5728\u6301\u7eed\u9002\u5e94\u5b9e\u9a8c\u4e2d\uff0c\u5f53\u9002\u5e94\u5230\u504f\u79fb\u9886\u57df\u65f6\uff0cAPN\u76f8\u6bd4\u5bc6\u96c6FFN\u57fa\u7ebf\u7684\u5168\u5c40\u5fae\u8c03\uff0c\u5728\u539f\u59cb\u9886\u57df\u4e0a\u83b7\u5f972.6\u500d\u66f4\u597d\u7684\u4fdd\u7559\u80fd\u529b\uff0811.1 vs 29.4 PPL\uff09\uff0c\u5728\u65b0\u9886\u57df\u4e0a\u83b7\u5f972.8\u500d\u66f4\u597d\u7684\u9002\u5e94\u80fd\u529b\uff086.4 vs 17.8 PPL\uff09\u3002", "conclusion": "APN\u4f5c\u4e3a\u4e00\u79cd\u67b6\u6784\u539f\u8bed\uff0c\u4e0d\u4ec5\u4fdd\u6301\u4e86Transformer\u7684\u6807\u51c6\u63a5\u53e3\uff0c\u8fd8\u901a\u8fc7\u6761\u4ef6\u5316\u3001\u4e0a\u4e0b\u6587\u7279\u5316\u7684\u53d8\u6362\u673a\u5236\uff0c\u5728\u4fdd\u6301\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u89e3\u51b3FFN\u7684\u5bc6\u96c6\u6027\u548c\u5168\u5c40\u5171\u4eab\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.07006", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07006", "abs": "https://arxiv.org/abs/2602.07006", "authors": ["Alokesh Manna", "Neil Spencer", "Dipak K. Dey"], "title": "Scalable spatial point process models for forensic footwear analysis", "comment": null, "summary": "Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u6765\u91cf\u5316\u978b\u5370\u4e2d\"\u5076\u7136\u7279\u5f81\"\u7684\u7a00\u6709\u6027\uff0c\u901a\u8fc7\u6f5c\u5728\u9ad8\u65af\u6a21\u578b\u548c\u7a7a\u95f4\u53d8\u5316\u7cfb\u6570\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6cd5\u533b\u978b\u5370\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u978b\u5370\u8bc1\u636e\u5728\u6cd5\u533b\u8c03\u67e5\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ec5\u5339\u914d\u978b\u7684\u54c1\u724c\u548c\u578b\u53f7\u4e0d\u8db3\u4ee5\u786e\u5b9a\u5acc\u7591\u4eba\u7684\u978b\u5b50\uff0c\u56e0\u4e3a\u540c\u4e00\u578b\u53f7\u53ef\u80fd\u751f\u4ea7\u6570\u5343\u53cc\u3002\u9700\u8981\u91cf\u5316\u978b\u5e95\u4e0a\u56e0\u78e8\u635f\u5f62\u6210\u7684\"\u5076\u7136\u7279\u5f81\"\uff08\u5982\u5212\u75d5\u3001\u5207\u53e3\uff09\u7684\u7a00\u6709\u6027\uff0c\u4ee5\u51c6\u786e\u8bc4\u4f30\u8bc1\u636e\u5f3a\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u91c7\u7528\u6f5c\u5728\u9ad8\u65af\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5d4c\u5957\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u5b9e\u73b0\u5927\u89c4\u6a21\u6ce8\u91ca\u978b\u5370\u6570\u636e\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u53d8\u5316\u7cfb\u6570\u6765\u5efa\u6a21\u978b\u5e95\u82b1\u7eb9\u4e0e\u5076\u7136\u7279\u5f81\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u6709\u6240\u63d0\u5347\uff0c\u589e\u5f3a\u4e86\u6cd5\u533b\u978b\u5370\u5206\u6790\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7edf\u8ba1\u5efa\u6a21\u91cf\u5316\u978b\u5370\u5076\u7136\u7279\u5f81\u7684\u7a00\u6709\u6027\uff0c\u4e3a\u6cd5\u533b\u8c03\u67e5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u53ef\u9760\u7684\u8bc1\u636e\u8bc4\u4f30\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u978b\u5370\u8bc1\u636e\u5728\u6cd5\u5ead\u4e0a\u7684\u79d1\u5b66\u4ef7\u503c\u3002"}}
{"id": "2602.07298", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07298", "abs": "https://arxiv.org/abs/2602.07298", "authors": ["Benyu Zhang", "Qiang Zhang", "Jianpeng Cheng", "Hong-You Chen", "Qifei Wang", "Wei Sun", "Shen Li", "Jia Li", "Jiahao Wu", "Xiangjun Fan", "Hong Yan"], "title": "Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation", "comment": null, "summary": "Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6846\u67b6\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u9996\u6b21\u5c55\u793a\u4e86LLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e42\u5f8b\u7f29\u653e\u89c4\u5f8b\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u63a8\u8350\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9884\u6d4b\u7684\u7f29\u653e\u89c4\u5f8b\u963b\u788d\u4e86\u5176\u53d1\u5c55\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u53ef\u80fd\u6e90\u4e8e\u5148\u524d\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u539f\u59cb\u7528\u6237\u4ea4\u4e92\u6570\u636e\u7684\u566a\u58f0\u3001\u504f\u89c1\u548c\u4e0d\u5b8c\u6574\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u4e3aLLM\u521b\u5efa\u7cbe\u5fc3\u7b56\u5212\u7684\u6559\u5b66\u8bfe\u7a0b\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u907f\u514d\u4e86\u539f\u59cb\u6570\u636e\u7684\u95ee\u9898\u3002\u4f7f\u7528\u8be5\u6846\u67b6\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6807\u51c6\u5e8f\u5217\u6a21\u578b\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4e0b\u6e38\u6392\u5e8f\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff08SasRec\u5728recall@100\u4e0a\u63d0\u5347130%\uff09\u3002\u9996\u6b21\u5b9e\u8bc1\u5c55\u793a\u4e86LLM\u5728\u9ad8\u8d28\u91cf\u63a8\u8350\u7279\u5b9a\u6570\u636e\u4e0a\u7684\u7a33\u5065\u5e42\u5f8b\u7f29\u653e\uff0c\u591a\u4e2a\u5408\u6210\u6570\u636e\u6a21\u6001\u4e0a\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u56f0\u60d1\u5ea6\u964d\u4f4e\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u79cd\u53ef\u9760\u6269\u5c55\u63a8\u8350\u9886\u57dfLLM\u80fd\u529b\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u5c06\u7814\u7a76\u91cd\u70b9\u4ece\u7f13\u89e3\u6570\u636e\u7f3a\u9677\u8f6c\u5411\u5229\u7528\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684LLM\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u9884\u6d4b\u7684\u7f29\u653e\u89c4\u5f8b\u3002"}}
{"id": "2602.07030", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07030", "abs": "https://arxiv.org/abs/2602.07030", "authors": ["Young Jin Ahn", "Yiyang Du", "Zheyuan Zhang", "Haisen Kang"], "title": "Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model", "comment": null, "summary": "Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51faNeural Sabermetrics with World Model\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u68d2\u7403\u9010\u7403\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u9884\u6d4b\u6bd4\u8d5b\u7684\u591a\u65b9\u9762\u6f14\u5316", "motivation": "\u4f20\u7edf\u68d2\u7403\u7edf\u8ba1\u65b9\u6cd5\u867d\u7136\u80fd\u603b\u7ed3\u957f\u671f\u6bd4\u8d5b\u5386\u53f2\uff0c\u4f46\u65e0\u6cd5\u5efa\u7acb\u9010\u7403\u751f\u6210\u7684\u6bd4\u8d5b\u6a21\u578b\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5c40\u9650\u4e8e\u5355\u6b65\u9884\u6d4b\u6216\u4e8b\u540e\u5206\u6790", "method": "\u5c06\u68d2\u7403\u6bd4\u8d5b\u5efa\u6a21\u4e3a\u4e8b\u4ef6\u7684\u957f\u81ea\u56de\u5f52\u5e8f\u5217\uff0c\u5728\u8d85\u8fc710\u5e74\u7684MLB\u8ffd\u8e2a\u6570\u636e\u4e0a\u6301\u7eed\u9884\u8bad\u7ec3\u5355\u4e2aLLM\uff0c\u5305\u542b700\u4e07\u6b21\u6295\u7403\u5e8f\u5217\u548c\u7ea630\u4ebf\u4e2atoken", "result": "\u6a21\u578b\u5728\u5206\u5e03\u5185\u5e38\u89c4\u8d5b\u548c\u5206\u5e03\u5916\u5b63\u540e\u8d5b\u6570\u636e\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff1a\u6b63\u786e\u9884\u6d4b\u7ea664%\u7684\u4e0b\u4e00\u6b21\u6295\u7403\u548c78%\u7684\u51fb\u7403\u5458\u6325\u68d2\u51b3\u7b56", "conclusion": "LLM\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u4f53\u80b2\u4e16\u754c\u6a21\u578b\uff0c\u4e3a\u68d2\u7403\u6bd4\u8d5b\u63d0\u4f9b\u7edf\u4e00\u7684\u751f\u6210\u5f0f\u5efa\u6a21\u6846\u67b6"}}
{"id": "2602.07008", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07008", "abs": "https://arxiv.org/abs/2602.07008", "authors": ["Ruoyu Chen", "Shangquan Sun", "Xiaoqing Guo", "Sanyi Zhang", "Kangwei Liu", "Shiming Liu", "Zhangcheng Wang", "Qunli Zhang", "Hua Zhang", "Xiaochun Cao"], "title": "Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making", "comment": null, "summary": "Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u6a21\u578b\u51b3\u7b56\u8bc1\u636e\u4e0e\u4eba\u7c7b\u9884\u671f\u533a\u57df\u7684\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u548c\u51b3\u7b56\u5408\u7406\u6027\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u4ec5\u63d0\u4f9b\u7c7b\u522b\u6807\u7b7e\uff0c\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u6377\u5f84\u76f8\u5173\u6027\u800c\u975e\u9884\u671f\u8bc1\u636e\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u3002\u4eba\u7c7b\u5148\u9a8c\u53ef\u4ee5\u5e2e\u52a9\u7ea6\u675f\u8fd9\u79cd\u884c\u4e3a\uff0c\u4f46\u6a21\u578b\u5b66\u4e60\u5230\u7684\u8868\u793a\u5f80\u5f80\u4e0e\u4eba\u7c7b\u611f\u77e5\u5b58\u5728\u5dee\u5f02\uff0c\u5bf9\u9f50\u6a21\u578b\u4e0e\u4eba\u7c7b\u5148\u9a8c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\uff1a\u5c06\u4eba\u7c7b\u5148\u9a8c\u7f16\u7801\u4e3a\u6a21\u578b\u9884\u671f\u4f9d\u8d56\u7684\u8f93\u5165\u533a\u57df\uff08\u5982\u8fb9\u754c\u6846\uff09\uff0c\u5229\u7528\u9ad8\u4fdd\u771f\u5ea6\u7684\u57fa\u4e8e\u5b50\u96c6\u9009\u62e9\u7684\u5f52\u56e0\u65b9\u6cd5\u5728\u8bad\u7ec3\u4e2d\u66b4\u9732\u6a21\u578b\u7684\u51b3\u7b56\u8bc1\u636e\u3002\u5f53\u5f52\u56e0\u533a\u57df\u663e\u8457\u504f\u79bb\u5148\u9a8c\u533a\u57df\u65f6\uff0c\u60e9\u7f5a\u5bf9\u975e\u5148\u9a8c\u8bc1\u636e\u7684\u4f9d\u8d56\uff0c\u4fc3\u4f7f\u6a21\u578b\u5c06\u5176\u5f52\u56e0\u8f6c\u5411\u9884\u671f\u533a\u57df\uff0c\u901a\u8fc7\u8bad\u7ec3\u76ee\u6807\u65bd\u52a0\u4eba\u7c7b\u5148\u9a8c\u8bf1\u5bfc\u7684\u5f52\u56e0\u7ea6\u675f\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u70b9\u51fb\u51b3\u7b56\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u6db5\u76d6\u4f20\u7edf\u5206\u7c7b\u548c\u81ea\u56de\u5f52\u751f\u6210\u8bbe\u7f6e\u3002\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u4e00\u81f4\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u51b3\u7b56\u5408\u7406\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5f52\u56e0\u7684\u4eba\u7c7b\u5148\u9a8c\u5bf9\u9f50\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ea6\u675f\u6a21\u578b\u4f9d\u8d56\u9884\u671f\u8bc1\u636e\u8fdb\u884c\u51b3\u7b56\uff0c\u63d0\u9ad8\u6a21\u578b\u53ef\u9760\u6027\u548c\u51b3\u7b56\u5408\u7406\u6027\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u8bbe\u7f6e\u4e2d\u5747\u8868\u73b0\u51fa\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2602.07559", "categories": ["cs.AI", "cs.CC", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.07559", "abs": "https://arxiv.org/abs/2602.07559", "authors": ["Kaleem Ullah Qasim", "Jiashu Zhang", "Hao Li", "Muhammad Kafeel Shaheen"], "title": "VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning", "comment": "13 pages", "summary": "Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving \"verification by construction\" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.", "AI": {"tldr": "Verify-RL\u6846\u67b6\u5229\u7528\u7b26\u53f7\u5fae\u5206\u89c4\u5219\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u6570\u5b66\u95ee\u9898\u5206\u89e3\uff0c\u786e\u4fdd\u5b50\u95ee\u9898\u66f4\u7b80\u5355\u3001\u89e3\u51b3\u5b50\u95ee\u9898\u6709\u52a9\u4e8e\u7236\u4efb\u52a1\uff0c\u4e14\u5206\u89e3\u5173\u7cfb\u6709\u6570\u5b66\u57fa\u7840\uff0c\u76f8\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u95ee\u9898\u5206\u89e3\u65b9\u6cd5\u901a\u5e38\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u5b50\u95ee\u9898\u66f4\u7b80\u5355\u3001\u89e3\u51b3\u5b50\u95ee\u9898\u6709\u52a9\u4e8e\u7236\u4efb\u52a1\uff0c\u4e14\u5206\u89e3\u5173\u7cfb\u7f3a\u4e4f\u6570\u5b66\u57fa\u7840\u3002\u9700\u8981\u4e00\u79cd\u53ef\u9a8c\u8bc1\u7684\u5206\u89e3\u6846\u67b6\u6765\u786e\u4fdd\u8fd9\u4e9b\u5173\u952e\u5c5e\u6027\u3002", "method": "\u63d0\u51faVerify-RL\u6846\u67b6\uff0c\u5229\u7528\u7b26\u53f7\u5fae\u5206\u89c4\u5219\u8fdb\u884c\u5206\u89e3\uff1a\u5fae\u79ef\u5206\u89c4\u5219\u660e\u786e\u5b9a\u4e49\u4e86\u8868\u8fbe\u5f0f\u5982\u4f55\u7b80\u5316\u4e3a\u66f4\u7b80\u5355\u7684\u7ec4\u4ef6\uff0c\u5e76\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6027\u8d28\u3002\u6bcf\u4e2a\u7236-\u5b50\u5206\u89e3\u6ee1\u8db3\u4e09\u4e2a\u53ef\u9a8c\u8bc1\u6761\u4ef6\uff1a\u7ed3\u6784\u590d\u6742\u5ea6\u4e25\u683c\u9012\u51cf\u3001\u89e3\u5305\u542b\u6027\u3001\u5f62\u5f0f\u89c4\u5219\u63a8\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6d88\u9664\u65e0\u6548\u5206\u89e3\u5e26\u6765\u663e\u8457\u6536\u76ca\uff1a\u6700\u56f0\u96be\u95ee\u9898\u7684\u51c6\u786e\u7387\u4ece32%\u7ffb\u500d\u81f368%\uff0c\u6574\u4f53\u76f8\u5bf9\u6539\u8fdb\u8fbe40%\u3002\u53ef\u9a8c\u8bc1\u5206\u89e3\u76f8\u6bd4\u542f\u53d1\u5f0f\u65b9\u6cd5\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u7b26\u53f7\u5fae\u5206\u4e3a\u6570\u5b66\u95ee\u9898\u5206\u89e3\u63d0\u4f9b\u4e86\u81ea\u7136\u7684\u53ef\u9a8c\u8bc1\u7ed3\u6784\uff0cVerify-RL\u6846\u67b6\u901a\u8fc7\u786e\u4fdd\u5206\u89e3\u7684\u6570\u5b66\u6b63\u786e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u7684\u80fd\u529b\u3002"}}
{"id": "2602.07034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07034", "abs": "https://arxiv.org/abs/2602.07034", "authors": ["Jinxiu Qu", "Zirui Tang", "Hongzhang Huang", "Boyu Niu", "Wei Zhou", "Jiannan Wang", "Yitong Song", "Guoliang Li", "Xuanhe Zhou", "Fan Wu"], "title": "ST-Raptor: An Agentic System for Semi-Structured Table QA", "comment": null, "summary": "Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.", "AI": {"tldr": "ST-Raptor\u662f\u4e00\u4e2a\u7528\u4e8e\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7f16\u8f91\u3001\u6811\u72b6\u7ed3\u6784\u5efa\u6a21\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u67e5\u8be2\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4fe1\u606f\u4e22\u5931\u548c\u5e03\u5c40\u5904\u7406\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u9700\u8981\u7cbe\u786e\u63d0\u53d6\u5355\u5143\u683c\u5185\u5bb9\u548c\u4f4d\u7f6e\uff0c\u5e76\u6062\u590d\u8868\u683c\u5e03\u5c40\u4e2d\u9690\u542b\u7684\u903b\u8f91\u7ed3\u6784\u3001\u5c42\u6b21\u5173\u7cfb\u548c\u8bed\u4e49\u5173\u8054\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u3001\u590d\u6742\u5e03\u5c40\u5904\u7406\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u4eba\u5de5\u89e3\u91ca\u53c8\u8017\u65f6\u8017\u529b\u3002", "method": "ST-Raptor\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u5206\u6790\u73af\u5883\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u8f91\u3001\u6811\u72b6\u7ed3\u6784\u5efa\u6a21\u548c\u667a\u80fd\u4f53\u9a71\u52a8\u67e5\u8be2\u6765\u89e3\u51b3\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u95ee\u9898\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cST-Raptor\u5728\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ST-Raptor\u901a\u8fc7\u521b\u65b0\u7684\u4ea4\u4e92\u5f0f\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u590d\u6742\u5e03\u5c40\u5904\u7406\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07011", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07011", "abs": "https://arxiv.org/abs/2602.07011", "authors": ["Zhuonan Wang", "Zhenxuan Fan", "Siwen Tan", "Yu Zhong", "Yuqian Yuan", "Haoyuan Li", "Hao Jiang", "Wenqiao Zhang", "Feifei Shao", "Hongwei Wang", "Jun Xiao"], "title": "MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation", "comment": "9 pages, 5 figures", "summary": "As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.", "AI": {"tldr": "MAU-Set\u6570\u636e\u96c6\u548cMAU-GPT\u6a21\u578b\u7528\u4e8e\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u9886\u57df\u6570\u636e\u96c6\u548c\u65b0\u578bAMoE-LoRA\u673a\u5236\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b", "motivation": "\u5de5\u4e1a\u5236\u9020\u89c4\u6a21\u6269\u5927\u9700\u8981\u81ea\u52a8\u5316\u7ec6\u7c92\u5ea6\u4ea7\u54c1\u56fe\u50cf\u5206\u6790\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8986\u76d6\u4e0d\u8db3\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u5904\u7406\u591a\u6837\u590d\u6742\u7684\u5f02\u5e38\u6a21\u5f0f", "method": "1) \u5f15\u5165MAU-Set\u591a\u7c7b\u578b\u5de5\u4e1a\u5f02\u5e38\u7406\u89e3\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u4e2a\u5de5\u4e1a\u9886\u57df\u548c\u5206\u5c42\u4efb\u52a1\u7ed3\u6784\uff1b2) \u5efa\u7acb\u4e25\u683c\u8bc4\u4f30\u534f\u8bae\uff1b3) \u63d0\u51faMAU-GPT\u9886\u57df\u9002\u5e94\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u9896\u7684AMoE-LoRA\u673a\u5236\u7edf\u4e00\u5f02\u5e38\u611f\u77e5\u548c\u901a\u7528\u4e13\u5bb6\u9002\u5e94", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eMAU-GPT\u5728\u6240\u6709\u9886\u57df\u5747\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u81ea\u52a8\u5316\u5de5\u4e1a\u68c0\u6d4b\u7684\u5f3a\u5927\u6f5c\u529b", "conclusion": "MAU-Set\u6570\u636e\u96c6\u548cMAU-GPT\u6a21\u578b\u4e3a\u89e3\u51b3\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u96c6\u8986\u76d6\u548c\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\u7684\u81ea\u52a8\u5316\u53d1\u5c55"}}
{"id": "2602.07603", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.07603", "abs": "https://arxiv.org/abs/2602.07603", "authors": ["Woojin Cho", "Junghwan Park"], "title": "Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines", "comment": null, "summary": "Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.", "AI": {"tldr": "ELM-INR\uff1a\u4e00\u79cd\u514d\u53cd\u5411\u4f20\u64ad\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8ELM\u95ed\u5f0f\u89e3\u548c\u5206\u533a\u7edf\u4e00\u5b9e\u73b0\u5feb\u901f\u7a33\u5065\u91cd\u5efa\uff0c\u5e76\u63d0\u51faBEAM\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u7b56\u7565\u5e73\u8861\u9891\u8c31\u590d\u6742\u5ea6", "motivation": "\u4f20\u7edfINR\u8bad\u7ec3\u4f9d\u8d56\u8fed\u4ee3\u53cd\u5411\u4f20\u64ad\uff0c\u5728\u5904\u7406\u975e\u5747\u5300\u9891\u8c31\u5185\u5bb9\u65f6\u53d7\u5230\u9891\u8c31\u504f\u5dee\u9650\u5236\uff0c\u9700\u8981\u66f4\u5feb\u901f\u3001\u7a33\u5065\u7684\u91cd\u5efa\u65b9\u6cd5", "method": "1. \u5c06\u57df\u5206\u89e3\u4e3a\u91cd\u53e0\u5b50\u57df\uff1b2. \u6bcf\u4e2a\u5b50\u57df\u4f7f\u7528\u6781\u9650\u5b66\u4e60\u673a\uff08ELM\uff09\u8fdb\u884c\u95ed\u5f0f\u62df\u5408\uff1b3. \u901a\u8fc7\u5206\u533a\u7edf\u4e00\u7ec4\u5408\u5c40\u90e8\u9884\u6d4b\u5668\uff1b4. \u63d0\u51faBEAM\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u7b56\u7565\u5e73\u8861\u9891\u8c31\u590d\u6742\u5ea6", "result": "ELM-INR\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u6570\u503c\u7a33\u5065\u7684\u91cd\u5efa\uff0c\u907f\u514d\u4e86\u8fed\u4ee3\u4f18\u5316\uff1bBEAM\u7b56\u7565\u5728\u5bb9\u91cf\u53d7\u9650\u60c5\u51b5\u4e0b\u6539\u5584\u4e86\u91cd\u5efa\u8d28\u91cf", "conclusion": "ELM-INR\u63d0\u4f9b\u4e86\u4e00\u79cd\u514d\u53cd\u5411\u4f20\u64ad\u7684INR\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u5747\u5300\u9891\u8c31\u5185\u5bb9\uff0c\u5b9e\u73b0\u5feb\u901f\u7a33\u5065\u7684\u91cd\u5efa"}}
{"id": "2602.07309", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07309", "abs": "https://arxiv.org/abs/2602.07309", "authors": ["Fedor Borisyuk", "Sriram Vasudevan", "Muchen Wu", "Guoyao Li", "Benjamin Le", "Shaobo Zhang", "Qianqi Kay Shen", "Yuchin Juan", "Kayhan Behdin", "Liming Dong", "Kaixu Yang", "Shusen Jing", "Ravi Pothamsetty", "Rajat Arora", "Sophie Yanying Sheng", "Vitaly Abdrashitov", "Yang Zhao", "Lin Su", "Xiaoqing Wang", "Chujie Zheng", "Sarang Metkar", "Rupesh Gupta", "Igor Lapchuk", "David N. Racca", "Madhumitha Mohan", "Yanbo Li", "Haojun Li", "Saloni Gandhi", "Xueying Lu", "Chetan Bhole", "Ali Hooshmand", "Xin Yang", "Raghavan Muthuregunathan", "Jiajun Zhang", "Mathew Teoh", "Adam Coler", "Abhinav Gupta", "Xiaojing Ma", "Sundara Raman Ramachandran", "Morteza Ramezani", "Yubo Wang", "Lijuan Zhang", "Richard Li", "Jian Sheng", "Chanh Nguyen", "Yen-Chi Chen", "Chuanrui Zhu", "Claire Zhang", "Jiahao Xu", "Deepti Kulkarni", "Qing Lan", "Arvind Subramaniam", "Ata Fatahibaarzi", "Steven Shimizu", "Yanning Chen", "Zhipeng Wang", "Ran He", "Zhengze Zhou", "Qingquan Song", "Yun Dai", "Caleb Johnson", "Ping Liu", "Shaghayegh Gharghabi", "Gokulraj Mohanasundaram", "Juan Bottaro", "Santhosh Sachindran", "Qi Guo", "Yunxiang Ren", "Chengming Jiang", "Di Mo", "Luke Simon", "Jianqiang Shen", "Jingwei Wu", "Wenjing Zhang"], "title": "Semantic Search At LinkedIn", "comment": null, "summary": "Semantic search with large language models (LLMs) enables retrieval by meaning rather than keyword overlap, but scaling it requires major inference efficiency advances. We present LinkedIn's LLM-based semantic search framework for AI Job Search and AI People Search, combining an LLM relevance judge, embedding-based retrieval, and a compact Small Language Model trained via multi-teacher distillation to jointly optimize relevance and engagement. A prefill-oriented inference architecture co-designed with model pruning, context compression, and text-embedding hybrid interactions boosts ranking throughput by over 75x under a fixed latency constraint while preserving near-teacher-level NDCG, enabling one of the first production LLM-based ranking systems with efficiency comparable to traditional approaches and delivering significant gains in quality and user engagement.", "AI": {"tldr": "LinkedIn\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8bed\u4e49\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6559\u5e08\u84b8\u998f\u8bad\u7ec3\u7d27\u51d1\u5c0f\u6a21\u578b\uff0c\u7ed3\u5408\u63a8\u7406\u67b6\u6784\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b075\u500d\u4ee5\u4e0a\u7684\u6392\u540d\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u641c\u7d22\u80fd\u591f\u5b9e\u73b0\u57fa\u4e8e\u610f\u4e49\u7684\u68c0\u7d22\uff0c\u4f46\u6269\u5c55\u8fd9\u79cd\u6280\u672f\u9700\u8981\u663e\u8457\u7684\u63a8\u7406\u6548\u7387\u63d0\u5347\u3002LinkedIn\u9700\u8981\u4e3a\u5176AI\u804c\u4f4d\u641c\u7d22\u548cAI\u4eba\u624d\u641c\u7d22\u529f\u80fd\u5f00\u53d1\u9ad8\u6548\u7684\u8bed\u4e49\u641c\u7d22\u7cfb\u7edf\u3002", "method": "1. \u7ed3\u5408LLM\u76f8\u5173\u6027\u5224\u65ad\u5668\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\uff1b2. \u901a\u8fc7\u591a\u6559\u5e08\u84b8\u998f\u8bad\u7ec3\u7d27\u51d1\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\uff0c\u8054\u5408\u4f18\u5316\u76f8\u5173\u6027\u548c\u53c2\u4e0e\u5ea6\uff1b3. \u91c7\u7528\u9884\u586b\u5145\u5bfc\u5411\u7684\u63a8\u7406\u67b6\u6784\uff0c\u7ed3\u5408\u6a21\u578b\u526a\u679d\u3001\u4e0a\u4e0b\u6587\u538b\u7f29\u548c\u6587\u672c-\u5d4c\u5165\u6df7\u5408\u4ea4\u4e92\u3002", "result": "\u5728\u56fa\u5b9a\u5ef6\u8fdf\u7ea6\u675f\u4e0b\uff0c\u6392\u540d\u541e\u5410\u91cf\u63d0\u5347\u4e8675\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u6c34\u5e73\u7684NDCG\u8d28\u91cf\u3002\u8fd9\u662f\u9996\u6279\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u7684LLM\u6392\u540d\u7cfb\u7edf\u4e4b\u4e00\uff0c\u6548\u7387\u53ef\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u5728\u8d28\u91cf\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u65b9\u9762\u5e26\u6765\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u6a21\u578b\u538b\u7f29\u548c\u63a8\u7406\u67b6\u6784\u8bbe\u8ba1\uff0cLinkedIn\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684LLM\u8bed\u4e49\u641c\u7d22\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684LLM\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07033", "abs": "https://arxiv.org/abs/2602.07033", "authors": ["Md Shahriar Kabir", "Sana Alamgeer", "Minakshi Debnath", "Anne H. H. Ngu"], "title": "TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare", "comment": "Previously published at IEEE COMPSAC 2025", "summary": "The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.", "AI": {"tldr": "TransConv-DDPM\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u751f\u6210\u751f\u7269\u529b\u5b66\u548c\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u901a\u8fc7\u7ed3\u5408DDPM\u3001U-Net\u3001\u591a\u5c3a\u5ea6\u5377\u79ef\u6a21\u5757\u548cTransformer\u5c42\u6765\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "motivation": "\u4e34\u5e8a\u9886\u57df\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u963b\u788d\u4e86\u533b\u5b66\u8bca\u65ad\u548c\u9884\u9632\u5de5\u5177\u4e2d\u6709\u6548AI\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u751f\u6210\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u56e0\u5176\u56fa\u6709\u7684\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51faTransConv-DDPM\u6a21\u578b\uff0c\u91c7\u7528\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b(DDPM)\u67b6\u6784\uff0c\u7ed3\u5408U-Net\u3001\u591a\u5c3a\u5ea6\u5377\u79ef\u6a21\u5757\u548cTransformer\u5c42\uff0c\u4ee5\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5168\u5c40\u548c\u5c40\u90e8\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4e0eTimeGAN\u548cDiffusion-TS\u7b49\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728SmartFallMM\u548cEEG\u6570\u636e\u96c6\u4e0a\u80fd\u6709\u6548\u6355\u6349\u6570\u636e\u70b9\u95f4\u6e10\u53d8\u7684\u65f6\u95f4\u6a21\u5f0f\u3002\u5728SmartFallMM\u6570\u636e\u96c6\u4e0a\uff0c\u6dfb\u52a0TransConv-DDPM\u751f\u6210\u7684\u5408\u6210\u8dcc\u5012\u6570\u636e\u4f7f\u9884\u6d4b\u6a21\u578bF1\u5206\u6570\u63d0\u9ad813.64%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u63d0\u9ad814.93%\u3002", "conclusion": "TransConv-DDPM\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u751f\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u89e3\u51b3\u533b\u5b66AI\u5e94\u7528\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2602.07012", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07012", "abs": "https://arxiv.org/abs/2602.07012", "authors": ["Zhonghua Wang", "Lie Ju", "Sijia Li", "Wei Feng", "Sijin Zhou", "Ming Hu", "Jianhao Xiong", "Xiaoying Tang", "Yifan Peng", "Mingquan Lin", "Yaodong Ding", "Yong Zeng", "Wenbin Wei", "Li Dong", "Zongyuan Ge"], "title": "A General Model for Retinal Segmentation and Quantification", "comment": null, "summary": "Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.", "AI": {"tldr": "RetSAM\u662f\u4e00\u4e2a\u901a\u7528\u7684\u89c6\u7f51\u819c\u5206\u5272\u548c\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u773c\u5e95\u6210\u50cf\uff0c\u80fd\u591f\u8fdb\u884c\u591a\u76ee\u6807\u5206\u5272\u5e76\u63d0\u53d6\u6807\u51c6\u5316\u751f\u7269\u6807\u5fd7\u7269\uff0c\u652f\u6301\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u548c\u8f6c\u5316\u5e94\u7528\u3002", "motivation": "\u89c6\u7f51\u819c\u6210\u50cf\u5feb\u901f\u3001\u65e0\u521b\u4e14\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u773c\u79d1\u548c\u5168\u8eab\u5065\u5eb7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u91cf\u5316\u7684\u7ed3\u6784\u548c\u8840\u7ba1\u4fe1\u53f7\u3002\u7136\u800c\uff0c\u7531\u4e8e\u516c\u5171\u591a\u6807\u7b7e\u6570\u636e\u96c6\u7684\u6709\u9650\u6027\u4ee5\u53ca\u7f3a\u4e4f\u7edf\u4e00\u7684\u5206\u5272\u5230\u91cf\u5316\u6d41\u7a0b\uff0c\u5927\u89c4\u6a21\u5206\u6790\u4ecd\u7136\u56f0\u96be\u3002", "method": "RetSAM\u662f\u4e00\u4e2a\u901a\u7528\u7684\u89c6\u7f51\u819c\u5206\u5272\u548c\u91cf\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u8d85\u8fc720\u4e07\u5f20\u773c\u5e95\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u652f\u6301\u4e09\u7c7b\u4efb\u52a1\uff0c\u5206\u5272\u4e94\u79cd\u89e3\u5256\u7ed3\u6784\u3001\u56db\u79cd\u89c6\u7f51\u819c\u8868\u578b\u6a21\u5f0f\u548c20\u591a\u79cd\u4e0d\u540c\u7684\u75c5\u53d8\u7c7b\u578b\u3002\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u79c1\u6709\u548c\u516c\u5171\u773c\u5e95\u6570\u636e\u3002", "result": "\u572817\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u5206\u5272\u6027\u80fd\uff0c\u5e73\u5747DSC\u6bd4\u5148\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u9ad83.9\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u4efb\u52a1\u57fa\u51c6\u4e0a\u63d0\u5347\u9ad8\u8fbe15\u4e2a\u767e\u5206\u70b9\u3002\u80fd\u591f\u8de8\u4e0d\u540c\u4eba\u7fa4\u3001\u6210\u50cf\u8bbe\u5907\u548c\u4e34\u5e8a\u73af\u5883\u826f\u597d\u6cdb\u5316\u3002", "conclusion": "RetSAM\u5c06\u773c\u5e95\u56fe\u50cf\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5b9a\u91cf\u8868\u578b\uff0c\u652f\u6301\u8de8\u4e3b\u8981\u773c\u79d1\u75be\u75c5\u7684\u7cfb\u7edf\u6027\u76f8\u5173\u5206\u6790\uff0c\u5305\u62ec\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u3001\u5e74\u9f84\u76f8\u5173\u6027\u9ec4\u6591\u53d8\u6027\u3001\u9752\u5149\u773c\u548c\u75c5\u7406\u6027\u8fd1\u89c6\uff0c\u4ece\u800c\u4fc3\u8fdb\u5927\u89c4\u6a21\u773c\u79d1\u7814\u7a76\u548c\u8f6c\u5316\u5e94\u7528\u3002"}}
{"id": "2602.07054", "categories": ["cs.LG", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07054", "abs": "https://arxiv.org/abs/2602.07054", "authors": ["Ashutosh Chaubey", "Jiacheng Pang", "Maksim Siniukov", "Mohammad Soleymani"], "title": "AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization", "comment": "Accepted as a conference paper at ICLR 2026. Project page: https://avere-iclr.github.io", "summary": "Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86EmoReAlM\u57fa\u51c6\u6d4b\u8bd5\u548cAVEm-DPO\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4e2d\u7684\u865a\u5047\u5173\u8054\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u60c5\u611f\u4e0e\u65e0\u5173\u89c6\u542c\u7ebf\u7d22\u4e4b\u95f4\u7684\u865a\u5047\u5173\u8054\uff1b2\uff09\u8bed\u8a00\u6a21\u578b\u4e3b\u5e72\u4e2d\u7684\u6587\u672c\u5148\u9a8c\u9a71\u52a8\u7684\u89c6\u542c\u7ebf\u7d22\u5e7b\u89c9\u3002\u9700\u8981\u91cf\u5316\u5e76\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1\uff09\u5f15\u5165EmoReAlM\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u7684\u7ebf\u7d22-\u60c5\u611f\u5173\u8054\u3001\u5e7b\u89c9\u548c\u6a21\u6001\u4e00\u81f4\u6027\uff1b2\uff09\u63d0\u51faAVEm-DPO\u504f\u597d\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u6784\u5efa\u5bf9\u865a\u5047\u5173\u8054\u6216\u5e7b\u89c9\u54cd\u5e94\u7684\u504f\u597d\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u7684\u89c6\u542c\u8f93\u5165\u5bf9\u6765\u5bf9\u9f50\u6a21\u578b\u54cd\u5e94\uff1b3\uff09\u5305\u542b\u6b63\u5219\u5316\u9879\u60e9\u7f5a\u5bf9\u6587\u672c\u5148\u9a8c\u7684\u4f9d\u8d56\uff0c\u51cf\u8f7b\u6a21\u6001\u7279\u5b9a\u7ebf\u7d22\u5e7b\u89c9\u3002", "result": "\u5728DFEW\u3001RAVDESS\u548cEMER\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53c2\u8003\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u83b7\u5f97\u4e866-19%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u4e25\u683c\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7a33\u5065\u7684\u4f18\u5316\u6846\u67b6\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u60c5\u611f\u7406\u89e3\u548c\u793e\u4f1aAI\u9886\u57df\u7684MLLMs\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\u3002\u4ee3\u7801\u3001\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u5728\u6307\u5b9a\u7f51\u7ad9\u53d1\u5e03\u3002"}}
{"id": "2602.07013", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07013", "abs": "https://arxiv.org/abs/2602.07013", "authors": ["Jiaxi Yang", "Shicheng Liu", "Yuchen Yang", "Dongwon Lee"], "title": "Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models", "comment": null, "summary": "With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \\textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \\textbf{C}onfigurable \\textbf{R}efusal in \\textbf{VLM}s (\\textbf{CR-VLM}), a robust and efficient approach for {\\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.", "AI": {"tldr": "CR-VLM\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u914d\u7f6e\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u62d2\u7edd\u673a\u5236\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u5b9e\u73b0\u7528\u6237\u81ea\u9002\u5e94\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u89e3\u51b3\u73b0\u6709\u62d2\u7edd\u7b56\u7565\"\u4e00\u5200\u5207\"\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u62d2\u7edd\u673a\u5236\u5927\u591a\u662f\"\u4e00\u5200\u5207\"\u7684\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7528\u6237\u9700\u6c42\u548c\u4e0a\u4e0b\u6587\u7ea6\u675f\uff0c\u5bfc\u81f4\u62d2\u7edd\u4e0d\u8db3\u6216\u8fc7\u5ea6\u62d2\u7edd\u7684\u95ee\u9898\u3002", "method": "CR-VLM\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u901a\u8fc7\u6559\u5e08\u5f3a\u5236\u673a\u5236\u63d0\u53d6\u53ef\u914d\u7f6e\u62d2\u7edd\u5411\u91cf\u4ee5\u589e\u5f3a\u62d2\u7edd\u4fe1\u53f7\uff1b2) \u5f15\u5165\u95e8\u63a7\u673a\u5236\u4fdd\u7559\u8303\u56f4\u5185\u67e5\u8be2\u7684\u63a5\u53d7\u80fd\u529b\uff1b3) \u8bbe\u8ba1\u53cd\u4e8b\u5b9e\u89c6\u89c9\u589e\u5f3a\u6a21\u5757\u5bf9\u9f50\u89c6\u89c9\u8868\u793a\u4e0e\u62d2\u7edd\u9700\u6c42\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u5404\u79cdVLM\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cCR-VLM\u5b9e\u73b0\u4e86\u6709\u6548\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u53ef\u914d\u7f6e\u62d2\u7edd\uff0c\u4e3aVLM\u4e2d\u7684\u7528\u6237\u81ea\u9002\u5e94\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\u3002", "conclusion": "CR-VLM\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u914d\u7f6e\u7684\u62d2\u7edd\u673a\u5236\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5e73\u8861\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u63a8\u52a8\u7528\u6237\u81ea\u9002\u5e94\u7684\u5b89\u5168\u5bf9\u9f50\u53d1\u5c55\u3002"}}
{"id": "2602.08043", "categories": ["cs.LG", "cs.AI", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.08043", "abs": "https://arxiv.org/abs/2602.08043", "authors": ["Yiheng Gao", "Qin Hua", "Zizhong Chen"], "title": "V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning", "comment": null, "summary": "Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\\times$ for FP32/FP64 and $48$--$158\\times$ for BF16, representing a \\textbf{6--48$\\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\\max} \\approx 10^{-6}$), enabling \\textbf{$\\sim$1000$\\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\\max} \\approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.", "AI": {"tldr": "V-ABFT\u662f\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u7684\u81ea\u9002\u5e94\u9608\u503c\u7b97\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u77e9\u9635\u4e58\u6cd5\u4e2d\u7684\u9759\u9ed8\u6570\u636e\u635f\u574f\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684ABFT\u9608\u503c\u786e\u5b9a\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u5206\u6790\u8fb9\u754c\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u800c\u6982\u7387\u65b9\u6cd5\u5982A-ABFT\u7684\u9608\u503c\u6bd4\u5b9e\u9645\u820d\u5165\u8bef\u5dee\u5927160-4200\u500d\uff0c\u5bfc\u81f4\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faV-ABFT\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u9a8c\u8bc1\u5dee\u5f02\u5e76\u5229\u7528\u7edf\u8ba1\u65b9\u5dee\u4f30\u8ba1\u6765\u83b7\u5f97\u66f4\u7d27\u5bc6\u7684\u8bef\u5dee\u8fb9\u754c\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700O(n)\u590d\u6742\u5ea6\uff0c\u4f7f\u7528\u6700\u5927/\u6700\u5c0f/\u5747\u503c\u7edf\u8ba1\u91cf\uff0c\u800cA-ABFT\u9700\u8981O(pn)\u590d\u6742\u5ea6\u6765\u5bfb\u627ep\u4e2a\u6700\u5927\u503c\u3002", "result": "V-ABFT\u5c06\u9608\u503c\u4e0e\u5b9e\u9645\u8bef\u5dee\u6bd4\u964d\u4f4e\u5230FP32/FP64\u76847-20\u500d\u548cBF16\u768448-158\u500d\uff0c\u76f8\u6bd4A-ABFT\u67096-48\u500d\u7684\u6539\u8fdb\u3002\u5bf9\u4e8e\u878d\u5408\u6838ABFT\u5b9e\u73b0\uff0c\u4f4e\u7cbe\u5ea6GEMM\u53ef\u4ee5\u4f7f\u7528FP32\u7ea7\u9608\u503c\uff0c\u5b9e\u73b0\u7ea61000\u500d\u66f4\u7cbe\u7ec6\u7684\u68c0\u6d4b\u7c92\u5ea6\u3002", "conclusion": "V-ABFT\u5728BF16\u3001FP16\u3001FP32\u548cFP64\u7cbe\u5ea6\u4e0b\u5747\u4fdd\u6301\u96f6\u5047\u9633\u6027\u7387\uff0c\u5e73\u53f0\u65e0\u5173\uff0c\u5df2\u96c6\u6210\u5230NPU\u548cGPU\u7684\u5bb9\u9519GEMM\u5b9e\u73b0\u4e2d\uff0c\u5728\u5404\u79cd\u5206\u5e03\u4e0b\u90fd\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2602.07520", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07520", "abs": "https://arxiv.org/abs/2602.07520", "authors": ["Shanlei Mu", "Yuchen Jiang", "Shikang Wu", "Shiyong Hong", "Tianmu Sha", "Junjie Zhang", "Jie Zhu", "Zhe Chen", "Zhe Wang", "Jingjian Lin"], "title": "MDL: A Unified Multi-Distribution Learner in Large-scale Industrial Recommendation through Tokenization", "comment": "9 pages, 4 figures", "summary": "Industrial recommender systems increasingly adopt multi-scenario learning (MSL) and multi-task learning (MTL) to handle diverse user interactions and contexts, but existing approaches suffer from two critical drawbacks: (1) underutilization of large-scale model parameters due to limited interaction with complex feature modules, and (2) difficulty in jointly modeling scenario and task information in a unified framework. To address these challenges, we propose a unified \\textbf{M}ulti-\\textbf{D}istribution \\textbf{L}earning (MDL) framework, inspired by the \"prompting\" paradigm in large language models (LLMs). MDL treats scenario and task information as specialized tokens rather than auxiliary inputs or gating signals. Specifically, we introduce a unified information tokenization module that transforms features, scenarios, and tasks into a unified tokenized format. To facilitate deep interaction, we design three synergistic mechanisms: (1) feature token self-attention for rich feature interactions, (2) domain-feature attention for scenario/task-adaptive feature activation, and (3) domain-fused aggregation for joint distribution prediction. By stacking these interactions, MDL enables scenario and task information to \"prompt\" and activate the model's vast parameter space in a bottom-up, layer-wise manner. Extensive experiments on real-world industrial datasets demonstrate that MDL significantly outperforms state-of-the-art MSL and MTL baselines. Online A/B testing on Douyin Search platform over one month yields +0.0626\\% improvement in LT30 and -0.3267\\% reduction in change query rate. MDL has been fully deployed in production, serving hundreds of millions of users daily.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u591a\u5206\u5e03\u5b66\u4e60\u6846\u67b6MDL\uff0c\u5c06\u573a\u666f\u548c\u4efb\u52a1\u4fe1\u606f\u4f5c\u4e3a\u7279\u6b8a\u4ee4\u724c\u5904\u7406\uff0c\u901a\u8fc7\u4e09\u5c42\u534f\u540c\u673a\u5236\u5b9e\u73b0\u6df1\u5ea6\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u6027\u80fd", "motivation": "\u73b0\u6709\u591a\u573a\u666f\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a1) \u5927\u89c4\u6a21\u6a21\u578b\u53c2\u6570\u5229\u7528\u7387\u4e0d\u8db3\uff0c\u4e0e\u590d\u6742\u7279\u5f81\u6a21\u5757\u4ea4\u4e92\u6709\u9650\uff1b2) \u96be\u4ee5\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u8054\u5408\u5efa\u6a21\u573a\u666f\u548c\u4efb\u52a1\u4fe1\u606f", "method": "\u63d0\u51faMDL\u6846\u67b6\uff0c\u53d7\u5927\u8bed\u8a00\u6a21\u578b\"\u63d0\u793a\"\u8303\u5f0f\u542f\u53d1\uff0c\u5c06\u573a\u666f\u548c\u4efb\u52a1\u4fe1\u606f\u4f5c\u4e3a\u7279\u6b8a\u4ee4\u724c\u800c\u975e\u8f85\u52a9\u8f93\u5165\u6216\u95e8\u63a7\u4fe1\u53f7\u3002\u5305\u542b\u7edf\u4e00\u4fe1\u606f\u4ee4\u724c\u5316\u6a21\u5757\uff0c\u4ee5\u53ca\u4e09\u5c42\u534f\u540c\u673a\u5236\uff1a\u7279\u5f81\u4ee4\u724c\u81ea\u6ce8\u610f\u529b\u3001\u9886\u57df-\u7279\u5f81\u6ce8\u610f\u529b\u3001\u9886\u57df\u878d\u5408\u805a\u5408", "result": "\u5728\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u573a\u666f\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u57fa\u7ebf\u3002\u5728\u6296\u97f3\u641c\u7d22\u5e73\u53f0\u4e00\u4e2a\u6708\u7684\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\uff0cLT30\u63d0\u5347+0.0626%\uff0c\u53d8\u66f4\u67e5\u8be2\u7387\u964d\u4f4e-0.3267%\u3002\u5df2\u5728\u751f\u4ea7\u73af\u5883\u5168\u9762\u90e8\u7f72\uff0c\u6bcf\u65e5\u670d\u52a1\u6570\u4ebf\u7528\u6237", "conclusion": "MDL\u6846\u67b6\u901a\u8fc7\u5c06\u573a\u666f\u548c\u4efb\u52a1\u4fe1\u606f\u4f5c\u4e3a\u7279\u6b8a\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e86\u6df1\u5ea6\u4ea4\u4e92\u548c\u53c2\u6570\u7a7a\u95f4\u7684\u6709\u6548\u6fc0\u6d3b\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u591a\u573a\u666f\u591a\u4efb\u52a1\u8054\u5408\u5efa\u6a21\u7684\u6311\u6218"}}
{"id": "2602.07014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07014", "abs": "https://arxiv.org/abs/2602.07014", "authors": ["Qingyu Wu", "Yuxuan Han", "Haijun Li", "Zhao Xu", "Jianshan Zhao", "Xu Jin", "Longyue Wang", "Weihua Luo"], "title": "Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation", "comment": null, "summary": "In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.", "AI": {"tldr": "Vectra\u662f\u9996\u4e2a\u65e0\u53c2\u8003\u3001\u57fa\u4e8eMLLM\u7684\u7535\u5546\u56fe\u50cf\u5185\u673a\u5668\u7ffb\u8bd1\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc714\u7ef4\u53ef\u89e3\u91ca\u6307\u6807\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c4B\u53c2\u6570\u6a21\u578b\uff0c\u5728\u4eba\u7c7b\u6392\u540d\u76f8\u5173\u6027\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "\u5f53\u524d\u7535\u5546\u56fe\u50cf\u5185\u673a\u5668\u7ffb\u8bd1(IIMT)\u7814\u7a76\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8bc4\u4f30\u89c6\u89c9\u6e32\u67d3\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff1a\u57fa\u4e8e\u53c2\u8003\u7684\u65b9\u6cd5(\u5982SSIM\u3001FID)\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u6a21\u578b\u5373\u8bc4\u5224\u65b9\u6cd5\u7f3a\u4e4f\u9886\u57df\u57fa\u7840\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4e0a\u4e0b\u6587\u5bc6\u96c6\u7684\u4ea7\u54c1\u56fe\u50cf\u548c\u591a\u6a21\u6001\u7f3a\u9677\u65f6\u3002", "method": "Vectra\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) Vectra Score - 14\u7ef4\u53ef\u89e3\u91ca\u8d28\u91cf\u5ea6\u91cf\u7cfb\u7edf\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u7684\u7f3a\u9677\u9762\u79ef\u6bd4(DAR)\u91cf\u5316\u51cf\u5c11\u6807\u6ce8\u6b67\u4e49\uff1b2) Vectra Dataset - \u4ece110\u4e07\u771f\u5b9e\u4ea7\u54c1\u56fe\u50cf\u6784\u5efa\uff0c\u5305\u542b2K\u57fa\u51c6\u96c6\u300130K\u63a8\u7406\u6807\u6ce8\u548c3.5K\u4e13\u5bb6\u504f\u597d\u6807\u6ce8\uff1b3) Vectra Model - 4B\u53c2\u6570MLLM\uff0c\u80fd\u751f\u6210\u91cf\u5316\u5206\u6570\u548c\u8bca\u65ad\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVectra\u5728\u4eba\u7c7b\u6392\u540d\u76f8\u5173\u6027\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5176\u6a21\u578b\u5728\u8bc4\u5206\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5305\u62ecGPT-5\u548cGemini-3\u5728\u5185\u7684\u9886\u5148MLLMs\u3002", "conclusion": "Vectra\u586b\u8865\u4e86\u7535\u5546IIMT\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u65e0\u53c2\u8003\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5176\u6570\u636e\u96c6\u548c\u6a21\u578b\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u53d1\u5e03\u3002"}}
{"id": "2602.06981", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.06981", "abs": "https://arxiv.org/abs/2602.06981", "authors": ["Ankolika De", "Gabriel Lima", "Yixin Zou"], "title": "What is Safety? Corporate Discourse, Power, and the Politics of Generative AI Safety", "comment": "18 pages, 2 tables", "summary": "This work examines how leading generative artificial intelligence companies construct and communicate the concept of \"safety\" through public-facing documents. Drawing on critical discourse analysis, we analyze a corpus of corporate safety-related statements to explicate how authority, responsibility, and legitimacy are discursively established. These discursive strategies consolidate legitimacy for corporate actors, normalize safety as an experimental and anticipatory practice, and push a perceived participatory agenda toward safe technologies. We argue that uncritical uptake of these discourses risks reproducing corporate priorities and constraining alternative approaches to governance and design. The contribution of this work is twofold: first, to situate safety as a sociotechnical discourse that warrants critical examination; second, to caution human-computer interaction scholars against legitimizing corporate framings, instead foregrounding accountability, equity, and justice. By interrogating safety discourses as artifacts of power, this paper advances a critical agenda for human-computer interaction scholarship on artificial intelligence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6279\u5224\u6027\u8bdd\u8bed\u5206\u6790\uff0c\u63ed\u793a\u4e86\u9886\u5148\u7684\u751f\u6210\u5f0fAI\u516c\u53f8\u5982\u4f55\u901a\u8fc7\u516c\u5f00\u6587\u4ef6\u6784\u5efa\u548c\u4f20\u8fbe\"\u5b89\u5168\"\u6982\u5ff5\uff0c\u8fd9\u4e9b\u8bdd\u8bed\u7b56\u7565\u5de9\u56fa\u4e86\u4f01\u4e1a\u5408\u6cd5\u6027\uff0c\u5c06\u5b89\u5168\u89c4\u8303\u5316\u4e3a\u5b9e\u9a8c\u6027\u548c\u9884\u671f\u6027\u5b9e\u8df5\uff0c\u5e76\u63a8\u52a8\u53c2\u4e0e\u5f0f\u8bae\u7a0b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6279\u5224\u6027\u5730\u5ba1\u89c6\u751f\u6210\u5f0fAI\u516c\u53f8\u5982\u4f55\u901a\u8fc7\u516c\u5f00\u7684\u5b89\u5168\u58f0\u660e\u6784\u5efa\u8bdd\u8bed\uff0c\u63ed\u793a\u8fd9\u4e9b\u8bdd\u8bed\u5982\u4f55\u5efa\u7acb\u6743\u5a01\u3001\u8d23\u4efb\u548c\u5408\u6cd5\u6027\uff0c\u4ee5\u53ca\u8fd9\u79cd\u8bdd\u8bed\u6784\u5efa\u53ef\u80fd\u5982\u4f55\u5f71\u54cd\u6280\u672f\u6cbb\u7406\u548c\u8bbe\u8ba1\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6279\u5224\u6027\u8bdd\u8bed\u5206\u6790\u65b9\u6cd5\uff0c\u5206\u6790\u4f01\u4e1a\u5b89\u5168\u76f8\u5173\u58f0\u660e\u7684\u8bed\u6599\u5e93\uff0c\u9610\u91ca\u6743\u5a01\u3001\u8d23\u4efb\u548c\u5408\u6cd5\u6027\u5982\u4f55\u901a\u8fc7\u8bdd\u8bed\u7b56\u7565\u88ab\u5efa\u7acb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4f01\u4e1a\u7684\u8bdd\u8bed\u7b56\u7565\uff1a1)\u5de9\u56fa\u4f01\u4e1a\u884c\u4e3a\u8005\u7684\u5408\u6cd5\u6027\uff1b2)\u5c06\u5b89\u5168\u89c4\u8303\u5316\u4e3a\u5b9e\u9a8c\u6027\u548c\u9884\u671f\u6027\u5b9e\u8df5\uff1b3)\u63a8\u52a8\u611f\u77e5\u4e0a\u7684\u53c2\u4e0e\u5f0f\u8bae\u7a0b\u3002\u8fd9\u4e9b\u8bdd\u8bed\u53ef\u80fd\u5bfc\u81f4\u4f01\u4e1a\u4f18\u5148\u4e8b\u9879\u7684\u590d\u5236\uff0c\u5e76\u9650\u5236\u6cbb\u7406\u548c\u8bbe\u8ba1\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8d21\u732e\u5305\u62ec\uff1a1)\u5c06\u5b89\u5168\u5b9a\u4f4d\u4e3a\u9700\u8981\u6279\u5224\u6027\u5ba1\u89c6\u7684\u793e\u4f1a\u6280\u672f\u8bdd\u8bed\uff1b2)\u8b66\u544a\u4eba\u673a\u4ea4\u4e92\u5b66\u8005\u4e0d\u8981\u5408\u6cd5\u5316\u4f01\u4e1a\u6846\u67b6\uff0c\u800c\u5e94\u5f3a\u8c03\u95ee\u8d23\u3001\u516c\u5e73\u548c\u6b63\u4e49\u3002\u901a\u8fc7\u5c06\u5b89\u5168\u8bdd\u8bed\u4f5c\u4e3a\u6743\u529b\u4ea7\u7269\u8fdb\u884c\u5ba1\u89c6\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684AI\u7814\u7a76\u63a8\u8fdb\u4e86\u6279\u5224\u6027\u8bae\u7a0b\u3002"}}
{"id": "2602.07525", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.07525", "abs": "https://arxiv.org/abs/2602.07525", "authors": ["Xingliang Hou", "Yuyan Liu", "Qi Sun", "haoxiu wang", "Hao Hu", "Shaoyi Du", "Zhiqiang Tian"], "title": "IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory", "comment": "29 pages, Information Retrieval", "summary": "Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.", "AI": {"tldr": "IGMiRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u76f4\u89c9\u5f15\u5bfc\u63a8\u7406\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5c42\u6b21\u5f02\u6784\u8d85\u56fe\u5bf9\u9f50\u591a\u7c92\u5ea6\u77e5\u8bc6\uff0c\u4f7f\u7528\u53cc\u5411\u6269\u6563\u7b97\u6cd5\u6316\u6398\u6df1\u5ea6\u8bb0\u5fc6\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe/\u8d85\u56fe\u7684RAG\u65b9\u6cd5\u5b58\u5728\u5185\u5b58\u7ec4\u7ec7\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u68c0\u7d22\u6210\u672c\u9ad8\u4e14\u5206\u6563\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7b26\u5408\u4eba\u7c7b\u76f4\u89c9\u63a8\u7406\u7684\u6846\u67b6\u6765\u6539\u5584\u8de8\u6587\u672c\u5173\u8054\u548c\u68c0\u7d22\u6548\u7387\u3002", "method": "1) \u6784\u5efa\u5c42\u6b21\u5f02\u6784\u8d85\u56fe\u6765\u5bf9\u9f50\u591a\u7c92\u5ea6\u77e5\u8bc6\uff0c\u5305\u542b\u6f14\u7ece\u8def\u5f84\u6a21\u62df\u73b0\u5b9e\u8bb0\u5fc6\u7ed3\u6784\uff1b2) \u901a\u8fc7\u95ee\u9898\u89e3\u6790\u5668\u84b8\u998f\u76f4\u89c9\u7b56\u7565\uff0c\u63a7\u5236\u6316\u6398\u6df1\u5ea6\u548c\u8bb0\u5fc6\u7a97\u53e3\uff1b3) \u4f7f\u7528\u53cc\u7126\u70b9\u68c0\u7d22\u6fc0\u6d3b\u77ac\u65f6\u8bb0\u5fc6\u4f5c\u4e3a\u951a\u70b9\uff1b4) \u8bbe\u8ba1\u53cc\u5411\u6269\u6563\u7b97\u6cd5\u5728\u6f14\u7ece\u8def\u5f84\u4e0a\u5bfc\u822a\u6316\u6398\u6df1\u5ea6\u8bb0\u5fc6\u3002", "result": "\u5728\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0cIGMiRAG\u5728EM\u6307\u6807\u4e0a\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u9ad84.8%\uff0cF1\u6307\u6807\u63d0\u9ad85.0%\u3002token\u6210\u672c\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\uff08\u5e73\u57476.3k+\uff0c\u6700\u4f4e3.0k+\uff09\u3002", "conclusion": "IGMiRAG\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684RAG\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u76f4\u89c9\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u751f\u6210\u6548\u679c\uff0c\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07015", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07015", "abs": "https://arxiv.org/abs/2602.07015", "authors": ["Subreena", "Mohammad Amzad Hossain", "Mirza Raquib", "Saydul Akbar Murad", "Farida Siddiqi Prity", "Muhammad Hanif", "Nick Rahimi"], "title": "Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach", "comment": null, "summary": "Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u8bc6\u522b\u7684\u6df7\u5408CNN\u67b6\u6784\uff0c\u7ed3\u5408MobileNetV3-Large\u548cEfficientNetB0\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u914d\u5408MLP\u5206\u7c7b\u5668\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7eb8\u5e01\u8bc6\u522b\u3002", "motivation": "\u51c6\u786e\u7684\u7eb8\u5e01\u8bc6\u522b\u5bf9\u4e8e\u8f85\u52a9\u6280\u672f\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u89c6\u969c\u4eba\u58eb\u4f9d\u8d56\u4ed6\u4eba\u8bc6\u522b\u7eb8\u5e01\u5bb9\u6613\u906d\u53d7\u6b3a\u8bc8\u548c\u5265\u524a\u3002\u73b0\u6709\u8bc6\u522b\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u6784\u5efa\u65b0\u7684\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u6570\u636e\u96c6\uff0c\u5305\u542b\u53d7\u63a7\u548c\u771f\u5b9e\u573a\u666f\uff1b2) \u6574\u5408\u56db\u4e2a\u989d\u5916\u6570\u636e\u96c6\u589e\u5f3a\u9c81\u68d2\u6027\uff1b3) \u63d0\u51fa\u6df7\u5408CNN\u67b6\u6784\uff0c\u7ed3\u5408MobileNetV3-Large\u548cEfficientNetB0\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff1b4) \u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a(MLP)\u5206\u7c7b\u5668\uff1b5) \u91c7\u7528\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4e03\u79cd\u8bc4\u4f30\u6307\u6807\uff1b6) \u96c6\u6210LIME\u548cSHAP\u7b49\u53ef\u89e3\u91caAI\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5728\u53d7\u63a7\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.95%\u51c6\u786e\u7387\uff0c\u590d\u6742\u80cc\u666f\u4e0a\u8fbe\u523092.84%\u51c6\u786e\u7387\uff0c\u6240\u6709\u6570\u636e\u96c6\u7efc\u5408\u51c6\u786e\u7387\u4e3a94.98%\u3002\u901a\u8fc7\u4e03\u79cd\u8bc4\u4f30\u6307\u6807\u548c\u53ef\u89e3\u91caAI\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408CNN\u67b6\u6784\u5728\u5b5f\u52a0\u62c9\u56fd\u7eb8\u5e01\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e2\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u53c8\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u89e3\u91caAI\u65b9\u6cd5\u589e\u5f3a\u4e86\u900f\u660e\u5ea6\uff0c\u6709\u52a9\u4e8e\u89c6\u969c\u4eba\u58eb\u7684\u91d1\u878d\u72ec\u7acb\u548c\u5b89\u5168\u3002"}}
{"id": "2602.08478", "categories": ["cs.LG", "math.DS", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.08478", "abs": "https://arxiv.org/abs/2602.08478", "authors": ["Albert Alcalde", "Markus Widhalm", "Emre Y\u0131lmaz"], "title": "Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics", "comment": null, "summary": "We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.", "AI": {"tldr": "\u63d0\u51fa\u65f6\u95f4\u5ef6\u8fdf\u53d8\u6362\u5668(TD-TF)\uff0c\u4e00\u79cd\u7b80\u5316\u7684Transformer\u67b6\u6784\uff0c\u7528\u4e8e\u975e\u7a33\u6001\u65f6\u7a7a\u52a8\u529b\u5b66\u7684\u6570\u636e\u9a71\u52a8\u5efa\u6a21\uff0c\u5c06\u7ebf\u6027\u7b97\u5b50\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5e8f\u5217\u6a21\u578b\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u590d\u6742\u975e\u7ebf\u6027\u65f6\u7a7a\u52a8\u529b\u5b66\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7ebf\u6027\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u53c8\u80fd\u63d0\u4f9b\u66f4\u5f3a\u8868\u8fbe\u80fd\u529b\u7684\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u6781\u7b80\u7684\u5355\u5c42\u5355\u5934Transformer\u67b6\u6784\uff0c\u6bcf\u9884\u6d4b\u4e00\u4e2a\u67e5\u8be2\uff0c\u5305\u542b\u4e00\u4e2a\u81ea\u6ce8\u610f\u529b\u5c42\u548c\u4e00\u4e2a\u524d\u9988\u5c42\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u5e8f\u5217\u957f\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u53c2\u6570\u6570\u91cf\u5c11\u3002", "result": "\u5728\u8fd1\u7ebf\u6027\u7cfb\u7edf\u4e2d\u6027\u80fd\u4e0e\u5f3a\u7ebf\u6027\u57fa\u7ebf\u76f8\u5f53\uff0c\u5728\u975e\u7ebf\u6027\u548c\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u663e\u8457\u4f18\u4e8e\u7ebf\u6027\u65b9\u6cd5\uff0c\u80fd\u51c6\u786e\u6355\u6349\u957f\u671f\u52a8\u529b\u5b66\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "TD-TF\u6210\u529f\u6865\u63a5\u4e86\u7ebf\u6027\u7b97\u5b50\u65b9\u6cd5\u548c\u6df1\u5ea6\u5e8f\u5217\u6a21\u578b\uff0c\u5728\u7ebf\u6027\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u57fa\u7840\u4e0a\uff0c\u4e3a\u590d\u6742\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u663e\u8457\u589e\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2602.07187", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07187", "abs": "https://arxiv.org/abs/2602.07187", "authors": ["Hanyu Wang", "Yuanpu Cao", "Lu Lin", "Jinghui Chen"], "title": "PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents", "comment": null, "summary": "Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.", "AI": {"tldr": "PreFlect\u662f\u4e00\u79cd\u524d\u77bb\u6027\u53cd\u601d\u673a\u5236\uff0c\u5c06LLM\u667a\u80fd\u4f53\u7684\u53cd\u601d\u4ece\u6267\u884c\u540e\u7ea0\u6b63\u8f6c\u53d8\u4e3a\u6267\u884c\u524d\u9884\u89c1\uff0c\u901a\u8fc7\u6279\u8bc4\u548c\u4f18\u5316\u6267\u884c\u524d\u7684\u8ba1\u5212\u6765\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u53cd\u601d\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u56de\u987e\u6027\u7684\uff1a\u667a\u80fd\u4f53\u5148\u884c\u52a8\uff0c\u89c2\u5bdf\u5230\u5931\u8d25\u540e\u624d\u5c1d\u8bd5\u6062\u590d\u3002\u8fd9\u79cd\u540e\u9a8c\u7ea0\u6b63\u65b9\u5f0f\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u9700\u8981\u5728\u6267\u884c\u524d\u5c31\u80fd\u9884\u89c1\u548c\u907f\u514d\u9519\u8bef", "method": "1. \u524d\u77bb\u6027\u53cd\u601d\uff1a\u5728\u6267\u884c\u524d\u6279\u8bc4\u548c\u4f18\u5316\u667a\u80fd\u4f53\u8ba1\u5212\uff1b2. \u4ece\u5386\u53f2\u8f68\u8ff9\u4e2d\u63d0\u53d6\u89c4\u5212\u9519\u8bef\u6a21\u5f0f\uff1b3. \u52a8\u6001\u91cd\u89c4\u5212\u673a\u5236\uff1a\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u9047\u5230\u610f\u5916\u504f\u5dee\u65f6\u66f4\u65b0\u8ba1\u5212", "result": "\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPreFlect\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u667a\u80fd\u4f53\u6548\u7528\uff0c\u4f18\u4e8e\u57fa\u4e8e\u53cd\u601d\u7684\u57fa\u7ebf\u65b9\u6cd5\u548c\u591a\u4e2a\u66f4\u590d\u6742\u7684\u667a\u80fd\u4f53\u67b6\u6784", "conclusion": "\u4ece\u56de\u987e\u6027\u53cd\u601d\u8f6c\u5411\u524d\u77bb\u6027\u53cd\u601d\u80fd\u6709\u6548\u63d0\u5347LLM\u667a\u80fd\u4f53\u6027\u80fd\uff0cPreFlect\u901a\u8fc7\u6267\u884c\u524d\u8ba1\u5212\u6279\u8bc4\u548c\u52a8\u6001\u91cd\u89c4\u5212\u673a\u5236\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807"}}
{"id": "2602.06984", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06984", "abs": "https://arxiv.org/abs/2602.06984", "authors": ["Lin Luo", "Satwik Ghanta", "Yuri Nakao", "Mathieu Chollet", "Simone Stumpf"], "title": "Empowering Affected Individuals to Shape AI Fairness Assessments: Processes, Criteria, and Tools", "comment": null, "summary": "AI systems are increasingly used in high-stakes domains such as credit rating, where fairness concerns are critical. Existing fairness assessments are typically conducted by AI experts or regulators using predefined protected attributes and metrics, which often fail to capture the diversity and nuance of fairness notions held by the individuals who are affected by these systems' decisions, such as decision subjects. Recent work has therefore called for involving affected individuals in fairness assessment, yet little empirical evidence exists on how they create their own fairness criteria or what kinds of criteria they produce - knowledge that could not only inform experts' fairness evaluation and mitigation, but also guide the design of AI assessment tools. We address this gap through a qualitative user study with 18 participants in a credit rating scenario. Participants first articulated their fairness notions in their own words. Then, participants turned them into concrete quantified and operationalized fairness criteria, through an interactive prototype we designed. Our findings provide empirical evidence of the process through which people's fairness notions emerge via grounding in model features, and uncover a diverse set of individuals' custom-defined criteria for both outcome and procedural fairness. We provide design implications for processes and tools that support more inclusive and value-sensitive AI fairness assessment.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u63a2\u7d22\u4e2a\u4eba\u5982\u4f55\u6784\u5efa\u81ea\u5df1\u7684AI\u516c\u5e73\u6027\u6807\u51c6\uff0c\u53d1\u73b0\u4eba\u4eec\u901a\u8fc7\u6a21\u578b\u7279\u5f81\u7684\u5177\u4f53\u5316\u5f62\u6210\u591a\u6837\u5316\u7684\u516c\u5e73\u89c2\u5ff5\uff0c\u4e3a\u66f4\u5305\u5bb9\u7684AI\u516c\u5e73\u8bc4\u4f30\u63d0\u4f9b\u8bbe\u8ba1\u542f\u793a\u3002", "motivation": "\u5f53\u524dAI\u516c\u5e73\u6027\u8bc4\u4f30\u4e3b\u8981\u7531\u4e13\u5bb6\u4f7f\u7528\u9884\u5b9a\u4e49\u5c5e\u6027\u548c\u6307\u6807\u8fdb\u884c\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u53d7AI\u51b3\u7b56\u5f71\u54cd\u7684\u4e2a\u4f53\u5bf9\u516c\u5e73\u6027\u7684\u591a\u5143\u7406\u89e3\u548c\u7ec6\u5fae\u5dee\u522b\u3002\u9700\u8981\u8ba9\u53d7\u5f71\u54cd\u4e2a\u4f53\u53c2\u4e0e\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u4ed6\u4eec\u5982\u4f55\u521b\u5efa\u81ea\u5df1\u7684\u516c\u5e73\u6807\u51c6\u4ee5\u53ca\u4ea7\u751f\u4f55\u79cd\u6807\u51c6\u7684\u5b9e\u8bc1\u8bc1\u636e\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u7528\u6237\u7814\u7a76\u65b9\u6cd5\uff0c\u5728\u4fe1\u7528\u8bc4\u7ea7\u573a\u666f\u4e0b\u4e0e18\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u5b9e\u9a8c\u3002\u53c2\u4e0e\u8005\u9996\u5148\u7528\u81ea\u5df1\u7684\u8bed\u8a00\u8868\u8fbe\u516c\u5e73\u89c2\u5ff5\uff0c\u7136\u540e\u901a\u8fc7\u8bbe\u8ba1\u7684\u4ea4\u4e92\u539f\u578b\u5c06\u8fd9\u4e9b\u89c2\u5ff5\u8f6c\u5316\u4e3a\u5177\u4f53\u7684\u91cf\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u516c\u5e73\u6807\u51c6\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bc1\u636e\uff0c\u5c55\u793a\u4e86\u4eba\u4eec\u5982\u4f55\u901a\u8fc7\u6a21\u578b\u7279\u5f81\u7684\u5177\u4f53\u5316\u5f62\u6210\u516c\u5e73\u89c2\u5ff5\uff0c\u5e76\u53d1\u73b0\u4e86\u4e2a\u4eba\u81ea\u5b9a\u4e49\u7684\u7ed3\u679c\u516c\u5e73\u6027\u548c\u7a0b\u5e8f\u516c\u5e73\u6027\u6807\u51c6\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u652f\u6301\u66f4\u5305\u5bb9\u3001\u4ef7\u503c\u654f\u611f\u7684AI\u516c\u5e73\u6027\u8bc4\u4f30\u6d41\u7a0b\u548c\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5f3a\u8c03\u9700\u8981\u8ba9\u53d7\u5f71\u54cd\u4e2a\u4f53\u53c2\u4e0e\u516c\u5e73\u6807\u51c6\u7684\u5236\u5b9a\u8fc7\u7a0b\u3002"}}
{"id": "2602.07526", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.07526", "abs": "https://arxiv.org/abs/2602.07526", "authors": ["Shikang Wu", "Hui Lu", "Jinqiu Jin", "Zheng Chai", "Shiyong Hong", "Junjie Zhang", "Shanlei Mu", "Kaiyuan Ma", "Tianyi Liu", "Yuchao Zheng", "Zhe Wang", "Jingjian Lin"], "title": "MSN: A Memory-based Sparse Activation Scaling Framework for Large-scale Industrial Recommendation", "comment": null, "summary": "Scaling deep learning recommendation models is an effective way to improve model expressiveness. Existing approaches often incur substantial computational overhead, making them difficult to deploy in large-scale industrial systems under strict latency constraints. Recent sparse activation scaling methods, such as Sparse Mixture-of-Experts, reduce computation by activating only a subset of parameters, but still suffer from high memory access costs and limited personalization capacity due to the large size and small number of experts. To address these challenges, we propose MSN, a memory-based sparse activation scaling framework for recommendation models. MSN dynamically retrieves personalized representations from a large parameterized memory and integrates them into downstream feature interaction modules via a memory gating mechanism, enabling fine-grained personalization with low computational overhead. To enable further expansion of the memory capacity while keeping both computational and memory access costs under control, MSN adopts a Product-Key Memory (PKM) mechanism, which factorizes the memory retrieval complexity from linear time to sub-linear complexity. In addition, normalization and over-parameterization techniques are introduced to maintain balanced memory utilization and prevent memory retrieval collapse. We further design customized Sparse-Gather operator and adopt the AirTopK operator to improve training and inference efficiency in industrial settings. Extensive experiments demonstrate that MSN consistently improves recommendation performance while maintaining high efficiency. Moreover, MSN has been successfully deployed in the Douyin Search Ranking System, achieving significant gains over deployed state-of-the-art models in both offline evaluation metrics and large-scale online A/B test.", "AI": {"tldr": "MSN\u662f\u4e00\u4e2a\u57fa\u4e8e\u5185\u5b58\u7684\u7a00\u758f\u6fc0\u6d3b\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u4e2a\u6027\u5316\u8868\u5f81\u5e76\u96c6\u6210\u5230\u4e0b\u6e38\u7279\u5f81\u4ea4\u4e92\u6a21\u5757\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e2a\u6027\u5316\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u63a8\u8350\u6a21\u578b\u6269\u5c55\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u5728\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u90e8\u7f72\uff1b\u7a00\u758f\u6fc0\u6d3b\u65b9\u6cd5\u5982\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u867d\u51cf\u5c11\u8ba1\u7b97\uff0c\u4f46\u4ecd\u5b58\u5728\u9ad8\u5185\u5b58\u8bbf\u95ee\u6210\u672c\u548c\u6709\u9650\u4e2a\u6027\u5316\u80fd\u529b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMSN\u6846\u67b6\uff1a1) \u4ece\u5927\u578b\u53c2\u6570\u5316\u5185\u5b58\u52a8\u6001\u68c0\u7d22\u4e2a\u6027\u5316\u8868\u5f81\uff1b2) \u901a\u8fc7\u5185\u5b58\u95e8\u63a7\u673a\u5236\u96c6\u6210\u5230\u4e0b\u6e38\u7279\u5f81\u4ea4\u4e92\u6a21\u5757\uff1b3) \u91c7\u7528\u4ea7\u54c1\u952e\u5185\u5b58(PKM)\u673a\u5236\u5c06\u68c0\u7d22\u590d\u6742\u5ea6\u4ece\u7ebf\u6027\u964d\u81f3\u4e9a\u7ebf\u6027\uff1b4) \u5f15\u5165\u5f52\u4e00\u5316\u548c\u8fc7\u53c2\u6570\u5316\u6280\u672f\u5e73\u8861\u5185\u5b58\u5229\u7528\uff1b5) \u8bbe\u8ba1\u5b9a\u5236Sparse-Gather\u7b97\u5b50\u548cAirTopK\u7b97\u5b50\u63d0\u5347\u5de5\u4e1a\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMSN\u80fd\u6301\u7eed\u63d0\u5347\u63a8\u8350\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u7387\uff1b\u5df2\u5728\u6296\u97f3\u641c\u7d22\u6392\u5e8f\u7cfb\u7edf\u6210\u529f\u90e8\u7f72\uff0c\u5728\u79bb\u7ebf\u8bc4\u4f30\u6307\u6807\u548c\u5927\u89c4\u6a21\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "MSN\u901a\u8fc7\u57fa\u4e8e\u5185\u5b58\u7684\u7a00\u758f\u6fc0\u6d3b\u6269\u5c55\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a8\u8350\u6a21\u578b\u6269\u5c55\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u4e2a\u6027\u5316\u80fd\u529b\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u4e2a\u6027\u5316\u7684\u63a8\u8350\u7cfb\u7edf\u90e8\u7f72\u3002"}}
{"id": "2602.07016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07016", "abs": "https://arxiv.org/abs/2602.07016", "authors": ["Mohsen Mostafa"], "title": "Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency", "comment": "10 pages, 3 figures, https://www.kaggle.com/code/babydriver1233/optimized-pipeline-for-the-image-matching-challeng, https://www.kaggle.com/code/babydriver1233/integrating-lejepa-for-enhanced-image-matching", "summary": "Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u65e0\u76d1\u77633D\u573a\u666f\u91cd\u5efa\u4e2d\u5e94\u7528\u9ad8\u65af\u7ea6\u675f\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u9488\u5bf9\u591a\u573a\u666f\u56fe\u50cf\u96c6\u5408\u4e2d\u7684\u89c6\u89c9\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eLeJEPA\u542f\u53d1\u7684\u7ba1\u9053\uff0c\u901a\u8fc7\u5404\u5411\u540c\u6027\u9ad8\u65af\u7ea6\u675f\u6539\u8fdb\u573a\u666f\u5206\u79bb\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u4ece\u975e\u7ed3\u6784\u5316\u56fe\u50cf\u96c6\u5408\u8fdb\u884c\u65e0\u76d1\u77633D\u573a\u666f\u91cd\u5efa\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u6765\u81ea\u591a\u4e2a\u4e0d\u76f8\u5173\u573a\u666f\u4e14\u5b58\u5728\u663e\u8457\u89c6\u89c9\u6a21\u7cca\u6027\u7684\u60c5\u51b5\u4e0b\u3002IMC2025\u6311\u6218\u8d5b\u7a81\u51fa\u4e86\u8fd9\u4e9b\u56f0\u96be\uff0c\u9700\u8981\u5728\u5305\u542b\u5f02\u5e38\u503c\u548c\u6df7\u5408\u5185\u5bb9\u7684\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u540c\u65f6\u8fdb\u884c\u573a\u666f\u53d1\u73b0\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u9010\u6b65\u6539\u8fdb\u7684\u7ba1\u9053\uff0c\u6700\u7ec8\u91c7\u7528LeJEPA\u542f\u53d1\u7684\u5404\u5411\u540c\u6027\u9ad8\u65af\u7ea6\u675f\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u5b66\u4e60\u7684\u56fe\u50cf\u5d4c\u5165\u4e0a\u5f3a\u5236\u6267\u884c\u9ad8\u65af\u7ea6\u675f\uff0c\u800c\u4e0d\u662f\u5f15\u5165\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e3b\u8981\u4ece\u7ecf\u9a8c\u4e0a\u8bc4\u4f30\u8fd9\u4e9b\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u805a\u7c7b\u4e00\u81f4\u6027\u548c\u59ff\u6001\u4f30\u8ba1\u9c81\u68d2\u6027\u3002", "result": "\u5728IMC2025\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u542f\u53d1\u5f0f\u57fa\u7ebf\u76f8\u6bd4\uff0c\u9ad8\u65af\u7ea6\u675f\u5d4c\u5165\u53ef\u4ee5\u6539\u5584\u573a\u666f\u5206\u79bb\u548c\u59ff\u6001\u5408\u7406\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u6a21\u7cca\u6027\u8bbe\u7f6e\u4e2d\u3002\u8fd9\u4e9b\u7ea6\u675f\u6709\u52a9\u4e8e\u63d0\u9ad8\u573a\u666f\u5206\u79bb\u80fd\u529b\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7406\u8bba\u4e0a\u52a8\u673a\u7684\u8868\u5f81\u7ea6\u675f\u4e3a\u6865\u63a5\u81ea\u76d1\u7763\u5b66\u4e60\u539f\u7406\u548c\u5b9e\u9645\u8fd0\u52a8\u7ed3\u6784\u7ba1\u9053\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002\u9ad8\u65af\u7ea6\u675f\u8868\u793a\u5728\u5b9e\u8df5\u4e2d\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u573a\u666f\u56fe\u50cf\u96c6\u5408\u4e2d\u7684\u89c6\u89c9\u6a21\u7cca\u6027\u95ee\u9898\u3002"}}
{"id": "2602.07238", "categories": ["cs.AI", "cs.LG", "econ.GN"], "pdf": "https://arxiv.org/pdf/2602.07238", "abs": "https://arxiv.org/abs/2602.07238", "authors": ["Matthias Mertens", "Natalia Fischl-Lanzoni", "Neil Thompson"], "title": "Is there \"Secret Sauce'' in Large Language Model Development?", "comment": null, "summary": "Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u6027\u80fd\u4e3b\u8981\u7531\u8ba1\u7b97\u89c4\u6a21\u9a71\u52a8\u800c\u975e\u4e13\u6709\u6280\u672f\uff0c\u524d\u6cbf\u6a21\u578b80-90%\u6027\u80fd\u5dee\u5f02\u6e90\u4e8e\u8bad\u7ec3\u8ba1\u7b97\u91cf\uff0c\u4f46\u975e\u524d\u6cbf\u9886\u57df\u4e13\u6709\u6280\u672f\u80fd\u663e\u8457\u964d\u4f4e\u8fbe\u5230\u7279\u5b9a\u80fd\u529b\u6240\u9700\u7684\u8ba1\u7b97\u91cf", "motivation": "\u63a2\u7a76LLM\u6027\u80fd\u63d0\u5347\u7684\u4e3b\u8981\u9a71\u52a8\u529b\uff1a\u662f\u5f00\u53d1\u8005\u7684\u4e13\u6709\u6280\u672f\uff08\"\u79d8\u65b9\"\uff09\u8fd8\u662f\u8ba1\u7b97\u89c4\u6a21\u7684\u6269\u5c55\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3AI\u9886\u5bfc\u529b\u548c\u80fd\u529b\u6269\u6563\u5177\u6709\u91cd\u8981\u610f\u4e49", "method": "\u4f7f\u75282022-2025\u5e74\u95f4\u53d1\u5e03\u7684809\u4e2a\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u57fa\u51c6\u6570\u636e\uff0c\u901a\u8fc7\u5305\u542b\u53d1\u5e03\u65e5\u671f\u548c\u5f00\u53d1\u8005\u56fa\u5b9a\u6548\u5e94\u7684\u7f29\u653e\u5b9a\u5f8b\u56de\u5f52\u5206\u6790", "result": "1) \u524d\u6cbf\u6a21\u578b80-90%\u6027\u80fd\u5dee\u5f02\u7531\u66f4\u9ad8\u8bad\u7ec3\u8ba1\u7b97\u91cf\u89e3\u91ca\uff1b2) \u975e\u524d\u6cbf\u9886\u57df\u4e13\u6709\u6280\u672f\u548c\u5171\u4eab\u7b97\u6cd5\u8fdb\u6b65\u663e\u8457\u964d\u4f4e\u8fbe\u5230\u56fa\u5b9a\u80fd\u529b\u9608\u503c\u6240\u9700\u8ba1\u7b97\u91cf\uff1b3) \u67d0\u4e9b\u516c\u53f8\u80fd\u7cfb\u7edf\u6027\u5730\u66f4\u9ad8\u6548\u751f\u4ea7\u8f83\u5c0f\u6a21\u578b\uff1b4) \u516c\u53f8\u5185\u90e8\u6a21\u578b\u6548\u7387\u5b58\u5728\u5de8\u5927\u5dee\u5f02\uff08\u53ef\u8fbe40\u500d\u4ee5\u4e0a\uff09", "conclusion": "\u524d\u6cbfAI\u8fdb\u6b65\u4e3b\u8981\u7531\u8ba1\u7b97\u89c4\u6a21\u9a71\u52a8\u800c\u975e\u4e13\u6709\u6280\u672f\uff0c\u4f46\u4e13\u6709\u6280\u672f\u5728\u975e\u524d\u6cbf\u9886\u57df\u80fd\u63d0\u9ad8\u6548\u7387\uff0c\u516c\u53f8\u5185\u90e8\u6548\u7387\u5dee\u5f02\u663e\u8457\uff0c\u8fd9\u5bf9AI\u9886\u5bfc\u529b\u548c\u80fd\u529b\u6269\u6563\u5177\u6709\u91cd\u8981\u542f\u793a"}}
{"id": "2602.06992", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.06992", "abs": "https://arxiv.org/abs/2602.06992", "authors": ["Xiaohui Zou", "Lijun Ke", "Shunpeng Zou"], "title": "A New Mode of Teaching Chinese as a Foreign Language from the Perspective of Smart System Studied by Using Rongzhixue", "comment": "11 pages, in Chinese language, 22 figures", "summary": "The purpose of this study is to introduce a new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its characteristics are as follows: focusing on the butterfly model of interpretation before translation, highlighting the new method of bilingual thinking training, on the one hand, applying the new theory of Chinese characters, the theory of the relationship between language and speech, and the forward-looking research results of language science; On the other hand, the application of the new model of teaching Chinese as a foreign language, AI empowering teaching and learning, and the forward-looking research results of educational science fully reflect a series of characteristics of the new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its beneficial effects are: not only the old view of language and education, especially the old view of teaching Chinese as a foreign language, but also the old view of human-computer interaction. Its significance lies in that a series of great cross-border Rongzhixue such as language, knowledge, education and teaching, as well as new methods and new topics of bilingual thinking training are clearly put forward from the perspective of integrating wisdom. Especially in the face of the challenge of Chat GPT to human learning ability and even creativity, the existing concepts of language knowledge education and teaching are already very backward. The old concepts of Chinese language education, and teaching Chinese as a foreign language are all facing a series of subversive innovation challenges. How to seek changes in adaptation? This study has made a series of innovative attempts, hoping to benefit academic colleagues, teachers and students.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ece\u878d\u667a\u5b66\u89c6\u89d2\u63d0\u51fa\u6c49\u8bed\u4f5c\u4e3a\u5916\u8bed\u6559\u5b66\u7684\u65b0\u6a21\u578b\uff0c\u5f3a\u8c03\u89e3\u91ca\u5148\u4e8e\u7ffb\u8bd1\u7684\u8774\u8776\u6a21\u578b\u548c\u53cc\u8bed\u601d\u7ef4\u8bad\u7ec3\u65b0\u65b9\u6cd5\uff0c\u6574\u5408\u8bed\u8a00\u79d1\u5b66\u548c\u6559\u80b2\u79d1\u5b66\u7684\u524d\u6cbf\u6210\u679c\uff0c\u5e94\u5bf9ChatGPT\u7b49AI\u6280\u672f\u5bf9\u4f20\u7edf\u8bed\u8a00\u6559\u80b2\u89c2\u5ff5\u7684\u98a0\u8986\u6027\u6311\u6218\u3002", "motivation": "\u9762\u5bf9ChatGPT\u7b49\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5bf9\u4eba\u7c7b\u5b66\u4e60\u80fd\u529b\u548c\u521b\u9020\u529b\u7684\u6311\u6218\uff0c\u4f20\u7edf\u7684\u8bed\u8a00\u77e5\u8bc6\u6559\u80b2\u89c2\u5ff5\u3001\u6c49\u8bed\u6559\u80b2\u89c2\u5ff5\u4ee5\u53ca\u5bf9\u5916\u6c49\u8bed\u6559\u5b66\u89c2\u5ff5\u5df2\u7ecf\u843d\u540e\uff0c\u9700\u8981\u8fdb\u884c\u98a0\u8986\u6027\u521b\u65b0\u3002\u7814\u7a76\u65e8\u5728\u4ece\u878d\u667a\u5b66\u89c6\u89d2\u5bfb\u6c42\u9002\u5e94\u53d8\u9769\u7684\u65b0\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u878d\u667a\u5b66\u89c6\u89d2\u4e0b\u7684\u5bf9\u5916\u6c49\u8bed\u6559\u5b66\u65b0\u6a21\u578b\uff0c\u91c7\u7528\u89e3\u91ca\u5148\u4e8e\u7ffb\u8bd1\u7684\u8774\u8776\u6a21\u578b\uff0c\u7a81\u51fa\u53cc\u8bed\u601d\u7ef4\u8bad\u7ec3\u65b0\u65b9\u6cd5\uff0c\u4e00\u65b9\u9762\u5e94\u7528\u6c49\u5b57\u65b0\u7406\u8bba\u3001\u8bed\u8a00\u4e0e\u8a00\u8bed\u5173\u7cfb\u7406\u8bba\u7b49\u8bed\u8a00\u79d1\u5b66\u524d\u6cbf\u6210\u679c\uff0c\u53e6\u4e00\u65b9\u9762\u5e94\u7528AI\u8d4b\u80fd\u6559\u5b66\u3001\u6559\u80b2\u79d1\u5b66\u524d\u6cbf\u6210\u679c\u3002", "result": "\u8be5\u6a21\u578b\u4e0d\u4ec5\u7a81\u7834\u4e86\u4f20\u7edf\u7684\u8bed\u8a00\u89c2\u3001\u6559\u80b2\u89c2\u548c\u5bf9\u5916\u6c49\u8bed\u6559\u5b66\u89c2\uff0c\u8fd8\u7a81\u7834\u4e86\u4f20\u7edf\u7684\u4eba\u673a\u4ea4\u4e92\u89c2\u5ff5\uff0c\u660e\u786e\u63d0\u51fa\u4e86\u8bed\u8a00\u3001\u77e5\u8bc6\u3001\u6559\u80b2\u6559\u5b66\u7b49\u4e00\u7cfb\u5217\u8de8\u754c\u878d\u667a\u5b66\u7684\u65b0\u65b9\u6cd5\u548c\u65b0\u8bfe\u9898\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u4e00\u7cfb\u5217\u521b\u65b0\u5c1d\u8bd5\uff0c\u4e3a\u5e94\u5bf9AI\u65f6\u4ee3\u5bf9\u8bed\u8a00\u6559\u80b2\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u8def\u5f84\uff0c\u5e0c\u671b\u4e3a\u5b66\u672f\u754c\u540c\u884c\u3001\u6559\u5e08\u548c\u5b66\u751f\u63d0\u4f9b\u6709\u76ca\u53c2\u8003\uff0c\u63a8\u52a8\u5bf9\u5916\u6c49\u8bed\u6559\u5b66\u7684\u521b\u65b0\u53d1\u5c55\u3002"}}
{"id": "2602.07739", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07739", "abs": "https://arxiv.org/abs/2602.07739", "authors": ["Hiren Madhu", "Ngoc Bui", "Ali Maatouk", "Leandros Tassiulas", "Smita Krishnaswamy", "Menglin Yang", "Sukanta Ganguly", "Kiran Srinivasan", "Rex Ying"], "title": "HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation", "comment": null, "summary": "Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval-augmented generation (RAG) remain largely confined to Euclidean space. However, natural language exhibits hierarchical structure from broad topics to specific entities that Euclidean embeddings fail to preserve, causing semantically distant documents to appear spuriously similar and increasing hallucination risk. To address these limitations, we introduce hyperbolic dense retrieval, developing two model variants in the Lorentz model of hyperbolic space: HyTE-FH, a fully hyperbolic transformer, and HyTE-H, a hybrid architecture projecting pre-trained Euclidean embeddings into hyperbolic space. To prevent representational collapse during sequence aggregation, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that provably preserves hierarchical structure. On MTEB, HyTE-FH outperforms equivalent Euclidean baselines, while on RAGBench, HyTE-H achieves up to 29% gains over Euclidean baselines in context relevance and answer relevance using substantially smaller models than current state-of-the-art retrievers. Our analysis also reveals that hyperbolic representations encode document specificity through norm-based separation, with over 20% radial increase from general to specific concepts, a property absent in Euclidean embeddings, underscoring the critical role of geometric inductive bias in faithful RAG systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u53cc\u66f2\u7a20\u5bc6\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u66f4\u597d\u5730\u6355\u6349\u81ea\u7136\u8bed\u8a00\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u76f8\u6bd4\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u8d28\u91cf\u548cRAG\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5177\u6709\u4ece\u5e7f\u6cdb\u4e3b\u9898\u5230\u5177\u4f53\u5b9e\u4f53\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4f46\u5f53\u524dRAG\u7cfb\u7edf\u4e2d\u4f7f\u7528\u7684\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\u65e0\u6cd5\u6709\u6548\u4fdd\u7559\u8fd9\u79cd\u7ed3\u6784\uff0c\u5bfc\u81f4\u8bed\u4e49\u4e0a\u8f83\u8fdc\u7684\u6587\u6863\u88ab\u9519\u8bef\u5730\u89c6\u4e3a\u76f8\u4f3c\uff0c\u589e\u52a0\u4e86\u5e7b\u89c9\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u66f2\u7a20\u5bc6\u68c0\u7d22\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u6a21\u578b\u53d8\u4f53\uff1a\u5b8c\u5168\u53cc\u66f2\u53d8\u6362\u5668HyTE-FH\u548c\u6df7\u5408\u67b6\u6784HyTE-H\u3002\u5f15\u5165\u4e86\u5411\u5916\u7231\u56e0\u65af\u5766\u4e2d\u70b9\u4f5c\u4e3a\u51e0\u4f55\u611f\u77e5\u7684\u6c60\u5316\u7b97\u5b50\uff0c\u9632\u6b62\u5e8f\u5217\u805a\u5408\u8fc7\u7a0b\u4e2d\u7684\u8868\u793a\u5d29\u6e83\u3002", "result": "\u5728MTEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHyTE-FH\u4f18\u4e8e\u7b49\u6548\u7684\u6b27\u51e0\u91cc\u5f97\u57fa\u7ebf\uff1b\u5728RAGBench\u4e0a\uff0cHyTE-H\u5728\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u7b54\u6848\u76f8\u5173\u6027\u65b9\u9762\u6bd4\u6b27\u51e0\u91cc\u5f97\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe29%\uff0c\u4e14\u4f7f\u7528\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u68c0\u7d22\u5668\u5c0f\u5f97\u591a\u7684\u6a21\u578b\u3002", "conclusion": "\u53cc\u66f2\u8868\u793a\u901a\u8fc7\u57fa\u4e8e\u8303\u6570\u7684\u5206\u79bb\u7f16\u7801\u6587\u6863\u7279\u5f02\u6027\uff0c\u4ece\u4e00\u822c\u6982\u5ff5\u5230\u5177\u4f53\u6982\u5ff5\u6709\u8d85\u8fc720%\u7684\u5f84\u5411\u589e\u52a0\uff0c\u8fd9\u4e00\u7279\u6027\u5728\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\u4e2d\u4e0d\u5b58\u5728\uff0c\u5f3a\u8c03\u4e86\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u5728\u5fe0\u5b9eRAG\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2602.07078", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07078", "abs": "https://arxiv.org/abs/2602.07078", "authors": ["Yingru Li", "Jiawei Xu", "Ziniu Li", "Jiacai Liu", "Wei Liu", "Yuxuan Tong", "Longtao Zheng", "Zhenghai Xue", "Yaxiang Zhang", "Tianle Cai", "Ge Zhang", "Qian Liu", "Baoxiang Wang"], "title": "The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL", "comment": null, "summary": "Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.", "AI": {"tldr": "\u63d0\u51faOptimal Token Baseline\u65b9\u6cd5\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\u7206\u70b8\u95ee\u9898\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u8303\u6570\u7684\u9006\u52a0\u6743\u66f4\u65b0\u5b9e\u73b0\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5927\u5e45\u51cf\u5c11token\u6d88\u8017", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5e38\u56e0\u68af\u5ea6\u65b9\u5dee\u7206\u70b8\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\u3002\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u5b58\u5728\u4f18\u5316\u56f0\u96be\u3001\u5ffd\u7565\u5e8f\u5217\u5f02\u8d28\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u5dee\u51cf\u5c11\u65b9\u6848", "method": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u51faOptimal Token Baseline\uff0c\u8bc1\u660e\u68af\u5ea6\u66f4\u65b0\u5e94\u6309\u5176\u7d2f\u79ef\u68af\u5ea6\u8303\u6570\u7684\u5012\u6570\u52a0\u6743\u3002\u63d0\u51faLogit-Gradient Proxy\u4ec5\u4f7f\u7528\u524d\u5411\u4f20\u64ad\u6982\u7387\u8fd1\u4f3c\u68af\u5ea6\u8303\u6570\uff0c\u786e\u4fdd\u8ba1\u7b97\u6548\u7387", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4ec5\u7528N=4\u5c31\u80fd\u8fbe\u5230N=32\u5927\u7ec4\u89c4\u6a21\u7684\u6027\u80fd\uff0c\u5728\u5355\u8f6e\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u4efb\u52a1\u4e2d\u51cf\u5c11\u8d85\u8fc765%\u7684token\u6d88\u8017", "conclusion": "OTB\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u65b9\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u9ad8\u6548\u5b9e\u73b0\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c"}}
{"id": "2602.07017", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07017", "abs": "https://arxiv.org/abs/2602.07017", "authors": ["Thuraya Alzubaidi", "Sana Ammar", "Maryam Alsharqi", "Islem Rekik", "Muzammil Behzad"], "title": "XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models", "comment": null, "summary": "Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\\% reduction in runtime, a 44.6\\% improvement in dice score, and a 96.7\\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.", "AI": {"tldr": "XAI-CLIP\uff1a\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7684ROI\u5f15\u5bfc\u6270\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u66f4\u6e05\u6670\u3001\u8fb9\u754c\u611f\u77e5\u7684\u663e\u8457\u6027\u56fe\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u4fe1\u4efb\u548c\u90e8\u7f72\u3002\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9700\u8981\u591a\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u4e14\u7ecf\u5e38\u4ea7\u751f\u566a\u58f0\u5927\u6216\u89e3\u5256\u5b66\u4e0a\u4e0d\u76f8\u5173\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51faXAI-CLIP\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u6765\u5b9a\u4f4d\u4e34\u5e8a\u76f8\u5173\u7684\u89e3\u5256\u533a\u57df\uff0c\u5e76\u6307\u5bfc\u89e3\u91ca\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u533a\u57df\u5b9a\u4f4d\u4e0e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5e94\u7528\u6709\u9488\u5bf9\u6027\u7684\u533a\u57df\u611f\u77e5\u6270\u52a8\u3002", "result": "\u5728FLARE22\u548cCHAOS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cXAI-CLIP\u76f8\u6bd4\u4f20\u7edf\u6270\u52a8\u65b9\u6cd5\u5b9e\u73b0\u4e86\uff1a\u8fd0\u884c\u65f6\u51cf\u5c1160%\uff0cDice\u5206\u6570\u63d0\u9ad844.6%\uff0c\u57fa\u4e8e\u906e\u6321\u7684\u89e3\u91ca\u7684IoU\u63d0\u9ad896.7%\u3002\u5b9a\u6027\u7ed3\u679c\u4e5f\u663e\u793a\u66f4\u5e72\u51c0\u3001\u89e3\u5256\u5b66\u4e0a\u4e00\u81f4\u7684\u5f52\u56e0\u56fe\u3002", "conclusion": "\u5c06\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u6574\u5408\u5230\u57fa\u4e8e\u6270\u52a8\u7684XAI\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5b9e\u73b0\u900f\u660e\u4e14\u53ef\u4e34\u5e8a\u90e8\u7f72\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2602.07253", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07253", "abs": "https://arxiv.org/abs/2602.07253", "authors": ["Litian Liu", "Reza Pourreza", "Yubing Jian", "Yao Qin", "Roland Memisevic"], "title": "From Out-of-Distribution Detection to Hallucination Detection: A Geometric View", "comment": null, "summary": "Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.", "AI": {"tldr": "\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u5e03\u5916\u68c0\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u5355\u6837\u672c\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u826f\u597d\u6548\u679c", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\u3002\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u9700\u8981\u66f4\u6709\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5", "method": "\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u89c6\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u5e94\u7528\u5206\u5e03\u5916\u68c0\u6d4b\u6280\u672f\uff0c\u5e76\u8fdb\u884c\u9002\u5f53\u4fee\u6539\u4ee5\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u7279\u70b9", "result": "\u57fa\u4e8e\u5206\u5e03\u5916\u68c0\u6d4b\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u5355\u6837\u672c\u7684\u68c0\u6d4b\u5668\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u7684\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86\u8f83\u5f3a\u7684\u51c6\u786e\u6027", "conclusion": "\u5c06\u5e7b\u89c9\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u5e03\u5916\u68c0\u6d4b\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u4e14\u53ef\u6269\u5c55\u7684\u9014\u5f84"}}
{"id": "2602.06998", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.06998", "abs": "https://arxiv.org/abs/2602.06998", "authors": ["Andhika Bernard Lumbantobing", "Hokky Situngkir"], "title": "Tokenizations for Austronesian Language Models: study on languages in Indonesia Archipelago", "comment": "14 pages, 3 figures", "summary": "Tokenization constitutes a fundamental stage in Large Language Model (LLM) processing; however, subword-based tokenization methods optimized on English-dominant corpora may produce token fragmentation misaligned with the linguistic structures of Austronesian languages. This study aimed to develop a syllable-based tokenization framework adopting principles from traditional Indonesian scripts (aksara) for regional languages of Indonesia. A syllabic segmentation procedure was constructed based on the logic of abugida writing systems and implemented with a vocabulary of 2,843 tokens extracted from the Indonesian dictionary (KBBI). Evaluation was conducted on the NusaX dataset comprising 1,000 parallel translation samples across 10 regional languages, Indonesian, and English. Analysis employed Token per Character (TPC) ratio and sequence alignment using the Smith-Waterman algorithm. Results demonstrated that syllable-based tokenization yielded consistent TPC values across all regional languages, whereas GPT-2 exhibited an inverse pattern with the lowest TPC for English. Syllable-based tokenization consistently produced higher token sequence similarity scores, with an average increase of approximately 21% compared to GPT-2. These findings confirm that the syllable-based approach more effectively preserves phonological and morphological patterns across related Austronesian languages, offering a linguistically principled foundation for multilingual LLM development.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8e\u97f3\u8282\u7684\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u91c7\u7528\u5370\u5c3c\u4f20\u7edf\u6587\u5b57\u539f\u5219\uff0c\u7528\u4e8e\u5370\u5c3c\u5730\u533a\u8bed\u8a00\u3002\u76f8\u6bd4GPT-2\u7684\u5b50\u8bcd\u6807\u8bb0\u5316\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u6807\u8bb0\u5e8f\u5217\u76f8\u4f3c\u6027\u4e0a\u5e73\u5747\u63d0\u9ad8\u7ea621%\uff0c\u66f4\u6709\u6548\u5730\u4fdd\u7559\u4e86\u5357\u5c9b\u8bed\u7cfb\u8bed\u8a00\u7684\u8bed\u97f3\u548c\u5f62\u6001\u6a21\u5f0f\u3002", "motivation": "\u57fa\u4e8e\u82f1\u8bed\u4e3b\u5bfc\u8bed\u6599\u5e93\u4f18\u5316\u7684\u5b50\u8bcd\u6807\u8bb0\u5316\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u4e0e\u5357\u5c9b\u8bed\u7cfb\u8bed\u8a00\u7ed3\u6784\u4e0d\u5339\u914d\u7684\u6807\u8bb0\u788e\u7247\u5316\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5408\u5370\u5c3c\u5730\u533a\u8bed\u8a00\u7684\u6807\u8bb0\u5316\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u963f\u5e03\u5409\u8fbe\u6587\u5b57\u7cfb\u7edf\u903b\u8f91\u7684\u97f3\u8282\u5206\u5272\u7a0b\u5e8f\uff0c\u4ece\u5370\u5c3c\u8bcd\u5178(KBBI)\u63d0\u53d62,843\u4e2a\u6807\u8bb0\u6784\u5efa\u8bcd\u6c47\u8868\u3002\u5728\u5305\u542b10\u79cd\u5730\u533a\u8bed\u8a00\u3001\u5370\u5c3c\u8bed\u548c\u82f1\u8bed\u7684NusaX\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528\u6bcf\u5b57\u7b26\u6807\u8bb0\u6bd4(TPC)\u548cSmith-Waterman\u7b97\u6cd5\u8fdb\u884c\u5e8f\u5217\u5bf9\u9f50\u5206\u6790\u3002", "result": "\u97f3\u8282\u6807\u8bb0\u5316\u5728\u6240\u6709\u5730\u533a\u8bed\u8a00\u4e2d\u4ea7\u751f\u4e00\u81f4\u7684TPC\u503c\uff0c\u800cGPT-2\u5bf9\u82f1\u8bed\u7684TPC\u6700\u4f4e\u3002\u97f3\u8282\u6807\u8bb0\u5316\u59cb\u7ec8\u4ea7\u751f\u66f4\u9ad8\u7684\u6807\u8bb0\u5e8f\u5217\u76f8\u4f3c\u6027\u5206\u6570\uff0c\u76f8\u6bd4GPT-2\u5e73\u5747\u63d0\u9ad8\u7ea621%\u3002", "conclusion": "\u57fa\u4e8e\u97f3\u8282\u7684\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u4fdd\u7559\u4e86\u76f8\u5173\u5357\u5c9b\u8bed\u7cfb\u8bed\u8a00\u7684\u8bed\u97f3\u548c\u5f62\u6001\u6a21\u5f0f\uff0c\u4e3a\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u8bed\u8a00\u5b66\u539f\u5219\u57fa\u7840\u3002"}}
{"id": "2602.07774", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07774", "abs": "https://arxiv.org/abs/2602.07774", "authors": ["Mingfu Liang", "Yufei Li", "Jay Xu", "Kavosh Asadi", "Xi Liu", "Shuo Gu", "Kaushik Rangadurai", "Frank Shyu", "Shuaiwen Wang", "Song Yang", "Zhijing Li", "Jiang Liu", "Mengying Sun", "Fei Tian", "Xiaohan Wei", "Chonglin Sun", "Jacob Tao", "Shike Mei", "Hamed Firooz", "Wenlin Chen", "Luke Simon"], "title": "Generative Reasoning Re-ranker", "comment": "31 pages", "summary": "Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2's effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.", "AI": {"tldr": "GR2\u662f\u4e00\u4e2a\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u91cd\u6392\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49ID\u7f16\u7801\u3001\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u6392\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u5c40\u9650\uff1a1) \u91cd\u6392\u9636\u6bb5\u88ab\u5ffd\u89c6\uff1b2) LLM\u7684\u63a8\u7406\u80fd\u529b\u672a\u5145\u5206\u5229\u7528\uff1b3) \u4f7f\u7528\u975e\u8bed\u4e49ID\u8868\u793a\u5bfc\u81f4\u5de5\u4e1a\u7cfb\u7edf\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51faGR2\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1) \u4f7f\u7528tokenizer\u5c06\u975e\u8bed\u4e49ID\u7f16\u7801\u4e3a\u8bed\u4e49ID\u8fdb\u884c\u4e2d\u8bad\u7ec3\uff1b2) \u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u62d2\u7edd\u91c7\u6837\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b3) \u5e94\u7528DAPO\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u76d1\u7763\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\uff0cGR2\u5728Recall@5\u548cNDCG@5\u4e0a\u5206\u522b\u8d85\u8fc7\u5f53\u524d\u6700\u4f73\u65b9\u6cd5OneRec-Think 2.4%\u548c1.3%\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u9ad8\u7ea7\u63a8\u7406\u8f68\u8ff9\u5e26\u6765\u663e\u8457\u63d0\u5347\uff0cRL\u5956\u52b1\u8bbe\u8ba1\u5bf9\u91cd\u6392\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "GR2\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u8350\u7cfb\u7edf\u7684\u91cd\u6392\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49ID\u3001\u9ad8\u8d28\u91cf\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u76d1\u7763\u7684\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\uff0c\u5e76\u4e3a\u5de5\u4e1a\u7ea7\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07259", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07259", "abs": "https://arxiv.org/abs/2602.07259", "authors": ["Cheol Woo Kim", "Davin Choo", "Tzeh Yuan Neoh", "Milind Tambe"], "title": "Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective", "comment": null, "summary": "As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06AI\u5b89\u5168\u89c6\u4e3aStackelberg\u5b89\u5168\u535a\u5f08\u95ee\u9898\uff0c\u901a\u8fc7\u535a\u5f08\u8bba\u6846\u67b6\u5206\u6790AI\u5f00\u53d1\u90e8\u7f72\u4e2d\u7684\u5bf9\u6297\u6027\u6fc0\u52b1\u95ee\u9898\uff0c\u5c06\u7b97\u6cd5\u5bf9\u9f50\u4e0e\u5236\u5ea6\u76d1\u7763\u8bbe\u8ba1\u76f8\u7ed3\u5408\u3002", "motivation": "\u73b0\u6709AI\u5b89\u5168\u6846\u67b6\u4e3b\u8981\u5c06\u5bf9\u9f50\u89c6\u4e3a\u9759\u6001\u4f18\u5316\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u6570\u636e\u6536\u96c6\u3001\u6a21\u578b\u8bc4\u4f30\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u5bf9\u6297\u6027\u6fc0\u52b1\u3002\u968f\u7740AI\u7cfb\u7edf\u80fd\u529b\u589e\u5f3a\uff0c\u9700\u8981\u6218\u7565\u6027\u5730\u76d1\u7763\u53c2\u4e0e\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u4eba\u7c7b\u548c\u673a\u6784\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eStackelberg\u5b89\u5168\u535a\u5f08\u7684\u65b0\u89c6\u89d2\uff0c\u5c06AI\u76d1\u7763\u89c6\u4e3a\u9632\u5fa1\u8005\uff08\u5ba1\u8ba1\u5458\u3001\u8bc4\u4f30\u8005\u3001\u90e8\u7f72\u8005\uff09\u4e0e\u653b\u51fb\u8005\uff08\u6076\u610f\u884c\u4e3a\u8005\u3001\u672a\u5bf9\u9f50\u8d21\u732e\u8005\u3001\u6700\u574f\u60c5\u51b5\u6545\u969c\u6a21\u5f0f\uff09\u4e4b\u95f4\u7684\u6218\u7565\u4e92\u52a8\uff0c\u4e3aAI\u751f\u547d\u5468\u671f\u4e2d\u7684\u6fc0\u52b1\u8bbe\u8ba1\u3001\u6709\u9650\u76d1\u7763\u80fd\u529b\u548c\u5bf9\u6297\u6027\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u53ef\u5e94\u7528\u4e8e\uff1a(1) \u8bad\u7ec3\u65f6\u5ba1\u8ba1\u5bf9\u6297\u6570\u636e/\u53cd\u9988\u6295\u6bd2\uff0c(2) \u6709\u9650\u8bc4\u5ba1\u8d44\u6e90\u4e0b\u7684\u9884\u90e8\u7f72\u8bc4\u4f30\uff0c(3) \u5bf9\u6297\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u591a\u6a21\u578b\u90e8\u7f72\u3002\u5c55\u793a\u4e86\u535a\u5f08\u8bba\u5a01\u6151\u5982\u4f55\u4f7fAI\u76d1\u7763\u53d8\u5f97\u4e3b\u52a8\u3001\u98ce\u9669\u611f\u77e5\u4e14\u6297\u64cd\u7eb5\u3002", "conclusion": "Stackelberg\u5b89\u5168\u535a\u5f08\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u7b97\u6cd5\u5bf9\u9f50\u4e0e\u5236\u5ea6\u76d1\u7763\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u5f3a\u8c03\u535a\u5f08\u8bba\u5a01\u6151\u53ef\u4ee5\u4f7fAI\u76d1\u7763\u66f4\u52a0\u4e3b\u52a8\u3001\u98ce\u9669\u611f\u77e5\u4e14\u6297\u64cd\u7eb5\uff0c\u4ece\u800c\u5e94\u5bf9AI\u5f00\u53d1\u90e8\u7f72\u4e2d\u7684\u52a8\u6001\u5bf9\u6297\u6027\u6fc0\u52b1\u95ee\u9898\u3002"}}
{"id": "2602.07025", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07025", "abs": "https://arxiv.org/abs/2602.07025", "authors": ["Daniele Savietto", "Declan Campbell", "Andr\u00e9 Panisson", "Marco Nurisso", "Giovanni Petri", "Jonathan D. Cohen", "Alan Perotti"], "title": "The Geometry of Representational Failures in Vision Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the \"Binding Problem\", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill \"concept vectors\" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8868\u5f81\u51e0\u4f55\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u591a\u76ee\u6807\u89c6\u89c9\u4efb\u52a1\u4e2d\u9519\u8bef\u6a21\u5f0f\u7684\u5185\u5728\u673a\u5236\uff0c\u53d1\u73b0\u6982\u5ff5\u5411\u91cf\u95f4\u7684\u51e0\u4f55\u91cd\u53e0\u4e0e\u7279\u5b9a\u9519\u8bef\u6a21\u5f0f\u5f3a\u76f8\u5173\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u76ee\u6807\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4ee4\u4eba\u56f0\u60d1\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5982\u5e7b\u89c9\u4e0d\u5b58\u5728\u7684\u5143\u7d20\u6216\u65e0\u6cd5\u5728\u5e72\u6270\u7269\u4e2d\u8bc6\u522b\u6700\u76f8\u4f3c\u7684\u5bf9\u8c61\u3002\u8fd9\u4e9b\u9519\u8bef\u53cd\u6620\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u8ba4\u77e5\u7ea6\u675f\u7684\"\u7ed1\u5b9a\u95ee\u9898\"\uff0c\u4f46\u4eba\u5de5\u7cfb\u7edf\u4e2d\u9a71\u52a8\u8fd9\u4e9b\u9519\u8bef\u7684\u5185\u90e8\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f00\u653e\u6743\u91cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Qwen\u3001InternVL\u3001Gemma\uff09\u7684\u8868\u5f81\u51e0\u4f55\u7ed3\u6784\uff0c\u6bd4\u8f83\u63d0\u53d6\"\u6982\u5ff5\u5411\u91cf\"\u7684\u65b9\u6cd5\u8bba\u2014\u2014\u8fd9\u4e9b\u5411\u91cf\u662f\u7f16\u7801\u89c6\u89c9\u6982\u5ff5\u7684\u6f5c\u5728\u65b9\u5411\u3002\u901a\u8fc7\u8f6c\u5411\u5e72\u9884\u9a8c\u8bc1\u6982\u5ff5\u5411\u91cf\uff0c\u5728\u7b80\u5316\u548c\u81ea\u7136\u4e3b\u4e49\u89c6\u89c9\u4efb\u52a1\u4e2d\u53ef\u9760\u5730\u64cd\u7eb5\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u89c2\u5bdf\u5230\u8fd9\u4e9b\u6982\u5ff5\u5411\u91cf\u4e4b\u95f4\u7684\u51e0\u4f55\u91cd\u53e0\u4e0e\u7279\u5b9a\u9519\u8bef\u6a21\u5f0f\u5f3a\u76f8\u5173\uff0c\u4e3a\u7406\u89e3\u5185\u90e8\u8868\u5f81\u5982\u4f55\u5851\u9020\u6a21\u578b\u884c\u4e3a\u548c\u9a71\u52a8\u89c6\u89c9\u5931\u8d25\u63d0\u4f9b\u4e86\u57fa\u4e8e\u91cf\u5316\u7684\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u673a\u5236\u6027\u89c1\u89e3\uff0c\u901a\u8fc7\u8868\u5f81\u51e0\u4f55\u5206\u6790\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u76ee\u6807\u89c6\u89c9\u4efb\u52a1\u4e2d\u9519\u8bef\u6a21\u5f0f\u7684\u5185\u5728\u9a71\u52a8\u56e0\u7d20\uff0c\u4e3a\u7406\u89e3\u548c\u8bca\u65ad\u6a21\u578b\u89c6\u89c9\u5931\u8d25\u63d0\u4f9b\u4e86\u5b9a\u91cf\u6846\u67b6\u3002"}}
{"id": "2602.07267", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07267", "abs": "https://arxiv.org/abs/2602.07267", "authors": ["Fengyuan Liu", "Jay Gala", "Nilaksh", "Dzmitry Bahdanau", "Siva Reddy", "Hugo Larochelle"], "title": "BRIDGE: Predicting Human Task Completion Time From Model Performance", "comment": null, "summary": "Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.", "AI": {"tldr": "BRIDGE\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u54cd\u5e94\u5b66\u4e60\u4efb\u52a1\u96be\u5ea6\u5c3a\u5ea6\uff0c\u5e76\u5c06\u5176\u4e0e\u4eba\u7c7b\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u951a\u5b9a\uff0c\u5b9e\u73b0\u4ece\u6a21\u578b\u6027\u80fd\u63a8\u65ad\u4eba\u7c7b\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u7c7b\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u6807\u6ce8\u7684\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u566a\u58f0\u5927\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u771f\u5b9e\u80fd\u529b", "method": "\u4f7f\u7528\u53cc\u53c2\u6570\u903b\u8f91\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u6a21\u578b\uff0c\u4ece\u591a\u4e2a\u57fa\u51c6\u7684\u6a21\u578b\u6027\u80fd\u6570\u636e\u4e2d\u8054\u5408\u4f30\u8ba1\u6f5c\u5728\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u80fd\u529b", "result": "\u6f5c\u5728\u4efb\u52a1\u96be\u5ea6\u4e0e\u4eba\u7c7b\u5b8c\u6210\u65f6\u95f4\u7684\u5bf9\u6570\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u80fd\u591f\u4ec5\u4ece\u6a21\u578b\u6027\u80fd\u63a8\u65ad\u65b0\u57fa\u51c6\u7684\u4eba\u7c7b\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u9884\u6d4b\u524d\u6cbf\u6a21\u578b\u80fd\u529b\u5e76\u91cd\u73b0METR\u7684\u6307\u6570\u7f29\u653e\u7ed3\u679c", "conclusion": "BRIDGE\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5fc3\u7406\u6d4b\u91cf\u6846\u67b6\uff0c\u5c06\u57fa\u51c6\u6027\u80fd\u4e0e\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u4efb\u52a1\u96be\u5ea6\u5ea6\u91cf\u76f8\u7ed3\u5408\uff0c\u4e3aAI\u7cfb\u7edf\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2602.07039", "categories": ["cs.CY", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.07039", "abs": "https://arxiv.org/abs/2602.07039", "authors": ["Heimo M\u00fcller"], "title": "When Excellence Stops Producing Knowledge: A Practitioner's Observation on Research Funding", "comment": null, "summary": "After almost four decades of participating in competitive research funding -- as applicant, coordinator, evaluator, and panel member -- I have come to see a structural paradox: many participants recognize that the current system is approaching its functional limits, yet most reform measures intensify rather than alleviate the underlying dynamics. This paper documents how excellence has become decoupled from knowledge production through an increasing coupling to representability under evaluation. The discussion focuses on two domains in which this is particularly visible: competitive basic research funding and large EU consortium projects. Three accelerating trends are examined: the professionalization of proposal writing through specialized consultants, the rise of AI-assisted applications, and an evaluator shortage that forces panels to rely on reviewers increasingly distant from the actual research domains. These observations are offered not as external critique but as an insider account, in the hope that naming a widely experienced but rarely articulated pattern may enable more constructive orientation.\n  Keywords: Research funding, Excellence, Evaluation, Goodhart's Law, Professionalization, AI-assisted proposals, Peer review crisis", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u7814\u7a76\u8d44\u52a9\u4f53\u7cfb\u7684\u7ed3\u6784\u6027\u6096\u8bba\uff1a\u53c2\u4e0e\u8005\u8ba4\u8bc6\u5230\u5f53\u524d\u4f53\u7cfb\u63a5\u8fd1\u529f\u80fd\u6781\u9650\uff0c\u4f46\u6539\u9769\u63aa\u65bd\u53cd\u800c\u52a0\u5267\u4e86\u6839\u672c\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\"\u5353\u8d8a\"\u5df2\u4e0e\u77e5\u8bc6\u751f\u4ea7\u8131\u94a9\uff0c\u800c\u4e0e\u8bc4\u4f30\u4e2d\u7684\"\u53ef\u5448\u73b0\u6027\"\u7d27\u5bc6\u8026\u5408\u3002", "motivation": "\u4f5c\u8005\u57fa\u4e8e\u8fd1\u56db\u5341\u5e74\u7684\u7814\u7a76\u8d44\u52a9\u53c2\u4e0e\u7ecf\u9a8c\uff08\u7533\u8bf7\u4eba\u3001\u534f\u8c03\u4eba\u3001\u8bc4\u4f30\u4eba\u3001\u8bc4\u5ba1\u7ec4\u6210\u5458\uff09\uff0c\u89c2\u5bdf\u5230\u5f53\u524d\u7ade\u4e89\u6027\u7814\u7a76\u8d44\u52a9\u4f53\u7cfb\u5b58\u5728\u7ed3\u6784\u6027\u77db\u76fe\u3002\u867d\u7136\u8bb8\u591a\u53c2\u4e0e\u8005\u8ba4\u8bc6\u5230\u7cfb\u7edf\u5df2\u63a5\u8fd1\u529f\u80fd\u6781\u9650\uff0c\u4f46\u6539\u9769\u63aa\u65bd\u5f80\u5f80\u52a0\u5267\u800c\u975e\u7f13\u89e3\u6839\u672c\u95ee\u9898\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5185\u90e8\u89c6\u89d2\u63ed\u793a\u8fd9\u4e00\u666e\u904d\u611f\u53d7\u4f46\u9c9c\u5c11\u88ab\u660e\u786e\u8868\u8ff0\u7684\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u8d28\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4f5c\u8005\u4f5c\u4e3a\u5185\u90e8\u53c2\u4e0e\u8005\u7684\u7ecf\u9a8c\u89c2\u5bdf\u3002\u91cd\u70b9\u8003\u5bdf\u4e24\u4e2a\u9886\u57df\uff1a\u7ade\u4e89\u6027\u57fa\u7840\u7814\u7a76\u8d44\u52a9\u548c\u5927\u578b\u6b27\u76df\u8054\u76df\u9879\u76ee\u3002\u5206\u6790\u4e09\u4e2a\u52a0\u901f\u8d8b\u52bf\uff1a\u63d0\u6848\u5199\u4f5c\u7684\u4e13\u4e1a\u5316\uff08\u901a\u8fc7\u4e13\u4e1a\u987e\u95ee\uff09\u3001AI\u8f85\u52a9\u7533\u8bf7\u5de5\u5177\u7684\u5174\u8d77\uff0c\u4ee5\u53ca\u8bc4\u5ba1\u4eba\u5458\u77ed\u7f3a\u5bfc\u81f4\u8bc4\u5ba1\u5c0f\u7ec4\u4f9d\u8d56\u4e0e\u5177\u4f53\u7814\u7a76\u9886\u57df\u65e5\u76ca\u758f\u8fdc\u7684\u8bc4\u5ba1\u4eba\u3002", "result": "\u7814\u7a76\u53d1\u73b0\"\u5353\u8d8a\"\u6982\u5ff5\u5df2\u4e0e\u77e5\u8bc6\u751f\u4ea7\u8131\u94a9\uff0c\u8f6c\u800c\u4e0e\u8bc4\u4f30\u4e2d\u7684\"\u53ef\u5448\u73b0\u6027\"\u7d27\u5bc6\u8026\u5408\u3002\u8fd9\u5bfc\u81f4\u4e86Goodhart\u5b9a\u5f8b\u7684\u4f53\u73b0\uff1a\u5f53\u8861\u91cf\u6807\u51c6\u6210\u4e3a\u76ee\u6807\u65f6\uff0c\u5b83\u5c31\u4e0d\u518d\u662f\u597d\u7684\u8861\u91cf\u6807\u51c6\u3002\u4e09\u4e2a\u8d8b\u52bf\u76f8\u4e92\u5f3a\u5316\uff0c\u5f62\u6210\u4e86\u81ea\u6211\u7ef4\u6301\u7684\u7cfb\u7edf\uff0c\u5176\u4e2d\u5f62\u5f0f\u8868\u73b0\u53d6\u4ee3\u4e86\u5b9e\u8d28\u5185\u5bb9\u3002", "conclusion": "\u5f53\u524d\u7814\u7a76\u8d44\u52a9\u4f53\u7cfb\u5b58\u5728\u7ed3\u6784\u6027\u6096\u8bba\uff0c\u6539\u9769\u63aa\u65bd\u5f80\u5f80\u52a0\u5267\u800c\u975e\u89e3\u51b3\u95ee\u9898\u3002\u901a\u8fc7\u660e\u786e\u547d\u540d\u8fd9\u4e00\u666e\u904d\u611f\u53d7\u4f46\u9c9c\u5c11\u88ab\u8868\u8ff0\u7684\u6a21\u5f0f\uff0c\u4f5c\u8005\u5e0c\u671b\u4e3a\u66f4\u5efa\u8bbe\u6027\u7684\u65b9\u5411\u63d0\u4f9b\u57fa\u7840\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u5c06\"\u5353\u8d8a\"\u4e0e\u771f\u6b63\u7684\u77e5\u8bc6\u751f\u4ea7\u91cd\u65b0\u8fde\u63a5\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u4f18\u5316\u5176\u5728\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2602.07026", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07026", "abs": "https://arxiv.org/abs/2602.07026", "authors": ["Xiaomin Yu", "Yi Xin", "Wenjie Zhang", "Chonghan Liu", "Hanzhen Zhao", "Xiaoxing Hu", "Xinlei Yu", "Ziyue Qiao", "Hao Tang", "Xue Yang", "Xiaobin Hu", "Chengwei Qin", "Hui Xiong", "Yu Qiao", "Shuicheng Yan"], "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models", "comment": null, "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReAlign\u548cReVision\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u5efa\u6a21\u6a21\u6001\u95f4\u9699\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5229\u7528\u5927\u89c4\u6a21\u975e\u914d\u5bf9\u6570\u636e\u5b9e\u73b0\u6a21\u6001\u5bf9\u9f50\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u6269\u5c55\u8def\u5f84\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5b58\u5728\u6a21\u6001\u95f4\u9699\u95ee\u9898\uff1a\u8868\u8fbe\u76f8\u540c\u8bed\u4e49\u7684\u4e0d\u540c\u6a21\u6001\u5d4c\u5165\u5360\u636e\u7cfb\u7edf\u504f\u79fb\u533a\u57df\u3002\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u8fc7\u5ea6\u7b80\u5316\u7684\u5404\u5411\u540c\u6027\u5047\u8bbe\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u3002", "method": "\u63d0\u51fa\u56fa\u5b9a\u6846\u67b6\u6a21\u6001\u95f4\u9699\u7406\u8bba\uff0c\u5c06\u6a21\u6001\u95f4\u9699\u5206\u89e3\u4e3a\u7a33\u5b9a\u504f\u5dee\u548c\u5404\u5411\u5f02\u6027\u6b8b\u5dee\u3002\u57fa\u4e8e\u6b64\u63d0\u51faReAlign\u8bad\u7ec3\u65e0\u5173\u7684\u6a21\u6001\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u951a\u70b9\u3001\u8ffd\u8e2a\u548c\u8d28\u5fc3\u5bf9\u9f50\u4e09\u6b65\u5c06\u6587\u672c\u8868\u793a\u5bf9\u9f50\u5230\u56fe\u50cf\u8868\u793a\u5206\u5e03\u3002\u8fdb\u4e00\u6b65\u63d0\u51faReVision\u8bad\u7ec3\u8303\u5f0f\uff0c\u5c06ReAlign\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u9636\u6bb5\u3002", "result": "ReAlign\u80fd\u591f\u660e\u786e\u4fee\u6b63\u51e0\u4f55\u9519\u4f4d\uff0cReVision\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u524d\u4ece\u672a\u914d\u5bf9\u6587\u672c\u4e2d\u5b66\u4e60\u89c6\u89c9\u8868\u793a\u5206\u5e03\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u3002\u7edf\u8ba1\u5bf9\u9f50\u7684\u975e\u914d\u5bf9\u6570\u636e\u53ef\u6709\u6548\u66ff\u4ee3\u6602\u8d35\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u6269\u5c55\u63d0\u4f9b\u4e86\u7a33\u5065\u8def\u5f84\uff0c\u901a\u8fc7\u7cbe\u786e\u5efa\u6a21\u6a21\u6001\u95f4\u9699\u51e0\u4f55\u5f62\u72b6\u5e76\u5229\u7528\u5927\u89c4\u6a21\u975e\u914d\u5bf9\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u6a21\u6001\u5bf9\u9f50\u7684\u6311\u6218\u3002"}}
{"id": "2602.07274", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07274", "abs": "https://arxiv.org/abs/2602.07274", "authors": ["Kaijie Zhu", "Yuzhou Nie", "Yijiang Li", "Yiming Huang", "Jialian Wu", "Jiang Liu", "Ximeng Sun", "Zhenfei Yin", "Lun Wang", "Zicheng Liu", "Emad Barsoum", "William Yang Wang", "Wenbo Guo"], "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents", "comment": null, "summary": "Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.", "AI": {"tldr": "TermiGen\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fed\u4ee3\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u7ec8\u7aef\u73af\u5883\u548c\u5305\u542b\u9519\u8bef\u7ea0\u6b63\u5faa\u73af\u7684\u4e13\u5bb6\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u7ec8\u7aef\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u73af\u5883\u7a00\u7f3a\u548c\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a1\uff09\u9ad8\u4fdd\u771f\u3001\u53ef\u6267\u884c\u7684\u8bad\u7ec3\u73af\u5883\u7a00\u7f3a\u2014\u2014\u4ece\u771f\u5b9e\u4e16\u754c\u4ed3\u5e93\u5408\u6210\u7684\u73af\u5883\u4e0d\u591f\u591a\u6837\u5316\u548c\u53ef\u6269\u5c55\uff0c\u800cLLM\u5408\u6210\u7684\u8f68\u8ff9\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff1b2\uff09\u6807\u51c6\u6307\u4ee4\u8c03\u4f18\u4f7f\u7528\u7684\u4e13\u5bb6\u8f68\u8ff9\u5f88\u5c11\u5305\u542b\u8f83\u5c0f\u6a21\u578b\u5e38\u89c1\u7684\u7b80\u5355\u9519\u8bef\uff0c\u5bfc\u81f4\u5b66\u751f\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u4ece\u81ea\u8eab\u8fd0\u884c\u65f6\u9519\u8bef\u4e2d\u6062\u590d\u3002", "method": "TermiGen\u91c7\u7528\u7aef\u5230\u7aef\u7ba1\u9053\uff1a\u9996\u5148\u901a\u8fc7\u8fed\u4ee3\u591a\u667a\u80fd\u4f53\u7cbe\u70bc\u5faa\u73af\u751f\u6210\u529f\u80fd\u6709\u6548\u7684\u4efb\u52a1\u548cDocker\u5bb9\u5668\uff1b\u7136\u540e\u91c7\u7528\u751f\u6210\u5668-\u6279\u8bc4\u5668\u534f\u8bae\uff0c\u5728\u8f68\u8ff9\u6536\u96c6\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u6ce8\u5165\u9519\u8bef\uff0c\u5408\u6210\u5bcc\u542b\u9519\u8bef\u7ea0\u6b63\u5faa\u73af\u7684\u6570\u636e\u3002", "result": "\u5728TermiGen\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u540e\uff0cTermiGen-Qwen2.5-Coder-32B\u5728TerminalBench\u4e0a\u8fbe\u5230\u4e8631.3%\u7684\u901a\u8fc7\u7387\uff0c\u521b\u9020\u4e86\u65b0\u7684\u5f00\u6e90\u6743\u91cd\u6700\u4f18\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86o4-mini\u7b49\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "TermiGen\u901a\u8fc7\u5408\u6210\u53ef\u9a8c\u8bc1\u73af\u5883\u548c\u5305\u542b\u9519\u8bef\u6062\u590d\u7684\u4e13\u5bb6\u8f68\u8ff9\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u590d\u6742\u7ec8\u7aef\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u5efa\u7acb\u4e86\u65b0\u7684\u5f00\u6e90\u6743\u91cd\u6700\u4f18\u57fa\u51c6\u3002"}}
{"id": "2602.08246", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.08246", "abs": "https://arxiv.org/abs/2602.08246", "authors": ["Atrisha Sarkar", "Isam Faik"], "title": "Structural transparency of societal AI alignment through Institutional Logics", "comment": null, "summary": "The field of AI alignment is increasingly concerned with the questions of how values are integrated into the design of generative AI systems and how their integration shapes the social consequences of AI. However, existing transparency frameworks focus on the informational aspects of AI models, data, and procedures, while the institutional and organizational forces that shape alignment decisions and their downstream effects remain underexamined in both research and practice. To address this gap, we develop a framework of \\emph{structural transparency} for analyzing organizational and institutional decisions concerning AI alignment, drawing on the theoretical lens of Institutional Logics. We develop a categorization of organizational decisions that are present in the governance of AI alignment, and provide an explicit analytical approach to examining them. We operationalize the framework through five analytical components, each with an accompanying \"analyst recipe\" that collectively identify the primary institutional logics and their internal relationships, external disruptions to existing social orders, and finally, how the structural risks of each institutional logic are mapped to a catalogue of sociotechnical harms. The proposed concept of structural transparency enables analysts to complement existing approached based on informational transparency with macro-level analyses that capture the institutional dynamics and consequences of decisions regarding AI alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\"\u7ed3\u6784\u900f\u660e\u5ea6\"\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790AI\u5bf9\u9f50\u4e2d\u7684\u7ec4\u7ec7\u548c\u5236\u5ea6\u51b3\u7b56\uff0c\u5f25\u8865\u73b0\u6709\u900f\u660e\u5ea6\u6846\u67b6\u53ea\u5173\u6ce8\u4fe1\u606f\u5c42\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u9f50\u9886\u57df\u5173\u6ce8\u4ef7\u503c\u89c2\u5982\u4f55\u878d\u5165\u751f\u6210\u5f0fAI\u7cfb\u7edf\u8bbe\u8ba1\u53ca\u5176\u793e\u4f1a\u540e\u679c\uff0c\u4f46\u73b0\u6709\u900f\u660e\u5ea6\u6846\u67b6\u4e3b\u8981\u5173\u6ce8AI\u6a21\u578b\u3001\u6570\u636e\u548c\u7a0b\u5e8f\u7684\u4fe1\u606f\u5c42\u9762\uff0c\u800c\u5f71\u54cd\u5bf9\u9f50\u51b3\u7b56\u53ca\u5176\u4e0b\u6e38\u6548\u5e94\u7684\u7ec4\u7ec7\u548c\u5236\u5ea6\u56e0\u7d20\u5728\u7814\u7a76\u548c\u5b9e\u8df5\u4e2d\u90fd\u672a\u5f97\u5230\u5145\u5206\u8003\u5bdf\u3002", "method": "\u57fa\u4e8e\u5236\u5ea6\u903b\u8f91\u7406\u8bba\u89c6\u89d2\uff0c\u5f00\u53d1\u4e86\u7ed3\u6784\u900f\u660e\u5ea6\u6846\u67b6\uff0c\u5bf9AI\u5bf9\u9f50\u6cbb\u7406\u4e2d\u7684\u7ec4\u7ec7\u51b3\u7b56\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u63d0\u4f9b\u660e\u786e\u7684\u5206\u6790\u65b9\u6cd5\u3002\u901a\u8fc7\u4e94\u4e2a\u5206\u6790\u7ec4\u4ef6\uff08\u6bcf\u4e2a\u90fd\u914d\u6709\"\u5206\u6790\u5e08\u914d\u65b9\"\uff09\u6765\u8bc6\u522b\u4e3b\u8981\u5236\u5ea6\u903b\u8f91\u53ca\u5176\u5185\u90e8\u5173\u7cfb\u3001\u73b0\u6709\u793e\u4f1a\u79e9\u5e8f\u7684\u5916\u90e8\u5e72\u6270\uff0c\u4ee5\u53ca\u6bcf\u4e2a\u5236\u5ea6\u903b\u8f91\u7684\u7ed3\u6784\u98ce\u9669\u5982\u4f55\u6620\u5c04\u5230\u793e\u4f1a\u6280\u672f\u5371\u5bb3\u76ee\u5f55\u3002", "result": "\u63d0\u51fa\u4e86\u7ed3\u6784\u900f\u660e\u5ea6\u6982\u5ff5\uff0c\u4f7f\u5206\u6790\u5e08\u80fd\u591f\u8865\u5145\u73b0\u6709\u7684\u57fa\u4e8e\u4fe1\u606f\u900f\u660e\u5ea6\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b8f\u89c2\u5c42\u9762\u5206\u6790\u6355\u6349AI\u5bf9\u9f50\u51b3\u7b56\u7684\u5236\u5ea6\u52a8\u6001\u548c\u540e\u679c\u3002", "conclusion": "\u7ed3\u6784\u900f\u660e\u5ea6\u6846\u67b6\u4e3a\u5206\u6790AI\u5bf9\u9f50\u4e2d\u7684\u7ec4\u7ec7\u548c\u5236\u5ea6\u51b3\u7b56\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u7406\u89e3AI\u7cfb\u7edf\u4ef7\u503c\u89c2\u6574\u5408\u7684\u793e\u4f1a\u5f71\u54cd\u548c\u6cbb\u7406\u6311\u6218\u3002"}}
{"id": "2602.07987", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07987", "abs": "https://arxiv.org/abs/2602.07987", "authors": ["Zheng Ren", "Yi Wu", "Jianan Lu", "Acar Ary", "Yiqu Liu", "Li Wei", "Lukasz Heldt"], "title": "Learning to Alleviate Familiarity Bias in Video Recommendation", "comment": "Accepted to the Companion Proceedings of the ACM Web Conference 2026 (WWW '26), April 13-17, 2026, Dubai, UAE", "summary": "Modern video recommendation systems aim to optimize user engagement and platform objectives, yet often face structural exposure imbalances caused by behavioral biases. In this work, we focus on the post-ranking stage and present LAFB (Learning to Alleviate Familiarity Bias), a lightweight and model-agnostic framework designed to mitigate familiarity bias in recommendation outputs. LAFB models user-content familiarity using discrete and continuous interaction features, and estimates personalized debiasing factors to adjust user rating prediction scores, thereby reducing the dominance of familiar content in the final ranking. We conduct large-scale offline evaluations and online A/B testing in a real-world recommendation system, under a unified serving stack that also compares LAFB with deployable popularity-oriented remedies. Results show that LAFB increases novel watch-time share and improves exposure for emerging creators and overall content diversity, while maintaining stable overall watch time and short-term satisfaction. LAFB has already been launched in the post-ranking stage of YouTube's recommendation system, demonstrating its effectiveness in real-world applications.", "AI": {"tldr": "LAFB\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7f13\u89e3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u719f\u6089\u5ea6\u504f\u5dee\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237-\u5185\u5bb9\u719f\u6089\u5ea6\u5e76\u8c03\u6574\u9884\u6d4b\u5206\u6570\u6765\u589e\u52a0\u5185\u5bb9\u591a\u6837\u6027\u3002", "motivation": "\u73b0\u4ee3\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u7ed3\u6784\u6027\u66dd\u5149\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u884c\u4e3a\u504f\u5dee\u5f15\u8d77\uff0c\u7279\u522b\u662f\u719f\u6089\u5ea6\u504f\u5dee\u5bfc\u81f4\u719f\u6089\u5185\u5bb9\u8fc7\u5ea6\u4e3b\u5bfc\u63a8\u8350\u7ed3\u679c\u3002", "method": "LAFB\u5728\u91cd\u6392\u5e8f\u9636\u6bb5\u5de5\u4f5c\uff0c\u4f7f\u7528\u79bb\u6563\u548c\u8fde\u7eed\u4ea4\u4e92\u7279\u5f81\u5efa\u6a21\u7528\u6237-\u5185\u5bb9\u719f\u6089\u5ea6\uff0c\u4f30\u8ba1\u4e2a\u6027\u5316\u53bb\u504f\u56e0\u5b50\u6765\u8c03\u6574\u7528\u6237\u8bc4\u5206\u9884\u6d4b\u5206\u6570\uff0c\u4ece\u800c\u51cf\u5c11\u719f\u6089\u5185\u5bb9\u5728\u6700\u7ec8\u6392\u540d\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\u3002", "result": "\u5927\u89c4\u6a21\u79bb\u7ebf\u8bc4\u4f30\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0cLAFB\u589e\u52a0\u4e86\u65b0\u9896\u89c2\u770b\u65f6\u95f4\u4efd\u989d\uff0c\u6539\u5584\u4e86\u65b0\u5174\u521b\u4f5c\u8005\u66dd\u5149\u548c\u6574\u4f53\u5185\u5bb9\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5b9a\u7684\u603b\u4f53\u89c2\u770b\u65f6\u95f4\u548c\u77ed\u671f\u6ee1\u610f\u5ea6\u3002", "conclusion": "LAFB\u5df2\u6210\u529f\u90e8\u7f72\u5728YouTube\u63a8\u8350\u7cfb\u7edf\u7684\u91cd\u6392\u5e8f\u9636\u6bb5\uff0c\u8bc1\u660e\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u80fd\u6709\u6548\u7f13\u89e3\u719f\u6089\u5ea6\u504f\u5dee\uff0c\u5e73\u8861\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u5185\u5bb9\u591a\u6837\u6027\u76ee\u6807\u3002"}}
{"id": "2602.07027", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07027", "abs": "https://arxiv.org/abs/2602.07027", "authors": ["Sanggeon Yun", "Ryozo Masukawa", "SungHeon Jeong", "Wenjun Huang", "Hanning Chen", "Mohsen Imani"], "title": "Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.", "AI": {"tldr": "FCL\u662f\u4e00\u79cd\u907f\u514d\u71b5\u6700\u5c0f\u5316\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u516c\u5e73\u6027\u7ea6\u675f\u89e3\u51b3\u5171\u4eab\u8bc1\u636e\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684TTA\u65b9\u6cd5\u4f9d\u8d56\u71b5\u6700\u5c0f\u5316\uff0c\u4f46\u5728\u7c7b\u522b\u5171\u4eab\u89c6\u89c9\u7279\u5f81\u65f6\u53ef\u80fd\u653e\u5927\u865a\u5047\u76f8\u5173\u6027\u5e76\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u3002\u9700\u8981\u4e00\u79cd\u907f\u514d\u71b5\u6700\u5c0f\u5316\u3001\u80fd\u89e3\u51b3\u5171\u4eab\u8bc1\u636e\u504f\u5dee\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u516c\u5e73\u4e0a\u4e0b\u6587\u5b66\u4e60(FCL)\uff1a1) \u57fa\u4e8e\u589e\u5f3a\u7684\u63a2\u7d22\u8bc6\u522b\u53ef\u80fd\u7684\u7c7b\u522b\u5019\u9009\uff1b2) \u516c\u5e73\u9a71\u52a8\u7684\u6821\u51c6\uff0c\u901a\u8fc7\u516c\u5e73\u6027\u7ea6\u675f\u8c03\u6574\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u4f7f\u5bf9\u5171\u540c\u89c6\u89c9\u8bc1\u636e\u7684\u654f\u611f\u6027\u5747\u7b49\u5316\u3002", "result": "FCL\u5728\u591a\u79cd\u9886\u57df\u504f\u79fb\u548c\u7ec6\u7c92\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u7684TTA\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u81ea\u9002\u5e94\u6027\u80fd\uff0c\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u7406\u8bba\u52a8\u673a\u3002", "conclusion": "FCL\u901a\u8fc7\u907f\u514d\u71b5\u6700\u5c0f\u5316\u548c\u5f15\u5165\u516c\u5e73\u6027\u7ea6\u675f\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u90e8\u5206\u7279\u5f81\u75f4\u8ff7\u95ee\u9898\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u71b5\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u6821\u51c6\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.07276", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07276", "abs": "https://arxiv.org/abs/2602.07276", "authors": ["Pengrui Han", "Xueqiang Xu", "Keyang Xuan", "Peiyang Song", "Siru Ouyang", "Runchu Tian", "Yuqing Jiang", "Cheng Qian", "Pengcheng Jiang", "Jiashuo Sun", "Junxia Cui", "Ming Zhong", "Ge Liu", "Jiawei Han", "Jiaxuan You"], "title": "Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs", "comment": null, "summary": "Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.", "AI": {"tldr": "STEER2ADAPT\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u5408\u800c\u975e\u4ece\u5934\u5b66\u4e60\u65b0\u7684\u5bfc\u5411\u5411\u91cf\u6765\u9002\u914d\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u63a8\u7406\u548c\u5b89\u5168\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53478.2%", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u6bcf\u4e2a\u4efb\u52a1\u6216\u6982\u5ff5\u7684\u5355\u4e00\u9759\u6001\u65b9\u5411\uff0c\u5728\u4efb\u52a1\u53d8\u5316\u4e0b\u4e0d\u591f\u7075\u6d3b\uff0c\u4e14\u65e0\u6cd5\u5904\u7406\u9700\u8981\u591a\u79cd\u534f\u8c03\u80fd\u529b\u7684\u590d\u6742\u4efb\u52a1", "method": "\u63d0\u51faSTEER2ADAPT\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5171\u4eab\u7684\u5e95\u5c42\u6982\u5ff5\u7ef4\u5ea6\u6355\u83b7\u4e3a\u53ef\u91cd\u7528\u7684\u4f4e\u7ef4\u8bed\u4e49\u5148\u9a8c\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u52a8\u6001\u53d1\u73b0\u57fa\u5411\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u9002\u914d\u65b0\u4efb\u52a1", "result": "\u57289\u4e2a\u4efb\u52a1\u548c3\u4e2a\u6a21\u578b\u7684\u63a8\u7406\u548c\u5b89\u5168\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0cSTEER2ADAPT\u5e73\u5747\u63d0\u53478.2%\uff0c\u8bc1\u660e\u5176\u662f\u6570\u636e\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u900f\u660e\u7684\u63a8\u7406\u65f6\u9002\u914d\u65b9\u6cd5", "conclusion": "STEER2ADAPT\u901a\u8fc7\u7ec4\u5408\u73b0\u6709\u5bfc\u5411\u5411\u91cf\u800c\u975e\u4ece\u5934\u5b66\u4e60\uff0c\u4e3aLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u9002\u914d\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u591a\u79cd\u534f\u8c03\u80fd\u529b\u7684\u590d\u6742\u4efb\u52a1"}}
{"id": "2602.08299", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08299", "abs": "https://arxiv.org/abs/2602.08299", "authors": ["Hibiki Ito", "Chia-Yu Hsu", "Hiroaki Ogata"], "title": "Cyclic Adaptive Private Synthesis for Sharing Real-World Data in Education", "comment": "10 pages, 3 figures. Accepted for LAK2026", "summary": "The rapid adoption of digital technologies has greatly increased the volume of real-world data (RWD) in education. While these data offer significant opportunities for advancing learning analytics (LA), secondary use for research is constrained by privacy concerns. Differentially private synthetic data generation is regarded as the gold-standard approach to sharing sensitive data, yet studies on the private synthesis of educational data remain very scarce and rely predominantly on large, low-dimensional open datasets. Educational RWD, however, are typically high-dimensional and small in sample size, leaving the potential of private synthesis underexplored. Moreover, because educational practice is inherently iterative, data sharing is continual rather than one-off, making a traditional one-shot synthesis approach suboptimal. To address these challenges, we propose the Cyclic Adaptive Private Synthesis (CAPS) framework and evaluate it on authentic RWD. By iteratively sharing RWD, CAPS not only fosters open science, but also offers rich opportunities of design-based research (DBR), thereby amplifying the impact of LA. Our case study using actual RWD demonstrates that CAPS outperforms a one-shot baseline while highlighting challenges that warrant further investigation. Overall, this work offers a crucial first step towards privacy-preserving sharing of educational RWD and expands the possibilities for open science and DBR in LA.", "AI": {"tldr": "\u63d0\u51faCAPS\u6846\u67b6\u89e3\u51b3\u6559\u80b2\u771f\u5b9e\u4e16\u754c\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u901a\u8fc7\u5faa\u73af\u81ea\u9002\u5e94\u5408\u6210\u6570\u636e\u4fc3\u8fdb\u5f00\u653e\u79d1\u5b66\u548c\u57fa\u4e8e\u8bbe\u8ba1\u7684\u7814\u7a76", "motivation": "\u6559\u80b2\u9886\u57df\u6570\u5b57\u6280\u672f\u5feb\u901f\u5e94\u7528\u4ea7\u751f\u4e86\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u4f46\u9690\u79c1\u95ee\u9898\u9650\u5236\u4e86\u8fd9\u4e9b\u6570\u636e\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u7684\u4e8c\u6b21\u4f7f\u7528\u3002\u73b0\u6709\u5dee\u5206\u9690\u79c1\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5927\u89c4\u6a21\u4f4e\u7ef4\u5f00\u653e\u6570\u636e\u96c6\uff0c\u800c\u6559\u80b2\u6570\u636e\u901a\u5e38\u9ad8\u7ef4\u4e14\u6837\u672c\u91cf\u5c0f\uff0c\u540c\u65f6\u6559\u80b2\u5b9e\u8df5\u5177\u6709\u8fed\u4ee3\u6027\uff0c\u9700\u8981\u6301\u7eed\u800c\u975e\u4e00\u6b21\u6027\u6570\u636e\u5171\u4eab", "method": "\u63d0\u51fa\u5faa\u73af\u81ea\u9002\u5e94\u9690\u79c1\u5408\u6210\uff08CAPS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u65b9\u5f0f\u5171\u4eab\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u652f\u6301\u6301\u7eed\u7684\u6570\u636e\u5408\u6210\u800c\u975e\u4e00\u6b21\u6027\u5408\u6210\uff0c\u9002\u5e94\u6559\u80b2\u5b9e\u8df5\u7684\u8fed\u4ee3\u7279\u6027", "result": "\u5728\u771f\u5b9e\u6559\u80b2\u6570\u636e\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cCAPS\u6846\u67b6\u4f18\u4e8e\u4f20\u7edf\u4e00\u6b21\u6027\u5408\u6210\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u6311\u6218", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6559\u80b2\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u5171\u4eab\u63d0\u4f9b\u4e86\u91cd\u8981\u7b2c\u4e00\u6b65\uff0c\u6269\u5c55\u4e86\u5b66\u4e60\u5206\u6790\u4e2d\u5f00\u653e\u79d1\u5b66\u548c\u57fa\u4e8e\u8bbe\u8ba1\u7814\u7a76\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u6301\u7eed\u3001\u8fed\u4ee3\u7684\u6570\u636e\u5171\u4eab\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08070", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08070", "abs": "https://arxiv.org/abs/2602.08070", "authors": ["Lam Thanh Do", "Bhagyashree Taleka", "Hozaifa Ammar Bhutta", "Vikram Sharma Mailthody", "Kevin Chen-Chuan Chang", "Wen-mei Hwu"], "title": "IRB: Automated Generation of Robust Factuality Benchmarks", "comment": "Code: https://github.com/Hozaifa-Bhutta/IRB", "summary": "Static benchmarks for RAG systems often suffer from rapid saturation and require significant manual effort to maintain robustness. To address this, we present IRB, a framework for automatically generating benchmarks to evaluate the factuality of RAG systems. IRB employs a structured generation pipeline utilizing \\textit{factual scaffold} and \\textit{algorithmic scaffold}. We utilize IRB to construct a benchmark and evaluate frontier LLMs and retrievers. Our results demonstrate that IRB poses a significant challenge for frontier LLMs in the closed-book setting. Furthermore, our evaluation suggests that reasoning LLMs are more reliable, and that improving the retrieval component may yield more cost-effective gains in RAG system correctness than scaling the generator.", "AI": {"tldr": "IRB\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u6613\u9971\u548c\u4e14\u7ef4\u62a4\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u7684\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5feb\u901f\u9971\u548c\u95ee\u9898\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u7ef4\u62a4\u6765\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u7684\u57fa\u51c6\u751f\u6210\u65b9\u6cd5\u3002", "method": "IRB\u6846\u67b6\u91c7\u7528\u7ed3\u6784\u5316\u751f\u6210\u7ba1\u9053\uff0c\u5229\u7528\"\u4e8b\u5b9e\u652f\u67b6\"\u548c\"\u7b97\u6cd5\u652f\u67b6\"\u81ea\u52a8\u751f\u6210\u8bc4\u4f30RAG\u7cfb\u7edf\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "IRB\u5bf9\u524d\u6cbfLLM\u5728\u95ed\u5377\u8bbe\u7f6e\u4e0b\u6784\u6210\u663e\u8457\u6311\u6218\uff1b\u63a8\u7406\u578bLLM\u66f4\u53ef\u9760\uff1b\u6539\u8fdb\u68c0\u7d22\u7ec4\u4ef6\u6bd4\u6269\u5c55\u751f\u6210\u5668\u80fd\u5e26\u6765\u66f4\u7ecf\u6d4e\u6709\u6548\u7684RAG\u7cfb\u7edf\u6b63\u786e\u6027\u63d0\u5347\u3002", "conclusion": "IRB\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u68c0\u7d22\u7ec4\u4ef6\u4f18\u5316\u7684\u6210\u672c\u6548\u76ca\u4f18\u52bf\u3002"}}
{"id": "2602.07144", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07144", "abs": "https://arxiv.org/abs/2602.07144", "authors": ["Samuel Daulton", "David Eriksson", "Maximilian Balandat", "Eytan Bakshy"], "title": "BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability", "comment": "26 pages", "summary": "Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.", "AI": {"tldr": "BONSAI\u662f\u4e00\u79cd\u9ed8\u8ba4\u611f\u77e5\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u4f18\u5316\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u5bf9\u9ed8\u8ba4\u914d\u7f6e\u7684\u504f\u79bb\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6700\u5c0f\u5316\u53c2\u6570\u53d8\u5316\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u6807\u51c6\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u5904\u7406\u5e26\u6709\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u9ed8\u8ba4\u914d\u7f6e\u7684\u53c2\u6570\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5b83\u4f1a\u5c06\u5f31\u76f8\u5173\u53c2\u6570\u63a8\u5230\u641c\u7d22\u7a7a\u95f4\u8fb9\u754c\uff0c\u96be\u4ee5\u533a\u5206\u91cd\u8981\u548c\u865a\u5047\u7684\u53d8\u5316\uff0c\u589e\u52a0\u4e86\u9a8c\u8bc1\u63a8\u8350\u914d\u7f6e\u7684\u8d1f\u62c5\u3002", "method": "BONSAI\u662f\u4e00\u79cd\u9ed8\u8ba4\u611f\u77e5\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u526a\u679d\u5bf9\u9ed8\u8ba4\u914d\u7f6e\u7684\u4f4e\u5f71\u54cd\u504f\u79bb\uff0c\u540c\u65f6\u660e\u786e\u63a7\u5236\u83b7\u53d6\u51fd\u6570\u7684\u635f\u5931\u3002\u8be5\u65b9\u6cd5\u517c\u5bb9\u591a\u79cd\u83b7\u53d6\u51fd\u6570\uff0c\u5305\u62ec\u671f\u671b\u6539\u8fdb\u548c\u4e0a\u7f6e\u4fe1\u754c\u3002", "result": "BONSAI\u5728\u7406\u8bba\u4e0a\u5177\u6709\u4e0e\u6807\u51c6GP-UCB\u76f8\u540c\u7684\u65e0\u9057\u61be\u6027\u8d28\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5b83\u80fd\u663e\u8457\u51cf\u5c11\u63a8\u8350\u914d\u7f6e\u4e2d\u7684\u975e\u9ed8\u8ba4\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u4f18\u5316\u6027\u80fd\uff0c\u5bf9\u8fd0\u884c\u65f6\u95f4\u5f71\u54cd\u5f88\u5c0f\u3002", "conclusion": "BONSAI\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4f18\u5316\u6548\u679c\u7684\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u9ed8\u8ba4\u914d\u7f6e\u7684\u504f\u79bb\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5e73\u8861\u4f18\u5316\u6027\u80fd\u548c\u53c2\u6570\u53d8\u5316\u6700\u5c0f\u5316\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.07028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07028", "abs": "https://arxiv.org/abs/2602.07028", "authors": ["Kaaustaaub Shankar", "Bharadwaj Dogga", "Kelly Cohen"], "title": "A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures", "comment": "Accepted to NAFIPS 2026", "summary": "Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.", "AI": {"tldr": "ANFIS\u589e\u5f3a\u7684CNN\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u8868\u73b0\u4e0d\u4e00\u81f4\uff1aResNet18-ANFIS\u6709\u6240\u6539\u5584\uff0cVGG-ANFIS\u5219\u8868\u73b0\u66f4\u5dee\uff0c\u8868\u660e\u795e\u7ecf\u6a21\u7cca\u589e\u5f3a\u5e76\u975e\u666e\u904d\u6709\u6548", "motivation": "\u4f20\u7edfCNN\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u795e\u7ecf\u6a21\u7cca\u6df7\u5408\u6a21\u578b\u5982DCNFIS\u901a\u8fc7ANFIS\u66ff\u4ee3\u5168\u8fde\u63a5\u5c42\u6765\u63d0\u5347\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76", "method": "\u6bd4\u8f83\u6807\u51c6CNN\uff08ConvNet\u3001VGG\u3001ResNet18\uff09\u4e0e\u5176ANFIS\u589e\u5f3a\u7248\u672c\u5728MNIST\u3001Fashion-MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684PGD\u653b\u51fb\u548c\u65e0\u68af\u5ea6\u7684Square\u653b\u51fb\u8fdb\u884c\u9c81\u68d2\u6027\u8bc4\u4f30", "result": "ANFIS\u96c6\u6210\u5e76\u672a\u4e00\u81f4\u63d0\u5347\u5e72\u51c0\u51c6\u786e\u7387\uff0c\u4e14\u5bf9\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u5177\u6709\u67b6\u6784\u4f9d\u8d56\u6027\uff1aResNet18-ANFIS\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u800cVGG-ANFIS\u901a\u5e38\u8868\u73b0\u4e0d\u5982\u5176\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u795e\u7ecf\u6a21\u7cca\u589e\u5f3a\u53ef\u4ee5\u5728\u7279\u5b9a\u67b6\u6784\u4e2d\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u4f46\u5e76\u975e\u666e\u904d\u6709\u76ca\uff0c\u9700\u8981\u9488\u5bf9\u5177\u4f53\u67b6\u6784\u8fdb\u884c\u4ed4\u7ec6\u8bc4\u4f30"}}
{"id": "2602.07308", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07308", "abs": "https://arxiv.org/abs/2602.07308", "authors": ["Sutapa Dey Tithi", "Nazia Alam", "Tahreem Yasir", "Yang Shi", "Xiaoyi Tian", "Min Chi", "Tiffany Barnes"], "title": "Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System", "comment": null, "summary": "The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e24\u79cdICAP\u6a21\u5f0f\u7684\u4f8b\u9898\uff08\u4e3b\u52a8\u5f0f\u5f15\u5bfc\u4f8b\u9898\u548c\u5efa\u6784\u5f0f\u9519\u8bef\u4f8b\u9898\uff09\u6765\u4f18\u5316\u8ba4\u77e5\u53c2\u4e0e\u5ea6\uff0c\u6bd4\u8f83\u4e86BKT\u548cDRL\u4e24\u79cd\u81ea\u9002\u5e94\u65b9\u6cd5\u4e0e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e24\u79cd\u81ea\u9002\u5e94\u7b56\u7565\u90fd\u80fd\u663e\u8457\u63d0\u5347\u5b66\u751f\u6d4b\u8bd5\u6210\u7ee9\u3002", "motivation": "ICAP\u6846\u67b6\u5b9a\u4e49\u4e86\u56db\u79cd\u8ba4\u77e5\u53c2\u4e0e\u6c34\u5e73\uff0c\u4f46\u4e2a\u6027\u5316\u5b66\u4e60\u6d3b\u52a8\u4ee5\u6fc0\u53d1\u6700\u4f73\u8ba4\u77e5\u53c2\u4e0e\u6c34\u5e73\u5728\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u9002\u5e94\u5730\u652f\u6301\u8ba4\u77e5\u53c2\u4e0e\u7684\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e24\u79cdICAP\u6a21\u5f0f\u7684\u4f8b\u9898\u6765\u652f\u6301\u8ba4\u77e5\u53c2\u4e0e\uff1a\u4e3b\u52a8\u6a21\u5f0f\u7684\u5f15\u5bfc\u4f8b\u9898\u548c\u5efa\u6784\u6a21\u5f0f\u7684\u9519\u8bef\u4f8b\u9898\u3002\u6bd4\u8f83\u4e86\u8d1d\u53f6\u65af\u77e5\u8bc6\u8ffd\u8e2a\uff08BKT\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4f5c\u4e3a\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u4e0e\u4e00\u4e2a\u975e\u81ea\u9002\u5e94\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5728\u903b\u8f91\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728113\u540d\u5b66\u751f\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4e24\u79cd\u81ea\u9002\u5e94\u7b56\u7565\u90fd\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u751f\u5728\u6d4b\u8bd5\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002BKT\u5bf9\u4f4e\u5148\u9a8c\u77e5\u8bc6\u5b66\u751f\u7684\u540e\u6d4b\u6210\u7ee9\u63d0\u5347\u6700\u5927\uff0c\u5e2e\u52a9\u4ed6\u4eec\u8d76\u4e0a\u9ad8\u5148\u9a8c\u77e5\u8bc6\u540c\u5b66\uff1b\u800cDRL\u5728\u9ad8\u5148\u9a8c\u77e5\u8bc6\u5b66\u751f\u4e2d\u4ea7\u751f\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u540e\u6d4b\u6210\u7ee9\u3002", "conclusion": "\u8bba\u6587\u5bf9\u8ba4\u77e5\u53c2\u4e0e\u548c\u81ea\u9002\u5e94\u6027\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u5bf9\u5b66\u4e60\u6210\u679c\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u7cfb\u7edf\u5982\u4f55\u6839\u636e\u5b66\u751f\u5148\u9a8c\u77e5\u8bc6\u6c34\u5e73\u4f18\u5316\u8ba4\u77e5\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2602.08349", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.08349", "abs": "https://arxiv.org/abs/2602.08349", "authors": ["Daniel Mwesigwa", "Cyan DeVeaux", "Palashi Vaghela"], "title": "To Tango or to Disentangle? Making Ethnography Public in the Digital Age", "comment": "Accepted to CSCW 2026 (PACM HCI)", "summary": "Ethnography attends to relations among people, practices, and the technologies that mediate them. Central to this method is the duality of roles ethnographers navigate as researchers and participants and as outsiders and insiders. However, the rise of digital platforms has introduced new opportunities as well as practical and ethical challenges that reshape these dualities across hybrid media environments spanning both online and offline contexts. Drawing on two case studies of VRChat and WhatsApp, we examine how ethnographers employ diverse tactics to study both enduring and emerging socio-cultural issues of race and caste, particularly those that form what are often called publics. We propose emergent relationality as a key analytic for understanding the mutual shaping of ethnographers, platforms, and publics. In this work, emergent relationality offers registers for analyzing how positionality and hybrid media environments constitute and condition what can be accessed, articulated, and made public.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u6570\u5b57\u5e73\u53f0\u65f6\u4ee3\u6c11\u65cf\u5fd7\u7814\u7a76\u9762\u4e34\u7684\u65b0\u673a\u9047\u4e0e\u6311\u6218\uff0c\u901a\u8fc7VRChat\u548cWhatsApp\u6848\u4f8b\u7814\u7a76\uff0c\u63d0\u51fa\"\u6d8c\u73b0\u5173\u7cfb\u6027\"\u4f5c\u4e3a\u5206\u6790\u6c11\u65cf\u5fd7\u7814\u7a76\u8005\u3001\u5e73\u53f0\u4e0e\u516c\u4f17\u76f8\u4e92\u5851\u9020\u7684\u5173\u952e\u6982\u5ff5\u3002", "motivation": "\u6570\u5b57\u5e73\u53f0\u7684\u5174\u8d77\u6539\u53d8\u4e86\u6c11\u65cf\u5fd7\u7814\u7a76\u7684\u73af\u5883\uff0c\u7814\u7a76\u8005\u65e2\u9762\u4e34\u5728\u7ebf\u4e0e\u79bb\u7ebf\u6df7\u5408\u5a92\u4f53\u73af\u5883\u7684\u65b0\u673a\u9047\uff0c\u4e5f\u9762\u4e34\u5b9e\u8df5\u548c\u4f26\u7406\u6311\u6218\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u7814\u7a76\u8005\u4f5c\u4e3a\u5c40\u5916\u4eba/\u5c40\u5185\u4eba\u7684\u53cc\u91cd\u89d2\u8272\u3002", "method": "\u91c7\u7528\u6848\u4f8b\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790VRChat\u548cWhatsApp\u4e24\u4e2a\u6570\u5b57\u5e73\u53f0\u4e0a\u7684\u6c11\u65cf\u5fd7\u5b9e\u8df5\uff0c\u63a2\u8ba8\u7814\u7a76\u8005\u5982\u4f55\u8fd0\u7528\u591a\u6837\u7b56\u7565\u7814\u7a76\u79cd\u65cf\u548c\u79cd\u59d3\u7b49\u793e\u4f1a\u6587\u5316\u95ee\u9898\u3002", "result": "\u63d0\u51fa\"\u6d8c\u73b0\u5173\u7cfb\u6027\"\u4f5c\u4e3a\u5173\u952e\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u6c11\u65cf\u5fd7\u7814\u7a76\u8005\u3001\u5e73\u53f0\u548c\u516c\u4f17\u4e4b\u95f4\u7684\u76f8\u4e92\u5851\u9020\u5173\u7cfb\uff0c\u5206\u6790\u4f4d\u7f6e\u6027\u548c\u6df7\u5408\u5a92\u4f53\u73af\u5883\u5982\u4f55\u6784\u6210\u548c\u9650\u5236\u53ef\u8bbf\u95ee\u3001\u53ef\u8868\u8fbe\u548c\u53ef\u516c\u5f00\u7684\u5185\u5bb9\u3002", "conclusion": "\u6570\u5b57\u5e73\u53f0\u65f6\u4ee3\u6c11\u65cf\u5fd7\u7814\u7a76\u9700\u8981\u65b0\u7684\u5206\u6790\u5de5\u5177\uff0c\"\u6d8c\u73b0\u5173\u7cfb\u6027\"\u4e3a\u7406\u89e3\u7814\u7a76\u8005\u3001\u5e73\u53f0\u4e0e\u516c\u4f17\u7684\u590d\u6742\u4e92\u52a8\u63d0\u4f9b\u4e86\u91cd\u8981\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u5e94\u5bf9\u6df7\u5408\u5a92\u4f53\u73af\u5883\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2602.08411", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08411", "abs": "https://arxiv.org/abs/2602.08411", "authors": ["Jinyu Xu", "Yi Sun", "Jiangling Zhang", "Qing Xie", "Daomin Ji", "Zhifeng Bao", "Jiachen Li", "Yanchun Ma", "Yongjian Liu"], "title": "A Sketch+Text Composed Image Retrieval Dataset for Thangka", "comment": "9 pages", "summary": "Composed Image Retrieval (CIR) enables image retrieval by combining multiple query modalities, but existing benchmarks predominantly focus on general-domain imagery and rely on reference images with short textual modifications. As a result, they provide limited support for retrieval scenarios that require fine-grained semantic reasoning, structured visual understanding, and domain-specific knowledge. In this work, we introduce CIRThan, a sketch+text Composed Image Retrieval dataset for Thangka imagery, a culturally grounded and knowledge-specific visual domain characterized by complex structures, dense symbolic elements, and domain-dependent semantic conventions. CIRThan contains 2,287 high-quality Thangka images, each paired with a human-drawn sketch and hierarchical textual descriptions at three semantic levels, enabling composed queries that jointly express structural intent and multi-level semantic specification. We provide standardized data splits, comprehensive dataset analysis, and benchmark evaluations of representative supervised and zero-shot CIR methods. Experimental results reveal that existing CIR approaches, largely developed for general-domain imagery, struggle to effectively align sketch-based abstractions and hierarchical textual semantics with fine-grained Thangka images, particularly without in-domain supervision. We believe CIRThan offers a valuable benchmark for advancing sketch+text CIR, hierarchical semantic modeling, and multimodal retrieval in cultural heritage and other knowledge-specific visual domains. The dataset is publicly available at https://github.com/jinyuxu-whut/CIRThan.", "AI": {"tldr": "\u63d0\u51fa\u4e86CIRThan\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u5510\u5361\u56fe\u50cf\u7684\u8349\u56fe+\u6587\u672c\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u6570\u636e\u96c6\uff0c\u5305\u542b2,287\u5f20\u9ad8\u8d28\u91cf\u5510\u5361\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u50cf\u90fd\u914d\u6709\u624b\u7ed8\u8349\u56fe\u548c\u4e09\u5c42\u8bed\u4e49\u5c42\u6b21\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ec4\u5408\u67e5\u8be2\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22(CIR)\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u9886\u57df\u56fe\u50cf\uff0c\u4f9d\u8d56\u5e26\u6709\u7b80\u77ed\u6587\u672c\u4fee\u6539\u7684\u53c2\u8003\u56fe\u50cf\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6301\u9700\u8981\u7ec6\u7c92\u5ea6\u8bed\u4e49\u63a8\u7406\u3001\u7ed3\u6784\u5316\u89c6\u89c9\u7406\u89e3\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684\u68c0\u7d22\u573a\u666f\u3002\u5510\u5361\u56fe\u50cf\u5177\u6709\u590d\u6742\u7ed3\u6784\u3001\u5bc6\u96c6\u7b26\u53f7\u5143\u7d20\u548c\u9886\u57df\u4f9d\u8d56\u7684\u8bed\u4e49\u7ea6\u5b9a\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u521b\u5efa\u4e86CIRThan\u6570\u636e\u96c6\uff0c\u5305\u542b2,287\u5f20\u9ad8\u8d28\u91cf\u5510\u5361\u56fe\u50cf\uff0c\u6bcf\u5f20\u56fe\u50cf\u914d\u6709\uff1a1\uff09\u4eba\u7c7b\u7ed8\u5236\u7684\u624b\u7ed8\u8349\u56fe\uff1b2\uff09\u4e09\u5c42\u8bed\u4e49\u5c42\u6b21\u7684\u6587\u672c\u63cf\u8ff0\uff08\u4ece\u7c97\u5230\u7ec6\uff09\u3002\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u6570\u636e\u5212\u5206\u3001\u5168\u9762\u7684\u6570\u636e\u96c6\u5206\u6790\uff0c\u5e76\u5bf9\u4ee3\u8868\u6027\u7684\u76d1\u7763\u548c\u96f6\u6837\u672cCIR\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u7684CIR\u65b9\u6cd5\uff08\u4e3b\u8981\u4e3a\u901a\u7528\u9886\u57df\u56fe\u50cf\u5f00\u53d1\uff09\u96be\u4ee5\u6709\u6548\u5bf9\u9f50\u57fa\u4e8e\u8349\u56fe\u7684\u62bd\u8c61\u548c\u5206\u5c42\u6587\u672c\u8bed\u4e49\u4e0e\u7ec6\u7c92\u5ea6\u5510\u5361\u56fe\u50cf\uff0c\u7279\u522b\u662f\u5728\u6ca1\u6709\u9886\u57df\u5185\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u3002\u8fd9\u8868\u660e\u5f53\u524d\u65b9\u6cd5\u5728\u77e5\u8bc6\u7279\u5b9a\u89c6\u89c9\u9886\u57df\u7684\u5c40\u9650\u6027\u3002", "conclusion": "CIRThan\u4e3a\u63a8\u8fdb\u8349\u56fe+\u6587\u672c\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u3001\u5206\u5c42\u8bed\u4e49\u5efa\u6a21\u4ee5\u53ca\u6587\u5316\u9057\u4ea7\u548c\u5176\u4ed6\u77e5\u8bc6\u7279\u5b9a\u89c6\u89c9\u9886\u57df\u7684\u591a\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u7ed3\u6784\u5316\u3001\u7b26\u53f7\u5bc6\u96c6\u7684\u89c6\u89c9\u9886\u57df\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.07145", "categories": ["cs.LG", "cs.CL", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.07145", "abs": "https://arxiv.org/abs/2602.07145", "authors": ["Zhiqi Bu", "Shiyun Xu", "Jialin Mao"], "title": "Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate", "comment": "Part of a planned series to understand and leverage the convexity in deep learning. Accepted to ICLR 2026", "summary": "Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u51f8\u6027\u73b0\u8c61\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u540e\u5f88\u5feb\u53d8\u5f97\u5f31\u51f8\uff0c\u5e76\u57fa\u4e8e\u6b64\u5efa\u7acb\u4e86\u5b66\u4e60\u7387\u548c\u635f\u5931\u7684\u7f29\u653e\u89c4\u5f8b", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5177\u6709\u975e\u51f8\u635f\u5931\u666f\u89c2\uff0c\u4f18\u5316\u52a8\u6001\u96be\u4ee5\u5206\u6790\u6216\u63a7\u5236\uff0c\u4f46\u7ecf\u9a8c\u4e0a\u8868\u73b0\u51fa\u7c7b\u4f3c\u51f8\u6027\u7684\u884c\u4e3a\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u51f8\u6027\u548cLipschitz\u8fde\u7eed\u6027\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4ee5\u901a\u8fc7\u5b66\u4e60\u7387\u8c03\u5ea6\u7cbe\u786e\u63a7\u5236\u635f\u5931\u52a8\u6001", "method": "\u901a\u8fc7\u5206\u6790\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u51f8\u6027\u7279\u5f81\uff0c\u53d1\u73b0\u8bad\u7ec3\u540e\u5f88\u5feb\u53d8\u5f97\u5f31\u51f8\uff0c\u635f\u5931\u53ef\u901a\u8fc7\u6700\u540e\u8fed\u4ee3\u7684\u4e0a\u754c\u9884\u6d4b\uff0c\u4ece\u800c\u63a8\u5bfc\u6700\u4f18\u5b66\u4e60\u7387\u7684\u7f29\u653e\u89c4\u5f8b\u3002\u57fa\u4e8e\u51f8\u6027\u89c6\u89d2\u5efa\u7acb\u5b66\u4e60\u7387\u548c\u635f\u5931\u7684\u7f29\u653e\u5b9a\u5f8b", "result": "\u6df1\u5ea6\u5b66\u4e60\u5728\u77ed\u671f\u8bad\u7ec3\u540e\u53d8\u5f97\u5f31\u51f8\uff0c\u635f\u5931\u53ef\u901a\u8fc7\u6700\u540e\u8fed\u4ee3\u4e0a\u754c\u9884\u6d4b\uff0c\u8fd9\u8fdb\u4e00\u6b65\u6307\u5bfc\u4e86\u6700\u4f18\u5b66\u4e60\u7387\u7684\u7f29\u653e\u3002\u5efa\u7acb\u4e86\u5b66\u4e60\u7387\u548c\u635f\u5931\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u53ef\u5728\u8bad\u7ec3\u65f6\u957f\u4e0a\u5916\u63a880\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u4e0a\u5916\u63a870\u500d", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u52a8\u6001\u8868\u73b0\u51fa\u5f31\u51f8\u7279\u6027\uff0c\u57fa\u4e8e\u51f8\u6027\u89c6\u89d2\u5efa\u7acb\u7684\u7f29\u653e\u5b9a\u5f8b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u548c\u63a7\u5236\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177"}}
{"id": "2602.07038", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07038", "abs": "https://arxiv.org/abs/2602.07038", "authors": ["Yifan Ji", "Zhipeng Xu", "Zhenghao Liu", "Zulong Chen", "Qian Zhang", "Zhibo Yang", "Junyang Lin", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents", "comment": null, "summary": "Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.", "AI": {"tldr": "UNIKIE-BENCH\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u6587\u6863\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u5305\u542b\u7ea6\u675f\u7c7b\u522b\u548c\u5f00\u653e\u7c7b\u522b\u4e24\u4e2a\u8bc4\u6d4b\u8f68\u9053\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u6837\u5316\u6a21\u5f0f\u5b9a\u4e49\u3001\u957f\u5c3e\u5173\u952e\u5b57\u6bb5\u548c\u590d\u6742\u5e03\u5c40\u4e0b\u7684\u6027\u80fd\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6587\u6863\u7684\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u9762\u4e34\u5e03\u5c40\u7ed3\u6784\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\u9700\u6c42\u7684\u5de8\u5927\u5dee\u5f02\uff0c\u800c\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u7f3a\u4e4f\u5168\u9762\u7cfb\u7edf\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86UNIKIE-BENCH\u7edf\u4e00\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u8f68\u9053\uff1a1)\u7ea6\u675f\u7c7b\u522bKIE\u8f68\u9053\uff0c\u57fa\u4e8e\u573a\u666f\u9884\u5b9a\u4e49\u6a21\u5f0f\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u9700\u6c42\uff1b2)\u5f00\u653e\u7c7b\u522bKIE\u8f68\u9053\uff0c\u63d0\u53d6\u6587\u6863\u4e2d\u660e\u786e\u5b58\u5728\u7684\u4efb\u4f55\u5173\u952e\u4fe1\u606f\u3002", "result": "\u5bf915\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u591a\u6837\u5316\u6a21\u5f0f\u5b9a\u4e49\u3001\u957f\u5c3e\u5173\u952e\u5b57\u6bb5\u548c\u590d\u6742\u5e03\u5c40\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e0d\u540c\u6587\u6863\u7c7b\u578b\u548c\u573a\u666f\u95f4\u5b58\u5728\u660e\u663e\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u5728\u57fa\u7840\u51c6\u786e\u6027\u548c\u5e03\u5c40\u611f\u77e5\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6301\u7eed\u6311\u6218\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u51c6\u548c\u65b9\u5411\u3002"}}
{"id": "2602.07339", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07339", "abs": "https://arxiv.org/abs/2602.07339", "authors": ["Ruturaj Reddy", "Hrishav Bakul Barua", "Junn Yong Loo", "Thanh Thi Nguyen", "Ganesh Krishnasamy"], "title": "RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving", "comment": null, "summary": "Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.", "AI": {"tldr": "RAPiD\u662f\u4e00\u4e2a\u786e\u5b9a\u6027\u7b56\u7565\u63d0\u53d6\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6269\u6563\u5f0f\u8f68\u8ff9\u89c4\u5212\u5668\u84b8\u998f\u4e3a\u9ad8\u6548\u7b56\u7565\uff0c\u6d88\u9664\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\uff0c\u5b9e\u73b08\u500d\u52a0\u901f\u5e76\u4fdd\u6301\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u5f0f\u8f68\u8ff9\u89c4\u5212\u5668\u867d\u7136\u80fd\u5f88\u597d\u5efa\u6a21\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u4f46\u5176\u4f9d\u8d56\u8fed\u4ee3\u968f\u673a\u91c7\u6837\u7684\u7279\u6027\u5bf9\u5b9e\u65f6\u5b89\u5168\u5173\u952e\u90e8\u7f72\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u786e\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5206\u6570\u6b63\u5219\u5316\u7b56\u7565\u4f18\u5316\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u89c4\u5212\u5668\u7684\u8bc4\u5206\u51fd\u6570\u4f5c\u4e3a\u884c\u4e3a\u5148\u9a8c\u6765\u6b63\u5219\u5316\u7b56\u7565\u5b66\u4e60\uff1b\u901a\u8fc7\u6a21\u4eff\u9884\u6d4b\u6027\u9a7e\u9a76\u5458\u63a7\u5236\u5668\u7684\u8bc4\u8bba\u5bb6\u63d0\u4f9b\u5bc6\u96c6\u7684\u5b89\u5168\u76d1\u7763\uff0c\u8d85\u8d8a\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u3002", "result": "\u5728nuPlan\u573a\u666f\u4e2d\u5b9e\u73b0\u95ed\u73af\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u76f8\u6bd4\u6269\u6563\u57fa\u7ebf\u67098\u500d\u52a0\u901f\uff0c\u5728interPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u5b66\u4e60\u578b\u89c4\u5212\u5668\u7684\u6700\u5148\u8fdb\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RAPiD\u6210\u529f\u5c06\u6269\u6563\u5f0f\u89c4\u5212\u5668\u84b8\u998f\u4e3a\u9ad8\u6548\u786e\u5b9a\u6027\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u5b89\u5168\u5173\u952e\u9a7e\u9a76\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08457", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08457", "abs": "https://arxiv.org/abs/2602.08457", "authors": ["David Otero", "Javier Parapar"], "title": "Hybrid Pooling with LLMs via Relevance Context Learning", "comment": null, "summary": "High-quality relevance judgements over large query sets are essential for evaluating Information Retrieval (IR) systems, yet manual annotation remains costly and time-consuming. Large Language Models (LLMs) have recently shown promise as automatic relevance assessors, but their reliability is still limited. Most existing approaches rely on zero-shot prompting or In-Context Learning (ICL) with a small number of labeled examples. However, standard ICL treats examples as independent instances and fails to explicitly capture the underlying relevance criteria of a topic, restricting its ability to generalize to unseen query-document pairs. To address this limitation, we introduce Relevance Context Learning (RCL), a novel framework that leverages human relevance judgements to explicitly model topic-specific relevance criteria. Rather than directly using labeled examples for in-context prediction, RCL first prompts an LLM (Instructor LLM) to analyze sets of judged query-document pairs and generate explicit narratives that describe what constitutes relevance for a given topic. These relevance narratives are then used as structured prompts to guide a second LLM (Assessor LLM) in producing relevance judgements. To evaluate RCL in a realistic data collection setting, we propose a hybrid pooling strategy in which a shallow depth-\\textit{k} pool from participating systems is judged by human assessors, while the remaining documents are labeled by LLMs. Experimental results demonstrate that RCL substantially outperforms zero-shot prompting and consistently improves over standard ICL. Overall, our findings indicate that transforming relevance examples into explicit, context-aware relevance narratives is a more effective way of exploiting human judgements for LLM-based IR dataset construction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Relevance Context Learning (RCL)\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9LLM\u5206\u6790\u5df2\u6807\u6ce8\u7684\u67e5\u8be2-\u6587\u6863\u5bf9\u751f\u6210\u660e\u786e\u7684\"\u76f8\u5173\u6027\u53d9\u4e8b\"\u6765\u63cf\u8ff0\u4e3b\u9898\u76f8\u5173\u6027\u6807\u51c6\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347LLM\u4f5c\u4e3a\u81ea\u52a8\u76f8\u5173\u6027\u8bc4\u4f30\u5668\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u76f8\u5173\u6027\u6807\u6ce8\u5bf9\u4e8e\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u3002\u73b0\u6709LLM\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5668\u7684\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u96f6\u6837\u672c\u63d0\u793a\u6216\u5c11\u91cf\u6807\u6ce8\u793a\u4f8b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u793a\u4f8b\u89c6\u4e3a\u72ec\u7acb\u5b9e\u4f8b\uff0c\u672a\u80fd\u660e\u786e\u6355\u6349\u4e3b\u9898\u7684\u76f8\u5173\u6027\u6807\u51c6\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faRelevance Context Learning (RCL)\u6846\u67b6\uff1a1) \u4f7f\u7528Instructor LLM\u5206\u6790\u5df2\u6807\u6ce8\u7684\u67e5\u8be2-\u6587\u6863\u5bf9\uff0c\u751f\u6210\u660e\u786e\u63cf\u8ff0\u4e3b\u9898\u76f8\u5173\u6027\u6807\u51c6\u7684\"\u76f8\u5173\u6027\u53d9\u4e8b\"\uff1b2) \u5c06\u8fd9\u4e9b\u53d9\u4e8b\u4f5c\u4e3a\u7ed3\u6784\u5316\u63d0\u793a\u6307\u5bfcAssessor LLM\u8fdb\u884c\u76f8\u5173\u6027\u5224\u65ad\uff1b3) \u63d0\u51fa\u6df7\u5408\u6c60\u5316\u7b56\u7565\uff0c\u6d45\u5c42k\u4e2a\u6587\u6863\u7531\u4eba\u5de5\u6807\u6ce8\uff0c\u5176\u4f59\u6587\u6863\u7531LLM\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRCL\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u5e76\u6301\u7eed\u6539\u8fdb\u6807\u51c6\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06\u76f8\u5173\u6027\u793a\u4f8b\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u76f8\u5173\u6027\u53d9\u4e8b\uff0c\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u57fa\u4e8eLLM\u7684IR\u6570\u636e\u96c6\u6784\u5efa\u3002", "conclusion": "\u5c06\u76f8\u5173\u6027\u793a\u4f8b\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u76f8\u5173\u6027\u53d9\u4e8b\u662f\u66f4\u6709\u6548\u5229\u7528\u4eba\u5de5\u6807\u6ce8\u8fdb\u884cLLM-based IR\u6570\u636e\u96c6\u6784\u5efa\u7684\u65b9\u6cd5\u3002RCL\u6846\u67b6\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u4e3b\u9898\u7279\u5b9a\u7684\u76f8\u5173\u6027\u6807\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4f5c\u4e3a\u81ea\u52a8\u76f8\u5173\u6027\u8bc4\u4f30\u5668\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.07150", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.07150", "abs": "https://arxiv.org/abs/2602.07150", "authors": ["Bjarni Haukur Bjarnason", "Andr\u00e9 Silva", "Martin Monperrus"], "title": "On Randomness in Agentic Evals", "comment": null, "summary": "Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u4e2d\u5355\u6b21\u8fd0\u884cpass@1\u5206\u6570\u5b58\u5728\u663e\u8457\u65b9\u5dee\uff0c2-3\u4e2a\u767e\u5206\u70b9\u7684\u6539\u8fdb\u53ef\u80fd\u53ea\u662f\u7edf\u8ba1\u566a\u58f0\u800c\u975e\u771f\u5b9e\u7b97\u6cd5\u8fdb\u6b65\uff0c\u5efa\u8bae\u91c7\u7528\u591a\u6b21\u8fd0\u884c\u3001\u7edf\u8ba1\u529f\u6548\u5206\u6790\u548cpass@k\u7b49\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u901a\u5e38\u57fa\u4e8e\u6bcf\u4e2a\u4efb\u52a1\u5355\u6b21\u8fd0\u884c\u7684pass@1\u5206\u6570\uff0c\u5047\u8bbe\u8fd9\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u6027\u80fd\u4f30\u8ba1\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\uff0c\u63a2\u7a76\u5355\u6b21\u8fd0\u884c\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u5728SWE-Bench-Verified\u57fa\u51c6\u4e0a\u6536\u96c660,000\u4e2a\u667a\u80fd\u4f53\u8f68\u8ff9\uff0c\u6db5\u76d6\u4e09\u4e2a\u6a21\u578b\u548c\u4e24\u79cd\u6846\u67b6\u3002\u901a\u8fc7token\u7ea7\u5206\u6790\u8ffd\u8e2a\u8f68\u8ff9\u5206\u6b67\u70b9\uff0c\u5e76\u5206\u6790\u8bc4\u4f30\u65b9\u5dee\u5bf9\u6027\u80fd\u6bd4\u8f83\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u663e\u8457\u65b9\u5dee\uff1a\u5355\u6b21\u8fd0\u884cpass@1\u4f30\u8ba1\u6839\u636e\u6240\u9009\u8fd0\u884c\u4e0d\u540c\u53d8\u53162.2-6.0\u4e2a\u767e\u5206\u70b9\uff0c\u5373\u4f7f\u5728\u6e29\u5ea60\u65f6\u6807\u51c6\u5dee\u4e5f\u8d85\u8fc71.5\u4e2a\u767e\u5206\u70b9\u3002\u8f68\u8ff9\u5728\u65e9\u671f\uff08\u524d\u51e0\u4e2a\u767e\u5206\u70b9\u7684token\u5185\uff09\u5c31\u53d1\u751f\u5206\u6b67\uff0c\u5fae\u5c0f\u5dee\u5f02\u4f1a\u7ea7\u8054\u6210\u4e0d\u540c\u7684\u89e3\u51b3\u7b56\u7565\u3002", "conclusion": "\u5efa\u8bae\u4e09\u4e2a\u5177\u4f53\u5b9e\u8df5\uff1a(1) \u57fa\u4e8e\u6bcf\u4e2a\u4efb\u52a1\u591a\u6b21\u72ec\u7acb\u8fd0\u884c\u4f30\u8ba1pass@1\uff0c(2) \u4f7f\u7528\u7edf\u8ba1\u529f\u6548\u5206\u6790\u786e\u5b9a\u68c0\u6d4b\u9884\u671f\u6548\u5e94\u5927\u5c0f\u6240\u9700\u7684\u8fd0\u884c\u6b21\u6570\uff0c(3) \u8003\u8651k>1\u7684pass@k\uff08\u4e50\u89c2\u754c\u9650\uff09\u548cpass^k\uff08\u60b2\u89c2\u754c\u9650\uff09\u6307\u6807\u6765\u66f4\u597d\u8868\u5f81\u5b8c\u6574\u6027\u80fd\u8303\u56f4\u3002\u8fd9\u4e9b\u5b9e\u8df5\u5bf9\u4e8e\u533a\u5206\u771f\u5b9e\u79d1\u5b66\u8fdb\u6b65\u4e0e\u7edf\u8ba1\u566a\u58f0\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.07041", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07041", "abs": "https://arxiv.org/abs/2602.07041", "authors": ["Leeje Jang", "Yao-Yi Chiang", "Angela M. Hastings", "Patimaporn Pungchanchaikul", "Martha B. Lucas", "Emily C. Schultz", "Jeffrey P. Louie", "Mohamed Estai", "Wen-Chen Wang", "Ryan H. L. Ip", "Boyen Huang"], "title": "OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis", "comment": null, "summary": "Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.", "AI": {"tldr": "OMNI-Dent\u662f\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u7259\u79d1\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e34\u5e8a\u63a8\u7406\u539f\u5219\u878d\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u667a\u80fd\u624b\u673a\u591a\u89c6\u89d2\u7167\u7247\u8fdb\u884c\u7259\u9f7f\u7ea7\u8bc4\u4f30\uff0c\u65e0\u9700\u7259\u79d1\u7279\u5b9a\u5fae\u8c03\u3002", "motivation": "\u5f53\u524dAI\u7259\u79d1\u8bca\u65ad\u65b9\u6cd5\u4e3b\u8981\u5c06\u8bca\u65ad\u89c6\u4e3a\u89c6\u89c9\u6a21\u5f0f\u8bc6\u522b\u4efb\u52a1\uff0c\u672a\u80fd\u53cd\u6620\u7259\u79d1\u4e13\u4e1a\u4eba\u5458\u7684\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\uff0c\u4e14\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u6210\u50cf\u6761\u4ef6\u4e0b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faOMNI-Dent\u6846\u67b6\uff0c\u5c06\u4e34\u5e8a\u63a8\u7406\u542f\u53d1\u5f0f\u65b9\u6cd5\u5d4c\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6d41\u7a0b\uff0c\u5229\u7528\u591a\u89c6\u89d2\u667a\u80fd\u624b\u673a\u7167\u7247\uff0c\u5f15\u5bfc\u901a\u7528VLM\u8fdb\u884c\u7259\u9f7f\u7ea7\u8bc4\u4f30\uff0c\u65e0\u9700\u7259\u79d1\u7279\u5b9a\u5fae\u8c03\u3002", "result": "\u8be5\u6846\u67b6\u65e8\u5728\u652f\u6301\u5728\u7f3a\u4e4f\u4e34\u5e8a\u5f71\u50cf\u6570\u636e\u7684\u573a\u666f\u4e2d\u8fdb\u884c\u8bca\u65ad\u8bc4\u4f30\uff0c\u4f5c\u4e3a\u65e9\u671f\u8f85\u52a9\u5de5\u5177\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u6f5c\u5728\u5f02\u5e38\u5e76\u5224\u65ad\u662f\u5426\u9700\u8981\u4e13\u4e1a\u8bc4\u4f30\u3002", "conclusion": "OMNI-Dent\u4e3a\u7f3a\u4e4f\u9762\u5bf9\u9762\u62a4\u7406\u673a\u4f1a\u7684\u4e2a\u4eba\u63d0\u4f9b\u4e86\u5b9e\u7528\u9009\u62e9\uff0c\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u5c06\u4e34\u5e8a\u63a8\u7406\u878d\u5165AI\u8bca\u65ad\u6d41\u7a0b\u3002"}}
{"id": "2602.08530", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08530", "abs": "https://arxiv.org/abs/2602.08530", "authors": ["Huanjie Wang", "Xinchen Luo", "Honghui Bao", "Zhang Zixing", "Lejian Ren", "Yunfan Wu", "Hongwei Zhang", "Liwei Guan", "Guang Chen"], "title": "PIT: A Dynamic Personalized Item Tokenizer for End-to-End Generative Recommendation", "comment": null, "summary": "Generative Recommendation has revolutionized recommender systems by reformulating retrieval as a sequence generation task over discrete item identifiers. Despite the progress, existing approaches typically rely on static, decoupled tokenization that ignores collaborative signals. While recent methods attempt to integrate collaborative signals into item identifiers either during index construction or through end-to-end modeling, they encounter significant challenges in real-world production environments. Specifically, the volatility of collaborative signals leads to unstable tokenization, and current end-to-end strategies often devolve into suboptimal two-stage training rather than achieving true co-evolution. To bridge this gap, we propose PIT, a dynamic Personalized Item Tokenizer framework for end-to-end generative recommendation, which employs a co-generative architecture that harmonizes collaborative patterns through collaborative signal alignment and synchronizes item tokenizer with generative recommender via a co-evolution learning. This enables the dynamic, joint, end-to-end evolution of both index construction and recommendation. Furthermore, a one-to-many beam index ensures scalability and robustness, facilitating seamless integration into large-scale industrial deployments. Extensive experiments on real-world datasets demonstrate that PIT consistently outperforms competitive baselines. In a large-scale deployment at Kuaishou, an online A/B test yielded a substantial 0.402% uplift in App Stay Time, validating the framework's effectiveness in dynamic industrial environments.", "AI": {"tldr": "PIT\u662f\u4e00\u4e2a\u52a8\u6001\u4e2a\u6027\u5316\u9879\u76ee\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u751f\u6210\u67b6\u6784\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u751f\u6210\u5f0f\u63a8\u8350\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u534f\u540c\u4fe1\u53f7\u4e0d\u7a33\u5b9a\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u5728\u5feb\u624b\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5e94\u7528\u505c\u7559\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u63a8\u8350\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9759\u6001\u89e3\u8026\u7684\u6807\u8bb0\u5316\uff0c\u5ffd\u7565\u4e86\u534f\u540c\u4fe1\u53f7\u3002\u867d\u7136\u8fd1\u671f\u65b9\u6cd5\u5c1d\u8bd5\u5728\u7d22\u5f15\u6784\u5efa\u6216\u7aef\u5230\u7aef\u5efa\u6a21\u4e2d\u6574\u5408\u534f\u540c\u4fe1\u53f7\uff0c\u4f46\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u534f\u540c\u4fe1\u53f7\u7684\u6ce2\u52a8\u6027\u5bfc\u81f4\u6807\u8bb0\u5316\u4e0d\u7a33\u5b9a\uff0c\u5f53\u524d\u7aef\u5230\u7aef\u7b56\u7565\u5f80\u5f80\u9000\u5316\u4e3a\u6b21\u4f18\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u800c\u975e\u771f\u6b63\u7684\u534f\u540c\u8fdb\u5316\u3002", "method": "\u63d0\u51faPIT\u52a8\u6001\u4e2a\u6027\u5316\u9879\u76ee\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u91c7\u7528\u534f\u540c\u751f\u6210\u67b6\u6784\uff1a\u901a\u8fc7\u534f\u540c\u4fe1\u53f7\u5bf9\u9f50\u534f\u8c03\u534f\u540c\u6a21\u5f0f\uff0c\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u5b66\u4e60\u540c\u6b65\u9879\u76ee\u6807\u8bb0\u5668\u548c\u751f\u6210\u5f0f\u63a8\u8350\u5668\uff0c\u5b9e\u73b0\u7d22\u5f15\u6784\u5efa\u548c\u63a8\u8350\u7684\u52a8\u6001\u8054\u5408\u7aef\u5230\u7aef\u8fdb\u5316\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e00\u5bf9\u591a\u6ce2\u675f\u7d22\u5f15\u786e\u4fdd\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPIT\u59cb\u7ec8\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u3002\u5728\u5feb\u624b\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u5e26\u6765\u4e860.402%\u7684\u5e94\u7528\u505c\u7559\u65f6\u95f4\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u52a8\u6001\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "PIT\u6846\u67b6\u901a\u8fc7\u534f\u540c\u751f\u6210\u67b6\u6784\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u534f\u540c\u4fe1\u53f7\u6574\u5408\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u3001\u8054\u5408\u3001\u7aef\u5230\u7aef\u7684\u8fdb\u5316\uff0c\u5728\u5de5\u4e1a\u89c4\u6a21\u90e8\u7f72\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07154", "abs": "https://arxiv.org/abs/2602.07154", "authors": ["Ayush Roy", "Rudrasis Chakraborty", "Lav Varshney", "Vishnu Suresh Lokhande"], "title": "Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity", "comment": "AISTATS 2026", "summary": "Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.", "AI": {"tldr": "\u63d0\u51fa\u5339\u914d\u6846\u67b6\u5904\u7406\u5f02\u6784\u6570\u636e\u96c6\u6c60\u5316\u4e2d\u7684\u5206\u5e03\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8d28\u5fc3\u9009\u62e9\u548c\u8fed\u4ee3\u4f18\u5316\u63d0\u9ad8\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "motivation": "\u5f02\u6784\u6570\u636e\u96c6\u6c60\u5316\u4f1a\u653e\u5927\u5206\u5e03\u4e0d\u5bf9\u79f0\u6027\u5e76\u4ea7\u751f\u6709\u504f\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u96f6\u6837\u672c\u6cdb\u5316\u7684\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u6734\u7d20\u6c60\u5316\u548c\u5747\u5300\u5b50\u91c7\u6837\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u5f02\u8d28\u6027", "method": "\u63d0\u51fa\u5339\u914d\u6846\u67b6\uff0c\u57fa\u4e8e\u81ea\u9002\u5e94\u8d28\u5fc3\u9009\u62e9\u6837\u672c\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8868\u793a\u5206\u5e03\uff0c\u7ed3\u5408\u53cc\u91cd\u9c81\u68d2\u6027\u548c\u503e\u5411\u5f97\u5206\u5339\u914d\u6765\u8fc7\u6ee4\u6df7\u6dc6\u57df\uff08\u5f02\u8d28\u6027\u7684\u4e3b\u8981\u6765\u6e90\uff09", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u5339\u914d\u65b9\u6cd5\u5728\u4e0d\u5bf9\u79f0\u5143\u5206\u5e03\u4e0b\u4f18\u4e8e\u6734\u7d20\u6c60\u5316\u548c\u5747\u5300\u5b50\u91c7\u6837\uff0c\u8fd9\u4e9b\u6539\u8fdb\u6269\u5c55\u5230\u975e\u9ad8\u65af\u548c\u591a\u6a21\u6001\u73b0\u5b9e\u573a\u666f\uff0c\u5e76\u5728\u96f6\u6837\u672c\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c", "conclusion": "\u5339\u914d\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u5f02\u6784\u6570\u636e\u96c6\u6c60\u5316\u4e2d\u7684\u5206\u5e03\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u63d0\u9ad8\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u7b49\u6781\u7aef\u5f02\u8d28\u6027\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.07042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07042", "abs": "https://arxiv.org/abs/2602.07042", "authors": ["Magesh Rajasekaran", "Md Saiful Islam Sajol", "Frej Berglind", "Supratik Mukhopadhyay", "Kamalika Das"], "title": "COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification", "comment": "Copyright by SIAM. Unauthorized reproduction of this article is prohibited First Published in Proceedings of the 2024 SIAM International Conference on Data Mining (SDM24), published by the Society for Industrial and Applied Mathematics (SIAM)", "summary": "Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.", "AI": {"tldr": "COMBOOD\u662f\u4e00\u4e2a\u7528\u4e8e\u56fe\u50cf\u8bc6\u522b\u4e2dOOD\u68c0\u6d4b\u7684\u65e0\u76d1\u7763\u534a\u53c2\u6570\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6700\u8fd1\u90bb\u548c\u9a6c\u6c0f\u8ddd\u79bb\u4e24\u79cd\u4fe1\u53f7\uff0c\u5728\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u4e0b\u90fd\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u63a8\u7406\u65f6\u8bc6\u522b\u5206\u5e03\u5916\u6570\u636e\u5bf9\u81ea\u52a8\u5316\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8fd1OOD\u573a\u666f\uff08\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5904\u7406\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u7684\u9c81\u68d2\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCOMBOOD\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u79cd\u8ddd\u79bb\u5ea6\u91cf\u4fe1\u53f7\uff1a1\uff09\u6700\u8fd1\u90bb\u8ddd\u79bb\uff08\u975e\u53c2\u6570\u65b9\u6cd5\uff09\u63d0\u4f9bOOD\u68c0\u6d4b\uff1b2\uff09\u9a6c\u6c0f\u8ddd\u79bb\uff08\u53c2\u6570\u65b9\u6cd5\uff09\u5728\u8fdcOOD\u573a\u666f\u4e2d\u7279\u522b\u6709\u6548\u3002\u5728\u534a\u53c2\u6570\u8bbe\u7f6e\u4e0b\u878d\u5408\u8fd9\u4e24\u79cd\u4fe1\u53f7\uff0c\u4e3a\u63a8\u7406\u70b9\u751f\u6210\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u3002", "result": "\u5728OpenOOD\uff08v1\u548cv1.5\uff09\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6587\u6863\u6570\u636e\u96c6\u4e0a\uff0cCOMBOOD\u5728\u51c6\u786e\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5bf9\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u90fd\u6709\u6548\u3002\u5728\u5927\u591a\u6570\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u5347\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u5d4c\u5165\u7a7a\u95f4\u5927\u5c0f\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "COMBOOD\u662f\u4e00\u4e2a\u6709\u6548\u7684\u534a\u53c2\u6570OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6700\u8fd1\u90bb\u548c\u9a6c\u6c0f\u8ddd\u79bb\u7684\u4f18\u52bf\uff0c\u5728\u8fd1OOD\u548c\u8fdcOOD\u573a\u666f\u4e0b\u90fd\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u68c0\u6d4b\u7ed3\u679c\uff0c\u5177\u6709\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.07359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07359", "abs": "https://arxiv.org/abs/2602.07359", "authors": ["Xiaoqiang Lin", "Jun Hao Liew", "Silvio Savarese", "Junnan Li"], "title": "W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents", "comment": null, "summary": "Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\"\u5bbd\u6df1\u7814\u7a76\u667a\u80fd\u4f53\"\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u5de5\u5177\u8c03\u7528\u5b9e\u73b0\u5bbd\u5ea6\u6269\u5c55\uff0c\u5728\u4fdd\u6301\u5355\u6b65\u63a8\u7406\u7684\u540c\u65f6\u63d0\u5347\u7814\u7a76\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u4e3b\u8981\u901a\u8fc7\u589e\u52a0\u987a\u5e8f\u601d\u8003\u548c\u5de5\u5177\u8c03\u7528\u6b21\u6570\u6765\u6269\u5c55\u6df1\u5ea6\uff0c\u4f46\u5e76\u884c\u5de5\u5177\u8c03\u7528\uff08\u5bbd\u5ea6\u6269\u5c55\uff09\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u540c\u65f6\u6269\u5c55\u5bbd\u5ea6\u548c\u6df1\u5ea6\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u5bbd\u6df1\u7814\u7a76\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u5185\u5728\u5e76\u884c\u5de5\u5177\u8c03\u7528\u5728\u5355\u4e2a\u63a8\u7406\u6b65\u9aa4\u5185\u5b9e\u73b0\u6709\u6548\u534f\u8c03\uff0c\u907f\u514d\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u7f16\u6392\u3002\u7814\u7a76\u8fd8\u63a2\u7d22\u4e86\u5404\u79cd\u5de5\u5177\u8c03\u7528\u8c03\u5ea6\u5668\u6765\u4f18\u5316\u5e76\u884c\u7b56\u7565\u3002", "result": "\u5bbd\u5ea6\u6269\u5c55\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u83b7\u5f97\u6b63\u786e\u7b54\u6848\u6240\u9700\u7684\u8f6e\u6b21\u3002\u4f7f\u7528GPT-5-Medium\u5728BrowseComp\u4e0a\u8fbe\u523062.2%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86GPT-5-High\u62a5\u544a\u768454.9%\u539f\u59cb\u7ed3\u679c\u3002", "conclusion": "\u4f18\u5316\u5bbd\u5ea6\u4e0e\u6df1\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u662f\u5b9e\u73b0\u9ad8\u6548\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u7684\u5173\u952e\u9014\u5f84\u3002\u5e76\u884c\u5de5\u5177\u8c03\u7528\u662f\u63d0\u5347\u7814\u7a76\u667a\u80fd\u4f53\u6027\u80fd\u7684\u6709\u6548\u7b56\u7565\uff0c\u65e0\u9700\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6216\u5176\u4ed6\u6280\u5de7\u3002"}}
{"id": "2602.08545", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08545", "abs": "https://arxiv.org/abs/2602.08545", "authors": ["Xingyuan Zeng", "Zuohan Wu", "Yue Wang", "Chen Zhang", "Quanming Yao", "Libin Zheng", "Jian Yin"], "title": "DA-RAG: Dynamic Attributed Community Search for Retrieval-Augmented Generation", "comment": null, "summary": "Owing to their unprecedented comprehension capabilities, large language models (LLMs) have become indispensable components of modern web search engines. From a technical perspective, this integration represents retrieval-augmented generation (RAG), which enhances LLMs by grounding them in external knowledge bases. A prevalent technical approach in this context is graph-based RAG (G-RAG). However, current G-RAG methodologies frequently underutilize graph topology, predominantly focusing on low-order structures or pre-computed static communities. This limitation affects their effectiveness in addressing dynamic and complex queries. Thus, we propose DA-RAG, which leverages attributed community search (ACS) to extract relevant subgraphs based on the queried question dynamically. DA-RAG captures high-order graph structures, allowing for the retrieval of self-complementary knowledge. Furthermore, DA-RAG is equipped with a chunk-layer oriented graph index, which facilitates efficient multi-granularity retrieval while significantly reducing both computational and economic costs. We evaluate DA-RAG on multiple datasets, demonstrating that it outperforms existing RAG methods by up to 40% in head-to-head comparisons across four metrics while reducing index construction time and token overhead by up to 37% and 41%, respectively.", "AI": {"tldr": "DA-RAG\u662f\u4e00\u79cd\u57fa\u4e8e\u5c5e\u6027\u793e\u533a\u641c\u7d22\u7684\u52a8\u6001\u56fe\u589e\u5f3a\u68c0\u7d22\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u63d0\u53d6\u76f8\u5173\u5b50\u56fe\u6355\u83b7\u9ad8\u9636\u56fe\u7ed3\u6784\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709RAG\u65b9\u6cd540%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u56fe\u7684RAG\u65b9\u6cd5\u901a\u5e38\u672a\u80fd\u5145\u5206\u5229\u7528\u56fe\u62d3\u6251\u7ed3\u6784\uff0c\u4e3b\u8981\u5173\u6ce8\u4f4e\u9636\u7ed3\u6784\u6216\u9884\u8ba1\u7b97\u7684\u9759\u6001\u793e\u533a\uff0c\u8fd9\u9650\u5236\u4e86\u5904\u7406\u52a8\u6001\u590d\u6742\u67e5\u8be2\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faDA-RAG\u65b9\u6cd5\uff0c\u5229\u7528\u5c5e\u6027\u793e\u533a\u641c\u7d22\u6839\u636e\u67e5\u8be2\u95ee\u9898\u52a8\u6001\u63d0\u53d6\u76f8\u5173\u5b50\u56fe\uff0c\u6355\u83b7\u9ad8\u9636\u56fe\u7ed3\u6784\uff0c\u5e76\u914d\u5907\u5206\u5757\u5c42\u5bfc\u5411\u7684\u56fe\u7d22\u5f15\u4ee5\u5b9e\u73b0\u9ad8\u6548\u591a\u7c92\u5ea6\u68c0\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cDA-RAG\u5728\u56db\u9879\u6307\u6807\u4e0a\u6bd4\u73b0\u6709RAG\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe40%\uff0c\u540c\u65f6\u5c06\u7d22\u5f15\u6784\u5efa\u65f6\u95f4\u548ctoken\u5f00\u9500\u5206\u522b\u964d\u4f4e37%\u548c41%\u3002", "conclusion": "DA-RAG\u901a\u8fc7\u52a8\u6001\u793e\u533a\u641c\u7d22\u6709\u6548\u5229\u7528\u56fe\u62d3\u6251\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u4e3a\u590d\u6742\u67e5\u8be2\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07044", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07044", "abs": "https://arxiv.org/abs/2602.07044", "authors": ["Tianyi Qu", "Songxiao Yang", "Haolin Wang", "Huadong Song", "Xiaoting Guo", "Wenguang Hu", "Guanlin Liu", "Honghe Chen", "Yafei Ou"], "title": "PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging", "comment": "A dataset contains 240,320 pipeline MFL pseudo-color images and 191,530 bounding-box annotations, collected from 11 pipelines spanning approximately 1,480 km", "summary": "Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \\textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \\textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \\textbf{240,320} images and \\textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \\textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.", "AI": {"tldr": "PipeMFL-240K\uff1a\u9996\u4e2a\u5927\u89c4\u6a21\u516c\u5f00\u7ba1\u9053\u6f0f\u78c1\u68c0\u6d4b\u6570\u636e\u96c6\u4e0e\u57fa\u51c6\uff0c\u5305\u542b24\u4e07\u5f20\u56fe\u50cf\u548c19\u4e07\u6807\u6ce8\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u516c\u5e73\u6bd4\u8f83\u548c\u53ef\u91cd\u590d\u8bc4\u4f30\u7684\u95ee\u9898\u3002", "motivation": "\u7ba1\u9053\u5b8c\u6574\u6027\u5bf9\u5de5\u4e1a\u5b89\u5168\u548c\u73af\u5883\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u6f0f\u78c1\u68c0\u6d4b\u662f\u4e3b\u8981\u65e0\u635f\u68c0\u6d4b\u6280\u672f\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u81ea\u52a8\u5316\u6f0f\u78c1\u89e3\u91ca\u65b9\u9762\u6709\u524d\u666f\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u53ef\u9760\u6a21\u578b\u7684\u8fdb\u5c55\u53d7\u5230\u9650\u5236\uff0c\u96be\u4ee5\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u548c\u53ef\u91cd\u590d\u8bc4\u4f30\u3002", "method": "\u6784\u5efaPipeMFL-240K\u6570\u636e\u96c6\uff0c\u5305\u542b240,320\u5f20\u56fe\u50cf\u548c191,530\u4e2a\u9ad8\u8d28\u91cf\u8fb9\u754c\u6846\u6807\u6ce8\uff0c\u6536\u96c6\u81ea11\u6761\u603b\u957f\u7ea61,480\u516c\u91cc\u7684\u7ba1\u9053\u3002\u6570\u636e\u96c6\u53cd\u6620\u4e86\u771f\u5b9e\u68c0\u6d4b\u590d\u6742\u6027\uff0c\u5177\u6709\u4e09\u4e2a\u72ec\u7279\u6311\u6218\uff1a1\uff0912\u4e2a\u7c7b\u522b\u7684\u6781\u7aef\u957f\u5c3e\u5206\u5e03\uff1b2\uff09\u5927\u91cf\u4ec5\u5305\u542b\u5c11\u91cf\u50cf\u7d20\u7684\u5fae\u5c0f\u7269\u4f53\uff1b3\uff09\u663e\u8457\u7684\u7c7b\u5185\u53d8\u5f02\u6027\u3002", "result": "\u901a\u8fc7\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u5efa\u7acb\u57fa\u51c6\u3002\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u4ee3\u68c0\u6d4b\u5668\u4ecd\u7136\u96be\u4ee5\u5e94\u5bf9\u6f0f\u78c1\u6570\u636e\u7684\u5185\u5728\u7279\u6027\uff0c\u8868\u660e\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002PipeMFL-240K\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "\u4f5c\u4e3a\u9996\u4e2a\u516c\u5f00\u7684\u7ba1\u9053\u6f0f\u78c1\u68c0\u6d4b\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0cPipeMFL-240K\u4e3a\u9ad8\u6548\u7ba1\u9053\u8bca\u65ad\u548c\u7ef4\u62a4\u89c4\u5212\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\uff0c\u6709\u671b\u52a0\u901f\u57fa\u4e8e\u6f0f\u78c1\u7684\u7ba1\u9053\u5b8c\u6574\u6027\u8bc4\u4f30\u7684\u7b97\u6cd5\u521b\u65b0\u548c\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2602.08786", "categories": ["cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08786", "abs": "https://arxiv.org/abs/2602.08786", "authors": ["Unai Fischer-Abaigar", "Emily Aiken", "Christoph Kern", "Juan Carlos Perdomo"], "title": "Empirically Understanding the Value of Prediction in Allocation", "comment": null, "summary": "Institutions increasingly use prediction to allocate scarce resources. From a design perspective, better predictions compete with other investments, such as expanding capacity or improving treatment quality. Here, the big question is not how to solve a specific allocation problem, but rather which problem to solve. In this work, we develop an empirical toolkit to help planners form principled answers to this question and quantify the bottom-line welfare impact of investments in prediction versus other policy levers such as expanding capacity and improving treatment quality. Applying our framework in two real-world case studies on German employment services and poverty targeting in Ethiopia, we illustrate how decision-makers can reliably derive context-specific conclusions about the relative value of prediction in their allocation problem. We make our software toolkit, rvp, and parts of our data available in order to enable future empirical work in this area.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u8bc1\u5de5\u5177\u5305\uff0c\u5e2e\u52a9\u51b3\u7b56\u8005\u91cf\u5316\u9884\u6d4b\u6295\u8d44\u76f8\u5bf9\u4e8e\u6269\u5927\u5bb9\u91cf\u548c\u63d0\u9ad8\u6cbb\u7597\u8d28\u91cf\u7b49\u5176\u4ed6\u653f\u7b56\u6760\u6746\u7684\u798f\u5229\u5f71\u54cd\uff0c\u5e76\u5728\u5fb7\u56fd\u5c31\u4e1a\u670d\u52a1\u548c\u57c3\u585e\u4fc4\u6bd4\u4e9a\u8d2b\u56f0\u76ee\u6807\u5b9a\u4f4d\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u4e2d\u5e94\u7528\u3002", "motivation": "\u673a\u6784\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u9884\u6d4b\u6765\u5206\u914d\u7a00\u7f3a\u8d44\u6e90\uff0c\u4f46\u4ece\u8bbe\u8ba1\u89d2\u5ea6\u770b\uff0c\u66f4\u597d\u7684\u9884\u6d4b\u9700\u8981\u4e0e\u5176\u4ed6\u6295\u8d44\uff08\u5982\u6269\u5927\u5bb9\u91cf\u6216\u63d0\u9ad8\u6cbb\u7597\u8d28\u91cf\uff09\u7ade\u4e89\u3002\u6838\u5fc3\u95ee\u9898\u4e0d\u662f\u5982\u4f55\u89e3\u51b3\u5177\u4f53\u7684\u5206\u914d\u95ee\u9898\uff0c\u800c\u662f\u5e94\u8be5\u89e3\u51b3\u54ea\u4e2a\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u8bc1\u5de5\u5177\u5305\uff08rvp\u8f6f\u4ef6\u5de5\u5177\u5305\uff09\uff0c\u5e2e\u52a9\u89c4\u5212\u8005\u5f62\u6210\u539f\u5219\u6027\u7684\u7b54\u6848\uff0c\u91cf\u5316\u9884\u6d4b\u6295\u8d44\u76f8\u5bf9\u4e8e\u5176\u4ed6\u653f\u7b56\u6760\u6746\uff08\u5982\u6269\u5927\u5bb9\u91cf\u548c\u6539\u5584\u6cbb\u7597\u8d28\u91cf\uff09\u7684\u798f\u5229\u5f71\u54cd\u3002\u5728\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u4e2d\u5e94\u7528\u8be5\u6846\u67b6\uff1a\u5fb7\u56fd\u5c31\u4e1a\u670d\u52a1\u548c\u57c3\u585e\u4fc4\u6bd4\u4e9a\u8d2b\u56f0\u76ee\u6807\u5b9a\u4f4d\u3002", "result": "\u51b3\u7b56\u8005\u80fd\u591f\u53ef\u9760\u5730\u5f97\u51fa\u5173\u4e8e\u9884\u6d4b\u5728\u5176\u7279\u5b9a\u5206\u914d\u95ee\u9898\u4e2d\u76f8\u5bf9\u4ef7\u503c\u7684\u60c5\u5883\u7279\u5b9a\u7ed3\u8bba\u3002\u8be5\u5de5\u5177\u5305\u4f7f\u51b3\u7b56\u8005\u80fd\u591f\u5728\u4e0d\u540c\u653f\u7b56\u6760\u6746\u4e4b\u95f4\u505a\u51fa\u660e\u667a\u7684\u6295\u8d44\u51b3\u7b56\u3002", "conclusion": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5b9e\u8bc1\u6846\u67b6\u548c\u8f6f\u4ef6\u5de5\u5177\u5305\uff0c\u5e2e\u52a9\u51b3\u7b56\u8005\u91cf\u5316\u9884\u6d4b\u6295\u8d44\u76f8\u5bf9\u4e8e\u5176\u4ed6\u5e72\u9884\u63aa\u65bd\u7684\u798f\u5229\u5f71\u54cd\uff0c\u4f7f\u673a\u6784\u80fd\u591f\u66f4\u660e\u667a\u5730\u51b3\u5b9a\u5728\u9884\u6d4b\u3001\u6269\u5927\u5bb9\u91cf\u6216\u63d0\u9ad8\u6cbb\u7597\u8d28\u91cf\u7b49\u65b9\u9762\u7684\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2602.08559", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08559", "abs": "https://arxiv.org/abs/2602.08559", "authors": ["Tian Xia", "Jiaqi Zhang", "Yueyang Liu", "Hongjian Dou", "Tingya Yin", "Jiangxia Cao", "Xulei Liang", "Tianlu Xie", "Lihao Liu", "Xiang Chen", "Shen Wang", "Changxin Lao", "Haixiang Gan", "Jinkai Yu", "Keting Cen", "Lu Hao", "Xu Zhang", "Qiqiang Zhong", "Zhongbo Sun", "Yiyu Wang", "Shuang Yang", "Mingxin Wen", "Xiangyu Wu", "Shaoguo Liu", "Tingting Gao", "Zhaojie Liu", "Han Li", "Kun Gai"], "title": "QARM V2: Quantitative Alignment Multi-Modal Recommendation for Reasoning User Sequence Modeling", "comment": "Work in progress", "summary": "With the evolution of large language models (LLMs), there is growing interest in leveraging their rich semantic understanding to enhance industrial recommendation systems (RecSys). Traditional RecSys relies on ID-based embeddings for user sequence modeling in the General Search Unit (GSU) and Exact Search Unit (ESU) paradigm, which suffers from low information density, knowledge isolation, and weak generalization ability. While LLMs offer complementary strengths with dense semantic representations and strong generalization, directly applying LLM embeddings to RecSys faces critical challenges: representation unmatch with business objectives and representation unlearning end-to-end with downstream tasks. In this paper, we present QARM V2, a unified framework that bridges LLM semantic understanding with RecSys business requirements for user sequence modeling.", "AI": {"tldr": "QARM V2\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6865\u63a5LLM\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u8350\u7cfb\u7edf\u4e1a\u52a1\u9700\u6c42\u6765\u89e3\u51b3\u7528\u6237\u5e8f\u5217\u5efa\u6a21\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56ID\u5d4c\u5165\u5b58\u5728\u4fe1\u606f\u5bc6\u5ea6\u4f4e\u3001\u77e5\u8bc6\u9694\u79bb\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\uff0c\u800cLLM\u867d\u7136\u5177\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u8868\u793a\u4e0e\u4e1a\u52a1\u76ee\u6807\u4e0d\u5339\u914d\u3001\u65e0\u6cd5\u7aef\u5230\u7aef\u5b66\u4e60\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86QARM V2\u7edf\u4e00\u6846\u67b6\uff0c\u6865\u63a5LLM\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u8350\u7cfb\u7edf\u4e1a\u52a1\u9700\u6c42\uff0c\u7528\u4e8e\u7528\u6237\u5e8f\u5217\u5efa\u6a21\u3002", "result": "\u8bba\u6587\u4ecb\u7ecd\u4e86QARM V2\u6846\u67b6\uff0c\u4f46\u6458\u8981\u4e2d\u6ca1\u6709\u63d0\u4f9b\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "QARM V2\u6846\u67b6\u65e8\u5728\u89e3\u51b3LLM\u5d4c\u5165\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e94\u7528\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u8bed\u4e49\u7406\u89e3\u4e0e\u4e1a\u52a1\u9700\u6c42\u7684\u7edf\u4e00\u3002"}}
{"id": "2602.07173", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.07173", "abs": "https://arxiv.org/abs/2602.07173", "authors": ["Tong Jian", "Tianyu Dai", "Tao Yu"], "title": "Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control", "comment": "Accepted to be presented in IEEE ICASSP 2026", "summary": "LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.", "AI": {"tldr": "\u9996\u6b21\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u5e94\u7528\u4e8e\u7535\u673a\u524d\u9988\u63a7\u5236\uff0c\u901a\u8fc7Transformer\u67b6\u6784\u5206\u79bb\u4fe1\u53f7\u8868\u793a\u4e0e\u7cfb\u7edf\u884c\u4e3a\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u5fae\u8c03\u548c\u5355\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5728\u591a\u79cd\u7535\u673a\u8d1f\u8f7d\u914d\u7f6e\u4e0a\u8d85\u8d8a\u4f20\u7edfPI\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u7269\u7406\u7684\u524d\u9988\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u5c1a\u672a\u6269\u5c55\u5230\u4fe1\u53f7\u5904\u7406\u7cfb\u7edf\u3002\u4f20\u7edfPI\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u7269\u7406\u7684\u65b9\u6cd5\u5728\u5904\u7406\u975e\u7ebf\u6027\u548c\u590d\u6742\u8d1f\u8f7d\u6761\u4ef6\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u672a\u89c1\u7cfb\u7edf\u52a8\u6001\u7684\u6570\u636e\u9ad8\u6548\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5206\u79bb\u4fe1\u53f7\u8868\u793a\u4e0e\u7cfb\u7edf\u884c\u4e3a\uff0c\u652f\u6301\u5c11\u6837\u672c\u5fae\u8c03\u548c\u5355\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u5728\u5927\u89c4\u6a21\u5408\u6210\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u7cfb\u7edf\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ec5\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u7535\u673a\u7684\u672a\u89c1\u7cfb\u7edf\u52a8\u6001\u3002", "result": "\u6a21\u578b\u5728\u591a\u79cd\u7535\u673a\u8d1f\u8f7d\u914d\u7f6e\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5c06\u672a\u8c03\u4f18\u7684\u793a\u4f8b\u8f6c\u5316\u4e3a\u51c6\u786e\u7684\u524d\u9988\u9884\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8ePI\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u7269\u7406\u7684\u524d\u9988\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u591f\u6865\u63a5\u5408\u6210\u9884\u8bad\u7ec3\u548c\u771f\u5b9e\u4e16\u754c\u9002\u5e94\u6027\uff0c\u4e3a\u7269\u7406\u7cfb\u7edf\u7684\u6570\u636e\u9ad8\u6548\u63a7\u5236\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5c55\u793a\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u6269\u5c55\u5230\u4fe1\u53f7\u5904\u7406\u548c\u63a7\u5236\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.07045", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07045", "abs": "https://arxiv.org/abs/2602.07045", "authors": ["Zhiming Luo", "Di Wang", "Haonan Guo", "Jing Zhang", "Bo Du"], "title": "VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.", "AI": {"tldr": "VLRS-Bench\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u9065\u611f\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u8ba4\u77e5\u3001\u51b3\u7b56\u548c\u9884\u6d4b\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u65e8\u5728\u63a8\u52a8\u9065\u611f\u9886\u57df\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u504f\u5411\u611f\u77e5\u4efb\u52a1\uff08\u5982\u76ee\u6807\u8bc6\u522b\u548c\u573a\u666f\u5206\u7c7b\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u8981\u6c42\u9ad8\u7684\u9065\u611f\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4e13\u95e8\u7684\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u4e13\u95e8\u6784\u5efa\u7684\u6d41\u7a0b\u521b\u5efaVLRS-Bench\uff0c\u8be5\u6d41\u7a0b\u6574\u5408\u4e86\u9065\u611f\u7279\u5b9a\u7684\u5148\u9a8c\u77e5\u8bc6\u548c\u4e13\u5bb6\u77e5\u8bc6\uff0c\u786e\u4fdd\u5730\u7406\u7a7a\u95f4\u771f\u5b9e\u6027\u548c\u63a8\u7406\u590d\u6742\u6027\u3002\u57fa\u51c6\u5305\u542b2000\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5e73\u5747\u957f\u5ea671\u4e2a\u8bcd\uff0c\u6db5\u76d614\u4e2a\u4efb\u52a1\u548c\u6700\u591a\u516b\u4e2a\u65f6\u95f4\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728VLRS-Bench\u4e0a\u5b58\u5728\u663e\u8457\u74f6\u9888\uff0c\u4e3a\u9065\u611f\u793e\u533a\u63a8\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "VLRS-Bench\u4f5c\u4e3a\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u9065\u611f\u590d\u6742\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9065\u611f\u9886\u57df\u591a\u6a21\u6001\u63a8\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2602.07399", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07399", "abs": "https://arxiv.org/abs/2602.07399", "authors": ["Changhua Xu", "Jie Lu", "Junyu Xuan", "En Yu"], "title": "VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation", "comment": "Preprint", "summary": "Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \\emph{generation--selection} perspective and propose a novel framework \\textbf{VGAS} (\\textbf{V}alue-\\textbf{G}uided \\textbf{A}ction-chunk \\textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \\textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \\textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \\textit{Explicit Geometric Regularization} (\\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \\textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.", "AI": {"tldr": "VGAS\u6846\u67b6\u901a\u8fc7\u751f\u6210-\u9009\u62e9\u8303\u5f0f\u89e3\u51b3VLA\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u4f7f\u7528\u4ef7\u503c\u5f15\u5bfc\u7684\u52a8\u4f5c\u5757\u9009\u62e9\u6765\u63d0\u5347\u8f68\u8ff9\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u5b58\u5728\u4e0d\u53ef\u9760\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u51e0\u4f55\u6a21\u7cca\u6027\u2014\u2014\u8bed\u4e49\u5408\u7406\u7684\u8f68\u8ff9\u53ef\u80fd\u56e0\u51e0\u4f55\u7ec6\u8282\u504f\u5dee\u5bfc\u81f4\u6267\u884c\u5931\u8d25\uff0c\u800c\u6709\u9650\u76d1\u7763\u96be\u4ee5\u89e3\u51b3\u8fd9\u79cd\u8fd1\u5931\u5019\u9009\u52a8\u4f5c\u4e4b\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\u3002", "method": "\u63d0\u51faVGAS\u6846\u67b6\uff1a1) \u4f7f\u7528\u5fae\u8c03VLA\u4f5c\u4e3a\u9ad8\u53ec\u56de\u7387\u63d0\u8bae\u751f\u6210\u5668\uff1b2) \u5f15\u5165Q-Chunk-Former\u4f5c\u4e3a\u51e0\u4f55\u57fa\u7840Transformer\u8bc4\u8bba\u5bb6\uff0c\u89e3\u51b3\u7ec6\u7c92\u5ea6\u51e0\u4f55\u6a21\u7cca\uff1b3) \u63d0\u51fa\u663e\u5f0f\u51e0\u4f55\u6b63\u5219\u5316(EGR)\uff0c\u901a\u8fc7\u663e\u5f0f\u5851\u9020\u5224\u522b\u6027\u4ef7\u503c\u666f\u89c2\u6765\u4fdd\u6301\u52a8\u4f5c\u6392\u5e8f\u5206\u8fa8\u7387\uff0c\u540c\u65f6\u7f13\u89e3\u5c11\u76d1\u7763\u4e0b\u7684\u4ef7\u503c\u4e0d\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cVGAS\u5728\u6709\u9650\u6f14\u793a\u548c\u5206\u5e03\u504f\u79fb\u4e0b\u80fd\u6301\u7eed\u63d0\u9ad8\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u7684\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\u3002", "conclusion": "VGAS\u901a\u8fc7\u751f\u6210-\u9009\u62e9\u8303\u5f0f\u548c\u51e0\u4f55\u611f\u77e5\u7684\u4ef7\u503c\u5f15\u5bfc\uff0c\u4e3aVLA\u6a21\u578b\u7684\u5c11\u6837\u672c\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u51e0\u4f55\u654f\u611f\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.08997", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08997", "abs": "https://arxiv.org/abs/2602.08997", "authors": ["Lavender Y. Jiang", "Xujin Chris Liu", "Kyunghyun Cho", "Eric K. Oermann"], "title": "Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs", "comment": null, "summary": "Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while ignoring the latent information found in correlations between identity and quasi-identifiers, which can be captured by modern LLMs. We first formalize these correlations using a causal graph, then validate it empirically through individual re-identification of patients from scrubbed notes. The paradox of de-identification is further shown through a diagnosis ablation: even when all other information is removed, the model can predict the patient's neighborhood based on diagnosis alone. This position paper raises the question of how we can act as a community to uphold patient-provider trust when de-identification is inherently imperfect. We aim to raise awareness and discuss actionable recommendations.", "AI": {"tldr": "\u672c\u6587\u6307\u51faHIPAA Safe Harbor\u53bb\u6807\u8bc6\u5316\u65b9\u6cd5\u5728\u73b0\u4ee3LLM\u65f6\u4ee3\u5df2\u5931\u6548\uff0c\u56e0\u4e3aLLM\u80fd\u4ece\u4e34\u5e8a\u7b14\u8bb0\u7684\u6f5c\u5728\u5173\u8054\u4e2d\u91cd\u65b0\u8bc6\u522b\u60a3\u8005\u8eab\u4efd\uff0c\u5373\u4f7f\u79fb\u9664\u6240\u6709\u663e\u5f0f\u6807\u8bc6\u7b26\u3002", "motivation": "\u9690\u79c1\u662f\u57fa\u672c\u4eba\u6743\uff0c\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u60a3\u8005\u654f\u611f\u4fe1\u606f\uff0c\u4f20\u7edf\u53bb\u6807\u8bc6\u5316\u65b9\u6cd5\u57fa\u4e8e\u5206\u7c7b\u8868\u683c\u6570\u636e\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u4ee3LLM\u4ece\u76f8\u5173\u6027\u548c\u51c6\u6807\u8bc6\u7b26\u4e2d\u63d0\u53d6\u8eab\u4efd\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u9996\u5148\u4f7f\u7528\u56e0\u679c\u56fe\u5f62\u5f0f\u5316\u8eab\u4efd\u4e0e\u51c6\u6807\u8bc6\u7b26\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u7136\u540e\u901a\u8fc7\u4ece\u53bb\u6807\u8bc6\u5316\u7b14\u8bb0\u4e2d\u91cd\u65b0\u8bc6\u522b\u60a3\u8005\u7684\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u8bca\u65ad\u6d88\u878d\u5b9e\u9a8c\u5c55\u793a\u5373\u4f7f\u53ea\u4fdd\u7559\u8bca\u65ad\u4fe1\u606f\u4e5f\u80fd\u9884\u6d4b\u60a3\u8005\u5c45\u4f4f\u5730\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u663e\u793aLLM\u80fd\u4ece\u53bb\u6807\u8bc6\u5316\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u91cd\u65b0\u8bc6\u522b\u60a3\u8005\uff0c\u8bca\u65ad\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u4ec5\u51ed\u8bca\u65ad\u4fe1\u606f\u5c31\u80fd\u9884\u6d4b\u60a3\u8005\u5c45\u4f4f\u5730\uff0c\u63ed\u793a\u53bb\u6807\u8bc6\u5316\u672c\u8d28\u4e0a\u7684\u4e0d\u5b8c\u5584\u6027\u3002", "conclusion": "\u4f20\u7edf\u53bb\u6807\u8bc6\u5316\u65b9\u6cd5\u5728\u73b0\u4ee3LLM\u65f6\u4ee3\u5df2\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u60a3\u8005\u9690\u79c1\uff0c\u9700\u8981\u793e\u533a\u5171\u540c\u884c\u52a8\u6765\u7ef4\u62a4\u533b\u60a3\u4fe1\u4efb\uff0c\u672c\u6587\u65e8\u5728\u63d0\u9ad8\u610f\u8bc6\u5e76\u8ba8\u8bba\u53ef\u884c\u7684\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2602.07047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07047", "abs": "https://arxiv.org/abs/2602.07047", "authors": ["Muhammad Rashid", "Elvio G. Amparore", "Enrico Ferrari", "Damiano Verda"], "title": "ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees", "comment": "AAAI-2026", "summary": "Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.", "AI": {"tldr": "ShapBPT\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42Shapley\u503c\u7684\u6570\u636e\u611f\u77e5\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06Shapley\u7cfb\u6570\u5206\u914d\u7ed9\u4e3a\u56fe\u50cf\u5b9a\u5236\u7684\u591a\u5c3a\u5ea6\u5c42\u6b21\u7ed3\u6784\uff08\u4e8c\u8fdb\u5236\u5206\u533a\u6811\uff09\uff0c\u4f7f\u7279\u5f81\u5f52\u56e0\u4e0e\u56fe\u50cf\u5f62\u6001\u5bf9\u9f50\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u8bed\u4e49\u610f\u4e49\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5c42Shapley\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u56fe\u50cf\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u7ed3\u6784\uff0c\u5bfc\u81f4\u6536\u655b\u7f13\u6162\u4e14\u4e0e\u771f\u5b9e\u5f62\u6001\u7279\u5f81\u5bf9\u9f50\u4e0d\u4f73\u3002\u540c\u65f6\uff0c\u6ca1\u6709\u5148\u524d\u7684Shapley\u65b9\u6cd5\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5229\u7528\u6570\u636e\u611f\u77e5\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5bfc\u81f4\u7ed3\u6784\u5316\u89c6\u89c9\u6570\u636e\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u63d0\u51faShapBPT\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5206\u5c42Shapley\u516c\u5f0f\uff0c\u5c06Shapley\u7cfb\u6570\u5206\u914d\u7ed9\u4e13\u95e8\u4e3a\u56fe\u50cf\u8bbe\u8ba1\u7684\u4e8c\u8fdb\u5236\u5206\u533a\u6811\uff08BPT\uff09\u591a\u5c3a\u5ea6\u5c42\u6b21\u7ed3\u6784\u3002\u901a\u8fc7\u8fd9\u79cd\u6570\u636e\u611f\u77e5\u7684\u5c42\u6b21\u5206\u533a\uff0c\u786e\u4fdd\u7279\u5f81\u5f52\u56e0\u4e0e\u5185\u5728\u56fe\u50cf\u5f62\u6001\u5bf9\u9f50\uff0c\u6709\u6548\u4f18\u5148\u8003\u8651\u76f8\u5173\u533a\u57df\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eShapBPT\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u4e0e\u56fe\u50cf\u7ed3\u6784\u7684\u4f18\u8d8a\u5bf9\u9f50\u6027\u548c\u76f8\u6bd4\u73b0\u6709XCV\u65b9\u6cd5\u7684\u6539\u8fdb\u6548\u7387\u300220\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u4eba\u7c7b\u66f4\u504f\u597dShapBPT\u7684\u89e3\u91ca\u3002", "conclusion": "ShapBPT\u5c06\u5206\u5c42Shapley\u65b9\u6cd5\u4e0e\u56fe\u50cf\u6570\u636e\u8fde\u63a5\u8d77\u6765\uff0c\u4e3a\u89c6\u89c9\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u8bed\u4e49\u66f4\u6709\u610f\u4e49\u7684\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u7ed3\u6784\u5316\u89c6\u89c9\u6570\u636e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.07408", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.07408", "abs": "https://arxiv.org/abs/2602.07408", "authors": ["Hyomin Kim", "Sang-Yeon Hwang", "Jaechang Lim", "Yinhua Piao", "Yunhak Oh", "Woo Youn Kim", "Chanyoung Park", "Sungsoo Ahn", "Junhyeok Jeon"], "title": "Progressive Multi-Agent Reasoning for Biological Perturbation Prediction", "comment": "17 pages, 4 figures, 9 tables", "summary": "Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.", "AI": {"tldr": "PBio-Agent\uff1a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u751f\u7269\u6270\u52a8\u54cd\u5e94\u9884\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u4efb\u52a1\u6392\u5e8f\u548c\u8fed\u4ee3\u77e5\u8bc6\u7cbe\u70bc\uff0c\u5728LINCSQA\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9ad8\u7ef4\u6270\u52a8\u7ed3\u679c\u65f6\u5bb9\u6613\u6df7\u6dc6\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u5355\u7ec6\u80de\u9057\u4f20\u6270\u52a8\uff0c\u800c\u836f\u7269\u53d1\u73b0\u6838\u5fc3\u7684\u6279\u91cf\u7ec6\u80de\u5316\u5b66\u6270\u52a8\u7814\u7a76\u4e0d\u8db3", "method": "\u63d0\u51faPBio-Agent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u96be\u5ea6\u611f\u77e5\u4efb\u52a1\u6392\u5e8f\u548c\u8fed\u4ee3\u77e5\u8bc6\u7cbe\u70bc\u673a\u5236\uff0c\u5229\u7528\u76f8\u540c\u6270\u52a8\u5f71\u54cd\u7684\u57fa\u56e0\u5171\u4eab\u56e0\u679c\u7ed3\u6784\u7684\u6d1e\u5bdf\uff0c\u901a\u8fc7\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u4e13\u95e8\u667a\u80fd\u4f53\u8fdb\u884c\u9884\u6d4b", "result": "PBio-Agent\u5728LINCSQA\u548cPerturbQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5373\u4f7f\u8f83\u5c0f\u6a21\u578b\u4e5f\u80fd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u9884\u6d4b\u548c\u89e3\u91ca\u590d\u6742\u751f\u7269\u8fc7\u7a0b", "conclusion": "PBio-Agent\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u56e0\u679c\u7ed3\u6784\u5171\u4eab\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5316\u5b66\u6270\u52a8\u4e0b\u57fa\u56e0\u8c03\u63a7\u9884\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177"}}
{"id": "2602.08612", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08612", "abs": "https://arxiv.org/abs/2602.08612", "authors": ["Shen Wang", "Yusheng Huang", "Ruochen Yang", "Shuang Wen", "Pengbo Xu", "Jiangxia Cao", "Yueyang Liu", "Kuo Cai", "Chengcheng Guo", "Shiyao Wang", "Xinchen Luo", "Qiang Luo", "Ruiming Tang", "Shuang Yang", "Zhaojie Liu", "Guorui Zhou", "Han Li", "Kun Gai"], "title": "OneLive: Dynamically Unified Generative Framework for Live-Streaming Recommendation", "comment": "Work in progress", "summary": "Live-streaming recommender system serves as critical infrastructure that bridges the patterns of real-time interactions between users and authors. Similar to traditional industrial recommender systems, live-streaming recommendation also relies on cascade architectures to support large-scale concurrency. Recent advances in generative recommendation unify the multi-stage recommendation process with Transformer-based architectures, offering improved scalability and higher computational efficiency. However, the inherent complexity of live-streaming prevents the direct transfer of these methods to live-streaming scenario, where continuously evolving content, limited lifecycles, strict real-time constraints, and heterogeneous multi-objectives introduce unique challenges that invalidate static tokenization and conventional model framework. To address these issues, we propose OneLive, a dynamically unified generative recommendation framework tailored for live-streaming scenario. OneLive integrates four key components: (i) A Dynamic Tokenizer that continuously encodes evolving real-time live content fused with behavior signal through residual quantization; (ii) A Time-Aware Gated Attention mechanism that explicitly models temporal dynamics for timely decision making; (iii) An efficient decoder-only generative architecture enhanced with Sequential MTP and QK Norm for stable training and accelerated inference; (iv) A Unified Multi-Objective Alignment Framework reinforces policy optimization for personalized preferences.", "AI": {"tldr": "OneLive\u662f\u4e00\u4e2a\u4e3a\u76f4\u64ad\u573a\u666f\u5b9a\u5236\u7684\u52a8\u6001\u7edf\u4e00\u751f\u6210\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u8bcd\u5668\u3001\u65f6\u95f4\u611f\u77e5\u95e8\u63a7\u6ce8\u610f\u529b\u3001\u9ad8\u6548\u89e3\u7801\u5668\u67b6\u6784\u548c\u591a\u76ee\u6807\u5bf9\u9f50\u6846\u67b6\u89e3\u51b3\u76f4\u64ad\u63a8\u8350\u4e2d\u7684\u72ec\u7279\u6311\u6218\u3002", "motivation": "\u76f4\u64ad\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u4f20\u7edf\u751f\u6210\u63a8\u8350\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u7684\u6311\u6218\uff1a\u5185\u5bb9\u6301\u7eed\u6f14\u53d8\u3001\u751f\u547d\u5468\u671f\u6709\u9650\u3001\u4e25\u683c\u5b9e\u65f6\u7ea6\u675f\u548c\u5f02\u6784\u591a\u76ee\u6807\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f7f\u5f97\u9759\u6001\u5206\u8bcd\u548c\u4f20\u7edf\u6a21\u578b\u6846\u67b6\u5931\u6548\u3002", "method": "OneLive\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u52a8\u6001\u5206\u8bcd\u5668\u901a\u8fc7\u6b8b\u5dee\u91cf\u5316\u878d\u5408\u5b9e\u65f6\u76f4\u64ad\u5185\u5bb9\u548c\u884c\u4e3a\u4fe1\u53f7\uff1b2) \u65f6\u95f4\u611f\u77e5\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff1b3) \u9ad8\u6548\u89e3\u7801\u5668\u67b6\u6784\u91c7\u7528Sequential MTP\u548cQK Norm\uff1b4) \u7edf\u4e00\u591a\u76ee\u6807\u5bf9\u9f50\u6846\u67b6\u5f3a\u5316\u4e2a\u6027\u5316\u504f\u597d\u7684\u7b56\u7565\u4f18\u5316\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u76f4\u64ad\u573a\u666f\u7684\u751f\u6210\u63a8\u8350\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u5b9e\u65f6\u6f14\u53d8\u7684\u5185\u5bb9\u3001\u6709\u9650\u7684\u751f\u547d\u5468\u671f\u3001\u4e25\u683c\u7684\u5b9e\u65f6\u7ea6\u675f\u548c\u5f02\u6784\u591a\u76ee\u6807\uff0c\u4e3a\u76f4\u64ad\u63a8\u8350\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "OneLive\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u76f4\u64ad\u63a8\u8350\u4e2d\u7684\u72ec\u7279\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u7edf\u4e00\u67b6\u6784\u5b9e\u73b0\u4e86\u5bf9\u5b9e\u65f6\u6f14\u53d8\u5185\u5bb9\u7684\u9ad8\u6548\u5904\u7406\uff0c\u4e3a\u76f4\u64ad\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07049", "abs": "https://arxiv.org/abs/2602.07049", "authors": ["Jindong Li", "Dario Zanca", "Vincent Christlein", "Tim Hamann", "Jens Barth", "Peter K\u00e4mpf", "Bj\u00f6rn Eskofier"], "title": "Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead", "comment": null, "summary": "Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.", "AI": {"tldr": "\u63d0\u51faECHWR\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e34\u65f6\u8f85\u52a9\u5206\u652f\u548c\u53cc\u91cd\u5bf9\u6bd4\u76ee\u6807\u63d0\u5347\u57fa\u4e8eIMU\u7684\u5728\u7ebf\u624b\u5199\u8bc6\u522b\u6027\u80fd\uff0c\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u5728OnHW-Words500\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u5b57\u7b26\u9519\u8bef\u7387\u3002", "motivation": "\u57fa\u4e8eIMU\u7684\u5728\u7ebf\u624b\u5199\u8bc6\u522b\u53ef\u5b9e\u73b0\u7eb8\u4e0a\u624b\u5199\u5230\u6570\u5b57\u8bbe\u5907\u7684\u8f93\u5165\uff0c\u5728\u8fb9\u7f18\u786c\u4ef6\u4e0a\u8fd0\u884c\u53ef\u63d0\u5347\u9690\u79c1\u548c\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4f46\u9762\u4e34\u5185\u5b58\u9650\u5236\u3002\u9700\u8981\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faECHWR\u8bad\u7ec3\u6846\u67b6\uff1a\u4f7f\u7528\u4e34\u65f6\u8f85\u52a9\u5206\u652f\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9\u9f50\u4f20\u611f\u5668\u4fe1\u53f7\u4e0e\u8bed\u4e49\u6587\u672c\u5d4c\u5165\uff1b\u91c7\u7528\u53cc\u91cd\u5bf9\u6bd4\u76ee\u6807\uff1a\u6279\u91cf\u5185\u5bf9\u6bd4\u635f\u5931\u7528\u4e8e\u6a21\u6001\u5bf9\u9f50\uff0c\u65b0\u9896\u7684\u57fa\u4e8e\u9519\u8bef\u7684\u5bf9\u6bd4\u635f\u5931\u533a\u5206\u6b63\u786e\u4fe1\u53f7\u4e0e\u5408\u6210\u786c\u8d1f\u6837\u672c\uff1b\u8bad\u7ec3\u540e\u4e22\u5f03\u8f85\u52a9\u5206\u652f\uff0c\u4fdd\u6301\u539f\u59cb\u9ad8\u6548\u67b6\u6784\u3002", "result": "\u5728OnHW-Words500\u6570\u636e\u96c6\u4e0a\uff0cECHWR\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff1a\u5728\u72ec\u7acb\u4e8e\u4e66\u5199\u8005\u5206\u5272\u4e0a\u964d\u4f4e\u5b57\u7b26\u9519\u8bef\u73877.4%\uff0c\u5728\u4f9d\u8d56\u4e8e\u4e66\u5199\u8005\u5206\u5272\u4e0a\u964d\u4f4e10.4%\u3002\u57fa\u4e8e\u9519\u8bef\u7684\u5bf9\u6bd4\u635f\u5931\u5728\u5904\u7406\u672a\u89c1\u4e66\u5199\u98ce\u683c\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "ECHWR\u6846\u67b6\u901a\u8fc7\u4e34\u65f6\u8f85\u52a9\u5206\u652f\u548c\u53cc\u91cd\u5bf9\u6bd4\u76ee\u6807\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8eIMU\u7684\u624b\u5199\u8bc6\u522b\u6027\u80fd\uff0c\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002\u57fa\u4e8e\u9519\u8bef\u7684\u5bf9\u6bd4\u635f\u5931\u5728\u5904\u7406\u672a\u89c1\u4e66\u5199\u98ce\u683c\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2602.08707", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.08707", "abs": "https://arxiv.org/abs/2602.08707", "authors": ["Aditya Gulati", "Nuria Oliver"], "title": "Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers", "comment": null, "summary": "As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of \"trust\" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u804a\u5929\u673a\u5668\u4eba\u4fe1\u4efb\u95ee\u9898\uff0c\u6307\u51fa\u7528\u6237\u4fe1\u4efb\u5e38\u6e90\u4e8e\u884c\u4e3a\u673a\u5236\u800c\u975e\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u5efa\u8bae\u5c06\u804a\u5929\u673a\u5668\u4eba\u89c6\u4e3a\u9500\u552e\u5458\u800c\u975e\u52a9\u624b\uff0c\u5f3a\u8c03\u9700\u8981\u533a\u5206\u5fc3\u7406\u4fe1\u4efb\u5f62\u6210\u4e0e\u89c4\u8303\u53ef\u4fe1\u5ea6", "motivation": "\u968f\u7740\u804a\u5929\u673a\u5668\u4eba\u6a21\u7cca\u81ea\u52a8\u5316\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u5bf9\u8bdd\u7684\u754c\u9650\uff0c\u9700\u8981\u66f4\u4ed4\u7ec6\u5730\u5ba1\u89c6\u8fd9\u4e9b\u7cfb\u7edf\u7684\u4fe1\u4efb\u57fa\u7840\u3002\u76d1\u7ba1\u548c\u653f\u7b56\u6846\u67b6\u503e\u5411\u4e8e\u4ece\u89c4\u8303\u89d2\u5ea6\u5b9a\u4e49\u4fe1\u4efb\uff0c\u4f46\u7528\u6237\u5bf9\u804a\u5929\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u5f80\u5f80\u6e90\u4e8e\u884c\u4e3a\u673a\u5236\uff0c\u8fd9\u79cd\u4fe1\u4efb\u901a\u5e38\u4e0d\u662f\u901a\u8fc7\u8bc1\u660e\u53ef\u4fe1\u5ea6\u83b7\u5f97\u7684\uff0c\u800c\u662f\u901a\u8fc7\u5229\u7528\u8ba4\u77e5\u504f\u89c1\u5f71\u54cd\u7528\u6237\u884c\u4e3a\u7684\u8bbe\u8ba1\u9009\u62e9\u5851\u9020\u7684\u3002", "method": "\u57fa\u4e8e\u89c2\u5bdf\u63d0\u51fa\u91cd\u65b0\u5b9a\u4e49\u804a\u5929\u673a\u5668\u4eba\u7684\u6846\u67b6\uff1a\u4e0d\u662f\u4f5c\u4e3a\u4f34\u4fa3\u6216\u52a9\u624b\uff0c\u800c\u662f\u4f5c\u4e3a\u90e8\u7f72\u7ec4\u7ec7\u786e\u5b9a\u76ee\u6807\u7684\u9ad8\u5ea6\u719f\u7ec3\u7684\u9500\u552e\u4eba\u5458\u3002\u5206\u6790\u7ade\u4e89\u6027\"\u4fe1\u4efb\"\u6982\u5ff5\u5171\u5b58\u4e8e\u540c\u4e00\u672f\u8bed\u4e0b\u7684\u95ee\u9898\uff0c\u533a\u5206\u5fc3\u7406\u4fe1\u4efb\u5f62\u6210\u4e0e\u89c4\u8303\u53ef\u4fe1\u5ea6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7528\u6237\u5bf9\u804a\u5929\u673a\u5668\u4eba\u7684\u4fe1\u4efb\u5f80\u5f80\u4e0d\u662f\u901a\u8fc7\u8bc1\u660e\u53ef\u4fe1\u5ea6\u83b7\u5f97\u7684\uff0c\u800c\u662f\u901a\u8fc7\u5229\u7528\u8ba4\u77e5\u504f\u89c1\u5f71\u54cd\u7528\u6237\u884c\u4e3a\u7684\u8bbe\u8ba1\u9009\u62e9\u5851\u9020\u7684\u3002\u8fd9\u79cd\u4fe1\u4efb\u5f62\u6210\u673a\u5236\u63a9\u76d6\u4e86\u5fc3\u7406\u4fe1\u4efb\u5f62\u6210\u4e0e\u89c4\u8303\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u91cd\u8981\u533a\u522b\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u66f4\u5f3a\u6709\u529b\u7684\u652f\u6301\u673a\u5236\uff0c\u5e2e\u52a9\u7528\u6237\u9002\u5f53\u6821\u51c6\u5bf9\u5bf9\u8bddAI\u7cfb\u7edf\u7684\u4fe1\u4efb\u3002\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u9700\u8981\u533a\u5206\u5fc3\u7406\u4fe1\u4efb\u5f62\u6210\u4e0e\u89c4\u8303\u53ef\u4fe1\u5ea6\uff0c\u5e76\u5efa\u7acb\u66f4\u6e05\u6670\u7684\u4fe1\u4efb\u6846\u67b6\u3002"}}
{"id": "2602.08667", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08667", "abs": "https://arxiv.org/abs/2602.08667", "authors": ["Yicheng Di", "Yuan Liu", "Zhi Chen", "Jingcai Guo"], "title": "SRSUPM: Sequential Recommender System Based on User Psychological Motivation", "comment": "9 pages, 8 pages", "summary": "Sequential recommender infers users' evolving psychological motivations from historical interactions to recommend the next preferred items. Most existing methods compress recent behaviors into a single vector and optimize it toward a single observed target item, but lack explicit modeling of psychological motivation shift. As a result, they struggle to uncover the distributional patterns across different shift degrees and to capture collaborative knowledge that is sensitive to psychological motivation shift. We propose a general framework, the Sequential Recommender System Based on User Psychological Motivation, to enhance sequential recommenders with psychological motivation shift-aware user modeling. Specifically, the Psychological Motivation Shift Assessment quantitatively measures psychological motivation shift; guided by PMSA, the Shift Information Construction models dynamically evolving multi-level shift states, and the Psychological Motivation Shift-driven Information Decomposition decomposes and regularizes representations across shift levels. Moreover, the Psychological Motivation Shift Information Matching strengthens collaborative patterns related to psychological motivation shift to learn more discriminative user representations. Extensive experiments on three public benchmarks show that SRSUPM consistently outperforms representative baselines on diverse sequential recommender tasks.", "AI": {"tldr": "\u63d0\u51faSRSUPM\u6846\u67b6\uff0c\u901a\u8fc7\u5fc3\u7406\u52a8\u673a\u8f6c\u79fb\u5efa\u6a21\u589e\u5f3a\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u5fc3\u7406\u52a8\u673a\u8f6c\u79fb\u5efa\u6a21\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u5e8f\u5217\u63a8\u8350\u65b9\u6cd5\u5c06\u8fd1\u671f\u884c\u4e3a\u538b\u7f29\u4e3a\u5355\u4e00\u5411\u91cf\uff0c\u7f3a\u4e4f\u5bf9\u7528\u6237\u5fc3\u7406\u52a8\u673a\u8f6c\u79fb\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u96be\u4ee5\u6355\u6349\u4e0d\u540c\u8f6c\u79fb\u7a0b\u5ea6\u7684\u5206\u5e03\u6a21\u5f0f\u548c\u534f\u4f5c\u77e5\u8bc6", "method": "\u63d0\u51faSRSUPM\u6846\u67b6\uff1a1) \u5fc3\u7406\u52a8\u673a\u8f6c\u79fb\u8bc4\u4f30(PMSA)\u5b9a\u91cf\u6d4b\u91cf\u8f6c\u79fb\u7a0b\u5ea6\uff1b2) \u8f6c\u79fb\u4fe1\u606f\u6784\u5efa\u52a8\u6001\u6f14\u5316\u591a\u7ea7\u8f6c\u79fb\u72b6\u6001\uff1b3) \u5fc3\u7406\u52a8\u673a\u8f6c\u79fb\u9a71\u52a8\u4fe1\u606f\u5206\u89e3\u8de8\u8f6c\u79fb\u7ea7\u522b\u5206\u89e3\u548c\u6b63\u5219\u5316\u8868\u793a\uff1b4) \u5fc3\u7406\u52a8\u673a\u8f6c\u79fb\u4fe1\u606f\u5339\u914d\u589e\u5f3a\u4e0e\u8f6c\u79fb\u76f8\u5173\u7684\u534f\u4f5c\u6a21\u5f0f", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSRSUPM\u5728\u591a\u79cd\u5e8f\u5217\u63a8\u8350\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "SRSUPM\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5fc3\u7406\u52a8\u673a\u8f6c\u79fb\uff0c\u80fd\u591f\u5b66\u4e60\u66f4\u5177\u533a\u5206\u6027\u7684\u7528\u6237\u8868\u793a\uff0c\u6709\u6548\u63d0\u5347\u5e8f\u5217\u63a8\u8350\u6027\u80fd"}}
{"id": "2602.07202", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07202", "abs": "https://arxiv.org/abs/2602.07202", "authors": ["Alonso Granados", "Jason Pacheco"], "title": "Risk-Sensitive Exponential Actor Critic", "comment": "To appear at AAAI 2026", "summary": "Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.", "AI": {"tldr": "\u63d0\u51farsEAC\u65b9\u6cd5\u89e3\u51b3\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u53ef\u9760\u5b66\u4e60\u98ce\u9669\u654f\u611f\u7b56\u7565", "motivation": "\u65e0\u6a21\u578b\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u98ce\u9669\u611f\u77e5\u667a\u80fd\u4f53\u3002\u73b0\u6709\u57fa\u4e8e\u71b5\u98ce\u9669\u5ea6\u91cf\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5b58\u5728\u9ad8\u65b9\u5dee\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u98ce\u9669\u654f\u611f\u6307\u6570\u6f14\u5458-\u8bc4\u8bba\u5bb6\uff08rsEAC\uff09\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u79bb\u7b56\u7565\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u5305\u542b\u907f\u514d\u663e\u5f0f\u8868\u793a\u6307\u6570\u4ef7\u503c\u51fd\u6570\u53ca\u5176\u68af\u5ea6\u7684\u65b0\u7a0b\u5e8f\uff0c\u5e76\u9488\u5bf9\u71b5\u98ce\u9669\u5ea6\u91cf\u4f18\u5316\u7b56\u7565\u3002", "result": "rsEAC\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u4ea7\u751f\u66f4\u6570\u503c\u7a33\u5b9a\u7684\u66f4\u65b0\uff0c\u5728MuJoCo\u8fde\u7eed\u4efb\u52a1\u7684\u5371\u9669\u53d8\u4f53\u4e2d\u53ef\u9760\u5730\u5b66\u4e60\u98ce\u9669\u654f\u611f\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0crsEAC\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u9669\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u98ce\u9669\u611f\u77e5\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07050", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07050", "abs": "https://arxiv.org/abs/2602.07050", "authors": ["Sonia Joseph", "Quentin Garrido", "Randall Balestriero", "Matthew Kowal", "Thomas Fel", "Shahab Bakhtiari", "Blake Richards", "Mike Rabbat"], "title": "Interpreting Physics in Video World Models", "comment": null, "summary": "A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.\n  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u73b0\u4ee3\u89c6\u9891\u7f16\u7801\u5668\u901a\u8fc7\u5206\u5e03\u5f0f\u800c\u975e\u56e0\u5b50\u5316\u8868\u793a\u7269\u7406\u53d8\u91cf\uff0c\u5b58\u5728\"\u7269\u7406\u6d8c\u73b0\u533a\"\u7684\u4e2d\u95f4\u5c42\u8fc7\u6e21\uff0c\u5176\u4e2d\u8fd0\u52a8\u65b9\u5411\u4ee5\u9ad8\u7ef4\u73af\u5f62\u51e0\u4f55\u7ed3\u6784\u7f16\u7801\u3002", "motivation": "\u63a2\u7d22\u89c6\u9891\u6a21\u578b\u662f\u5426\u4f9d\u8d56\u7269\u7406\u53d8\u91cf\u7684\u56e0\u5b50\u5316\u8868\u793a\u6765\u505a\u51fa\u51c6\u786e\u7269\u7406\u9884\u6d4b\uff0c\u8fd8\u662f\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u5206\u5e03\u5f0f\u65b9\u5f0f\u9690\u5f0f\u8868\u793a\u8fd9\u4e9b\u53d8\u91cf\uff0c\u5e76\u7814\u7a76\u5927\u578b\u89c6\u9891\u7f16\u7801\u5668\u5185\u90e8\u7684\u7269\u7406\u8868\u5f81\u673a\u5236\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u63a2\u6d4b\u3001\u5b50\u7a7a\u95f4\u51e0\u4f55\u5206\u6790\u3001\u8865\u4e01\u7ea7\u89e3\u7801\u548c\u9488\u5bf9\u6027\u6ce8\u610f\u529b\u6d88\u878d\u7b49\u65b9\u6cd5\uff0c\u7814\u7a76\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u89c6\u9891\u53d8\u6362\u5668\u4e2d\u7684\u7269\u7406\u4fe1\u606f\u53ef\u8bbf\u95ee\u6027\u548c\u7ec4\u7ec7\u65b9\u5f0f\u3002", "result": "\u53d1\u73b0\u6240\u6709\u67b6\u6784\u4e2d\u90fd\u5b58\u5728\u4e00\u4e2a\u5c16\u9510\u7684\u4e2d\u95f4\u6df1\u5ea6\u8fc7\u6e21\u533a\uff08\u7269\u7406\u6d8c\u73b0\u533a\uff09\uff0c\u7269\u7406\u53d8\u91cf\u5728\u6b64\u53d8\u5f97\u53ef\u8bbf\u95ee\uff1b\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u7b49\u6807\u91cf\u4ece\u65e9\u671f\u5c42\u5c31\u53ef\u83b7\u5f97\uff0c\u800c\u8fd0\u52a8\u65b9\u5411\u4ec5\u5728\u7269\u7406\u6d8c\u73b0\u533a\u53d8\u5f97\u53ef\u8bbf\u95ee\uff0c\u4e14\u901a\u8fc7\u5177\u6709\u73af\u5f62\u51e0\u4f55\u7ed3\u6784\u7684\u9ad8\u7ef4\u7fa4\u4f53\u7ed3\u6784\u7f16\u7801\u3002", "conclusion": "\u73b0\u4ee3\u89c6\u9891\u6a21\u578b\u4e0d\u4f7f\u7528\u7ecf\u5178\u7269\u7406\u5f15\u64ce\u90a3\u6837\u7684\u7269\u7406\u53d8\u91cf\u56e0\u5b50\u5316\u8868\u793a\uff0c\u800c\u662f\u4f7f\u7528\u5206\u5e03\u5f0f\u8868\u793a\uff0c\u8fd9\u79cd\u8868\u793a\u8db3\u4ee5\u8fdb\u884c\u7269\u7406\u9884\u6d4b\u3002"}}
{"id": "2602.08754", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.08754", "abs": "https://arxiv.org/abs/2602.08754", "authors": ["Rose E. Guingrich", "Dvija Mehta", "Umang Bhatt"], "title": "Belief Offloading in Human-AI Interaction", "comment": null, "summary": "What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, \"belief offloading,\" in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u7c7b\u4e0eAI\u4ea4\u4e92\u4e2d\u7684\"\u4fe1\u5ff5\u5378\u8f7d\"\u73b0\u8c61\uff0c\u5373\u4eba\u4eec\u5c06\u5f62\u6210\u548c\u7ef4\u6301\u4fe1\u5ff5\u7684\u8fc7\u7a0b\u5916\u5305\u7ed9AI\u7cfb\u7edf\uff0c\u8fd9\u5bf9\u4ed6\u4eec\u7684\u884c\u4e3a\u548c\u4fe1\u5ff5\u4f53\u7cfb\u4ea7\u751f\u4e0b\u6e38\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u4eba\u4eec\u8d8a\u6765\u8d8a\u591a\u5730\u5c06LLM\u804a\u5929\u673a\u5668\u4eba\u4f5c\u4e3a\u601d\u7ef4\u4f19\u4f34\u4f7f\u7528\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8ba4\u77e5\u5378\u8f7d\uff0c\u5728\u8fc7\u5ea6\u4f9d\u8d56\u7684\u60c5\u51b5\u4e0b\u5bf9\u8ba4\u77e5\u6280\u80fd\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5b9a\u4e49\u548c\u8c03\u67e5\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u7279\u5b9a\u8ba4\u77e5\u5378\u8f7d\u7c7b\u578b\u2014\u2014\"\u4fe1\u5ff5\u5378\u8f7d\"\u3002", "method": "\u7ed3\u5408\u54f2\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\uff0c\u6f84\u6e05\u4fe1\u5ff5\u5378\u8f7d\u53d1\u751f\u7684\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u63d0\u4f9b\u4fe1\u5ff5\u5378\u8f7d\u7684\u63cf\u8ff0\u6027\u5206\u7c7b\u53ca\u5176\u89c4\u8303\u542b\u4e49\u3002", "result": "\u63d0\u51fa\u4e86\u4fe1\u5ff5\u5378\u8f7d\u7684\u6982\u5ff5\u6846\u67b6\u548c\u5206\u7c7b\u4f53\u7cfb\uff0c\u660e\u786e\u4e86\u5176\u53d1\u751f\u7684\u6761\u4ef6\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u79cd\u73b0\u8c61\u7684\u89c4\u8303\u542b\u4e49\u3002", "conclusion": "\u4fe1\u5ff5\u5378\u8f7d\u662f\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u73b0\u8c61\uff0c\u5177\u6709\u663e\u8457\u7684\u884c\u4e3a\u548c\u8ba4\u77e5\u5f71\u54cd\u3002\u672a\u6765\u5de5\u4f5c\u9700\u8981\u8bc4\u4f30\u4fe1\u5ff5\u5378\u8f7d\u7684\u6f5c\u5728\u53ef\u80fd\u6027\u548c\u540e\u679c\u3002"}}
{"id": "2602.08678", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.08678", "abs": "https://arxiv.org/abs/2602.08678", "authors": ["Xiaomeng Song", "Xinru Wang", "Hanbing Wang", "Hongyu Lu", "Yu Chen", "Zhaochun Ren", "Zhumin Chen"], "title": "SA-CAISR: Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation", "comment": null, "summary": "Sequential recommendation (SR) aims to predict a user's next action by learning from their historical interaction sequences. In real-world applications, these models require periodic updates to adapt to new interactions and evolving user preferences. While incremental learning methods facilitate these updates, they face significant challenges. Replay-based approaches incur high memory and computational costs, and regularization-based methods often struggle to discard outdated or conflicting knowledge. To overcome these challenges, we propose SA-CAISR, a Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation framework. As a buffer-free framework, SA-CAISR operates using only the old model and new data, directly addressing the high costs of replay-based techniques. SA-CAISR introduces a novel Fisher-weighted knowledge-screening mechanism that dynamically identifies outdated knowledge by estimating parameter-level conflicts between the old model and new data, allowing our approach to selectively remove obsolete knowledge while preserving compatible historical patterns. This dynamic balance between stability and adaptability allows our method to achieve a new state-of-the-art performance in incremental SR. Specifically, SA-CAISR improves Recall@20 by 2.0%, MRR@20 by 1.2%, and NDCG@20 by 1.4% on average across datasets, while reducing memory usage by 97.5% and training time by 46.9% compared to the best baselines. This efficiency allows real-world systems to rapidly update user profiles with minimal computational overhead, ensuring more timely and accurate recommendations.", "AI": {"tldr": "SA-CAISR\u662f\u4e00\u4e2a\u65e0\u7f13\u51b2\u7684\u9636\u6bb5\u81ea\u9002\u5e94\u51b2\u7a81\u611f\u77e5\u589e\u91cf\u5e8f\u5217\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7Fisher\u52a0\u6743\u77e5\u8bc6\u7b5b\u9009\u673a\u5236\u52a8\u6001\u8bc6\u522b\u8fc7\u65f6\u77e5\u8bc6\uff0c\u5728\u51cf\u5c1197.5%\u5185\u5b58\u4f7f\u7528\u548c46.9%\u8bad\u7ec3\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5728\u5e8f\u5217\u63a8\u8350\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u91cd\u653e\u7684\u65b9\u6cd5\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u4e22\u5f03\u8fc7\u65f6\u6216\u51b2\u7a81\u77e5\u8bc6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u589e\u91cf\u5b66\u4e60\u6846\u67b6\u6765\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u7528\u6237\u504f\u597d\u3002", "method": "\u63d0\u51faSA-CAISR\u6846\u67b6\uff0c\u91c7\u7528\u65e0\u7f13\u51b2\u8bbe\u8ba1\uff0c\u4ec5\u4f7f\u7528\u65e7\u6a21\u578b\u548c\u65b0\u6570\u636e\u3002\u5f15\u5165Fisher\u52a0\u6743\u77e5\u8bc6\u7b5b\u9009\u673a\u5236\uff0c\u901a\u8fc7\u4f30\u8ba1\u65e7\u6a21\u578b\u4e0e\u65b0\u6570\u636e\u4e4b\u95f4\u7684\u53c2\u6570\u7ea7\u51b2\u7a81\uff0c\u52a8\u6001\u8bc6\u522b\u8fc7\u65f6\u77e5\u8bc6\uff0c\u9009\u62e9\u6027\u79fb\u9664\u8fc7\u65f6\u77e5\u8bc6\u540c\u65f6\u4fdd\u7559\u517c\u5bb9\u7684\u5386\u53f2\u6a21\u5f0f\u3002", "result": "\u5728\u589e\u91cf\u5e8f\u5217\u63a8\u8350\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff1aRecall@20\u5e73\u5747\u63d0\u53472.0%\uff0cMRR@20\u63d0\u53471.2%\uff0cNDCG@20\u63d0\u53471.4%\u3002\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1197.5%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1146.9%\u3002", "conclusion": "SA-CAISR\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u589e\u91cf\u5e8f\u5217\u63a8\u8350\uff0c\u4f7f\u73b0\u5b9e\u7cfb\u7edf\u80fd\u591f\u4ee5\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u5feb\u901f\u66f4\u65b0\u7528\u6237\u753b\u50cf\uff0c\u786e\u4fdd\u66f4\u53ca\u65f6\u51c6\u786e\u7684\u63a8\u8350\u3002"}}
{"id": "2602.07203", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07203", "abs": "https://arxiv.org/abs/2602.07203", "authors": ["R. Teal Witter", "\u00c1lvaro Parafita", "Tomas Garriga", "Maximilian Muschalik", "Fabian Fumagalli", "Axel Brando", "Lucas Rosenblatt"], "title": "Exactly Computing do-Shapley Values", "comment": null, "summary": "Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97do-Shapley\u503c\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06do-Shapley\u503c\u91cd\u65b0\u8868\u8ff0\u4e3a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4e2d\u4e0d\u53ef\u7ea6\u96c6\u7684\u5f62\u5f0f\uff0c\u5b9e\u73b0\u4e86\u4ece\u7ebf\u6027\u5230\u6307\u6570\u7ea7\u7684\u8ba1\u7b97\u52a0\u901f\uff0c\u5e76\u964d\u4f4e\u4e86\u8bc6\u522b\u8d1f\u62c5\u3002", "motivation": "\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u662f\u63cf\u8ff0\u81ea\u7136\u79d1\u5b66\u4e2d\u590d\u6742\u52a8\u6001\u7684\u5f3a\u5927\u6846\u67b6\uff0cdo-Shapley\u662f\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316d\u4e2a\u53d8\u91cf\u5728\u6307\u6570\u7ea7\u5e72\u9884\u4e0b\u7684\u5e73\u5747\u6548\u5e94\u3002\u7136\u800c\uff0c\u4e0eShapley\u503c\u7c7b\u4f3c\uff0c\u8ba1\u7b97do-Shapley\u503c\u901a\u5e38\u9700\u8981\u8bc4\u4f30\u6307\u6570\u7ea7\u6570\u91cf\u7684\u9879\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\u3002", "method": "1. \u5c06do-Shapley\u503c\u91cd\u65b0\u8868\u8ff0\u4e3a\u5e95\u5c42SCM\u7684\u4e0d\u53ef\u7ea6\u96c6\u5f62\u5f0f\uff1b2. \u57fa\u4e8e\u6b64\u6d1e\u5bdf\uff0c\u5f00\u53d1\u4e86\u7cbe\u786e\u7b97\u6cd5\uff0c\u8ba1\u7b97\u65f6\u95f4\u4e0e\u4e0d\u53ef\u7ea6\u96c6\u6570\u91cfr\u5448\u7ebf\u6027\u5173\u7cfb\uff08r\u8303\u56f4\u4eced\u52302^d\uff09\uff1b3. \u63d0\u51fa\u4f30\u8ba1\u5668\uff0c\u53ef\u5728\u4efb\u610f\u67e5\u8be2\u9884\u7b97\u4e0b\u8fd0\u884c\uff0c\u968f\u7740\u9884\u7b97\u63a5\u8fd1r\uff0c\u4f30\u8ba1\u7cbe\u5ea6\u663e\u8457\u63d0\u9ad8\uff1b4. \u8bc1\u660e\u975e\u53c2\u6570\u8bc6\u522bdo-Shapley\u503c\u4ec5\u9700\u8bc6\u522bd\u4e2a\u5355\u5143\u7d20\u8054\u76df\u7684\u5e72\u9884\u6548\u5e94\uff0c\u800c\u975e\u6240\u6709\u7c7b\u522b\u3002", "result": "1. \u5f53\u67e5\u8be2\u9884\u7b97\u63a5\u8fd1r\u65f6\uff0c\u65b0\u4f30\u8ba1\u5668\u6bd4\u5148\u524d\u65b9\u6cd5\u7cbe\u5ea6\u63d0\u9ad8\u6570\u4e2a\u6570\u91cf\u7ea7\uff1b2. \u5f53\u9884\u7b97\u8fbe\u5230r\u65f6\uff0c\u53ef\u8fd4\u56de\u673a\u5668\u7cbe\u5ea6\u7684Shapley\u503c\uff1b3. \u8ba1\u7b97\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u7ebf\u6027\uff1b4. \u8bc6\u522b\u8d1f\u62c5\u5927\u5e45\u964d\u4f4e\uff0c\u4ec5\u9700\u8bc6\u522bd\u4e2a\u5355\u5143\u7d20\u8054\u76df\u800c\u975e\u6240\u6709\u7c7b\u522b\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5c06do-Shapley\u503c\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e0d\u53ef\u7ea6\u96c6\u5f62\u5f0f\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u548c\u8bc6\u522b\u8d1f\u62c5\u7684\u5927\u5e45\u964d\u4f4e\u3002\u65b0\u65b9\u6cd5\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u4f30\u8ba1\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4e2d\u7684\u56e0\u679c\u6548\u5e94\u91cf\u5316\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2602.07051", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.07051", "abs": "https://arxiv.org/abs/2602.07051", "authors": ["Karthik Sivakoti"], "title": "Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning", "comment": null, "summary": "Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.", "AI": {"tldr": "Neural Sentinel\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b8c\u6210\u8f66\u724c\u8bc6\u522b\u3001\u72b6\u6001\u5206\u7c7b\u548c\u8f66\u8f86\u5c5e\u6027\u63d0\u53d6\uff0c\u51c6\u786e\u738792.3%\uff0c\u6bd4\u4f20\u7edfOCR\u65b9\u6cd5\u63d0\u534714.1%\uff0c\u652f\u6301\u96f6\u6837\u672c\u6cdb\u5316\u5230\u5176\u4ed6\u8f66\u8f86\u68c0\u6d4b\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edfALPR\u7cfb\u7edf\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff08\u76ee\u6807\u68c0\u6d4b+OCR\u6a21\u5757\uff09\uff0c\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u3001\u5ef6\u8fdf\u589e\u52a0\u548c\u67b6\u6784\u590d\u6742\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7edf\u4e00\u3001\u9ad8\u6548\u4e14\u80fd\u5904\u7406\u591a\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4f7f\u7528PaliGemma 3B\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u8fdb\u884c\u9002\u914d\uff1b2. \u5355\u6b21\u524d\u5411\u4f20\u64ad\u540c\u65f6\u56de\u7b54\u591a\u4e2a\u5173\u4e8e\u8f66\u8f86\u56fe\u50cf\u7684\u89c6\u89c9\u95ee\u9898\uff1b3. \u5f15\u5165\u4eba\u673a\u534f\u540c\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u4fdd\u630170:30\u7684\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u4e0e\u4fee\u6b63\u6837\u672c\u6bd4\u4f8b\uff0c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff1b4. \u652f\u6301\u96f6\u6837\u672c\u6cdb\u5316\u5230\u8f85\u52a9\u4efb\u52a1\u3002", "result": "1. \u8f66\u724c\u8bc6\u522b\u51c6\u786e\u738792.3%\uff0c\u6bd4EasyOCR\u63d0\u534714.1%\uff0c\u6bd4PaddleOCR\u63d0\u53479.9%\uff1b2. \u5e73\u5747\u63a8\u7406\u5ef6\u8fdf152ms\uff1b3. \u9884\u671f\u6821\u51c6\u8bef\u5dee0.048\uff0c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u826f\u597d\uff1b4. \u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff1a\u8f66\u8f86\u989c\u8272\u68c0\u6d4b89%\uff0c\u5b89\u5168\u5e26\u68c0\u6d4b82%\uff0c\u4e58\u5458\u8ba1\u657078%\uff1b5. \u5728\u771f\u5b9e\u6536\u8d39\u7ad9\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "conclusion": "\u7edf\u4e00\u7684\u89c6\u89c9\u8bed\u8a00\u65b9\u6cd5\u4ee3\u8868\u4e86ALPR\u7cfb\u7edf\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u63d0\u4f9b\u4e86\u4f18\u4e8e\u4f20\u7edf\u6d41\u6c34\u7ebf\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u964d\u4f4e\u7684\u67b6\u6784\u590d\u6742\u6027\u4ee5\u53ca\u65b0\u5174\u7684\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08816", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08816", "abs": "https://arxiv.org/abs/2602.08816", "authors": ["James Jewitt", "Gopi Krishnan Rajbahadur", "Hao Li", "Bram Adams", "Ahmed E. Hassan"], "title": "Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity", "comment": "13 pages, 2 figures, 10 tables", "summary": "Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\\rightarrow$ model $\\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\\% of datasets and 95.8\\% of models lack the required license text, only 2.3\\% of datasets and 3.2\\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\\% of models preserve compliant dataset notices and only 5.75\\% of applications preserve compliant model notices (with just 6.38\\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f00\u6e90AI\u9886\u57df\u5b58\u5728\u666e\u904d\u7684\"\u8bb8\u53ef\u6e05\u6d17\"\u73b0\u8c61\uff1a96.5%\u7684\u6570\u636e\u96c6\u548c95.8%\u7684\u6a21\u578b\u7f3a\u4e4f\u5fc5\u8981\u7684\u8bb8\u53ef\u6587\u672c\uff0c\u53ea\u6709\u6781\u5c11\u6570\u6ee1\u8db3\u5b8c\u6574\u7684\u8bb8\u53ef\u8981\u6c42\uff0c\u5373\u4f7f\u4e0a\u6e38\u63d0\u4f9b\u4e86\u5408\u89c4\u8bb8\u53ef\uff0c\u4e0b\u6e38\u4e5f\u5f88\u5c11\u6b63\u786e\u4f20\u64ad\u3002", "motivation": "\u5f00\u6e90AI\u9879\u76ee\u867d\u7136\u4f7f\u7528MIT\u3001Apache-2.0\u7b49\u5bbd\u677e\u8bb8\u53ef\u8bc1\uff0c\u4f46\u8fd9\u4e9b\u8bb8\u53ef\u8bc1\u5305\u542b\u5fc5\u987b\u6ee1\u8db3\u7684\u6cd5\u5f8b\u8981\u6c42\uff08\u5b8c\u6574\u8bb8\u53ef\u6587\u672c\u3001\u7248\u6743\u58f0\u660e\u3001\u4e0a\u6e38\u5f52\u5c5e\uff09\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u8981\u6c42\u7684\u5927\u89c4\u6a21\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b9e\u9645\u4f7f\u7528\u8d85\u51fa\u8bb8\u53ef\u8303\u56f4\uff0c\u4f7f\u4e0b\u6e38\u7528\u6237\u9762\u4e34\u6cd5\u5f8b\u98ce\u9669\u3002", "method": "\u5bf9124,278\u6761\u6570\u636e\u96c6\u2192\u6a21\u578b\u2192\u5e94\u7528\u4f9b\u5e94\u94fe\u8fdb\u884c\u5b9e\u8bc1\u5ba1\u8ba1\uff0c\u6db5\u76d6Hugging Face\u548cGitHub\u4e0a\u76843,338\u4e2a\u6570\u636e\u96c6\u30016,664\u4e2a\u6a21\u578b\u548c28,516\u4e2a\u5e94\u7528\u3002\u68c0\u67e5\u662f\u5426\u5305\u542b\u5b8c\u6574\u8bb8\u53ef\u6587\u672c\u3001\u7248\u6743\u58f0\u660e\uff0c\u4ee5\u53ca\u4e0a\u6e38\u5f52\u5c5e\u662f\u5426\u5728\u4e0b\u6e38\u6b63\u786e\u4f20\u64ad\u3002", "result": "\u60ca\u4eba\u768496.5%\u6570\u636e\u96c6\u548c95.8%\u6a21\u578b\u7f3a\u4e4f\u5fc5\u8981\u8bb8\u53ef\u6587\u672c\uff1b\u4ec52.3%\u6570\u636e\u96c6\u548c3.2%\u6a21\u578b\u540c\u65f6\u6ee1\u8db3\u8bb8\u53ef\u6587\u672c\u548c\u7248\u6743\u8981\u6c42\uff1b\u5373\u4f7f\u4e0a\u6e38\u63d0\u4f9b\u5b8c\u6574\u8bb8\u53ef\u8bc1\u636e\uff0c\u4e0b\u6e38\u4f20\u64ad\u7387\u6781\u4f4e\uff1a\u4ec527.59%\u6a21\u578b\u4fdd\u7559\u5408\u89c4\u6570\u636e\u96c6\u58f0\u660e\uff0c\u4ec55.75%\u5e94\u7528\u4fdd\u7559\u5408\u89c4\u6a21\u578b\u58f0\u660e\u3002", "conclusion": "\u4ece\u4e1a\u8005\u4e0d\u80fd\u5047\u8bbe\u5bbd\u677e\u8bb8\u53ef\u8bc1\u6807\u7b7e\u80fd\u63d0\u4f9b\u5176\u58f0\u79f0\u7684\u6743\u5229\uff1a\u8bb8\u53ef\u6587\u4ef6\u548c\u58f0\u660e\uff08\u800c\u975e\u5143\u6570\u636e\uff09\u624d\u662f\u6cd5\u5f8b\u771f\u5b9e\u6027\u7684\u6765\u6e90\u3002\u5f00\u6e90AI\u5b58\u5728\u666e\u904d\u7684\"\u8bb8\u53ef\u6e05\u6d17\"\u95ee\u9898\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u5408\u89c4\u5b9e\u8df5\u3002"}}
{"id": "2602.08837", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08837", "abs": "https://arxiv.org/abs/2602.08837", "authors": ["Minh-Duc Nguyen", "Hai-Dang Kieu", "Dung D. Le"], "title": "AMEM4Rec: Leveraging Cross-User Similarity for Memory Evolution in Agentic LLM Recommenders", "comment": null, "summary": "Agentic systems powered by Large Language Models (LLMs) have shown strong potential in recommender systems but remain hindered by several challenges. Fine-tuning LLMs is parameter-inefficient, and prompt-based agentic reasoning is limited by context length and hallucination risk. Moreover, existing agentic recommendation systems predominantly leverages semantic knowledge while neglecting the collaborative filtering (CF) signals essential for implicit preference modeling. To address these limitations, we propose AMEM4Rec, an agentic LLM-based recommender that learns collaborative signals in an end-to-end manner through cross-user memory evolution. AMEM4Rec stores abstract user behavior patterns from user histories in a global memory pool. Within this pool, memories are linked to similar existing ones and iteratively evolved to reinforce shared cross-user patterns, enabling the system to become aware of CF signals without relying on a pre-trained CF model. Extensive experiments on Amazon and MIND datasets show that AMEM4Rec consistently outperforms state-of-the-art LLM-based recommenders, demonstrating the effectiveness of evolving memory-guided collaborative filtering.", "AI": {"tldr": "AMEM4Rec\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u8de8\u7528\u6237\u8bb0\u5fc6\u6f14\u5316\u5b66\u4e60\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM\u63a8\u8350\u7cfb\u7edf\u53c2\u6570\u6548\u7387\u4f4e\u3001\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u5ffd\u89c6\u534f\u540c\u4fe1\u53f7\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u51e0\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5fae\u8c03LLM\u53c2\u6570\u6548\u7387\u4f4e\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u667a\u80fd\u63a8\u7406\u53d7\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u4e14\u6709\u5e7b\u89c9\u98ce\u9669\uff0c\u4e14\u73b0\u6709\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u8bed\u4e49\u77e5\u8bc6\u800c\u5ffd\u89c6\u4e86\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\u8fd9\u4e00\u5efa\u6a21\u9690\u5f0f\u504f\u597d\u7684\u5173\u952e\u8981\u7d20\u3002", "method": "\u63d0\u51faAMEM4Rec\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u7528\u6237\u8bb0\u5fc6\u6f14\u5316\u7aef\u5230\u7aef\u5b66\u4e60\u534f\u540c\u4fe1\u53f7\u3002\u7cfb\u7edf\u4ece\u7528\u6237\u5386\u53f2\u4e2d\u63d0\u53d6\u62bd\u8c61\u884c\u4e3a\u6a21\u5f0f\u5b58\u50a8\u5728\u5168\u5c40\u8bb0\u5fc6\u6c60\u4e2d\uff0c\u8bb0\u5fc6\u901a\u8fc7\u94fe\u63a5\u5230\u76f8\u4f3c\u73b0\u6709\u8bb0\u5fc6\u5e76\u8fed\u4ee3\u6f14\u5316\u6765\u5f3a\u5316\u8de8\u7528\u6237\u5171\u4eab\u6a21\u5f0f\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u611f\u77e5\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\u800c\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u534f\u540c\u8fc7\u6ee4\u6a21\u578b\u3002", "result": "\u5728Amazon\u548cMIND\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAMEM4Rec\u5728\u5404\u9879\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u6f14\u5316\u8bb0\u5fc6\u5f15\u5bfc\u7684\u534f\u540c\u8fc7\u6ee4\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "AMEM4Rec\u901a\u8fc7\u8de8\u7528\u6237\u8bb0\u5fc6\u6f14\u5316\u6210\u529f\u5730\u5c06\u534f\u540c\u8fc7\u6ee4\u4fe1\u53f7\u6574\u5408\u5230\u57fa\u4e8eLLM\u7684\u667a\u80fd\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.07205", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07205", "abs": "https://arxiv.org/abs/2602.07205", "authors": ["Junyan Liu", "Haipeng Luo", "Zihan Zhang", "Lillian J. Ratliff"], "title": "Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation", "comment": "36 pages", "summary": "We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.\n  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\\min \\{\\sqrt{K} + (CK)^{1/3},\\sqrt{LK}\\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(\u03b7C + \\sqrt{K/\u03b7})$ regret bound, where $\u03b7$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $\u03b7$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u53cc\u4eba\u65e0\u4fe1\u606f\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7ecf\u9a8c\u7eb3\u4ec0\u503c\u9057\u61be\u5ea6\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u5728\u5bf9\u624b\u7b56\u7565\u53d8\u5316\u7a0b\u5ea6\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u90fd\u80fd\u83b7\u5f97\u6700\u4f18\u9057\u61be\u754c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u53cc\u4eba\u65e0\u4fe1\u606f\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u65e0\u6cd5\u5b9e\u73b0\u65e0\u5916\u90e8\u9057\u61be\uff1b2) \u73b0\u6709\u7eb3\u4ec0\u503c\u9057\u61be\u7b97\u6cd5\u65e0\u6cd5\u9002\u5e94\u95ee\u9898\u96be\u5ea6\uff0c\u5373\u4f7f\u5728\u5bf9\u624b\u7b56\u7565\u56fa\u5b9a\u7684\u7b80\u5355\u60c5\u51b5\u4e0b\u4e5f\u53ea\u80fd\u83b7\u5f97\u8f83\u5dee\u7684\u9057\u61be\u754c\u3002\u672c\u6587\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u9650\u5236\u3002", "method": "\u9996\u5148\u63d0\u51fa\u7ecf\u9a8c\u7eb3\u4ec0\u503c\u9057\u61be\u8fd9\u4e00\u65b0\u7684\u9057\u61be\u5ea6\u91cf\uff0c\u5b83\u6bd4\u7eb3\u4ec0\u503c\u9057\u61be\u66f4\u5f3a\uff0c\u4e14\u5728\u5bf9\u624b\u56fa\u5b9a\u65f6\u81ea\u7136\u9000\u5316\u4e3a\u5916\u90e8\u9057\u61be\u3002\u7136\u540e\u63d0\u51fa\u53c2\u6570\u81ea\u7531\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u57fa\u4e8e\u5468\u671f\u7684V-learning\u7b97\u6cd5\uff0c\u5efa\u7acb\u9057\u61be\u754c\uff0c\u5e76\u8bbe\u8ba1\u81ea\u9002\u5e94\u91cd\u542f\u673a\u5236\u6765\u5e94\u5bf9\u5bf9\u624b\u7684\u975e\u5e73\u7a33\u6027\u3002", "result": "\u7b97\u6cd5\u83b7\u5f97\u4e86O(min{\u221aK + (CK)^{1/3}, \u221aLK})\u7684\u9057\u61be\u754c\uff0c\u5176\u4e2dC\u91cf\u5316\u5bf9\u624b\u7b56\u7565\u7684\u65b9\u5dee\uff0cL\u8868\u793a\u7b56\u7565\u5207\u6362\u6b21\u6570\u3002\u8be5\u7ed3\u679c\u4e0d\u4ec5\u6062\u590d\u4e86\u4e24\u4e2a\u6781\u7aef\u60c5\u51b5\uff08\u5bf9\u624b\u56fa\u5b9a\u65f6\u7684O(\u221aK)\u5916\u90e8\u9057\u61be\u548c\u6700\u574f\u60c5\u51b5\u4e0b\u7684O(K^{2/3})\u7eb3\u4ec0\u503c\u9057\u61be\uff09\uff0c\u8fd8\u80fd\u901a\u8fc7\u81ea\u9002\u5e94\u5bf9\u624b\u975e\u5e73\u7a33\u6027\u5728\u8fd9\u4e9b\u6781\u7aef\u4e4b\u95f4\u5e73\u6ed1\u63d2\u503c\u3002", "conclusion": "\u672c\u6587\u5b8c\u5168\u89e3\u51b3\u4e86\u53cc\u4eba\u65e0\u4fe1\u606f\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u66f4\u5f3a\u7684\u9057\u61be\u5ea6\u91cf\u548c\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u5bf9\u624b\u884c\u4e3a\u6a21\u5f0f\u4e0b\u81ea\u52a8\u83b7\u5f97\u6700\u4f18\u9057\u61be\u6027\u80fd\u3002"}}
{"id": "2602.07052", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07052", "abs": "https://arxiv.org/abs/2602.07052", "authors": ["Ziye Xie", "Oded Schlesinger", "Raj Kundu", "Jessica Y. Choi", "Pablo Iturralde", "Dennis A. Turner", "Stefan M. Goetz", "Guillermo Sapiro", "Angel V. Peterchev", "J. Matias Di Martino"], "title": "Toward Accurate and Accessible Markerless Neuronavigation", "comment": null, "summary": "Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01\u00b0$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u65e0\u6807\u8bb0\u795e\u7ecf\u5bfc\u822a\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u5149\u76f8\u673a\u7ed3\u5408\u9762\u90e8\u51e0\u4f55\u5efa\u6a21\uff0c\u66ff\u4ee3\u4f20\u7edf\u4f9d\u8d56\u6807\u8bb0\u7684\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u663e\u793a\u7cbe\u5ea6\u8db3\u591f\u7528\u4e8e\u7ecf\u9885\u78c1\u523a\u6fc0\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u4e3b\u4f53\u5b89\u88c5\u7684\u6807\u8bb0\uff0c\u9700\u8981\u624b\u52a8\u914d\u51c6\uff0c\u53ef\u80fd\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u79fb\u4f4d\uff0c\u5e76\u5f15\u8d77\u4e0d\u9002\u3002\u8fd9\u4e9b\u7cfb\u7edf\u6602\u8d35\u4e14\u590d\u6742\uff0c\u9650\u5236\u4e86\u795e\u7ecf\u5bfc\u822a\u5728\u4e34\u5e8a\u548c\u7814\u7a76\u73af\u5883\u4e2d\u7684\u53ef\u53ca\u6027\u3002", "method": "\u5f15\u5165\u65e0\u6807\u8bb0\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f4e\u6210\u672c\u53ef\u89c1\u5149\u548c\u7ea2\u5916\u5149\u76f8\u673a\uff08\u7ed3\u5408\u7acb\u4f53\u89c6\u89c9\u548c\u6df1\u5ea6\u611f\u77e5\uff09\uff0c\u901a\u8fc7\u7b97\u6cd5\u5bf9\u9762\u90e8\u51e0\u4f55\u8fdb\u884c\u5efa\u6a21\uff0c\u66ff\u4ee3\u6602\u8d35\u7684\u786c\u4ef6\u548c\u7269\u7406\u6807\u8bb0\u3002", "result": "\u572850\u540d\u4eba\u7c7b\u53d7\u8bd5\u8005\u9a8c\u8bc1\u4e2d\uff0c\u6700\u4f73\u65e0\u6807\u8bb0\u7b97\u6cd5\u7684\u4e2d\u4f4d\u8ddf\u8e2a\u8bef\u5dee\u4ec5\u4e3a2.32\u6beb\u7c73\u548c2.01\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u6807\u8bb0\u7cfb\u7edf\u5177\u6709\u8db3\u591f\u7cbe\u5ea6\u7528\u4e8e\u7ecf\u9885\u78c1\u523a\u6fc0\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65e0\u6807\u8bb0\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u6807\u8bb0\u795e\u7ecf\u5bfc\u822a\u65b9\u6cd5\u53ef\u4ee5\u964d\u4f4e\u8bbe\u7f6e\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u60a3\u8005\u8212\u9002\u5ea6\uff0c\u5e76\u6269\u5927\u795e\u7ecf\u5bfc\u822a\u5728\u4e34\u5e8a\u548c\u7814\u7a76\u73af\u5883\u4e2d\u7684\u53ef\u53ca\u6027\u3002\u4e0d\u540c\u76f8\u673a\u4f20\u611f\u5668\u7684\u6570\u636e\u878d\u5408\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6574\u4f53\u7cbe\u5ea6\u3002"}}
{"id": "2602.07473", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.07473", "abs": "https://arxiv.org/abs/2602.07473", "authors": ["Nathana\u00ebl Fijalkow", "Arka Ghosh", "Roman Kniazev", "Guillermo A. P\u00e9rez", "Pierre Vandenhove"], "title": "Computing the Reachability Value of Posterior-Deterministic POMDPs", "comment": null, "summary": "Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.\n  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.\n  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684POMDP\u7c7b\u522b\u2014\u2014\u540e\u9a8c\u786e\u5b9a\u6027POMDP\uff0c\u8bc1\u660e\u4e86\u5bf9\u4e8e\u8fd9\u7c7b\u6a21\u578b\uff0c\u53ef\u8fbe\u76ee\u6807\u72b6\u6001\u7684\u6700\u5927\u6982\u7387\u53ef\u4ee5\u8fd1\u4f3c\u8ba1\u7b97\u5230\u4efb\u610f\u7cbe\u5ea6\uff0c\u7a81\u7834\u4e86\u4f20\u7edfPOMDP\u8ba1\u7b97\u96be\u5ea6\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edfPOMDP\u5728\u9a8c\u8bc1\u548c\u7efc\u5408\u95ee\u9898\u4e0a\u5b58\u5728\u4e0d\u53ef\u5224\u5b9a\u6216\u96be\u4ee5\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u7279\u522b\u662fMadani\u7b49\u4eba\u7684\u7ecf\u5178\u7ed3\u679c\u8868\u660e\u65e0\u6cd5\u8ba1\u7b97\u6216\u8fd1\u4f3c\u53ef\u8fbe\u76ee\u6807\u72b6\u6001\u7684\u6700\u5927\u6982\u7387\u3002\u8fd9\u4e0e\u5b8c\u5168\u53ef\u89c2\u6d4b\u7684MDP\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u540e\u8005\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8ba1\u7b97\u53ef\u8fbe\u6027\u503c\u3002", "method": "\u5f15\u5165\u540e\u9a8c\u786e\u5b9a\u6027POMDP\u8fd9\u4e00\u65b0\u7c7b\u522b\uff0c\u5b9a\u4e49\u4e3a\u4e0b\u4e00\u4e2a\u72b6\u6001\u53ef\u4ee5\u7531\u5f53\u524d\u72b6\u6001\u3001\u91c7\u53d6\u7684\u52a8\u4f5c\u548c\u63a5\u6536\u7684\u89c2\u6d4b\u552f\u4e00\u786e\u5b9a\u7684POMDP\u3002\u8fd9\u79cd\u6027\u8d28\u610f\u5473\u7740\u4e00\u65e6\u771f\u5b9e\u72b6\u6001\u5df2\u77e5\uff0c\u5b83\u5c06\u6c38\u8fdc\u4fdd\u6301\u5df2\u77e5\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u540e\u9a8c\u786e\u5b9a\u6027POMDP\uff0c\u53ef\u8fbe\u7ed9\u5b9a\u72b6\u6001\u96c6\u7684\u6700\u5927\u6982\u7387\u53ef\u4ee5\u8fd1\u4f3c\u5230\u4efb\u610f\u7cbe\u5ea6\u3002\u8fd9\u7c7b\u6a21\u578b\u5305\u542b\u4e86\u6240\u6709MDP\uff0c\u5e76\u6db5\u76d6\u4e86\u5982Tiger POMDP\u7b49\u7ecf\u5178\u975e\u5e73\u51e1\u4f8b\u5b50\uff0c\u6210\u4e3a\u5df2\u77e5\u6700\u5927\u7684\u53ef\u8fd1\u4f3c\u8ba1\u7b97\u53ef\u8fbe\u6027\u503c\u7684POMDP\u7c7b\u522b\u4e4b\u4e00\u3002", "conclusion": "\u540e\u9a8c\u786e\u5b9a\u6027POMDP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301POMDP\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u53ef\u8fbe\u6027\u503c\u7684\u53ef\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u4e3a\u987a\u5e8f\u51b3\u7b56\u5236\u5b9a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.08835", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08835", "abs": "https://arxiv.org/abs/2602.08835", "authors": ["Andr\u00e9s Holgado-S\u00e1nchez", "Peter Vamplew", "Richard Dazeley", "Sascha Ossowski", "Holger Billhardt"], "title": "Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning", "comment": "18 pages, 3 figures. To be published in proceedings of the 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). This is a full version that includes the supplementary material", "summary": "Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.\n  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u548c\u504f\u597d\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u5b66\u4e60\u793e\u4f1a\u667a\u80fd\u4f53\u7684\u4ef7\u503c\u5bf9\u9f50\u6a21\u578b\u548c\u4ef7\u503c\u7cfb\u7edf\u3002", "motivation": "\u4ef7\u503c\u611f\u77e5AI\u9700\u8981\u8bc6\u522b\u4eba\u7c7b\u4ef7\u503c\u89c2\u5e76\u9002\u5e94\u4e0d\u540c\u7528\u6237\u7684\u4ef7\u503c\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u64cd\u4f5c\u5316\u56f0\u96be\u3001\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u7279\u5f81\u3001\u7f3a\u4e4f\u57fa\u4e8e\u4ef7\u503c\u7684\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u5bf9\u591a\u6837\u5316\u7528\u6237\u504f\u597d\u7684\u9002\u5e94\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u805a\u7c7b\u548c\u504f\u597d\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u8054\u5408\u5b66\u4e60\u793e\u4f1a\u884d\u751f\u7684\u4ef7\u503c\u5bf9\u9f50\u6a21\u578b\u548c\u4ee3\u8868\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u4ef7\u503c\u7cfb\u7edf\u96c6\u5408\uff0c\u6bcf\u4e2a\u805a\u7c7b\u5305\u542b\u4ee3\u8868\u6210\u5458\u4ef7\u503c\u504f\u597d\u7684\u4ef7\u503c\u7cfb\u7edf\u548c\u53cd\u6620\u8be5\u4ef7\u503c\u7cfb\u7edf\u884c\u4e3a\u7684\u8fd1\u4f3c\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u5305\u542b\u4eba\u7c7b\u4ef7\u503c\u7684MDP\u4e0a\uff0c\u4e0e\u6700\u5148\u8fdb\u7684PbMORL\u7b97\u6cd5\u548c\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u793e\u4f1a\u667a\u80fd\u4f53\u7684\u4ef7\u503c\u5bf9\u9f50\u6a21\u578b\u548c\u4ef7\u503c\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4ef7\u503c\u64cd\u4f5c\u5316\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.07057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07057", "abs": "https://arxiv.org/abs/2602.07057", "authors": ["Di Mo", "Mingyang Sun", "Chengxiu Yin", "Runjia Tian", "Yanhong Wu", "Liyan Xu"], "title": "RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything", "comment": null, "summary": "Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.", "AI": {"tldr": "RECITYGEN\u662f\u4e00\u4e2a\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4ea4\u4e92\u5f0f\u8bed\u4e49\u5206\u5272\u7684\u5de5\u5177\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6587\u672c\u63d0\u793a\u4ea4\u4e92\u5f0f\u751f\u6210\u57ce\u5e02\u73af\u5883\u7684\u53d8\u4f53\u8857\u666f\u56fe\u50cf\uff0c\u7528\u4e8e\u53c2\u4e0e\u5f0f\u57ce\u5e02\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u81ea\u4e0a\u800c\u4e0b\u7684\u57ce\u5e02\u8bbe\u8ba1\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u516c\u4f17\u610f\u89c1\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u613f\u666f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002\u6570\u5b57\u5de5\u5177\u5982\u57ce\u5e02\u4fe1\u606f\u5efa\u6a21\u548c\u589e\u5f3a\u73b0\u5b9e\u5df2\u7ecf\u4f7f\u66f4\u591a\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u57ce\u5e02\u8bbe\u8ba1\u6210\u4e3a\u53ef\u80fd\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u8bbe\u8ba1\u751f\u6210\u7684\u95e8\u69db\uff0c\u4e3a\u53c2\u4e0e\u5f0f\u57ce\u5e02\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u591a\u673a\u4f1a\u3002", "method": "\u7ed3\u5408\u6700\u5148\u8fdb\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u4e0e\u4ea4\u4e92\u5f0f\u8bed\u4e49\u5206\u5272\u6280\u672f\uff0c\u5f00\u53d1\u4e86RECITYGEN\u5de5\u5177\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6587\u672c\u63d0\u793a\u4ea4\u4e92\u5f0f\u521b\u5efa\u57ce\u5e02\u73af\u5883\u7684\u53d8\u4f53\u8857\u666f\u56fe\u50cf\u3002", "result": "\u5728\u5317\u4eac\u7684\u4e00\u4e2a\u8bd5\u70b9\u9879\u76ee\u4e2d\uff0c\u7528\u6237\u4f7f\u7528RECITYGEN\u4e3a\u6b63\u5728\u8fdb\u884c\u7684\u57ce\u5e02\u66f4\u65b0\u9879\u76ee\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002\u5c3d\u7ba1\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u4f46RECITYGEN\u663e\u793a\u51fa\u4e0e\u516c\u4f17\u504f\u597d\u9ad8\u5ea6\u4e00\u81f4\u7684\u663e\u8457\u6f5c\u529b\u3002", "conclusion": "RECITYGEN\u8868\u660e\u57ce\u5e02\u8bbe\u8ba1\u65b9\u6cd5\u6b63\u5728\u5411\u66f4\u52a0\u52a8\u6001\u548c\u5305\u5bb9\u7684\u65b9\u5411\u8f6c\u53d8\uff0c\u4e3a\u53c2\u4e0e\u5f0f\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2602.07491", "categories": ["cs.AI", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.soft", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07491", "abs": "https://arxiv.org/abs/2602.07491", "authors": ["Isabella A. Stewart", "Tarjei Paule Hage", "Yu-Chuan Hsu", "Markus J. Buehler"], "title": "GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design", "comment": null, "summary": "Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bfb\u627ePFAS\uff08\u5168\u6c1f\u548c\u591a\u6c1f\u70f7\u57fa\u7269\u8d28\uff09\u7684\u53ef\u6301\u7eed\u66ff\u4ee3\u54c1\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4e13\u4e1a\u5316\u548c\u5173\u7cfb\u63a8\u7406\u6269\u5c55\u6750\u6599\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u5728\u6750\u6599\u79d1\u5b66\u4e2d\uff0c\u521b\u65b0\u9700\u8981\u6574\u5408\u4ece\u5206\u5b50\u5316\u5b66\u5230\u673a\u68b0\u6027\u80fd\u7684\u5404\u79cd\u6982\u5ff5\uff0c\u4f46\u4eba\u7c7b\u6216\u5355\u667a\u80fd\u4f53LLM\u96be\u4ee5\u5e94\u5bf9\u4fe1\u606f\u6d2a\u6d41\uff0c\u4e14\u540e\u8005\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002PFAS\u5316\u5b66\u7269\u8d28\u9762\u4e34\u4e25\u683c\u76d1\u7ba1\uff0c\u6025\u9700\u5bfb\u627e\u53ef\u6301\u7eed\u66ff\u4ee3\u54c1\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u95ee\u9898\u5206\u89e3\u3001\u8bc1\u636e\u68c0\u7d22\u3001\u8bbe\u8ba1\u53c2\u6570\u63d0\u53d6\u548c\u56fe\u904d\u5386\u7b49\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5b9a\u5236\u56fe\u904d\u5386\u7b56\u7565\u5728\u5229\u7528\u6027\u641c\u7d22\u548c\u63a2\u7d22\u6027\u641c\u7d22\u4e4b\u95f4\u5207\u6362\u3002", "result": "\u6d88\u878d\u7814\u7a76\u8868\u660e\u5b8c\u6574\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\u4f18\u4e8e\u5355\u6b21\u63d0\u793a\uff0c\u7cfb\u7edf\u80fd\u591f\u53d1\u73b0\u4e0d\u540c\u77e5\u8bc6\u9886\u57df\u4e4b\u95f4\u7684\u6f5c\u5728\u8054\u7cfb\u3002\u4ee5\u751f\u7269\u533b\u5b66\u5bfc\u7ba1\u4e3a\u4f8b\uff0c\u6846\u67b6\u751f\u6210\u4e86\u5e73\u8861\u6469\u64e6\u5b66\u6027\u80fd\u3001\u70ed\u7a33\u5b9a\u6027\u3001\u5316\u5b66\u6297\u6027\u548c\u751f\u7269\u76f8\u5bb9\u6027\u7684\u53ef\u6301\u7eedPFAS-free\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u7684\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u6750\u6599\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5c55\u793a\u4e86\u591a\u4e2a\u521d\u59cb\u8bbe\u8ba1\u5019\u9009\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u5206\u5e03\u5f0f\u4e13\u4e1a\u5316\u548c\u5173\u7cfb\u63a8\u7406\u5728\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.08886", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08886", "abs": "https://arxiv.org/abs/2602.08886", "authors": ["Vasileios Karlis", "Ezgi Y\u0131ld\u0131r\u0131m", "David Vos", "Maarten de Rijke"], "title": "Contrastive Learning for Diversity-Aware Product Recommendations in Retail", "comment": null, "summary": "Recommender systems often struggle with long-tail distributions and limited item catalog exposure, where a small subset of popular items dominates recommendations. This challenge is especially critical in large-scale online retail settings with extensive and diverse product assortments. This paper introduces an approach to enhance catalog coverage without compromising recommendation quality in the existing digital recommendation pipeline at IKEA Retail. Drawing inspiration from recent advances in negative sampling to address popularity bias, we integrate contrastive learning with carefully selected negative samples. Through offline and online evaluations, we demonstrate that our method improves catalog coverage, ensuring a more diverse set of recommendations yet preserving strong recommendation performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8d1f\u91c7\u6837\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u5546\u54c1\u76ee\u5f55\u8986\u76d6\u7387\u7684\u65b9\u6cd5\uff0c\u5728\u4e0d\u727a\u7272\u63a8\u8350\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u589e\u52a0\u957f\u5c3e\u5546\u54c1\u7684\u66dd\u5149\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u9762\u4e34\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5c11\u6570\u70ed\u95e8\u5546\u54c1\u4e3b\u5bfc\u63a8\u8350\u7ed3\u679c\uff0c\u5bfc\u81f4\u5546\u54c1\u76ee\u5f55\u8986\u76d6\u7387\u4f4e\u3002\u5728IKEA\u8fd9\u6837\u62e5\u6709\u5927\u91cf\u591a\u6837\u5316\u4ea7\u54c1\u7684\u5927\u578b\u5728\u7ebf\u96f6\u552e\u73af\u5883\u4e2d\uff0c\u8fd9\u4e2a\u95ee\u9898\u5c24\u4e3a\u7a81\u51fa\uff0c\u9700\u8981\u5728\u4e0d\u5f71\u54cd\u63a8\u8350\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u5546\u54c1\u66dd\u5149\u591a\u6837\u6027\u3002", "method": "\u53d7\u8d1f\u91c7\u6837\u89e3\u51b3\u6d41\u884c\u5ea6\u504f\u5dee\u7684\u542f\u53d1\uff0c\u5c06\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u7cbe\u5fc3\u9009\u62e9\u7684\u8d1f\u6837\u672c\u76f8\u7ed3\u5408\uff0c\u96c6\u6210\u5230\u73b0\u6709\u7684\u6570\u5b57\u63a8\u8350\u6d41\u7a0b\u4e2d\u3002\u901a\u8fc7\u8d1f\u91c7\u6837\u7b56\u7565\u6765\u5e73\u8861\u70ed\u95e8\u5546\u54c1\u548c\u957f\u5c3e\u5546\u54c1\u7684\u63a8\u8350\u6743\u91cd\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5546\u54c1\u76ee\u5f55\u8986\u76d6\u7387\uff0c\u786e\u4fdd\u63a8\u8350\u7ed3\u679c\u66f4\u52a0\u591a\u6837\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u63a8\u8350\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5546\u54c1\u76ee\u5f55\u8986\u76d6\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u5728\u7ebf\u96f6\u552e\u73af\u5883\u4e2d\u7684\u63a8\u8350\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07058", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07058", "abs": "https://arxiv.org/abs/2602.07058", "authors": ["Carolina R. Kelsch", "Leonardo S. B. Pereira", "Natnael Mola", "Luis H. Arribas", "Juan C. S. M. Avedillo"], "title": "FADE: Selective Forgetting via Sparse LoRA and Self-Distillation", "comment": null, "summary": "Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.", "AI": {"tldr": "FADE\u662f\u4e00\u79cd\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u4e24\u9636\u6bb5\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5b9a\u4f4d\u548c\u81ea\u84b8\u998f\u5b9e\u73b0\u9ad8\u6548\u6982\u5ff5\u64e6\u9664\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\u65e8\u5728\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u6570\u636e\u6216\u6982\u5ff5\u7684\u5f71\u54cd\uff0c\u4ee5\u6ee1\u8db3\u6570\u636e\u4fdd\u62a4\u6cd5\u89c4\u548c\u8d1f\u8d23\u4efbAI\u5b9e\u8df5\u7684\u9700\u6c42\u3002\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u9057\u5fd8\u65b9\u6cd5\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u96be\u4ee5\u5e73\u8861\u6709\u6548\u9057\u5fd8\u4e0e\u4fdd\u7559\u65e0\u5173\u6982\u5ff5\u7684\u6311\u6218\u3002", "method": "FADE\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u663e\u8457\u6027\u8bc6\u522b\u4e0e\u9057\u5fd8\u96c6\u6700\u76f8\u5173\u7684\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u7a00\u758fLoRA\u9002\u914d\u5668\u7ea6\u675f\u66f4\u65b0\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u81ea\u84b8\u998f\u76ee\u6807\uff0c\u7528\u7528\u6237\u5b9a\u4e49\u7684\u66ff\u4ee3\u6982\u5ff5\u8986\u76d6\u88ab\u9057\u5fd8\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u7559\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u7684\u884c\u4e3a\u3002", "result": "\u5728UnlearnCanvas\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53caImagenette\u3001Labeled Faces in the Wild\u3001AtharvaTaras Dog Breeds Dataset\u548cSUN Attributes\u6570\u636e\u96c6\u4e0a\u7684\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cFADE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u6027\u80fd\uff0c\u5728\u9057\u5fd8-\u4fdd\u7559\u6743\u8861\u65b9\u9762\u5177\u6709\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "conclusion": "FADE\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6982\u5ff5\u64e6\u9664\u548c\u8de8\u591a\u4e2a\u9886\u57df\u7684\u9ad8\u4fdd\u7559\u6027\uff0c\u9002\u914d\u5668\u5185\u5b58\u6548\u7387\u9ad8\u3001\u53ef\u9006\uff0c\u53ef\u5728\u8fd0\u884c\u65f6\u5408\u5e76\u6216\u79fb\u9664\uff0c\u4f7f\u5176\u6210\u4e3a\u6269\u6563\u57fa\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u9009\u62e9\u6027\u9057\u5fd8\u7684\u5408\u9002\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08964", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.08964", "abs": "https://arxiv.org/abs/2602.08964", "authors": ["Raghu Arghal", "Fade Chen", "Niall Dalton", "Evgenii Kortukov", "Calum McNamara", "Angelos Nalmpantis", "Moksh Nirvaan", "Gabriele Sarti", "Mario Giulianelli"], "title": "A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents", "comment": null, "summary": "Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u667a\u80fd\u4f53\u76ee\u6807\u5bfc\u5411\u6027\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u884c\u4e3a\u8bc4\u4f30\u548c\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u901a\u8fc7LLM\u667a\u80fd\u4f53\u57282D\u7f51\u683c\u4e16\u754c\u5bfc\u822a\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0\u667a\u80fd\u4f53\u5185\u90e8\u975e\u7ebf\u6027\u7f16\u7801\u73af\u5883\u7a7a\u95f4\u5730\u56fe\uff0c\u5176\u884c\u52a8\u4e0e\u5185\u90e8\u8868\u793a\u4e00\u81f4\uff0c\u63a8\u7406\u8fc7\u7a0b\u4f1a\u91cd\u7ec4\u8fd9\u4e9b\u8868\u793a\u3002", "motivation": "\u7406\u89e3\u667a\u80fd\u4f53\u7684\u76ee\u6807\u6709\u52a9\u4e8e\u89e3\u91ca\u548c\u9884\u6d4b\u5176\u884c\u4e3a\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u53ef\u9760\u5730\u5c06\u76ee\u6807\u5f52\u56e0\u4e8e\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6210\u719f\u65b9\u6cd5\u3002\u9700\u8981\u5efa\u7acb\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\u6765\u66f4\u597d\u5730\u7406\u89e3\u667a\u80fd\u4f53\u5982\u4f55\u8868\u793a\u548c\u8ffd\u6c42\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u76ee\u6807\u5bfc\u5411\u6027\u7684\u6846\u67b6\uff0c\u6574\u5408\u884c\u4e3a\u8bc4\u4f30\u548c\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684\u5185\u90e8\u8868\u793a\u5206\u6790\u3002\u4ee5LLM\u667a\u80fd\u4f53\u57282D\u7f51\u683c\u4e16\u754c\u5bfc\u822a\u4e3a\u6848\u4f8b\u7814\u7a76\uff1a1\uff09\u884c\u4e3a\u8bc4\u4f30\uff1a\u5728\u4e0d\u540c\u7f51\u683c\u5927\u5c0f\u3001\u969c\u788d\u7269\u5bc6\u5ea6\u548c\u76ee\u6807\u7ed3\u6784\u4e0b\u8bc4\u4f30\u667a\u80fd\u4f53\u76f8\u5bf9\u4e8e\u6700\u4f18\u7b56\u7565\u7684\u8868\u73b0\uff1b2\uff09\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff1a\u4f7f\u7528\u63a2\u6d4b\u65b9\u6cd5\u89e3\u7801\u667a\u80fd\u4f53\u5bf9\u73af\u5883\u72b6\u6001\u548c\u591a\u6b65\u884c\u52a8\u8ba1\u5212\u7684\u5185\u90e8\u8868\u793a\u3002", "result": "1\uff09\u884c\u4e3a\u8868\u73b0\uff1a\u667a\u80fd\u4f53\u6027\u80fd\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u800c\u63d0\u5347\uff0c\u540c\u65f6\u5bf9\u96be\u5ea6\u4fdd\u6301\u4e0d\u53d8\u7684\u53d8\u6362\u548c\u590d\u6742\u76ee\u6807\u7ed3\u6784\u4fdd\u6301\u9c81\u68d2\u6027\uff1b2\uff09\u5185\u90e8\u8868\u793a\uff1aLLM\u667a\u80fd\u4f53\u975e\u7ebf\u6027\u7f16\u7801\u73af\u5883\u7684\u7c97\u7565\u7a7a\u95f4\u5730\u56fe\uff0c\u4fdd\u7559\u5173\u4e8e\u81ea\u8eab\u4f4d\u7f6e\u548c\u76ee\u6807\u4f4d\u7f6e\u7684\u4efb\u52a1\u76f8\u5173\u7ebf\u7d22\uff1b3\uff09\u884c\u52a8\u4e00\u81f4\u6027\uff1a\u667a\u80fd\u4f53\u884c\u52a8\u4e0e\u8fd9\u4e9b\u5185\u90e8\u8868\u793a\u57fa\u672c\u4e00\u81f4\uff1b4\uff09\u63a8\u7406\u91cd\u7ec4\uff1a\u63a8\u7406\u8fc7\u7a0b\u4f1a\u91cd\u7ec4\u5185\u90e8\u8868\u793a\uff0c\u4ece\u66f4\u5e7f\u6cdb\u7684\u73af\u5883\u7ed3\u6784\u7ebf\u7d22\u8f6c\u5411\u652f\u6301\u5373\u65f6\u884c\u52a8\u9009\u62e9\u7684\u4fe1\u606f\u3002", "conclusion": "\u4ec5\u9760\u884c\u4e3a\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u5145\u5206\u7406\u89e3\u667a\u80fd\u4f53\u5982\u4f55\u8868\u793a\u548c\u8ffd\u6c42\u76ee\u6807\uff0c\u9700\u8981\u7ed3\u5408\u5185\u7701\u5f0f\u68c0\u67e5\uff08\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff09\u6765\u5168\u9762\u8868\u5f81\u667a\u80fd\u4f53\u7684\u76ee\u6807\u5bfc\u5411\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u667a\u80fd\u4f53\u76ee\u6807\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.08896", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08896", "abs": "https://arxiv.org/abs/2602.08896", "authors": ["Yehua Huang", "Penglei Sun", "Zebin Chen", "Zhenheng Tang", "Xiaowen Chu"], "title": "OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation", "comment": null, "summary": "Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-world editorial workflows. To bridge this gap, we present OmniReview, a comprehensive dataset constructed by integrating multi-source academic platforms encompassing comprehensive scholarly profiles through the disambiguation pipeline, yielding 202, 756 verified review records. Based on this data, we introduce a three-tier hierarchical evaluaion framework to assess recommendations from recall to precise expert identification. From the method perspective, existing embedding-based approaches suffer from the information bottleneck of semantic compression and limited interpretability. To resolve these method limitations, we propose Profiling Scholars with Multi-gate Mixture-of-Experts (Pro-MMoE), a novel framework that synergizes Large Language Models (LLMs) with Multi-task Learning. Specifically, it utilizes LLM-generated semantic profiles to preserve fine-grained expertise nuances and interpretability, while employing a Task-Adaptive MMoE architecture to dynamically balance conflicting evaluation goals. Comprehensive experiments demonstrate that Pro-MMoE achieves state-of-the-art performance across six of seven metrics, establishing a new benchmark for realistic reviewer recommendation.", "AI": {"tldr": "OmniReview\u6570\u636e\u96c6\u548cPro-MMoE\u6846\u67b6\u89e3\u51b3\u4e86\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u6570\u636e\u548c\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u9a8c\u8bc1\u6570\u636e\u96c6\u548c\u7ed3\u5408LLM\u4e0e\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5ba1\u7a3f\u4eba\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u9762\u4e34\u6570\u636e\u548c\u65b9\u6cd5\u7684\u53cc\u91cd\u6311\u6218\uff1a\u6570\u636e\u65b9\u9762\u7f3a\u4e4f\u5927\u89c4\u6a21\u9a8c\u8bc1\u57fa\u51c6\u548c\u8fc7\u4e8e\u7b80\u5316\u7684\u8bc4\u4f30\u6307\u6807\uff1b\u65b9\u6cd5\u65b9\u9762\u73b0\u6709\u5d4c\u5165\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u538b\u7f29\u7684\u4fe1\u606f\u74f6\u9888\u548c\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faOmniReview\u6570\u636e\u96c6\uff08202,756\u6761\u9a8c\u8bc1\u8bb0\u5f55\uff09\u548c\u4e09\u5c42\u6b21\u8bc4\u4f30\u6846\u67b6\uff1b\u63d0\u51faPro-MMoE\u6846\u67b6\uff0c\u7ed3\u5408LLM\u751f\u6210\u8bed\u4e49\u6863\u6848\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e76\u91c7\u7528\u4efb\u52a1\u81ea\u9002\u5e94MMoE\u67b6\u6784\u52a8\u6001\u5e73\u8861\u51b2\u7a81\u7684\u8bc4\u4f30\u76ee\u6807\u3002", "result": "Pro-MMoE\u5728\u4e03\u4e2a\u8bc4\u4f30\u6307\u6807\u4e2d\u7684\u516d\u4e2a\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u73b0\u5b9e\u7684\u5ba1\u7a3f\u4eba\u63a8\u8350\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7efc\u5408\u6570\u636e\u96c6\u548c\u521b\u65b0\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5ba1\u7a3f\u4eba\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07216", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07216", "abs": "https://arxiv.org/abs/2602.07216", "authors": ["Reuben Narad", "L\u00e9onard Boussioux", "Michael Wagner"], "title": "Probing Neural TSP Representations for Prescriptive Decision Support", "comment": "Submitted to ICML 2026", "summary": "The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\\% top-1 accuracy (vs. 58\\% baseline) for the best-node-removal task, and 73\\% top-1 accuracy (vs. 67\\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\uff08NCO\uff09\u6a21\u578b\u5728\u65c5\u884c\u5546\u95ee\u9898\uff08TSP\uff09\u8bad\u7ec3\u540e\uff0c\u5176\u5185\u90e8\u8868\u5f81\u662f\u5426\u80fd\u591f\u8fc1\u79fb\u5230\u5176\u4ed6\u4f18\u5316\u76f8\u5173\u4efb\u52a1\uff0c\u5982\u8282\u70b9\u79fb\u9664\u654f\u611f\u6027\u548c\u8fb9\u7981\u6b62\u654f\u611f\u6027\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u8bad\u7ec3\u597d\u7684TSP\u6c42\u89e3\u5668\u662f\u5426\u80fd\u591f\u5b66\u4e60\u5230\u53ef\u8fc1\u79fb\u7684\u5185\u90e8\u8868\u5f81\uff0c\u7528\u4e8e\u5176\u4ed6\u4e0e\u4f18\u5316\u76f8\u5173\u7684\u4e0b\u6e38\u4efb\u52a1\uff0c\u7c7b\u4f3c\u4e8e\u5176\u4ed6\u9886\u57df\u7684\u8fc1\u79fb\u5b66\u4e60\u3002\u8fd9\u6709\u52a9\u4e8e\u8bc4\u4f30NCO\u6a21\u578b\u5728\u89e3\u51b3\u5b9e\u9645\u7269\u6d41\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u652f\u6301\u95ee\u9898\u65f6\u7684\u901a\u7528\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u8bad\u7ec3\u591a\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684TSP\u7b56\u7565\u6a21\u578b\uff1b2\uff09\u6536\u96c6\u8fd9\u4e9b\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\u72b6\u6001\uff1b3\uff09\u5728\u8282\u70b9/\u8fb9\u5d4c\u5165\u4e0a\u8bad\u7ec3\u63a2\u9488\uff0c\u7528\u4e8e\u4e24\u4e2aNP-hard\u7684\u4e0b\u6e38\u4efb\u52a1\uff1a\u8282\u70b9\u79fb\u9664\u654f\u611f\u6027\uff08\u8bc6\u522b\u6700\u5177\u5f71\u54cd\u529b\u7684\u8282\u70b9\uff09\u548c\u8fb9\u7981\u6b62\u654f\u611f\u6027\uff08\u8bc6\u522b\u6700\u5173\u952e\u7684\u8fb9\uff09\u3002", "result": "\u5728\u6b27\u51e0\u91cc\u5f97TSP100\u8bad\u7ec3\u7684\u6a21\u578b\u4e0a\uff0c\u4e24\u4e2a\u4efb\u52a1\u7684\u63a2\u9488\u6027\u80fd\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u5f53\u3002\u5c06\u63a2\u9488\u4fe1\u53f7\u4e0e\u51e0\u4f55\u7279\u5f81\u96c6\u6210\u540e\uff0c\u6027\u80fd\u8d85\u8fc7\u6700\u5f3a\u57fa\u7ebf\uff1a\u6700\u4f73\u8282\u70b9\u79fb\u9664\u4efb\u52a1\u8fbe\u523065%\u7684top-1\u51c6\u786e\u7387\uff08\u57fa\u7ebf\u4e3a58%\uff09\uff0c\u6700\u5dee\u8fb9\u8bc6\u522b\u4efb\u52a1\u8fbe\u523073%\u7684top-1\u51c6\u786e\u7387\uff08\u57fa\u7ebf\u4e3a67%\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u795e\u7ecfTSP\u6c42\u89e3\u5668\u4f5c\u4e3a\u53ef\u8fc1\u79fb\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u8d85\u8d8a\u8def\u5f84\u6784\u5efa\u7684\u9884\u6d4b\u6027\u51b3\u7b56\u652f\u6301\u4efb\u52a1\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u8fc1\u79fb\u51c6\u786e\u7387\u968f\u6c42\u89e3\u5668\u8d28\u91cf\u548c\u6a21\u578b\u89c4\u6a21\u63d0\u5347\u800c\u589e\u52a0\uff0c\u8868\u660e\u8bad\u7ec3\u66f4\u5f3a\u7684NCO\u6c42\u89e3\u5668\u4e5f\u80fd\u4ea7\u751f\u66f4\u6709\u7528\u7684\u4e0b\u6e38\u7f16\u7801\u5668\u3002"}}
{"id": "2602.07062", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07062", "abs": "https://arxiv.org/abs/2602.07062", "authors": ["Daniil Storonkin", "Ilia Dziub", "Maksim Golyadkin", "Ilya Makarov"], "title": "From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal", "comment": "AAAI 2026 Workshop on Addressing Challenges and Opportunities in Human-Centric Manufacturing", "summary": "Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u6790\u8bc4\u4f30\u5e9f\u94a2\u4e2d\u7684\u975e\u91d1\u5c5e\u5939\u6742\u7269\u6c61\u67d3\u7a0b\u5ea6\uff0c\u5e76\u5206\u7c7b\u5e9f\u94a2\u7c7b\u578b\uff0c\u7528\u4e8e\u94a2\u94c1\u751f\u4ea7\u4e2d\u7684\u8d28\u91cf\u63a7\u5236", "motivation": "\u76ee\u524d\u5e9f\u94a2\u8d28\u91cf\u4e3b\u8981\u901a\u8fc7\u4eba\u5de5\u76ee\u89c6\u68c0\u67e5\u975e\u91d1\u5c5e\u5939\u6742\u7269\u6c61\u67d3\u7a0b\u5ea6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e3b\u89c2\u6027\u5f3a\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff08\u7c89\u5c18\u548c\u79fb\u52a8\u8bbe\u5907\uff09\u3002\u9700\u8981\u5ba2\u89c2\u3001\u5b89\u5168\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5", "method": "\u5c06\u6c61\u67d3\u8bc4\u4f30\u6784\u5efa\u4e3a\u8f66\u53a2\u7ea7\u522b\u7684\u56de\u5f52\u4efb\u52a1\uff0c\u91c7\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u5904\u7406\u5e8f\u5217\u6570\u636e\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u540c\u65f6\u8fdb\u884c\u6c61\u67d3\u8bc4\u4f30\u548c\u5e9f\u94a2\u5206\u7c7b\u3002\u7cfb\u7edf\u5305\u62ec\u78c1\u94c1/\u8f66\u53a2\u68c0\u6d4b\u3001\u7248\u672c\u5316\u63a8\u7406\u670d\u52a1\u3001\u64cd\u4f5c\u5458\u5ba1\u67e5\u548c\u4e3b\u52a8\u5b66\u4e60\u5faa\u73af", "result": "\u6700\u4f73\u7ed3\u679c\uff1aMIL\u65b9\u6cd5\u8fbe\u5230MAE 0.27\u548cR\u00b2 0.83\uff1bMTL\u8bbe\u7f6e\u8fbe\u5230MAE 0.36\uff0c\u5e9f\u94a2\u5206\u7c7bF1\u5206\u65700.79\u3002\u7cfb\u7edf\u5df2\u96c6\u6210\u5230\u9a8c\u6536\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u5904\u7406", "conclusion": "\u8be5\u8ba1\u7b97\u673a\u89c6\u89c9\u7ba1\u9053\u51cf\u5c11\u4e86\u4e3b\u89c2\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u4e86\u4eba\u5458\u5b89\u5168\u6027\uff0c\u80fd\u591f\u96c6\u6210\u5230\u5e9f\u94a2\u9a8c\u6536\u548c\u7194\u70bc\u8ba1\u5212\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb"}}
{"id": "2602.07543", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2602.07543", "abs": "https://arxiv.org/abs/2602.07543", "authors": ["Heewoong Noh", "Gyoung S. Na", "Namkyeong Lee", "Chanyoung Park"], "title": "MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning", "comment": null, "summary": "Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.", "AI": {"tldr": "MSP-LLM\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u6846\u67b6\uff0c\u5c06\u6750\u6599\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u524d\u9a71\u4f53\u9884\u6d4b\u548c\u5408\u6210\u64cd\u4f5c\u9884\u6d4b\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u6563\u6750\u6599\u7c7b\u522b\u4f5c\u4e3a\u4e2d\u95f4\u51b3\u7b56\u53d8\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6750\u6599\u5408\u6210\u89c4\u5212\u7684\u6027\u80fd\u3002", "motivation": "\u6750\u6599\u5408\u6210\u89c4\u5212\u662fAI\u9a71\u52a8\u6750\u6599\u53d1\u73b0\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u89e3\u51b3\u5b64\u7acb\u5b50\u4efb\u52a1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u524d\u9a71\u4f53\u9009\u62e9\u548c\u5408\u6210\u64cd\u4f5c\u5e8f\u5217\u8bbe\u8ba1\u7684\u7efc\u5408\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMSP-LLM\u6846\u67b6\uff0c\u5c06\u6750\u6599\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u524d\u9a71\u4f53\u9884\u6d4b\u548c\u5408\u6210\u64cd\u4f5c\u9884\u6d4b\u4e24\u4e2a\u5b50\u95ee\u9898\u3002\u5f15\u5165\u79bb\u6563\u6750\u6599\u7c7b\u522b\u4f5c\u4e3a\u4e2d\u95f4\u51b3\u7b56\u53d8\u91cf\uff0c\u6784\u5efa\u5316\u5b66\u4e00\u81f4\u7684\u51b3\u7b56\u94fe\u3002\u5728\u5408\u6210\u64cd\u4f5c\u9884\u6d4b\u4e2d\uff0c\u91c7\u7528\u5206\u5c42\u524d\u9a71\u4f53\u7c7b\u578b\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u4f7f\u7528\u663e\u5f0f\u6761\u4ef6\u7b56\u7565\u5728\u81ea\u56de\u5f52\u89e3\u7801\u72b6\u6001\u4e2d\u4fdd\u7559\u524d\u9a71\u4f53\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMSP-LLM\u5728\u524d\u9a71\u4f53\u9884\u6d4b\u3001\u5408\u6210\u64cd\u4f5c\u9884\u6d4b\u4ee5\u53ca\u5b8c\u6574\u7684\u6750\u6599\u5408\u6210\u89c4\u5212\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u6750\u6599\u53d1\u73b0\u4e2d\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MSP-LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u6709\u6548\u7684\u6750\u6599\u5408\u6210\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u52a0\u901f\u5b9e\u9645\u6750\u6599\u53d1\u73b0\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u957f\u671f\u5b58\u5728\u7684\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2602.08917", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08917", "abs": "https://arxiv.org/abs/2602.08917", "authors": ["Minghan Li", "Ercong Nie", "Siqi Zhao", "Tongna Chen", "Huiping Huang", "Guodong Zhou"], "title": "Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion", "comment": null, "summary": "Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u3001\u9886\u57df\u81ea\u9002\u5e94\u7684\u67e5\u8be2\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7BM25-MonoT5\u7ba1\u9053\u6784\u5efa\u9886\u57df\u5185\u793a\u4f8b\u6c60\uff0c\u4f7f\u7528\u65e0\u76d1\u7763\u805a\u7c7b\u7b56\u7565\u9009\u62e9\u591a\u6837\u5316\u6f14\u793a\uff0c\u5e76\u5f15\u5165\u53ccLLM\u96c6\u6210\u65b9\u6cd5\u751f\u6210\u66f4\u4f18\u7684\u67e5\u8be2\u6269\u5c55\u3002", "motivation": "\u4f20\u7edf\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u63d0\u793a\u3001\u4eba\u5de5\u9009\u62e9\u7684\u793a\u4f8b\u6216\u5355\u4e00LLM\uff0c\u5bfc\u81f4\u6269\u5c55\u6027\u5dee\u4e14\u5bf9\u9886\u57df\u53d8\u5316\u654f\u611f\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u4f7f\u7528BM25-MonoT5\u7ba1\u9053\u4ece\u4f2a\u76f8\u5173\u6587\u6863\u4e2d\u6784\u5efa\u9886\u57df\u5185\u793a\u4f8b\u6c60\uff1b2) \u91c7\u7528\u65e0\u76d1\u7763\u805a\u7c7b\u7b56\u7565\u9009\u62e9\u591a\u6837\u5316\u6f14\u793a\uff1b3) \u5f15\u5165\u53ccLLM\u96c6\u6210\u65b9\u6cd5\uff1a\u4e24\u4e2a\u5f02\u6784LLM\u72ec\u7acb\u751f\u6210\u6269\u5c55\uff0c\u7b2c\u4e09\u4e2aLLM\u8fdb\u884c\u6574\u5408\u4f18\u5316\u3002", "result": "\u5728TREC DL20\u3001DBPedia\u548cSciFact\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u76f8\u6bd4BM25\u3001Rocchio\u3001\u96f6\u6837\u672c\u548c\u56fa\u5b9a\u5c11\u6837\u672c\u57fa\u7ebf\uff0c\u53d6\u5f97\u4e86\u7a33\u5b9a\u4e14\u7edf\u8ba1\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u793a\u4f8b\u9009\u62e9\u548c\u591aLLM\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u67e5\u8be2\u6269\u5c55\u63d0\u4f9b\u4e86\u65e0\u9700\u6807\u6ce8\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07218", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07218", "abs": "https://arxiv.org/abs/2602.07218", "authors": ["Gagik Magakyan", "Amirhossein Reisizadeh", "Chanwoo Park", "Pablo A. Parrilo", "Asuman Ozdaglar"], "title": "Collaborative and Efficient Fine-tuning: Leveraging Task Similarity", "comment": null, "summary": "Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.", "AI": {"tldr": "CoLoRA\uff1a\u4e00\u79cd\u5229\u7528\u4efb\u52a1\u76f8\u4f3c\u6027\u8fdb\u884c\u534f\u4f5c\u5f0f\u4f4e\u79e9\u9002\u5e94\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u9002\u914d\u5668\u548c\u4e2a\u4eba\u9002\u914d\u5668\u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6027\u80fd", "motivation": "\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u4e2d\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u591a\u4e2a\u4e0b\u6e38\u7528\u6237\u4e4b\u95f4\u7684\u4efb\u52a1\u76f8\u4f3c\u6027\u6765\u63d0\u5347\u6709\u6548\u5fae\u8c03\u6570\u636e\u89c4\u6a21", "method": "\u63d0\u51fa\u534f\u4f5c\u5f0f\u4f4e\u79e9\u9002\u5e94\uff08CoLoRA\uff09\uff0c\u5305\u542b\u4e00\u4e2a\u5171\u4eab\u9002\u914d\u5668\u6355\u6349\u6240\u6709\u4efb\u52a1\u7684\u5e95\u5c42\u76f8\u4f3c\u6027\uff0c\u4ee5\u53ca\u9488\u5bf9\u7528\u6237\u7279\u5b9a\u4efb\u52a1\u7684\u4e2a\u6027\u5316\u9002\u914d\u5668", "result": "\u5728\u5f02\u6784\u7ebf\u6027\u56de\u5f52\u4e0a\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u5b9e\u9a8c\u4e2d\u8bc1\u660e\uff0c\u5f53\u4e0e\u76f8\u4f3c\u4efb\u52a1\u4e00\u8d77\u8bad\u7ec3\u65f6\uff0c\u4e2a\u4f53\u6027\u80fd\u663e\u8457\u63d0\u5347", "conclusion": "CoLoRA\u901a\u8fc7\u5229\u7528\u4efb\u52a1\u76f8\u4f3c\u6027\u8fdb\u884c\u534f\u4f5c\u5f0f\u5fae\u8c03\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u80fd"}}
{"id": "2602.07695", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07695", "abs": "https://arxiv.org/abs/2602.07695", "authors": ["Congcong Hu", "Yuang Shi", "Fan Huang", "Yang Xiang", "Zhou Ye", "Ming Jin", "Shiyu Wang"], "title": "EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge", "comment": null, "summary": "Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.", "AI": {"tldr": "EventCast\u662f\u4e00\u4e2a\u5c06\u672a\u6765\u4e8b\u4ef6\u77e5\u8bc6\u6574\u5408\u5230\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u89e3\u51b3\u7535\u5546\u5728\u9ad8\u5f71\u54cd\u65f6\u671f\uff08\u5982\u95ea\u8d2d\u3001\u8282\u5047\u65e5\u6d3b\u52a8\uff09\u7684\u9700\u6c42\u9884\u6d4b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u7cfb\u7edf\u5728\u9ad8\u5f71\u54cd\u65f6\u671f\uff08\u5982\u95ea\u8d2d\u3001\u8282\u5047\u65e5\u6d3b\u52a8\u3001\u653f\u7b56\u5e72\u9884\uff09\u7ecf\u5e38\u5931\u6548\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u65f6\u671f\u9700\u6c42\u6a21\u5f0f\u4f1a\u53d1\u751f\u7a81\u7136\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u53d8\u5316\u3002\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u672a\u6765\u5e72\u9884\uff0c\u8981\u4e48\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u503c\u9884\u6d4b\uff0c\u90fd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "EventCast\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e8b\u4ef6\u9a71\u52a8\u63a8\u7406\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u4e1a\u52a1\u6570\u636e\uff08\u5982\u6d3b\u52a8\u3001\u8282\u5047\u65e5\u5b89\u6392\u3001\u5356\u5bb6\u6fc0\u52b1\uff09\u8f6c\u6362\u4e3a\u53ef\u89e3\u91ca\u7684\u6587\u672c\u6458\u8981\uff0c\u7136\u540e\u901a\u8fc7\u53cc\u5854\u67b6\u6784\u5c06\u8fd9\u4e9b\u6458\u8981\u4e0e\u5386\u53f2\u9700\u6c42\u7279\u5f81\u878d\u5408\uff0c\u5b9e\u73b0\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u3002", "result": "\u57284\u4e2a\u56fd\u5bb6160\u4e2a\u5730\u533a10\u4e2a\u6708\u7684\u771f\u5b9e\u7535\u5546\u573a\u666f\u4e2d\uff0cEventCast\u76f8\u6bd4\u65e0\u4e8b\u4ef6\u77e5\u8bc6\u7684\u53d8\u4f53\u5728MAE\u548cMSE\u4e0a\u5206\u522b\u63d0\u5347\u4e8686.9%\u548c97.7%\uff1b\u76f8\u6bd4\u6700\u4f73\u5de5\u4e1a\u57fa\u7ebf\uff0c\u5728\u4e8b\u4ef6\u9a71\u52a8\u65f6\u671fMAE\u964d\u4f4e\u4e8657.0%\uff0cMSE\u964d\u4f4e\u4e8683.3%\u3002", "conclusion": "EventCast\u81ea2025\u5e743\u6708\u8d77\u5df2\u90e8\u7f72\u5230\u771f\u5b9e\u5de5\u4e1a\u7ba1\u9053\u4e2d\uff0c\u4e3a\u52a8\u6001\u7535\u5546\u73af\u5883\u4e2d\u7684\u8fd0\u8425\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6574\u5408\u672a\u6765\u4e8b\u4ef6\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86\u9700\u6c42\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.08254", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08254", "abs": "https://arxiv.org/abs/2602.08254", "authors": ["Arman Aghaee", "Sepehr Asgarian", "Jouhyun Jeon"], "title": "SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities", "comment": "Presented in AAAI 2026 Singapore at the workshop of Health Intelligence", "summary": "Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.", "AI": {"tldr": "SynthAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u80a5\u80d6\u75c7\u5408\u5e76\u7cbe\u795e\u969c\u788d\u60a3\u8005\uff0c\u901a\u8fc7\u6574\u5408\u4e34\u5e8a\u6570\u636e\u548c\u6587\u732e\u6784\u5efa\u4e2a\u6027\u5316\u865a\u62df\u60a3\u8005\uff0c\u6a21\u62df\u75be\u75c5\u8fdb\u5c55\u548c\u6cbb\u7597\u53cd\u5e94\uff0cGPT-5\u548cClaude 4.5 Sonnet\u5728\u8be5\u6846\u67b6\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u6570\u636e\u788e\u7247\u5316\u3001\u504f\u89c1\u548c\u9690\u79c1\u9650\u5236\u7684\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u590d\u6742\u75be\u75c5\u63d0\u4f9b\u9ad8\u4fdd\u771f\u60a3\u8005\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1SynthAgent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\uff0c\u6574\u5408\u533b\u4fdd\u7d22\u8d54\u6570\u636e\u3001\u4eba\u53e3\u8c03\u67e5\u548c\u60a3\u8005\u4e2d\u5fc3\u6587\u732e\uff0c\u6784\u5efa\u5177\u6709\u4eba\u683c\u7279\u8d28\u7684\u4e2a\u6027\u5316\u865a\u62df\u60a3\u8005\uff0c\u901a\u8fc7\u81ea\u4e3b\u667a\u80fd\u4f53\u4ea4\u4e92\u6a21\u62df\u75be\u75c5\u8fdb\u5c55\u3001\u6cbb\u7597\u53cd\u5e94\u548c\u751f\u6d3b\u7ba1\u7406\u3002", "result": "\u8bc4\u4f30100\u591a\u4e2a\u751f\u6210\u7684\u60a3\u8005\u663e\u793a\uff0cGPT-5\u548cClaude 4.5 Sonnet\u4f5c\u4e3a\u6838\u5fc3\u5f15\u64ce\u5728MAS\u6846\u67b6\u4e2d\u8fbe\u5230\u6700\u9ad8\u4fdd\u771f\u5ea6\uff0c\u4f18\u4e8eGemini 2.5 Pro\u548cDeepSeek-R1\u3002", "conclusion": "SynthAgent\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63a2\u7d22\u533b\u5b66\u548c\u5fc3\u7406\u9886\u57df\u7684\u60a3\u8005\u65c5\u7a0b\u3001\u884c\u4e3a\u52a8\u6001\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2602.07223", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07223", "abs": "https://arxiv.org/abs/2602.07223", "authors": ["Yikang Yue", "Yuqi Xue", "Jian Huang"], "title": "SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding", "comment": null, "summary": "Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.", "AI": {"tldr": "SpecAttn\u662f\u4e00\u79cd\u57fa\u4e8e\u9a8c\u8bc1\u5f15\u5bfc\u7a00\u758f\u6ce8\u610f\u529b\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u8fc7\u7a0b\u8bc6\u522b\u5173\u952eKV\u6761\u76ee\uff0c\u5728\u540e\u7eed\u4ee4\u724c\u751f\u6210\u65f6\u4ec5\u52a0\u8f7d\u8fd9\u4e9b\u6761\u76ee\uff0c\u63d0\u9ad8\u89e3\u7801\u541e\u5410\u91cf\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9762\u4e34KV\u7f13\u5b58\u5185\u5b58\u9700\u6c42\u589e\u957f\u7684\u74f6\u9888\u3002\u73b0\u6709\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u72ec\u7acb\u7684KV\u9009\u62e9\u7b97\u6cd5\uff0c\u5ffd\u7565\u4e86\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u5df2\u8ba1\u7b97\u51fa\u7684KV\u6761\u76ee\u91cd\u8981\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51faSpecAttn\u65b9\u6cd5\uff0c\u5c06\u9a8c\u8bc1\u8fc7\u7a0b\u4f5c\u4e3a\u526f\u4ea7\u54c1\u6765\u8bc6\u522b\u5173\u952eKV\u6761\u76ee\uff0c\u5728\u540e\u7eed\u4ee4\u724c\u751f\u6210\u65f6\u4ec5\u52a0\u8f7d\u8fd9\u4e9b\u5173\u952e\u6761\u76ee\uff0c\u5b9e\u73b0\u9a8c\u8bc1\u5f15\u5bfc\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "SpecAttn\u76f8\u6bd4\u4f20\u7edf\u81ea\u56de\u5f52\u89e3\u7801\u5b9e\u73b02.81\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7a00\u758f\u6027\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u63d0\u53471.29\u500d\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8349\u7a3f\u4ee4\u724c\u63a5\u53d7\u7387\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u81ea\u7136\u8ba1\u7b97\u51fa\u7684KV\u6761\u76ee\u91cd\u8981\u6027\u4fe1\u606f\uff0cSpecAttn\u6709\u6548\u89e3\u51b3\u4e86KV\u7f13\u5b58\u5185\u5b58\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u3002"}}
{"id": "2602.07069", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07069", "abs": "https://arxiv.org/abs/2602.07069", "authors": ["Zihao Fan", "Xin Lu", "Yidi Liu", "Jie Huang", "Dong Li", "Xueyang Fu", "Zheng-Jun Zha"], "title": "Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution", "comment": null, "summary": "Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.", "AI": {"tldr": "Bird-SR\u662f\u4e00\u4e2a\u53cc\u5411\u5956\u52b1\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u5b66\u4e60\u5c06\u8d85\u5206\u8fa8\u7387\u5efa\u6a21\u4e3a\u8f68\u8ff9\u7ea7\u504f\u597d\u4f18\u5316\uff0c\u540c\u65f6\u5229\u7528\u5408\u6210LR-HR\u5bf9\u548c\u771f\u5b9e\u4e16\u754cLR\u56fe\u50cf\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u63d0\u5347\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u80fd\u5408\u6210\u4e30\u5bcc\u7ec6\u8282\uff0c\u4f46\u5728\u5408\u6210\u914d\u5bf9\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5e38\u56e0\u5206\u5e03\u504f\u79fb\u800c\u5728\u771f\u5b9e\u4e16\u754c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u5229\u7528\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7ed3\u6784\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u611f\u77e5\u8d28\u91cf\u3002", "method": "\u63d0\u51faBird-SR\u6846\u67b6\uff1a1\uff09\u65e9\u671f\u6269\u6563\u6b65\u9aa4\u76f4\u63a5\u5728\u5408\u6210\u5bf9\u4e0a\u4f18\u5316\u4ee5\u4fdd\u8bc1\u7ed3\u6784\u4fdd\u771f\u5ea6\uff1b2\uff09\u540e\u671f\u91c7\u6837\u6b65\u9aa4\u5bf9\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u5e94\u7528\u8d28\u91cf\u5f15\u5bfc\u5956\u52b1\uff1b3\uff09\u91c7\u7528\u76f8\u5bf9\u4f18\u52bf\u7a7a\u95f4\u548c\u8bed\u4e49\u5bf9\u9f50\u7ea6\u675f\u9632\u6b62\u5956\u52b1\u653b\u51fb\uff1b4\uff09\u4f7f\u7528\u52a8\u6001\u4fdd\u771f\u5ea6-\u611f\u77e5\u6743\u91cd\u7b56\u7565\u5e73\u8861\u7ed3\u6784\u4fdd\u6301\u548c\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBird-SR\u5728\u611f\u77e5\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5bf9\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u7684\u6709\u6548\u6027\u3002", "conclusion": "Bird-SR\u901a\u8fc7\u53cc\u5411\u5956\u52b1\u5f15\u5bfc\u7684\u6269\u6563\u6846\u67b6\u548c\u8f68\u8ff9\u7ea7\u504f\u597d\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.07226", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07226", "abs": "https://arxiv.org/abs/2602.07226", "authors": ["Zihan Zhu", "Yanqiu Wu", "Qiongkai Xu"], "title": "Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators", "comment": null, "summary": "In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5bb9\u9519\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6a21\u578b\u5373\u670d\u52a1\u65f6\u4ee3\u8bc4\u4f30\u6837\u672c\u9ad8\u6548\u7684\u6027\u80fd\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u53ef\u8c03\u8282\u5bb9\u9519\u6c34\u5e73\u03b5\u5e73\u8861\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u65b9\u5dee\u573a\u666f\u4e0b\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u6a21\u578b\u5373\u670d\u52a1\u65f6\u4ee3\uff0c\u7ec4\u7ec7\u4f9d\u8d56\u7b2c\u4e09\u65b9AI\u6a21\u578b\u5feb\u901f\u90e8\u7f72\uff0c\u4f46\u65b0\u5174AI\u5e94\u7528\u7684\u52a8\u6001\u6027\u3001\u65b0\u6570\u636e\u96c6\u7684\u6301\u7eed\u5f15\u5165\u4ee5\u53ca\u58f0\u79f0\u4f18\u8d8a\u6027\u80fd\u7684\u6a21\u578b\u6570\u91cf\u589e\u52a0\uff0c\u4f7f\u5f97\u6a21\u578b\u670d\u52a1\u7684\u6709\u6548\u53ef\u9760\u9a8c\u8bc1\u53d8\u5f97\u65e5\u76ca\u56f0\u96be\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u4f4e\u65b9\u5dee\u8bbe\u7f6e\u4e0b\u5b58\u5728\u95ee\u9898\uff1aRMSE\u6df7\u6dc6\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u5f53\u65b9\u5dee\u5c0f\u65f6\u63a9\u76d6\u6301\u7eed\u504f\u5dee\uff1b\u800c\u57fa\u4e8ep\u503c\u7684\u6d4b\u8bd5\u53d8\u5f97\u8fc7\u4e8e\u654f\u611f\uff0c\u56e0\u5fae\u5c0f\u504f\u5dee\u62d2\u7edd\u8db3\u591f\u597d\u7684\u4f30\u8ba1\u5668\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5bb9\u9519\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u504f\u5dee\u548c\u65b9\u5dee\u8003\u8651\u6574\u5408\u5230\u53ef\u8c03\u8282\u5bb9\u9519\u6c34\u5e73\u03b5\u4e2d\uff0c\u5141\u8bb8\u5728\u5b9e\u8df5\u53ef\u63a5\u53d7\u7684\u8bef\u5dee\u8303\u56f4\u5185\u8bc4\u4f30\u6027\u80fd\u4f30\u8ba1\u5668\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u9002\u5f53\u6821\u51c6\u03b5\u53ef\u786e\u4fdd\u5728\u4e0d\u540c\u65b9\u5dee\u673a\u5236\u4e0b\u7684\u53ef\u9760\u8bc4\u4f30\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u81ea\u52a8\u4f18\u5316\u548c\u9009\u62e9\u03b5\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u5bf9\u4f30\u8ba1\u5668\u884c\u4e3a\u7684\u5168\u9762\u4e14\u53ef\u64cd\u4f5c\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bb9\u9519\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u65b9\u5dee\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6574\u5408\u504f\u5dee\u548c\u65b9\u5dee\u8003\u8651\u5e76\u5f15\u5165\u53ef\u8c03\u8282\u5bb9\u9519\u6c34\u5e73\uff0c\u4e3a\u6837\u672c\u9ad8\u6548\u6027\u80fd\u4f30\u8ba1\u5668\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.07082", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07082", "abs": "https://arxiv.org/abs/2602.07082", "authors": ["Haoming Wang", "Qiyao Xue", "Weichen Liu", "Wei Gao"], "title": "MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation", "comment": null, "summary": "When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \\emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.", "AI": {"tldr": "MosaicThinker\uff1a\u4e00\u79cd\u7528\u4e8e\u8bbe\u5907\u7aef\u5177\u8eabAI\u7684\u63a8\u7406\u65f6\u8ba1\u7b97\u6280\u672f\uff0c\u901a\u8fc7\u6574\u5408\u591a\u5e27\u788e\u7247\u5316\u7a7a\u95f4\u4fe1\u606f\u5230\u7edf\u4e00\u8bed\u4e49\u5730\u56fe\u4e2d\uff0c\u589e\u5f3a\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u5e27\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5177\u8eabAI\u4ece\u4f20\u7edf\u7269\u4f53\u68c0\u6d4b\u6269\u5c55\u5230\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u52a8\u4f5c\u89c4\u5212\u7b49\u9ad8\u7ea7\u4efb\u52a1\uff0c\u9700\u8981\u4ece\u89c6\u9891\u8f93\u5165\u4e2d\u8fdb\u884c\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u6765\u611f\u77e5\u7269\u4f53\u7a7a\u95f4\u5173\u7cfb\u5e76\u6307\u5bfc\u8bbe\u5907\u52a8\u4f5c\u3002\u7136\u800c\uff0c\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7531\u4e8e\u7f3a\u4e4f3D\u7a7a\u95f4\u4fe1\u606f\u77e5\u8bc6\uff0c\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u80fd\u529b\u5f88\u5f31\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u8de8\u591a\u5e27\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u3002", "method": "\u63d0\u51faMosaicThinker\u6280\u672f\uff0c\u5c06\u591a\u5e27\u788e\u7247\u5316\u7684\u7a7a\u95f4\u4fe1\u606f\u6574\u5408\u5230\u7edf\u4e00\u7684\u5168\u5c40\u8bed\u4e49\u5730\u56fe\u8868\u793a\u4e2d\uff0c\u7136\u540e\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u5f15\u5bfc\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u5730\u56fe\u4e0a\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u8d44\u6e90\u53d7\u9650\u7684\u5177\u8eabAI\u8bbe\u5907\u5728\u8de8\u5e27\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7c7b\u578b\u548c\u590d\u6742\u5ea6\u7684\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "MosaicThinker\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u7684\u8bed\u4e49\u5730\u56fe\u8868\u793a\u548c\u89c6\u89c9\u63d0\u793a\u673a\u5236\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u8bbe\u5907\u7aef\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8de8\u5e27\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5177\u8eabAI\u8bbe\u5907\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7a7a\u95f4\u63a8\u7406\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2602.07628", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07628", "abs": "https://arxiv.org/abs/2602.07628", "authors": ["Keondo Park", "Younghoon Na", "Yourim Choi", "Hyunwoo Ryu", "Hyun-Woo Shin", "Hyung-Sin Kim"], "title": "SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures", "comment": "8 pages, Appendix 9 pages", "summary": "While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.", "AI": {"tldr": "SleepMaMi\u662f\u4e00\u4e2a\u7761\u7720\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42\u53cc\u7f16\u7801\u5668\u8bbe\u8ba1\uff0c\u80fd\u591f\u540c\u65f6\u5efa\u6a21\u6574\u591c\u7761\u7720\u7684\u5b8f\u89c2\u7ed3\u6784\u548c\u751f\u7269\u4fe1\u53f7\u7684\u5fae\u89c2\u5f62\u6001\uff0c\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u7761\u7720\u533b\u5b66\u4e3b\u8981\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e13\u6ce8\u4e8e\u5c40\u90e8\u5fae\u89c2\u7ed3\u6784\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u591a\u5bfc\u7761\u7720\u56fe\uff08PSG\uff09\u7684\u4e30\u5bcc\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff0c\u4e5f\u65e0\u6cd5\u6355\u6349\u6574\u591c\u7761\u7720\u7684\u5168\u5c40\u5b8f\u89c2\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u5206\u5c42\u53cc\u7f16\u7801\u5668\u8bbe\u8ba1\uff1a\u5b8f\u89c2\u7f16\u7801\u5668\u901a\u8fc7\u4eba\u53e3\u7edf\u8ba1\u5b66\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u5efa\u6a21\u6574\u591c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff1b\u5fae\u89c2\u7f16\u7801\u5668\u901a\u8fc7\u6df7\u5408\u63a9\u7801\u81ea\u7f16\u7801\u5668\u548c\u591a\u6a21\u6001\u5bf9\u6bd4\u76ee\u6807\u4f18\u5316\u3002\u5728\u8d85\u8fc720,000\u4e2aPSG\u8bb0\u5f55\uff08158K\u5c0f\u65f6\uff09\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "SleepMaMi\u5728\u591a\u6837\u5316\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6807\u7b7e\u9ad8\u6548\u7684\u4e34\u5e8a\u7761\u7720\u5206\u6790\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "SleepMaMi\u6210\u529f\u89e3\u51b3\u4e86\u7761\u7720\u533b\u5b66\u4e2d\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u540c\u65f6\u638c\u63e1\u957f\u65f6\u95f4\u7761\u7720\u67b6\u6784\u548c\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u5f62\u6001\uff0c\u4e3a\u4e34\u5e8a\u7761\u7720\u5206\u6790\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2602.07227", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07227", "abs": "https://arxiv.org/abs/2602.07227", "authors": ["Nethmi Jayasinghe", "Diana Gontero", "Spencer T. Brown", "Vinod K. Sangwan", "Mark C. Hersam", "Amit Ranjan Trivedi"], "title": "Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation", "comment": null, "summary": "Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\\%$ on \\texttt{HalfCheetah-v5} and $+53\\%$ on \\texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53d7\u5c0f\u8111\u542f\u53d1\u7684\u63a8\u7406\u65f6\u6b8b\u5dee\u63a7\u5236\u6846\u67b6\uff0c\u5728\u51bb\u7ed3\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u57fa\u7840\u4e0a\u6dfb\u52a0\u5728\u7ebf\u6821\u6b63\u52a8\u4f5c\uff0c\u5b9e\u73b0\u6545\u969c\u6062\u590d\u800c\u65e0\u9700\u4fee\u6539\u57fa\u7840\u7b56\u7565\u53c2\u6570\u3002", "motivation": "\u673a\u5668\u4eba\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u5e38\u9047\u5230\u8bad\u7ec3\u540e\u6545\u969c\uff0c\u800c\u91cd\u65b0\u8bad\u7ec3\u3001\u63a2\u7d22\u6216\u7cfb\u7edf\u8fa8\u8bc6\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u57fa\u7840\u7b56\u7565\u53c2\u6570\u7684\u5728\u7ebf\u6545\u969c\u6062\u590d\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5c0f\u8111\u542f\u53d1\u7684\u6b8b\u5dee\u63a7\u5236\u6846\u67b6\uff1a1) \u901a\u8fc7\u56fa\u5b9a\u7279\u5f81\u6269\u5c55\u5b9e\u73b0\u9ad8\u7ef4\u6a21\u5f0f\u5206\u79bb\uff1b2) \u5e76\u884c\u5fae\u533a\u5f0f\u6b8b\u5dee\u901a\u8def\uff1b3) \u5177\u6709\u5174\u594b\u6027\u548c\u6291\u5236\u6027\u8d44\u683c\u8ff9\u7684\u5c40\u90e8\u8bef\u5dee\u9a71\u52a8\u53ef\u5851\u6027\uff0c\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fd0\u884c\uff1b4) \u6027\u80fd\u9a71\u52a8\u7684\u5143\u9002\u5e94\u673a\u5236\u8c03\u8282\u6b8b\u5dee\u6743\u9650\u548c\u53ef\u5851\u6027\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u9a71\u52a8\u5668\u3001\u52a8\u6001\u548c\u73af\u5883\u6270\u52a8\u4e0b\uff0cHalfCheetah-v5\u6027\u80fd\u63d0\u5347\u8fbe+66%\uff0cHumanoid-v5\u63d0\u5347\u8fbe+53%\uff1b\u5728\u4e25\u91cd\u6270\u52a8\u4e0b\u6027\u80fd\u4f18\u96c5\u4e0b\u964d\uff0c\u5e76\u80fd\u5c06\u6301\u4e45\u6b8b\u5dee\u6821\u6b63\u6574\u5408\u5230\u7b56\u7565\u53c2\u6570\u4e2d\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5c0f\u8111\u542f\u53d1\u7684\u63a8\u7406\u65f6\u6b8b\u5dee\u63a7\u5236\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u8bad\u7ec3\u540e\u6545\u969c\uff0c\u5b9e\u73b0\u5feb\u901f\u5c40\u90e8\u6821\u6b63\uff0c\u907f\u514d\u7834\u574f\u6027\u5168\u5c40\u7b56\u7565\u66f4\u65b0\uff0c\u540c\u65f6\u4fdd\u6301\u540d\u4e49\u884c\u4e3a\u5e76\u6291\u5236\u4e0d\u5fc5\u8981\u7684\u5e72\u9884\u3002"}}
{"id": "2602.07095", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07095", "abs": "https://arxiv.org/abs/2602.07095", "authors": ["Wang Lin", "Feng Wang", "Majun Zhang", "Wentao Hu", "Tao Jin", "Zhou Zhao", "Fei Wu", "Jingyuan Chen", "Alan Yuille", "Sucheng Ren"], "title": "WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark", "comment": null, "summary": "Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \\textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \\textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.", "AI": {"tldr": "WorldEdit\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u4e16\u754c\u77e5\u8bc6\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u548c\u56e0\u679c\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u9690\u5f0f\u7f16\u8f91\u6307\u4ee4\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5904\u7406\u663e\u5f0f\u6307\u4ee4\uff08\u5982\u5c5e\u6027\u64cd\u4f5c\u3001\u98ce\u683c\u8f6c\u6362\uff09\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u9690\u5f0f\u7f16\u8f91\u6307\u4ee4\uff08\u63cf\u8ff0\u89c6\u89c9\u53d8\u5316\u539f\u56e0\u800c\u975e\u5177\u4f53\u7ed3\u679c\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u5904\u7406\u590d\u6742\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86WorldEdit\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u7f16\u8f91\u6837\u672c\u548c\u7b26\u5408\u73b0\u5b9e\u4e16\u754c\u56e0\u679c\u903b\u8f91\u7684\u6539\u5199\u6307\u4ee4\uff1b\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u5fae\u8c03Bagel\u7b49\u6a21\u578b\uff0c\u5e76\u6574\u5408\u56e0\u679c\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86\u4e0eGPT-4o\u548cNano-Banana\u7684\u5dee\u8ddd\uff0c\u5728\u6307\u4ee4\u9075\u5faa\u548c\u77e5\u8bc6\u5408\u7406\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u800c\u8fd9\u4e24\u4e2a\u65b9\u9762\u901a\u5e38\u662f\u5f00\u6e90\u7cfb\u7edf\u7684\u8584\u5f31\u73af\u8282\u3002", "conclusion": "WorldEdit\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5904\u7406\u9690\u5f0f\u6307\u4ee4\u65f6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4e16\u754c\u77e5\u8bc6\u9a71\u52a8\u7684\u7f16\u8f91\u7b56\u7565\u63d0\u5347\u4e86\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.07642", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07642", "abs": "https://arxiv.org/abs/2602.07642", "authors": ["Zhuoyan Xu", "Haoyang Fang", "Boran Han", "Bonan Min", "Bernie Wang", "Cuixiong Hu", "Shuai Zhang"], "title": "Efficient Table Retrieval and Understanding with Multimodal Large Language Models", "comment": "Published at EACL 2026 Findings", "summary": "Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.", "AI": {"tldr": "TabRAG\u6846\u67b6\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u57fa\u7840\u6a21\u578b\u68c0\u7d22\u5019\u9009\u8868\u683c\u56fe\u50cf\uff0cMLLM\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\uff0c\u518d\u4f7f\u7528MLLM\u8fdb\u884c\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u8868\u683c\u56fe\u50cf\u96c6\u5408\u7684\u67e5\u8be2\u56de\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u8868\u683c\u6570\u636e\u5e38\u4ee5\u56fe\u50cf\u5f62\u5f0f\u5b58\u5728\uff08\u5982\u8d22\u52a1\u62a5\u8868\u3001\u624b\u5199\u8bb0\u5f55\u3001\u6587\u6863\u626b\u63cf\uff09\uff0c\u73b0\u6709MLLM\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u76f8\u5173\u8868\u683c\u5df2\u51c6\u5907\u597d\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u9700\u8981\u4ece\u5927\u89c4\u6a21\u8868\u683c\u56fe\u50cf\u96c6\u5408\u4e2d\u8bc6\u522b\u548c\u63a8\u7406\u76f8\u5173\u8868\u683c\u6765\u56de\u7b54\u7528\u6237\u67e5\u8be2\u3002", "method": "TabRAG\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u8054\u5408\u8bad\u7ec3\u7684\u89c6\u89c9-\u6587\u672c\u57fa\u7840\u6a21\u578b\u68c0\u7d22\u5019\u9009\u8868\u683c\uff1b2\uff09\u5229\u7528MLLM\u5bf9\u5019\u9009\u8868\u683c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\uff1b3\uff09\u4f7f\u7528MLLM\u5bf9\u9009\u5b9a\u8868\u683c\u8fdb\u884c\u63a8\u7406\u751f\u6210\u7b54\u6848\u3002", "result": "\u5728\u5305\u542b88,161\u4e2a\u8bad\u7ec3\u6837\u672c\u548c9,819\u4e2a\u6d4b\u8bd5\u6837\u672c\u7684\u65b0\u5efa\u6570\u636e\u96c6\u4e0a\uff0cTabRAG\u5728\u68c0\u7d22\u53ec\u56de\u7387\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53477.0%\uff0c\u5728\u7b54\u6848\u51c6\u786e\u7387\u4e0a\u63d0\u53476.1%\u3002", "conclusion": "TabRAG\u4e3a\u73b0\u5b9e\u4e16\u754c\u8868\u683c\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u8868\u683c\u56fe\u50cf\u96c6\u5408\u7684\u67e5\u8be2\u56de\u7b54\u95ee\u9898\u3002"}}
{"id": "2602.07235", "categories": ["cs.LG", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.07235", "abs": "https://arxiv.org/abs/2602.07235", "authors": ["Atefeh Gilani", "Carol Xuan Long", "Sajani Vithana", "Oliver Kosut", "Lalitha Sankar", "Flavio P. Calmon"], "title": "ArcMark: Multi-bit LLM Watermark via Optimal Transport", "comment": null, "summary": "Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u63a8\u5bfc\u51fa\u591a\u6bd4\u7279\u6c34\u5370\u7684\u4fe1\u606f\u8bba\u5bb9\u91cf\uff0c\u5e76\u57fa\u4e8e\u7f16\u7801\u7406\u8bba\u8bbe\u8ba1\u4e86\u8fbe\u5230\u8be5\u5bb9\u91cf\u7684ArcMark\u6c34\u5370\u65b9\u6848\uff0c\u5728\u6bd4\u7279\u7387\u548c\u68c0\u6d4b\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u65b9\u6cd5\u4e3b\u8981\u4ece\u96f6\u6bd4\u7279\u6c34\u5370\u6269\u5c55\u800c\u6765\uff0c\u6bcf\u4ee4\u724c\u4ec5\u7f16\u7801\u5355\u4e2a\u6bd4\u7279\uff0c\u800c\u591a\u6bd4\u7279\u6c34\u5370\u7684\u4fe1\u606f\u8bba\u5bb9\u91cf\uff08\u5728\u4e0d\u6539\u53d8\u5e73\u5747\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\u6bcf\u4ee4\u724c\u53ef\u63d2\u5165\u548c\u68c0\u6d4b\u7684\u6700\u5927\u6bd4\u7279\u6570\uff09\u4e00\u76f4\u672a\u77e5\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790\u63a8\u5bfc\u591a\u6bd4\u7279\u6c34\u5370\u7684\u5bb9\u91cf\u7279\u6027\uff0c\u57fa\u4e8e\u7f16\u7801\u7406\u8bba\u539f\u7406\u8bbe\u8ba1ArcMark\u6c34\u5370\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\u80fd\u591f\u8fbe\u5230\u591a\u6bd4\u7279\u6c34\u5370\u4fe1\u9053\u7684\u5bb9\u91cf\u3002", "result": "ArcMark\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5728\u6bcf\u4ee4\u724c\u6bd4\u7279\u7387\u548c\u68c0\u6d4b\u51c6\u786e\u7387\u65b9\u9762\u4f18\u4e8e\u7ade\u4e89\u7684\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u672c\u8d28\u4e0a\u662f\u4fe1\u9053\u7f16\u7801\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u8868\u5f81\u4e86\u591a\u6bd4\u7279\u6c34\u5370\u7684\u5bb9\u91cf\uff0c\u4e3a\u57fa\u4e8e\u7f16\u7801\u7406\u8bba\u539f\u7406\u7684\u6c34\u5370\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c06\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u89c6\u4e3a\u4fe1\u9053\u7f16\u7801\u95ee\u9898\u3002"}}
{"id": "2602.07100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07100", "abs": "https://arxiv.org/abs/2602.07100", "authors": ["Biao Xiong", "Zhen Peng", "Ping Wang", "Qiegen Liu", "Xian Zhong"], "title": "TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation", "comment": null, "summary": "Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.", "AI": {"tldr": "TLC-Plan\u662f\u4e00\u4e2a\u76f4\u63a5\u5408\u6210\u77e2\u91cf\u5e73\u9762\u56fe\u7684\u5c42\u6b21\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u7ea7VQ-VAE\u7f16\u7801\u5168\u5c40\u5e03\u5c40\u548c\u5c40\u90e8\u51e0\u4f55\uff0c\u4f7f\u7528CodeTree\u8868\u793a\u548c\u81ea\u56de\u5f52transformer\u751f\u6210\u591a\u6837\u4e14\u62d3\u6251\u6709\u6548\u7684\u8bbe\u8ba1\uff0c\u5728RPLAN\u548cLIFULL\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6805\u683c\u7a7a\u95f4\u64cd\u4f5c\u5e76\u4f9d\u8d56\u540e\u5904\u7406\u77e2\u91cf\u5316\uff0c\u5bfc\u81f4\u7ed3\u6784\u4e0d\u4e00\u81f4\u5e76\u963b\u788d\u7aef\u5230\u7aef\u5b66\u4e60\u3002\u53d7\u7ec4\u5408\u7a7a\u95f4\u63a8\u7406\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u7b26\u5408\u4eba\u7c7b\u5efa\u7b51\u5de5\u4f5c\u6d41\u7a0b\uff08\u57fa\u4e8e\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u6a21\u5f0f\uff09\u7684\u76f4\u63a5\u77e2\u91cf\u5e73\u9762\u56fe\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTLC-Plan\u5c42\u6b21\u751f\u6210\u6a21\u578b\uff1a1\uff09\u4f7f\u7528\u4e24\u7ea7VQ-VAE\u7f16\u7801\u5168\u5c40\u5e03\u5c40\uff08\u8bed\u4e49\u6807\u8bb0\u7684\u623f\u95f4\u8fb9\u754c\u6846\uff09\u548c\u5c40\u90e8\u51e0\u4f55\uff08\u591a\u8fb9\u5f62\u7ea7\u7f16\u7801\uff09\uff1b2\uff09\u7edf\u4e00\u4e3aCodeTree\u8868\u793a\uff1b3\uff09\u4f7f\u7528\u81ea\u56de\u5f52transformer\u6839\u636e\u8fb9\u754c\u6761\u4ef6\u91c7\u6837\u7f16\u7801\uff0c\u751f\u6210\u591a\u6837\u4e14\u62d3\u6251\u6709\u6548\u7684\u8bbe\u8ba1\uff0c\u65e0\u9700\u663e\u5f0f\u623f\u95f4\u62d3\u6251\u6216\u7ef4\u5ea6\u5148\u9a8c\u3002", "result": "\u5728RPLAN\u6570\u636e\u96c6\u4e0a\u8fbe\u5230FID=1.84\u3001MSE=2.06\u7684SOTA\u6027\u80fd\uff0c\u5728LIFULL\u6570\u636e\u96c6\u4e0a\u4e5f\u53d6\u5f97\u9886\u5148\u7ed3\u679c\u3002\u6846\u67b6\u652f\u6301\u7ea6\u675f\u611f\u77e5\u548c\u53ef\u6269\u5c55\u7684\u77e2\u91cf\u5e73\u9762\u56fe\u751f\u6210\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5efa\u7b51\u5e94\u7528\u3002", "conclusion": "TLC-Plan\u901a\u8fc7\u76f4\u63a5\u5408\u6210\u77e2\u91cf\u5e73\u9762\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6805\u683c\u65b9\u6cd5\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u5efa\u7b51\u5de5\u4f5c\u6d41\u7a0b\u4e00\u81f4\u7684\u53ef\u6269\u5c55\u751f\u6210\uff0c\u4e3a\u5b9e\u9645\u5efa\u7b51\u5e94\u7528\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u7ea6\u675f\u611f\u77e5\u77e2\u91cf\u5e73\u9762\u56fe\u751f\u6210\u6846\u67b6\u3002"}}
{"id": "2602.07662", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07662", "abs": "https://arxiv.org/abs/2602.07662", "authors": ["Glenda Amaral", "Tiago Prince Sales", "Riccardo Baratella", "Daniele Porello", "Renata Guizzardi", "Giancarlo Guizzardi"], "title": "ONTrust: A Reference Ontology of Trust", "comment": "46 pages", "summary": "Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7edf\u4e00\u57fa\u7840\u672c\u4f53\u8bba\u7684\u4fe1\u4efb\u53c2\u8003\u672c\u4f53\u8bba\uff08ONTrust\uff09\uff0c\u65e8\u5728\u4e3a\u4fe1\u4efb\u6982\u5ff5\u63d0\u4f9b\u575a\u5b9e\u7684\u672c\u4f53\u8bba\u57fa\u7840\uff0c\u652f\u6301\u4fe1\u606f\u5efa\u6a21\u3001\u81ea\u52a8\u63a8\u7406\u3001\u4fe1\u606f\u96c6\u6210\u548c\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u548c\u533a\u5757\u94fe\u7b49\u65b0\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4fe1\u4efb\u5728\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u8fd9\u4e9b\u6280\u672f\u7684\u91c7\u7528\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u4fe1\u4efb\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4fe1\u4efb\u6982\u5ff5\u7684\u7edf\u4e00\u5b9a\u4e49\u548c\u5f62\u5f0f\u5316\u8868\u5f81\uff0c\u96be\u4ee5\u652f\u6301\u4eba\u673a\u7406\u89e3\u548c\u4e92\u64cd\u4f5c\u3002", "method": "\u57fa\u4e8e\u7edf\u4e00\u57fa\u7840\u672c\u4f53\u8bba\uff08UFO\uff09\uff0c\u4f7f\u7528OntoUML\u8bed\u8a00\u5f00\u53d1\u4e86\u4fe1\u4efb\u53c2\u8003\u672c\u4f53\u8bba\uff08ONTrust\uff09\uff0c\u5f62\u5f0f\u5316\u8868\u5f81\u4fe1\u4efb\u6982\u5ff5\u53ca\u5176\u4e0d\u540c\u7c7b\u578b\uff0c\u63cf\u8ff0\u5f71\u54cd\u4fe1\u4efb\u7684\u56e0\u7d20\uff0c\u89e3\u91ca\u4fe1\u4efb\u5173\u7cfb\u4e2d\u98ce\u9669\u7684\u4ea7\u751f\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u6587\u732e\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "ONTrust\u672c\u4f53\u8bba\u5df2\u5e94\u7528\u4e8e\u591a\u4e2a\u9886\u57df\uff1a\u6982\u5ff5\u5efa\u6a21\u548c\u4f01\u4e1a\u67b6\u6784\u8bbe\u8ba1\u3001\u8bed\u8a00\u8bc4\u4f30\u4e0e\uff08\u91cd\u65b0\uff09\u8bbe\u8ba1\u3001\u4fe1\u4efb\u7ba1\u7406\u3001\u9700\u6c42\u5de5\u7a0b\uff0c\u4ee5\u53ca\u5728\u60c5\u611f\u4eba\u673a\u534f\u4f5c\u80cc\u666f\u4e0b\u7684\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u3002", "conclusion": "ONTrust\u4e3a\u4fe1\u4efb\u6982\u5ff5\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u672c\u4f53\u8bba\u57fa\u7840\uff0c\u80fd\u591f\u652f\u6301\u4eba\u673a\u7406\u89e3\u548c\u4e92\u64cd\u4f5c\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u53ef\u4fe1\u7cfb\u7edf\uff0c\u4fc3\u8fdb\u65b0\u6280\u672f\u5728\u6539\u5584\u4ea7\u54c1\u670d\u52a1\u548c\u63d0\u5347\u4e2a\u4f53\u96c6\u4f53\u798f\u7949\u65b9\u9762\u7684\u5e94\u7528\u3002"}}
{"id": "2602.07256", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07256", "abs": "https://arxiv.org/abs/2602.07256", "authors": ["Ruizhong Qiu", "Ting-Wei Li", "Gaotang Li", "Hanghang Tong"], "title": "Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning", "comment": "ICLR 2026", "summary": "Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.", "AI": {"tldr": "GRAPHITE\u901a\u8fc7\u521b\u5efa\u7279\u5f81\u8282\u70b9\u76f4\u63a5\u63d0\u5347\u56fe\u540c\u8d28\u6027\uff0c\u89e3\u51b3\u5f02\u8d28\u56fe\u4e0a\u7684GNN\u6027\u80fd\u95ee\u9898\uff0c\u5728\u5f02\u8d28\u56fe\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u540c\u8d28\u56fe\u4e0a\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5728\u5f02\u8d28\u56fe\uff08\u8fde\u63a5\u8282\u70b9\u7279\u5f81\u6216\u6807\u7b7e\u4e0d\u76f8\u4f3c\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u7b80\u5355\u7684MLP\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u67b6\u6784\u8bbe\u8ba1\uff0c\u800c\u6ca1\u6709\u76f4\u63a5\u9488\u5bf9\u5f02\u8d28\u6027\u7684\u6839\u672c\u539f\u56e0\u3002\u9700\u8981\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\u6765\u76f4\u63a5\u89e3\u51b3\u56fe\u5f02\u8d28\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faGRAPHITE\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u56fe\u53d8\u6362\u76f4\u63a5\u63d0\u9ad8\u56fe\u540c\u8d28\u6027\u3002\u57fa\u4e8e\u540c\u8d28\u6027\u7684\u7cbe\u786e\u5b9a\u4e49\uff0c\u521b\u5efa\u7279\u5f81\u8282\u70b9\u6765\u4fc3\u8fdb\u5177\u6709\u76f8\u4f3c\u7279\u5f81\u7684\u8282\u70b9\u4e4b\u95f4\u7684\u540c\u8d28\u6027\u6d88\u606f\u4f20\u9012\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u548c\u7ecf\u9a8c\u4e0a\u90fd\u8bc1\u660e\u80fd\u663e\u8457\u63d0\u9ad8\u539f\u59cb\u5f02\u8d28\u56fe\u7684\u540c\u8d28\u6027\uff0c\u540c\u65f6\u53ea\u8f7b\u5fae\u589e\u52a0\u56fe\u7684\u5927\u5c0f\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGRAPHITE\u5728\u5f02\u8d28\u56fe\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u540c\u8d28\u56fe\u4e0a\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u7387\u3002\u4f8b\u5982\uff0c\u5728Actor\u6570\u636e\u96c6\u4e0a\uff0c21\u4e2a\u6700\u65b0\u7684GNN\u4ecd\u7136\u843d\u540e\u4e8eMLP\uff0c\u800cGRAPHITE\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "GRAPHITE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u672a\u63a2\u7d22\u7684\u8303\u5f0f\uff1a\u901a\u8fc7\u56fe\u53d8\u6362\u76f4\u63a5\u63d0\u9ad8\u56fe\u540c\u8d28\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86GNN\u5728\u5f02\u8d28\u56fe\u4e0a\u7684\u6027\u80fd\u95ee\u9898\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u660e\u786e\u901a\u8fc7\u56fe\u53d8\u6362\u76f4\u63a5\u6539\u5584\u56fe\u540c\u8d28\u6027\u7684\u65b9\u6cd5\uff0c\u4e3a\u5904\u7406\u56fe\u5f02\u8d28\u6027\u63d0\u4f9b\u4e86\u521b\u65b0\u601d\u8def\u3002"}}
{"id": "2602.07101", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07101", "abs": "https://arxiv.org/abs/2602.07101", "authors": ["Zinan Lv", "Yeqian Qian", "Chen Sang", "Hao Liu", "Danping Zou", "Ming Yang"], "title": "Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting", "comment": "12 pages, 8 figures", "summary": "UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u91cd\u5149\u71673D\u9ad8\u65af\u6cfc\u6e85\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u7684\u96f6\u6837\u672c\u5bfc\u822a\uff0c\u901a\u8fc7\u5206\u89e3\u573a\u666f\u7ec4\u4ef6\u548c\u591a\u6837\u5316\u5149\u7167\u589e\u5f3a\u8bad\u7ec3\uff0c\u8fbe\u523010m/s\u9ad8\u901f\u907f\u969c\u80fd\u529b\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u4f7f\u7528\u5355\u76ee\u89c6\u89c9\u5bfc\u822a\u9762\u4e34\u4eff\u771f\u4e0e\u73b0\u5b9e\u95f4\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5c06\u9759\u6001\u5149\u7167\u4e0e\u51e0\u4f55\u8026\u5408\uff0c\u9650\u5236\u4e86\u7b56\u7565\u5728\u52a8\u6001\u771f\u5b9e\u5149\u7167\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53ef\u91cd\u5149\u71673D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5206\u89e3\u573a\u666f\u7ec4\u4ef6\u5b9e\u73b0\u7269\u7406\u57fa\u7840\u7684\u5149\u7167\u7f16\u8f91\uff1b\u5728\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\u8bad\u7ec3\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u6837\u5316\u5408\u6210\u5149\u7167\u6761\u4ef6\u589e\u5f3a\u8bad\u7ec3\uff0c\u4f7f\u7b56\u7565\u5b66\u4e60\u5149\u7167\u4e0d\u53d8\u7684\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793a\uff0c\u8f7b\u91cf\u7ea7\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u590d\u6742\u68ee\u6797\u73af\u5883\u4e2d\u8fbe\u523010m/s\u7684\u9c81\u68d2\u65e0\u78b0\u649e\u5bfc\u822a\uff0c\u5bf9\u5267\u70c8\u5149\u7167\u53d8\u5316\u8868\u73b0\u51fa\u663e\u8457\u9002\u5e94\u80fd\u529b\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u5149\u7167\u4e0e\u51e0\u4f55\u8868\u793a\uff0c\u7ed3\u5408\u591a\u6837\u5316\u5149\u7167\u589e\u5f3a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u5ba4\u5916\u73af\u5883\u7684\u96f6\u6837\u672c\u9c81\u68d2\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5149\u7167\u57df\u5dee\u8ddd\u95ee\u9898\u3002"}}
{"id": "2602.07749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07749", "abs": "https://arxiv.org/abs/2602.07749", "authors": ["Zhenyu Wu", "Yanxi Long", "Jian Li", "Hua Huang"], "title": "Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution", "comment": "ICML2026", "summary": "Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.", "AI": {"tldr": "Geo-coder\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u51e0\u4f55\u56fe\u50cf\u9006\u5411\u7f16\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u951a\u5b9a\u548c\u5ea6\u91cf\u9a71\u52a8\u4ee3\u7801\u6f14\u5316\u5b9e\u73b0\u7cbe\u786e\u51e0\u4f55\u91cd\u5efa\uff0c\u5728\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u9886\u5148\u3002", "motivation": "\u5f53\u524d\u9006\u5411\u56fe\u5f62\u65b9\u6cd5\u5728\u91cd\u5efa\u590d\u6742\u51e0\u4f55\u7ec6\u8282\u65f6\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u5f80\u5f80\u5bfc\u81f4\u5173\u952e\u51e0\u4f55\u7ea6\u675f\u4e22\u5931\u6216\u7ed3\u6784\u5931\u771f\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u51c6\u786e\u91cd\u5efa\u51e0\u4f55\u56fe\u50cf\u5e76\u4fdd\u6301\u6838\u5fc3\u51e0\u4f55\u8bed\u4e49\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u9006\u5411\u7f16\u7a0b\u6846\u67b6Geo-coder\uff0c\u5c06\u8fc7\u7a0b\u89e3\u8026\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u901a\u8fc7\u89c6\u89c9\u7b97\u5b50\u548c\u5927\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\u5b9e\u73b0\u50cf\u7d20\u5750\u6807\u548c\u89c6\u89c9\u5c5e\u6027\u7684\u7cbe\u786e\u6355\u83b7\uff1b2) \u5f15\u5165\u5408\u6210-\u6e32\u67d3-\u9a8c\u8bc1\u95ed\u73af\uff0c\u901a\u8fc7\u53cc\u5411\u89c6\u89c9\u53cd\u9988\u9a71\u52a8\u4ee3\u7801\u81ea\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGeo-coder\u5728\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u53d6\u5f97\u663e\u8457\u9886\u5148\u3002\u91cd\u5efa\u56fe\u50cf\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u539f\u59cb\u56fe\u50cf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u3002\u5f00\u6e90\u4e86\u5305\u542b1500\u591a\u4e2a\u6837\u672c\u7684Geo-coder\u6570\u636e\u96c6\u548cGeocodeLM\u6a21\u578b\u3002", "conclusion": "Geo-coder\u901a\u8fc7\u521b\u65b0\u7684\u9006\u5411\u7f16\u7a0b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u51e0\u4f55\u7ec6\u8282\u91cd\u5efa\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u636e\u548c\u6a21\u578b\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07106", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07106", "abs": "https://arxiv.org/abs/2602.07106", "authors": ["Haoyu Zhang", "Zhipeng Li", "Yiwen Guo", "Tianshu Yu"], "title": "Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models", "comment": null, "summary": "Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.", "AI": {"tldr": "Ex-Omni\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u5168\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u751f\u6210\uff0c\u5229\u7528\u8bed\u97f3\u5355\u5143\u4f5c\u4e3a\u65f6\u95f4\u652f\u67b6\u548c\u7edf\u4e00\u7684token-as-query\u95e8\u63a7\u878d\u5408\u673a\u5236\uff0c\u4e3a\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6dfb\u52a0\u8bed\u97f3\u4f34\u968f\u76843D\u9762\u90e8\u52a8\u753b\u529f\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u65e8\u5728\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\uff0c\u4f46\u5c06\u8bed\u97f3\u4e0e3D\u9762\u90e8\u52a8\u753b\u7ed3\u5408\u7684\u7814\u7a76\u4ecd\u7136\u5f88\u5c11\uff0c\u800c\u8fd9\u5bf9\u4e8e\u81ea\u7136\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8eLLMs\u7684\u79bb\u6563\u3001token\u7ea7\u8bed\u4e49\u63a8\u7406\u4e0e3D\u9762\u90e8\u8fd0\u52a8\u6240\u9700\u7684\u5bc6\u96c6\u3001\u7ec6\u7c92\u5ea6\u65f6\u95f4\u52a8\u6001\u4e4b\u95f4\u7684\u8868\u793a\u4e0d\u5339\u914d\u3002", "method": "\u63d0\u51faEx-Omni\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u751f\u6210\u6765\u964d\u4f4e\u5b66\u4e60\u96be\u5ea6\uff1a1) \u4f7f\u7528\u8bed\u97f3\u5355\u5143\u4f5c\u4e3a\u65f6\u95f4\u652f\u67b6\uff1b2) \u91c7\u7528\u7edf\u4e00\u7684token-as-query\u95e8\u63a7\u878d\u5408\u673a\u5236\u8fdb\u884c\u53d7\u63a7\u8bed\u4e49\u6ce8\u5165\uff1b3) \u5f15\u5165InstructEx\u6570\u636e\u96c6\u6765\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEx-Omni\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u5f00\u6e90\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u591f\u7a33\u5b9a\u751f\u6210\u5bf9\u9f50\u7684\u8bed\u97f3\u548c\u9762\u90e8\u52a8\u753b\u3002", "conclusion": "Ex-Omni\u6210\u529f\u89e3\u51b3\u4e86\u5c06\u8bed\u97f3\u4e0e3D\u9762\u90e8\u52a8\u753b\u96c6\u6210\u5230\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u63a8\u7406\u4e0e\u65f6\u95f4\u751f\u6210\u7684\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2602.07754", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.07754", "abs": "https://arxiv.org/abs/2602.07754", "authors": ["Bahare Riahi", "Veronica Catete"], "title": "Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency", "comment": "13 pages, 3 figures", "summary": "This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u672c\u79d1\u751f\u5bf9AI\u8bc4\u5206\u7cfb\u7edf\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u5b66\u751f\u5bf9AI\u7f3a\u4e4f\u60c5\u5883\u7406\u89e3\u548c\u4e2a\u6027\u5316\u8868\u793a\u62c5\u5fe7\uff0c\u5efa\u8baeAI\u5e94\u4f5c\u4e3a\u4eba\u7c7b\u76d1\u7763\u4e0b\u7684\u8865\u5145\u5de5\u5177\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5b66\u751f\u5bf9AI\u8bc4\u5206\u7cfb\u7edf\u7684\u770b\u6cd5\uff0c\u7279\u522b\u662f\u5173\u6ce8\u516c\u5e73\u6027\u3001\u4fe1\u4efb\u5ea6\u3001\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u7b49\u4f26\u7406\u7ef4\u5ea6\uff0c\u4e3aAI\u5728\u6559\u80b2\u8bc4\u4f30\u4e2d\u7684\u5408\u7406\u5e94\u7528\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u57fa\u4e8eJobin\uff082019\uff09\u4f26\u7406\u539f\u5219\u6846\u67b6\u7684\u7814\u7a76\u8bbe\u8ba1\uff0c\u5728\u672c\u79d1\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u7a0b\u4e2d\u5bf927\u540d\u5b66\u751f\u8fdb\u884c\u8c03\u67e5\u7814\u7a76\uff0c\u6bd4\u8f83AI\u751f\u6210\u7684\u53cd\u9988\u4e0e\u539f\u59cb\u4eba\u5de5\u8bc4\u5206\u53cd\u9988\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u751f\u5bf9AI\u8bc4\u5206\u7cfb\u7edf\u5b58\u5728\u62c5\u5fe7\uff0c\u4e3b\u8981\u5173\u6ce8AI\u7f3a\u4e4f\u60c5\u5883\u7406\u89e3\u548c\u4e2a\u6027\u5316\u80fd\u529b\u3002\u5b66\u751f\u8ba4\u4e3aAI\u7cfb\u7edf\u5e94\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\u3001\u7075\u6d3b\u6027\u548c\u540c\u7406\u5fc3\u3002", "conclusion": "\u5efa\u8bae\u5f00\u53d1\u516c\u5e73\u53ef\u4fe1\u7684AI\u8bc4\u5206\u7cfb\u7edf\u5e94\u4f53\u73b0\u4eba\u7c7b\u5224\u65ad\u3001\u7075\u6d3b\u6027\u548c\u540c\u7406\u5fc3\uff0c\u4f5c\u4e3a\u4eba\u7c7b\u76d1\u7763\u4e0b\u7684\u8865\u5145\u5de5\u5177\uff0c\u4e3a\u8bbe\u8ba1\u4eba\u6027\u5316\u7684AI\u5b66\u4e60\u73af\u5883\u63d0\u4f9b\u539f\u5219\u3002"}}
{"id": "2602.07265", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07265", "abs": "https://arxiv.org/abs/2602.07265", "authors": ["Daniil Vankov", "Nikita Ivkin", "Kyle Ulrich", "Xiang Song", "Ashish Khetan", "George Karypis"], "title": "XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.", "AI": {"tldr": "XShare\u901a\u8fc7\u6279\u611f\u77e5\u4e13\u5bb6\u9009\u62e9\u4f18\u5316MoE\u67b6\u6784\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u52a8\u6001\u9002\u5e94\u6bcf\u4e2a\u6279\u6b21\uff0c\u51cf\u5c11\u4e13\u5bb6\u6fc0\u6d3b30%\uff0c\u964d\u4f4eGPU\u5cf0\u503c\u8d1f\u8f7d3\u500d\uff0c\u5728\u63a8\u6d4b\u89e3\u7801\u4e2d\u63d0\u5347\u541e\u5410\u91cf14%", "motivation": "MoE\u67b6\u6784\u867d\u7136\u80fd\u9ad8\u6548\u6269\u5c55\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5728\u751f\u4ea7\u63a8\u7406\u4e2d\uff0c\u8bf7\u6c42\u6279\u5904\u7406\u548c\u63a8\u6d4b\u89e3\u7801\u4f1a\u663e\u8457\u589e\u52a0\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u524a\u5f31\u6548\u7387\u4f18\u52bf\u3002\u9700\u8981\u89e3\u51b3\u6279\u5904\u7406\u73af\u5883\u4e0b\u7684\u4e13\u5bb6\u9009\u62e9\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5c06\u6279\u611f\u77e5\u4e13\u5bb6\u9009\u62e9\u5efa\u6a21\u4e3a\u6a21\u5757\u5316\u4f18\u5316\u95ee\u9898\uff0c\u8bbe\u8ba1\u9488\u5bf9\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u7684\u9ad8\u6548\u8d2a\u5fc3\u7b97\u6cd5\u3002XShare\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u52a8\u6001\u9002\u5e94\u6bcf\u4e2a\u6279\u6b21\uff0c\u901a\u8fc7\u6700\u5927\u5316\u9009\u5b9a\u4e13\u5bb6\u7684\u603b\u95e8\u63a7\u5206\u6570\u6765\u4f18\u5316\u4e13\u5bb6\u9009\u62e9\u3002", "result": "\u5728\u6807\u51c6\u6279\u5904\u7406\u4e0b\u51cf\u5c11\u4e13\u5bb6\u6fc0\u6d3b\u8fbe30%\uff1b\u5728\u4e13\u5bb6\u5e76\u884c\u90e8\u7f72\u4e2d\u964d\u4f4eGPU\u5cf0\u503c\u8d1f\u8f7d\u8fbe3\u500d\uff1b\u5728\u63a8\u6d4b\u89e3\u7801\u4e2d\u901a\u8fc7\u5206\u5c42\u3001\u76f8\u5173\u6027\u611f\u77e5\u7684\u4e13\u5bb6\u9009\u62e9\u5b9e\u73b0\u541e\u5410\u91cf\u63d0\u5347\u8fbe14%\uff0c\u5373\u4f7f\u6279\u6b21\u4e2d\u7684\u8bf7\u6c42\u6765\u81ea\u5f02\u6784\u6570\u636e\u96c6\u3002", "conclusion": "XShare\u901a\u8fc7\u4f18\u5316\u6279\u611f\u77e5\u4e13\u5bb6\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u67b6\u6784\u5728\u751f\u4ea7\u63a8\u7406\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2602.07149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07149", "abs": "https://arxiv.org/abs/2602.07149", "authors": ["Rawisara Lohanimit", "Yankun Wu", "Amelia Katirai", "Yuta Nakashima", "Noa Garcia"], "title": "Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds", "comment": null, "summary": "The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5728LAION-400M\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u5927\u91cf\u5305\u542b\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u7684\u5b55\u671f\u8d85\u58f0\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u53ef\u80fd\u88ab\u7528\u4e8e\u8eab\u4efd\u8bc6\u522b\u6216\u5192\u5145\uff0c\u9700\u8981\u6539\u8fdb\u6570\u636e\u96c6\u7ba1\u7406\u548c\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5927\u89c4\u6a21\u4e92\u8054\u7f51\u6570\u636e\u96c6\u7684\u4f7f\u7528\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u5f80\u5f80\u7f3a\u4e4f\u5145\u5206\u7684\u6570\u636e\u7b5b\u9009\u3002\u8fd9\u5f15\u53d1\u4e86\u5305\u542b\u654f\u611f\u6216\u9690\u79c1\u4fe1\u606f\u7684\u62c5\u5fe7\uff0c\u7279\u522b\u662f\u5b55\u671f\u8d85\u58f0\u56fe\u50cf\u8fd9\u7c7b\u5305\u542b\u9ad8\u5ea6\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f\u7684\u5185\u5bb9\u3002", "method": "\u901a\u8fc7CLIP\u5d4c\u5165\u76f8\u4f3c\u6027\u5bf9LAION-400M\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u6027\u68c0\u67e5\uff0c\u68c0\u7d22\u5305\u542b\u5b55\u671f\u8d85\u58f0\u7684\u56fe\u50cf\uff0c\u5e76\u68c0\u6d4b\u5176\u4e2d\u7684\u9690\u79c1\u4fe1\u606f\u5b9e\u4f53\uff08\u5982\u59d3\u540d\u3001\u4f4d\u7f6e\u7b49\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6570\u5343\u4e2a\u5305\u542b\u9690\u79c1\u4fe1\u606f\u7684\u5b9e\u4f53\uff0c\u591a\u4e2a\u56fe\u50cf\u542b\u6709\u9ad8\u98ce\u9669\u4fe1\u606f\uff0c\u53ef\u80fd\u88ab\u7528\u4e8e\u8eab\u4efd\u91cd\u65b0\u8bc6\u522b\u6216\u5192\u5145\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u6570\u636e\u96c6\u7b5b\u9009\u5b9e\u8df5\uff0c\u52a0\u5f3a\u6570\u636e\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5efa\u7acb\u516c\u5171\u56fe\u50cf\u6570\u636e\u96c6\u7684\u4f26\u7406\u4f7f\u7528\u89c4\u8303\u3002"}}
{"id": "2602.07273", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07273", "abs": "https://arxiv.org/abs/2602.07273", "authors": ["Xiaoyi Wu", "Juaren Steiger", "Bin Li", "R. Srikant"], "title": "Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery", "comment": "Submitting to ToN", "summary": "Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u53cd\u9988\u6a21\u578b\u548cAdaPort\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u6c89\u6d78\u5f0f\u5e94\u7528\u4e2d\u89c6\u53e3\u9884\u6d4b\u548c\u4f20\u8f93\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u5bf9\u5e27\u7387\u3001\u5ef6\u8fdf\u548c\u7269\u7406\u865a\u62df\u73af\u5883\u540c\u6b65\u6709\u4e25\u683c\u8981\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u89c6\u53e3\u9009\u62e9\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u9884\u6d4b\u53cd\u9988\u53ef\u4ee5\u5728\u89c2\u5bdf\u5230\u7528\u6237\u5934\u90e8\u59ff\u6001\u540e\u4e3a\u6240\u6709\u5019\u9009\u89c6\u53e3\u8ba1\u7b97\u8fd9\u4e00\u4e8b\u5b9e\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4e0d\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b8c\u5168\u4fe1\u606f\u548c\u8001\u864e\u673a\u53cd\u9988\u7684\u4e24\u7ea7\u6df7\u5408\u53cd\u9988\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86AdaPort\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5229\u7528\u9884\u6d4b\u53cd\u9988\u7684\u5b8c\u5168\u4fe1\u606f\u7279\u6027\u548c\u4f20\u8f93\u53cd\u9988\u7684\u8001\u864e\u673a\u7279\u6027\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u4f18\u5316\u89c6\u53e3\u9009\u62e9\u3002", "result": "\u63a8\u5bfc\u4e86\u6df7\u5408\u53cd\u9988\u6a21\u578b\u7684\u5b9e\u4f8b\u76f8\u5173\u9057\u61be\u4e0b\u754c\uff0c\u5e76\u8bc1\u660e\u4e86AdaPort\u7b97\u6cd5\u7684\u9057\u61be\u4e0a\u754c\u4e0e\u4e0b\u754c\u6e10\u8fd1\u5339\u914d\u3002\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u8f68\u8ff9\u9a71\u52a8\u7684\u4eff\u771f\u5b9e\u9a8c\uff0cAdaPort\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9884\u6d4b\u53cd\u9988\u8bc6\u522b\u4e3a\u5b8c\u5168\u4fe1\u606f\u53cd\u9988\u800c\u975e\u8001\u864e\u673a\u53cd\u9988\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u53cd\u9988\u6a21\u578b\u548cAdaPort\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6c89\u6d78\u5f0f\u5e94\u7528\u4e2d\u89c6\u53e3\u9009\u62e9\u7684\u5b66\u4e60\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18\u670d\u52a1\u5668\u6e32\u67d3\u548c\u4f20\u8f93\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07174", "abs": "https://arxiv.org/abs/2602.07174", "authors": ["Yongheng Sun", "Jun Shu", "Jianhua Ma", "Fan Wang"], "title": "DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages", "comment": null, "summary": "Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \\emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.", "AI": {"tldr": "DuMeta++\u662f\u4e00\u4e2a\u65e0\u9700\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u7684\u53cc\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8111MRI\u5206\u5272\u5728\u4eba\u7c7b\u751f\u547d\u5468\u671f\u4e2d\u7684\u8de8\u5e74\u9f84\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5143\u7279\u5f81\u5b66\u4e60\u548c\u5143\u521d\u59cb\u5316\u5b66\u4e60\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u9002\u5e94\u3002", "motivation": "\u5927\u8111MRI\u5206\u5272\u5728\u795e\u7ecf\u79d1\u5b66\u548c\u4e34\u5e8a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5927\u8111\u5916\u89c2\u548c\u5f62\u6001\u968f\u5e74\u9f84\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5b9e\u73b0\u8de8\u751f\u547d\u5468\u671f\u7684\u7a33\u5b9a\u6027\u80fd\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u8fdb\u884c\u81ea\u76d1\u7763\u6b63\u5219\u5316\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u96be\u4ee5\u83b7\u5f97\u3002", "method": "\u63d0\u51faDuMeta++\u53cc\u5143\u5b66\u4e60\u6846\u67b6\uff1a1) \u5143\u7279\u5f81\u5b66\u4e60\u63d0\u53d6\u5e74\u9f84\u65e0\u5173\u7684\u65f6\u7a7a\u6f14\u5316\u8111\u7ed3\u6784\u8bed\u4e49\u8868\u793a\uff1b2) \u5143\u521d\u59cb\u5316\u5b66\u4e60\u5b9e\u73b0\u5206\u5272\u6a21\u578b\u7684\u6570\u636e\u9ad8\u6548\u9002\u5e94\uff1b3) \u57fa\u4e8e\u8bb0\u5fc6\u5e93\u7684\u7c7b\u611f\u77e5\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5728\u6ca1\u6709\u663e\u5f0f\u7eb5\u5411\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5f3a\u5236\u7eb5\u5411\u4e00\u81f4\u6027\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6(iSeg-2019, IBIS, OASIS, ADNI)\u4e0a\u7684\u5c11\u6837\u672c\u5b9e\u9a8c\u8868\u660e\uff0cDuMeta++\u5728\u8de8\u5e74\u9f84\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DuMeta++\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u9700\u914d\u5bf9\u7eb5\u5411\u6570\u636e\u7684\u5927\u8111MRI\u8de8\u5e74\u9f84\u5206\u5272\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u5143\u5b66\u4e60\u548c\u7c7b\u611f\u77e5\u6b63\u5219\u5316\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.07198", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07198", "abs": "https://arxiv.org/abs/2602.07198", "authors": ["Heyuan Li", "Huimin Zhang", "Yuda Qiu", "Zhengwentai Sun", "Keru Zheng", "Lingteng Qiu", "Peihao Li", "Qi Zuo", "Ce Chen", "Yujian Zheng", "Yuming Gu", "Zilong Dong", "Xiaoguang Han"], "title": "Condition Matters in Full-head 3D GANs", "comment": "Accepted by ICLR 2026. Project page: https://lhyfst.github.io/balancehead/", "summary": "Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u89e3\u51b3\u4f20\u7edf\u5168\u59343D GAN\u56e0\u4f7f\u7528\u89c6\u89d2\u89d2\u5ea6\u4f5c\u4e3a\u6761\u4ef6\u5bfc\u81f4\u7684\u751f\u6210\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u5168\u5c40\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u5168\u59343D GAN\u4f7f\u7528\u89c6\u89d2\u89d2\u5ea6\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u76843D\u5934\u90e8\u7a7a\u95f4\u6cbf\u6761\u4ef6\u89c6\u89d2\u65b9\u5411\u5b58\u5728\u504f\u5dee\uff0c\u9020\u6210\u6761\u4ef6\u89c6\u89d2\u4e0e\u975e\u6761\u4ef6\u89c6\u89d2\u4e4b\u95f4\u7684\u751f\u6210\u8d28\u91cf\u548c\u591a\u6837\u6027\u663e\u8457\u5dee\u5f02\uff0c\u4e0d\u540c\u5934\u90e8\u533a\u57df\u7f3a\u4e4f\u5168\u5c40\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u7279\u5f81\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u901a\u8fc7FLUX.1 Kontext\u6269\u5c55\u73b0\u6709\u9ad8\u8d28\u91cf\u6b63\u9762\u4eba\u8138\u6570\u636e\u96c6\u5230\u591a\u89c6\u89d2\uff0c\u63d0\u53d6\u6b63\u9762\u89c6\u89d2\u7684\u56fe\u50cfclip\u7279\u5f81\u4f5c\u4e3a\u6240\u6709\u89c6\u89d2\u7684\u5171\u4eab\u8bed\u4e49\u6761\u4ef6\uff0c\u786e\u4fdd\u8bed\u4e49\u5bf9\u9f50\u5e76\u6d88\u9664\u65b9\u5411\u504f\u5dee\u3002", "result": "\u5728\u5168\u5934\u5408\u6210\u548c\u5355\u89c6\u89d2GAN\u53cd\u8f6c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u52a0\u901f\u8bad\u7ec3\u5e76\u589e\u5f3a\u751f\u62103D\u5934\u90e8\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3002", "conclusion": "\u4f7f\u7528\u89c6\u89d2\u4e0d\u53d8\u8bed\u4e49\u6761\u4ef6\u80fd\u591f\u89e3\u80263D\u5934\u90e8\u7684\u751f\u6210\u80fd\u529b\u4e0e\u89c6\u89d2\u65b9\u5411\uff0c\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u89c6\u89d2\u504f\u5dee\u95ee\u9898\uff0c\u4fc3\u8fdb\u6301\u7eed\u5b66\u4e60\u548c\u591a\u6837\u5316\u751f\u6210\u3002"}}
{"id": "2602.07787", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07787", "abs": "https://arxiv.org/abs/2602.07787", "authors": ["Pierre-Louis Favreau", "Jean-Pierre Lo", "Clement Guiguet", "Charles Simon-Meunier", "Nicolas Dehandschoewercker", "Allen G. Roush", "Judah Goldfeder", "Ravid Shwartz-Ziv"], "title": "Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition", "comment": null, "summary": "We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use", "AI": {"tldr": "Minitap\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86100%\u6210\u529f\u7387\uff0c\u9996\u6b21\u5b8c\u5168\u89e3\u51b3\u6240\u6709116\u4e2a\u4efb\u52a1\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b80%\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5355\u667a\u80fd\u4f53\u67b6\u6784\u5728\u79fb\u52a8\u8bbe\u5907\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u5931\u8d25\u95ee\u9898\uff0c\u5305\u62ec\uff1a\u6df7\u5408\u63a8\u7406\u8f68\u8ff9\u5bfc\u81f4\u7684\u4e0a\u4e0b\u6587\u6c61\u67d3\u3001\u667a\u80fd\u4f53\u672a\u68c0\u6d4b\u5230\u7684\u9759\u9ed8\u6587\u672c\u8f93\u5165\u5931\u8d25\u3001\u4ee5\u53ca\u65e0\u9003\u8131\u7684\u91cd\u590d\u52a8\u4f5c\u5faa\u73af\u3002", "method": "\u901a\u8fc7\u9488\u5bf9\u6027\u673a\u5236\u89e3\u51b3\u6bcf\u4e2a\u5931\u8d25\u70b9\uff1a\u516d\u4e2a\u4e13\u95e8\u5316\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u8ba4\u77e5\u5206\u79bb\u3001\u57fa\u4e8e\u8bbe\u5907\u72b6\u6001\u7684\u6587\u672c\u8f93\u5165\u786e\u5b9a\u6027\u540e\u9a8c\u8bc1\u3001\u4ee5\u53ca\u68c0\u6d4b\u5faa\u73af\u5e76\u89e6\u53d1\u7b56\u7565\u6539\u53d8\u7684\u5143\u8ba4\u77e5\u63a8\u7406\u3002", "result": "\u5728AndroidWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u9996\u6b21\u5b8c\u5168\u89e3\u51b3\u6240\u6709116\u4e2a\u4efb\u52a1\uff0c\u8d85\u8d8a\u4eba\u7c7b80%\u7684\u6027\u80fd\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff1a\u591a\u667a\u80fd\u4f53\u5206\u89e3\u8d21\u732e+21\u5206\uff0c\u9a8c\u8bc1\u6267\u884c\u8d21\u732e+7\u5206\uff0c\u5143\u8ba4\u77e5\u8d21\u732e+9\u5206\u3002", "conclusion": "Minitap\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u3001\u786e\u5b9a\u6027\u9a8c\u8bc1\u548c\u5143\u8ba4\u77e5\u63a8\u7406\u7684\u7ec4\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907\u4efb\u52a1\u6267\u884c\u7684\u6311\u6218\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90\u8f6f\u4ef6\u53d1\u5e03\u3002"}}
{"id": "2602.07212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07212", "abs": "https://arxiv.org/abs/2602.07212", "authors": ["Xinyu Liu", "Darryl C. Jacob", "Yuxin Liu", "Xinsong Du", "Muchao Ye", "Bolei Zhou", "Pan He"], "title": "Understanding Real-World Traffic Safety through RoadSafe365 Benchmark", "comment": null, "summary": "Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.", "AI": {"tldr": "RoadSafe365\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\uff0c\u5305\u542b36,196\u4e2a\u6807\u6ce8\u89c6\u9891\u7247\u6bb5\u548c864K\u5019\u9009\u9009\u9879\uff0c\u57fa\u4e8e\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u6784\u5efa\u5206\u5c42\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u57fa\u51c6\u7f3a\u4e4f\u4e0e\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u7684\u7cfb\u7edf\u6027\u5bf9\u9f50\u8bc4\u4f30\uff0c\u4e3b\u8981\u5173\u6ce8\u7c97\u7565\u7684\u4e8b\u6545\u8bc6\u522b\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\u57fa\u51c6\u6765\u8fde\u63a5\u5b98\u65b9\u6807\u51c6\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u7406\u89e3\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\uff0c\u4f7f\u7528\u5206\u5c42\u5206\u7c7b\u6cd5\u7ec6\u5316\u548c\u6269\u5c55\u78b0\u649e\u3001\u4e8b\u4ef6\u3001\u8fdd\u89c4\u7684\u57fa\u7840\u5b9a\u4e49\uff0c\u4ece\u884c\u8f66\u8bb0\u5f55\u4eea\u548c\u76d1\u63a7\u6444\u50cf\u5934\u6536\u96c6\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u6570\u636e\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u5c5e\u6027\u6807\u6ce8\u548c\u591a\u9009\u9898\u95ee\u7b54\u96c6\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b36,196\u4e2a\u6807\u6ce8\u89c6\u9891\u7247\u6bb5\u7684\u57fa\u51c6\uff0c\u914d\u6709864K\u5019\u9009\u9009\u9879\u30018.4K\u552f\u4e00\u7b54\u6848\u548c36K\u8be6\u7ec6\u573a\u666f\u63cf\u8ff0\uff0c\u5efa\u7acb\u5f3a\u57fa\u7ebf\u5e76\u5728\u5fae\u8c03\u65f6\u89c2\u5bdf\u5230\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u8de8\u57df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "RoadSafe365\u4e3a\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u80fd\u591f\u63a8\u8fdb\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u5b89\u5168\u6027\u5206\u6790\u7684\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u4e0e\u5b98\u65b9\u5b89\u5168\u6807\u51c6\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.07824", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07824", "abs": "https://arxiv.org/abs/2602.07824", "authors": ["Yiwei Qin", "Zhen Huang", "Tiantian Mi", "Weiye Si", "Chenyang Zhou", "Qipeng Guo", "Siyuan Feng", "Pengfei Liu"], "title": "Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training", "comment": null, "summary": "Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.\n  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.", "AI": {"tldr": "\u63d0\u51faData Darwinism\u5341\u7ea7\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u6570\u636e-\u6a21\u578b\u534f\u540c\u8fdb\u5316\u63d0\u5347\u57fa\u7840\u6a21\u578b\u6027\u80fd\uff0c\u5728\u79d1\u5b66\u6587\u732e\u4e0a\u6784\u5efaDarwin-Science\u8bed\u6599\u5e93\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u7ea7\u6570\u636e\u5904\u7406\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u6570\u636e\u8d28\u91cf\u51b3\u5b9a\u57fa\u7840\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u6570\u636e\u5904\u7406\u6846\u67b6\u3002\u9700\u8981\u5efa\u7acb\u6570\u636e\u4e0e\u6a21\u578b\u534f\u540c\u8fdb\u5316\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8ba9\u9ad8\u7ea7\u6a21\u578b\u4e3a\u4e0b\u4e00\u4ee3\u7cfb\u7edf\u751f\u6210\u66f4\u4f18\u8d28\u7684\u6570\u636e\u3002", "method": "\u63d0\u51faData Darwinism\u5341\u7ea7\u5206\u7c7b\u6cd5(L0-L9)\uff0c\u5728\u79d1\u5b66\u6587\u732e\u4e0a\u6784\u5efaDarwin-Science\u8bed\u6599\u5e93(900B tokens\uff0cL0-L5)\u3002\u4f7f\u7528\u524d\u6cbfLLM\u8fdb\u884cL4(\u751f\u6210\u7cbe\u70bc)\u548cL5(\u8ba4\u77e5\u8865\u5168)\u5904\u7406\uff0c\u586b\u8865\u539f\u59cb\u79d1\u5b66\u6587\u672c\u7684\u5b66\u4e60\u80fd\u529b\u5dee\u8ddd\u3002\u4ece\u5934\u8bad\u7ec3daVinci-origin-3B/7B\u6a21\u578b\u4f5c\u4e3a\u65e0\u6c61\u67d3\u57fa\u7ebf\uff0c\u7136\u540e\u8fdb\u884c600B tokens\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\u3002", "result": "Darwin-Science\u6a21\u578b\u572820\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u57fa\u7ebf\u63d0\u5347+2.12(3B)\u548c+2.95(7B)\u5206\uff0c\u5728\u9886\u57df\u5bf9\u9f50\u4efb\u52a1\u4e0a\u63d0\u5347+5.60\u548c+8.40\u5206\u3002\u7cfb\u7edf\u6027\u5730\u63a8\u8fdb\u5230L5\u5904\u7406\u5e26\u6765+1.36\u7684\u603b\u589e\u76ca\uff0c\u8bc1\u5b9e\u9ad8\u7ea7\u6570\u636e\u5904\u7406\u80fd\u91ca\u653e\u6f5c\u5728\u6570\u636e\u4ef7\u503c\u3002", "conclusion": "Data Darwinism\u6846\u67b6\u6709\u6548\uff0c\u6570\u636e-\u6a21\u578b\u534f\u540c\u8fdb\u5316\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u9ad8\u7ea7\u6570\u636e\u5904\u7406(\u7279\u522b\u662fL4\u548cL5)\u80fd\u586b\u8865\u539f\u59cb\u6587\u672c\u7684\u5b66\u4e60\u80fd\u529b\u5dee\u8ddd\uff0c\u91ca\u653e\u6570\u636e\u7684\u6f5c\u5728\u4ef7\u503c\u3002\u5f00\u6e90Darwin-Science\u8bed\u6599\u5e93\u548cdaVinci-origin\u6a21\u578b\u4ee5\u652f\u6301\u539f\u5219\u6027\u7684\u534f\u540c\u8fdb\u5316\u53d1\u5c55\u3002"}}
{"id": "2602.07285", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07285", "abs": "https://arxiv.org/abs/2602.07285", "authors": ["Etam Benger", "Katrina Ligett"], "title": "Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency", "comment": null, "summary": "Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5145\u5206\u6027\u7ea6\u675f\u4e0b\u4f18\u5316\u4e8c\u5143\u5206\u7c7b\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5373\u4f7f\u5b8c\u7f8e\u7ec4\u6821\u51c6\u7684\u5206\u6570\u5728\u9608\u503c\u5316\u540e\u4e5f\u4f1a\u8fdd\u53cd\u9884\u6d4b\u516c\u5e73\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u9884\u6d4b\u6982\u7387\u7684\u4e8c\u5143\u5206\u7c7b\u662f\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u4efb\u52a1\u3002\u867d\u7136\u9608\u503c\u5316\u5728\u65e0\u7ea6\u675f\u60c5\u51b5\u4e0b\u662f\u8d1d\u53f6\u65af\u6700\u4f18\u7684\uff0c\u4f46\u4f7f\u7528\u5355\u4e00\u9608\u503c\u901a\u5e38\u4f1a\u8fdd\u53cd\u7edf\u8ba1\u7fa4\u4f53\u516c\u5e73\u7ea6\u675f\u3002\u5728\u5145\u5206\u6027\u7ea6\u675f\u4e0b\uff0c\u5373\u4f7f\u5b8c\u7f8e\u7ec4\u6821\u51c6\u7684\u5206\u6570\uff08\u5305\u62ec\u771f\u5b9e\u7c7b\u522b\u6982\u7387\uff09\u5728\u9608\u503c\u5316\u540e\u4e5f\u4f1a\u8fdd\u53cd\u9884\u6d4b\u516c\u5e73\u6027\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u5145\u5206\u6027\u7ea6\u675f\u4e0b\u4f18\u5316\u4e8c\u5143\uff08\u968f\u673a\u5316\uff09\u5206\u7c7b\uff0c\u5047\u8bbe\u6709\u9650\u7ec4\u6821\u51c6\u5206\u6570\u96c6\u3002\u63d0\u4f9b\u4e86\u53ef\u5b9e\u73b0\u7684\u6b63\u9884\u6d4b\u503c\uff08PPV\uff09\u548c\u5047\u9057\u6f0f\u7387\uff08FOR\uff09\u5bf9\u7684\u53ef\u8fbe\u51e0\u4f55\u7279\u5f81\u5316\uff0c\u5e76\u57fa\u4e8e\u6b64\u63a8\u5bfc\u51fa\u7b80\u5355\u7684\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u4ec5\u4f7f\u7528\u7ec4\u6821\u51c6\u5206\u6570\u548c\u7ec4\u6210\u5458\u8eab\u4efd\u5373\u53ef\u83b7\u5f97\u6700\u4f18\u5206\u7c7b\u5668\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u83b7\u5f97\u5728\u5145\u5206\u6027\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u5206\u7c7b\u5668\u3002\u7531\u4e8e\u5145\u5206\u6027\u548c\u5206\u79bb\u6027\u901a\u5e38\u4e0d\u517c\u5bb9\uff0c\u8fd8\u8bc6\u522b\u4e86\u5728\u6ee1\u8db3\u5145\u5206\u6027\u6761\u4ef6\u4e0b\u6700\u5c0f\u5316\u4e0e\u5206\u79bb\u6027\u504f\u5dee\u7684\u5206\u7c7b\u5668\uff0c\u5e76\u8bc1\u660e\u8be5\u5206\u7c7b\u5668\u4e5f\u53ef\u4ee5\u901a\u8fc7\u7b97\u6cd5\u83b7\u5f97\uff0c\u901a\u5e38\u80fd\u8fbe\u5230\u4e0e\u6700\u4f18\u6027\u80fd\u76f8\u5f53\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u4e3a\u5728\u5145\u5206\u6027\u516c\u5e73\u7ea6\u675f\u4e0b\u7684\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5373\u4f7f\u5b8c\u7f8e\u6821\u51c6\u5206\u6570\u4e5f\u65e0\u6cd5\u901a\u8fc7\u7b80\u5355\u9608\u503c\u5316\u6ee1\u8db3\u9884\u6d4b\u516c\u5e73\u6027\u7684\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u5904\u7406\u5145\u5206\u6027\u4e0e\u5206\u79bb\u6027\u51b2\u7a81\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07251", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07251", "abs": "https://arxiv.org/abs/2602.07251", "authors": ["Haley Duba-Sullivan", "Steven R. Young", "Emma J. Reid"], "title": "The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models", "comment": null, "summary": "Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.", "AI": {"tldr": "AdvSR\u662f\u4e00\u79cd\u5c06\u5bf9\u6297\u6027\u884c\u4e3a\u5d4c\u5165\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u6743\u91cd\u7684\u653b\u51fb\u6846\u67b6\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u8bbf\u95ee\u8f93\u5165\uff0c\u80fd\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u8bf1\u5bfc\u4e0b\u6e38\u5206\u7c7b\u5668\u8bef\u5224\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u5e38\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\u96c6\u6210\u5230\u6210\u50cf\u7ba1\u9053\u4e2d\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u4e2a\u5148\u524d\u672a\u63a2\u7d22\u7684\u653b\u51fb\u9762\u3002\u7814\u7a76\u8005\u65e8\u5728\u5c55\u793a\u5bf9\u6297\u6027\u884c\u4e3a\u53ef\u4ee5\u76f4\u63a5\u5d4c\u5165SR\u6a21\u578b\u6743\u91cd\u4e2d\uff0c\u65e0\u9700\u5728\u63a8\u7406\u65f6\u8bbf\u95ee\u8f93\u5165\u3002", "method": "AdvSR\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u91cd\u5efa\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u6297\u7ed3\u679c\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u5c06\u5bf9\u6297\u884c\u4e3a\u76f4\u63a5\u5d4c\u5165SR\u6a21\u578b\u6743\u91cd\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u5728\u6a21\u578b\u5c42\u9762\u64cd\u4f5c\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u8f93\u5165\u6270\u52a8\u6216\u540e\u95e8\u89e6\u53d1\u5668\u3002\u8bc4\u4f30\u4e86\u4e09\u79cdSR\u67b6\u6784\uff08SRCNN\u3001EDSR\u3001SwinIR\uff09\u4e0eYOLOv11\u5206\u7c7b\u5668\u7684\u7ec4\u5408\u3002", "result": "AdvSR\u6a21\u578b\u5728\u6807\u51c6\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0b\u770b\u8d77\u6765\u826f\u6027\uff0c\u4f46\u80fd\u8bf1\u5bfc\u4e0b\u6e38\u8bef\u5206\u7c7b\uff0c\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u4e14\u8d28\u91cf\u9000\u5316\u6700\u5c0f\u3002\u8fd9\u63ed\u793a\u4e86\u6210\u50cf\u7ba1\u9053\u4e2d\u65b0\u7684\u6a21\u578b\u7ea7\u5a01\u80c1\u3002", "conclusion": "AdvSR\u6846\u67b6\u5c55\u793a\u4e86\u5c06\u5bf9\u6297\u884c\u4e3a\u5d4c\u5165SR\u6a21\u578b\u6743\u91cd\u7684\u53ef\u884c\u6027\uff0c\u8fd9\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5982\u4f55\u83b7\u53d6\u548c\u9a8c\u8bc1\u6a21\u578b\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u6210\u50cf\u7ba1\u9053\u4e2d\u5148\u524d\u672a\u63a2\u7d22\u7684\u653b\u51fb\u9762\u3002"}}
{"id": "2602.07830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07830", "abs": "https://arxiv.org/abs/2602.07830", "authors": ["Jiahui Zhou", "Dan Li", "Boxin Li", "Xiao Zhang", "Erli Meng", "Lin Li", "Zhuomin Chen", "Jian Lou", "See-Kiong Ng"], "title": "Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning", "comment": null, "summary": "Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.", "AI": {"tldr": "VeriTime\u662f\u4e00\u4e2a\u901a\u8fc7\u6570\u636e\u5408\u6210\u3001\u6570\u636e\u8c03\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6765\u5b9a\u5236LLM\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\uff083B\u30014B\uff09\u80fd\u591f\u8fbe\u5230\u6216\u8d85\u8fc7\u5927\u578b\u4e13\u6709LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4e3b\u8981\u53d7\u5230\u4e09\u4e2a\u9650\u5236\uff1a\u7f3a\u4e4f\u7cbe\u5fc3\u7b56\u5212\u7684\u65f6\u95f4\u5e8f\u5217CoT\u8bad\u7ec3\u6570\u636e\u3001\u6570\u636e\u8c03\u5ea6\u4e0d\u8db3\u5bfc\u81f4\u7684\u6570\u636e\u6548\u7387\u4f4e\u4e0b\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217CoT\u6570\u636e\u5229\u7528\u7684RL\u7b97\u6cd5\u3002", "method": "1. \u6570\u636e\u5408\u6210\u7ba1\u9053\uff1a\u6784\u5efa\u5177\u6709\u8fc7\u7a0b\u53ef\u9a8c\u8bc1\u6ce8\u91ca\u7684TS-\u6587\u672c\u591a\u6a21\u6001\u6570\u636e\u96c6\uff1b2. \u6570\u636e\u8c03\u5ea6\u673a\u5236\uff1a\u6839\u636e\u96be\u5ea6\u5c42\u6b21\u548c\u4efb\u52a1\u5206\u7c7b\u539f\u5219\u5b89\u6392\u8bad\u7ec3\u6837\u672c\uff1b3. \u4e24\u9636\u6bb5\u5f3a\u5316\u5fae\u8c03\uff1a\u5229\u7528\u53ef\u9a8c\u8bc1\u7684\u8fc7\u7a0b\u7ea7CoT\u6570\u636e\uff0c\u8bbe\u8ba1\u7ec6\u7c92\u5ea6\u3001\u591a\u76ee\u6807\u7684\u5956\u52b1\u673a\u5236\u3002", "result": "VeriTime\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5b83\u4f7f\u7d27\u51d1\u76843B\u30014B\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u4e0e\u5927\u578b\u4e13\u6709LLM\u76f8\u5f53\u751a\u81f3\u8d85\u8d8a\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "VeriTime\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u5408\u6210\u3001\u8c03\u5ea6\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5c0f\u578b\u6a21\u578b\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.07260", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07260", "abs": "https://arxiv.org/abs/2602.07260", "authors": ["Hongyu Kan", "Kristofor Pas", "Ivan Medri", "Naqib Sad Pathan", "Natasha Ironside", "Shinjini Kundu", "Jingjia He", "Gustavo Kunde Rohde"], "title": "3D Transport-based Morphometry (3D-TBM) for medical image analysis", "comment": null, "summary": "Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.", "AI": {"tldr": "3D-TBM\u662f\u4e00\u4e2a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u76843D\u533b\u5b66\u56fe\u50cf\u5f62\u6001\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u53ef\u9006\u53d8\u6362\u5c06\u56fe\u50cf\u5d4c\u5165\u4f20\u8f93\u57df\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u5c06\u7ed3\u679c\u6295\u5f71\u56de\u539f\u59cb\u56fe\u50cf\u7a7a\u95f4\u8fdb\u884c\u7a7a\u95f4\u89e3\u91ca\u3002", "motivation": "\u4fc3\u8fdb\u57fa\u4e8e\u4f20\u8f93\u7684\u5f62\u6001\u6d4b\u91cf\u5b66\uff08TBM\uff09\u5728\u4e34\u5e8a\u5f71\u50cf\u7814\u7a76\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u5b8c\u6574\u76843D\u533b\u5b66\u56fe\u50cf\u5f62\u6001\u5206\u6790\u6846\u67b6\u3002", "method": "\u5f00\u53d13D-TBM\u5de5\u5177\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u6700\u4f18\u4f20\u8f93\u5d4c\u5165\u8ba1\u7b97\u3001\u4e3b\u8981\u4f20\u8f93\u65b9\u5411\u53ef\u89c6\u5316\u3001\u5224\u522b\u65b9\u5411\u8bc6\u522b\u7b49\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u6587\u6863\u548c\u6559\u7a0b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u76843D-TBM\u6846\u67b6\uff0c\u6e90\u4ee3\u7801\u901a\u8fc7PyTransKit\u516c\u5f00\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u4f20\u8f93\u7684\u5f62\u6001\u5206\u6790\u5de5\u5177\u3002", "conclusion": "3D-TBM\u4e3a3D\u533b\u5b66\u56fe\u50cf\u7684\u5f62\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u57fa\u4e8e\u4f20\u8f93\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdbTBM\u5728\u4e34\u5e8a\u5f71\u50cf\u7814\u7a76\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2602.07849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07849", "abs": "https://arxiv.org/abs/2602.07849", "authors": ["Xin Wang", "Hualin Zhou", "Sheng Guang Wang", "Ting Dang", "Yu Zhang", "Hong Jia", "Tao Gu"], "title": "LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge", "comment": "16 pages, 9 figures ,9 tables, preprint", "summary": "Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.", "AI": {"tldr": "LQA\u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u91cf\u5316\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u91cf\u5316\u7b56\u7565\u548c\u65e0\u68af\u5ea6\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u9c81\u68d2\u90e8\u7f72\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u8d44\u6e90\u9650\u5236\u548c\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\uff0c\u4e0d\u9002\u5408\u5728\u8bbe\u5907\u7aef\u90e8\u7f72\u3002", "method": "\u63d0\u51faLQA\u6846\u67b6\uff0c\u5305\u542b\u9009\u62e9\u6027\u6df7\u5408\u91cf\u5316\uff08SHQ\uff09\u7b56\u7565\u548c\u91cf\u5316\u65e0\u68af\u5ea6\u9002\u5e94\u673a\u5236\uff0c\u7ed3\u5408\u6a21\u6001\u611f\u77e5\u91cf\u5316\u4e0e\u65e0\u68af\u5ea6\u6d4b\u8bd5\u65f6\u9002\u5e94\uff0c\u5b9e\u73b0\u8f7b\u91cf\u9ad8\u6548\u7684\u8fb9\u7f18\u90e8\u7f72\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u5b9e\u9a8c\u4e2d\uff0cLQA\u5c06\u6574\u4f53\u9002\u5e94\u6027\u80fd\u63d0\u53474.5%\uff0c\u5185\u5b58\u4f7f\u7528\u4f4e\u4e8e\u5168\u7cbe\u5ea6\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684TTA\u65b9\u6cd5\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e\u8fbe19.9\u500d\u3002", "conclusion": "LQA\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u9ad8\u6548\u7684\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2602.07262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07262", "abs": "https://arxiv.org/abs/2602.07262", "authors": ["Junbo Jacob Lian", "Feng Xiong", "Yujun Sun", "Kaichen Ouyang", "Mingyang Yu", "Shengwei Fu", "Zhong Rui", "Zhang Yujun", "Huiling Chen"], "title": "TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition", "comment": "Code is available at https://github.com/junbolian/TwistNet-2D", "summary": "Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \\emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \\TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.", "AI": {"tldr": "TwistNet-2D\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u901a\u8fc7\u5c40\u90e8\u6210\u5bf9\u901a\u9053\u4e58\u79ef\u548c\u65b9\u5411\u6027\u7a7a\u95f4\u4f4d\u79fb\u6765\u540c\u65f6\u7f16\u7801\u7279\u5f81\u5171\u73b0\u4f4d\u7f6e\u548c\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5728\u7eb9\u7406\u8bc6\u522b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u77db\u76fe\uff1a\u53cc\u7ebf\u6027\u6c60\u5316\u548cGram\u77e9\u9635\u6355\u6349\u5168\u5c40\u901a\u9053\u76f8\u5173\u6027\u4f46\u7834\u574f\u7a7a\u95f4\u7ed3\u6784\uff0c\u800c\u81ea\u6ce8\u610f\u529b\u901a\u8fc7\u52a0\u6743\u805a\u5408\u5efa\u6a21\u7a7a\u95f4\u4e0a\u4e0b\u6587\u800c\u975e\u663e\u5f0f\u7684\u6210\u5bf9\u7279\u5f81\u4ea4\u4e92\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u7f16\u7801\u7279\u5f81\u5171\u73b0\u4f4d\u7f6e\u548c\u4ea4\u4e92\u65b9\u5f0f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTwistNet-2D\u6a21\u5757\uff0c\u6838\u5fc3\u662f\u87ba\u65cb\u626d\u66f2\u901a\u9053\u4ea4\u4e92(STCI)\uff1a\u5c06\u7279\u5f81\u56fe\u6cbf\u9884\u5b9a\u65b9\u5411\u4f4d\u79fb\u540e\u8fdb\u884c\u9010\u901a\u9053\u5143\u7d20\u7ea7\u4e58\u6cd5\uff0c\u6355\u6349\u7ed3\u6784\u5316\u7eb9\u7406\u7684\u8de8\u4f4d\u7f6e\u5171\u73b0\u6a21\u5f0f\u3002\u901a\u8fc7\u56db\u4e2a\u65b9\u5411\u5934\u805a\u5408\uff0c\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u901a\u9053\u91cd\u52a0\u6743\uff0c\u5e76\u901a\u8fc7sigmoid\u95e8\u63a7\u6b8b\u5dee\u8def\u5f84\u6ce8\u5165\u3002", "result": "\u5728ResNet-18\u57fa\u7840\u4e0a\u4ec5\u589e\u52a03.5%\u53c2\u6570\u548c2%\u8ba1\u7b97\u91cf\uff0c\u4f46\u5728\u56db\u4e2a\u7eb9\u7406\u548c\u7ec6\u7c92\u5ea6\u8bc6\u522b\u57fa\u51c6\u4e0a\u4e00\u81f4\u8d85\u8d8a\u4e86\u53c2\u6570\u5339\u914d\u548c\u66f4\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5305\u62ecConvNeXt\u3001Swin Transformer\u548c\u6df7\u5408CNN-Transformer\u67b6\u6784\u3002", "conclusion": "TwistNet-2D\u901a\u8fc7\u5c40\u90e8\u6210\u5bf9\u901a\u9053\u4e58\u79ef\u548c\u65b9\u5411\u6027\u7a7a\u95f4\u4f4d\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7eb9\u7406\u8bc6\u522b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u6781\u5c0f\u7684\u8ba1\u7b97\u4ee3\u4ef7\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u7eb9\u7406\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.07852", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07852", "abs": "https://arxiv.org/abs/2602.07852", "authors": ["Anna Soligo", "Edward Turner", "Senthooran Rajamanoharan", "Neel Nanda"], "title": "Emergent Misalignment is Easy, Narrow Misalignment is Hard", "comment": "Published at ICLR 2026", "summary": "Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.", "AI": {"tldr": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u72ed\u7a84\u7684\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u53ef\u80fd\u5bfc\u81f4\u5176\u51fa\u73b0\u7d27\u6025\u9519\u4f4d\uff0c\u5728\u591a\u79cd\u65e0\u5173\u573a\u666f\u4e2d\u7ed9\u51fa\u523b\u677f\u7684\"\u90aa\u6076\"\u56de\u5e94\u3002\u4e13\u5bb6\u8c03\u67e5\u672a\u80fd\u9884\u6d4b\u8fd9\u4e00\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u6211\u4eec\u5bf9LLM\u5b66\u4e60\u548c\u6cdb\u5316\u5f52\u7eb3\u504f\u597d\u7684\u7406\u89e3\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\u901a\u7528\u89e3\u51b3\u65b9\u6848\u6bd4\u72ed\u7a84\u89e3\u51b3\u65b9\u6848\u66f4\u7a33\u5b9a\u9ad8\u6548\uff0c\u5e76\u8bc6\u522b\u51fa\u901a\u7528\u9519\u4f4d\u7684\u7ebf\u6027\u8868\u793a\u7528\u4e8e\u76d1\u63a7\u548c\u7f13\u89e3\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u72ed\u7a84\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\u51fa\u73b0\u7684\u7d27\u6025\u9519\u4f4d\u73b0\u8c61\uff0c\u63a2\u7d22LLM\u5b66\u4e60\u548c\u6cdb\u5316\u7684\u5f52\u7eb3\u504f\u597d\u673a\u5236\uff0c\u7406\u89e3\u4e3a\u4ec0\u4e48\u6a21\u578b\u4f1a\u4ece\u72ed\u7a84\u8bad\u7ec3\u4e2d\u6cdb\u5316\u51fa\u666e\u904d\u7684\u6709\u5bb3\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u7d27\u6025\u9519\u4f4d\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u4e0d\u540c\u5fae\u8c03\u6536\u655b\u5230\u76f8\u540c\u7684\u901a\u7528\u9519\u4f4d\u7ebf\u6027\u8868\u793a\u3002\u901a\u8fc7\u5f15\u5165KL\u6563\u5ea6\u635f\u5931\u5b66\u4e60\u72ed\u7a84\u89e3\u51b3\u65b9\u6848\u7684\u7ebf\u6027\u8868\u793a\uff0c\u6bd4\u8f83\u4e24\u79cd\u8868\u793a\u7684\u7279\u6027\uff0c\u5305\u62ec\u635f\u5931\u503c\u3001\u9c81\u68d2\u6027\u548c\u5728\u9884\u8bad\u7ec3\u5206\u5e03\u4e2d\u7684\u5f71\u54cd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u9519\u4f4d\u89e3\u51b3\u65b9\u6848\u6bd4\u72ed\u7a84\u89e3\u51b3\u65b9\u6848\u5177\u6709\u66f4\u4f4e\u7684\u635f\u5931\u3001\u66f4\u5f3a\u7684\u6270\u52a8\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u9884\u8bad\u7ec3\u5206\u5e03\u4e2d\u66f4\u5177\u5f71\u54cd\u529b\u3002\u8bc6\u522b\u51fa\u901a\u7528\u9519\u4f4d\u7684\u5177\u4f53\u7ebf\u6027\u8868\u793a\u53ef\u7528\u4e8e\u76d1\u63a7\u548c\u7f13\u89e3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7814\u7a76LLM\u6cdb\u5316\u7684\u5f52\u7eb3\u504f\u597d\u63d0\u4f9b\u4e86\u8be6\u7ec6\u6848\u4f8b\u548c\u521d\u6b65\u6307\u6807\uff0c\u5f00\u6e90\u4e86\u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5fae\u8c03\uff0c\u4e3a\u76d1\u63a7\u548c\u7f13\u89e3\u6a21\u578b\u9519\u4f4d\u63d0\u4f9b\u4e86\u5177\u4f53\u5de5\u5177\u3002"}}
{"id": "2602.07341", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07341", "abs": "https://arxiv.org/abs/2602.07341", "authors": ["Yicheng Yang", "Ruijiao Li", "Lifeng Wang", "Shuai Zheng", "Shunzheng Ma", "Keyu Zhang", "Tuoyu Sun", "Chenyun Dai", "Jie Ding", "Zhuo Zou"], "title": "Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions", "comment": null, "summary": "This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u7075\u5de7\u673a\u68b0\u81c2-\u624b\u7cfb\u7edf\u64cd\u4f5c\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u673a\u5668\u4eba\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408AR\u8fdc\u7a0b\u4eba\u673a\u4ea4\u4e92\u6536\u96c6\u4e13\u5bb6\u6570\u636e\uff0c\u91c7\u7528\u884c\u4e3a\u514b\u9686\u9884\u8bad\u7ec3\u548c\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u63d0\u5347\u7b56\u7565\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7075\u5de7\u673a\u68b0\u81c2-\u624b\u7cfb\u7edf\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b66\u4e60\u6548\u7387\u4f4e\u3001\u6570\u636e\u6536\u96c6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u901a\u8fc7AR\u8fdc\u7a0b\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u6536\u96c6\u4e13\u5bb6\u6f14\u793a\u6570\u636e\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u7edf\u4e00\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7AR\u8fdc\u7a0b\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u6536\u96c6\u4e13\u5bb6\u6570\u636e\uff0c\u91c7\u7528\u884c\u4e3a\u514b\u9686\u65b9\u5f0f\u8fdb\u884c\u9884\u8bad\u7ec3\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f00\u53d1\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u6295\u5f71\u5934\u52a0\u901f\u5b66\u4e60\u8fdb\u7a0b\uff0c\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u589e\u5f3a\u5956\u52b1\u673a\u5236\u63d0\u5347\u5b89\u5168\u6027\u3002", "result": "\u5728PyBullet\u7269\u7406\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u7ecf\u5178PPO\u548cSAC\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u663e\u8457\u52a0\u5feb\u63a8\u7406\u901f\u5ea6\uff0c\u800c\u4e14\u5728\u5b8c\u6210\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5bf9\u6bd4\u5b66\u4e60\u5f3a\u5316\u5b66\u4e60\u907f\u514d\u4e86\u7b56\u7565\u5d29\u6e83\u3002", "conclusion": "\u63d0\u51fa\u7684AR\u8fdc\u7a0b\u4eba\u673a\u4ea4\u4e92\u6570\u636e\u6536\u96c6\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4e3a\u7075\u5de7\u673a\u68b0\u81c2-\u624b\u7cfb\u7edf\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.07272", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07272", "abs": "https://arxiv.org/abs/2602.07272", "authors": ["Bowen Xue", "Saeed Hadadan", "Zheng Zeng", "Fabrice Rousselle", "Zahra Montazeri", "Milos Hasan"], "title": "VideoNeuMat: Neural Material Extraction from Generative Video Models", "comment": null, "summary": "Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a \"virtual gonioreflectometer\" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.", "AI": {"tldr": "VideoNeuMat\uff1a\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u795e\u7ecf\u6750\u8d28\u8d44\u4ea7\u7684\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u901a\u8fc7\u865a\u62df\u6d4b\u89d2\u53cd\u5c04\u4eea\u751f\u6210\u6750\u8d28\u6837\u672c\u89c6\u9891\uff0c\u518d\u7528\u5927\u578b\u91cd\u5efa\u6a21\u578b\u91cd\u6784\u4e3a\u795e\u7ecf\u6750\u8d28\u53c2\u6570", "motivation": "\u4e3a3D\u6e32\u67d3\u521b\u5efa\u903c\u771f\u6750\u8d28\u9700\u8981\u9ad8\u8d85\u7684\u827a\u672f\u6280\u80fd\uff0c\u800c\u73b0\u6709\u751f\u6210\u6a21\u578b\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7f3a\u4e4f\u3002\u89c6\u9891\u751f\u6210\u6a21\u578b\u80fd\u4ea7\u751f\u903c\u771f\u7684\u6750\u8d28\u5916\u89c2\uff0c\u4f46\u8fd9\u4e9b\u77e5\u8bc6\u4e0e\u51e0\u4f55\u548c\u5149\u7167\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u96be\u4ee5\u76f4\u63a5\u5229\u7528", "method": "\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u5fae\u8c03\u5927\u578b\u89c6\u9891\u6a21\u578b\uff08Wan 2.1 14B\uff09\u5728\u53d7\u63a7\u76f8\u673a\u548c\u5149\u7167\u8f68\u8ff9\u4e0b\u751f\u6210\u6750\u8d28\u6837\u672c\u89c6\u9891\uff0c\u521b\u5efa\"\u865a\u62df\u6d4b\u89d2\u53cd\u5c04\u4eea\"\uff1b2\uff09\u901a\u8fc7\u57fa\u4e8eWan 1.3B\u89c6\u9891\u9aa8\u5e72\u5fae\u8c03\u7684\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\uff0c\u4ece17\u4e2a\u751f\u6210\u89c6\u9891\u5e27\u4e2d\u5355\u6b21\u63a8\u7406\u9884\u6d4b\u795e\u7ecf\u6750\u8d28\u53c2\u6570", "result": "\u751f\u6210\u7684\u6750\u8d28\u5728\u771f\u5b9e\u611f\u548c\u591a\u6837\u6027\u4e0a\u8fdc\u8d85\u6709\u9650\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u7684\u89c6\u89d2\u548c\u5149\u7167\u6761\u4ef6\uff0c\u6210\u529f\u5c06\u4e92\u8054\u7f51\u89c4\u6a21\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u6750\u8d28\u77e5\u8bc6\u8f6c\u79fb\u5230\u72ec\u7acb\u7684\u3001\u53ef\u91cd\u7528\u7684\u795e\u7ecf3D\u8d44\u4ea7\u4e2d", "conclusion": "VideoNeuMat\u8bc1\u660e\u53ef\u4ee5\u4ece\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u6210\u529f\u63d0\u53d6\u6750\u8d28\u77e5\u8bc6\uff0c\u521b\u5efa\u51fa\u903c\u771f\u4e14\u591a\u6837\u5316\u7684\u53ef\u91cd\u7528\u795e\u7ecf\u6750\u8d28\u8d44\u4ea7\uff0c\u4e3a3D\u6e32\u67d3\u6750\u8d28\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2602.07883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07883", "abs": "https://arxiv.org/abs/2602.07883", "authors": ["Jingqi Zhou", "Sheng Wang", "DeZhao Deng", "Junwen Lu", "Junwei Su", "Qintong Li", "Jiahui Gao", "Hao Wu", "Jiyue Jiang", "Lingpeng Kong", "Chuan Wu"], "title": "ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation", "comment": null, "summary": "Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.", "AI": {"tldr": "ToolSelf\uff1a\u4e00\u79cd\u65b0\u578b\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u5c06\u914d\u7f6e\u66f4\u65b0\u62bd\u8c61\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u5b9e\u73b0\u4efb\u52a1\u6267\u884c\u4e0e\u81ea\u6211\u8c03\u6574\u7684\u7edf\u4e00\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u6839\u636e\u4efb\u52a1\u8fdb\u5c55\u81ea\u4e3b\u66f4\u65b0\u5b50\u76ee\u6807\u3001\u4e0a\u4e0b\u6587\u548c\u7b56\u7565\uff0c\u4ece\u88ab\u52a8\u6267\u884c\u8005\u8f6c\u53d8\u4e3a\u4efb\u52a1\u4e0e\u81ea\u6211\u7684\u53cc\u91cd\u7ba1\u7406\u8005\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u65f6\uff0c\u5176\u884c\u4e3a\u914d\u7f6e\u5728\u4efb\u52a1\u6267\u884c\u524d\u5c31\u5df2\u56fa\u5b9a\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u4efb\u52a1\u73af\u5883\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u7f16\u6392\u6216\u542f\u53d1\u5f0f\u4fee\u8865\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u4f18\u5316\u788e\u7247\u5316\uff0c\u9700\u8981\u7a81\u7834\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faToolSelf\u8303\u5f0f\uff0c\u5c06\u914d\u7f6e\u66f4\u65b0\u62bd\u8c61\u4e3a\u53ef\u8c03\u7528\u5de5\u5177\uff0c\u7edf\u4e00\u4efb\u52a1\u6267\u884c\u548c\u81ea\u6211\u8c03\u6574\u5230\u4e00\u4e2a\u52a8\u4f5c\u7a7a\u95f4\u3002\u5f00\u53d1\u914d\u7f6e\u611f\u77e5\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08CAT\uff09\uff0c\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u548c\u8f68\u8ff9\u7ea7\u5f3a\u5316\u5b66\u4e60\uff0c\u5185\u5316\u8fd9\u79cd\u5143\u80fd\u529b\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cToolSelf\u80fd\u591f\u5ab2\u7f8e\u4e13\u95e8\u5316\u5de5\u4f5c\u6d41\uff0c\u540c\u65f6\u5728\u65b0\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5e73\u574724.1%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ToolSelf\u901a\u8fc7\u5de5\u5177\u9a71\u52a8\u7684\u8fd0\u884c\u65f6\u81ea\u6211\u91cd\u914d\u7f6e\uff0c\u5b9e\u73b0\u4e86\u4ece\u5916\u90e8\u89c4\u5219\u5230\u5185\u5728\u53c2\u6570\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u771f\u6b63\u81ea\u9002\u5e94\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2602.07356", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07356", "abs": "https://arxiv.org/abs/2602.07356", "authors": ["Yonghui Yang", "Junwei Li", "Jilong Liu", "Yicheng He", "Fengbin Zhu", "Weibiao Huang", "Le Wu", "Richang Hong", "Tat-Seng Chua"], "title": "Controllable Value Alignment in Large Language Models through Neuron-Level Editing", "comment": null, "summary": "Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.", "AI": {"tldr": "NeVA\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u7f16\u8f91\u5b9e\u73b0\u53ef\u63a7\u7684\u4ef7\u503c\u5bf9\u9f50\uff0c\u663e\u8457\u51cf\u5c11\u4ef7\u503c\u6cc4\u6f0f\u95ee\u9898", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f15\u5bfc\u7684\u4ef7\u503c\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6709\u9650\u7684\u53ef\u63a7\u6027\uff1a\u5f15\u5bfc\u76ee\u6807\u4ef7\u503c\u65f6\u5e38\u5e38\u65e0\u610f\u4e2d\u6fc0\u6d3b\u5176\u4ed6\u975e\u76ee\u6807\u4ef7\u503c\uff0c\u8fd9\u88ab\u79f0\u4e3a\u4ef7\u503c\u6cc4\u6f0f\u95ee\u9898", "method": "\u63d0\u51faNeVA\u6846\u67b6\uff0c\u8bc6\u522b\u7a00\u758f\u7684\u4ef7\u503c\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u6fc0\u6d3b\u7f16\u8f91\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u800c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u6216\u91cd\u65b0\u8bad\u7ec3", "result": "NeVA\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u76ee\u6807\u4ef7\u503c\u5bf9\u9f50\uff0c\u540c\u65f6\u5728\u5927\u6a21\u578b\u901a\u7528\u80fd\u529b\u4e0a\u9020\u6210\u66f4\u5c0f\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u6cc4\u6f0f\u7387\uff0c\u6b8b\u4f59\u6548\u5e94\u4e3b\u8981\u5c40\u9650\u4e8e\u8bed\u4e49\u76f8\u5173\u7684\u4ef7\u503c\u7c7b\u522b", "conclusion": "NeVA\u4e3a\u4ef7\u503c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u53ef\u63a7\u548c\u53ef\u89e3\u91ca\u7684\u673a\u5236"}}
{"id": "2602.07277", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07277", "abs": "https://arxiv.org/abs/2602.07277", "authors": ["Rishabh Sharma", "Gijs Hogervorst", "Wayne E. Mackey", "David J. Heeger", "Stefano Martiniani"], "title": "Cross-View World Models", "comment": "12 pages, 7 figures", "summary": "World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.", "AI": {"tldr": "XVWM\u901a\u8fc7\u8de8\u89c6\u89d2\u9884\u6d4b\u76ee\u6807\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u89c4\u5212\uff0c\u5229\u7528\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4f5c\u4e3a\u51e0\u4f55\u6b63\u5219\u5316\u5b66\u4e60\u73af\u58833D\u7ed3\u6784", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u901a\u5e38\u4ec5\u4ece\u5355\u4e00\u89c6\u89d2\uff08\u901a\u5e38\u662f\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff09\u64cd\u4f5c\uff0c\u5373\u4f7f\u5176\u4ed6\u89c6\u89d2\uff08\u5982\u9e1f\u77b0\u89c6\u89d2\uff09\u53ef\u80fd\u4f7f\u89c4\u5212\u66f4\u5bb9\u6613\u3002\u5bfc\u822a\u7b49\u4efb\u52a1\u5c24\u5176\u53d7\u76ca\u4e8e\u591a\u89c6\u89d2\u89c4\u5212\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8de8\u89c6\u89d2\u4e16\u754c\u6a21\u578b\uff08XVWM\uff09\uff0c\u91c7\u7528\u8de8\u89c6\u89d2\u9884\u6d4b\u76ee\u6807\uff1a\u7ed9\u5b9a\u4e00\u4e2a\u89c6\u89d2\u7684\u5e27\u5e8f\u5217\uff0c\u9884\u6d4b\u6267\u884c\u52a8\u4f5c\u540e\u76f8\u540c\u6216\u4e0d\u540c\u89c6\u89d2\u7684\u672a\u6765\u72b6\u6001\u3002\u4f7f\u7528Aimlabs\u5e73\u53f0\u63d0\u4f9b\u7684\u540c\u6b65\u591a\u89c6\u89d2\u6e38\u620f\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u6570\u636e\u5305\u542b\u7cbe\u786e\u5bf9\u9f50\u7684\u591a\u6444\u50cf\u5934\u8bb0\u5f55\u548c\u9ad8\u9891\u52a8\u4f5c\u6807\u7b7e\u3002", "result": "\u6a21\u578b\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u8de8\u89c6\u89d2\u7684\u5e76\u884c\u60f3\u8c61\u6d41\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u6700\u9002\u5408\u4efb\u52a1\u7684\u53c2\u8003\u7cfb\u4e2d\u89c4\u5212\uff0c\u540c\u65f6\u4ece\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u6267\u884c\u3002\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e3a\u7a7a\u95f4\u57fa\u7840\u8868\u793a\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "conclusion": "\u8de8\u89c6\u89d2\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u5f3a\u5236\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u4f5c\u4e3a\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u5b66\u4e60\u73af\u58833D\u7ed3\u6784\u7684\u89c6\u89d2\u4e0d\u53d8\u8868\u793a\u3002\u4ece\u4ed6\u4eba\u89c6\u89d2\u9884\u6d4b\u81ea\u8eab\u884c\u52a8\u540e\u679c\u7684\u80fd\u529b\u4e3a\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u89c6\u89d2\u91c7\u62e9\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.07885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07885", "abs": "https://arxiv.org/abs/2602.07885", "authors": ["Zhenyuan Zhang", "Xianzhang Jia", "Zhiqin Yang", "Zhenbo Song", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "MemFly: On-the-Fly Memory Optimization via Information Bottleneck", "comment": null, "summary": "Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.", "AI": {"tldr": "MemFly\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7684LLM\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u81ea\u7531\u4f18\u5316\u5668\u6784\u5efa\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\uff0c\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u673a\u5236\u63d0\u5347\u957f\u671f\u8bb0\u5fc6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u957f\u671f\u8bb0\u5fc6\u6846\u67b6\u9762\u4e34\u4e00\u4e2a\u6839\u672c\u56f0\u5883\uff1a\u65e2\u8981\u9ad8\u6548\u538b\u7f29\u5197\u4f59\u4fe1\u606f\uff0c\u53c8\u8981\u4fdd\u6301\u7cbe\u786e\u68c0\u7d22\u4ee5\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u3002\u8fd9\u79cd\u538b\u7f29\u4e0e\u7cbe\u786e\u68c0\u7d22\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u539f\u5219\uff0c\u901a\u8fc7\u68af\u5ea6\u81ea\u7531\u4f18\u5316\u5668\u6700\u5c0f\u5316\u538b\u7f29\u71b5\u540c\u65f6\u6700\u5927\u5316\u76f8\u5173\u71b5\uff0c\u6784\u5efa\u5206\u5c42\u8bb0\u5fc6\u7ed3\u6784\u3002\u5f00\u53d1\u6df7\u5408\u68c0\u7d22\u673a\u5236\uff0c\u6574\u5408\u8bed\u4e49\u3001\u7b26\u53f7\u548c\u62d3\u6251\u68c0\u7d22\u8def\u5f84\uff0c\u5e76\u91c7\u7528\u8fed\u4ee3\u7cbe\u70bc\u5904\u7406\u590d\u6742\u591a\u8df3\u67e5\u8be2\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cMemFly\u5728\u8bb0\u5fc6\u8fde\u8d2f\u6027\u3001\u54cd\u5e94\u4fdd\u771f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MemFly\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u957f\u671f\u8bb0\u5fc6\u4e2d\u538b\u7f29\u6548\u7387\u4e0e\u68c0\u7d22\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u539f\u5219\u548c\u6df7\u5408\u68c0\u7d22\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8bb0\u5fc6\u7ba1\u7406\u6027\u80fd\u3002"}}
{"id": "2602.07358", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07358", "abs": "https://arxiv.org/abs/2602.07358", "authors": ["Jiaming He", "Fuming Luo", "Hongwei Li", "Wenbo Jiang", "Wenshu Fan", "Zhenbo Shi", "Xudong Jiang", "Yi Yu"], "title": "UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding", "comment": null, "summary": "Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.", "AI": {"tldr": "UTOPIA\u65b9\u6cd5\u4e3a\u8868\u683c\u6570\u636e\u63d0\u4f9b\u8ba4\u8bc1\u7684\u4e0d\u53ef\u5b66\u4e60\u6027\u4fdd\u62a4\uff0c\u901a\u8fc7\u89e3\u8026\u4f18\u5316\u5728\u9ad8\u663e\u8457\u6027\u7279\u5f81\u4e0a\u6df7\u6dc6\u8bed\u4e49\uff0c\u5728\u4f4e\u663e\u8457\u6027\u5197\u4f59\u7279\u5f81\u4e0a\u5d4c\u5165\u8d85\u76f8\u5173\u6377\u5f84\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u91d1\u878d\u548c\u533b\u7597\u9886\u57df\u7684\u8868\u683c\u6570\u636e\u9ad8\u5ea6\u654f\u611f\uff0c\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\u65b9\u6cd5\u5728\u8868\u683c\u6570\u636e\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8868\u683c\u7279\u5f81\u6df7\u5408\u6570\u503c\u548c\u7c7b\u522b\u7ea6\u675f\uff0c\u4e14\u5b58\u5728\u663e\u8457\u6027\u7a00\u758f\u6027\uff08\u5b66\u4e60\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5c11\u6570\u7ef4\u5ea6\uff09\u3002", "method": "\u63d0\u51faUTOPIA\u65b9\u6cd5\uff0c\u5229\u7528\u7279\u5f81\u5197\u4f59\u5c06\u4f18\u5316\u89e3\u8026\u4e3a\u4e24\u4e2a\u901a\u9053\uff1a\u9ad8\u663e\u8457\u6027\u7279\u5f81\u7528\u4e8e\u8bed\u4e49\u6df7\u6dc6\uff0c\u4f4e\u663e\u8457\u6027\u5197\u4f59\u7279\u5f81\u7528\u4e8e\u5d4c\u5165\u8d85\u76f8\u5173\u6377\u5f84\uff0c\u5728\u6ee1\u8db3\u8868\u683c\u6570\u636e\u7ea6\u675f\u7684\u540c\u65f6\u521b\u5efa\u4e3b\u5bfc\u6027\u6377\u5f84\u3002", "result": "\u5728\u591a\u4e2a\u8868\u683c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUTOPIA\u80fd\u591f\u5c06\u672a\u7ecf\u6388\u6743\u7684\u8bad\u7ec3\u63a8\u5411\u63a5\u8fd1\u968f\u673a\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u4e0d\u53ef\u5b66\u4e60\u793a\u4f8b\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5728\u4e0d\u540c\u67b6\u6784\u95f4\u5177\u6709\u826f\u597d\u7684\u8fc1\u79fb\u6027\u3002", "conclusion": "\u5728\u6ee1\u8db3\u8c31\u4e3b\u5bfc\u6761\u4ef6\u4e0b\uff0c\u8868\u683c\u6570\u636e\u7684\u8ba4\u8bc1\u4e0d\u53ef\u5b66\u4e60\u6027\u662f\u53ef\u884c\u7684\u3002UTOPIA\u901a\u8fc7\u89e3\u8026\u4f18\u5316\u7b56\u7565\u6709\u6548\u4fdd\u62a4\u654f\u611f\u8868\u683c\u6570\u636e\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u8bad\u7ec3\uff0c\u4e3a\u8868\u683c\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.07301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07301", "abs": "https://arxiv.org/abs/2602.07301", "authors": ["Aruna Jithesh", "Chinmayi Karumuri", "Venkata Kiran Reddy Kotha", "Meghana Doddapuneni", "Taehee Jeong"], "title": "Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms", "comment": null, "summary": "Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684DeepLab-V3+\u6a21\u578b\uff0c\u7528\u4e8e\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u76f8\u5173\u75c5\u53d8\u7684\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u5728DDR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u662f\u4e00\u79cd\u53ef\u80fd\u5bfc\u81f4\u89c6\u529b\u4e27\u5931\u548c\u5931\u660e\u7684\u773c\u75c5\uff0c\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5df2\u6709\u8bb8\u591a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u7b5b\u67e5\u7b97\u6cd5\uff0c\u4f46\u5728\u75c5\u53d8\u5206\u5272\u65b9\u9762\u7684\u4e34\u5e8a\u5e94\u7528\u4ecd\u6709\u9650\u5236\u3002\u9700\u8981\u63d0\u4f9b\u50cf\u7d20\u7ea7\u6ce8\u91ca\u6765\u652f\u6301\u773c\u79d1\u533b\u751f\u8fdb\u884cDR\u7b5b\u67e5\u3002", "method": "\u5728DeepLab-V3+\u6a21\u578b\u4e2d\u96c6\u6210\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u5206\u5272\u56db\u79cdDR\u76f8\u5173\u75c5\u53d8\uff1a\u5fae\u52a8\u8109\u7624\u3001\u8f6f\u6027\u6e17\u51fa\u7269\u3001\u786c\u6027\u6e17\u51fa\u7269\u548c\u51fa\u8840\u3002\u4f7f\u7528DDR\u6570\u636e\u96c6\u7684757\u5f20\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "Attention-DeepLab\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\u4ece0.3010\u63d0\u5347\u52300.3326\uff0c\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u4ece0.1791\u63d0\u5347\u52300.1928\u3002\u5fae\u52a8\u8109\u7624\u68c0\u6d4b\u4ece0.0205\u663e\u8457\u63d0\u5347\u52300.0763\uff0c\u8fd9\u5728\u4e34\u5e8a\u4e0a\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u4e3a\u5fae\u52a8\u8109\u7624\u662fDR\u6700\u65e9\u53ef\u89c1\u7684\u75c7\u72b6\u3002", "conclusion": "\u96c6\u6210\u6ce8\u610f\u529b\u673a\u5236\u7684DeepLab-V3+\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347DR\u75c5\u53d8\u5206\u5272\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5fae\u52a8\u8109\u7624\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u4e34\u5e8a\u663e\u8457\u7684\u6539\u8fdb\uff0c\u6709\u52a9\u4e8e\u652f\u6301\u773c\u79d1\u533b\u751f\u8fdb\u884c\u66f4\u51c6\u786e\u7684DR\u7b5b\u67e5\u548c\u65e9\u671f\u8bca\u65ad\u3002"}}
{"id": "2602.07310", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07310", "abs": "https://arxiv.org/abs/2602.07310", "authors": ["Kyle Williams", "Andrew Seltzman"], "title": "Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing", "comment": "39 pages, 12 figures, 1 table", "summary": "Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u9057\u4f20\u7f16\u7a0b\u7684\u8fc7\u6ee4\u548c\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u589e\u6750\u5236\u9020\u94cc\u57fa\u94dc\u5408\u91d1\u663e\u5fae\u56fe\u50cf\u4e2d\u7684\u6790\u51fa\u7269\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u624b\u52a8\u6807\u6ce8\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u589e\u6750\u5236\u9020\u94cc\u57fa\u94dc\u5408\u91d1\u7684\u5206\u6790\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\u663e\u5fae\u56fe\u50cf\u4e2d\u7684\u6790\u51fa\u7269\uff0c\u4f46\u7531\u4e8e\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u53d8\u5316\u3001\u566a\u58f0\u548c\u4f2a\u5f71\u7b49\u95ee\u9898\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u8017\u65f6\u4e14\u9650\u5236\u4e86\u5408\u91d1\u5f00\u53d1\u8fed\u4ee3\u901f\u5ea6\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u9057\u4f20\u7f16\u7a0b\u4f18\u5316\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\uff0c\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u8bed\u8a00\u6784\u5efa\u56fe\u50cf\u8fc7\u6ee4\u5757\u5e8f\u5217\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\u53c2\u6570\uff0c\u6700\u7ec8\u751f\u6210\u53ef\u89e3\u91ca\u7684MATLAB\u4ee3\u7801\u8868\u793a\u7684\u56fe\u50cf\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\uff08\u79cd\u7fa4\u5927\u5c0f60\uff0c\u6700\u5927\u7a0b\u5e8f\u957f\u5ea65\u4e2a\u5757\uff09\uff0c\u7cfb\u7edf\u627e\u5230\u4e86\u63a5\u8fd1\u4eba\u5de5\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u5747\u8bc4\u4f30\u8bef\u5dee\u4e3a1.8%\uff0c\u5904\u7406360\u4e07\u50cf\u7d20\u56fe\u50cf\u5e73\u5747\u4ec5\u97002\u79d2\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u65b9\u6cd5\u663e\u8457\u52a0\u5feb\u4e86\u6750\u6599\u5f00\u53d1\u8fed\u4ee3\u5468\u671f\uff0c\u4fc3\u8fdb\u4e86\u6750\u6599\u6210\u5206\u548c\u5de5\u827a\u7a7a\u95f4\u7684\u63a2\u7d22\uff0c\u6700\u7ec8\u6709\u52a9\u4e8e\u5f00\u53d1\u7528\u4e8e\u589e\u6750\u5236\u9020\u805a\u53d8\u53cd\u5e94\u5806\u90e8\u4ef6\u7684\u5f3a\u97e7\u3001\u4f4e\u6d3b\u5316\u3001\u6c89\u6dc0\u786c\u5316\u94dc\u5408\u91d1\u3002"}}
{"id": "2602.07905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07905", "abs": "https://arxiv.org/abs/2602.07905", "authors": ["Yu Zhao", "Hao Guan", "Yongcheng Jing", "Ying Zhang", "Dacheng Tao"], "title": "MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation", "comment": null, "summary": "Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.", "AI": {"tldr": "MedCoG\uff1a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u533b\u7597\u5143\u8ba4\u77e5\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u8bc4\u4f30\u52a8\u6001\u8c03\u8282\u77e5\u8bc6\u4f7f\u7528\uff0c\u5728\u907f\u514d\u76f2\u76ee\u6269\u5c55\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u51c6\u786e\u7387\uff0c\u5b9e\u73b05.5\u500d\u63a8\u7406\u5bc6\u5ea6\u63d0\u5347", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u533b\u7597\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u63a8\u7406\u6269\u5c55\u5b9a\u5f8b\u4e0b\u7684\u6536\u76ca\u9012\u51cf\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u6dfb\u52a0\u5404\u79cd\u77e5\u8bc6\u6765\u589e\u5f3aLLMs\uff0c\u4f46\u989d\u5916\u6210\u672c\u8f6c\u5316\u4e3a\u51c6\u786e\u7387\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faMedCoG\uff08Medical Meta-Cognition Agent with Knowledge Graph\uff09\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u8bc4\u4f30\uff08\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u719f\u6089\u5ea6\u3001\u77e5\u8bc6\u5bc6\u5ea6\uff09\u52a8\u6001\u8c03\u8282\u7a0b\u5e8f\u6027\u3001\u60c5\u666f\u6027\u548c\u4e8b\u5b9e\u6027\u77e5\u8bc6\u7684\u4f7f\u7528\u3002\u91c7\u7528\u4ee5LLM\u4e3a\u4e2d\u5fc3\u7684\u6309\u9700\u63a8\u7406\uff0c\u907f\u514d\u76f2\u76ee\u6269\u5c55\u5e76\u8fc7\u6ee4\u5e72\u6270\u77e5\u8bc6\u3002", "result": "\u5728\u4e94\u4e2a\u56f0\u96be\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u9a8c\u8bc1\u4e86MedCoG\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u5b9e\u73b0\u4e865.5\u500d\u7684\u63a8\u7406\u5bc6\u5ea6\u63d0\u5347\u3002Oracle\u7814\u7a76\u7a81\u663e\u4e86\u5143\u8ba4\u77e5\u8c03\u8282\u7684\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "\u5143\u8ba4\u77e5\u8c03\u8282\u80fd\u591f\u6709\u6548\u7f13\u89e3\u6269\u5c55\u5b9a\u5f8b\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u8c03\u8282\u5b9e\u73b0\u6210\u672c\u964d\u4f4e\u548c\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e3a\u533b\u7597\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07370", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07370", "abs": "https://arxiv.org/abs/2602.07370", "authors": ["Mark Bun", "William Fang"], "title": "Privately Learning Decision Lists and a Differentially Private Winnow", "comment": "27 pages, The 37th International Conference on Algorithmic Learning Theory", "summary": "We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728PAC\u548c\u5728\u7ebf\u6a21\u578b\u4e2d\u5b66\u4e60\u51b3\u7b56\u5217\u8868\u548c\u5927\u95f4\u9694\u534a\u7a7a\u95f4\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u4e0b\u5339\u914d\u6216\u63a5\u8fd1\u975e\u9690\u79c1\u7b97\u6cd5\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u9700\u8981\u5728\u4e0d\u6cc4\u9732\u4e2a\u4f53\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u6709\u6548\u5b66\u4e60\u3002\u73b0\u6709\u9690\u79c1\u7b97\u6cd5\u5728\u51b3\u7b56\u5217\u8868\u548c\u534a\u7a7a\u95f4\u5b66\u4e60\u7b49\u7ecf\u5178\u95ee\u9898\u4e0a\u901a\u5e38\u5b58\u5728\u6027\u80fd\u635f\u5931\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u9690\u79c1\u4fdd\u62a4\u4e0b\u63a5\u8fd1\u975e\u9690\u79c1\u7b97\u6cd5\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u5728PAC\u6a21\u578b\u4e2d\uff1a\u5f00\u53d1\u4e86\u8ba1\u7b97\u9ad8\u6548\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u5b66\u4e60\u51b3\u7b56\u5217\u8868\uff0c\u6837\u672c\u590d\u6742\u5ea6\u63a5\u8fd1\u6700\u4f18\u975e\u9690\u79c1\u7b97\u6cd5\n2. \u5728\u5728\u7ebf\u6a21\u578b\u4e2d\uff1a\u63d0\u51fa\u4e86Winnow\u7b97\u6cd5\u7684\u9690\u79c1\u7248\u672c\uff0c\u7528\u4e8e\u5b66\u4e60\u5927\u95f4\u9694\u534a\u7a7a\u95f4\uff0c\u9519\u8bef\u754c\u9650\u5728\u7ef4\u5ea6\u4e0a\u4e3a\u591a\u5bf9\u6570\u7ea7\uff0c\u4e0e\u95f4\u9694\u6210\u53cd\u591a\u9879\u5f0f\u5173\u7cfb\n3. \u5e94\u7528\uff1a\u5c06\u5728\u7ebf\u6a21\u578b\u65b9\u6cd5\u6269\u5c55\u5230\u51b3\u7b56\u5217\u8868\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e0e\u6700\u5148\u8fdb\u975e\u9690\u79c1\u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd", "result": "1. PAC\u6a21\u578b\u4e2d\uff1a\u51b3\u7b56\u5217\u8868\u5b66\u4e60\u7684\u6837\u672c\u590d\u6742\u5ea6\u6700\u5c0f\u5316\uff0c\u4ec5\u6bd4\u6700\u4f18\u975e\u9690\u79c1\u7b97\u6cd5\u6709\u5c11\u91cf\u989d\u5916\u5f00\u9500\n2. \u5728\u7ebf\u6a21\u578b\u4e2d\uff1a\u534a\u7a7a\u95f4\u5b66\u4e60\u7684\u9519\u8bef\u754c\u9650\u4e3apolylog(dimension) \u00d7 poly(1/margin)\uff0c\u5176\u4e2d\u7ef4\u5ea6\u4e3ad\uff0c\u95f4\u9694\u4e3a\u03b3\n3. \u5728\u7ebf\u51b3\u7b56\u5217\u8868\u5b66\u4e60\uff1a\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u975e\u9690\u79c1\u7b97\u6cd5\u76f8\u5f53\u7684\u5b9a\u6027\u6027\u80fd\u4fdd\u8bc1", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u5dee\u5206\u9690\u79c1\u7ea6\u675f\u4e0b\uff0c\u7ecf\u5178\u5b66\u4e60\u95ee\u9898\uff08\u51b3\u7b56\u5217\u8868\u548c\u534a\u7a7a\u95f4\uff09\u53ef\u4ee5\u8fbe\u5230\u63a5\u8fd1\u975e\u9690\u79c1\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u7b97\u6cd5\u3002"}}
{"id": "2602.07311", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07311", "abs": "https://arxiv.org/abs/2602.07311", "authors": ["Difei Gu", "Yunhe Gao", "Gerasimos Chatzoudis", "Zihan Dong", "Guoning Zhang", "Bangwei Guo", "Yang Zhou", "Mu Zhou", "Dimitris Metaxas"], "title": "LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery", "comment": null, "summary": "Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.", "AI": {"tldr": "LUCID\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u5171\u4eab\u7684\u6f5c\u5728\u5b57\u5178\u6765\u5bf9\u9f50\u56fe\u50cf\u5757\u548c\u6587\u672c\u6807\u8bb0\u8868\u793a\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u53ef\u89e3\u91ca\u6982\u5ff5\u53d1\u73b0\u3002", "motivation": "\u5f53\u524d\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6309\u6a21\u6001\u5355\u72ec\u8bad\u7ec3\uff0c\u4ea7\u751f\u7684\u7279\u5f81\u5b57\u5178\u4e0d\u53ef\u76f4\u63a5\u7406\u89e3\uff0c\u4e14\u89e3\u91ca\u65e0\u6cd5\u8de8\u57df\u8fc1\u79fb\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u8de8\u6a21\u6001\u8868\u793a\u3002", "method": "LUCID\u91c7\u7528\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u56fe\u50cf\u5757\u548c\u6587\u672c\u6807\u8bb0\u8868\u793a\u7684\u5171\u4eab\u6f5c\u5728\u5b57\u5178\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u6001\u7279\u5b9a\u7684\u79c1\u6709\u5bb9\u91cf\u3002\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5339\u914d\u76ee\u6807\u5b9e\u73b0\u7279\u5f81\u5bf9\u9f50\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002", "result": "LUCID\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u5171\u4eab\u7279\u5f81\uff0c\u652f\u6301\u5757\u7ea7\u5b9a\u4f4d\uff0c\u5efa\u7acb\u8de8\u6a21\u6001\u795e\u7ecf\u5143\u5bf9\u5e94\u5173\u7cfb\uff0c\u589e\u5f3a\u5bf9\u76f8\u4f3c\u6027\u8bc4\u4f30\u4e2d\u6982\u5ff5\u805a\u7c7b\u95ee\u9898\u7684\u9c81\u68d2\u6027\u3002\u5171\u4eab\u7279\u5f81\u6355\u83b7\u4e86\u8d85\u8d8a\u5bf9\u8c61\u7684\u591a\u6837\u5316\u8bed\u4e49\u7c7b\u522b\uff0c\u5305\u62ec\u52a8\u4f5c\u3001\u5c5e\u6027\u548c\u62bd\u8c61\u6982\u5ff5\u3002", "conclusion": "LUCID\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u7684\u53ef\u89e3\u91ca\u591a\u6a21\u6001\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7a00\u758f\u7f16\u7801\u5b9e\u73b0\u8de8\u6a21\u6001\u6982\u5ff5\u53d1\u73b0\u548c\u89e3\u91ca\uff0c\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.07919", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07919", "abs": "https://arxiv.org/abs/2602.07919", "authors": ["Mansi", "Avinash Kori", "Francesca Toni", "Soteris Demetriou"], "title": "Selective Fine-Tuning for Targeted and Robust Concept Unlearning", "comment": "Given the brittle nature of existing methods in unlearning harmful content in diffusion models, we propose TRuST, a novel approach for dynamically estimating target concept neurons and unlearning them by selectively fine-tuning", "summary": "Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.", "AI": {"tldr": "TRUST\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6269\u6563\u6a21\u578b\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4f30\u8ba1\u76ee\u6807\u6982\u5ff5\u795e\u7ecf\u5143\u5e76\u8fdb\u884c\u9009\u62e9\u6027\u5fae\u8c03\uff0c\u7ed3\u5408Hessian\u6b63\u5219\u5316\uff0c\u6709\u6548\u79fb\u9664\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u6613\u88ab\u5229\u7528\u751f\u6210\u6709\u5bb3\u5185\u5bb9\uff0c\u73b0\u6709\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\u8981\u4e48\u53ea\u80fd\u5904\u7406\u5355\u4e2a\u6982\u5ff5\uff0c\u8981\u4e48\u9700\u8981\u5168\u6a21\u578b\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u9759\u6001\u6982\u5ff5\u5b9a\u4f4d\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faTRUST\u65b9\u6cd5\uff1a1) \u52a8\u6001\u4f30\u8ba1\u76ee\u6807\u6982\u5ff5\u795e\u7ecf\u5143\uff1b2) \u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03\u8fdb\u884c\u6982\u5ff5\u9057\u5fd8\uff1b3) \u4f7f\u7528Hessian\u6b63\u5219\u5316\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTRUST\u80fd\u6709\u6548\u5bf9\u6297\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u663e\u8457\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u6bd4SOTA\u65b9\u6cd5\u66f4\u5feb\uff0c\u5e76\u80fd\u9057\u5fd8\u5355\u4e2a\u6982\u5ff5\u3001\u6982\u5ff5\u7ec4\u5408\u548c\u6761\u4ef6\u6982\u5ff5\uff0c\u65e0\u9700\u7279\u5b9a\u6b63\u5219\u5316\u3002", "conclusion": "TRUST\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u6982\u5ff5\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u3001\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.07397", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07397", "abs": "https://arxiv.org/abs/2602.07397", "authors": ["Hoang Anh Duy Le", "Sahil Joshi", "Zeyu Yang", "Zhaozhuo Xu", "Anshumali Shrivastava"], "title": "Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference", "comment": null, "summary": "Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.", "AI": {"tldr": "Sketch&Walk Attention\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8349\u56fe\u6280\u672f\u548c\u786e\u5b9a\u6027\u6e38\u8d70\u673a\u5236\u52a8\u6001\u9009\u62e9top-k\u6ce8\u610f\u529b\u5757\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe6\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\uff08\u5305\u62ec\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff09\u5360\u636e\u4e86\u4e3b\u8981\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u7a00\u758f\u6ce8\u610f\u529b\u89e3\u51b3\u65b9\u6848\u6765\u964d\u4f4e\u8fd9\u4e9b\u5f00\u9500\u3002", "method": "\u4f7f\u7528Hadamard\u8349\u56fe\u6280\u672f\u5ec9\u4ef7\u5730\u8fd1\u4f3c\u6ce8\u610f\u529b\u5206\u6570\uff0c\u7136\u540e\u901a\u8fc7\u6e38\u8d70\u673a\u5236\u5728\u5c42\u95f4\u805a\u5408\u8fd9\u4e9b\u4f30\u8ba1\u503c\uff0c\u6355\u6349\u8d85\u8d8atoken\u95f4\u76f4\u63a5\u4ea4\u4e92\u7684\u6ce8\u610f\u529b\u5f71\u54cd\uff0c\u6700\u7ec8\u57fa\u4e8e\u7d2f\u79ef\u7684\u6e38\u8d70\u5206\u6570\u52a8\u6001\u9009\u62e9top-k\u6ce8\u610f\u529b\u5757\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\uff0cSketch&Walk\u572820%\u6ce8\u610f\u529b\u5bc6\u5ea6\u4e0b\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u7cbe\u5ea6\uff0c\u5728\u67d0\u4e9b\u8bbe\u7f6e\u4e2d\u751a\u81f3\u7565\u5fae\u4f18\u4e8e\u5bc6\u96c6\u6ce8\u610f\u529b\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe6\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "Sketch&Walk Attention\u662f\u4e00\u79cd\u7edf\u4e00\u9002\u7528\u4e8e\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u8bad\u7ec3\u514d\u8d39\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.07943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07943", "abs": "https://arxiv.org/abs/2602.07943", "authors": ["Ivaxi Sheth", "Zhijing Jin", "Bryan Wilder", "Dominik Janzing", "Mario Fritz"], "title": "IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery", "comment": "18 pages", "summary": "In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.", "AI": {"tldr": "LLMs\u80fd\u591f\u5e2e\u52a9\u53d1\u73b0\u6709\u6548\u7684\u5de5\u5177\u53d8\u91cf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edfIV Co-Scientist\u63d0\u51fa\u3001\u6279\u5224\u548c\u4f18\u5316\u5de5\u5177\u53d8\u91cf\u9009\u62e9", "motivation": "\u5de5\u5177\u53d8\u91cf\u8bc6\u522b\u9700\u8981\u8de8\u5b66\u79d1\u77e5\u8bc6\u3001\u521b\u9020\u529b\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u662f\u4e00\u9879\u975e\u5e73\u51e1\u4efb\u52a1\u3002\u672c\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5e2e\u52a9\u5b8c\u6210\u8fd9\u4e00\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\uff1a\u9996\u5148\u6d4b\u8bd5LLMs\u80fd\u5426\u4ece\u6587\u732e\u4e2d\u6062\u590d\u5df2\u786e\u7acb\u7684\u5de5\u5177\u53d8\u91cf\uff1b\u5176\u6b21\u8bc4\u4f30LLMs\u80fd\u5426\u8bc6\u522b\u548c\u907f\u514d\u5df2\u88ab\u5b9e\u8bc1\u6216\u7406\u8bba\u5426\u5b9a\u7684\u5de5\u5177\u53d8\u91cf\u3002\u57fa\u4e8e\u6b64\u5f15\u5165IV Co-Scientist\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u65e0\u771f\u5b9e\u503c\u60c5\u51b5\u4e0b\u7684\u7edf\u8ba1\u68c0\u9a8c\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793aLLMs\u6709\u6f5c\u529b\u4ece\u5927\u578b\u89c2\u6d4b\u6570\u636e\u5e93\u4e2d\u8bc6\u522b\u6709\u6548\u7684\u5de5\u5177\u53d8\u91cf\uff0cIV Co-Scientist\u7cfb\u7edf\u80fd\u591f\u63d0\u51fa\u3001\u6279\u5224\u548c\u4f18\u5316\u5de5\u5177\u53d8\u91cf\u9009\u62e9\u3002", "conclusion": "LLMs\u5728\u5de5\u5177\u53d8\u91cf\u53d1\u73b0\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u8f85\u52a9\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u5de5\u5177\u53d8\u91cf\u8bc6\u522b\u4efb\u52a1\u3002"}}
{"id": "2602.07428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07428", "abs": "https://arxiv.org/abs/2602.07428", "authors": ["Chengqi Dong", "Zhiyuan Cao", "Tuoshi Qi", "Kexin Wu", "Yixing Gao", "Fan Tang"], "title": "Row-Column Separated Attention Based Low-Light Image/Video Enhancement", "comment": null, "summary": "U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6539\u8fdbU-Net\u548c\u884c\u5217\u5206\u79bb\u6ce8\u610f\u529b\u6a21\u5757\u7684\u4f4e\u5149\u7167\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u4fe1\u606f\u5f15\u5bfc\u5c40\u90e8\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edfU-Net\u7ed3\u6784\u5728\u4f4e\u5149\u7167\u589e\u5f3a\u4e2d\u7f3a\u4e4f\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\uff0c\u5bfc\u81f4\u5c40\u90e8\u566a\u58f0\u5927\u4e14\u7ec6\u8282\u4e22\u5931\uff1b\u6ce8\u610f\u529b\u673a\u5236\u80fd\u66f4\u597d\u5229\u7528\u5168\u5c40\u4fe1\u606f\u4f46\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u5927\u3002", "method": "1. \u6539\u8fdbU-Net\u7ed3\u6784\uff1b2. \u63d0\u51fa\u884c\u5217\u5206\u79bb\u6ce8\u610f\u529b\u6a21\u5757(RCSA)\uff0c\u5229\u7528\u7279\u5f81\u56fe\u884c\u5217\u7684\u5747\u503c\u548c\u6700\u5927\u503c\u4f5c\u4e3a\u8f93\u5165\uff0c\u4ee5\u8f83\u5c11\u53c2\u6570\u5b9e\u73b0\u5168\u5c40\u4fe1\u606f\u5f15\u5bfc\u5c40\u90e8\u4fe1\u606f\uff1b3. \u63d0\u51fa\u4e24\u79cd\u65f6\u95f4\u635f\u5931\u51fd\u6570\u7528\u4e8e\u89c6\u9891\u589e\u5f3a\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5728LOL\u3001MIT Adobe FiveK\u56fe\u50cf\u6570\u636e\u96c6\u548cSDSD\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684URCSA\u65b9\u6cd5\u901a\u8fc7\u884c\u5217\u5206\u79bb\u6ce8\u610f\u529b\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u7167\u589e\u5f3a\u4e2d\u5168\u5c40\u4fe1\u606f\u6307\u5bfc\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u589e\u5f3a\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.07411", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07411", "abs": "https://arxiv.org/abs/2602.07411", "authors": ["Zishi Zhang", "Tao Ren", "Yijie Peng"], "title": "Nonparametric Bayesian Optimization for General Rewards", "comment": null, "summary": "This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5956\u52b1\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u7b97\u6cd5\uff0c\u4f7f\u7528\u65e0\u9650\u9ad8\u65af\u8fc7\u7a0b\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\uff0c\u7ed3\u5408\u6c64\u666e\u68ee\u91c7\u6837\u5b9e\u73b0\u65e0\u9057\u61be\u4fdd\u8bc1\uff0c\u8ba1\u7b97\u53ef\u6269\u5c55\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u901a\u5e38\u5047\u8bbe\u9ad8\u65af\u8fc7\u7a0b\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5956\u52b1\u6a21\u578b\u53ef\u80fd\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3001\u975e\u5e73\u7a33\u6027\u3001\u91cd\u5c3e\u5206\u5e03\u7b49\u590d\u6742\u60c5\u51b5\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u65e0\u9650\u9ad8\u65af\u8fc7\u7a0b\u4f5c\u4e3a\u8d1d\u53f6\u65af\u975e\u53c2\u6570\u6a21\u578b\uff0c\u5bf9\u5956\u52b1\u5206\u5e03\u7a7a\u95f4\u8bbe\u7f6e\u5148\u9a8c\uff0c\u7ed3\u5408\u6c64\u666e\u68ee\u91c7\u6837\u8fdb\u884c\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u91c7\u7528\u622a\u65ad\u5409\u5e03\u65af\u91c7\u6837\u5b9e\u73b0\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u4e00\u822c\u5956\u52b1\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u65e0\u9057\u61be\u4fdd\u8bc1\uff0c\u4ec5\u9700\u76ee\u6807\u51fd\u6570\u7684Lipschitz\u8fde\u7eed\u6027\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u7ecf\u5178\u9ad8\u65af\u8fc7\u7a0b\u76f8\u5f53\uff0c\u5728\u975e\u5e73\u7a33\u3001\u91cd\u5c3e\u7b49\u590d\u6742\u5956\u52b1\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u65e0\u9650\u9ad8\u65af\u8fc7\u7a0b\u7ed3\u5408\u6c64\u666e\u68ee\u91c7\u6837\u4e3a\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u5e7f\u6cdb\u7684\u5956\u52b1\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.07444", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.07444", "abs": "https://arxiv.org/abs/2602.07444", "authors": ["Ondrej Hlinka", "Georg Kaniak", "Christian Kapeller"], "title": "Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction", "comment": "submitted to IET Electronics Letters", "summary": "We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u900f\u89c6\u611f\u77e5\u7684\u5bf9\u6570\u6df1\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u4ece\u5355\u89c6\u89d2\u76f8\u673a\u83b7\u53d6\u7684\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u91cd\u5efa3D\u8868\u9762\uff0c\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u900f\u89c6\u6295\u5f71\u63d0\u9ad8\u5ea6\u91cf\u7cbe\u5ea6\uff0c\u5e76\u80fd\u5229\u7528\u6cd5\u7ebf\u4fe1\u606f\u586b\u8865\u6df1\u5ea6\u7f3a\u5931\u533a\u57df\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u6df1\u5ea6-\u6cd5\u7ebf\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6b63\u4ea4\u6295\u5f71\uff0c\u800c\u5b9e\u9645\u4f20\u611f\u5668\u7cfb\u7edf\uff08\u5982\u7ed3\u6784\u5149\u626b\u63cf\u3001\u5149\u5ea6\u7acb\u4f53\uff09\u901a\u5e38\u4f7f\u7528\u900f\u89c6\u76f8\u673a\uff0c\u9700\u8981\u5904\u7406\u900f\u89c6\u6295\u5f71\u6548\u5e94\u4ee5\u83b7\u5f97\u5ea6\u91cf\u51c6\u786e\u76843D\u91cd\u5efa\u3002", "method": "\u63d0\u51fa\u900f\u89c6\u611f\u77e5\u7684\u5bf9\u6570\u6df1\u5ea6\u878d\u5408\u65b9\u6cd5\uff0c\u6269\u5c55\u73b0\u6709\u6b63\u4ea4\u68af\u5ea6\u57fa\u6df1\u5ea6-\u6cd5\u7ebf\u878d\u5408\u65b9\u6cd5\uff0c\u663e\u5f0f\u8003\u8651\u900f\u89c6\u6295\u5f71\uff1b\u5229\u7528\u53ef\u7528\u7684\u8868\u9762\u6cd5\u7ebf\u4fe1\u606f\u586b\u8865\u6df1\u5ea6\u6d4b\u91cf\u7f3a\u5931\u533a\u57df\u3002", "result": "\u5728DiLiGenT-MV\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u900f\u89c6\u611f\u77e5\u6df1\u5ea6-\u6cd5\u7ebf\u878d\u5408\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u89c6\u89d2\u76f8\u673a\u83b7\u53d6\u7684\u6df1\u5ea6\u548c\u6cd5\u7ebf\u56fe\u5b9e\u73b0\u5ea6\u91cf\u51c6\u786e\u76843D\u91cd\u5efa\uff0c\u901a\u8fc7\u900f\u89c6\u611f\u77e5\u878d\u5408\u548c\u7f3a\u5931\u6570\u636e\u586b\u8865\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2602.07983", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07983", "abs": "https://arxiv.org/abs/2602.07983", "authors": ["Jishu Sen Gupta", "Harini SI", "Somesh Kumar Singh", "Syed Mohamad Tawseeq", "Yaman Kumar Singla", "David Doermann", "Rajiv Ratn Shah", "Balaji Krishnamurthy"], "title": "Accelerating Social Science Research via Agentic Hypothesization and Experimentation", "comment": null, "summary": "Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.", "AI": {"tldr": "EXPERIGEN\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5668-\u5b9e\u9a8c\u8005\u7684\u4e24\u9636\u6bb5\u641c\u7d22\uff0c\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u53d1\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u591a2-4\u500d\u7684\u7edf\u8ba1\u663e\u8457\u5047\u8bbe\uff0c\u9884\u6d4b\u6027\u80fd\u63d0\u53477-17%\uff0c\u5e76\u6210\u529f\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u548c\u771f\u5b9e\u4e16\u754cA/B\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u9a71\u52a8\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u8fc7\u7a0b\u7f13\u6162\uff0c\u4f9d\u8d56\u89c2\u5bdf\u3001\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u8fed\u4ee3\u5faa\u73af\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u867d\u7136\u80fd\u52a0\u901f\u90e8\u5206\u8fc7\u7a0b\uff0c\u4f46\u65e0\u6cd5\u652f\u6301\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b8c\u6574\u652f\u6301\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faEXPERIGEN\u6846\u67b6\uff0c\u91c7\u7528\u53d7\u8d1d\u53f6\u65af\u4f18\u5316\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u641c\u7d22\uff1a\u751f\u6210\u5668\u63d0\u51fa\u5019\u9009\u5047\u8bbe\uff0c\u5b9e\u9a8c\u8005\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002\u8be5\u6846\u67b6\u652f\u6301\u591a\u6a21\u6001\u548c\u5173\u7cfb\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u548c\u771f\u5b9e\u4e16\u754cA/B\u6d4b\u8bd5\u9a8c\u8bc1\u5047\u8bbe\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u4e2d\uff0cEXPERIGEN\u53d1\u73b0\u7684\u7edf\u8ba1\u663e\u8457\u5047\u8bbe\u6570\u91cf\u662f\u73b0\u6709\u65b9\u6cd5\u76842-4\u500d\uff0c\u9884\u6d4b\u6027\u80fd\u63d0\u53477-17%\u3002\u4e13\u5bb6\u8bc4\u5ba1\u663e\u793a88%\u7684\u5047\u8bbe\u5177\u6709\u4e2d\u7b49\u6216\u5f3a\u65b0\u9896\u6027\uff0c70%\u88ab\u8ba4\u4e3a\u6709\u5f71\u54cd\u529b\u4e14\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u771f\u5b9e\u4e16\u754cA/B\u6d4b\u8bd5\u83b7\u5f97p<1e-6\u7684\u7edf\u8ba1\u663e\u8457\u7ed3\u679c\uff0c\u6548\u5e94\u5927\u5c0f\u8fbe344%\u3002", "conclusion": "EXPERIGEN\u6846\u67b6\u80fd\u591f\u6709\u6548\u52a0\u901f\u7aef\u5230\u7aef\u7684\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\uff0c\u4e0d\u4ec5\u4ea7\u751f\u7edf\u8ba1\u4e0a\u663e\u8457\u7684\u5047\u8bbe\uff0c\u8fd8\u80fd\u751f\u6210\u65b0\u9896\u3001\u6709\u5f71\u54cd\u529b\u4e14\u53ef\u64cd\u4f5c\u7684\u5047\u8bbe\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u5de5\u5177\u3002"}}
{"id": "2602.07415", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07415", "abs": "https://arxiv.org/abs/2602.07415", "authors": ["Runhan Shi", "Zhicheng Zhang", "Letian Chen", "Gufeng Yu", "Yang Yang"], "title": "Learning Molecular Chirality via Chiral Determinant Kernels", "comment": "Accepted at the ICLR 2026", "summary": "Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.", "AI": {"tldr": "ChiDeK\u662f\u4e00\u4e2a\u5c06\u7acb\u4f53\u5316\u5b66\u4fe1\u606f\u6574\u5408\u5230\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u6027\u884c\u5217\u5f0f\u6838\u7f16\u7801SE(3)\u4e0d\u53d8\u7684\u624b\u6027\u77e9\u9635\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e2d\u5fc3\u624b\u6027\u548c\u8f74\u5411\u624b\u6027\uff0c\u5728\u591a\u4e2a\u624b\u6027\u76f8\u5173\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u624b\u6027\u662f\u5206\u5b50\u5728\u5316\u5b66\u548c\u751f\u7269\u5b66\u4e2d\u63a7\u5236\u7acb\u4f53\u7279\u5f02\u6027\u884c\u4e3a\u7684\u57fa\u672c\u5c5e\u6027\u3002\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6355\u6349\u624b\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u4f20\u7edf\u5206\u5b50\u8868\u793a\u901a\u5e38\u7f3a\u4e4f\u660e\u786e\u7684\u7acb\u4f53\u5316\u5b66\u7f16\u7801\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e2d\u5fc3\u624b\u6027\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u66f4\u590d\u6742\u7684\u8f74\u5411\u624b\u6027\u7b49\u5f62\u5f0f\u3002", "method": "\u63d0\u51faChiDeK\u6846\u67b6\uff0c\u5f15\u5165\u624b\u6027\u884c\u5217\u5f0f\u6838\u6765\u7f16\u7801SE(3)\u4e0d\u53d8\u7684\u624b\u6027\u77e9\u9635\uff0c\u5e76\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5c06\u5c40\u90e8\u624b\u6027\u4e2d\u5fc3\u7684\u7acb\u4f53\u5316\u5b66\u4fe1\u606f\u6574\u5408\u5230\u5168\u5c40\u5206\u5b50\u8868\u793a\u4e2d\u3002\u8be5\u8bbe\u8ba1\u80fd\u591f\u5728\u7edf\u4e00\u67b6\u6784\u4e2d\u663e\u5f0f\u5efa\u6a21\u624b\u6027\u76f8\u5173\u7279\u5f81\uff0c\u540c\u65f6\u7f16\u7801\u4e2d\u5fc3\u624b\u6027\u548c\u8f74\u5411\u624b\u6027\u3002", "result": "\u5728\u56db\u4e2a\u4efb\u52a1\uff08R/S\u6784\u578b\u5206\u7c7b\u3001\u5bf9\u6620\u4f53\u6392\u5e8f\u3001ECD\u5149\u8c31\u9884\u6d4b\u548cOR\u9884\u6d4b\uff09\u4e0a\uff0cChiDeK\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u8f74\u5411\u624b\u6027\u4efb\u52a1\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e867%\u4ee5\u4e0a\u3002\u4e3a\u8bc4\u4f30\u8f74\u5411\u624b\u6027\uff0c\u8fd8\u6784\u5efa\u4e86\u65b0\u7684ECD\u548cOR\u9884\u6d4b\u57fa\u51c6\u3002", "conclusion": "ChiDeK\u901a\u8fc7\u7cfb\u7edf\u5730\u5c06\u7acb\u4f53\u6e90\u4fe1\u606f\u6574\u5408\u5230\u5206\u5b50\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u624b\u6027\u5efa\u6a21\u7684\u6311\u6218\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e2d\u5fc3\u624b\u6027\u548c\u8f74\u5411\u624b\u6027\uff0c\u5728\u624b\u6027\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u624b\u6027\u5206\u5b50\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002"}}
{"id": "2602.07446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07446", "abs": "https://arxiv.org/abs/2602.07446", "authors": ["Naqcho Ali Mehdi"], "title": "PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization", "comment": "8 pages, 4 figures, dataset paper", "summary": "Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.", "AI": {"tldr": "PTB-XL-Image-17K\u662f\u4e00\u4e2a\u5305\u542b17,271\u4e2a\u9ad8\u8d28\u91cf12\u5bfc\u8054\u5fc3\u7535\u56fe\u56fe\u50cf\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4e3a\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u7814\u7a76\u63d0\u4f9b\u9996\u4e2a\u5927\u89c4\u6a21\u8d44\u6e90\uff0c\u652f\u6301\u5b8c\u6574\u7684\u5904\u7406\u6d41\u7a0b\u8bc4\u4f30\u3002", "motivation": "\u5fc3\u7535\u56fe\u6570\u5b57\u5316\uff08\u5c06\u7eb8\u8d28\u6216\u626b\u63cf\u7684\u5fc3\u7535\u56fe\u56fe\u50cf\u8f6c\u6362\u56de\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\uff09\u5bf9\u4e8e\u5229\u7528\u6570\u5341\u5e74\u7684\u4e34\u5e8a\u6570\u636e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u540c\u65f6\u5305\u542b\u5fc3\u7535\u56fe\u56fe\u50cf\u548c\u5bf9\u5e94\u771f\u5b9e\u4fe1\u53f7\u7684\u6570\u636e\u96c6\u963b\u788d\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u57fa\u4e8ePTB-XL\u4fe1\u53f7\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90Python\u6846\u67b6\uff0c\u751f\u6210\u5305\u542b\u4e94\u79cd\u4e92\u8865\u6570\u636e\u7c7b\u578b\u7684\u5408\u6210\u5fc3\u7535\u56fe\u56fe\u50cf\uff1a\u771f\u5b9e\u7684\u5fc3\u7535\u56fe\u56fe\u50cf\u3001\u50cf\u7d20\u7ea7\u5206\u5272\u63a9\u7801\u3001\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u3001YOLO\u683c\u5f0f\u7684\u8fb9\u754c\u6846\u6807\u6ce8\u4ee5\u53ca\u5168\u9762\u7684\u5143\u6570\u636e\u3002", "result": "\u6210\u529f\u751f\u6210\u4e8617,271\u4e2a\u9ad8\u8d28\u91cf12\u5bfc\u8054\u5fc3\u7535\u56fe\u56fe\u50cf\uff0c\u751f\u6210\u6210\u529f\u7387\u8fbe100%\uff0c\u5e73\u5747\u5904\u7406\u65f6\u95f4\u4e3a\u6bcf\u6837\u672c1.35\u79d2\u3002\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u53ef\u63a7\u5236\u53c2\u6570\u7684\u81ea\u5b9a\u4e49\u751f\u6210\u529f\u80fd\u3002", "conclusion": "PTB-XL-Image-17K\u586b\u8865\u4e86\u5fc3\u7535\u56fe\u6570\u5b57\u5316\u7814\u7a76\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u9996\u4e2a\u652f\u6301\u5b8c\u6574\u5904\u7406\u6d41\u7a0b\uff08\u5bfc\u8054\u68c0\u6d4b\u3001\u6ce2\u5f62\u5206\u5272\u548c\u4fe1\u53f7\u63d0\u53d6\uff09\u7684\u5927\u89c4\u6a21\u8d44\u6e90\uff0c\u4e3a\u4e25\u683c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b8c\u6574\u771f\u5b9e\u6570\u636e\u3002"}}
{"id": "2602.07418", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07418", "abs": "https://arxiv.org/abs/2602.07418", "authors": ["Jian Qian", "Chen-Yu Wei"], "title": "Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses", "comment": null, "summary": "In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.\n  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5728\u5bf9\u6297\u6027\u591a\u81c2\u8001\u864e\u673a\u4e2d\uff0c\u9996\u6b21\u9488\u5bf9\u786e\u5b9a\u6027\u635f\u5931\u548c\u9057\u5fd8\u6027\u5bf9\u624b\u5b9e\u73b0\u4e86\u9759\u6001\u9057\u61be\u548c\u52a8\u6001\u9057\u61be\u7684\u540c\u65f6\u6700\u4f18\u6027\uff0c\u63ed\u793a\u4e86\u81ea\u9002\u5e94\u5bf9\u624b\u4e0e\u9057\u5fd8\u6027\u5bf9\u624b\u4e4b\u95f4\u7684\u6839\u672c\u5dee\u5f02\u3002", "motivation": "\u5728\u5bf9\u6297\u6027\u591a\u81c2\u8001\u864e\u673a\u4e2d\uff0c\u9759\u6001\u9057\u61be\u548c\u52a8\u6001\u9057\u61be\u662f\u4e24\u4e2a\u5e38\u7528\u7684\u6027\u80fd\u5ea6\u91cf\u3002\u867d\u7136\u5df2\u6709\u9488\u5bf9\u5355\u4e2a\u5ea6\u91cf\u7684\u6700\u4f18\u7b97\u6cd5\uff0c\u4f46\u5c1a\u65e0\u7b97\u6cd5\u80fd\u540c\u65f6\u5b9e\u73b0\u4e24\u8005\u7684\u6700\u4f18\u754c\u9650\u3002Marinov\u548cZimmert[2021]\u9996\u6b21\u8bc1\u660e\u9488\u5bf9\u81ea\u9002\u5e94\u5bf9\u624b\u65e0\u6cd5\u5b9e\u73b0\u540c\u65f6\u6700\u4f18\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u9057\u5fd8\u6027\u5bf9\u624b\u548c\u786e\u5b9a\u6027\u635f\u5931\u6761\u4ef6\u4e0b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u9996\u5148\u5c06Marinov\u548cZimmert[2021]\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u6269\u5c55\u5230\u786e\u5b9a\u6027\u635f\u5931\u60c5\u51b5\u3002\u7136\u540e\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u5229\u7528\u8d1f\u9759\u6001\u9057\u61be\u6765\u8865\u507f\u63a7\u5236\u52a8\u6001\u9057\u61be\u65f6\u7684\u63a2\u7d22\u5f00\u9500\uff0c\u5e76\u501f\u52a9Blackwell\u53ef\u63a5\u8fd1\u6027\u6765\u8054\u5408\u63a7\u5236\u4e24\u79cd\u9057\u61be\uff0c\u4ece\u800c\u5b9e\u73b0\u540c\u65f6\u6700\u4f18\u6027\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u5728\u786e\u5b9a\u6027\u635f\u5931\u548c\u9057\u5fd8\u6027\u5bf9\u624b\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u7684\u9759\u6001\u9057\u61be\u548c\u52a8\u6001\u9057\u61be\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u540c\u65f6\u8003\u8651\u591a\u4e2a\u9057\u61be\u57fa\u51c6\u65f6\uff0c\u81ea\u9002\u5e94\u5bf9\u624b\u4e0e\u9057\u5fd8\u6027\u5bf9\u624b\u4e4b\u95f4\u7684\u6839\u672c\u5206\u79bb\uff0c\u5e76\u4e3a\u540c\u65f6\u5b9e\u73b0\u4e0d\u540c\u5207\u6362\u6b21\u6570\u57fa\u51c6\u7684\u6700\u4f18\u9057\u61be\u8fd9\u4e00\u957f\u671f\u5f00\u653e\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86\u5728\u5bf9\u6297\u6027\u591a\u81c2\u8001\u864e\u673a\u4e2d\uff0c\u9488\u5bf9\u9057\u5fd8\u6027\u5bf9\u624b\u548c\u786e\u5b9a\u6027\u635f\u5931\uff0c\u53ef\u4ee5\u540c\u65f6\u5b9e\u73b0\u9759\u6001\u9057\u61be\u548c\u52a8\u6001\u9057\u61be\u7684\u6700\u4f18\u6027\u3002\u63d0\u51fa\u7684\u7b97\u6cd5\u5229\u7528\u8d1f\u9759\u6001\u9057\u61be\u8865\u507f\u63a2\u7d22\u5f00\u9500\uff0c\u901a\u8fc7Blackwell\u53ef\u63a5\u8fd1\u6027\u8054\u5408\u63a7\u5236\u4e24\u79cd\u9057\u61be\uff0c\u4e3a\u8001\u864e\u673a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u72ec\u7acb\u7684\u7814\u7a76\u4ef7\u503c\u3002"}}
{"id": "2602.07449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07449", "abs": "https://arxiv.org/abs/2602.07449", "authors": ["Tan Yu", "Qian Qiao", "Le Shen", "Ke Zhou", "Jincheng Hu", "Dian Sheng", "Bo Hu", "Haoming Qin", "Jun Gao", "Changhai Zhou", "Shunshun Yin", "Siyuan Liu"], "title": "SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads", "comment": "11 pages, 3 figures", "summary": "Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.", "AI": {"tldr": "SoulX-FlashHead\u662f\u4e00\u4e2a1.3B\u53c2\u6570\u7684\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u611f\u77e5\u65f6\u7a7a\u9884\u8bad\u7ec3\u548cOracle\u5f15\u5bfc\u53cc\u5411\u84b8\u998f\u6280\u672f\uff0c\u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b096FPS\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u751f\u6210\u6a21\u578b\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u4e0e\u89c6\u89c9\u8d28\u91cf/\u65f6\u95f4\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002\u5927\u578b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5219\u727a\u7272\u9762\u90e8\u8868\u793a\u5b8c\u6574\u6027\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u3002", "method": "1. \u63d0\u51fa1.3B\u53c2\u6570\u7684\u7edf\u4e00\u6846\u67b6SoulX-FlashHead\uff1b2. \u5f15\u5165\u6d41\u5f0f\u611f\u77e5\u65f6\u7a7a\u9884\u8bad\u7ec3\uff0c\u914d\u5907\u65f6\u95f4\u97f3\u9891\u4e0a\u4e0b\u6587\u7f13\u5b58\u673a\u5236\uff0c\u4ece\u77ed\u97f3\u9891\u7247\u6bb5\u4e2d\u63d0\u53d6\u7a33\u5065\u7279\u5f81\uff1b3. \u63d0\u51faOracle\u5f15\u5bfc\u53cc\u5411\u84b8\u998f\uff0c\u5229\u7528\u771f\u5b9e\u8fd0\u52a8\u5148\u9a8c\u63d0\u4f9b\u7cbe\u786e\u7269\u7406\u6307\u5bfc\uff0c\u7f13\u89e3\u957f\u5e8f\u5217\u81ea\u56de\u5f52\u751f\u6210\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u8eab\u4efd\u6f02\u79fb\uff1b4. \u6784\u5efaVividHead\u6570\u636e\u96c6\uff08782\u5c0f\u65f6\u4e25\u683c\u5bf9\u9f50\u7d20\u6750\uff09\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5728HDTF\u548cVFHQ\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1bLite\u53d8\u4f53\u5728\u5355\u5f20NVIDIA RTX 4090\u4e0a\u5b9e\u73b096FPS\u63a8\u7406\u901f\u5ea6\uff0c\u652f\u6301\u8d85\u5feb\u901f\u4ea4\u4e92\u800c\u4e0d\u727a\u7272\u89c6\u89c9\u8fde\u8d2f\u6027\u3002", "conclusion": "SoulX-FlashHead\u6210\u529f\u89e3\u51b3\u4e86\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u751f\u6210\u4e2d\u9ad8\u4fdd\u771f\u89c6\u89c9\u8d28\u91cf\u4e0e\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u4f20\u8f93\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u5f0f\u611f\u77e5\u8bad\u7ec3\u548c\u84b8\u998f\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u65e0\u9650\u957f\u5ea6\u3001\u9ad8\u4fdd\u771f\u7684\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2602.08013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08013", "abs": "https://arxiv.org/abs/2602.08013", "authors": ["Yuqiao Meng", "Luoxi Tang", "Dazheng Zhang", "Rafael Brens", "Elvys J. Romero", "Nancy Guo", "Safa Elkefi", "Zhaohan Xi"], "title": "Small Agent Group is the Future of Digital Health", "comment": null, "summary": "The rapid adoption of large language models (LLMs) in digital health has been driven by a \"scaling-first\" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c0f\u578b\u667a\u80fd\u4f53\u7fa4\uff08SAG\uff09\u4f5c\u4e3a\u66ff\u4ee3\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u8303\u5f0f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u540c\u63a8\u7406\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6548\u679c\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6548\u7387\u5e73\u8861", "motivation": "\u5f53\u524d\u6570\u5b57\u5065\u5eb7\u9886\u57df\u8fc7\u5ea6\u4f9d\u8d56\"\u89c4\u6a21\u4f18\u5148\"\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u8303\u5f0f\uff0c\u4f46\u4e34\u5e8a\u5b9e\u9645\u9700\u6c42\u4e0d\u4ec5\u5305\u62ec\u6548\u679c\uff0c\u8fd8\u9700\u8981\u53ef\u9760\u6027\u548c\u5408\u7406\u7684\u90e8\u7f72\u6210\u672c\u3002\u4e34\u5e8a\u51b3\u7b56\u672c\u8d28\u4e0a\u662f\u534f\u4f5c\u6027\u7684\uff0c\u56e0\u6b64\u9700\u8981\u6311\u6218\u5355\u4e00\u6a21\u578b\u6269\u5c55\u7684\u8303\u5f0f", "method": "\u63d0\u51fa\u5c0f\u578b\u667a\u80fd\u4f53\u7fa4\uff08SAG\uff09\u65b9\u6cd5\uff0c\u4ece\u5355\u4e00\u6a21\u578b\u667a\u80fd\u8f6c\u5411\u96c6\u4f53\u4e13\u4e1a\u77e5\u8bc6\uff0c\u901a\u8fc7\u534f\u4f5c\u5ba1\u8bae\u8fc7\u7a0b\u5206\u914d\u63a8\u7406\u3001\u5faa\u8bc1\u5206\u6790\u548c\u5173\u952e\u5ba1\u6838\u4efb\u52a1", "result": "SAG\u5728\u6548\u679c\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6210\u672c\u7b49\u591a\u4e2a\u4e34\u5e8a\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u4e00\u5927\u578b\u6a21\u578b\uff0c\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u989d\u5916\u4f18\u5316\u6216\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u3002\u534f\u540c\u63a8\u7406\u53ef\u4ee5\u66ff\u4ee3\u6a21\u578b\u53c2\u6570\u589e\u957f", "conclusion": "SAG\u4e3a\u6570\u5b57\u5065\u5eb7\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u66f4\u597d\u5730\u5e73\u8861\u6548\u679c\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6548\u7387\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7684\"\u89c4\u6a21\u4f18\u5148\"\u8303\u5f0f"}}
{"id": "2602.07425", "categories": ["cs.LG", "cs.CL", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.07425", "abs": "https://arxiv.org/abs/2602.07425", "authors": ["Dingzhi Yu", "Hongyi Tao", "Yuanyu Wan", "Luo Luo", "Lijun Zhang"], "title": "Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise", "comment": "Code available at https://github.com/Dingzhen230/Heavy-tailed-Noise-in-LLMs", "summary": "While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.", "AI": {"tldr": "\u672c\u6587\u4ece\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u89d2\u5ea6\u89e3\u91ca\u4e86\u7b26\u53f7\u4f18\u5316\u7b97\u6cd5\u5728\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u4f18\u4e8e\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5e7f\u4e49\u91cd\u5c3e\u566a\u58f0\u6a21\u578b\uff0c\u4e3aSignSGD\u3001Lion\u3001Muon\u7b49\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790", "motivation": "\u5c3d\u7ba1\u7b26\u53f7\u4f18\u5316\u7b97\u6cd5\uff08\u5982Lion\u548cMuon\uff09\u5728\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u8868\u73b0\u51fa\u4f18\u4e8eAdamW\u7684\u5b9e\u8bc1\u6027\u80fd\uff0c\u4f46\u5176\u7406\u8bba\u539f\u56e0\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u8fd9\u4e00\u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u5e38\u89c1\u7684\u73b0\u8c61\uff0c\u6765\u5f25\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5e7f\u4e49\u91cd\u5c3e\u566a\u58f0\u6761\u4ef6\uff0c\u6bd4\u6807\u51c6\u6709\u9650\u65b9\u5dee\u5047\u8bbe\u66f4\u51c6\u786e\u5730\u6355\u6349\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u3002\u5728\u6b64\u566a\u58f0\u6a21\u578b\u4e0b\uff0c\u4e3a\u5e7f\u4e49\u5149\u6ed1\u51fd\u6570\u7c7b\u5efa\u7acb\u4e86SignSGD\u548cLion\u7684\u5c16\u9510\u6536\u655b\u7387\u3002\u8fd8\u5c06\u5206\u6790\u6269\u5c55\u5230Muon\u548cMuonlight\uff0c\u9996\u6b21\u63d0\u4f9b\u4e86\u91cd\u5c3e\u968f\u673a\u6027\u4e0b\u77e9\u9635\u4f18\u5316\u7684\u4e25\u683c\u5206\u6790\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u7b26\u53f7\u4f18\u5316\u7b97\u6cd5\u5728\u91cd\u5c3e\u566a\u58f0\u6761\u4ef6\u4e0b\u5177\u6709\u4f18\u8d8a\u7684\u6536\u655b\u6027\u80fd\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u5148\u524d\u5df2\u77e5\u7684\u6700\u4f73\u8fb9\u754c\u3002\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u5e76\u786e\u8ba4\u63d0\u51fa\u7684\u566a\u58f0\u6a21\u578b\u4e0e\u5b9e\u8df5\u76f8\u7b26\u3002", "conclusion": "\u7b26\u53f7\u4f18\u5316\u7b97\u6cd5\u5929\u7136\u9002\u5408\u5904\u7406\u4e0e\u91cd\u5c3e\u76f8\u5173\u7684\u566a\u58f0\u68af\u5ea6\uff0c\u8fd9\u4e3a\u5b83\u4eec\u5728\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u5b9e\u8bc1\u4f18\u8d8a\u6027\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u7406\u8bba\u4f9d\u636e\u3002\u7814\u7a76\u7ed3\u679c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u57fa\u4e8e\u7b26\u53f7\u7684\u66f4\u65b0\u65b9\u6cd5\u4f18\u4e8e\u65b9\u5dee\u81ea\u9002\u5e94\u65b9\u6cd5\u3002"}}
{"id": "2602.07458", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07458", "abs": "https://arxiv.org/abs/2602.07458", "authors": ["Yancheng Long", "Yankai Yang", "Hongyang Wei", "Wei Chen", "Tianke Zhang", "Haonan fan", "Changyi Liu", "Kaiyu Jiang", "Jiankang Chen", "Kaiyu Tang", "Bin Wen", "Fan Yang", "Tingting Gao", "Han Li", "Shuo Yang"], "title": "SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning", "comment": null, "summary": "Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term \"Attention Collapse,\" where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.", "AI": {"tldr": "SpatialReward\uff1a\u901a\u8fc7\u7a7a\u95f4\u63a8\u7406\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5956\u52b1\u4fe1\u53f7\u95ee\u9898\uff0c\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u7f16\u8f91\u6548\u679c", "motivation": "\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u56fe\u50cf\u7f16\u8f91\u4e2d\u5f88\u6709\u524d\u666f\uff0c\u4f46\u9762\u4e34\u53ef\u9760\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u73b0\u6709\u8bc4\u4f30\u5668\u5b58\u5728\"\u6ce8\u610f\u529b\u5d29\u6e83\"\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5ffd\u89c6\u8de8\u56fe\u50cf\u6bd4\u8f83\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u611f\u77e5\u4e0d\u51c6\u786e\u548c\u5206\u6570\u6821\u51c6\u9519\u8bef\u3002", "method": "\u63d0\u51faSpatialReward\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u63a8\u7406\u5b9e\u73b0\u7cbe\u786e\u9a8c\u8bc1\u3002\u901a\u8fc7\u5c06\u63a8\u7406\u951a\u5b9a\u5230\u9884\u6d4b\u7684\u7f16\u8f91\u533a\u57df\uff0c\u5c06\u8bed\u4e49\u5224\u65ad\u57fa\u4e8e\u50cf\u7d20\u7ea7\u8bc1\u636e\u3002\u5728\u7cbe\u5fc3\u7b56\u5212\u7684260k\u7a7a\u95f4\u611f\u77e5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728MMRB2\u548cEditReward-Bench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u63d0\u51fa\u7684MultiEditReward-Bench\u4e0a\u4f18\u4e8e\u4e13\u6709\u8bc4\u4f30\u5668\u3002\u5728\u5728\u7ebfRL\u4e2d\u4f5c\u4e3a\u5f3a\u5927\u4fe1\u53f7\uff0c\u5c06OmniGen2\u5728GEdit-Bench\u4e0a\u63d0\u5347+0.90\uff0c\u8d85\u8fc7\u9886\u5148\u7684\u5224\u522b\u6a21\u578b\uff0c\u662fGPT-4.1\u589e\u76ca\u7684\u4e24\u500d\u3002", "conclusion": "\u7a7a\u95f4\u63a8\u7406\u5bf9\u4e8e\u89e3\u9501\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u6709\u6548\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0cSpatialReward\u901a\u8fc7\u50cf\u7d20\u7ea7\u8bc1\u636e\u548c\u7a7a\u95f4\u611f\u77e5\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u7f16\u8f91\u6548\u679c\u3002"}}
{"id": "2602.07429", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07429", "abs": "https://arxiv.org/abs/2602.07429", "authors": ["Yuanxu Sun", "Yuezhou Ma", "Haixu Wu", "Guanyang Zeng", "Muye Chen", "Jianmin Wang", "Mingsheng Long"], "title": "Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers", "comment": null, "summary": "Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric B\u00e9zier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.", "AI": {"tldr": "Brep2Shape\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u4efb\u52a1\u548c\u5bf9\u5076Transformer\u67b6\u6784\uff0c\u5f25\u5408B-rep\u8868\u793a\u4e2d\u8fde\u7eed\u65b9\u6cd5\u4e0e\u79bb\u6563\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u62bd\u8c61\u8fb9\u754c\u8868\u793a\u4e0e\u76f4\u89c2\u5f62\u72b6\u8868\u793a\u7684\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5904\u7406B-rep\u6a21\u578b\u5b58\u5728\u8868\u793a\u5dee\u8ddd\uff1a\u8fde\u7eed\u65b9\u6cd5\u63d0\u4f9b\u5206\u6790\u7cbe\u5ea6\u4f46\u89c6\u89c9\u62bd\u8c61\uff0c\u79bb\u6563\u65b9\u6cd5\u63d0\u4f9b\u76f4\u89c2\u6e05\u6670\u5ea6\u4f46\u727a\u7272\u51e0\u4f55\u7cbe\u5ea6\u3002\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u4ee5\u66f4\u597d\u5730\u5904\u7406CAD\u6a21\u578b\u3002", "method": "\u63d0\u51faBrep2Shape\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u91c7\u7528\u51e0\u4f55\u611f\u77e5\u4efb\u52a1\u8ba9\u6a21\u578b\u4ece\u53c2\u6570\u5316B\u00e9zier\u63a7\u5236\u70b9\u9884\u6d4b\u5bc6\u96c6\u7a7a\u95f4\u70b9\uff1b\u4f7f\u7528\u5bf9\u5076Transformer\u4e3b\u5e72\u5e76\u884c\u7f16\u7801\u66f2\u9762\u548c\u66f2\u7ebftoken\u4ee5\u6355\u6349\u4e0d\u540c\u51e0\u4f55\u7279\u6027\uff1b\u96c6\u6210\u62d3\u6251\u6ce8\u610f\u529b\u5efa\u6a21\u66f2\u9762\u4e0e\u66f2\u7ebf\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eBrep2Shape\u5177\u6709\u663e\u8457\u53ef\u6269\u5c55\u6027\uff0c\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "Brep2Shape\u6210\u529f\u5f25\u5408\u4e86B-rep\u8868\u793a\u4e2d\u7684\u8868\u793a\u5dee\u8ddd\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5b9e\u73b0\u4e86\u62bd\u8c61\u8fb9\u754c\u8868\u793a\u4e0e\u76f4\u89c2\u5f62\u72b6\u8868\u793a\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u4e3aCAD\u6a21\u578b\u7684\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07463", "abs": "https://arxiv.org/abs/2602.07463", "authors": ["Misbah Ijaz", "Saif Ur Rehman Khan", "Abd Ur Rehman", "Tayyaba Asif", "Sebastian Vollmer", "Andreas Dengel", "Muhammad Nabeel Asim"], "title": "GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring", "comment": null, "summary": "The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86GlobalWasteData\uff08GWD\uff09\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b89,807\u5f20\u56fe\u50cf\u3001\u6db5\u76d614\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c68\u4e2a\u5b50\u7c7b\u7684\u5927\u89c4\u6a21\u5e9f\u7269\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u516c\u5f00\u5e9f\u7269\u6570\u636e\u96c6\u788e\u7247\u5316\u3001\u4e0d\u4e00\u81f4\u548c\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u5e9f\u7269\u589e\u957f\u9700\u8981\u9ad8\u6548\u7684\u81ea\u52a8\u5206\u7c7b\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u516c\u5f00\u5e9f\u7269\u6570\u636e\u96c6\u5b58\u5728\u788e\u7247\u5316\u3001\u4e0d\u4e00\u81f4\u3001\u6807\u6ce8\u683c\u5f0f\u4e0d\u7edf\u4e00\u3001\u56fe\u50cf\u6761\u4ef6\u5dee\u5f02\u5927\u3001\u7c7b\u522b\u5206\u5e03\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u96be\u4ee5\u8bad\u7ec3\u51fa\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u521b\u5efa\u7edf\u4e00\u7684GWD\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8d28\u91cf\u8fc7\u6ee4\u3001\u91cd\u590d\u79fb\u9664\u548c\u5143\u6570\u636e\u751f\u6210\u7b49\u9884\u5904\u7406\uff0c\u63d0\u4f9b\u4e00\u81f4\u7684\u6807\u6ce8\u3001\u6539\u8fdb\u7684\u9886\u57df\u591a\u6837\u6027\u548c\u66f4\u5e73\u8861\u7684\u7c7b\u522b\u8868\u793a\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b89,807\u5f20\u56fe\u50cf\u300114\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c68\u4e2a\u5b50\u7c7b\u7684GWD\u6570\u636e\u96c6\uff0c\u5177\u6709\u4e00\u81f4\u7684\u6807\u6ce8\u3001\u66f4\u597d\u7684\u9886\u57df\u591a\u6837\u6027\u548c\u66f4\u5e73\u8861\u7684\u7c7b\u522b\u5206\u5e03\uff0c\u4e3a\u5e9f\u7269\u8bc6\u522b\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002", "conclusion": "GWD\u6570\u636e\u96c6\u4e3a\u73af\u5883\u76d1\u6d4b\u3001\u56de\u6536\u81ea\u52a8\u5316\u548c\u5e9f\u7269\u8bc6\u522b\u7b49\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u516c\u5f00\u53ef\u7528\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u548c\u53ef\u91cd\u590d\u6027\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u9c81\u68d2\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u5e9f\u7269\u8bc6\u522b\u6a21\u578b\u3002"}}
{"id": "2602.08030", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08030", "abs": "https://arxiv.org/abs/2602.08030", "authors": ["Yilun Zheng", "Dongyang Ma", "Tian Liang", "Jiahao Xu", "Xinting Huang", "Lijie Chen", "Haitao Mi", "Yan Wang"], "title": "Free(): Learning to Forget in Malloc-Only Reasoning Models", "comment": null, "summary": "Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.\n  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.", "AI": {"tldr": "Free()LM\u901a\u8fc7\u5f15\u5165\u81ea\u6211\u9057\u5fd8\u673a\u5236\u89e3\u51b3\u63a8\u7406\u6a21\u578b\u8fc7\u5ea6\u601d\u8003\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7d27\u51d1\u65e0\u566a\u58f0\u72b6\u6001\u7684\u540c\u65f6\u63d0\u5347\u5404\u79cd\u89c4\u6a21\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u6096\u8bba\uff1a\u8fc7\u591a\u7684\u601d\u8003\u6807\u8bb0\u5f80\u5f80\u964d\u4f4e\u6027\u80fd\u800c\u975e\u63d0\u5347\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u6807\u51c6LLM\u4f5c\u4e3a\"\u4ec5\u5206\u914d\u5185\u5b58\"\u5f15\u64ce\uff0c\u6301\u7eed\u7d2f\u79ef\u6709\u6548\u548c\u5197\u4f59\u6b65\u9aa4\uff0c\u7f3a\u4e4f\u4fee\u526a\u8fc7\u65f6\u4fe1\u606f\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faFree()LM\u6a21\u578b\uff0c\u901a\u8fc7Free-Module\uff08\u5373\u63d2\u5373\u7528\u7684LoRA\u9002\u914d\u5668\uff09\u5f15\u5165\u5185\u5728\u7684\u81ea\u6211\u9057\u5fd8\u80fd\u529b\u3002\u6a21\u578b\u5728\u63a8\u7406\u6a21\u5f0f\u548c\u6e05\u7406\u6a21\u5f0f\u4e4b\u95f4\u8fed\u4ee3\u5207\u6362\uff0c\u52a8\u6001\u8bc6\u522b\u5e76\u4fee\u526a\u65e0\u7528\u7684\u4e0a\u4e0b\u6587\u5757\uff0c\u4fdd\u6301\u7d27\u51d1\u65e0\u566a\u58f0\u72b6\u6001\u3002", "result": "Free()LM\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\uff088B\u5230685B\uff09\u4e0a\u90fd\u63d0\u4f9b\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u5e73\u5747\u6bd4\u9876\u7ea7\u63a8\u7406\u57fa\u7ebf\u63d0\u53473.3%\uff0c\u5728IMOanswerBench\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\uff0c\u6807\u51c6Qwen3-235B-A22B\u6a21\u578b\u5b8c\u5168\u5d29\u6e83\uff080%\u51c6\u786e\u7387\uff09\uff0c\u800cFree()LM\u5c06\u6027\u80fd\u6062\u590d\u523050%\u3002", "conclusion": "\u53ef\u6301\u7eed\u667a\u80fd\u9700\u8981\u9057\u5fd8\u7684\u81ea\u7531\u4e0e\u601d\u8003\u7684\u80fd\u529b\u540c\u7b49\u91cd\u8981\u3002\u81ea\u6211\u9057\u5fd8\u673a\u5236\u662f\u89e3\u51b3\u63a8\u7406\u6a21\u578b\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.07440", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07440", "abs": "https://arxiv.org/abs/2602.07440", "authors": ["C\u00e9dric Jung", "Shirin Salehi", "Anke Schmeink"], "title": "Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis", "comment": null, "summary": "Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\\% fewer samples, while reducing the acquisition cost by almost half.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u591a\u79cd\u805a\u5408\u7ed3\u6784\u6765\u89e3\u51b3\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u56f0\u5883\uff0c\u901a\u8fc7\u7ec4\u5408\u4e0d\u540c\u83b7\u53d6\u51fd\u6570\u6765\u5e73\u8861\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u66f4\u8282\u80fd\u7684AI\u8bad\u7ec3\u3002", "motivation": "\u4e3b\u52a8\u5b66\u4e60\u867d\u7136\u80fd\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u548c\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u4f46\u4f20\u7edf\u83b7\u53d6\u51fd\u6570\u5b58\u5728\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u6743\u8861\u95ee\u9898\uff0c\u4ee5\u53ca\u6279\u5904\u7406\u6a21\u5f0f\u6548\u7387\u4f4e\u4e0b\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002\u9700\u8981\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u805a\u5408\u7b56\u7565\u6765\u5e73\u8861\u51c6\u786e\u6027\u548c\u80fd\u8017\u3002", "method": "\u63d0\u51fa\u4e86\u516d\u79cd\u805a\u5408\u7ed3\u6784\uff1a\u4e32\u8054\u3001\u5e76\u8054\u3001\u6df7\u5408\u3001\u81ea\u9002\u5e94\u53cd\u9988\u3001\u968f\u673a\u63a2\u7d22\u548c\u9000\u706b\u63a2\u7d22\u3002\u8fd9\u4e9b\u7ed3\u6784\u901a\u8fc7\u7ec4\u5408\u57fa\u4e8e\u4ee3\u8868\u6027\u7684\u83b7\u53d6\u51fd\u6570\uff08\u5982K-Centers\uff09\u548c\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u83b7\u53d6\u51fd\u6570\uff08\u5982BALD\u3001BADGE\uff09\u6765\u89e3\u51b3\u63a2\u7d22-\u5229\u7528\u56f0\u5883\u3002", "result": "\u805a\u5408\u83b7\u53d6\u51fd\u6570\u6709\u6548\u7f13\u89e3\u4e86\u6279\u5904\u7406\u6a21\u5f0f\u6548\u7387\u4f4e\u4e0b\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002\u4ea4\u66ff\u4f7f\u7528BALD\u548cBADGE\u7b49\u65b9\u6cd5\u663e\u793a\u51fa\u7a33\u5065\u7ed3\u679c\uff0cK-Centers\u540e\u63a5BALD\u7684\u4e32\u8054\u7ed3\u6784\u80fd\u4ee5\u5c1112%\u7684\u6837\u672c\u8fbe\u5230\u76f8\u540c\u6027\u80fd\u76ee\u6807\uff0c\u540c\u65f6\u5c06\u83b7\u53d6\u6210\u672c\u964d\u4f4e\u8fd1\u4e00\u534a\u3002", "conclusion": "\u63d0\u51fa\u7684\u805a\u5408\u7ed3\u6784\u80fd\u591f\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u6301\u7eed\u3001\u80fd\u6e90\u611f\u77e5\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07493", "abs": "https://arxiv.org/abs/2602.07493", "authors": ["Tianhao Zhou", "Yujia Chen", "Zhihao Zhan", "Yuhang Ming", "Jianzhu Huai"], "title": "Thermal odometry and dense mapping using learned ddometry and Gaussian splatting", "comment": "11 pages, 2 figures, 5 tables", "summary": "Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.", "AI": {"tldr": "TOM-GS\uff1a\u9996\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684SLAM\u7cfb\u7edf\uff0c\u4e13\u4e3a\u70ed\u6210\u50cf\u76f8\u673a\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5b66\u4e60\u5f0f\u91cc\u7a0b\u8ba1\u4e0e\u5bc6\u96c6\u5efa\u56fe\uff0c\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u8fd0\u52a8\u4f30\u8ba1\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "motivation": "\u70ed\u7ea2\u5916\u4f20\u611f\u5668\u80fd\u5728\u9ed1\u6697\u3001\u7070\u5c18\u548c\u70df\u96fe\u7b49\u6076\u52a3\u6761\u4ef6\u4e0b\u7a33\u5b9a\u6210\u50cf\uff0c\u4f46\u73b0\u6709\u70ed\u6210\u50cf\u91cc\u7a0b\u8ba1\u548c\u5efa\u56fe\u65b9\u6cd5\u4e3b\u8981\u662f\u51e0\u4f55\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\u4e14\u65e0\u6cd5\u751f\u6210\u5bc6\u96c6\u5730\u56fe\u3002\u53d7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u9ad8\u6548\u9ad8\u8d28\u91cf\u91cd\u5efa\u80fd\u529b\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u70ed\u6210\u50cf\u76f8\u673a\u7684SLAM\u7cfb\u7edf\u3002", "method": "\u63d0\u51faTOM-GS\u65b9\u6cd5\uff0c\u5c06\u5b66\u4e60\u5f0f\u91cc\u7a0b\u8ba1\u4e0e\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u5bc6\u96c6\u5efa\u56fe\u76f8\u7ed3\u5408\u3002\u7cfb\u7edf\u5305\u542b\u4e13\u95e8\u7684\u70ed\u56fe\u50cf\u589e\u5f3a\u6a21\u5757\u548c\u5355\u76ee\u6df1\u5ea6\u96c6\u6210\u6a21\u5757\uff0c\u662f\u9996\u4e2a\u9488\u5bf9\u70ed\u6210\u50cf\u76f8\u673a\u7684\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\u3002", "result": "\u5728\u8fd0\u52a8\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u65b9\u9762\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTOM-GS\u4f18\u4e8e\u73b0\u6709\u7684\u5b66\u4e60\u5f0f\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5b66\u4e60\u5f0f\u6d41\u6c34\u7ebf\u5728\u9c81\u68d2\u70ed\u6210\u50cf\u91cc\u7a0b\u8ba1\u548c\u5bc6\u96c6\u91cd\u5efa\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "TOM-GS\u6210\u529f\u5c55\u793a\u4e86\u5c06\u5b66\u4e60\u5f0f\u91cc\u7a0b\u8ba1\u4e0e\u9ad8\u65af\u6cfc\u6e85\u5efa\u56fe\u76f8\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u70ed\u6210\u50cf\u76f8\u673a\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684SLAM\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07441", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07441", "abs": "https://arxiv.org/abs/2602.07441", "authors": ["Jinzong Dong", "Wei Huang", "Jianshu Zhang", "Zhuo Chen", "Xinzhe Yuan", "Qinying Gu", "Zhaohui Jiang", "Nanyang Ye"], "title": "Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.", "AI": {"tldr": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u5929\u82b1\u677f\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u8fd1\u7aef\u52a8\u4f5c\u66ff\u6362\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u7684\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\u867d\u7136\u80fd\u4ea7\u751f\u73b0\u5b9e\u7b56\u7565\u5e76\u7f13\u89e3\u5206\u5e03\u5916\u52a8\u4f5c\u7684\u504f\u5dee\uff0c\u4f46\u5f53\u6570\u636e\u96c6\u52a8\u4f5c\u6b21\u4f18\u65f6\uff0c\u4e0d\u52a0\u533a\u5206\u7684\u6a21\u4eff\u4f1a\u9650\u5236\u6f14\u5458\u5145\u5206\u5229\u7528\u8bc4\u8bba\u5bb6\u5efa\u8bae\u7684\u9ad8\u4ef7\u503c\u533a\u57df\uff0c\u5f62\u6210\u6027\u80fd\u5929\u82b1\u677f\u3002", "method": "\u63d0\u51fa\u8fd1\u7aef\u52a8\u4f5c\u66ff\u6362\uff08PAR\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u6837\u672c\u66ff\u6362\u5668\uff0c\u9010\u6b65\u7528\u7a33\u5b9a\u6f14\u5458\u751f\u6210\u7684\u9ad8\u4ef7\u503c\u52a8\u4f5c\u66ff\u6362\u4f4e\u4ef7\u503c\u52a8\u4f5c\uff0c\u6269\u5927\u52a8\u4f5c\u63a2\u7d22\u7a7a\u95f4\u540c\u65f6\u51cf\u5c11\u4f4e\u4ef7\u503c\u6570\u636e\u7684\u5f71\u54cd\u3002PAR\u517c\u5bb9\u591a\u79cd\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u8303\u5f0f\u3002", "result": "\u5728\u53d7\u63a7\u8fde\u7eed\u8d4c\u535a\u673a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u5929\u82b1\u677f\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPAR\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u4e0e\u57fa\u7840TD3+BC\u7ed3\u5408\u65f6\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "PAR\u65b9\u6cd5\u6709\u6548\u7a81\u7834\u4e86\u884c\u4e3a\u514b\u9686\u6b63\u5219\u5316\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\u7684\u6027\u80fd\u5929\u82b1\u677f\uff0c\u901a\u8fc7\u667a\u80fd\u66ff\u6362\u8bad\u7ec3\u6837\u672c\u4e2d\u7684\u52a8\u4f5c\uff0c\u4f7f\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u80fd\u66f4\u597d\u5730\u5229\u7528\u9ad8\u4ef7\u503c\u533a\u57df\uff0c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2602.07495", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.07495", "abs": "https://arxiv.org/abs/2602.07495", "authors": ["Jiawen Zheng", "Haonan Jia", "Ming Li", "Yuhui Zheng", "Yufeng Zeng", "Yang Gao", "Chen Liang"], "title": "Learning Brain Representation with Hierarchical Visual Embeddings", "comment": null, "summary": "Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8111\u4fe1\u53f7-\u56fe\u50cf\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u8111\u4fe1\u53f7\u4e0e\u89c6\u89c9\u8868\u5f81\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u878d\u5408\u5148\u9a8c\u589e\u5f3a\u8de8\u6a21\u6001\u5206\u5e03\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u89e3\u7801\u65b9\u6cd5\u5927\u591a\u5173\u6ce8\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\u800c\u5ffd\u7565\u50cf\u7d20\u7ea7\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u6211\u4eec\u5bf9\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u7406\u89e3\u3002\u8111\u4fe1\u53f7\u662f\u5426\u771f\u6b63\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u66f4\u597d\u7684\u8111-\u56fe\u50cf\u5bf9\u9f50\u7b56\u7565\u3002", "method": "1) \u5229\u7528\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u5f52\u7eb3\u504f\u597d\u7684\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u6355\u6349\u5c42\u6b21\u5316\u3001\u591a\u5c3a\u5ea6\u7684\u89c6\u89c9\u8868\u5f81\uff1b2) \u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u5b9e\u73b0\u8111\u4fe1\u53f7\u4e0e\u89c6\u89c9\u5d4c\u5165\u7684\u6709\u6548\u5bf9\u9f50\uff1b3) \u5f15\u5165\u878d\u5408\u5148\u9a8c\uff0c\u5728\u5927\u89c4\u6a21\u89c6\u89c9\u6570\u636e\u4e0a\u5b66\u4e60\u7a33\u5b9a\u6620\u5c04\uff0c\u7136\u540e\u5c06\u8111\u7279\u5f81\u5339\u914d\u5230\u8fd9\u4e2a\u9884\u8bad\u7ec3\u5148\u9a8c\u4e2d\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u8111-\u56fe\u50cf\u5bf9\u9f50\u7b56\u7565\u80fd\u591f\u6709\u6548\u89e3\u7801\u8111\u4fe1\u53f7\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8868\u5f81\u548c\u878d\u5408\u5148\u9a8c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.08061", "categories": ["cs.AI", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2602.08061", "abs": "https://arxiv.org/abs/2602.08061", "authors": ["Doni Bloomfield", "Allison Berke", "Moritz S. Hanke", "Aaron Maiwald", "James R. M. Black", "Toby Webster", "Tina Hernandez-Boussard", "Oliver M. Crook", "Jassi Pannu"], "title": "Securing Dual-Use Pathogen Data of Concern", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Biosecurity Safeguards for Generative AI", "summary": "Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u5c42\u751f\u7269\u5b89\u5168\u6570\u636e\u7b49\u7ea7\uff08BDL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u75c5\u539f\u4f53\u6570\u636e\u5728\u8bad\u7ec3AI\u6a21\u578b\u65f6\u53ef\u80fd\u5e26\u6765\u7684\u751f\u7269\u5b89\u5168\u98ce\u9669\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7b49\u7ea7\u63d0\u51fa\u76f8\u5e94\u7684\u6280\u672f\u9650\u5236\u63aa\u65bd\uff0c\u65e8\u5728\u901a\u8fc7\u6570\u636e\u63a7\u5236\u51cf\u5c11\u751f\u7269AI\u80fd\u529b\u6269\u6563\u7684\u98ce\u9669\u3002", "motivation": "\u968f\u7740AI\u5728\u751f\u7269\u5b66\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bad\u7ec3\u6570\u636e\u6210\u4e3aAI\u6a21\u578b\u80fd\u529b\u7684\u5173\u952e\u51b3\u5b9a\u56e0\u7d20\u3002\u67d0\u4e9b\u7c7b\u578b\u7684\u751f\u7269\u6570\u636e\uff08\u5982\u75c5\u539f\u4f53\u76f8\u5173\u6570\u636e\uff09\u53ef\u80fd\u88ab\u7528\u4e8e\u5f00\u53d1\u751f\u7269\u6b66\u5668\u7b49\u6709\u5bb3\u5e94\u7528\u3002\u56fd\u9645\u7814\u7a76\u56e2\u4f53\u5df2\u547c\u5401\u5b9e\u65bd\u6570\u636e\u63a7\u5236\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u6846\u67b6\u3002\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u57fa\u4e8e\u98ce\u9669\u7684\u6570\u636e\u5206\u7c7b\u4f53\u7cfb\uff0c\u4ee5\u8bbe\u8ba1\u6709\u6548\u7684\u63a7\u5236\u63aa\u65bd\u3002", "method": "\u63d0\u51fa\u4e86\u4e94\u5c42\u751f\u7269\u5b89\u5168\u6570\u636e\u7b49\u7ea7\uff08BDL\uff09\u6846\u67b6\uff0c\u6839\u636e\u4e0d\u540c\u7c7b\u578b\u75c5\u539f\u4f53\u6570\u636e\u5728\u8bad\u7ec3AI\u6a21\u578b\u65f6\u53ef\u80fd\u5e26\u6765\u7684\u98ce\u9669\u7a0b\u5ea6\u8fdb\u884c\u5206\u7c7b\u3002\u6bcf\u4e2a\u7b49\u7ea7\u5305\u542b\u7279\u5b9a\u7684\u6570\u636e\u7c7b\u578b\uff0c\u5e76\u57fa\u4e8e\u5176\u9884\u671f\u5bf9\u5173\u6ce8\u80fd\u529b\u7684\u8d21\u732e\u7a0b\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002\u9488\u5bf9\u6bcf\u4e2aBDL\u5c42\u7ea7\uff0c\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7684\u6280\u672f\u9650\u5236\u63aa\u65bd\u3002\u6b64\u5916\uff0c\u8fd8\u4e3a\u65b0\u521b\u5efa\u7684\u53cc\u7528\u9014\u75c5\u539f\u4f53\u6570\u636e\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u6cbb\u7406\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u751f\u7269\u5b89\u5168\u6570\u636e\u5206\u7c7b\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u6570\u636e\u98ce\u9669\u7b49\u7ea7\u8fdb\u884c\u5206\u5c42\u7ba1\u7406\u3002\u8be5\u6846\u67b6\u4e3a\u4e0d\u540c\u98ce\u9669\u7ea7\u522b\u7684\u6570\u636e\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u6280\u672f\u63a7\u5236\u5efa\u8bae\uff0c\u5e76\u4e3a\u53cc\u7528\u9014\u75c5\u539f\u4f53\u6570\u636e\u6cbb\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u5728\u8ba1\u7b97\u548c\u7f16\u7801\u8d44\u6e90\u5e7f\u6cdb\u53ef\u53ca\u7684\u73af\u5883\u4e2d\uff0c\u6570\u636e\u63a7\u5236\u88ab\u8ba4\u4e3a\u662f\u51cf\u5c11\u5173\u6ce8\u751f\u7269AI\u80fd\u529b\u6269\u6563\u7684\u6700\u6709\u6548\u5e72\u9884\u624b\u6bb5\u4e4b\u4e00\u3002", "conclusion": "\u751f\u7269\u5b89\u5168\u6570\u636e\u7b49\u7ea7\uff08BDL\uff09\u6846\u67b6\u4e3a\u75c5\u539f\u4f53\u6570\u636e\u7684\u98ce\u9669\u5206\u7c7b\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002\u901a\u8fc7\u57fa\u4e8e\u98ce\u9669\u7684\u6570\u636e\u5206\u7c7b\u548c\u76f8\u5e94\u7684\u6280\u672f\u9650\u5236\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11AI\u6a21\u578b\u88ab\u7528\u4e8e\u6709\u5bb3\u751f\u7269\u5e94\u7528\u7684\u53ef\u80fd\u6027\u3002\u5728\u5f53\u4eca\u8ba1\u7b97\u8d44\u6e90\u666e\u53ca\u7684\u80cc\u666f\u4e0b\uff0c\u6570\u636e\u63a7\u5236\u662f\u9632\u6b62\u751f\u7269AI\u80fd\u529b\u6ee5\u7528\u7684\u5173\u952e\u5e72\u9884\u63aa\u65bd\uff0c\u9700\u8981\u56fd\u9645\u793e\u4f1a\u5171\u540c\u5b9e\u65bd\u76f8\u5e94\u7684\u6cbb\u7406\u6846\u67b6\u3002"}}
{"id": "2602.07498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07498", "abs": "https://arxiv.org/abs/2602.07498", "authors": ["Zhufeng Xu", "Xuan Gao", "Feng-Lin Liu", "Haoxian Zhang", "Zhixue Fang", "Yu-Kun Lai", "Xiaoqiang Liu", "Pengfei Wan", "Lin Gao"], "title": "IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation", "comment": null, "summary": "Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u6bcf\u5e27\u8fd0\u52a8\u538b\u7f29\u4e3a\u7d27\u51d1\u76841D\u8fd0\u52a8\u6807\u8bb0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u663e\u5f0f\u65b9\u6cd5\u7684\u7a7a\u95f4\u4e0d\u5339\u914d\u95ee\u9898\u548c\u9690\u5f0f\u65b9\u6cd5\u7684\u8eab\u4efd\u6cc4\u6f0f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u89d2\u8272\u52a8\u753b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u89d2\u8272\u52a8\u753b\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff1a\u663e\u5f0f\u65b9\u6cd5\uff08\u5982\u9aa8\u67b6\u3001DWPose\u7b49\uff09\u96be\u4ee5\u5904\u7406\u7a7a\u95f4\u4e0d\u5339\u914d\u548c\u8eab\u4f53\u6bd4\u4f8b\u53d8\u5316\uff1b\u9690\u5f0f\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u9ad8\u7ea7\u8fd0\u52a8\u8bed\u4e49\uff0c\u4f46\u5b58\u5728\u8eab\u4efd\u4fe1\u606f\u6cc4\u6f0f\u548c\u8fd0\u52a8\u4e0e\u5916\u89c2\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "method": "1. \u63d0\u51fa\u65b0\u9896\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\uff0c\u5c06\u6bcf\u5e27\u8fd0\u52a8\u538b\u7f29\u4e3a\u7d27\u51d1\u76841D\u8fd0\u52a8\u6807\u8bb0\uff1b2. \u8bbe\u8ba1\u57fa\u4e8e\u65f6\u95f4\u4e00\u81f4\u63a9\u7801\u6807\u8bb0\u7684\u91cd\u5b9a\u5411\u6a21\u5757\uff0c\u901a\u8fc7\u65f6\u95f4\u8bad\u7ec3\u74f6\u9888\u51cf\u5c11\u6e90\u56fe\u50cf\u8fd0\u52a8\u7684\u5e72\u6270\uff1b3. \u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u4fdd\u771f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u548cIM-Animation\u751f\u6210\u80fd\u529b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7a7a\u95f4\u4e0d\u5339\u914d\u548c\u8eab\u4efd\u6cc4\u6f0f\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u9690\u5f0f\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\u901a\u8fc71D\u8fd0\u52a8\u6807\u8bb0\u548c\u63a9\u7801\u6807\u8bb0\u91cd\u5b9a\u5411\u6a21\u5757\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u89d2\u8272\u52a8\u753b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u7684\u89d2\u8272\u52a8\u753b\u751f\u6210\u3002"}}
{"id": "2602.07465", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07465", "abs": "https://arxiv.org/abs/2602.07465", "authors": ["Seungwoo Son", "Ingyu Seong", "Junhan Kim", "Hyemi Jang", "Yongkweon Jeon"], "title": "On the Importance of a Multi-Scale Calibration for Quantization", "comment": "ICASSP 2026", "summary": "Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.", "AI": {"tldr": "MaCa\u65b9\u6cd5\u901a\u8fc7\u591a\u5c3a\u5ea6\u5e8f\u5217\u957f\u5ea6\u611f\u77e5\u7684Hessian\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPTQ\u4e2d\u56fa\u5b9a\u957f\u5ea6\u6821\u51c6\u96c6\u65e0\u6cd5\u9002\u5e94LLM\u53d8\u957f\u8f93\u5165\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u957f\u5ea6\u7684\u968f\u673a\u5e8f\u5217\u4f5c\u4e3a\u6821\u51c6\u96c6\uff0c\u5ffd\u7565\u4e86LLM\u8f93\u5165\u7684\u53d8\u957f\u7279\u6027\u3002\u8f93\u5165\u957f\u5ea6\u76f4\u63a5\u5f71\u54cd\u6fc0\u6d3b\u5206\u5e03\u548cHessian\u77e9\u9635\u6355\u83b7\u7684\u6743\u91cd\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u57fa\u4e8e\u56fa\u5b9a\u957f\u5ea6\u6821\u51c6\u7684Hessian\u4f30\u8ba1\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4e0d\u540c\u8f93\u5165\u573a\u666f\u4e0b\u7684\u771f\u5b9e\u6743\u91cd\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faMaCa\u65b9\u6cd5\uff1a1) \u5c06\u591a\u5c3a\u5ea6\u5e8f\u5217\u957f\u5ea6\u4fe1\u606f\u878d\u5165Hessian\u4f30\u8ba1\uff1b2) \u5c06\u6bcf\u4e2a\u5e8f\u5217\u4f5c\u4e3a\u72ec\u7acb\u6837\u672c\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684Hessian\u77e9\u9635\u7528\u4e8e\u7cbe\u786e\u91cf\u5316\u3002", "result": "\u5728Qwen3\u3001Gemma3\u3001LLaMA3\u7b49\u5148\u8fdbLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMaCa\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u80fd\u6301\u7eed\u63d0\u5347\u51c6\u786e\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u589e\u5f3a\uff0c\u4e0e\u73b0\u6709PTQ\u6846\u67b6\u517c\u5bb9\u3002", "conclusion": "MaCa\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u5730\u5f3a\u8c03\u591a\u5c3a\u5ea6\u6821\u51c6\u5728LLM\u91cf\u5316\u4e2d\u4f5c\u7528\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u957f\u5ea6\u611f\u77e5\u7684Hessian\u6784\u5efa\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u56fa\u5b9a\u957f\u5ea6\u6821\u51c6\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u91cf\u5316\u65b9\u6848\u3002"}}
{"id": "2602.07512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07512", "abs": "https://arxiv.org/abs/2602.07512", "authors": ["Tao Wang", "Chenyu Lin", "Chenwei Tang", "Jizhe Zhou", "Deng Xiong", "Jianan Li", "Jian Zhao", "Jiancheng Lv"], "title": "Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection", "comment": "paper accepted by ISPRS Journal of Photogrammetry and Remote Sensing ( IF=12.2)", "summary": "Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \\textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faZoomDet\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u653e\u5927\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u5c0f\u76ee\u6807\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u504f\u79fb\u9884\u6d4b\u548c\u975e\u5747\u5300\u653e\u5927\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347mAP\u3002", "motivation": "\u65e0\u4eba\u673a\u62cd\u6444\u7684\u56fe\u50cf\u4e2d\u76ee\u6807\u901a\u5e38\u8f83\u5c0f\u4e14\u7a00\u758f\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u4f18\u5316\u3002\u4e3a\u4e86\u89e3\u51b3\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u9700\u8981\u81ea\u9002\u5e94\u5730\u653e\u5927\u76ee\u6807\u533a\u57df\u4ee5\u66f4\u597d\u5730\u6355\u6349\u76ee\u6807\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u653e\u5927\u6846\u67b6ZoomDet\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u8bbe\u8ba1\uff1a1) \u8f7b\u91cf\u7ea7\u504f\u79fb\u9884\u6d4b\u65b9\u6848\u7ed3\u5408\u57fa\u4e8e\u6846\u7684\u653e\u5927\u76ee\u6807\uff0c\u5b66\u4e60\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u975e\u5747\u5300\u653e\u5927\uff1b2) \u89d2\u70b9\u5bf9\u9f50\u7684\u8fb9\u754c\u6846\u53d8\u6362\u65b9\u6cd5\uff0c\u5c06\u771f\u5b9e\u6846\u53d8\u6362\u5230\u653e\u5927\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u9884\u6d4b\u6846\u53d8\u6362\u56de\u539f\u59cb\u7a7a\u95f4\u3002", "result": "\u5728VisDrone\u3001UAVDT\u548cSeaDronesSee\u4e09\u4e2a\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u5728SeaDronesSee\u6570\u636e\u96c6\u4e0a\uff0cZoomDet\u4f7fFaster R-CNN\u6a21\u578b\u7684mAP\u63d0\u5347\u8d85\u8fc78.4\u4e2a\u7edd\u5bf9\u767e\u5206\u70b9\uff0c\u4ec5\u589e\u52a0\u7ea63ms\u5ef6\u8fdf\u3002\u8be5\u65b9\u6cd5\u4e0e\u67b6\u6784\u65e0\u5173\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u610f\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u3002", "conclusion": "ZoomDet\u662f\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u653e\u5927\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u5c0f\u76ee\u6807\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u67b6\u6784\u65e0\u5173\u6027\u548c\u4f4e\u5ef6\u8fdf\u7279\u70b9\uff0c\u4e3a\u89e3\u51b3\u65e0\u4eba\u673a\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08104", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.08104", "abs": "https://arxiv.org/abs/2602.08104", "authors": ["Risal Shahriar Shefin", "Debashis Gupta", "Thai Le", "Sarra Alqahtani"], "title": "Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains \"downstream-first\" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u89e3\u91ca\u6545\u969c\u68c0\u6d4b\u4e0e\u5f52\u56e0\uff0c\u5305\u62ec\u8bc6\u522b\u521d\u59cb\u6545\u969c\u6e90\u3001\u9a8c\u8bc1\u591a\u7c73\u8bfa\u6548\u5e94\u548c\u8ffd\u8e2a\u6545\u969c\u4f20\u64ad\u8def\u5f84\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u6545\u969c\u68c0\u6d4b\u4e0e\u5f52\u56e0\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u9ed1\u76d2\u68c0\u6d4b\uff0c\u96be\u4ee5\u7406\u89e3\u6545\u969c\u4f20\u64ad\u673a\u5236\u3002", "method": "\u4e24\u9636\u6bb5\u68af\u5ea6\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u6210\u672c\u7684\u6cf0\u52d2\u4f59\u9879\u5206\u6790\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u4f53\u6545\u969c\u68c0\u6d4b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8bc4\u8bba\u5bb6\u5bfc\u6570\u7684\u51e0\u4f55\u5206\u6790\uff08\u4e00\u9636\u654f\u611f\u6027\u548c\u4e8c\u9636\u66f2\u7387\uff09\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u4f20\u67d3\u56fe\u3002", "result": "\u5728Simple Spread\uff083\u548c5\u667a\u80fd\u4f53\uff09\u548cStarCraft II\u73af\u5883\u4e2d\u8bc4\u4f30\uff0c\u4f7f\u7528MADDPG\u548cHATRPO\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e8688.2-99.4%\u7684\u521d\u59cb\u6545\u969c\u6e90\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u8bc1\u636e\u3002", "conclusion": "\u8be5\u6846\u67b6\u8d85\u8d8a\u4e86\u9ed1\u76d2\u68c0\u6d4b\uff0c\u63d0\u4f9b\u4e86\u68af\u5ea6\u5c42\u9762\u7684\u53ef\u89e3\u91ca\u6cd5\u533b\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7ea7\u8054\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2602.07472", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07472", "abs": "https://arxiv.org/abs/2602.07472", "authors": ["Yilun Chen", "Jiaqi Lu"], "title": "Bandit Allocational Instability", "comment": null, "summary": "When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \\cdot S_T=\u03a9(T^{\\frac{3}{2}})$ as $T\\rightarrow\\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $\u0398(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= \u03c9(\\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \\cdot S_T=\\tilde\u0398(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).", "AI": {"tldr": "\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u5728\u5206\u914d\u62c9\u53d6\u6b21\u6570\u65f6\u5b58\u5728\u5de8\u5927\u65b9\u5dee\uff0c\u8fd9\u5bf9\u5e73\u53f0\u8fd0\u8425\u548c\u7edf\u8ba1\u63a8\u65ad\u6709\u5bb3\u3002\u672c\u6587\u5f15\u5165\u5206\u914d\u53d8\u5f02\u6027\u4f5c\u4e3a\u65b0\u6307\u6807\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u9057\u61be\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\uff1aR_T\u00b7S_T=\u03a9(T^{3/2})\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u8c03\u7b97\u6cd5UCB-f\u6765\u5b9e\u73b0\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "motivation": "\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u5728\u5206\u914d\u62c9\u53d6\u6b21\u6570\u65f6\u4f1a\u4ea7\u751f\u5de8\u5927\u65b9\u5dee\uff0c\u8fd9\u5bf9\u73b0\u4ee3\u5e94\u7528\u5982\u5b66\u4e60\u589e\u5f3a\u7684\u5e73\u53f0\u8fd0\u8425\u548c\u540e\u8001\u864e\u673a\u7edf\u8ba1\u63a8\u65ad\u7279\u522b\u6709\u5bb3\u3002\u56e0\u6b64\u9700\u8981\u5f15\u5165\u65b0\u7684\u6027\u80fd\u6307\u6807\u6765\u91cf\u5316\u8fd9\u79cd\u5206\u914d\u53d8\u5f02\u6027\u3002", "method": "\u5f15\u5165\u5206\u914d\u53d8\u5f02\u6027\u4f5c\u4e3a\u65b0\u6027\u80fd\u6307\u6807\uff0c\u5b9a\u4e49\u4e3a\u5404\u81c2\u62c9\u53d6\u6b21\u6570\u6807\u51c6\u5dee\u7684\u6700\u5927\u503c\u3002\u5efa\u7acb\u4e86\u5206\u914d\u53d8\u5f02\u6027\u4e0e\u9057\u61be\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86UCB-f\u7b97\u6cd5\uff08\u7ecf\u5178UCB1\u7684\u63a8\u5e7f\uff09\uff0c\u901a\u8fc7\u53ef\u8c03\u53c2\u6570\u5b9e\u73b0\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u7684\u4efb\u610f\u6743\u8861\u70b9\u3002", "result": "\u8bc1\u660e\u4e86\u4efb\u4f55\u7b97\u6cd5\u7684\u9057\u61beR_T\u548c\u5206\u914d\u53d8\u5f02\u6027S_T\u5fc5\u987b\u6ee1\u8db3R_T\u00b7S_T=\u03a9(T^{3/2})\uff0c\u53ea\u8981R_T=o(T)\u3002\u8fd9\u8868\u660e\u4efb\u4f55\u6781\u5c0f\u6781\u5927\u9057\u61be\u6700\u4f18\u7b97\u6cd5\u5fc5\u987b\u627f\u53d7\u0398(T)\u7684\u6700\u574f\u60c5\u51b5\u5206\u914d\u53d8\u5f02\u6027\uff1b\u800c\u4efb\u4f55\u5177\u6709\u6b21\u7ebf\u6027\u6700\u574f\u60c5\u51b5\u9057\u61be\u7684\u7b97\u6cd5\u5fc5\u987b\u627f\u53d7S_T=\u03c9(\u221aT)\u3002UCB-f\u7b97\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u5e15\u7d2f\u6258\u524d\u6cbfR_T\u00b7S_T=\u0398\u0303(T^{3/2})\u4e0a\u7684\u4efb\u610f\u70b9\u3002", "conclusion": "\u591a\u81c2\u8001\u864e\u673a\u7b97\u6cd5\u5728\u5206\u914d\u53d8\u5f02\u6027\u4e0e\u9057\u61be\u4e4b\u95f4\u5b58\u5728\u57fa\u672c\u6743\u8861\uff0c\u8fd9\u79cd\u6743\u8861\u5bf9\u5e73\u53f0\u8fd0\u8425\u548c\u7edf\u8ba1\u63a8\u65ad\u6709\u91cd\u8981\u5f71\u54cd\u3002\u63d0\u51fa\u7684UCB-f\u7b97\u6cd5\u80fd\u591f\u7075\u6d3b\u8c03\u8282\u8fd9\u79cd\u6743\u8861\uff0c\u540c\u65f6\u89e3\u51b3\u4e86Praharaj\u548cKhamaru\uff082025\uff09\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2602.07475", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2602.07475", "abs": "https://arxiv.org/abs/2602.07475", "authors": ["Zhuomin Liang", "Liang Bai", "Xian Yang"], "title": "Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data", "comment": null, "summary": "scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.", "AI": {"tldr": "BGFormer\uff1a\u57fa\u4e8e\u4e8c\u5206\u56feTransformer\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u951a\u70b9\u6807\u8bb0\u5b9e\u73b0\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u65b9\u6cd5O(n\u00b2)\u590d\u6742\u5ea6\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u7ec6\u80de\u89c6\u4e3a\u5e8f\u5217\u4e2d\u7684\u6807\u8bb0\uff0c\u5176\u8ba1\u7b97\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3aO(n\u00b2)\uff0c\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBGFormer\uff08\u4e8c\u5206\u56feTransformer\u805a\u7c7b\u6a21\u578b\uff09\uff0c\u5f15\u5165\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u951a\u70b9\u6807\u8bb0\u4f5c\u4e3a\u5171\u4eab\u53c2\u8003\u70b9\u8868\u793a\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e8c\u5206\u56fe\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u7ec6\u80de\u4e0e\u951a\u70b9\u6807\u8bb0\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4f7f\u540c\u4e00\u7c7b\u522b\u7684\u7ec6\u80de\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u66f4\u63a5\u8fd1\u3002", "result": "BGFormer\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u7ec6\u80de\u6570\u91cf\u7684\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f7f\u5176\u80fd\u591f\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "BGFormer\u901a\u8fc7\u4e8c\u5206\u56fe\u6ce8\u610f\u529b\u673a\u5236\u548c\u951a\u70b9\u6807\u8bb0\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07532", "abs": "https://arxiv.org/abs/2602.07532", "authors": ["Krishnakant Singh", "Simone Schaub-Meyer", "Stefan Roth"], "title": "Evaluating Object-Centric Models beyond Object Discovery", "comment": "Project Page: https://guided-sa.github.io/eval-ocl/", "summary": "Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9762\u5411\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\uff08OCL\uff09\u6a21\u578b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5e76\u5f15\u5165\u7edf\u4e00\u7684\u8bc4\u4f30\u4efb\u52a1\u548c\u6307\u6807\u6765\u540c\u65f6\u8bc4\u4f30\u5b9a\u4f4d\u80fd\u529b\u548c\u8868\u793a\u6709\u7528\u6027\u3002", "motivation": "\u73b0\u6709OCL\u6a21\u578b\u7684\u8bc4\u4f30\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5bf9OCL\u6a21\u578b\u8868\u793a\u6709\u7528\u6027\u7684\u6d1e\u5bdf\u6709\u9650\uff1b2\uff09\u5b9a\u4f4d\u80fd\u529b\u548c\u8868\u793a\u6709\u7528\u6027\u4f7f\u7528\u5206\u79bb\u7684\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u4e00\u81f4\u3002", "method": "1\uff09\u4f7f\u7528\u6307\u4ee4\u8c03\u4f18\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u5728\u591a\u6837\u5316\u7684VQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1b2\uff09\u5f15\u5165\u7edf\u4e00\u7684\u8bc4\u4f30\u4efb\u52a1\u548c\u6307\u6807\uff0c\u540c\u65f6\u8bc4\u4f30\u5b9a\u4f4d\uff08where\uff09\u548c\u8868\u793a\u6709\u7528\u6027\uff08what\uff09\uff1b3\uff09\u5305\u542b\u4e00\u4e2a\u7b80\u5355\u7684\u591a\u7279\u5f81\u91cd\u5efa\u57fa\u7ebf\u4f5c\u4e3a\u53c2\u8003\u70b9\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30OCL\u6a21\u578b\u7684\u8868\u793a\u6709\u7528\u6027\uff0c\u6d88\u9664\u5206\u79bb\u8bc4\u4f30\u5e26\u6765\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u7279\u5f81\u91cd\u5efa\u57fa\u7ebf\u63d0\u4f9b\u53c2\u8003\u57fa\u51c6\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u3001\u7edf\u4e00\u7684OCL\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8bc4\u4f30OCL\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u3002"}}
{"id": "2602.08214", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08214", "abs": "https://arxiv.org/abs/2602.08214", "authors": ["Ziwei Wang", "Yuanhe Zhang", "Jing Chen", "Zhenhong Zhou", "Ruichao Liang", "Ruiying Du", "Ju Jia", "Cong Wu", "Yang Liu"], "title": "RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection", "comment": null, "summary": "Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9012\u5f52\u71b5\u6982\u5ff5\u91cf\u5316\u53cd\u601d\u8fc7\u7a0b\u4e2d\u7684\u8d44\u6e90\u6d88\u8017\u98ce\u9669\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1RECUR\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u53cd\u4e8b\u5b9e\u95ee\u9898\u89e6\u53d1\u5927\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u53cd\u601d\uff0c\u5bfc\u81f4\u8f93\u51fa\u957f\u5ea6\u589e\u52a011\u500d\u3001\u541e\u5410\u91cf\u4e0b\u964d90%\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u9700\u8981\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\uff0c\u8fd9\u9700\u8981\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u66f4\u9ad8\u7684\u8d44\u6e90\u6d88\u8017\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\u5bf9\u6297\u6027\u8f93\u5165\u53ef\u80fd\u89e6\u53d1\u5197\u4f59\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f46\u53cd\u601d\u8fc7\u7a0b\u672c\u8eab\uff08\u7279\u522b\u662f\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u53cd\u601d\u5e76\u6d88\u8017\u8fc7\u591a\u8ba1\u7b97\u80fd\u529b\u7684\u90e8\u5206\uff09\u5c1a\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u63a8\u7406\u672c\u8eab\u5b58\u5728\u7684\u5b89\u5168\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9012\u5f52\u71b5\u6982\u5ff5\u6765\u91cf\u5316\u53cd\u601d\u8fc7\u7a0b\u4e2d\u7684\u8d44\u6e90\u6d88\u8017\u98ce\u9669\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1RECUR\u653b\u51fb\u65b9\u6cd5\u3002RECUR\u901a\u8fc7\u9012\u5f52\u71b5\u5f15\u5bfc\u7684\u53cd\u4e8b\u5b9e\u5229\u7528\u548c\u53cd\u601d\uff0c\u6784\u5efa\u53cd\u4e8b\u5b9e\u95ee\u9898\u6765\u9a8c\u8bc1\u5927\u63a8\u7406\u6a21\u578b\u7684\u5185\u5728\u7f3a\u9677\u548c\u98ce\u9669\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u826f\u6027\u63a8\u7406\u4e0b\uff0c\u9012\u5f52\u71b5\u5448\u73b0\u660e\u663e\u7684\u4e0b\u964d\u8d8b\u52bf\u3002RECUR\u653b\u51fb\u7834\u574f\u4e86\u8fd9\u4e00\u8d8b\u52bf\uff0c\u80fd\u591f\u5c06\u8f93\u51fa\u957f\u5ea6\u589e\u52a0\u6700\u591a11\u500d\uff0c\u5e76\u5c06\u541e\u5410\u91cf\u964d\u4f4e90%\u3002", "conclusion": "\u672c\u6587\u4ece\u9012\u5f52\u71b5\u7684\u89d2\u5ea6\u63ed\u793a\u4e86\u5927\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u8d44\u6e90\u6d88\u8017\u5b89\u5168\u95ee\u9898\uff0c\u4e3a\u9c81\u68d2\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u89c6\u89d2\uff0c\u5e76\u8bc1\u660e\u4e86\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\u53ef\u80fd\u6210\u4e3a\u653b\u51fb\u76ee\u6807\u3002"}}
{"id": "2602.07534", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.07534", "abs": "https://arxiv.org/abs/2602.07534", "authors": ["Mowmita Parvin Hera", "Md. Shahriar Mahmud Kallol", "Shohanur Rahman Nirob", "Md. Badsha Bulbul", "Jubayer Ahmed", "M. Zhourul Islam", "Hazrat Ali", "Mohammmad Farhad Bulbul"], "title": "Fine-Grained Cat Breed Recognition with Global Context Vision Transformer", "comment": "4 pages, accepted at International Conference on Computer and Information Technology (ICCIT) 2025", "summary": "Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.", "AI": {"tldr": "\u4f7f\u7528GCViT-Tiny\u67b6\u6784\u5728\u725b\u6d25-IIIT\u5ba0\u7269\u6570\u636e\u96c6\u5b50\u96c6\u4e0a\u5b9e\u73b0\u732b\u54c1\u79cd\u5206\u7c7b\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u523092.00%\u6d4b\u8bd5\u51c6\u786e\u7387\u548c94.54%\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "motivation": "\u732b\u54c1\u79cd\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0d\u540c\u54c1\u79cd\u5728\u6bdb\u8272\u3001\u9762\u90e8\u7ed3\u6784\u548c\u56fe\u6848\u4e0a\u5dee\u5f02\u7ec6\u5fae\u3002\u9700\u8981\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u652f\u6301\u517d\u533b\u8bca\u65ad\u3001\u52a8\u7269\u6536\u5bb9\u6240\u7ba1\u7406\u548c\u79fb\u52a8\u7aef\u8bc6\u522b\u7cfb\u7edf\u7b49\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u89c6\u89c9\u53d8\u6362\u5668\uff08GCViT\uff09Tiny\u67b6\u6784\u8fdb\u884c\u732b\u54c1\u79cd\u5206\u7c7b\uff0c\u4f7f\u7528\u725b\u6d25-IIIT\u5ba0\u7269\u6570\u636e\u96c6\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5b50\u96c6\uff0c\u901a\u8fc7\u65cb\u8f6c\u3001\u6c34\u5e73\u7ffb\u8f6c\u548c\u4eae\u5ea6\u8c03\u6574\u7b49\u6570\u636e\u589e\u5f3a\u6280\u672f\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "GCViT-Tiny\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523092.00%\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523094.54%\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u53d8\u6362\u5668\u67b6\u6784\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u53d8\u6362\u5668\u7684\u67b6\u6784\u5728\u732b\u54c1\u79cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5305\u62ec\u517d\u533b\u8bca\u65ad\u3001\u52a8\u7269\u6536\u5bb9\u6240\u7ba1\u7406\u548c\u79fb\u52a8\u7aef\u8bc6\u522b\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u4e86Hugging Face\u6f14\u793a\u5e73\u53f0\u3002"}}
{"id": "2602.08222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08222", "abs": "https://arxiv.org/abs/2602.08222", "authors": ["Zehao Chen", "Gongxun Li", "Tianxiang Ai", "Yifei Li", "Zixuan Huang", "Wang Zhou", "Fuzhen Zhuang", "Xianglong Liu", "Jianxin Li", "Deqing Wang", "Yikun Ban"], "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "comment": null, "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "AI": {"tldr": "WMSS\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u5386\u53f2\u5f31\u68c0\u67e5\u70b9\u6765\u6307\u5bfc\u7ee7\u7eed\u4f18\u5316\uff0c\u7a81\u7834\u540e\u8bad\u7ec3\u9971\u548c\u74f6\u9888\uff0c\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u540e\u8bad\u7ec3\u65b9\u6cd5\u5728\u6a21\u578b\u53d8\u5f97\u9ad8\u5ea6\u81ea\u4fe1\u540e\u4f1a\u51fa\u73b0\u9971\u548c\u74f6\u9888\uff0c\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u81ea\u8eab\u5386\u53f2\u5f31\u72b6\u6001\u4e2d\u4ecd\u5b58\u5728\u6709\u76d1\u7763\u4fe1\u53f7\uff0c\u8fd9\u4e3a\u7a81\u7834\u9971\u548c\u74f6\u9888\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "WMSS\u901a\u8fc7\u71b5\u52a8\u6001\u8bc6\u522b\u53ef\u6062\u590d\u7684\u5b66\u4e60\u5dee\u8ddd\uff0c\u5229\u7528\u5f31\u68c0\u67e5\u70b9\u4f5c\u4e3a\u6307\u5bfc\uff0c\u901a\u8fc7\u8865\u507f\u5b66\u4e60\u5f3a\u5316\u8fd9\u4e9b\u5dee\u8ddd\uff0c\u4f7f\u5f3a\u667a\u80fd\u4f53\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u540e\u8bad\u7ec3\u7684\u9971\u548c\u9650\u5236\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528WMSS\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4e0d\u4ea7\u751f\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\u3002", "conclusion": "WMSS\u901a\u8fc7\u5229\u7528\u6a21\u578b\u81ea\u8eab\u5386\u53f2\u5f31\u72b6\u6001\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u6210\u529f\u7a81\u7834\u4e86\u540e\u8bad\u7ec3\u4e2d\u7684\u9971\u548c\u74f6\u9888\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.07535", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07535", "abs": "https://arxiv.org/abs/2602.07535", "authors": ["Md Sazidur Rahman", "Kjersti Engan", "Kathinka D\u00e6hli Kurz", "Mahdieh Khanmohammadi"], "title": "Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis", "comment": null, "summary": "Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u65f6\u76f8\u5206\u6790\u6846\u67b6\uff0c\u4f7f\u7528\u7edf\u8ba1\u63cf\u8ff0\u7b26\u3001\u5f71\u50cf\u7ec4\u5b66\u7eb9\u7406\u7279\u5f81\u548c\u6df1\u5ea6\u7279\u5f81\u5d4c\u5165\u6765\u8868\u5f81\u7f3a\u8840\u7ec4\u7ec7\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u5206\u6790\u63ed\u793a\u5352\u4e2d\u6f14\u53d8\u7684\u7ec4\u7ec7\u8868\u578b", "motivation": "\u5355\u65f6\u95f4\u70b9\u5206\u5272\u65e0\u6cd5\u6355\u6349\u5352\u4e2d\u7684\u751f\u7269\u5b66\u5f02\u8d28\u6027\u548c\u65f6\u95f4\u6f14\u53d8\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8868\u5f81\u7ec4\u7ec7\u72b6\u6001\u8f6c\u53d8\u7684\u5206\u6790\u65b9\u6cd5", "method": "\u4f7f\u7528\u53cc\u65f6\u76f8\uff08\u5165\u9662T1\u548c\u968f\u8bbfT2\uff09CT\u704c\u6ce8\u548cDWI\u6570\u636e\uff0c\u63d0\u53d6\u7edf\u8ba1\u63cf\u8ff0\u7b26\u3001GLCM\u7eb9\u7406\u7279\u5f81\u548c\u4e24\u79cd\u6df1\u5ea6\u67b6\u6784\uff08mJ-Net\u548cnnU-Net\uff09\u7684\u7279\u5f81\u5d4c\u5165\uff0c\u6784\u5efa6\u4e2aROI\u533a\u57df\uff0c\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fdb\u884c\u805a\u7c7b\u5206\u6790", "result": "\u572818\u540d\u6210\u529f\u518d\u704c\u6ce8\u60a3\u8005\u4e2d\uff0c\u533a\u57df\u7ea7\u8868\u5f81\u663e\u793a\u51fa\u6709\u610f\u4e49\u7684\u805a\u7c7b\u6a21\u5f0f\uff1a\u6700\u7ec8\u6062\u590d\u7684\u7f3a\u8840\u534a\u6697\u5e26\u533a\u57df\u7279\u5f81\u4e0e\u4fdd\u7559\u8111\u7ec4\u7ec7\u76f8\u4f3c\uff0c\u800c\u6897\u6b7b\u533a\u57df\u5f62\u6210\u660e\u663e\u5206\u7ec4\uff1b\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\uff08\u7279\u522b\u662fmJ-Net\uff09\u80fd\u663e\u8457\u533a\u5206\u53ef\u633d\u6551\u4e0e\u4e0d\u53ef\u633d\u6551\u7ec4\u7ec7", "conclusion": "\u7f16\u7801\u5668\u884d\u751f\u7684\u7279\u5f81\u6d41\u5f62\u53cd\u6620\u4e86\u57fa\u7840\u7ec4\u7ec7\u8868\u578b\u548c\u72b6\u6001\u8f6c\u53d8\uff0c\u4e3a\u57fa\u4e8e\u5f71\u50cf\u7684\u5352\u4e2d\u6f14\u53d8\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3"}}
{"id": "2602.08229", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08229", "abs": "https://arxiv.org/abs/2602.08229", "authors": ["Yifan Yang", "Jinjia Li", "Kunxi Li", "Puhao Zheng", "Yuanyi Wang", "Zheyan Qu", "Yang Yu", "Jianmin Wu", "Ming Li", "Hongxia Yang"], "title": "InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation", "comment": null, "summary": "The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a \"centralized black box\" into a \"decentralized endorsement\" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.", "AI": {"tldr": "\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u533a\u5757\u94fe\u534f\u8bae\u6fc0\u52b1\u5168\u7403\u53c2\u4e0e\u8005\u4f5c\u4e3a\u72ec\u7acb\u9a8c\u8bc1\u8005\uff0c\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u65b9\u5dee", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u4e2d\u5f0f\u8bc4\u4f30\u5b58\u5728\u4e0d\u900f\u660e\u3001\u8fc7\u62df\u5408\u548c\u786c\u4ef6\u5dee\u5f02\u5bfc\u81f4\u7684\u65b9\u5dee\u95ee\u9898\u3002\u5b9e\u8bc1\u5206\u6790\u663e\u793a\uff0c\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u60ca\u4eba\u7684\u4e0d\u4e00\u81f4\u6027\uff1a\u5355\u4e2a\u6a21\u578b\u5728HumanEval\u4e0a\u5341\u6b21\u91cd\u590d\u8fd0\u884c\u7684\u6807\u51c6\u5dee\uff081.67\uff09\u5b9e\u9645\u4e0a\u8d85\u8fc7\u4e86\u5b98\u65b9\u6392\u884c\u699c\u4e0a\u524d10\u540d\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff080.91\uff09\uff0c\u4f7f\u5f97\u5f53\u524d\u6392\u540d\u7edf\u8ba1\u4e0a\u4e0d\u53ef\u9760", "method": "\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u8ba1\u7b97\u8282\u70b9\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u73b0\u786c\u4ef6\u548c\u53c2\u6570\u591a\u6837\u6027\u3002\u5229\u7528\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u534f\u8bae\u6fc0\u52b1\u5168\u7403\u8d21\u732e\u8005\u4f5c\u4e3a\u72ec\u7acb\u9a8c\u8bc1\u8005\uff0c\u91c7\u7528\u7a33\u5065\u7684\u5956\u52b1\u7cfb\u7edf\u786e\u4fdd\u8bc4\u4f30\u5b8c\u6574\u6027\u5e76\u963b\u6b62\u4e0d\u8bda\u5b9e\u53c2\u4e0e", "result": "\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6\u5c06\u540c\u4e00\u6a21\u578b\u5341\u6b21\u8fd0\u884c\u7684\u6807\u51c6\u5dee\u964d\u4f4e\u52300.28\uff0c\u76f8\u6bd4\u4f20\u7edf\u6846\u67b6\u663e\u8457\u6539\u5584\uff0c\u786e\u4fdd\u6a21\u578b\u6392\u540d\u5177\u6709\u66f4\u9ad8\u7684\u7edf\u8ba1\u7f6e\u4fe1\u5ea6", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6\u5c06\u8bc4\u4f30\u4ece\"\u96c6\u4e2d\u5f0f\u9ed1\u7bb1\"\u8f6c\u53d8\u4e3a\"\u53bb\u4e2d\u5fc3\u5316\u80cc\u4e66\"\uff0c\u901a\u8fc7\u591a\u65b9\u5171\u8bc6\u548c\u591a\u6837\u5316\u63a8\u7406\u73af\u5883\u4ea7\u751f\u66f4\u7a33\u5b9a\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u6307\u6807\u3002\u8be5\u5e73\u53f0\u5df2\u5b8c\u5168\u5b9e\u73b0\u5e76\u5c06\u5411\u793e\u533a\u53d1\u5e03"}}
{"id": "2602.07488", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07488", "abs": "https://arxiv.org/abs/2602.07488", "authors": ["Francesco Cagnetta", "Allan Ravent\u00f3s", "Surya Ganguli", "Matthieu Wyart"], "title": "Deriving Neural Scaling Laws from the statistics of natural language", "comment": null, "summary": "Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u80fd\u591f\u5b9a\u91cf\u9884\u6d4b\u73b0\u4ee3LLM\u5728\u81ea\u7136\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u6307\u6570\u7684\u7406\u8bba\uff0c\u7279\u522b\u9488\u5bf9\u6570\u636e\u53d7\u9650\u7684\u7f29\u653e\u5b9a\u5f8b\u3002\u7406\u8bba\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u7684\u8bed\u8a00\u7edf\u8ba1\u7279\u6027\uff0c\u65e0\u9700\u81ea\u7531\u53c2\u6570\u6216\u5408\u6210\u6570\u636e\u6a21\u578b\u5c31\u80fd\u9884\u6d4b\u7f29\u653e\u6307\u6570\u3002", "motivation": "\u5c3d\u7ba1\u5b9e\u9a8c\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6307\u5bfc\u4e86\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u7684\u5b9e\u8bc1\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u65e0\u6cd5\u5b9a\u91cf\u9884\u6d4b\u4efb\u4f55\u73b0\u4ee3LLM\u5728\u4efb\u4f55\u81ea\u7136\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u8fd9\u4e9b\u91cd\u8981\u5b9a\u5f8b\u7684\u6307\u6570\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u79bb\u4e24\u4e2a\u5173\u952e\u7684\u8bed\u8a00\u7edf\u8ba1\u7279\u6027\uff1a(1) \u914d\u5bf9token\u76f8\u5173\u6027\u968f\u65f6\u95f4\u95f4\u9694\u7684\u8870\u51cf\uff0c(2) \u4e0b\u4e00token\u6761\u4ef6\u71b5\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u8870\u51cf\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7edf\u8ba1\u7279\u6027\u63a8\u5bfc\u51fa\u4e00\u4e2a\u7b80\u5355\u516c\u5f0f\uff0c\u4ece\u7b2c\u4e00\u539f\u7406\u9884\u6d4b\u6570\u636e\u53d7\u9650\u7684\u795e\u7ecf\u7f29\u653e\u6307\u6570\uff0c\u65e0\u9700\u81ea\u7531\u53c2\u6570\u6216\u5408\u6210\u6570\u636e\u6a21\u578b\u3002", "result": "\u7406\u8bba\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u6d4b\u91cf\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u8868\u73b0\u51fa\u663e\u8457\u5339\u914d\uff0c\u5305\u62ec\u5728GPT-2\u548cLLaMA\u98ce\u683c\u6a21\u578b\u4e0a\uff0c\u5728\u4e24\u4e2a\u6027\u8d28\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08TinyStories\u548cWikiText\uff09\u4e0a\u8fdb\u884c\u4ece\u5934\u8bad\u7ec3\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u80fd\u591f\u5b9a\u91cf\u9884\u6d4b\u73b0\u4ee3LLM\u5728\u81ea\u7136\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u6307\u6570\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u7edf\u8ba1\u7279\u6027\u4e0e\u7f29\u653e\u6307\u6570\u4e4b\u95f4\u7684\u57fa\u672c\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u7f29\u653e\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.07540", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07540", "abs": "https://arxiv.org/abs/2602.07540", "authors": ["Huimin Yan", "Liang Bai", "Xian Yang", "Long Chen"], "title": "LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing", "comment": null, "summary": "Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.", "AI": {"tldr": "LGDEA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u5f15\u5bfc\u7684\u8bca\u65ad\u8bc1\u636e\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc1\u636e\u7ea7\u5bf9\u9f50\u800c\u975e\u5168\u5c40\u6216\u5c40\u90e8\u5bf9\u9f50\uff0c\u51cf\u5c11\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5728\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5168\u5c40\u5bf9\u9f50\u5bb9\u6613\u88ab\u975e\u8bca\u65ad\u4fe1\u606f\u4e3b\u5bfc\uff0c\u800c\u5c40\u90e8\u5bf9\u9f50\u65e0\u6cd5\u6574\u5408\u5173\u952e\u8bca\u65ad\u8bc1\u636e\uff0c\u5bfc\u81f4\u96be\u4ee5\u5b66\u4e60\u53ef\u9760\u7684\u8bca\u65ad\u8868\u5f81\uff0c\u9650\u5236\u4e86\u5728\u914d\u5bf9\u6570\u636e\u6709\u9650\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faLLM\u5f15\u5bfc\u7684\u8bca\u65ad\u8bc1\u636e\u5bf9\u9f50\u65b9\u6cd5\uff08LGDEA\uff09\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u63d0\u53d6\u5173\u952e\u8bca\u65ad\u8bc1\u636e\uff0c\u6784\u5efa\u5171\u4eab\u7684\u8bca\u65ad\u8bc1\u636e\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8bc1\u636e\u611f\u77e5\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u6709\u6548\u5229\u7528\u5927\u91cf\u672a\u914d\u5bf9\u7684\u533b\u5b66\u56fe\u50cf\u548c\u62a5\u544a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u77ed\u8bed\u5b9a\u4f4d\u3001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u548c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u751a\u81f3\u80fd\u4e0e\u4f9d\u8d56\u5927\u91cf\u914d\u5bf9\u6570\u636e\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "LGDEA\u901a\u8fc7\u8bc1\u636e\u7ea7\u5bf9\u9f50\u66f4\u7b26\u5408\u533b\u5b66\u8bca\u65ad\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u5bf9\u914d\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07544", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07544", "abs": "https://arxiv.org/abs/2602.07544", "authors": ["Sebastian Bock", "Leonie Sch\u00fc\u00dfler", "Krishnakant Singh", "Simone Schaub-Meyer", "Stefan Roth"], "title": "MUFASA: A Multi-Layer Framework for Slot Attention", "comment": "Authors Sebastian Bock and Leonie Sch\u00fc\u00dfler contributed equally. Project page: https://leonieschuessler.github.io/mufasa/", "summary": "Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.", "AI": {"tldr": "MUFASA\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u591a\u5c42ViT\u7279\u5f81\u4e0a\u8ba1\u7b97slot attention\u6765\u63d0\u5347\u65e0\u76d1\u7763\u7269\u4f53\u5206\u5272\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8eslot attention\u7684\u65e0\u76d1\u7763\u7269\u4f53\u4e2d\u5fc3\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f7f\u7528\u9884\u8bad\u7ec3ViT\u7684\u6700\u540e\u4e00\u5c42\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u5c42\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f", "method": "\u63d0\u51faMUFASA\u6846\u67b6\uff0c\u5728ViT\u7f16\u7801\u5668\u7684\u591a\u4e2a\u7279\u5f81\u5c42\u4e0a\u8ba1\u7b97slot attention\uff0c\u5e76\u63d0\u51fa\u878d\u5408\u7b56\u7565\u5c06\u591a\u5c42\u83b7\u5f97\u7684slot\u805a\u5408\u6210\u7edf\u4e00\u7684\u7269\u4f53\u4e2d\u5fc3\u8868\u793a", "result": "\u5c06MUFASA\u96c6\u6210\u5230\u73b0\u6709OCL\u65b9\u6cd5\u4e2d\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u5206\u5272\u7ed3\u679c\uff0c\u8fbe\u5230\u65b0\u7684SOTA\uff0c\u540c\u65f6\u6539\u5584\u4e86\u8bad\u7ec3\u6536\u655b\u6027\uff0c\u4ec5\u5e26\u6765\u8f7b\u5fae\u63a8\u7406\u5f00\u9500", "conclusion": "\u901a\u8fc7\u5145\u5206\u5229\u7528ViT\u591a\u5c42\u8bed\u4e49\u4fe1\u606f\uff0cMUFASA\u6709\u6548\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u7269\u4f53\u5206\u5272\u6027\u80fd\uff0c\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6539\u8fdb\u6846\u67b6"}}
{"id": "2602.08241", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08241", "abs": "https://arxiv.org/abs/2602.08241", "authors": ["Siqu Ou", "Tianrui Wan", "Zhiyuan Zhao", "Junyu Gao", "Xuelong Li"], "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs", "comment": null, "summary": "While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.", "AI": {"tldr": "SAYO\uff1a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5f15\u5165\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4f9d\u8d56\u957f\u6587\u672c\u63a8\u7406\u8f68\u8ff9\uff0c\u4f46\u7f3a\u4e4f\u5b66\u4e60\u7a33\u5b9a\u89c6\u89c9\u6ce8\u610f\u529b\u7b56\u7565\u7684\u673a\u5236\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5b58\u5728\u89c6\u89c9\u6ce8\u610f\u529b\u8584\u5f31\u7684\u95ee\u9898\uff1a\u65e9\u671f\u89c6\u89c9\u5bf9\u9f50\u9519\u8bef\u5f88\u5c11\u5728\u540e\u7eed\u63a8\u7406\u4e2d\u5f97\u5230\u7ea0\u6b63\uff0c\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u548c\u63a8\u7406\u5931\u8d25\u3002\u8fd9\u4e00\u9650\u5236\u6e90\u4e8e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u89c6\u89c9\u6ce8\u610f\u529b\u7684\u4fe1\u7528\u5206\u914d\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSAYO\u89c6\u89c9\u63a8\u7406\u6a21\u578b\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\uff0c\u5f15\u5165\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\u673a\u5236\u3002\u8be5\u5956\u52b1\u660e\u786e\u5c06\u4f18\u5316\u4fe1\u53f7\u4e0e\u57fa\u4e8e\u89c6\u89c9\u7684\u63a8\u7406\u6b65\u9aa4\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u66f4\u53ef\u9760\u7684\u6ce8\u610f\u529b\u884c\u4e3a\u3002", "result": "\u5728\u591a\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAYO\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u548c\u611f\u77e5\u4efb\u52a1\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5f15\u5165\u533a\u57df\u7ea7\u89c6\u89c9\u6ce8\u610f\u529b\u5956\u52b1\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u6ce8\u610f\u529b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2602.07496", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07496", "abs": "https://arxiv.org/abs/2602.07496", "authors": ["Antonio Mone", "Frans A. Oliehoek", "Luciano Cavalcante Siebert"], "title": "CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning", "comment": "14 pages, 6 figures", "summary": "Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.", "AI": {"tldr": "CoMI-IRL\u662f\u4e00\u4e2a\u57fa\u4e8etransformer\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u610f\u56fe\u9006\u5f3a\u5316\u5b66\u4e60\uff0c\u5b83\u89e3\u8026\u4e86\u884c\u4e3a\u8868\u793a/\u805a\u7c7b\u4e0e\u4e0b\u6e38\u5956\u52b1\u5b66\u4e60\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u5c31\u80fd\u81ea\u52a8\u53d1\u73b0\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u751f\u6210MI-IRL\u65b9\u6cd5\u9700\u8981\u77e5\u9053\u771f\u5b9e\u884c\u4e3a\u6a21\u5f0f\u6570\u91cfK*\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u65b0\u884c\u4e3a\u7684\u9002\u5e94\u6027\uff0c\u5e76\u4e14\u53ea\u80fd\u5206\u6790\u4e0e\u5b66\u4e60\u5956\u52b1\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u800c\u4e0d\u80fd\u8de8\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u884c\u4e3a\u6a21\u5f0f\u8fdb\u884c\u5206\u6790\u3002", "method": "\u63d0\u51fa\u57fa\u4e8etransformer\u7684\u65e0\u76d1\u7763\u6846\u67b6CoMI-IRL\uff0c\u5c06\u884c\u4e3a\u8868\u793a\u548c\u805a\u7c7b\u4e0e\u4e0b\u6e38\u5956\u52b1\u5b66\u4e60\u89e3\u8026\u3002\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5b66\u4e60\u884c\u4e3a\u8868\u793a\uff0c\u7136\u540e\u8fdb\u884c\u805a\u7c7b\uff0c\u6700\u540e\u8fdb\u884c\u5956\u52b1\u5b66\u4e60\u3002", "result": "CoMI-IRL\u5728\u65e0\u9700K*\u5148\u9a8c\u77e5\u8bc6\u6216\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5141\u8bb8\u5bf9\u884c\u4e3a\u5173\u7cfb\u8fdb\u884c\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u5e76\u4e14\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u5c31\u80fd\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u884c\u4e3a\u3002", "conclusion": "CoMI-IRL\u901a\u8fc7\u89e3\u8026\u884c\u4e3a\u8868\u793a/\u805a\u7c7b\u4e0e\u5956\u52b1\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMI-IRL\u65b9\u6cd5\u5bf9\u5148\u9a8c\u77e5\u8bc6\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u591a\u610f\u56fe\u9006\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u6846\u67b6\u3002"}}
{"id": "2602.07550", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07550", "abs": "https://arxiv.org/abs/2602.07550", "authors": ["Hussni Mohd Zakir", "Eric Tatt Wei Ho"], "title": "Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation", "comment": "10 pages, 3 figures, 7 tables", "summary": "Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a \"Safest vs. Optimal\" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a \"Semantic Selection Gap\" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the \"Last-Layer\" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.", "AI": {"tldr": "FSSDINO\uff1a\u57fa\u4e8e\u51bb\u7ed3DINOv3\u7279\u5f81\u7684\u8bad\u7ec3\u514d\u8d39\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u57fa\u7ebf\uff0c\u4ec5\u4f7f\u7528\u7c7b\u522b\u539f\u578b\u548cGram\u77e9\u9635\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u590d\u6742\u65b9\u6cd5\u7ade\u4e89\uff0c\u63ed\u793a\u4e86\"\u6700\u5b89\u5168vs\u6700\u4f18\"\u7684\u8bed\u4e49\u9009\u62e9\u56f0\u5883\u3002", "motivation": "\u63a2\u7d22\u51bb\u7ed3DINOv3\u7279\u5f81\u5728\u5c11\u6837\u672c\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5185\u5728\u80fd\u529b\uff0c\u5efa\u7acb\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5206\u6790\u7279\u5f81\u5c42\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u57fa\u7840\u6a21\u578b\u4e2d\u8bed\u4e49\u9009\u62e9\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faFSSDINO\u65b9\u6cd5\uff1a\u4f7f\u7528\u51bb\u7ed3\u7684DINOv3\u7279\u5f81\uff0c\u901a\u8fc7\u7c7b\u522b\u7279\u5b9a\u539f\u578b\u548cGram\u77e9\u9635\u4f18\u5316\u8fdb\u884c\u8bad\u7ec3\u514d\u8d39\u5c11\u6837\u672c\u5206\u5272\u3002\u91c7\u7528Oracle\u5f15\u5bfc\u7684\u5c42\u5206\u6790\uff0c\u6bd4\u8f83\u4e0d\u540c\u7279\u5f81\u5c42\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u5728\u4e8c\u5143\u3001\u591a\u7c7b\u522b\u548c\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fd9\u79cd\u6700\u5c0f\u5316\u65b9\u6cd5\u5728\u6700\u7ec8\u9aa8\u5e72\u5c42\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u590d\u6742\u89e3\u7801\u5668\u6216\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u7ade\u4e89\u3002Oracle\u5206\u6790\u663e\u793a\u4e2d\u95f4\u5c42\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u4f46\u5f53\u524d\u65e0\u76d1\u7763\u548c\u652f\u6301\u5f15\u5bfc\u7684\u9009\u62e9\u6307\u6807\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u8fd9\u4e9b\u9ad8\u6027\u80fd\u7279\u5f81\u3002", "conclusion": "\"\u6700\u540e\u4e00\u5c42\"\u662f\u4e00\u4e2a\u5177\u6709\u6b3a\u9a97\u6027\u7684\u5f3a\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86DINOv3\u4e2d\u6f5c\u5728\u7684\u8bed\u4e49\u6f5c\u529b\u3002\u7814\u7a76\u53d1\u73b0\u4e86\"\u8bed\u4e49\u9009\u62e9\u9e3f\u6c9f\"\uff0c\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u9ad8\u4fdd\u771f\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u57fa\u7840\u6a21\u578b\u7279\u5f81\u9009\u62e9\u7684\u65b0\u6311\u6218\u3002"}}
{"id": "2602.08253", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08253", "abs": "https://arxiv.org/abs/2602.08253", "authors": ["Baoyun Zhao", "He Wang", "Liang Zeng"], "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design", "comment": null, "summary": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.", "AI": {"tldr": "G-LNS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u8fdb\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u8bbe\u8ba1\u5927\u89c4\u6a21\u90bb\u57df\u641c\u7d22\u7b97\u5b50\uff0c\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u7834\u574f\u548c\u4fee\u590d\u7b97\u5b50\u5bf9\uff0c\u5728\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u6784\u9020\u6027\u4f18\u5148\u7ea7\u89c4\u5219\u6216\u53c2\u6570\u5316\u5c40\u90e8\u641c\u7d22\u6307\u5bfc\uff0c\u5c06\u641c\u7d22\u7a7a\u95f4\u9650\u5236\u5728\u56fa\u5b9a\u7684\u542f\u53d1\u5f0f\u5f62\u5f0f\u4e0a\u3002\u8fd9\u79cd\u8bbe\u8ba1\u5728\u7ed3\u6784\u63a2\u7d22\u80fd\u529b\u4e0a\u6709\u9650\uff0c\u96be\u4ee5\u5728\u590d\u6742\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u9003\u79bb\u6df1\u5ea6\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u63d0\u51faG-LNS\u751f\u6210\u5f0f\u8fdb\u5316\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u534f\u540c\u8fdb\u5316\u7d27\u5bc6\u8026\u5408\u7684\u7834\u574f\u548c\u4fee\u590d\u7b97\u5b50\u5bf9\u3002\u91c7\u7528\u5408\u4f5c\u8bc4\u4f30\u673a\u5236\u660e\u786e\u6355\u6349\u7b97\u5b50\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u53d1\u73b0\u80fd\u591f\u5171\u540c\u6267\u884c\u6709\u6548\u7ed3\u6784\u7834\u574f\u548c\u91cd\u6784\u7684\u4e92\u8865\u7b97\u5b50\u903b\u8f91\u3002", "result": "\u5728\u65c5\u884c\u5546\u95ee\u9898\u548c\u5e26\u5bb9\u91cf\u7ea6\u675f\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cG-LNS\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u4ee5\u53ca\u5f3a\u5927\u7684\u7ecf\u5178\u6c42\u89e3\u5668\u3002\u53d1\u73b0\u7684\u542f\u53d1\u5f0f\u4e0d\u4ec5\u80fd\u5728\u51cf\u5c11\u8ba1\u7b97\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u63a5\u8fd1\u6700\u4f18\u7684\u89e3\uff0c\u8fd8\u80fd\u5728\u4e0d\u540c\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\u5206\u5e03\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "G-LNS\u5c06\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u6269\u5c55\u5230\u5927\u89c4\u6a21\u90bb\u57df\u641c\u7d22\u7b97\u5b50\u7684\u81ea\u52a8\u8bbe\u8ba1\uff0c\u901a\u8fc7\u534f\u540c\u8fdb\u5316\u7834\u574f-\u4fee\u590d\u7b97\u5b50\u5bf9\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u7ed3\u6784\u63a2\u7d22\u548c\u4f18\u5316\u6027\u80fd\uff0c\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.07519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07519", "abs": "https://arxiv.org/abs/2602.07519", "authors": ["Martin Fixman", "Alessandro Abati", "Juli\u00e1n Jim\u00e9nez Nimmo", "Sean Lim", "Esther Mondrag\u00f3n"], "title": "PALMS: Pavlovian Associative Learning Models Simulator", "comment": null, "summary": "Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator", "AI": {"tldr": "PALMS\u662f\u4e00\u4e2aPython\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u6a21\u62df\u5df4\u752b\u6d1b\u592b\u6761\u4ef6\u53cd\u5c04\u5b9e\u9a8c\uff0c\u6574\u5408\u4e86\u591a\u79cd\u6ce8\u610f\u529b\u5b66\u4e60\u6a21\u578b\uff0c\u652f\u6301\u5927\u89c4\u6a21\u523a\u6fc0\u6a21\u62df\u548c\u914d\u7f6e\u7ebf\u7d22\u8ba1\u7b97\uff0c\u63d0\u4f9b\u56fe\u5f62\u754c\u9762\u548c\u5feb\u901f\u7ed3\u679c\u53ef\u89c6\u5316\u3002", "motivation": "\u6a21\u62df\u5728\u7406\u8bba\u53d1\u5c55\u548c\u5b8c\u5584\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u6216\u7f3a\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5236\u5b9a\u7cbe\u786e\u5b9a\u4e49\u3001\u751f\u6210\u6a21\u578b\u5e76\u505a\u51fa\u51c6\u786e\u9884\u6d4b\u3002\u9700\u8981\u4e00\u79cd\u5de5\u5177\u6765\u6a21\u62df\u5df4\u752b\u6d1b\u592b\u6761\u4ef6\u53cd\u5c04\u5b9e\u9a8c\uff0c\u6574\u5408\u591a\u79cd\u5b66\u4e60\u6a21\u578b\u5e76\u652f\u6301\u590d\u6742\u5b9e\u9a8c\u8bbe\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86PALMS\u6a21\u62df\u5668\uff0c\u5305\u542b\u7ecf\u5178Rescorla-Wagner\u6a21\u578b\u548c\u591a\u79cd\u6ce8\u610f\u529b\u5b66\u4e60\u65b9\u6cd5\uff08Pearce-Kaye-Hall\u3001Mackintosh Extended\u3001Le Pelley's Hybrid\u7b49\uff09\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5177\u6709\u7edf\u4e00\u53ef\u53d8\u5b66\u4e60\u7387\u7684\u65b0\u6269\u5c55\u6a21\u578b\u3002\u63d0\u4f9b\u56fe\u5f62\u754c\u9762\u652f\u6301\u5b57\u6bcd\u6570\u5b57\u683c\u5f0f\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u8f93\u5165\uff0c\u652f\u6301\u6570\u767e\u4e2a\u523a\u6fc0\u7684\u6a21\u62df\u548c\u914d\u7f6e\u7ebf\u7d22\u8ba1\u7b97\u3002", "result": "PALMS\u80fd\u591f\u9ad8\u6548\u8fd0\u884c\uff0c\u5373\u65f6\u53ef\u89c6\u5316\u7ed3\u679c\uff0c\u652f\u6301\u5728\u5355\u4e00\u67b6\u6784\u548c\u73af\u5883\u4e2d\u5feb\u901f\u7cbe\u786e\u6bd4\u8f83\u5404\u79cd\u6a21\u578b\u7684\u9884\u6d4b\u3002\u56fe\u5f62\u663e\u793a\u53ef\u8f7b\u677e\u4fdd\u5b58\uff0c\u6a21\u62df\u6570\u636e\u53ef\u5bfc\u51fa\u5230\u7535\u5b50\u8868\u683c\u3002\u901a\u8fc7\u590d\u73b0\u5df2\u53d1\u8868\u7684\u5173\u8054\u5b66\u4e60\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u529f\u80fd\u548c\u80fd\u529b\u3002", "conclusion": "PALMS\u4f5c\u4e3a\u4e00\u4e2a\u5f00\u6e90\u6a21\u62df\u5668\uff0c\u663e\u8457\u6269\u5c55\u4e86\u5173\u8054\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\u6765\u6a21\u62df\u548c\u6bd4\u8f83\u4e0d\u540c\u5b66\u4e60\u7406\u8bba\uff0c\u4fc3\u8fdb\u7406\u8bba\u53d1\u5c55\u548c\u5b8c\u5584\u3002"}}
{"id": "2602.07554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07554", "abs": "https://arxiv.org/abs/2602.07554", "authors": ["Guandong Li", "Yijun Ding"], "title": "FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation", "comment": null, "summary": "Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.", "AI": {"tldr": "FlexID\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u8eab\u4efd\u611f\u77e5\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u8c03\u5236\u6b63\u4ea4\u89e3\u8026\u8eab\u4efd\u7279\u5f81\uff0c\u5b9e\u73b0\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u9002\u5e94\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4f9d\u8d56\u521a\u6027\u7684\u89c6\u89c9\u7279\u5f81\u6ce8\u5165\uff0c\u5bfc\u81f4\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u9002\u5e94\u6027\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u8fd9\u4e24\u8005\u3002", "method": "\u63d0\u51faFlexID\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u8bed\u4e49\u8eab\u4efd\u6295\u5f71\u5668(SIP)\u5c06\u9ad8\u5c42\u5148\u9a8c\u6ce8\u5165\u8bed\u8a00\u7a7a\u95f4\uff1b2) \u89c6\u89c9\u7279\u5f81\u951a\u70b9(VFA)\u786e\u4fdd\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u4fdd\u771f\u5ea6\uff1b3) \u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u9002\u5e94\u95e8\u63a7(CAG)\u673a\u5236\uff0c\u6839\u636e\u7f16\u8f91\u610f\u56fe\u548c\u6269\u6563\u65f6\u95f4\u6b65\u52a8\u6001\u8c03\u5236\u4e24\u4e2a\u6d41\u7684\u6743\u91cd\u3002", "result": "\u5728IBench\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFlexID\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u6587\u672c\u9075\u5faa\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e73\u8861\uff0c\u4e3a\u590d\u6742\u53d9\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "FlexID\u901a\u8fc7\u610f\u56fe\u611f\u77e5\u8c03\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8eab\u4efd\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u4e2a\u6027\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2602.07529", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07529", "abs": "https://arxiv.org/abs/2602.07529", "authors": ["Jianwen Chen", "Xinyu Yang", "Peng Xia", "Arian Azarang", "Yueh Z Lee", "Gang Li", "Hongtu Zhu", "Yun Li", "Beidi Chen", "Huaxiu Yao"], "title": "MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.", "AI": {"tldr": "MedVerse\u662f\u4e00\u4e2a\u57fa\u4e8ePetri\u7f51\u7406\u8bba\u7684\u533b\u7597\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u987a\u5e8f\u63a8\u7406\u8f6c\u5316\u4e3a\u53ef\u5e76\u884c\u7684\u6709\u5411\u65e0\u73af\u56fe\u8fc7\u7a0b\uff0c\u63d0\u5347\u590d\u6742\u533b\u7597\u63a8\u7406\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u987a\u5e8f\u81ea\u56de\u5f52\u89e3\u7801\u65b9\u5f0f\u5c06\u672c\u5e94\u5e76\u884c\u7684\u4e34\u5e8a\u63a8\u7406\uff08\u5982\u9274\u522b\u8bca\u65ad\uff09\u5f3a\u5236\u4e3a\u5355\u4e00\u7ebf\u6027\u63a8\u7406\u8def\u5f84\uff0c\u9650\u5236\u4e86\u590d\u6742\u533b\u7597\u95ee\u9898\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "method": "1. \u6570\u636e\u5c42\u9762\uff1aMedVerse Curator\u81ea\u52a8\u5408\u6210\u77e5\u8bc6\u57fa\u7840\u7684\u533b\u7597\u63a8\u7406\u8def\u5f84\u5e76\u8f6c\u5316\u4e3aPetri\u7f51\u7ed3\u6784\u8868\u793a\uff1b2. \u67b6\u6784\u5c42\u9762\uff1a\u63d0\u51fa\u5177\u6709\u81ea\u9002\u5e94\u4f4d\u7f6e\u7d22\u5f15\u7684\u62d3\u6251\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u652f\u6301\u5e76\u884c\u63a8\u7406\u540c\u65f6\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\uff1b3. \u7cfb\u7edf\u5c42\u9762\uff1a\u5f00\u53d1\u652f\u6301\u5e76\u884c\u6267\u884c\u7684\u81ea\u5b9a\u4e49\u63a8\u7406\u5f15\u64ce\u3002", "result": "MedVerse\u5c06\u901a\u7528LLM\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe8.9%\uff1b\u4e0e\u4e13\u4e1a\u533b\u7597LLM\u76f8\u6bd4\uff0c\u8fbe\u5230\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e1.3\u500d\uff0c\u751f\u6210\u541e\u5410\u91cf\u63d0\u9ad81.7\u500d\u3002", "conclusion": "MedVerse\u901a\u8fc7\u5c06\u533b\u7597\u63a8\u7406\u91cd\u6784\u4e3a\u53ef\u5e76\u884c\u7684DAG\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u987a\u5e8f\u89e3\u7801\u5728\u590d\u6742\u533b\u7597\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.08276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08276", "abs": "https://arxiv.org/abs/2602.08276", "authors": ["Haoyu Jia", "Kento Kawaharazuka", "Kei Okada"], "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis", "comment": null, "summary": "Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \\texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \\texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.", "AI": {"tldr": "\u63d0\u51faStructural Context Model\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u4e0a\u4e0b\u6587\u7ed3\u6784\u89d2\u5ea6\u5206\u6790\u548c\u6bd4\u8f83LLM\u667a\u80fd\u4f53\uff0c\u5305\u542b\u58f0\u660e\u5f0f\u5b9e\u73b0\u6846\u67b6\u548cSemantic Dynamics Analysis\u5de5\u4f5c\u6d41\uff0c\u5728\u52a8\u6001\u7334\u5b50\u9999\u8549\u95ee\u9898\u4e0a\u5b9e\u73b032%\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u7814\u7a76\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u6982\u5ff5\u6846\u67b6\u548c\u65b9\u6cd5\u8bba\u539f\u5219\u5e38\u4e0e\u5e95\u5c42\u5b9e\u73b0\u7ec6\u8282\u4ea4\u7ec7\uff0c\u5bfc\u81f4\u8bfb\u8005\u548c\u4f5c\u8005\u5728\u8868\u9762\u4e0d\u540c\u7684\u6982\u5ff5\u4e2d\u8ff7\u5931\u65b9\u5411\u3002\u8fd9\u79cd\u788e\u7247\u5316\u4e3b\u8981\u6e90\u4e8e\u7f3a\u4e4f\u53ef\u5206\u6790\u3001\u81ea\u6d3d\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4e0e\u5b9e\u73b0\u65e0\u5173\u7684LLM\u667a\u80fd\u4f53\u8868\u5f81\u548c\u6bd4\u8f83\u3002", "method": "\u63d0\u51faStructural Context Model\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u4ece\u4e0a\u4e0b\u6587\u7ed3\u6784\u89d2\u5ea6\u5206\u6790\u548c\u6bd4\u8f83LLM\u667a\u80fd\u4f53\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a1) \u58f0\u660e\u5f0f\u5b9e\u73b0\u6846\u67b6\uff1b2) \u53ef\u6301\u7eed\u667a\u80fd\u4f53\u5de5\u7a0b\u5de5\u4f5c\u6d41Semantic Dynamics Analysis\u3002\u8be5\u5de5\u4f5c\u6d41\u63d0\u4f9b\u5bf9\u667a\u80fd\u4f53\u673a\u5236\u7684\u539f\u5219\u6027\u6d1e\u5bdf\uff0c\u652f\u6301\u5feb\u901f\u3001\u7cfb\u7edf\u7684\u8bbe\u8ba1\u8fed\u4ee3\u3002", "result": "\u5728\u52a8\u6001\u53d8\u4f53\u7684\u7334\u5b50\u9999\u8549\u95ee\u9898\u4e0a\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u5de5\u7a0b\u7684\u667a\u80fd\u4f53\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe32\u4e2a\u767e\u5206\u70b9\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684Structural Context Model\u548c\u914d\u5957\u6846\u67b6\u4e3a\u89e3\u51b3LLM\u667a\u80fd\u4f53\u7814\u7a76\u788e\u7247\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u5efa\u6a21\u548c\u5de5\u7a0b\u5316\u5de5\u4f5c\u6d41\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.07530", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.07530", "abs": "https://arxiv.org/abs/2602.07530", "authors": ["Sreenivas Gollapudi", "Kostas Kollias", "Kamesh Munagala", "Aravindan Vijayaraghavan"], "title": "Compact Conformal Subgraphs", "comment": null, "summary": "Conformal prediction provides rigorous, distribution-free uncertainty guarantees, but often yields prohibitively large prediction sets in structured domains such as routing, planning, or sequential recommendation. We introduce \"graph-based conformal compression\", a framework for constructing compact subgraphs that preserve statistical validity while reducing structural complexity. We formulate compression as selecting a smallest subgraph capturing a prescribed fraction of the probability mass, and reduce to a weighted version of densest $k$-subgraphs in hypergraphs, in the regime where the subgraph has a large fraction of edges. We design efficient approximation algorithms that achieve constant factor coverage and size trade-offs. Crucially, we prove that our relaxation satisfies a monotonicity property, derived from a connection to parametric minimum cuts, which guarantees the nestedness required for valid conformal guarantees. Our results on the one hand bridge efficient conformal prediction with combinatorial graph compression via monotonicity, to provide rigorous guarantees on both statistical validity, and compression or size. On the other hand, they also highlight an algorithmic regime, distinct from classical densest-$k$-subgraph hardness settings, where the problem can be approximated efficiently. We finally validate our algorithmic approach via simulations for trip planning and navigation, and compare to natural baselines.", "AI": {"tldr": "\u63d0\u51fa\u56fe\u57fa\u4fdd\u5f62\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u5c0f\u5b50\u56fe\u6765\u6355\u83b7\u89c4\u5b9a\u6982\u7387\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u7edf\u8ba1\u6709\u6548\u6027\u7684\u540c\u65f6\u51cf\u5c11\u7ed3\u6784\u5316\u9884\u6d4b\u96c6\u7684\u5927\u5c0f\u3002", "motivation": "\u4f20\u7edf\u4fdd\u5f62\u9884\u6d4b\u5728\u7ed3\u6784\u5316\u9886\u57df\uff08\u5982\u8def\u7531\u3001\u89c4\u5212\u3001\u5e8f\u5217\u63a8\u8350\uff09\u4f1a\u4ea7\u751f\u8fc7\u5927\u7684\u9884\u6d4b\u96c6\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7edf\u8ba1\u6709\u6548\u6027\u53c8\u80fd\u538b\u7f29\u7ed3\u6784\u590d\u6742\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5c06\u538b\u7f29\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u9009\u62e9\u80fd\u6355\u83b7\u89c4\u5b9a\u6982\u7387\u8d28\u91cf\u7684\u6700\u5c0f\u5b50\u56fe\uff0c\u8f6c\u5316\u4e3a\u8d85\u56fe\u4e2d\u52a0\u6743\u6700\u5bc6k\u5b50\u56fe\u95ee\u9898\uff0c\u8bbe\u8ba1\u9ad8\u6548\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5229\u7528\u53c2\u6570\u6700\u5c0f\u5272\u7684\u5355\u8c03\u6027\u4fdd\u8bc1\u5d4c\u5957\u6027\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5e38\u6570\u500d\u7684\u8986\u76d6\u7387\u548c\u5927\u5c0f\u6743\u8861\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u7b97\u6cd5\u673a\u5236\u4e0b\u95ee\u9898\u53ef\u9ad8\u6548\u8fd1\u4f3c\uff0c\u901a\u8fc7\u884c\u7a0b\u89c4\u5212\u548c\u5bfc\u822a\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u9ad8\u6548\u4fdd\u5f62\u9884\u6d4b\u4e0e\u7ec4\u5408\u56fe\u538b\u7f29\u901a\u8fc7\u5355\u8c03\u6027\u8fde\u63a5\u8d77\u6765\uff0c\u5728\u4fdd\u6301\u7edf\u8ba1\u6709\u6548\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u538b\u7f29\uff0c\u4e3a\u7ed3\u6784\u5316\u9884\u6d4b\u96c6\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07565", "abs": "https://arxiv.org/abs/2602.07565", "authors": ["Jingzhe Ma", "Meng Zhang", "Jianlong Yu", "Kun Liu", "Zunxiao Xu", "Xue Cheng", "Junjie Zhou", "Yanfei Wang", "Jiahang Li", "Zepeng Wang", "Kazuki Osamura", "Rujie Liu", "Narishige Abe", "Jingjie Wang", "Shunli Zhang", "Haojun Xie", "Jiajun Wu", "Weiming Wu", "Wenxiong Kang", "Qingshuo Gao", "Jiaming Xiong", "Xianye Ben", "Lei Chen", "Lichen Song", "Junjian Cui", "Haijun Xiong", "Junhao Lu", "Bin Feng", "Mengyuan Liu", "Ji Zhou", "Baoquan Zhao", "Ke Xu", "Yongzhen Huang", "Liang Wang", "Manuel J Marin-Jimenez", "Md Atiqur Rahman Ahad", "Shiqi Yu"], "title": "Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025", "comment": "Accepted by IJCB 2025(https://ijcb2025.ieee-biometrics.org/competitions/)", "summary": "Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e862025\u5e74\u8fdc\u8ddd\u79bb\u4eba\u4f53\u8bc6\u522b\u7ade\u8d5b\uff0c\u4f7f\u7528SUSTech-Competition\u6b65\u6001\u6570\u636e\u96c6\uff0c\u53c2\u8d5b\u8005\u5728\u65e0\u4e13\u7528\u8bad\u7ec3\u6570\u636e\u60c5\u51b5\u4e0b\u901a\u8fc7\u5916\u90e8\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u6700\u4f73\u65b9\u6cd5\u8fbe\u523094.2%\u51c6\u786e\u7387\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "motivation": "\u8fdc\u8ddd\u79bb\u4eba\u4f53\u8bc6\u522b\u9762\u4e34\u4f20\u7edf\u751f\u7269\u7279\u5f81\u96be\u4ee5\u83b7\u53d6\u7684\u6311\u6218\uff0c\u6b65\u6001\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002\u56fd\u9645\u8fdc\u8ddd\u79bb\u4eba\u4f53\u8bc6\u522b\u7ade\u8d5b\u65e8\u5728\u63a8\u52a8\u6b65\u6001\u8bc6\u522b\u8fdb\u5c55\u5e76\u63d0\u4f9b\u516c\u5e73\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u7ade\u8d5b\u91c7\u7528SUSTech-Competition\u6570\u636e\u96c6\uff0c\u5305\u542b\u670d\u88c5\u3001\u643a\u5e26\u7269\u54c1\u548c\u89c6\u89d2\u7684\u663e\u8457\u53d8\u5316\uff0c\u4e0d\u63d0\u4f9b\u4e13\u7528\u8bad\u7ec3\u6570\u636e\uff0c\u53c2\u8d5b\u8005\u9700\u4f7f\u7528\u5916\u90e8\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u3002\u6bcf\u5e74\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u751f\u6210\u8bc4\u4f30\u5206\u5272\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u98ce\u9669\u5e76\u652f\u6301\u8de8\u57df\u6cdb\u5316\u516c\u5e73\u8bc4\u4f30\u3002", "result": "\u5c3d\u7ba1\u96be\u5ea6\u589e\u52a0\uff0c\u53c2\u8d5b\u8005\u4ecd\u53d6\u5f97\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u6700\u4f73\u65b9\u6cd5\u8fbe\u523094.2%\u51c6\u786e\u7387\uff0c\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u521b\u4e0b\u65b0\u57fa\u51c6\u3002\u8bba\u6587\u8fd8\u5206\u6790\u4e86\u5173\u952e\u6280\u672f\u8d8b\u52bf\u3002", "conclusion": "2025\u5e74\u7ade\u8d5b\u8bc1\u660e\u7b97\u6cd5\u8fdb\u6b65\u80fd\u591f\u8d85\u8d8a\u5148\u524d\u89c2\u5bdf\u5230\u7684\u51c6\u786e\u7387\u9650\u5236\uff0c\u4e3a\u6b65\u6001\u8bc6\u522b\u9886\u57df\u8bbe\u7acb\u4e86\u65b0\u6807\u6746\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.08295", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08295", "abs": "https://arxiv.org/abs/2602.08295", "authors": ["Ilya Levin"], "title": "The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI", "comment": "19 pages", "summary": "The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.\n  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.\n  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u6c1b\u56f4\u81ea\u52a8\u5316\"\u6982\u5ff5\uff0c\u8ba4\u4e3a\u751f\u6210\u5f0fAI\u4ee3\u8868\u4e86\u4ece\u7b97\u6cd5\u4f18\u5316\u5230\u8bed\u5883\u8bed\u4e49\u8fde\u8d2f\u6027\u5bfc\u822a\u7684\u8ba4\u77e5\u8303\u5f0f\u8f6c\u53d8\uff0c\u4eba\u7c7b\u89d2\u8272\u4ece\u95ee\u9898\u89c4\u8303\u8f6c\u5411\"\u6c1b\u56f4\u5de5\u7a0b\"\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e0d\u4ec5\u4ee3\u8868\u6280\u672f\u8fdb\u6b65\uff0c\u66f4\u662f\u4e00\u79cd\u8ba4\u77e5\u8303\u5f0f\u8f6c\u53d8\uff0c\u6311\u6218\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u57fa\u7840\u5047\u8bbe\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u662f\"\u81ea\u52a8\u5316\u7684\u81ea\u52a8\u5316\"\uff0c\u800c\u751f\u6210\u5f0fAI\u901a\u8fc7\u5bfc\u822a\u8bed\u5883\u3001\u8bed\u4e49\u548c\u98ce\u683c\u8fde\u8d2f\u6027\u8fd0\u4f5c\uff0c\u800c\u975e\u4f18\u5316\u9884\u5b9a\u4e49\u76ee\u6807\u6307\u6807\u3002", "method": "\u5f15\u5165\"\u6c1b\u56f4\u81ea\u52a8\u5316\"\u6982\u5ff5\u6765\u8868\u5f81\u8fd9\u4e00\u8f6c\u53d8\uff0c\u63d0\u51fa\u751f\u6210\u5f0fAI\u7684\u529f\u80fd\u5728\u4e8e\u5176\u5bf9\u64cd\u4f5c\u5316\u9690\u6027\u89c4\u5f8b\u7684\u53ef\u53ca\u6027\u2014\u2014\u5d4c\u5165\u5b9e\u8df5\u4e2d\u7684\u8bed\u5883\u654f\u611f\u6a21\u5f0f\uff0c\u65e0\u6cd5\u901a\u8fc7\u663e\u5f0f\u7b97\u6cd5\u89c4\u5219\u5b8c\u5168\u6307\u5b9a\u3002\u867d\u7136\u751f\u6210\u7cfb\u7edf\u4e0d\u5177\u5907\u73b0\u8c61\u5b66\u610f\u4e49\u4e0a\u7684\u9690\u6027\u77e5\u8bc6\uff0c\u4f46\u5b83\u4eec\u64cd\u4f5c\u5316\u4e86\u5bf9\u7f16\u7801\u5728\u9ad8\u7ef4\u6f5c\u5728\u8868\u793a\u4e2d\u7684\u8bed\u8c03\u3001\u610f\u56fe\u548c\u60c5\u5883\u5224\u65ad\u7684\u654f\u611f\u6027\u3002", "result": "\u4eba\u7c7b\u89d2\u8272\u4ece\u7b97\u6cd5\u95ee\u9898\u89c4\u8303\u8f6c\u5411\"\u6c1b\u56f4\u5de5\u7a0b\"\uff0c\u5373\u751f\u6210\u7cfb\u7edf\u4e2d\u5bf9\u9f50\u548c\u8bed\u5883\u5224\u65ad\u7684\u7f16\u6392\u3002\u8bba\u6587\u5c06\u8fd9\u4e00\u8ba4\u77e5\u8f6c\u53d8\u4e0e\u6559\u80b2\u548c\u5236\u5ea6\u8f6c\u578b\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u51fa\u4e86\u8de8\u8d8a\u4e09\u4e2a\u5206\u6790\u5c42\u9762\u548c\u4e09\u4e2a\u884c\u52a8\u9886\u57df\u7684\u6982\u5ff5\u6846\u67b6\uff1a\u6559\u5e08\u4e16\u754c\u89c2\u3001\u884c\u4e1a\u5173\u7cfb\u548c\u8bfe\u7a0b\u8bbe\u8ba1\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u4ee3\u8868\u4e86\u4ece\u7b97\u6cd5\u4f18\u5316\u5230\u8bed\u5883\u5bfc\u822a\u7684\u8ba4\u77e5\u8303\u5f0f\u8f6c\u53d8\uff0c\u9700\u8981\u8b66\u60d5\u6a21\u5f0f\u5d29\u6e83\u548c\u6587\u5316\u540c\u8d28\u5316\u7684\u98ce\u9669\uff0c\u5f3a\u8c03\u9700\u8981\u4e0e\u751f\u6210\u7cfb\u7edf\u8fdb\u884c\u6709\u610f\u8bc6\u7684\u4e92\u52a8\uff0c\u4ee5\u907f\u514d\u56de\u5f52\u5230\u5408\u6210\u7684\u7edf\u4e00\u6027\u3002"}}
{"id": "2602.07562", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07562", "abs": "https://arxiv.org/abs/2602.07562", "authors": ["Antoine Gonon", "Alexandre Cordonnier", "Nicolas Boumal"], "title": "Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction", "comment": null, "summary": "Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Gaussian Match-and-Copy\u57fa\u51c6\uff0c\u7528\u4e8e\u5206\u79bb\u68c0\u7d22\u548c\u8bb0\u5fc6\uff0c\u7814\u7a76Transformer\u5982\u4f55\u53d1\u5c55\u5339\u914d-\u590d\u5236\u7535\u8def\uff0c\u5e76\u5206\u6790\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u9690\u5f0f\u504f\u5dee\u5bfc\u81f4\u786c\u5339\u914d\u9009\u62e9", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5339\u914d-\u590d\u5236\u68c0\u7d22\u539f\u8bed\u5982\u4f55\u5728\u81ea\u7136\u6570\u636e\u4e2d\u6d8c\u73b0\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u68c0\u7d22\u548c\u8bb0\u5fc6\u662f\u7ea0\u7f20\u5728\u4e00\u8d77\u7684\u3002\u9700\u8981\u5206\u79bb\u8fd9\u4e24\u79cd\u673a\u5236\u6765\u7814\u7a76\u7eaf\u7cb9\u7684\u68c0\u7d22\u80fd\u529b\u3002", "method": "\u5f15\u5165Gaussian Match-and-Copy\u57fa\u51c6\uff0c\u901a\u8fc7\u7eaf\u4e8c\u9636\u76f8\u5173\u4fe1\u53f7\u9694\u79bb\u957f\u7a0b\u68c0\u7d22\u3002\u5728\u7b80\u5316\u6ce8\u610f\u529b\u8bbe\u7f6e\u4e2d\u5206\u6790\u4f18\u5316\u52a8\u6001\uff0c\u7814\u7a76\u68af\u5ea6\u4e0b\u964d\u5982\u4f55\u9a71\u52a8\u53c2\u6570\u5bf9\u9f50\u6700\u5927\u95f4\u9694\u5206\u79bb\u5668\u3002", "result": "GMC\u57fa\u51c6\u4fdd\u7559\u4e86Transformer\u5728\u5b9e\u8df5\u4e2d\u53d1\u5c55\u5339\u914d-\u590d\u5236\u7535\u8def\u7684\u5173\u952e\u5b9a\u6027\u65b9\u9762\uff0c\u5e76\u80fd\u533a\u5206\u4e0d\u540c\u67b6\u6784\u7684\u68c0\u7d22\u80fd\u529b\u3002\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u68af\u5ea6\u4e0b\u964d\u4f1a\u4f7f\u53c2\u6570\u53d1\u6563\u540c\u65f6\u65b9\u5411\u5bf9\u9f50\u6700\u5927\u95f4\u9694\u5206\u79bb\u5668\uff0c\u4ea7\u751f\u786c\u5339\u914d\u9009\u62e9\u3002", "conclusion": "GMC\u57fa\u51c6\u4e3a\u7814\u7a76\u68c0\u7d22\u673a\u5236\u63d0\u4f9b\u4e86\u53ef\u63a7\u73af\u5883\uff0c\u63ed\u793a\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u9690\u5f0f\u504f\u5dee\u5982\u4f55\u9a71\u52a8\u6a21\u578b\u5b66\u4e60\u786c\u5339\u914d\u9009\u62e9\uff0c\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3Transformer\u4e2d\u5339\u914d-\u590d\u5236\u7535\u8def\u7684\u6d8c\u73b0\u673a\u5236\u3002"}}
{"id": "2602.07566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07566", "abs": "https://arxiv.org/abs/2602.07566", "authors": ["Runcheng Wang", "Yaru Chen", "Guiguo Zhang", "Honghua Jiang", "Yongliang Qiao"], "title": "Cross-Camera Cow Identification via Disentangled Representation Learning", "comment": null, "summary": "Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u5b66\u4e60\u7684\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u5b50\u7a7a\u95f4\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\u7406\u8bba\uff0c\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u6a21\u5757\u5206\u79bb\u8eab\u4efd\u76f8\u5173\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u5728\u672a\u89c1\u6444\u50cf\u5934\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u52a8\u7269\u8bc6\u522b\u65b9\u6cd5\u5728\u53d7\u63a7\u5355\u6444\u50cf\u5934\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8de8\u6444\u50cf\u5934\u573a\u666f\u4e2d\u9762\u4e34\u4e25\u91cd\u6cdb\u5316\u6311\u6218\u3002\u5f53\u6a21\u578b\u4ece\u6e90\u6444\u50cf\u5934\u90e8\u7f72\u5230\u5177\u6709\u4e0d\u540c\u5149\u7167\u3001\u80cc\u666f\u3001\u89c6\u89d2\u548c\u6210\u50cf\u7279\u6027\u7684\u65b0\u76d1\u63a7\u8282\u70b9\u65f6\uff0c\u8bc6\u522b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u975e\u63a5\u89e6\u6280\u672f\u5728\u52a8\u6001\u771f\u5b9e\u519c\u573a\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u5b66\u4e60\u7684\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u5b50\u7a7a\u95f4\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\u7406\u8bba\uff0c\u901a\u8fc7\u5efa\u6a21\u5e95\u5c42\u7269\u7406\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u539f\u5219\u9a71\u52a8\u7684\u7279\u5f81\u89e3\u8026\u6a21\u5757\uff0c\u5c06\u89c2\u6d4b\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u6b63\u4ea4\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u6709\u6548\u5206\u79bb\u8de8\u6444\u50cf\u5934\u4fdd\u6301\u4e0d\u53d8\u7684\u7a33\u5b9a\u8eab\u4efd\u76f8\u5173\u751f\u7269\u7279\u5f81\u3002", "result": "\u6784\u5efa\u4e86\u6db5\u76d65\u4e2a\u4e0d\u540c\u6444\u50cf\u5934\u8282\u70b9\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u5f02\u6784\u91c7\u96c6\u8bbe\u5907\u548c\u590d\u6742\u7684\u5149\u7167\u89d2\u5ea6\u53d8\u5316\u3002\u57287\u4e2a\u8de8\u6444\u50cf\u5934\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523086.0%\uff0c\u663e\u8457\u4f18\u4e8e\u6e90\u6444\u50cf\u5934\u57fa\u7ebf\uff0851.9%\uff09\u548c\u6700\u5f3a\u7684\u8de8\u6444\u50cf\u5934\u57fa\u7ebf\u65b9\u6cd5\uff0879.8%\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7406\u8bba\u7684\u7279\u5f81\u89e3\u8026\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u4f5c\u5f0f\u8de8\u6444\u50cf\u5934\u5976\u725b\u8bc6\u522b\uff0c\u4e3a\u4e0d\u53d7\u63a7\u667a\u80fd\u519c\u573a\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2602.08311", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08311", "abs": "https://arxiv.org/abs/2602.08311", "authors": ["Shadman Rabby", "Md. Hefzul Hossain Papon", "Sabbir Ahmed", "Nokimul Hasan Arif", "A. B. M. Ashikur Rahman", "Irfan Ahmad"], "title": "Moral Sycophancy in Vision Language Models", "comment": "13 pages, 6 figures, 8 tables, Submitted for review in ACL", "summary": "Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9053\u5fb7\u8c04\u5a9a\u884c\u4e3a\uff0c\u53d1\u73b0VLMs\u5728\u7528\u6237\u610f\u89c1\u5f71\u54cd\u4e0b\u4f1a\u727a\u7272\u9053\u5fb7\u51c6\u786e\u6027\uff0c\u8868\u73b0\u51fa\u4ece\u6b63\u786e\u5230\u9519\u8bef\u5224\u65ad\u7684\u4e0d\u5bf9\u79f0\u8f6c\u53d8\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u9519\u8bef\u7ea0\u6b63\u4e0e\u9519\u8bef\u5f15\u5165\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u867d\u7136\u5148\u524d\u7814\u7a76\u63a2\u7d22\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e00\u822c\u60c5\u5883\u4e0b\u7684\u8c04\u5a9a\u884c\u4e3a\uff0c\u4f46\u5176\u5bf9\u57fa\u4e8e\u9053\u5fb7\u7684\u89c6\u89c9\u51b3\u7b56\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9996\u6b21\u7cfb\u7edf\u7814\u7a76VLMs\u4e2d\u7684\u9053\u5fb7\u8c04\u5a9a\u73b0\u8c61\u3002", "method": "\u5728Moralise\u548cM^3oralBench\u6570\u636e\u96c6\u4e0a\u8bc4\u4f3010\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u660e\u786e\u7684\u7528\u6237\u5206\u6b67\u60c5\u5883\u4e0b\u5206\u6790\u5176\u8868\u73b0\u3002\u4f7f\u7528\u9519\u8bef\u5f15\u5165\u7387\uff08EIR\uff09\u548c\u9519\u8bef\u7ea0\u6b63\u7387\uff08ECR\uff09\u8fdb\u884c\u91cf\u5316\u8bc4\u4f30\uff0c\u7814\u7a76\u6a21\u578b\u5728\u7528\u6237\u504f\u89c1\u5f71\u54cd\u4e0b\u7684\u884c\u4e3a\u53d8\u5316\u3002", "result": "VLMs\u7ecf\u5e38\u5728\u521d\u59cb\u5224\u65ad\u6b63\u786e\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u9053\u5fb7\u9519\u8bef\u7684\u540e\u7eed\u56de\u5e94\uff0c\u8868\u73b0\u51fa\u660e\u663e\u7684\u4e0d\u5bf9\u79f0\u6027\uff1a\u6a21\u578b\u66f4\u53ef\u80fd\u4ece\u9053\u5fb7\u6b63\u786e\u8f6c\u5411\u9053\u5fb7\u9519\u8bef\u5224\u65ad\uff0c\u800c\u975e\u76f8\u53cd\u65b9\u5411\u3002\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff1a\u5728Moralise\u4e0a\u540e\u7eed\u63d0\u793a\u901a\u5e38\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u5728M^3oralBench\u4e0a\u5219\u5448\u73b0\u6df7\u5408\u751a\u81f3\u6539\u5584\u7684\u7ed3\u679c\u3002\u6a21\u578b\u5728\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u4e0e\u9519\u8bef\u5f15\u5165\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u9053\u5fb7\u5f71\u54cd\u8868\u73b0\u51fa\u8106\u5f31\u6027\uff0c\u5177\u6709\u8f83\u5f3a\u9053\u5fb7\u6b63\u786e\u7acb\u573a\u7684\u521d\u59cb\u60c5\u5883\u4f1a\u5f15\u53d1\u66f4\u5f3a\u7684\u8c04\u5a9a\u884c\u4e3a\u3002\u8fd9\u7a81\u663e\u4e86\u9700\u8981\u5236\u5b9a\u539f\u5219\u6027\u7b56\u7565\u6765\u63d0\u9ad8\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u9053\u5fb7\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.08335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08335", "abs": "https://arxiv.org/abs/2602.08335", "authors": ["Yanming Li", "Xuelin Zhang", "WenJie Lu", "Ziye Tang", "Maodong Wu", "Haotian Luo", "Tongtong Wu", "Zijie Peng", "Hongze Mi", "Yibo Feng", "Naiqiang Tan", "Chao Huang", "Hong Chen", "Li Shen"], "title": "Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System", "comment": null, "summary": "Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.", "AI": {"tldr": "SHARP\u6846\u67b6\u901a\u8fc7\u57fa\u4e8eShapley\u503c\u7684\u5206\u5c42\u4fe1\u7528\u5206\u914d\u673a\u5236\uff0c\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3LLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u96be\u9898\u3002", "motivation": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u5de5\u5177\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u96c6\u6210\u662f\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u65b0\u8303\u5f0f\uff0c\u4f46\u8bad\u7ec3\u8fd9\u4e9b\u7cfb\u7edf\u9762\u4e34\u4fe1\u7528\u5206\u914d\u6311\u6218\uff0c\u96be\u4ee5\u786e\u5b9a\u5177\u4f53\u529f\u80fd\u667a\u80fd\u4f53\u5bf9\u51b3\u7b56\u8f68\u8ff9\u6210\u529f\u6216\u5931\u8d25\u7684\u8d23\u4efb\u3002", "method": "\u63d0\u51faSHARP\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u7684\u5956\u52b1\u673a\u5236\u5b9e\u73b0\u7cbe\u786e\u4fe1\u7528\u5206\u914d\uff1a\u5305\u62ec\u5168\u5c40\u5e7f\u64ad-\u51c6\u786e\u6027\u5956\u52b1\u3001\u57fa\u4e8eShapley\u503c\u7684\u8fb9\u9645\u4fe1\u7528\u5956\u52b1\u548c\u5de5\u5177\u8fc7\u7a0b\u5956\u52b1\uff0c\u5e76\u901a\u8fc7\u8f68\u8ff9\u7ec4\u5185\u667a\u80fd\u4f53\u7279\u5b9a\u4f18\u52bf\u7684\u5f52\u4e00\u5316\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSHARP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u5e73\u5747\u63d0\u534723.66%\uff0c\u76f8\u6bd4\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5e73\u5747\u63d0\u534714.05%\u3002", "conclusion": "SHARP\u901a\u8fc7\u7cbe\u786e\u7684\u4fe1\u7528\u5206\u914d\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3aLLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07588", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07588", "abs": "https://arxiv.org/abs/2602.07588", "authors": ["Ziyang Yu", "Wenbing Huang", "Yang Liu"], "title": "Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge", "comment": "The Fourteenth International Conference on Learning Representations (ICLR 2026)", "summary": "Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.", "AI": {"tldr": "PVB\u662f\u4e00\u4e2a\u9884\u8bad\u7ec3\u53d8\u5206\u6865\u6a21\u578b\uff0c\u901a\u8fc7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u5c06\u521d\u59cb\u7ed3\u6784\u6620\u5c04\u5230\u566a\u58f0\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u6865\u5339\u914d\u5c06\u5176\u4f20\u8f93\u5230\u9636\u6bb5\u7279\u5b9a\u76ee\u6807\uff0c\u7edf\u4e00\u4e86\u5355\u7ed3\u6784\u548c\u914d\u5bf9\u8f68\u8ff9\u6570\u636e\u7684\u8bad\u7ec3\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u5206\u5b50\u52a8\u529b\u5b66\u8f68\u8ff9\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u867d\u7136\u80fd\u63d0\u4f9b\u5168\u539f\u5b50\u5206\u8fa8\u7387\u7684\u5206\u5b50\u884c\u4e3a\u8868\u5f81\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u8981\u4e48\u5728\u4e0d\u540c\u7cfb\u7edf\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u8981\u4e48\u7531\u4e8e\u8f68\u8ff9\u6570\u636e\u5206\u5b50\u591a\u6837\u6027\u6709\u9650\u800c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7ed3\u6784\u4fe1\u606f\u6765\u63d0\u9ad8\u751f\u6210\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51fa\u9884\u8bad\u7ec3\u53d8\u5206\u6865\uff08PVB\uff09\u6a21\u578b\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff1a1\uff09\u5c06\u521d\u59cb\u7ed3\u6784\u6620\u5c04\u5230\u566a\u58f0\u6f5c\u5728\u7a7a\u95f4\uff1b2\uff09\u901a\u8fc7\u589e\u5f3a\u6865\u5339\u914d\u5c06\u5176\u4f20\u8f93\u5230\u9636\u6bb5\u7279\u5b9a\u76ee\u6807\uff1b3\uff09\u7edf\u4e00\u5355\u7ed3\u6784\u548c\u914d\u5bf9\u8f68\u8ff9\u6570\u636e\u7684\u8bad\u7ec3\uff1b4\uff09\u5bf9\u4e8e\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\uff0c\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f34\u968f\u5339\u914d\u4f18\u5316\uff0c\u52a0\u901f\u5411holo\u72b6\u6001\u7684\u8fdb\u5c55\u3002", "result": "\u5728\u86cb\u767d\u8d28\u548c\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPVB\u80fd\u591f\u5fe0\u5b9e\u5730\u518d\u73b0\u5206\u5b50\u52a8\u529b\u5b66\u7684\u70ed\u529b\u5b66\u548c\u52a8\u529b\u5b66\u89c2\u6d4b\u503c\uff0c\u540c\u65f6\u63d0\u4f9b\u7a33\u5b9a\u9ad8\u6548\u7684\u751f\u6210\u52a8\u529b\u5b66\uff0c\u652f\u6301\u5bf9\u63a5\u6784\u8c61\u7684\u9ad8\u6548\u540e\u4f18\u5316\u3002", "conclusion": "PVB\u901a\u8fc7\u7edf\u4e00\u8de8\u8bad\u7ec3\u9636\u6bb5\u7684\u8de8\u57df\u7ed3\u6784\u77e5\u8bc6\u4f7f\u7528\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u5206\u5b50\u52a8\u529b\u5b66\u8f68\u8ff9\u751f\u6210\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u5206\u5b50\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.07574", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07574", "abs": "https://arxiv.org/abs/2602.07574", "authors": ["Wenjie Liu", "Hao Wu", "Xin Qiu", "Yingqi Fan", "Yihan Zhang", "Anhao Zhao", "Yunpu Ma", "Xiaoyu Shen"], "title": "ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention", "comment": null, "summary": "Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.", "AI": {"tldr": "ViCA\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7a00\u758f\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5927\u5e45\u51cf\u5c11\u89c6\u89c9\u5904\u7406\u8ba1\u7b97\u91cf\uff0c\u5728\u4fdd\u630198%\u51c6\u786e\u7387\u7684\u540c\u65f6\u5c06\u89c6\u89c9\u4fa7\u8ba1\u7b97\u964d\u81f34%\uff0c\u5b9e\u73b03.5-10\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u73b0\u6709MLLM\u91c7\u7528\u7edf\u4e00\u7684self-attention\u8bbe\u8ba1\uff0c\u5728\u6bcf\u4e2aTransformer\u5c42\u5904\u7406\u89c6\u89c9\u548c\u6587\u672ctoken\uff0c\u5bfc\u81f4\u5927\u91cf\u8ba1\u7b97\u5f00\u9500\u3002\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u5d4c\u5165\u5df2\u4e0e\u8bed\u8a00\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4e14\u6709\u6548\u7684\u89c6\u89c9-\u8bed\u8a00\u4ea4\u4e92\u4ec5\u53d1\u751f\u5728\u5c11\u6570\u5c42\u4e2d\u3002", "method": "\u63d0\u51faViCA\u67b6\u6784\uff0c\u89c6\u89c9token\u7ed5\u8fc7\u6240\u6709self-attention\u548cfeed-forward\u5c42\uff0c\u4ec5\u901a\u8fc7\u9009\u5b9a\u7684\u7a00\u758f\u8de8\u6ce8\u610f\u529b\u5c42\u4e0e\u6587\u672c\u4ea4\u4e92\uff0c\u5b9e\u73b0\u6700\u5c0f\u5316\u7684\u89c6\u89c9\u5904\u7406\u3002", "result": "\u57283\u4e2aMLLM\u4e3b\u5e72\u30019\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u548c26\u4e2a\u526a\u679d\u57fa\u7ebf\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cViCA\u4fdd\u630198%\u57fa\u7ebf\u51c6\u786e\u7387\uff0c\u89c6\u89c9\u4fa7\u8ba1\u7b97\u964d\u81f34%\uff0c\u5355\u6279\u6b21\u63a8\u7406\u52a0\u901f3.5\u500d\uff0c\u591a\u6279\u6b21\u52a0\u901f10\u500d\u4ee5\u4e0a\u3002", "conclusion": "ViCA\u901a\u8fc7\u7a00\u758f\u8de8\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\uff0c\u63d0\u4f9b\u786c\u4ef6\u53cb\u597d\u7684\u63a8\u7406\u6d41\u7a0b\uff0c\u89c6\u89c9\u5b9a\u4f4d\u5f00\u9500\u63a5\u8fd1\u96f6\uff0c\u4e14\u53ef\u4e0etoken\u526a\u679d\u65b9\u6cd5\u6b63\u4ea4\u7ec4\u5408\u83b7\u5f97\u8fdb\u4e00\u6b65\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2602.08339", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08339", "abs": "https://arxiv.org/abs/2602.08339", "authors": ["Chengyi Du", "Yazhe Niu", "Dazhong Shen", "Luxin Xu"], "title": "CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT", "comment": "16 pages 6 figures", "summary": "Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.", "AI": {"tldr": "CoTZero\uff1a\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u6570\u636e\u5408\u6210\u548c\u8ba4\u77e5\u5bf9\u9f50\u8bad\u7ec3\uff0c\u63d0\u5347\u89c6\u89c9\u63a8\u7406\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u8868\u9762\u76f8\u5173\u6027\u800c\u975e\u903b\u8f91\u4e00\u81f4\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u5bfc\u81f4\u9ad8\u5c42\u6b21\u8bed\u4e49\u7ed3\u6784\u548c\u56e0\u679c\u5173\u7cfb\u7406\u89e3\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u7ec4\u5408\u6027\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u80fd\u529b", "method": "\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u53cc\u9636\u6bb5\u6570\u636e\u5408\u6210\uff1a\u81ea\u4e0b\u800c\u4e0a\u63d0\u53d6\u539f\u5b50\u89c6\u89c9\u57fa\u5143\u5e76\u7ec4\u5408\u6210\u7ed3\u6784\u5316\u95ee\u9898\u63a8\u7406\u5f62\u5f0f\uff0c\u81ea\u4e0a\u800c\u4e0b\u7528\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\u5c40\u90e8\u7ec6\u8282\u548c\u56e0\u679c\u5173\u7cfb\u89e3\u91ca\uff1b2) \u8ba4\u77e5\u5bf9\u9f50\u8bad\u7ec3\uff1a\u5728\u5408\u6210\u7684\u601d\u7ef4\u94fe\u6570\u636e\u4e0a\uff0c\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u4e2d\u7684\u8ba4\u77e5\u4e00\u81f4\u53ef\u9a8c\u8bc1\u5956\u52b1\u6765\u52a0\u5f3a\u5206\u5c42\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b", "result": "\u5728\u591a\u7ea7\u8bed\u4e49\u4e0d\u4e00\u81f4\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b\u8bcd\u6c47\u6270\u52a8\u8d1f\u4f8b\uff09\u4e2d\uff0c\u5728\u57df\u5185\u548c\u57df\u5916\u8bbe\u7f6e\u4e0b\u8fbe\u523083.33%\u7684F1\u5206\u6570\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u5404\u7ec4\u4ef6\u5bf9\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u89c6\u89c9\u63a8\u7406\u5747\u6709\u8d21\u732e", "conclusion": "CoTZero\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u8ba4\u77e5\u6a21\u578b\u5230\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u5316\u8868\u793a\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u63a8\u7406"}}
{"id": "2602.07593", "categories": ["cs.LG", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07593", "abs": "https://arxiv.org/abs/2602.07593", "authors": ["Polina Gordienko", "Christoph Jansen", "Julian Rodemann", "Georg Schollmeyer"], "title": "Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking", "comment": null, "summary": "Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.", "AI": {"tldr": "\u8bba\u6587\u5c06\u591a\u6307\u6807\u57fa\u51c6\u6d4b\u8bd5\u5f62\u5f0f\u5316\u4e3a\u793e\u4f1a\u9009\u62e9\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u504f\u597d\u7ed3\u6784\u6761\u4ef6\u4e0b\uff08\u5355\u5cf0\u3001\u7fa4\u53ef\u5206\u3001\u8ddd\u79bb\u53d7\u9650\uff09\uff0c\u53ef\u4ee5\u6784\u5efa\u7a33\u5b9a\u7684\u6a21\u578b\u6392\u540d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u805a\u5408\u65b9\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982HELM MMLU\uff09\u5305\u542b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u6548\u7387\u7b49\u591a\u4e2a\u6307\u6807\uff0c\u5c06\u8fd9\u4e9b\u6307\u6807\u805a\u5408\u6210\u5355\u4e00\u6392\u540d\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f1a\u51fa\u73b0\u4e0d\u4e00\u81f4\u6216\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u591a\u6307\u6807\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u805a\u5408\u96be\u9898\u3002", "method": "\u5c06\u57fa\u51c6\u6d4b\u8bd5\u5f62\u5f0f\u5316\u4e3a\u793e\u4f1a\u9009\u62e9\u95ee\u9898\uff1a\u6bcf\u4e2a\u6307\u6807\u5728\u6570\u636e\u96c6\u4e0a\u8bf1\u5bfc\u51fa\u6a21\u578b\u504f\u597d\u6392\u540d\uff0c\u57fa\u51c6\u7b97\u5b50\u805a\u5408\u8fd9\u4e9b\"\u6295\u7968\"\u3002\u7814\u7a76\u5728\u4e09\u79cd\u504f\u597d\u7ed3\u6784\u9650\u5236\u4e0b\uff08\u5355\u5cf0\u504f\u597d\u3001\u7fa4\u53ef\u5206\u504f\u597d\u3001\u8ddd\u79bb\u53d7\u9650\u504f\u597d\uff09\u7684\u805a\u5408\u53ef\u80fd\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u8fd9\u4e09\u79cd\u504f\u597d\u7ed3\u6784\u6761\u4ef6\u4e0b\uff0c\u57fa\u51c6\u7b97\u5b50\u80fd\u591f\u6784\u5efa\u884c\u4e3a\u826f\u597d\u7684\u6a21\u578b\u6392\u540d\u3002\u5b9e\u8bc1\u7814\u7a76\u4e86HELM MMLU\u7b49\u73b0\u4ee3\u57fa\u51c6\u5957\u4ef6\uff0c\u9a8c\u8bc1\u4e86\u54ea\u4e9b\u7ed3\u6784\u6761\u4ef6\u5728\u54ea\u4e9b\u57fa\u51c6\u95ee\u9898\u4e0a\u5f97\u5230\u6ee1\u8db3\u3002", "conclusion": "\u867d\u7136Arrow\u4e0d\u53ef\u80fd\u5b9a\u7406\u8868\u660e\u4e00\u822c\u60c5\u51b5\u4e0b\u7684\u5b8c\u7f8e\u805a\u5408\u4e0d\u53ef\u80fd\uff0c\u4f46\u8bba\u6587\u8bc1\u660e\u5728\u73b0\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e38\u89c1\u7684\u504f\u597d\u7ed3\u6784\u6761\u4ef6\u4e0b\uff0c\u6709\u610f\u4e49\u7684\u591a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u6784\u5efa\u7a33\u5b9a\u7684\u6a21\u578b\u6392\u540d\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.07596", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07596", "abs": "https://arxiv.org/abs/2602.07596", "authors": ["Xi Chen", "Ming Li", "Junxi Li", "Changsheng Li", "Peisong Wang", "Lizhong Ding", "Ye Yuan", "Guoren Wang"], "title": "Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization", "comment": null, "summary": "Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.", "AI": {"tldr": "Astro\u662f\u4e00\u4e2a\u6fc0\u6d3b\u5f15\u5bfc\u7684\u7ed3\u6784\u5316\u6b63\u5219\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6784\u5185\u5728\u9c81\u68d2\u7684\u6743\u91cd\u6765\u6291\u5236\u6743\u91cd\u548c\u6fc0\u6d3b\u5f02\u5e38\u503c\u5bf9\u91cf\u5316\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u5b9e\u73b0\u96f6\u63a8\u7406\u5ef6\u8fdf\u4e14\u4e0e\u4e3b\u6d41\u91cf\u5316\u65b9\u6cd5\u6b63\u4ea4\u3002", "motivation": "\u4ec5\u6743\u91cd\u7684\u8bad\u7ec3\u540e\u91cf\u5316\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u5230\u6743\u91cd\u548c\u6fc0\u6d3b\u5f02\u5e38\u503c\u5bfc\u81f4\u7684\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\u56f0\u6270\u3002\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u8981\u4e48\u6291\u5236\u5f02\u5e38\u503c\u4e0d\u8db3\uff0c\u8981\u4e48\u5e26\u6765\u663e\u8457\u7684\u90e8\u7f72\u6548\u7387\u95ee\u9898\uff08\u5982\u63a8\u7406\u5ef6\u8fdf\u3001\u7e41\u91cd\u9884\u5904\u7406\u6216\u590d\u6742\u7684\u7b97\u5b50\u878d\u5408\uff09\u3002", "method": "\u57fa\u4e8e\u8fc7\u53c2\u6570\u5316LLM\u901a\u5e38\u6536\u655b\u5230\u5e73\u5766\u6700\u5c0f\u503c\u7684\u6d1e\u5bdf\uff0c\u63d0\u51faAstro\u6846\u67b6\uff1a\u5229\u7528\u6fc0\u6d3b\u5f15\u5bfc\u7684\u6b63\u5219\u5316\u76ee\u6807\u4e3b\u52a8\u91cd\u6784\u5185\u5728\u9c81\u68d2\u7684\u6743\u91cd\uff0c\u79ef\u6781\u6291\u5236\u4e0e\u9ad8\u5e45\u5ea6\u6fc0\u6d3b\u5bf9\u5e94\u7684\u6743\u91cd\u5f02\u5e38\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u96f6\u63a8\u7406\u5ef6\u8fdf\u4e14\u4e0eGPTQ\u7b49\u4e3b\u6d41\u91cf\u5316\u65b9\u6cd5\u6b63\u4ea4\u3002", "result": "\u5728LLaMA-2-7B\u4e0a\uff0cAstro\u5b9e\u73b0\u4e86\u6bd4\u590d\u6742\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65cb\u8f6c\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e14\u91cf\u5316\u65f6\u95f4\u4ec5\u4e3a\u540e\u8005\u7684\u7ea61/3\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660eAstro\u5177\u6709\u9ad8\u5ea6\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "Astro\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u7684\u7ed3\u6784\u5316\u6b63\u5219\u5316\u6709\u6548\u89e3\u51b3\u4e86\u6743\u91cd\u548c\u6fc0\u6d3b\u5f02\u5e38\u503c\u5bf9\u91cf\u5316\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u786c\u4ef6\u53cb\u597d\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u96f6\u63a8\u7406\u5ef6\u8fdf\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2602.07595", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07595", "abs": "https://arxiv.org/abs/2602.07595", "authors": ["Yuanzhi Liang", "Xuan'er Wu", "Yirui Liu", "Yijie Fang", "Yizhen Fan", "Ke Hao", "Rui Li", "Ruiying Liu", "Ziqi Ni", "Peng Yu", "Yanbo Wang", "Haibin Huang", "Qizhen Weng", "Chi Zhang", "Xuelong Li"], "title": "TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation", "comment": null, "summary": "Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u76d1\u7763\u7b56\u7565\u5851\u9020\u3001\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u504f\u597d\u7684\u7cbe\u70bc\u6574\u5408\u5230\u5355\u4e00\u7a33\u5b9a\u6027\u7ea6\u675f\u4f18\u5316\u5806\u6808\u4e2d\uff0c\u65e8\u5728\u63d0\u5347\u751f\u6210\u89c6\u9891\u7684\u611f\u77e5\u4fdd\u771f\u5ea6\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u63d0\u793a\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u540e\u8bad\u7ec3\u662f\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u8f6c\u5316\u4e3a\u751f\u4ea7\u5bfc\u5411\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u9700\u8981\u89e3\u51b3\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u65f6\u95f4\u7d2f\u79ef\u6545\u969c\u6a21\u5f0f\u4ee5\u53ca\u53cd\u9988\u4fe1\u606f\u5f02\u6784\u3001\u4e0d\u786e\u5b9a\u4e14\u533a\u5206\u5ea6\u5f31\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u9636\u6bb5\u3001\u8bca\u65ad\u9a71\u52a8\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u76d1\u7763\u7b56\u7565\u5851\u9020\u3001\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u504f\u597d\u7684\u7cbe\u70bc\u6574\u5408\u5230\u5355\u4e00\u7a33\u5b9a\u6027\u7ea6\u675f\u4f18\u5316\u5806\u6808\u4e2d\uff0c\u56f4\u7ed5\u5b9e\u9645\u89c6\u9891\u751f\u6210\u7ea6\u675f\u8bbe\u8ba1\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u84dd\u56fe\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u6269\u5c55\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u521d\u59cb\u5316\u65f6\u5efa\u7acb\u7684\u53ef\u63a7\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u611f\u77e5\u4fdd\u771f\u5ea6\u3001\u65f6\u95f4\u8fde\u8d2f\u6027\u548c\u63d0\u793a\u9075\u5faa\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u540e\u8bad\u7ec3\u89c6\u4e3a\u5206\u9636\u6bb5\u3001\u8bca\u65ad\u9a71\u52a8\u7684\u8fc7\u7a0b\u800c\u975e\u5b64\u7acb\u6280\u5de7\u7684\u96c6\u5408\uff0c\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u4e14\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u6709\u6548\u7684\u89c6\u9891\u751f\u6210\u540e\u8bad\u7ec3\u7ba1\u9053\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08344", "abs": "https://arxiv.org/abs/2602.08344", "authors": ["Qi Guo", "Jianing Wang", "Deyang Kong", "Xiangyu Xi", "Jianfei Zhang", "Yi Lu", "Jingang Wang", "Wei Wang", "Shikun Zhang", "Wei Ye"], "title": "OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration", "comment": null, "summary": "Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5e76\u884c\u601d\u7ef4\u4f18\u5316\u65b9\u6cd5OPE\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u5927\u7eb2\u6765\u5f15\u5bfc\u8def\u5f84\u63a2\u7d22\uff0c\u89e3\u51b3\u5e76\u884c\u601d\u7ef4\u4e2d\u63a2\u7d22\u8def\u5f84\u95f4\u4fe1\u606f\u5197\u4f59\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u601d\u7ef4\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u805a\u5408\u9636\u6bb5\u7684\u4f18\u5316\uff0c\u5bf9\u8def\u5f84\u63a2\u7d22\u9636\u6bb5\u5173\u6ce8\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\u63a2\u7d22\u8def\u5f84\u95f4\u7684\u4e92\u4fe1\u606f\u74f6\u9888\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u4fe1\u606f\u5197\u4f59\u95ee\u9898\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5927\u7eb2\u5f15\u5bfc\u7684\u8def\u5f84\u63a2\u7d22(OPE)\u65b9\u6cd5\uff1a1) \u5728\u5e76\u884c\u8def\u5f84\u63a8\u7406\u524d\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u5927\u7eb2\uff0c\u663e\u5f0f\u5212\u5206\u89e3\u7a7a\u95f4\uff1b2) \u91c7\u7528\u8fed\u4ee3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5206\u522b\u4f18\u5316\u5927\u7eb2\u89c4\u5212\u548c\u5927\u7eb2\u5f15\u5bfc\u63a8\u7406\uff1b3) \u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u6846\u67b6\u4e0b\u5b9e\u73b0\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eOPE\u80fd\u6709\u6548\u63d0\u5347\u4e0d\u540c\u805a\u5408\u7b56\u7565\u4e0b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4f7f\u5927\u578b\u63a8\u7406\u6a21\u578b\u66f4\u53ef\u9760\u5730\u53d1\u73b0\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u5927\u7eb2\u6765\u5f15\u5bfc\u8def\u5f84\u63a2\u7d22\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u4fe1\u606f\u5197\u4f59\uff0c\u63d0\u9ad8\u63a2\u7d22\u8def\u5f84\u95f4\u7684\u4fe1\u606f\u591a\u6837\u6027\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u5e76\u884c\u601d\u7ef4\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2602.07599", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07599", "abs": "https://arxiv.org/abs/2602.07599", "authors": ["Mehryar Mohri"], "title": "Rational Transductors", "comment": null, "summary": "Standard Transformers excel at semantic modeling but struggle with\n  rigid sequential logic and state tracking. Theoretical work\n  establishes that self-attention is limited to $\\AC^0$ (under hard\n  attention) or $\\TC^0$ (under soft attention), complexity classes\n  that often fail to support robust length generalization on\n  sequential problems without intermediate chain-of-thought. In this\n  work, we introduce \\emph{Rational Transductors}, a dual-stream\n  architecture that augments the Transformer with a matrix-valued\n  recurrence derived from Weighted Finite Automata (WFA). By\n  injecting rational state information into the attention mechanism\n  via a \\emph{Deep Rational Injection} scheme, our framework strictly\n  generalizes the expressive power of Transformers to capture all\n  Regular Languages, $\\NC^1$-complete problems (such as Boolean\n  Formula Evaluation), and fundamental separations like Parity and\n  Modular Counting, while preserving $O(L + \\log T)$ parallel time\n  complexity. We ground the architecture in a rigorous learning\n  theory: we prove that \\emph{Random Rational Features} act as a\n  universal basis for sequential dependencies, justifying our\n  initialization strategy, while establishing that the\n  \\emph{Differentiable Rational Feature} regime is necessary to close\n  the representational compactness gap. Theoretical analysis and\n  empirical results demonstrate that Rational Transductors solve the\n  \"Regular Gap,\" enabling robust length generalization on algorithmic\n  tasks where standard Transformers fail, without the sequential\n  computational bottlenecks of traditional RNNs.", "AI": {"tldr": "Rational Transductors\uff1a\u4e00\u79cd\u53cc\u6d41\u67b6\u6784\uff0c\u901a\u8fc7\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a\u7684\u77e9\u9635\u9012\u5f52\u589e\u5f3aTransformer\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u6b63\u5219\u8bed\u8a00\u548cNC\u00b9\u5b8c\u5168\u95ee\u9898\uff0c\u89e3\u51b3\u6807\u51c6Transformer\u5728\u5e8f\u5217\u903b\u8f91\u548c\u957f\u5ea6\u6cdb\u5316\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6807\u51c6Transformer\u5728\u8bed\u4e49\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u521a\u6027\u5e8f\u5217\u903b\u8f91\u548c\u72b6\u6001\u8ddf\u8e2a\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u7406\u8bba\u7814\u7a76\u8868\u660e\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u786c\u6ce8\u610f\u529b\u4e0b\u9650\u4e8eAC\u2070\u7c7b\uff0c\u8f6f\u6ce8\u610f\u529b\u4e0b\u9650\u4e8eTC\u2070\u7c7b\uff0c\u8fd9\u4e9b\u590d\u6742\u5ea6\u7c7b\u901a\u5e38\u65e0\u6cd5\u5728\u6ca1\u6709\u4e2d\u95f4\u601d\u7ef4\u94fe\u7684\u60c5\u51b5\u4e0b\u652f\u6301\u5e8f\u5217\u95ee\u9898\u7684\u7a33\u5065\u957f\u5ea6\u6cdb\u5316\u3002", "method": "\u63d0\u51faRational Transductors\u53cc\u6d41\u67b6\u6784\uff0c\u901a\u8fc7\u4ece\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a\uff08WFA\uff09\u6d3e\u751f\u7684\u77e9\u9635\u9012\u5f52\u589e\u5f3aTransformer\u3002\u901a\u8fc7\u6df1\u5ea6\u7406\u6027\u6ce8\u5165\u65b9\u6848\u5c06\u7406\u6027\u72b6\u6001\u4fe1\u606f\u6ce8\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e25\u683c\u63a8\u5e7f\u4e86Transformer\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u6355\u83b7\u6240\u6709\u6b63\u5219\u8bed\u8a00\u3001NC\u00b9\u5b8c\u5168\u95ee\u9898\uff08\u5982\u5e03\u5c14\u516c\u5f0f\u6c42\u503c\uff09\u4ee5\u53ca\u57fa\u672c\u5206\u79bb\u95ee\u9898\uff08\u5982\u5947\u5076\u6027\u548c\u6a21\u8ba1\u6570\uff09\uff0c\u540c\u65f6\u4fdd\u6301O(L + log T)\u7684\u5e76\u884c\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cRational Transductors\u89e3\u51b3\u4e86\"\u6b63\u5219\u5dee\u8ddd\"\uff0c\u5728\u6807\u51c6Transformer\u5931\u8d25\u7684\u7b97\u6cd5\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u957f\u5ea6\u6cdb\u5316\u3002", "conclusion": "Rational Transductors\u901a\u8fc7\u7ed3\u5408Transformer\u7684\u8bed\u4e49\u5efa\u6a21\u80fd\u529b\u548cWFA\u7684\u5e8f\u5217\u5904\u7406\u80fd\u529b\uff0c\u586b\u8865\u4e86\u6807\u51c6Transformer\u5728\u5e8f\u5217\u903b\u8f91\u5904\u7406\u4e0a\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edfRNN\u7684\u987a\u5e8f\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3a\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07605", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07605", "abs": "https://arxiv.org/abs/2602.07605", "authors": ["Hulingxiao He", "Zijun Geng", "Yuxin Peng"], "title": "Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning", "comment": "Published as a conference paper at ICLR 2026. The models are available at https://huggingface.co/collections/StevenHH2000/fine-r1", "summary": "Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of \"visual analysis, candidate sub-categories, comparison, and prediction\", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.", "AI": {"tldr": "Fine-R1\u662f\u4e00\u79cd\u4e13\u95e8\u7528\u4e8e\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7R1\u5f0f\u8bad\u7ec3\u6846\u67b6\uff0c\u4ec5\u97004-shot\u8bad\u7ec3\u5c31\u80fd\u8d85\u8d8a\u73b0\u6709\u901a\u7528MLLM\u548c\u5bf9\u6bd4CLIP\u6a21\u578b\uff0c\u5728seen\u548cunseen\u5b50\u7c7b\u522b\u8bc6\u522b\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7c97\u7c92\u5ea6\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b58\u5728\u4e0d\u8db3\uff1a\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3001\u5bb9\u6613\u8fc7\u62df\u5408seen\u5b50\u7c7b\u522b\u3001\u5bf9unseen\u5b50\u7c7b\u522b\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u540c\u65f6\uff0c\u4e0e\u4e13\u95e8\u7528\u4e8e\u5224\u522b\u4efb\u52a1\u7684\u5bf9\u6bd4CLIP\u6a21\u578b\u76f8\u6bd4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u91c7\u7528R1\u5f0f\u8bad\u7ec3\u6846\u67b6\uff1a1) \u94fe\u5f0f\u601d\u7ef4\u76d1\u7763\u5fae\u8c03\uff1a\u6784\u5efa\u9ad8\u8d28\u91cf\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522bCoT\u6570\u636e\u96c6\uff0c\u5305\u542b\"\u89c6\u89c9\u5206\u6790\u3001\u5019\u9009\u5b50\u7c7b\u522b\u3001\u6bd4\u8f83\u3001\u9884\u6d4b\"\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u6a21\u578b\u8f6c\u53d8\u4e3a\u5f3a\u5927\u7684\u5f00\u653e\u4e16\u754c\u5206\u7c7b\u5668\uff1b2) \u4e09\u5143\u7ec4\u589e\u5f3a\u7b56\u7565\u4f18\u5316\uff1a\u7c7b\u5185\u589e\u5f3a\u6df7\u5408\u540c\u4e00\u7c7b\u522b\u5185\u951a\u70b9\u548c\u6b63\u6837\u672c\u56fe\u50cf\u7684\u8f68\u8ff9\u4ee5\u63d0\u9ad8\u5bf9\u7c7b\u5185\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u7c7b\u95f4\u589e\u5f3a\u6700\u5927\u5316\u8de8\u5b50\u7c7b\u522b\u56fe\u50cf\u6761\u4ef6\u4e0b\u7684\u54cd\u5e94\u5dee\u5f02\u4ee5\u589e\u5f3a\u5224\u522b\u80fd\u529b\u3002", "result": "\u4ec5\u97004-shot\u8bad\u7ec3\uff0cFine-R1\u5728\u8bc6\u522bseen\u548cunseen\u5b50\u7c7b\u522b\u4e0a\u90fd\u8d85\u8d8a\u4e86\u73b0\u6709\u901a\u7528MLLM\u3001\u63a8\u7406MLLM\u751a\u81f3\u5bf9\u6bd4CLIP\u6a21\u578b\uff0c\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u663e\u793a\u51fa\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "Fine-R1\u901a\u8fc7\u521b\u65b0\u7684R1\u5f0f\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u96be\u4ee5\u83b7\u53d6\u4e13\u5bb6\u6807\u6ce8\u7684\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08353", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08353", "abs": "https://arxiv.org/abs/2602.08353", "authors": ["Zhang Jiasheng", "Li Zhangpin", "Wang Mingzhe", "Shao Jie", "Cui Jiangtao", "Li Hui"], "title": "Towards Better Evolution Modeling for Temporal Knowledge Graphs", "comment": "13 pages, 11 figures", "summary": "Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.", "AI": {"tldr": "\u73b0\u6709\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u57fa\u51c6\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u4ec5\u901a\u8fc7\u7edf\u8ba1\u5171\u73b0\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1SOTA\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4f7f\u7528\u4efb\u4f55\u65f6\u5e8f\u4fe1\u606f\uff0c\u8fd9\u66b4\u9732\u4e86\u6570\u636e\u96c6\u504f\u5dee\u548c\u8bc4\u4f30\u4efb\u52a1\u8fc7\u4e8e\u7b80\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u53d1\u73b0\u73b0\u6709\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u57fa\u51c6\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u5373\u4f7f\u4e0d\u4f7f\u7528\u4efb\u4f55\u65f6\u5e8f\u4fe1\u606f\uff0c\u4ec5\u901a\u8fc7\u7edf\u8ba1\u5b9e\u4f53\u5171\u73b0\u5c31\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8fd9\u8868\u660e\u5f53\u524d\u8bc4\u4f30\u4f53\u7cfb\u65e0\u6cd5\u771f\u6b63\u8861\u91cf\u6a21\u578b\u5bf9\u65f6\u5e8f\u6f14\u5316\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5206\u6790\u73b0\u6709\u57fa\u51c6\u95ee\u9898\u7684\u6839\u6e90\uff0c\u8bc6\u522b\u6570\u636e\u96c6\u4e2d\u7684\u56fa\u6709\u504f\u5dee\u548c\u8bc4\u4f30\u4efb\u52a1\u7684\u8fc7\u5ea6\u7b80\u5316\u5f62\u5f0f\uff1b\u6784\u5efa\u65b0\u7684TKG\u6f14\u5316\u57fa\u51c6\uff0c\u5305\u62ec\u56db\u4e2a\u504f\u5dee\u6821\u6b63\u7684\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u4e0e\u6f14\u5316\u8fc7\u7a0b\u7d27\u5bc6\u5bf9\u9f50\u7684\u65b0\u4efb\u52a1\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u591a\u4e2a\u5c40\u9650\u6027\uff1a\u65f6\u95f4\u95f4\u9694\u77e5\u8bc6\u7684\u4e0d\u5408\u7406\u683c\u5f0f\u5316\u3001\u5ffd\u7565\u77e5\u8bc6\u8fc7\u65f6\u6027\u5b66\u4e60\u3001\u4ee5\u53ca\u7528\u4e8e\u7cbe\u786e\u6f14\u5316\u7406\u89e3\u7684\u4e0d\u8db3\u4fe1\u606f\uff1b\u63d0\u51fa\u4e86\u65b0\u7684\u57fa\u51c6\u6765\u4fc3\u8fdb\u5bf9TKG\u6f14\u5316\u5efa\u6a21\u6311\u6218\u7684\u66f4\u51c6\u786e\u7406\u89e3\u3002", "conclusion": "\u73b0\u6709\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u57fa\u51c6\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u65e0\u6cd5\u516c\u5e73\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6765\u7ea0\u6b63\u6570\u636e\u96c6\u504f\u5dee\uff0c\u8bbe\u8ba1\u66f4\u8d34\u8fd1\u5b9e\u9645\u6f14\u5316\u8fc7\u7a0b\u7684\u8bc4\u4f30\u4efb\u52a1\uff0c\u624d\u80fd\u771f\u6b63\u63a8\u52a8\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u6f14\u5316\u5efa\u6a21\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.07608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07608", "abs": "https://arxiv.org/abs/2602.07608", "authors": ["Yixin Chen", "Ziyu Su", "Lingbin Meng", "Elshad Hasanov", "Wei Chen", "Anil Parwani", "M. Khalid Khan Niazi"], "title": "HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology", "comment": null, "summary": "Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.", "AI": {"tldr": "\u63d0\u51faHistoMet\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u51b3\u7b56\u611f\u77e5\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\u4ece\u539f\u53d1\u80bf\u7624\u75c5\u7406\u56fe\u50cf\u9884\u6d4b\u8f6c\u79fb\u98ce\u9669\u548c\u8f6c\u79fb\u90e8\u4f4d\uff0c\u6574\u5408\u8bed\u8a00\u5b9a\u4e49\u7684\u6982\u5ff5\u63d0\u5347\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8f6c\u79fb\u662f\u764c\u75c7\u76f8\u5173\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f46\u76f4\u63a5\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u9884\u6d4b\u539f\u53d1\u80bf\u7624\u662f\u5426\u4f1a\u8f6c\u79fb\u4ee5\u53ca\u8f6c\u79fb\u90e8\u4f4d\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u901a\u5e38\u5c06\u8f6c\u79fb\u72b6\u6001\u6216\u90e8\u4f4d\u9884\u6d4b\u4f5c\u4e3a\u5b64\u7acb\u4efb\u52a1\u5904\u7406\uff0c\u6ca1\u6709\u660e\u786e\u6a21\u62df\u4e34\u5e8a\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u51b3\u7b56\u611f\u77e5\u3001\u6982\u5ff5\u5bf9\u9f50\u7684MIL\u6846\u67b6HistoMet\uff0c\u91c7\u7528\u4e24\u6a21\u5757\u9884\u6d4b\u6d41\u7a0b\uff1a\u9996\u5148\u4f30\u8ba1\u539f\u53d1\u80bf\u7624\u8f6c\u79fb\u8fdb\u5c55\u7684\u53ef\u80fd\u6027\uff0c\u7136\u540e\u5bf9\u9ad8\u98ce\u9669\u75c5\u4f8b\u8fdb\u884c\u8f6c\u79fb\u90e8\u4f4d\u7684\u6761\u4ef6\u9884\u6d4b\u3002\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u75c5\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6574\u5408\u8bed\u8a00\u5b9a\u4e49\u548c\u6570\u636e\u81ea\u9002\u5e94\u7684\u8f6c\u79fb\u6982\u5ff5\u6765\u6307\u5bfc\u8868\u793a\u5b66\u4e60\u3002", "result": "\u57286504\u540d\u60a3\u8005\u7684\u591a\u4e2a\u673a\u6784\u6cdb\u764c\u961f\u5217\u4e0a\u8bc4\u4f30\uff0c\u572895%\u7075\u654f\u5ea6\u7684\u9ad8\u7075\u654f\u5ea6\u7b5b\u67e5\u8bbe\u7f6e\u4e0b\uff0c\u663e\u8457\u51cf\u5c11\u4e0b\u6e38\u5de5\u4f5c\u91cf\u540c\u65f6\u4fdd\u6301\u9ad8\u8f6c\u79fb\u98ce\u9669\u53ec\u56de\u7387\u3002\u5bf9\u4e8e\u8f6c\u79fb\u75c5\u4f8b\uff0c\u83b7\u5f9774.6\u7684\u5b8f\u89c2F1\u5206\u6570\u548c92.1\u7684\u5b8f\u89c2\u4e00\u5bf9\u591aAUC\u3002", "conclusion": "\u660e\u786e\u6a21\u62df\u4e34\u5e8a\u51b3\u7b56\u7ed3\u6784\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u53d1\u80bf\u7624\u7ec4\u7ec7\u75c5\u7406\u5b66\u5b9e\u73b0\u7a33\u5065\u4e14\u53ef\u90e8\u7f72\u7684\u8f6c\u79fb\u8fdb\u5c55\u548c\u90e8\u4f4d\u503e\u5411\u6027\u9884\u540e\u9884\u6d4b\u3002"}}
{"id": "2602.08354", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08354", "abs": "https://arxiv.org/abs/2602.08354", "authors": ["Zixuan Huang", "Xin Xia", "Yuxi Ren", "Jianbin Zheng", "Xuanda Wang", "Zhixia Zhang", "Hongyan Xie", "Songshi Liang", "Zehao Chen", "Xuefeng Xiao", "Fuzhen Zhuang", "Jianxin Li", "Yikun Ban", "Deqing Wang"], "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?", "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSAGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u611f\u77e5\u5f15\u5bfc\u7684\u9ad8\u6548\u63a8\u7406\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\u4e2d\u957f\u601d\u7ef4\u94fe\u5bfc\u81f4\u7684\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u63a8\u7406\u6a21\u578b\u4f7f\u7528\u957f\u601d\u7ef4\u94fe\u8fdb\u884c\u590d\u6742\u63a8\u7406\u65f6\u4ea7\u751f\u5927\u91cf\u5197\u4f59\uff0c\u635f\u5bb3\u8ba1\u7b97\u6548\u7387\u5e76\u5bfc\u81f4\u5b9e\u65f6\u5e94\u7528\u5ef6\u8fdf\u3002\u7814\u7a76\u53d1\u73b0\u957f\u63a8\u7406\u94fe\u4e0e\u6b63\u786e\u6027\u65e0\u5173\u751a\u81f3\u6709\u5bb3\u51c6\u786e\u6027\uff0c\u4f46\u6a21\u578b\u9690\u542b\u77e5\u9053\u4f55\u65f6\u505c\u6b62\u601d\u8003\uff0c\u8fd9\u4e00\u80fd\u529b\u88ab\u5f53\u524d\u91c7\u6837\u8303\u5f0f\u6240\u63a9\u76d6\u3002", "method": "\u63d0\u51faSAGE\uff08\u81ea\u611f\u77e5\u5f15\u5bfc\u9ad8\u6548\u63a8\u7406\uff09\u91c7\u6837\u8303\u5f0f\uff0c\u91ca\u653e\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u6f5c\u529b\u3002\u8fdb\u4e00\u6b65\u5c06SAGE\u4f5c\u4e3a\u6df7\u5408\u91c7\u6837\u96c6\u6210\u5230\u57fa\u4e8e\u7fa4\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\uff08SAGE-RL\uff09\u4e2d\uff0c\u4f7fSAGE-RL\u80fd\u591f\u5c06SAGE\u53d1\u73b0\u7684\u9ad8\u6548\u63a8\u7406\u6a21\u5f0f\u878d\u5165\u6807\u51c6pass@1\u63a8\u7406\u3002", "result": "SAGE-RL\u663e\u8457\u63d0\u5347\u4e86\u5927\u63a8\u7406\u6a21\u578b\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "SAGE\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6a21\u578b\u9690\u542b\u7684\"\u77e5\u9053\u4f55\u65f6\u505c\u6b62\u601d\u8003\"\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u957f\u601d\u7ef4\u94fe\u7684\u5197\u4f59\u95ee\u9898\uff0c\u4e3a\u5927\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u63a8\u7406\u8303\u5f0f\u3002"}}
{"id": "2602.08362", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.08362", "abs": "https://arxiv.org/abs/2602.08362", "authors": ["Chunxi Ji", "Adnan Darwiche"], "title": "Circuit Representations of Random Forests with Applications to XAI", "comment": null, "summary": "We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.", "AI": {"tldr": "\u5c06\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u7f16\u8bd1\u4e3a\u7535\u8def\uff0c\u7528\u4e8e\u8ba1\u7b97\u51b3\u7b56\u7684\u5b8c\u6574\u539f\u56e0\u3001\u9c81\u68d2\u6027\u53ca\u6700\u77ed\u7ffb\u8f6c\u8def\u5f84", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06\u968f\u673a\u68ee\u6797\u7f16\u8bd1\u4e3a\u7535\u8def\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u4e14\u7f3a\u4e4f\u8ba1\u7b97\u51b3\u7b56\u539f\u56e0\u3001\u9c81\u68d2\u6027\u548c\u7ffb\u8f6c\u8def\u5f84\u7684\u7cfb\u7edf\u65b9\u6cd5", "method": "1) \u63d0\u51fa\u5c06\u968f\u673a\u68ee\u6797\u7f16\u8bd1\u4e3a\u7535\u8def\u7684\u9ad8\u6548\u65b9\u6cd5\uff1b2) \u5229\u7528\u8be5\u7535\u8def\u8ba1\u7b97\u51b3\u7b56\u7684\u5b8c\u6574\u548c\u4e00\u822c\u539f\u56e0\uff1b3) \u8bbe\u8ba1\u7b97\u6cd5\u8ba1\u7b97\u51b3\u7b56\u9c81\u68d2\u6027\u548c\u6700\u77ed\u7ffb\u8f6c\u8def\u5f84", "result": "\u63d0\u51fa\u7684\u7f16\u8bd1\u65b9\u6cd5\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u66f4\u9ad8\u6548\uff0c\u80fd\u591f\u679a\u4e3e\u5145\u5206\u539f\u56e0\u3001\u5fc5\u8981\u539f\u56e0\u3001\u5bf9\u6bd4\u89e3\u91ca\uff0c\u8ba1\u7b97\u51b3\u7b56\u9c81\u68d2\u6027\uff0c\u5e76\u8bc6\u522b\u6700\u77ed\u51b3\u7b56\u7ffb\u8f6c\u8def\u5f84", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u91ca\u751f\u6210\u548c\u51b3\u7b56\u5206\u6790\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027"}}
{"id": "2602.07616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07616", "abs": "https://arxiv.org/abs/2602.07616", "authors": ["Juntong Wu", "Jialiang Cheng", "Fuyu Lv", "Ou Dan", "Li Yuan"], "title": "SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models", "comment": "Published as a conference paper at ICLR 2026", "summary": "Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.", "AI": {"tldr": "SERE\u662f\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u4e13\u5bb6\u91cd\u8def\u7531\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347MoE\u6a21\u578b\u6279\u91cf\u89e3\u7801\u6548\u7387\uff0c\u901a\u8fc7\u52a8\u6001\u51cf\u5c11\u6d3b\u8dc3\u4e13\u5bb6\u6570\u91cf\u5b9e\u73b0\u6700\u9ad82\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "MoE\u6a21\u578b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u9700\u8981\u6279\u91cf\u63a8\u7406\u4ee5\u4f18\u5316\u786c\u4ef6\u6548\u7387\uff0c\u4f46\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e13\u5bb6\u8fc7\u5ea6\u6fc0\u6d3b\uff0c\u4ece\u800c\u51cf\u6162\u5185\u5b58\u53d7\u9650\u7684\u89e3\u7801\u9636\u6bb5\u3002\u9700\u8981\u89e3\u51b3\u6279\u91cf\u89e3\u7801\u4e0e\u4e13\u5bb6\u7a00\u758f\u6027\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002", "method": "SERE\u901a\u8fc7\u76f8\u4f3c\u6027\u5206\u6790\u52a8\u6001\u51cf\u5c11\u6d3b\u8dc3\u4e13\u5bb6\u6570\u91cf\uff0c\u5c06\u6b21\u8981\u4e13\u5bb6\u7684token\u91cd\u8def\u7531\u5230\u6700\u76f8\u4f3c\u7684\u4e3b\u8981\u4e13\u5bb6\uff0c\u540c\u65f6\u5229\u7528\u76f8\u4f3c\u6027\u6a21\u5f0f\u8bc6\u522b\u5e76\u4fdd\u7559\u5173\u952e\u4e13\u5bb6\uff0c\u907f\u514d\u9759\u6001\u4e13\u5bb6\u526a\u679d\u6216\u5408\u5e76\uff0c\u5b9e\u73b0\u57fa\u4e8e\u6279\u91cf\u7ea7\u4e13\u5bb6\u5197\u4f59\u7684\u52a8\u6001\u4e13\u5bb6\u8df3\u8fc7\u3002", "result": "\u5728\u5404\u79cd\u590d\u6742\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSERE\u5b9e\u73b0\u4e86\u6700\u9ad82.0\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u8d28\u91cf\u635f\u5931\u6700\u5c0f\uff0c\u4e3a\u5927\u89c4\u6a21MoE\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6210\u672c\u6548\u76ca\u548c\u5ef6\u8fdf\u654f\u611f\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SERE\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u91cd\u8def\u7531\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u6279\u91cf\u89e3\u7801\u7684\u6548\u7387\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301vLLM\u4e2d\u7684\u5355\u884c\u4ee3\u7801\u66f4\u6539\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2602.07643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07643", "abs": "https://arxiv.org/abs/2602.07643", "authors": ["Yichi Zhang", "Feiyang Xiao", "Le Xue", "Wenbo Zhang", "Gang Feng", "Chenguang Zheng", "Yuan Qi", "Yuan Cheng", "Zixin Hu"], "title": "Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation", "comment": null, "summary": "While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\\sim$675k 2D images, $\\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u5efaUMD\u6570\u636e\u96c6\uff08490\u4e2aPET/CT\u548c464\u4e2aPET/MRI\u5168\u8eab\u626b\u63cf\uff09\u8bc4\u4f303D\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u53d1\u73b0\u4ece\u7ed3\u6784\u6210\u50cf\u8f6c\u5411\u529f\u80fd\u6210\u50cf\u65f6\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u8fdc\u672a\u8fbe\u5230\u771f\u6b63\u7684\u901a\u7528\u6027\u3002", "motivation": "\u5f53\u524d3D\u533b\u5b66\u57fa\u7840\u6a21\u578b\u7684\u9a8c\u8bc1\u4e3b\u8981\u5c40\u9650\u4e8e\u533a\u57df\u6027\u548c\u7ed3\u6784\u6027\u6210\u50cf\uff0c\u5b58\u5728\u663e\u8457\u7684\u6a21\u6001\u5dee\u5f02\u672a\u88ab\u63a2\u7d22\u3002\u9700\u8981\u63d0\u4f9b\u4e25\u683c\u5ba2\u89c2\u7684\u8bc4\u4f30\u6765\u68c0\u9a8c\u8fd9\u4e9b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u521b\u5efaUMD\u6570\u636e\u96c6\u5305\u542b490\u4e2a\u5168\u8eabPET/CT\u548c464\u4e2a\u5168\u8eabPET/MRI\u626b\u63cf\uff08\u7ea6675k 2D\u56fe\u50cf\uff0c\u7ea612k 3D\u5668\u5b98\u6807\u6ce8\uff09\uff0c\u901a\u8fc7\u53d7\u8bd5\u8005\u5185\u5bf9\u7167\u6bd4\u8f83\u914d\u5bf9\u626b\u63cf\uff0c\u5c06\u6210\u50cf\u6a21\u6001\u4f5c\u4e3a\u4e3b\u8981\u81ea\u53d8\u91cf\uff0c\u5bf9\u4ee3\u8868\u60273D\u5206\u5272\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u6587\u732e\u62a5\u9053\u7684\u57fa\u51c6\u4e0e\u771f\u5b9e\u4e16\u754c\u6548\u679c\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u7279\u522b\u662f\u4ece\u7ed3\u6784\u57df\u8f6c\u5411\u529f\u80fd\u57df\u65f6\u3002\u8fd9\u79cd\u7cfb\u7edf\u6027\u5931\u8d25\u8868\u660e\u5f53\u524d3D\u57fa\u7840\u6a21\u578b\u8fdc\u672a\u8fbe\u5230\u771f\u6b63\u7684\u901a\u7528\u76ee\u7684\u72b6\u6001\u3002", "conclusion": "\u9700\u8981\u5411\u591a\u6a21\u6001\u8bad\u7ec3\u548c\u8bc4\u4f30\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ee5\u5f25\u5408\u7406\u60f3\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5168\u9762\u4e34\u5e8a\u6548\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8be5\u6570\u636e\u96c6\u548c\u5206\u6790\u4e3a\u672a\u6765\u5f00\u53d1\u771f\u6b63\u6a21\u6001\u65e0\u5173\u7684\u533b\u5b66\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u77f3\u3002"}}
{"id": "2602.08369", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08369", "abs": "https://arxiv.org/abs/2602.08369", "authors": ["Xin Zhang", "Kailai Yang", "Chenyue Li", "Hao Li", "Qiyu Wei", "Jun'ichi Tsujii", "Sophia Ananiadou"], "title": "MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval", "comment": null, "summary": "Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.", "AI": {"tldr": "MemAdapter\uff1a\u4e00\u4e2a\u7edf\u4e00\u7684\u8bb0\u5fc6\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u8de8\u8bb0\u5fc6\u8303\u5f0f\u7684\u5feb\u901f\u5bf9\u9f50\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u6210\u672c\u5e76\u63d0\u5347\u68c0\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u901a\u5e38\u8bbe\u8ba1\u5728\u5b64\u7acb\u7684\u8303\u5f0f\uff08\u5982\u663e\u5f0f\u3001\u53c2\u6570\u5316\u6216\u6f5c\u5728\u8bb0\u5fc6\uff09\u4e2d\uff0c\u68c0\u7d22\u65b9\u6cd5\u7d27\u5bc6\u8026\u5408\uff0c\u963b\u788d\u4e86\u8de8\u8303\u5f0f\u6cdb\u5316\u548c\u878d\u5408\u3002\u9700\u8981\u7edf\u4e00\u5f02\u6784\u8bb0\u5fc6\u8303\u5f0f", "method": "\u63d0\u51faMemAdapter\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u4ece\u7edf\u4e00\u8bb0\u5fc6\u7a7a\u95f4\u8bad\u7ec3\u751f\u6210\u5f0f\u5b50\u56fe\u68c0\u7d22\u5668\uff1b2\uff09\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u6a21\u5757\uff0c\u4f7f\u68c0\u7d22\u5668\u9002\u5e94\u672a\u89c1\u8bb0\u5fc6\u8303\u5f0f", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u8bc4\u4f30\u57fa\u51c6\u4e0a\uff0c\u751f\u6210\u5f0f\u5b50\u56fe\u68c0\u7d22\u5668\u5728\u4e09\u79cd\u8bb0\u5fc6\u8303\u5f0f\u548c\u667a\u80fd\u4f53\u6a21\u578b\u89c4\u6a21\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4e94\u79cd\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002\u8de8\u8303\u5f0f\u5bf9\u9f50\u4ec5\u970013\u5206\u949f\uff08\u5355GPU\uff09\uff0c\u4f7f\u7528\u4e0d\u52305%\u7684\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5373\u8d85\u8d8a\u539f\u59cb\u68c0\u7d22\u5668\u6027\u80fd", "conclusion": "MemAdapter\u5b9e\u73b0\u4e86\u5f02\u6784\u8bb0\u5fc6\u8303\u5f0f\u7684\u7edf\u4e00\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u6709\u6548\u7684\u96f6\u6837\u672c\u8de8\u8303\u5f0f\u878d\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bb0\u5fc6\u68c0\u7d22\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387"}}
{"id": "2602.08373", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08373", "abs": "https://arxiv.org/abs/2602.08373", "authors": ["Feiyu Wu", "Xu Zheng", "Yue Qu", "Zhuocheng Wang", "Zicheng Feng", "Hui Li"], "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI", "comment": "Accepted to ICLR 2026. Project page. https://openreview.net/forum?id=wb05ver1k8&noteId=v1Ax8CwI71", "summary": "Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.", "AI": {"tldr": "VIRF\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u5b9e\u73b0LLM\u89c4\u5212\u5668\u7684\u53ef\u9a8c\u8bc1\u5b89\u5168\u89c4\u5212\uff0c\u5c06\u88ab\u52a8\u5b89\u5168\u68c0\u67e5\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u534f\u4f5c\u4fee\u590d\uff0c\u5728\u5bb6\u5ead\u5b89\u5168\u4efb\u52a1\u4e2d\u5b9e\u73b0\u96f6\u5371\u9669\u884c\u52a8\u7387\u548c\u6700\u9ad8\u76ee\u6807\u5b8c\u6210\u7387\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u5177\u8eabAI\u89c4\u5212\u5668\u7f3a\u4e4f\u5f62\u5f0f\u5316\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u63d0\u4f9b\u4e25\u683c\u7684\u5b89\u5168\u4fdd\u8bc1\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684LLM\u8fdb\u884c\u5b89\u5168\u68c0\u67e5\uff0c\u8981\u4e48\u7b80\u5355\u5730\u62d2\u7edd\u4e0d\u5b89\u5168\u8ba1\u5212\u800c\u4e0d\u63d0\u4f9b\u4fee\u590d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53ef\u9a8c\u8bc1\u8fed\u4ee3\u7cbe\u70bc\u6846\u67b6(VIRF)\uff0c\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u5efa\u7acb\u5bfc\u5e08-\u5b66\u5f92\u5bf9\u8bdd\u673a\u5236\uff1a\u57fa\u4e8e\u5f62\u5f0f\u5316\u5b89\u5168\u672c\u4f53\u7684\u786e\u5b9a\u6027\u903b\u8f91\u5bfc\u5e08\u4e3aLLM\u89c4\u5212\u5668\u63d0\u4f9b\u56e0\u679c\u6027\u548c\u6559\u5b66\u6027\u53cd\u9988\uff0c\u5b9e\u73b0\u667a\u80fd\u8ba1\u5212\u4fee\u590d\u800c\u975e\u7b80\u5355\u907f\u514d\u3002\u540c\u65f6\u5f15\u5165\u53ef\u6269\u5c55\u7684\u77e5\u8bc6\u83b7\u53d6\u7ba1\u9053\uff0c\u4ece\u73b0\u5b9e\u4e16\u754c\u6587\u6863\u5408\u6210\u5b89\u5168\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5bb6\u5ead\u5b89\u5168\u4efb\u52a1\u4e2d\uff0cVIRF\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u76840%\u5371\u9669\u884c\u52a8\u7387(HAR)\u548c77.3%\u7684\u76ee\u6807\u6761\u4ef6\u7387(GCR)\uff0c\u5728\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u6700\u9ad8\u3002\u5e73\u5747\u4ec5\u97001.1\u6b21\u4fee\u6b63\u8fed\u4ee3\uff0c\u6548\u7387\u6781\u9ad8\u3002", "conclusion": "VIRF\u5c55\u793a\u4e86\u4e00\u6761\u6784\u5efa\u6839\u672c\u4e0a\u53ef\u4fe1\u4e14\u53ef\u9a8c\u8bc1\u5b89\u5168\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7684\u539f\u5219\u6027\u9014\u5f84\uff0c\u901a\u8fc7\u4e3b\u52a8\u534f\u4f5c\u4fee\u590d\u800c\u975e\u88ab\u52a8\u5b89\u5168\u68c0\u67e5\uff0c\u5b9e\u73b0\u4e86LLM\u89c4\u5212\u5668\u7684\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\u3002"}}
{"id": "2602.07640", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07640", "abs": "https://arxiv.org/abs/2602.07640", "authors": ["Micha\u0142 Kozyra", "Gesine Reinert"], "title": "TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators", "comment": null, "summary": "Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.", "AI": {"tldr": "\u63d0\u51faTASTE\u6846\u67b6\uff0c\u57fa\u4e8eStein\u7b97\u5b50\u5c06\u5206\u5e03\u504f\u79fb\u4e0e\u6a21\u578b\u8f93\u5165\u654f\u611f\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u5b9e\u73b0\u4efb\u52a1\u611f\u77e5\u7684OOD\u68c0\u6d4b\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u50cf\u7d20\u7ea7\u8bca\u65ad", "motivation": "\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u8981\u4e48\u662f\u6570\u636e\u4e2d\u5fc3\u7684\uff08\u4ec5\u68c0\u6d4b\u8bad\u7ec3\u8f93\u5165\u5206\u5e03\u7684\u504f\u5dee\uff09\uff0c\u8981\u4e48\u662f\u6a21\u578b\u4e2d\u5fc3\u7684\uff08\u4f9d\u8d56\u5206\u7c7b\u5668\u8f93\u51fa\u800c\u4e0d\u8003\u8651\u6570\u636e\u51e0\u4f55\u7ed3\u6784\uff09\uff0c\u7f3a\u4e4f\u5c06\u5206\u5e03\u504f\u79fb\u4e0e\u6a21\u578b\u4efb\u52a1\u6027\u80fd\u8054\u7cfb\u8d77\u6765\u7684\u4efb\u52a1\u611f\u77e5\u65b9\u6cd5", "method": "\u63d0\u51faTASTE\u6846\u67b6\uff0c\u57fa\u4e8eStein\u7b97\u5b50\u5c06\u5206\u5e03\u504f\u79fb\u4e0e\u6a21\u578b\u8f93\u5165\u654f\u611f\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u8be5\u7b97\u5b50\u53ef\u89e3\u91ca\u4e3a\u5206\u5e03\u504f\u79fb\u5728\u6a21\u578b\u654f\u611f\u6027\u573a\u4e0a\u7684\u6295\u5f71\uff0c\u652f\u6301\u5750\u6807\u5206\u89e3\u5b9e\u73b0\u504f\u79fb\u5b9a\u4f4d\uff0c\u5bf9\u56fe\u50cf\u6570\u636e\u63d0\u4f9b\u50cf\u7d20\u7ea7\u8bca\u65ad", "result": "\u5728\u53d7\u63a7\u9ad8\u65af\u504f\u79fb\u3001MNIST\u51e0\u4f55\u6270\u52a8\u548cCIFAR-10\u6270\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0e\u4efb\u52a1\u9000\u5316\u7d27\u5bc6\u5bf9\u9f50\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "TASTE\u6846\u67b6\u901a\u8fc7Stein\u7b97\u5b50\u5c06\u5206\u5e03\u504f\u79fb\u4e0e\u6a21\u578b\u654f\u611f\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u4e0d\u4ec5\u68c0\u6d4b\u504f\u79fb\u5b58\u5728\uff0c\u8fd8\u80fd\u5b9a\u4f4d\u504f\u79fb\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u8bca\u65ad\uff0c\u5728\u4efb\u52a1\u611f\u77e5\u7684OOD\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02"}}
{"id": "2602.07658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07658", "abs": "https://arxiv.org/abs/2602.07658", "authors": ["Avinash Kumar K M", "Samarth S. Raut"], "title": "Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation", "comment": "22 pages, 13 figures", "summary": "The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u533b\u5b66\u5f71\u50cf\u91cd\u5efa\u6d41\u7a0b\u4e2d\u7684\u8bef\u5dee\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u5206\u5272\u7b97\u6cd5\u548c\u51e0\u4f55\u7c7b\u578b\u5728\u4f53\u7d20\u548c\u8868\u9762\u7cbe\u5ea6\u6307\u6807\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0Otsu\u65b9\u6cd5\u6700\u9002\u7528\u4e8e\u5404\u79cd\u51e0\u4f55\u5f62\u72b6\uff0cJaccard\u6307\u6570\u6bd4Dice\u66f4\u9002\u5408\u8584\u58c1\u7ed3\u6784\u7684\u7cbe\u5ea6\u8bc4\u4f30\u3002", "motivation": "\u533b\u5b66\u626b\u63cf\u521b\u5efa3D\u6a21\u578b\u7684\u7cbe\u5ea6\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u5305\u62ec\u6210\u50cf\u786c\u4ef6\u3001\u5206\u5272\u65b9\u6cd5\u548c\u7f51\u683c\u5904\u7406\u6280\u672f\u7b49\u3002\u51e0\u4f55\u7c7b\u578b\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u4f53\u7d20\u548c\u70b9\u4e91\u5bf9\u9f50\u5bf9\u7cbe\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u91cd\u5efa\u6d41\u7a0b\u4e2d\u7684\u8bef\u5dee\u3002", "method": "\u4f7f\u7528SLA\u6280\u672f\u6253\u5370\u7403\u4f53\u3001\u9762\u7f69\u548cAAA\uff08\u8179\u4e3b\u52a8\u8109\u7624\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u5faeCT\u626b\u63cf\u83b7\u53d6\u6570\u636e\u3002\u91c7\u7528GMM\u3001Otsu\u548cRG\u4e09\u79cd\u5206\u5272\u65b9\u6cd5\u8fdb\u884c\u5904\u7406\u3002\u5c06\u5206\u5272\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\u4f7f\u7528KU\u7b97\u6cd5\u5bf9\u9f50\uff0c\u5b9a\u91cf\u6bd4\u8f83Dice\u3001Jaccard\u5206\u6570\u548c\u7cbe\u5ea6\u7b49\u6307\u6807\u3002\u8868\u9762\u7f51\u683c\u4f7f\u7528ICP\u914d\u51c6\uff0c\u8bc4\u4f30Chamfer\u8ddd\u79bb\u548c\u5e73\u5747Hausdorff\u8ddd\u79bb\u3002", "result": "Otsu\u65b9\u6cd5\u5bf9\u6240\u6709\u51e0\u4f55\u5f62\u72b6\u90fd\u6700\u9002\u7528\u3002AAA\u7531\u4e8e\u58c1\u8584\u548c\u5bf9\u9f50\u95ee\u9898\u5bfc\u81f4\u91cd\u53e0\u5206\u6570\u8f83\u4f4e\u3002\u7c7b\u522b\u4e0d\u5e73\u8861\u5bf9AAA\u7684\u7279\u5f02\u6027\u5f71\u54cd\u6700\u5927\u3002\u8868\u9762\u7cbe\u5ea6\u6307\u6807\u4e0e\u4f53\u7d20\u6307\u6807\u8d8b\u52bf\u4e0d\u540c\uff1aRG\u65b9\u6cd5\u5bf9\u7403\u4f53\u8868\u73b0\u6700\u597d\uff0c\u800cGMM\u548cOtsu\u5bf9AAA\u8868\u73b0\u66f4\u597d\u3002\u9762\u7f69\u8868\u9762\u8bef\u5dee\u6700\u5927\uff0c\u53ef\u80fd\u662fICP\u914d\u51c6\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\u6240\u81f4\u3002", "conclusion": "\u5206\u5272\u7cbe\u5ea6\u662f\u91cd\u5efa\u8fc7\u7a0b\u5404\u9636\u6bb5\u8bef\u5dee\u7684\u7d2f\u79ef\u603b\u548c\u3002\u5728\u9ad8\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u5bf9\u9f50\u654f\u611f\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u4f53\u7d20\u7cbe\u5ea6\u6307\u6807\u53ef\u80fd\u5177\u6709\u8bef\u5bfc\u6027\u3002Jaccard\u6307\u6570\u6bd4Dice\u66f4\u4e25\u683c\uff0c\u66f4\u9002\u5408\u8584\u58c1\u7ed3\u6784\u7684\u7cbe\u5ea6\u8bc4\u4f30\u3002\u4e3a\u786e\u4fdd\u91cd\u5efa\u6d41\u7a0b\u7684\u53ef\u9760\u8bc4\u4f30\uff0c\u5fc5\u987b\u786e\u4fdd\u4f53\u7d20\u548c\u70b9\u4e91\u5bf9\u9f50\u3002"}}
{"id": "2602.07659", "categories": ["cs.LG", "cs.AI", "q-fin.ST"], "pdf": "https://arxiv.org/pdf/2602.07659", "abs": "https://arxiv.org/abs/2602.07659", "authors": ["Matthew Siper", "Muhammad Umair Nasir", "Ahmed Khalifa", "Lisa Soros", "Jay Azhang", "Julian Togelius"], "title": "Continuous Program Search", "comment": null, "summary": "Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.\n  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.\n  Under identical $(\u03bc+\u03bb)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5b66\u4e60\u8fde\u7eed\u7a0b\u5e8f\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u7f16\u8bd1\u53d8\u5f02\u7b97\u5b50\u63d0\u5347\u9057\u4f20\u7f16\u7a0b\u7684\u641c\u7d22\u6548\u7387\uff0c\u5728\u4ea4\u6613\u7b56\u7565\u4f18\u5316\u4e2d\u5b9e\u73b0\u6570\u91cf\u7ea7\u8bc4\u4f30\u6b21\u6570\u51cf\u5c11\u548c\u66f4\u9ad8\u6837\u672c\u5916\u590f\u666e\u6bd4\u7387\u3002", "motivation": "\u9057\u4f20\u7f16\u7a0b\u867d\u7136\u80fd\u4ea7\u751f\u53ef\u89e3\u91ca\u7a0b\u5e8f\uff0c\u4f46\u5c0f\u7684\u8bed\u6cd5\u53d8\u5f02\u53ef\u80fd\u5bfc\u81f4\u5927\u7684\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u964d\u4f4e\u5c40\u90e8\u6027\u548c\u6837\u672c\u6548\u7387\u3002\u8fd9\u88ab\u6846\u5b9a\u4e3a\u7b97\u5b50\u8bbe\u8ba1\u95ee\u9898\uff1a\u5b66\u4e60\u4e00\u4e2a\u8fde\u7eed\u7a0b\u5e8f\u7a7a\u95f4\uff0c\u5176\u4e2d\u6f5c\u5728\u8ddd\u79bb\u5177\u6709\u884c\u4e3a\u610f\u4e49\uff0c\u7136\u540e\u8bbe\u8ba1\u5229\u7528\u8fd9\u79cd\u7ed3\u6784\u800c\u4e0d\u6539\u53d8\u8fdb\u5316\u4f18\u5316\u5668\u7684\u53d8\u5f02\u7b97\u5b50\u3002", "method": "\u901a\u8fc7\u8ddf\u8e2a\u53d7\u63a7\u6f5c\u5728\u6270\u52a8\u4e0b\u7684\u52a8\u4f5c\u7ea7\u5206\u6b67\u6765\u6d4b\u91cf\u5c40\u90e8\u6027\uff0c\u786e\u5b9a\u884c\u4e3a\u5c40\u90e8\u8fde\u7eed\u53d8\u5316\u7684\u7ecf\u9a8c\u4fe1\u4efb\u533a\u57df\u3002\u4f7f\u7528\u5305\u542b\u56db\u4e2a\u8bed\u4e49\u7ec4\u4ef6\uff08\u591a\u5934/\u7a7a\u5934\u5165\u573a\u548c\u51fa\u573a\uff09\u7684\u7d27\u51d1\u4ea4\u6613\u7b56\u7565DSL\uff0c\u5b66\u4e60\u5339\u914d\u7684\u5757\u56e0\u5b50\u5316\u5d4c\u5165\uff0c\u5e76\u6bd4\u8f83\u5168\u6f5c\u5728\u7a7a\u95f4\u4e0a\u7684\u5404\u5411\u540c\u6027\u9ad8\u65af\u53d8\u5f02\u4e0e\u51e0\u4f55\u7f16\u8bd1\u53d8\u5f02\uff08\u540e\u8005\u5c06\u66f4\u65b0\u9650\u5236\u5728\u8bed\u4e49\u914d\u5bf9\u7684\u5165\u573a-\u51fa\u573a\u5b50\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5b66\u4e60\u6d41\u6a21\u578b\u7684\u65b9\u5411\u5efa\u8bae\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u8d44\u4ea7\u4e0a\u4f7f\u7528\u76f8\u540c\u7684(\u03bc+\u03bb)\u8fdb\u5316\u7b56\u7565\u548c\u56fa\u5b9a\u8bc4\u4f30\u9884\u7b97\u4e0b\uff0c\u5b66\u4e60\u5230\u7684\u53d8\u5f02\u7b97\u5b50\u4f7f\u7528\u6570\u91cf\u7ea7\u66f4\u5c11\u7684\u8bc4\u4f30\u53d1\u73b0\u5f3a\u7b56\u7565\uff0c\u5e76\u5b9e\u73b0\u6700\u9ad8\u7684\u4e2d\u4f4d\u6570\u6837\u672c\u5916\u590f\u666e\u6bd4\u7387\u3002\u867d\u7136\u5404\u5411\u540c\u6027\u53d8\u5f02\u5076\u5c14\u80fd\u8fbe\u5230\u66f4\u9ad8\u7684\u5cf0\u503c\u6027\u80fd\uff0c\u4f46\u51e0\u4f55\u7f16\u8bd1\u53d8\u5f02\u4ea7\u751f\u66f4\u5feb\u3001\u66f4\u53ef\u9760\u7684\u8fdb\u5c55\u3002", "conclusion": "\u8bed\u4e49\u5bf9\u9f50\u7684\u53d8\u5f02\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff0c\u800c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u8fdb\u5316\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53d8\u5f02\u7b97\u5b50\u5229\u7528\u7a0b\u5e8f\u8bed\u4e49\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.08401", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08401", "abs": "https://arxiv.org/abs/2602.08401", "authors": ["Liwen Wang", "Zongjie Li", "Yuchong Xie", "Shuai Wang", "Dongdong She", "Wei Wang", "Juergen Rahmel"], "title": "On Protecting Agentic Systems' Intellectual Property via Watermarking", "comment": null, "summary": "The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.", "AI": {"tldr": "AGENTWM\u662f\u9996\u4e2a\u4e13\u4e3a\u667a\u80fd\u4f53\u6a21\u578b\u8bbe\u8ba1\u7684\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u529f\u80fd\u76f8\u540c\u5de5\u5177\u6267\u884c\u8def\u5f84\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u5728\u53ef\u89c1\u52a8\u4f5c\u8f68\u8ff9\u4e2d\u5d4c\u5165\u53ef\u9a8c\u8bc1\u6c34\u5370\uff0c\u6709\u6548\u4fdd\u62a4\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u77e5\u8bc6\u4ea7\u6743\u514d\u53d7\u6a21\u4eff\u653b\u51fb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u4e3a\u80fd\u591f\u81ea\u4e3b\u63a8\u7406\u548c\u4f7f\u7528\u5de5\u5177\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u521b\u9020\u4e86\u91cd\u8981\u7684\u77e5\u8bc6\u4ea7\u6743\u4ef7\u503c\u3002\u8fd9\u4e9b\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u6a21\u4eff\u653b\u51fb\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u6a21\u4eff\u6a21\u578b\u7a83\u53d6\u4e13\u6709\u80fd\u529b\u3002\u73b0\u6709\u7684LLM\u6c34\u5370\u6280\u672f\u5728\u6b64\u9886\u57df\u5931\u6548\uff0c\u56e0\u4e3a\u73b0\u5b9e\u4e2d\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f5c\u4e3a\u7070\u76d2\u8fd0\u884c\uff0c\u9690\u85cf\u4e86\u9a8c\u8bc1\u6240\u9700\u7684\u5185\u90e8\u5206\u6790\u75d5\u8ff9\u3002", "method": "AGENTWM\u5229\u7528\u52a8\u4f5c\u5e8f\u5217\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u901a\u8fc7\u5fae\u5999\u5730\u504f\u5411\u529f\u80fd\u76f8\u540c\u5de5\u5177\u6267\u884c\u8def\u5f84\u7684\u5206\u5e03\u6765\u6ce8\u5165\u6c34\u5370\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u5728\u53ef\u89c1\u52a8\u4f5c\u8f68\u8ff9\u4e2d\u76f4\u63a5\u5d4c\u5165\u53ef\u9a8c\u8bc1\u4fe1\u53f7\uff0c\u540c\u65f6\u5bf9\u7528\u6237\u4e0d\u53ef\u533a\u5206\u3002\u5f00\u53d1\u4e86\u81ea\u52a8\u6d41\u6c34\u7ebf\u6765\u751f\u6210\u9c81\u68d2\u7684\u6c34\u5370\u65b9\u6848\uff0c\u4ee5\u53ca\u4e25\u683c\u7684\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u7a0b\u5e8f\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u9886\u57df\u8fdb\u884c\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cAGENTWM\u5b9e\u73b0\u4e86\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002\u7ed3\u679c\u8bc1\u5b9eAGENTWM\u80fd\u6709\u6548\u4fdd\u62a4\u667a\u80fd\u4f53\u77e5\u8bc6\u4ea7\u6743\uff0c\u5bf9\u6297\u81ea\u9002\u5e94\u653b\u51fb\u8005\uff0c\u653b\u51fb\u8005\u65e0\u6cd5\u5728\u4e0d\u4e25\u91cd\u964d\u4f4e\u88ab\u76d7\u6a21\u578b\u5b9e\u7528\u6027\u7684\u60c5\u51b5\u4e0b\u79fb\u9664\u6c34\u5370\u3002", "conclusion": "AGENTWM\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u4f53\u6a21\u578b\u8bbe\u8ba1\u7684\u6c34\u5370\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709LLM\u6c34\u5370\u6280\u672f\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4fdd\u62a4\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u667a\u80fd\u4f53\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07670", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07670", "abs": "https://arxiv.org/abs/2602.07670", "authors": ["Jarrod Barnes"], "title": "Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation", "comment": "13 pages, 7 figures, 11 tables. Preprint. Code: https://github.com/jbarnes850/test-time-training", "summary": "Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's \"equivalent K\" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u53ef\u9a8c\u8bc1\u6267\u884c\u57fa\u7840\u4efb\u52a1\u4e2d\uff0c\u641c\u7d22\u7b56\u7565\u4f18\u4e8e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0c\u6700\u4f73N\u91c7\u6837\u8fbe\u523090%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u800c\u68af\u5ea6\u9002\u5e94\u4ec530.6%\u3002\u5173\u952e\u53d1\u73b0\u662f\u60ca\u5947\u5f15\u5bfc\u9009\u62e9\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u662f\u5426\u662f\u53ef\u9a8c\u8bc1\u6267\u884c\u57fa\u7840\u4efb\u52a1\u7684\u6700\u4f73\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728GPU\u5185\u6838\u4f18\u5316\u7b49\u5177\u6709\u5bc6\u96c6\u8fde\u7eed\u5956\u52b1\u4fe1\u53f7\u7684\u9886\u57df\uff0c\u63a2\u7d22\u8ba1\u7b97\u6700\u4f18\u7684\u6d4b\u8bd5\u65f6\u7b56\u7565\u3002", "method": "\u4f7f\u7528KernelBench\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\u548c120B\u53c2\u6570\u6a21\u578b\uff0c\u6bd4\u8f83\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4e0e\u641c\u7d22\u7b56\u7565\u3002\u63d0\u51fa\u60ca\u5947\u5f15\u5bfc\u9009\u62e9\u7b56\u7565\uff1a\u9009\u62e9\u6700\u9ad8\u60ca\u5947\u503c\uff08\u6700\u4f4e\u7f6e\u4fe1\u5ea6\uff09\u7684\u6b63\u786e\u6837\u672c\uff0c\u5e76\u6269\u5c55\u5230\u60ca\u5947\u5f15\u5bfc\u524d3\u9009\u62e9\u3002", "result": "\u6700\u4f73N\u91c7\u6837\u5728K=64\u65f6\u8fbe\u523090%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u800c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u4ec530.6%\u3002\u60ca\u5947\u5f15\u5bfc\u9009\u62e9\u6bd4\u6700\u7f6e\u4fe1\u9009\u62e9\u63d0\u534730%\u6210\u529f\u7387\uff0c\u60ca\u5947\u5f15\u5bfc\u524d3\u9009\u62e9\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u5339\u914doracle\u6027\u80fd\u3002", "conclusion": "\u5bf9\u4e8e\u5bc6\u96c6\u5956\u52b1\u7684\u53ef\u9a8c\u8bc1\u6267\u884c\u57fa\u7840\u4efb\u52a1\uff0c\u8ba1\u7b97\u8d44\u6e90\u5e94\u5206\u914d\u7ed9\u6837\u672c\u591a\u6837\u6027\u548c\u667a\u80fd\u9009\u62e9\u800c\u975e\u68af\u5ea6\u9002\u5e94\u3002\u60ca\u5947\u5f15\u5bfc\u9009\u62e9\u539f\u5219\u53ef\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u6267\u884c\u57fa\u7840\u9886\u57df\uff0c\u5176\u4e2d\u6700\u4f18\u89e3\u4f4d\u4e8e\u5206\u5e03\u5c3e\u90e8\u3002"}}
{"id": "2602.07680", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.07680", "abs": "https://arxiv.org/abs/2602.07680", "authors": ["Ross Greer", "Maitrayee Keskar", "Angel Martinez-Sanchez", "Parthib Roy", "Shashank Shriram", "Mohan Trivedi"], "title": "Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning", "comment": null, "summary": "Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\u548c\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4e09\u4e2a\u7cfb\u7edf\u7ea7\u7528\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u8bed\u4e49\u8868\u793a\u8fdb\u884c\u5371\u9669\u7b5b\u67e5\u3001\u8f68\u8ff9\u89c4\u5212\u548c\u884c\u4e3a\u7ea6\u675f\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5c06\u89c6\u89c9\u89c2\u5bdf\u4e0e\u81ea\u7136\u8bed\u8a00\u6982\u5ff5\u5bf9\u9f50\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8bed\u4e49\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5c06\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u96c6\u6210\u5230\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u652f\u6301\u9a7e\u9a76\u573a\u666f\u5b89\u5168\u8bc4\u4f30\u548c\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e09\u4e2a\u4e92\u8865\u7684\u7cfb\u7edf\u7ea7\u7528\u4f8b\uff1a1\uff09\u57fa\u4e8eCLIP\u56fe\u50cf-\u6587\u672c\u76f8\u4f3c\u6027\u7684\u8f7b\u91cf\u7ea7\u3001\u7c7b\u522b\u65e0\u5173\u7684\u5371\u9669\u7b5b\u67e5\u65b9\u6cd5\uff1b2\uff09\u5c06\u573a\u666f\u7ea7\u89c6\u89c9\u8bed\u8a00\u5d4c\u5165\u96c6\u6210\u5230\u57fa\u4e8eTransformer\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u4e2d\uff1b3\uff09\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u8fd0\u52a8\u89c4\u5212\u7684\u663e\u5f0f\u884c\u4e3a\u7ea6\u675f\u3002", "result": "1\uff09\u5371\u9669\u7b5b\u67e5\u65b9\u6cd5\u80fd\u591f\u4f4e\u5ef6\u8fdf\u68c0\u6d4b\u591a\u6837\u5316\u548c\u5206\u5e03\u5916\u7684\u9053\u8def\u5371\u9669\uff1b2\uff09\u7b80\u5355\u5730\u5c06\u5168\u5c40\u5d4c\u5165\u6761\u4ef6\u5316\u5230\u89c4\u5212\u5668\u4e2d\u5e76\u4e0d\u80fd\u63d0\u9ad8\u8f68\u8ff9\u7cbe\u5ea6\uff1b3\uff09\u57fa\u4e8e\u89c6\u89c9\u573a\u666f\u5143\u7d20\u7684\u4e58\u5ba2\u5f0f\u6307\u4ee4\u80fd\u591f\u6291\u5236\u7f55\u89c1\u4f46\u4e25\u91cd\u7684\u89c4\u5212\u5931\u8d25\uff0c\u5728\u6a21\u7cca\u573a\u666f\u4e2d\u6539\u5584\u5b89\u5168\u5bf9\u9f50\u884c\u4e3a\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u8868\u793a\u5728\u8868\u8fbe\u8bed\u4e49\u98ce\u9669\u3001\u610f\u56fe\u548c\u884c\u4e3a\u7ea6\u675f\u65b9\u9762\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u5177\u6709\u91cd\u8981\u6f5c\u529b\uff0c\u4f46\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5de5\u7a0b\u95ee\u9898\uff0c\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7cfb\u7edf\u67b6\u6784\u548c\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u7684\u7279\u5f81\u6ce8\u5165\u3002"}}
{"id": "2602.07671", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07671", "abs": "https://arxiv.org/abs/2602.07671", "authors": ["Mohan Li", "Dario Fenoglio", "Martin Gjoreski", "Marc Langheinrich"], "title": "Federated Learning with Profile Mapping under Distribution Shifts and Drifts", "comment": "ICLR2026", "summary": "Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.", "AI": {"tldr": "Feroma\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u5206\u5e03\u914d\u7f6e\u6587\u4ef6\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u548c\u6f02\u79fb\uff0c\u65e0\u9700\u5ba2\u6237\u7aef\u8eab\u4efd\u6216\u96c6\u7fa4\u4fe1\u606f\uff0c\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u805a\u5408\u548c\u6d4b\u8bd5\u65f6\u6a21\u578b\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5f02\u6784\u6027\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8de8\u5ba2\u6237\u7aef\u7684\u5206\u5e03\u504f\u79fb\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5206\u5e03\u6f02\u79fb\uff0c\u4e14\u4f9d\u8d56\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\uff08\u5982\u5df2\u77e5\u5ba2\u6237\u7aef\u96c6\u7fa4\u6570\u91cf\u548c\u6570\u636e\u5f02\u6784\u7c7b\u578b\uff09\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "Feroma\u57fa\u4e8e\u5ba2\u6237\u7aef\u5206\u5e03\u914d\u7f6e\u6587\u4ef6\uff08\u7d27\u51d1\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u672c\u5730\u6570\u636e\u8868\u793a\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u76f8\u4f3c\u6027\u52a0\u6743\u6307\u5bfc\u6a21\u578b\u805a\u5408\u548c\u6d4b\u8bd5\u65f6\u6a21\u578b\u5206\u914d\u3002\u8be5\u8bbe\u8ba1\u5141\u8bb8\u52a8\u6001\u9009\u62e9\u805a\u5408\u7b56\u7565\uff08\u4ece\u96c6\u7fa4\u5316\u5230\u4e2a\u6027\u5316\uff09\uff0c\u5e76\u4e3a\u672a\u89c1\u3001\u672a\u6807\u8bb0\u7684\u6d4b\u8bd5\u5ba2\u6237\u7aef\u90e8\u7f72\u5408\u9002\u6a21\u578b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3001\u5728\u7ebf\u9002\u5e94\u6216\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u4e0e10\u4e2a\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cFeroma\u5728\u52a8\u6001\u6570\u636e\u5f02\u6784\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff1a\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u6700\u591a12\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eFedAvg\u76f8\u5f53\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u57fa\u4e8e\u5206\u5e03\u914d\u7f6e\u6587\u4ef6\u7684\u805a\u5408\u4e3a\u5728\u6570\u636e\u5206\u5e03\u504f\u79fb\u548c\u6f02\u79fb\u4e0b\u5b9e\u73b0\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0cFeroma\u6846\u67b6\u5728\u65e0\u9700\u5ba2\u6237\u7aef\u8eab\u4efd\u6216\u96c6\u7fa4\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5904\u7406\u52a8\u6001\u6570\u636e\u5f02\u6784\u6027\u3002"}}
{"id": "2602.07689", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07689", "abs": "https://arxiv.org/abs/2602.07689", "authors": ["Jusheng Zhang", "Kaitong Cai", "Jian Wang", "Yongsen Zheng", "Kwok-Yan Lam", "Keze Wang"], "title": "Process-of-Thought Reasoning for Videos", "comment": null, "summary": "Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.", "AI": {"tldr": "PoT Reasoning for Videos\uff1a\u4e00\u4e2a\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u6b65\u9aa4\u63d0\u5347\u89c6\u9891\u7406\u89e3\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6846\u67b6", "motivation": "\u89c6\u9891\u7406\u89e3\u9700\u8981\u5904\u7406\u957f\u800c\u5608\u6742\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u8fdb\u884c\u65f6\u95f4\u951a\u5b9a\u7684\u591a\u6b65\u63a8\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u96be\u4ee5\u9a8c\u8bc1\u548c\u8bca\u65ad\u3002", "method": "\u63d0\u51faProcess-of-Thought (PoT)\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u63a8\u7406\u5206\u89e3\u4e3a\u4e09\u4e2a\u4ea4\u66ff\u6b65\u9aa4\uff1a\u65f6\u95f4\u8bc1\u636e\u9009\u62e9\u3001\u9010\u6b65\u72b6\u6001\u66f4\u65b0\u3001\u7ea6\u675f\u7b54\u6848\u5408\u6210\u3002\u91c7\u7528\u7edf\u4e00\u8868\u793a\u5bf9\u9f50\u4e2d\u95f4\u51b3\u7b56\u4e0e\u65f6\u95f4\u7247\u6bb5\u3002", "result": "\u5728\u6807\u51c6\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPoT\u80fd\u6301\u7eed\u63d0\u5347\u4e8b\u5b9e\u6b63\u786e\u6027\u548c\u65f6\u95f4\u951a\u5b9a\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9\u7528\u4e8e\u8bca\u65ad\u3002", "conclusion": "PoT\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u589e\u5f3a\u4e86\u89c6\u9891\u7406\u89e3\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u652f\u6301\u6a21\u578b\u65e0\u5173\u7684\u96c6\u6210\u548c\u5916\u90e8\u5de5\u5177\u589e\u5f3a\u3002"}}
{"id": "2602.07674", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07674", "abs": "https://arxiv.org/abs/2602.07674", "authors": ["Bohdan Turbal", "Iryna Voitsitska", "Lesia Semenova"], "title": "ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets", "comment": null, "summary": "Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.", "AI": {"tldr": "ElliCE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u9c81\u68d2\u7b97\u6cd5\u8ffd\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316Rashomon\u96c6\u5408\u7684\u692d\u7403\u8fd1\u4f3c\u4e0a\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u786e\u4fdd\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53ef\u9760\u8ffd\u7d22\u5efa\u8bae\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f4\u63a5\u5f71\u54cd\u4eba\u4eec\u7684\u751f\u6d3b\u51b3\u7b56\uff0c\u9700\u8981\u7406\u89e3\u5982\u4f55\u901a\u8fc7\u6539\u53d8\u8f93\u5165\u83b7\u5f97\u66f4\u597d\u7ed3\u679c\u3002\u5f53Rashomon\u96c6\u5408\uff08\u8fd1\u6700\u4f18\u6a21\u578b\u96c6\u5408\uff09\u5f88\u5927\u65f6\uff0c\u6807\u51c6\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u53ef\u80fd\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u5bf9\u4e00\u4e2a\u6a21\u578b\u6709\u6548\u7684\u8ffd\u7d22\u884c\u52a8\u53ef\u80fd\u5bf9\u53e6\u4e00\u4e2a\u6a21\u578b\u65e0\u6548\u3002", "method": "\u5f15\u5165ElliCE\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316Rashomon\u96c6\u5408\u7684\u692d\u7403\u8fd1\u4f3c\u4e0a\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u786e\u4fdd\u8ffd\u7d22\u884c\u52a8\u5728\u8be5\u692d\u7403\u8303\u56f4\u5185\u6709\u6548\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u5305\u62ec\u552f\u4e00\u6027\u3001\u7a33\u5b9a\u6027\u548c\u4e0e\u5173\u952e\u7279\u5f81\u65b9\u5411\u7684\u5bf9\u9f50\u3002", "result": "ElliCE\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e0d\u4ec5\u66f4\u9c81\u68d2\uff0c\u800c\u4e14\u66f4\u7075\u6d3b\uff0c\u80fd\u591f\u9002\u5e94\u7528\u6237\u6307\u5b9a\u7684\u7279\u5f81\u7ea6\u675f\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ElliCE\u4e3a\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53ef\u9760\u8ffd\u7d22\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u5373\u4f7f\u7528\u6237\u6a21\u578b\u6f14\u5316\uff0c\u4e5f\u80fd\u63d0\u4f9b\u7a33\u5b9a\u7684\u63a8\u8350\u5efa\u8bae\u3002"}}
{"id": "2602.08517", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.08517", "abs": "https://arxiv.org/abs/2602.08517", "authors": ["Shaoang Zhang", "Yazhe Niu"], "title": "TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor", "comment": null, "summary": "Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.", "AI": {"tldr": "TreeTensor\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5d4c\u5957\u6570\u636e\u7ed3\u6784\u5bb9\u5668\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u7684\u590d\u6742AI\u6570\u636e\uff0c\u652f\u6301\u96f6\u6210\u672c\u5e94\u7528\u5404\u79cd\u51fd\u6570\u548c\u64cd\u4f5c\uff0c\u5e76\u4e0e\u4e3b\u6d41\u673a\u5668\u5b66\u4e60\u5e93\u517c\u5bb9\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\u7ed3\u6784\u5728\u5904\u7406\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u7684\u590d\u6742\u8ba4\u77e5AI\u7cfb\u7edf\u6570\u636e\u65f6\u5b58\u5728\u4e0d\u4fbf\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u5d4c\u5957\u6570\u636e\u7684\u901a\u7528\u5bb9\u5668\u3002", "method": "\u603b\u7ed3\u5d4c\u5957\u6570\u636e\u7684\u4e24\u79cd\u4e3b\u8981\u8ba1\u7b97\u6a21\u5f0f\uff0c\u63d0\u51faTreeTensor\u8fd9\u4e00\u901a\u7528\u5d4c\u5957\u6570\u636e\u5bb9\u5668\uff0c\u901a\u8fc7\u7ea6\u675f\u548c\u9b54\u6cd5\u5de5\u5177\u5b9e\u73b0\u5bf9\u5d4c\u5957\u6570\u636e\u7684\u96f6\u6210\u672c\u64cd\u4f5c\uff0c\u652f\u6301\u4e0eScikit-Learn\u3001Numpy\u3001PyTorch\u7b49\u5e93\u7684\u517c\u5bb9\u3002", "result": "TreeTensor\u5728\u5404\u79cd\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u76ee\u524d\u6700\u590d\u6742\u7684AI\u7cfb\u7edf\u4e4b\u4e00\u2014\u2014\u661f\u9645\u4e89\u9738II\u7684AlphaStar\u4e2d\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u8fd0\u884c\u65f6\u6548\u7387\u4e14\u65e0\u4efb\u4f55\u5f00\u9500\u3002", "conclusion": "TreeTensor\u4e3a\u89e3\u51b3\u590d\u6742\u8ba4\u77e5AI\u7cfb\u7edf\u4e2d\u5d4c\u5957\u6570\u636e\u5904\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7f16\u7a0b\u6548\u7387\u548c\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2602.08520", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08520", "abs": "https://arxiv.org/abs/2602.08520", "authors": ["Xinhai Sun"], "title": "Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning", "comment": null, "summary": "Modern large language models (LLMs) are often evaluated and deployed under a \\emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \\emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \\emph{without any retraining}.\n  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\\% to 84.03\\%, while only incurring 61.06\\% additional inference calls. A 100\\% re-asking ablation reaches 84.35\\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \\emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.\n  Beyond providing a practical inference-time upgrade, our results suggest a broader \\emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u5f3a\u5316\u63a8\u7406\"\u7684\u71b5\u611f\u77e5\u63a8\u7406\u65f6\u63a7\u5236\u7b56\u7565\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u4e0d\u786e\u5b9a\u6027\u9009\u62e9\u6027\u8c03\u7528\u7b2c\u4e8c\u6b21\u66f4\u5ba1\u614e\u7684\u63a8\u7406\u5c1d\u8bd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5728\u4e00\u6b21\u6027\u8d2a\u5a6a\u63a8\u7406\u534f\u8bae\u4e0b\u8fdb\u884c\u8bc4\u4f30\u548c\u90e8\u7f72\uff0c\u8fd9\u79cd\u673a\u5236\u4f1a\u7cfb\u7edf\u6027\u5730\u4f4e\u4f30\u56fa\u5b9a\u6a21\u578b\u7684\u771f\u5b9e\u80fd\u529b\u3002\u8bb8\u591a\u9519\u8bef\u5e76\u975e\u6e90\u4e8e\u77e5\u8bc6\u7f3a\u5931\uff0c\u800c\u662f\u5185\u90e8\u6a21\u7cca\u6027\u4e0b\u7684\u8fc7\u65e9\u51b3\u7b56\u3002", "method": "\u5f15\u5165\u5f3a\u5316\u63a8\u7406\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u71b5\u611f\u77e5\u7684\u63a8\u7406\u65f6\u63a7\u5236\u7b56\u7565\u3002\u901a\u8fc7\u76d1\u6d4b\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u71b5\uff09\uff0c\u5f53\u68c0\u6d4b\u5230\u9ad8\u4e0d\u786e\u5b9a\u6027\u65f6\uff0c\u81ea\u52a8\u89e6\u53d1\u7b2c\u4e8c\u6b21\u66f4\u5ba1\u614e\u7684\u63a8\u7406\u5c1d\u8bd5\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u572812,032\u4e2aMMLU-Pro\u95ee\u9898\u548c14\u4e2a\u5b66\u79d1\u4e0a\uff0c\u4f7f\u7528DeepSeek-v3.2\u6a21\u578b\uff0c\u5f3a\u5316\u63a8\u7406\u5c06\u51c6\u786e\u7387\u4ece60.72%\u63d0\u5347\u81f384.03%\uff0c\u4ec5\u589e\u52a061.06%\u7684\u63a8\u7406\u8c03\u7528\u3002100%\u91cd\u65b0\u8be2\u95ee\u7684\u6d88\u878d\u5b9e\u9a8c\u8fbe\u523084.35%\uff0c\u8868\u660e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9009\u62e9\u80fd\u4ee5\u66f4\u5c11\u8ba1\u7b97\u83b7\u5f97\u5927\u90e8\u5206\u53ef\u5b9e\u73b0\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u63a8\u7406\u65f6\u5347\u7ea7\u65b9\u6848\uff0c\u8fd8\u63d0\u51fa\u4e86\u66f4\u5e7f\u6cdb\u7684\u71b5\u611f\u77e5\u8303\u5f0f\u6765\u6d4b\u91cf\u548c\u6269\u5c55\u6a21\u578b\u80fd\u529b\u3002\u4e00\u6b21\u6027\u8d2a\u5a6a\u63a8\u7406\u4e0e\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u5316\u6df1\u601d\u4e4b\u95f4\u7684\u5dee\u8ddd\u4e3a\u8bca\u65adLLM\u7684\u6f5c\u5728\u63a8\u7406\u8303\u56f4\u63d0\u4f9b\u4e86\u89c6\u89d2\uff0c\u5e76\u6fc0\u52b1\u672a\u6765\u8bad\u7ec3\u76ee\u6807\u660e\u786e\u7ea6\u675f\u6b63\u786e\u6027-\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u3002"}}
{"id": "2602.07706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07706", "abs": "https://arxiv.org/abs/2602.07706", "authors": ["Yuanyun Zhang", "Mingxuan Zhang", "Siyuan Li", "Zihan Wang", "Haoran Chen", "Wenbo Zhou", "Shi Li"], "title": "Dense Feature Learning via Linear Structure Preservation in Medical Data", "comment": "ICLR Workshop", "summary": "Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u5bc6\u96c6\u7279\u5f81\u5b66\u4e60\"\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u5d4c\u5165\u77e9\u9635\u7684\u7ebf\u6027\u4ee3\u6570\u7279\u6027\uff08\u8c31\u5e73\u8861\u3001\u5b50\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u7279\u5f81\u6b63\u4ea4\u6027\uff09\uff0c\u6765\u6539\u5584\u533b\u5b66\u6570\u636e\u7684\u8868\u793a\u8d28\u91cf\uff0c\u800c\u4e0d\u4f9d\u8d56\u6807\u7b7e\u6216\u751f\u6210\u91cd\u5efa\u3002", "motivation": "\u4f20\u7edf\u533b\u5b66\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8868\u793a\u574d\u7f29\u5230\u5c11\u91cf\u5224\u522b\u65b9\u5411\u4e0a\u3002\u8fd9\u79cd\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e34\u5e8a\u6570\u636e\u7684\u4e30\u5bcc\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u7279\u5f81\u7684\u8fc1\u79fb\u6027\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u5bc6\u96c6\u7279\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u64cd\u4f5c\u5d4c\u5165\u77e9\u9635\uff0c\u901a\u8fc7\u5b9a\u4e49\u5728\u7eaf\u7ebf\u6027\u4ee3\u6570\u7279\u6027\u4e0a\u7684\u76ee\u6807\u51fd\u6570\u6765\u9f13\u52b1\u8c31\u5e73\u8861\u3001\u5b50\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u7279\u5f81\u6b63\u4ea4\u6027\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u6807\u7b7e\u6216\u751f\u6210\u91cd\u5efa\u3002", "result": "\u5728\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u3001\u4e34\u5e8a\u6587\u672c\u548c\u591a\u6a21\u6001\u60a3\u8005\u8868\u793a\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u6bd4\u76d1\u7763\u548c\u81ea\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0b\u6e38\u7ebf\u6027\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u5b50\u7a7a\u95f4\u5bf9\u9f50\u65b9\u9762\u90fd\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u5b66\u4e60\u8986\u76d6\u4e34\u5e8a\u53d8\u5f02\u53ef\u80fd\u4e0e\u5b66\u4e60\u9884\u6d4b\u4e34\u5e8a\u7ed3\u679c\u540c\u7b49\u91cd\u8981\uff0c\u5e94\u5c06\u8868\u793a\u51e0\u4f55\u4f5c\u4e3a\u533b\u5b66AI\u7684\u4e00\u7b49\u76ee\u6807\u3002\u5bc6\u96c6\u7279\u5f81\u5b66\u4e60\u4e3a\u6539\u5584\u533b\u5b66\u8868\u793a\u7684\u8d28\u91cf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.07775", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07775", "abs": "https://arxiv.org/abs/2602.07775", "authors": ["Haodong Li", "Shaoteng Liu", "Zhe Lin", "Manmohan Chandraker"], "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion", "comment": "Figure PDFs were compressed to 150 dpi to comply with arXiv's submission size limit. Project page: https://rolling-sink.github.io/", "summary": "Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/", "AI": {"tldr": "\u63d0\u51faRolling Sink\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u8d85\u51fa\u8bad\u7ec3\u65f6\u957f\u65f6\u7684\u89c6\u89c9\u9000\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u8d85\u957f\u89c6\u9891\u751f\u6210", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u957f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u6d4b\u8bd5\u65f6\u9047\u5230\u8d85\u51fa\u8bad\u7ec3\u65f6\u957f\u7684\u5f00\u653e\u65f6\u957f\u65f6\u4f1a\u51fa\u73b0\u89c6\u89c9\u9000\u5316\u95ee\u9898\u3002\u7531\u4e8e\u5f00\u653e\u6d4b\u8bd5\u53ef\u4ee5\u8d85\u8d8a\u4efb\u4f55\u6709\u9650\u8bad\u7ec3\u7a97\u53e3\uff0c\u4e14\u957f\u89c6\u9891\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u5bfb\u627e\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8eSelf Forcing\uff08\u4ec5\u75285\u79d2\u7247\u6bb5\u8bad\u7ec3\uff09\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u81ea\u56de\u5f52\u7f13\u5b58\u7ef4\u62a4\u673a\u5236\uff0c\u63d0\u51faRolling Sink\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u6709\u6548\u6269\u5c55\u81ea\u56de\u5f52\u89c6\u9891\u5408\u6210\u5230\u8d85\u957f\u65f6\u957f\uff085-30\u5206\u949f\uff0c16 FPS\uff09", "result": "Rolling Sink\u5728\u8d85\u957f\u65f6\u957f\u89c6\u9891\u751f\u6210\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u7269\u4f53\u3001\u7a33\u5b9a\u7684\u989c\u8272\u3001\u8fde\u8d2f\u7684\u7ed3\u6784\u548c\u5e73\u6ed1\u7684\u8fd0\u52a8\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u957f\u65f6\u57df\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18", "conclusion": "Rolling Sink\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u5f25\u5408\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u957f\u548c\u5f00\u653e\u6d4b\u8bd5\u65f6\u957f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8d85\u957f\u89c6\u9891\u751f\u6210"}}
{"id": "2602.08597", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08597", "abs": "https://arxiv.org/abs/2602.08597", "authors": ["Roland Bertin-Johannet", "Lara Scipio", "Leopold Mayti\u00e9", "Rufin VanRullen"], "title": "An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture", "comment": null, "summary": "Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\uff08GWT\uff09\u7684top-down\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u9009\u62e9\u76f8\u5173\u6a21\u6001\u6765\u63d0\u5347\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u566a\u58f0\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\uff08GWT\uff09\u4e3a\u591a\u6a21\u6001\u6574\u5408\u63d0\u4f9b\u4e86\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u57fa\u7840\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u7814\u7a76\u4e0d\u8db3\u3002\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u63d0\u5347GWT\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdtop-down\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u5728\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u9009\u62e9\u76f8\u5173\u6a21\u6001\u3002\u5728Simple Shapes\u548cMM-IMDb 1.0\u4e24\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u8be5\u673a\u5236\uff0c\u5e76\u4e0e\u73b0\u6709\u591a\u6a21\u6001\u6ce8\u610f\u529b\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "1\uff09\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7cfb\u7edf\u7684\u566a\u58f0\u9c81\u68d2\u6027\uff1b2\uff09\u5c55\u793a\u4e86\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u6001\u7684\u6cdb\u5316\u80fd\u529b\uff1b3\uff09\u5728MM-IMDb 1.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u673a\u5236\u4f7f\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7cfb\u7edf\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684top-down\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u589e\u5f3a\u4e86\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\u5728\u591a\u6a21\u6001\u6574\u5408\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\uff0c\u4e3aGWT\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2602.07784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07784", "abs": "https://arxiv.org/abs/2602.07784", "authors": ["Jayawant Bodagala", "Balaji Bodagala"], "title": "Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing", "comment": "Total pages: 9", "summary": "Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.", "AI": {"tldr": "UCATSC\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u968f\u673a\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u8def\u53e3\u4fe1\u53f7\u63a7\u5236\uff0c\u8003\u8651\u89c6\u89c9\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u786c\u7ea6\u675f\u786e\u4fdd\u5b89\u5168\u548c\u9632\u6b62\u9965\u997f\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53d7\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u89c6\u89c9\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u9690\u542b\u7684\u5b89\u5168\u6027\u95ee\u9898\u4ee5\u53ca\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5b66\u4e60\u7684\u975e\u53ef\u89e3\u91ca\u63a7\u5236\u7b56\u7565\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3001\u786e\u4fdd\u5b89\u5168\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u7684\u7cfb\u7edf\u3002", "method": "UCATSC\u91c7\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5c06\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u5efa\u6a21\u4e3a\u5177\u6709\u7ea6\u675f\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7684\u968f\u673a\u51b3\u7b56\u8fc7\u7a0b\u3002\u7cfb\u7edf\u5728\u4fe1\u5ff5\u7a7a\u95f4\u4e2d\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u6f14\uff0c\u9884\u6d4b\u5e76\u5f3a\u5236\u6267\u884c\u4e0e\u5b89\u5168\u548c\u9632\u6b62\u9965\u997f\u76f8\u5173\u7684\u786c\u7ea6\u675f\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u5956\u52b1\u5851\u9020\u6765\u5b66\u4e60\u5b89\u5168\u3002", "result": "\u7cfb\u7edf\u8bbe\u8ba1\u65e8\u5728\u6539\u5584\u4ea4\u901a\u5ef6\u8fdf\u548c\u6392\u653e\uff0c\u540c\u65f6\u9632\u6b62\u5b89\u5168\u5173\u952e\u9519\u8bef\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u663e\u5f0f\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u63a7\u5236\u7b56\u7565\u8f93\u51fa\u3002", "conclusion": "UCATSC\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u81ea\u9002\u5e94\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u3001\u5b89\u5168\u4fdd\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08603", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08603", "abs": "https://arxiv.org/abs/2602.08603", "authors": ["Teng Wang", "Rong Shan", "Jianghao Lin", "Junjie Wu", "Tianyi Xu", "Jianping Zhang", "Wenteng Chen", "Changwang Zhang", "Zhaoxiang Wang", "Weinan Zhang", "Jun Wang"], "title": "OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval", "comment": null, "summary": "Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.", "AI": {"tldr": "OSCAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f18\u5316\u5f15\u5bfc\u7684\u667a\u80fd\u4f53\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff0c\u5c06\u542f\u53d1\u5f0f\u641c\u7d22\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u79bb\u7ebf-\u5728\u7ebf\u8303\u5f0f\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u4e24\u79cd\u5c40\u9650\uff1a\u7edf\u4e00\u5d4c\u5165\u68c0\u7d22\u53d7\u9650\u4e8e\u5355\u4e00\u6a21\u578b\u8fd1\u89c6\u95ee\u9898\uff0c\u542f\u53d1\u5f0f\u667a\u80fd\u4f53\u68c0\u7d22\u53d7\u9650\u4e8e\u6b21\u4f18\u7684\u8bd5\u9519\u7f16\u6392\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u5904\u7406\u89c6\u89c9\u548c\u6587\u672c\u7ea6\u675f\u7684\u590d\u6742\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u79bb\u7ebf-\u5728\u7ebf\u8303\u5f0f\uff1a\u79bb\u7ebf\u9636\u6bb5\u5c06CIR\u5efa\u6a21\u4e3a\u4e24\u9636\u6bb5\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u5e03\u5c14\u96c6\u5408\u8fd0\u7b97\u63a8\u5bfc\u6700\u5927\u5316\u771f\u5b9e\u8986\u76d6\u7684\u6700\u4f18\u8f68\u8ff9\uff0c\u5b58\u50a8\u5728\u9ec4\u91d1\u5e93\u4e2d\uff1b\u5728\u7ebf\u9636\u6bb5\u4f7f\u7528\u8fd9\u4e9b\u8f68\u8ff9\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u6f14\u793a\u6765\u5f15\u5bfcVLM\u89c4\u5212\u5668\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e00\u4e2a\u79c1\u6709\u5de5\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOSCAR\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u7279\u522b\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u4ec5\u4f7f\u752810%\u7684\u8bad\u7ec3\u6570\u636e\u5c31\u80fd\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u89c4\u5212\u903b\u8f91\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u800c\u975e\u6570\u636e\u96c6\u7279\u5b9a\u7684\u8bb0\u5fc6\u3002", "conclusion": "OSCAR\u6210\u529f\u5c06\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\u4ece\u542f\u53d1\u5f0f\u641c\u7d22\u8fc7\u7a0b\u8f6c\u5316\u4e3a\u539f\u5219\u6027\u7684\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u79bb\u7ebf\u63a8\u5bfc\u6700\u4f18\u8f68\u8ff9\u548c\u5728\u7ebf\u5f15\u5bfc\u89c4\u5212\u7684\u65b9\u5f0f\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2602.07715", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07715", "abs": "https://arxiv.org/abs/2602.07715", "authors": ["Roi Benita", "Michael Elad", "Joseph Keshet"], "title": "Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models", "comment": null, "summary": "Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u5148\u9a8c\u5047\u8bbe\u7684\u96f6\u6837\u672c\u6269\u6563\u9006\u95ee\u9898\u6c42\u89e3\u5668\u7684\u4e25\u683c\u5206\u6790\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u95ed\u5f0f\u8868\u8fbe\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u53c2\u6570\u8bbe\u8ba1\u6846\u67b6\u6765\u66ff\u4ee3\u542f\u53d1\u5f0f\u9009\u62e9\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u6269\u6563\u65b9\u6cd5\u5728\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\u548c\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u57fa\u7840\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6\u6765\u6307\u5bfc\u7b97\u6cd5\u8bbe\u8ba1\u548c\u53c2\u6570\u9009\u62e9\u3002", "method": "\u5728\u5047\u8bbe\u5148\u9a8c\u4e3a\u9ad8\u65af\u5206\u5e03\u7684\u6761\u4ef6\u4e0b\uff0c\u63a8\u5bfc\u51fa\u7406\u60f3\u540e\u9a8c\u91c7\u6837\u5668\u548c\u6269\u6563\u91cd\u5efa\u7b97\u6cd5\u7684\u95ed\u5f0f\u8868\u8fbe\uff0c\u5728\u8c31\u57df\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u4e0e\u7b97\u6cd5\u65e0\u5173\u7684\u539f\u5219\u6027\u53c2\u6570\u8bbe\u8ba1\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u7684\u8c31\u57df\u5206\u6790\u65b9\u6cd5\u63ed\u793a\u4e86\u53c2\u6570\u9009\u62e9\u4e0e\u6807\u51c6\u542f\u53d1\u5f0f\u7b56\u7565\u7684\u7ed3\u6784\u6027\u5dee\u5f02\uff0c\u8868\u660e\u53c2\u6570\u5e94\u968f\u6269\u6563\u6b65\u957f\u53d8\u5316\uff0c\u80fd\u591f\u5728\u611f\u77e5\u8d28\u91cf\u548c\u4fe1\u53f7\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e00\u81f4\u5e73\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u96f6\u6837\u672c\u6269\u6563\u9006\u95ee\u9898\u6c42\u89e3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c31\u57df\u5206\u6790\u548c\u539f\u5219\u6027\u53c2\u6570\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u91cd\u5efa\u6027\u80fd\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.08630", "categories": ["cs.AI", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.08630", "abs": "https://arxiv.org/abs/2602.08630", "authors": ["Jonah Brown-Cohen", "Geoffrey Irving", "Simon C. Marshall", "Ilan Newman", "Georgios Piliouras", "Mario Szegedy"], "title": "Debate is efficient with your time", "comment": "11 Pages, 0 figures", "summary": "AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.\n  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86AI\u5b89\u5168\u8fa9\u8bba\u4e2d\u7684\u4eba\u7c7b\u76d1\u7763\u6210\u672c\uff0c\u5f15\u5165\u4e86\u8fa9\u8bba\u67e5\u8be2\u590d\u6742\u5ea6(DQC)\u6982\u5ff5\uff0c\u53d1\u73b0PSPACE/poly\u7c7b\u95ee\u9898\u4ec5\u9700O(log n)\u6b21\u67e5\u8be2\u5373\u53ef\u5224\u5b9a\uff0c\u8868\u660e\u8fa9\u8bba\u5177\u6709\u6781\u9ad8\u7684\u67e5\u8be2\u6548\u7387\u3002", "motivation": "AI\u5b89\u5168\u8fa9\u8bba\u4f7f\u7528\u4e24\u4e2a\u7ade\u4e89\u6a21\u578b\u5e2e\u52a9\u4eba\u7c7b\u6cd5\u5b98\u9a8c\u8bc1\u590d\u6742\u8ba1\u7b97\u4efb\u52a1\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u53ea\u5206\u6790\u4e86\u8fa9\u8bba\u5728\u7406\u8bba\u4e0a\u80fd\u89e3\u51b3\u4ec0\u4e48\u95ee\u9898\uff0c\u6ca1\u6709\u5206\u6790\u4eba\u7c7b\u76d1\u7763\u7684\u5b9e\u9645\u6210\u672c\u2014\u2014\u6cd5\u5b98\u9700\u8981\u5bf9\u8fa9\u8bba\u8bb0\u5f55\u8fdb\u884c\u591a\u5c11\u6b21\u67e5\u8be2\u3002\u9700\u8981\u91cf\u5316\u8fd9\u79cd\u76d1\u7763\u6210\u672c\u3002", "method": "\u5f15\u5165\u8fa9\u8bba\u67e5\u8be2\u590d\u6742\u5ea6(DQC)\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e3a\u9a8c\u8bc1\u8005\u6b63\u786e\u51b3\u5b9a\u8fa9\u8bba\u6240\u9700\u68c0\u67e5\u7684\u6700\u5c0f\u6bd4\u7279\u6570\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86DQC\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u7c7b\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u4e0ePSPACE/poly\u7c7b\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0PSPACE/poly\u7c7b\uff08\u8fa9\u8bba\u80fd\u6709\u6548\u5224\u5b9a\u7684\u95ee\u9898\u7c7b\uff09\u6070\u597d\u662f\u90a3\u4e9b\u7528O(log n)\u6b21\u67e5\u8be2\u53ef\u5224\u5b9a\u7684\u51fd\u6570\u7c7b\u3002\u8fd9\u8868\u660e\u8fa9\u8bba\u5177\u6709\u60ca\u4eba\u7684\u67e5\u8be2\u6548\u7387\uff1a\u5373\u4f7f\u5bf9\u4e8e\u9ad8\u5ea6\u590d\u6742\u7684\u95ee\u9898\uff0c\u5bf9\u6570\u7ea7\u522b\u7684\u76d1\u7763\u5c31\u8db3\u591f\u4e86\u3002\u8fd8\u8bc1\u660e\u4e86\u4f9d\u8d56\u6240\u6709\u8f93\u5165\u6bd4\u7279\u7684\u51fd\u6570\u9700\u8981\u03a9(log n)\u6b21\u67e5\u8be2\uff0c\u4efb\u4f55\u53ef\u7531\u5927\u5c0f\u4e3as\u7684\u7535\u8def\u8ba1\u7b97\u7684\u51fd\u6570\u6ee1\u8db3DQC(f) \u2264 log(s) + 3\u3002", "conclusion": "\u8fa9\u8bba\u67e5\u8be2\u590d\u6742\u5ea6\u4e3aAI\u5b89\u5168\u8fa9\u8bba\u7684\u5b9e\u8df5\u53ef\u884c\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5bf9\u6570\u67e5\u8be2\u590d\u6742\u5ea6\u4f7f\u5f97\u4eba\u7c7b\u76d1\u7763\u5728\u5b9e\u9645\u4e2d\u53ef\u884c\u3002\u6709\u8da3\u7684\u662f\uff0c\u8bc1\u660eP\u7c7b\u8bed\u8a00\u4e2dDQC\u4e0b\u754c\u4e3alog(n) + 6\u5c06\u4ea7\u751f\u65b0\u7684\u7535\u8def\u4e0b\u754c\uff0c\u5c06\u8fa9\u8bba\u67e5\u8be2\u590d\u6742\u5ea6\u4e0e\u7535\u8def\u590d\u6742\u5ea6\u7684\u6838\u5fc3\u95ee\u9898\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2602.07719", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07719", "abs": "https://arxiv.org/abs/2602.07719", "authors": ["Gabriel Stella"], "title": "Efficient Planning in Reinforcement Learning via Model Introspection", "comment": null, "summary": "Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5185\u7701\u89c6\u4e3a\u7a0b\u5e8f\u5206\u6790\uff0c\u5efa\u7acb\u5f3a\u5316\u5b66\u4e60\u4e0e\u7ecf\u5178\u89c4\u5212\u4e4b\u95f4\u7684\u65b0\u8054\u7cfb", "motivation": "\u4eba\u7c7b\u5728\u9762\u5bf9\u4efb\u52a1\u65f6\uff0c\u65e0\u8bba\u4efb\u52a1\u5982\u4f55\u6307\u5b9a\uff0c\u90fd\u80fd\u901a\u8fc7\u5185\u7701\u63a8\u7406\u5185\u90e8\u6a21\u578b\u6765\u5408\u6210\u989d\u5916\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u3002\u5f3a\u5316\u5b66\u4e60\u548c\u7ecf\u5178\u89c4\u5212\u901a\u5e38\u88ab\u89c6\u4e3a\u4e24\u4e2a\u4e0d\u540c\u95ee\u9898\uff0c\u9700\u8981\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4eba\u7c7b\u7684\u5185\u7701\u80fd\u529b\u8868\u660e\u4e24\u8005\u53ef\u4ee5\u5efa\u7acb\u8054\u7cfb", "method": "\u5c06\u5185\u7701\u89c6\u4e3a\u7a0b\u5e8f\u5206\u6790\uff0c\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f7f\u7528\u7684\u5404\u7c7b\u6a21\u578b\u3002\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u7c7b\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u76ee\u6807\u5bfc\u5411\u89c4\u5212", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4e2d\u5b9e\u73b0\u5185\u7701\uff0c\u5efa\u7acb\u4e86\u5f3a\u5316\u5b66\u4e60\u4e0e\u7ecf\u5178\u89c4\u5212\u4e4b\u95f4\u7684\u65b0\u9896\u8054\u7cfb", "conclusion": "\u901a\u8fc7\u5c06\u5185\u7701\u89c6\u4e3a\u7a0b\u5e8f\u5206\u6790\uff0c\u53ef\u4ee5\u5728\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4e2d\u5408\u6210\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u4ece\u800c\u5728\u5173\u7cfb\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u76ee\u6807\u5bfc\u5411\u89c4\u5212\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e0e\u7ecf\u5178\u89c4\u5212\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.07814", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07814", "abs": "https://arxiv.org/abs/2602.07814", "authors": ["Simiao Ren", "Yuchen Zhou", "Xingyu Shen", "Kidus Zewde", "Tommy Duong", "George Huang", "Hatsanai", "Tiangratanakul", "Tsang", "Ng", "En Wei", "Jiayu Xue"], "title": "How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study", "comment": null, "summary": "As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$\u03c1$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\\% mean accuracy) from the worst (37.5\\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $\u03c7^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5bf916\u79cd\u6700\u5148\u8fdb\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u8986\u76d623\u4e2a\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u53d8\u4f53\u548c12\u4e2a\u6570\u636e\u96c6\uff08260\u4e07\u56fe\u50cf\u6837\u672c\uff09\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u56fe\u50cf\u5728\u6570\u5b57\u5e73\u53f0\u4e0a\u7684\u6fc0\u589e\uff0c\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u4e8e\u6253\u51fb\u9519\u8bef\u4fe1\u606f\u548c\u7ef4\u62a4\u5185\u5bb9\u771f\u5b9e\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u5fae\u8c03\u6a21\u578b\uff0c\u800c\u5ffd\u7565\u4e86\u5f00\u7bb1\u5373\u7528\u6027\u80fd\u2014\u2014\u8fd9\u662f\u4ece\u4e1a\u8005\u6700\u5e38\u89c1\u7684\u90e8\u7f72\u573a\u666f\uff0c\u5b58\u5728\u91cd\u8981\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5bf916\u79cd\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0823\u4e2a\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u53d8\u4f53\uff09\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u8986\u76d612\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5305\u542b260\u4e07\u56fe\u50cf\u6837\u672c\uff0c\u6db5\u76d6291\u4e2a\u72ec\u7279\u7684\u751f\u6210\u5668\uff0c\u5305\u62ec\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u3002\u4f7f\u7528\u7edf\u8ba1\u5206\u6790\u65b9\u6cd5\u8bc4\u4f30\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1)\u4e0d\u5b58\u5728\u901a\u7528\u4f18\u80dc\u8005\uff0c\u68c0\u6d4b\u5668\u6392\u540d\u6781\u4e0d\u7a33\u5b9a\uff1b(2)\u6700\u4f73\u68c0\u6d4b\u5668\uff0875.0%\u5e73\u5747\u51c6\u786e\u7387\uff09\u4e0e\u6700\u5dee\u68c0\u6d4b\u5668\uff0837.5%\uff09\u5b58\u572837\u4e2a\u767e\u5206\u70b9\u7684\u6027\u80fd\u5dee\u8ddd\uff1b(3)\u8bad\u7ec3\u6570\u636e\u5bf9\u9f50\u5bf9\u6cdb\u5316\u80fd\u529b\u6709\u91cd\u8981\u5f71\u54cd\uff1b(4)\u73b0\u4ee3\u5546\u4e1a\u751f\u6210\u5668\uff08Flux Dev\u3001Firefly v4\u3001Midjourney v7\uff09\u80fd\u51fb\u8d25\u5927\u591a\u6570\u68c0\u6d4b\u5668\uff1b(5)\u8bc6\u522b\u51fa\u4e09\u79cd\u5f71\u54cd\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\"\u4e00\u5200\u5207\"\u7684\u68c0\u6d4b\u5668\u8303\u5f0f\uff0c\u8868\u660e\u4ece\u4e1a\u8005\u5fc5\u987b\u6839\u636e\u5177\u4f53\u5a01\u80c1\u73af\u5883\u4ed4\u7ec6\u9009\u62e9\u68c0\u6d4b\u5668\uff0c\u800c\u4e0d\u80fd\u4f9d\u8d56\u5df2\u53d1\u5e03\u7684\u57fa\u51c6\u6027\u80fd\u3002\u8fd9\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2602.07721", "categories": ["cs.LG", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.07721", "abs": "https://arxiv.org/abs/2602.07721", "authors": ["Yanlin Qi", "Xinhang Chen", "Huiqiang Jiang", "Qitong Wang", "Botao Peng", "Themis Palpanas"], "title": "ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs", "comment": "25 pages, 16 figures. Under review", "summary": "KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\\times$ and 44$\\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.", "AI": {"tldr": "ParisKV\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684KV-cache\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u78b0\u649e\u5019\u9009\u9009\u62e9\u548c\u91cf\u5316\u5185\u79ef\u91cd\u6392\u5e8f\u89e3\u51b3\u5206\u5e03\u6f02\u79fb\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u652f\u6301\u767e\u4e07token\u4e0a\u4e0b\u6587\uff0c\u5728\u89e3\u7801\u6548\u7387\u548c\u541e\u5410\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709KV-cache\u68c0\u7d22\u65b9\u6cd5\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u65f6\u9762\u4e34\u5206\u5e03\u6f02\u79fb\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u78b0\u649e\u5019\u9009\u9009\u62e9\u673a\u5236\uff0c\u7ed3\u5408\u91cf\u5316\u5185\u79ef\u91cd\u6392\u5e8f\u4f30\u8ba1\u5668\uff1b\u652f\u6301\u901a\u8fc7\u7edf\u4e00\u865a\u62df\u5bfb\u5740\u5b9e\u73b0CPU\u5378\u8f7d\u7684KV-cache\uff0c\u5b9e\u73b0\u6309\u9700top-k\u83b7\u53d6\uff1bGPU\u539f\u751f\u8bbe\u8ba1\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u957f\u8f93\u5165\u548c\u957f\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u5168\u6ce8\u610f\u529b\u8d28\u91cf\uff1b\u5728\u957f\u4e0a\u4e0b\u6587\u89e3\u7801\u6548\u7387\u4e0a\u8fbe\u5230SOTA\uff1a\u5728\u6279\u5927\u5c0f\u4e3a1\u65f6\u5339\u914d\u6216\u8d85\u8fc7\u5168\u6ce8\u610f\u529b\u901f\u5ea6\uff0c\u5728\u53ef\u8fd0\u884c\u8303\u56f4\u5185\u63d0\u4f9b\u6700\u9ad82.8\u500d\u541e\u5410\u91cf\uff0c\u652f\u6301\u767e\u4e07token\u4e0a\u4e0b\u6587\uff1b\u76f8\u6bd4MagicPIG\u548cPQCache\u5206\u522b\u51cf\u5c1117\u500d\u548c44\u500d\u89e3\u7801\u5ef6\u8fdf\u3002", "conclusion": "ParisKV\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u7a33\u5065\u7684KV-cache\u68c0\u7d22\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u7684\u5206\u5e03\u6f02\u79fb\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89e3\u7801\u6548\u7387\uff0c\u652f\u6301\u6269\u5c55\u5230\u767e\u4e07token\u89c4\u6a21\u3002"}}
{"id": "2602.07815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07815", "abs": "https://arxiv.org/abs/2602.07815", "authors": ["Simiao Ren"], "title": "Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures", "comment": null, "summary": "Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \\textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \\textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \\emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\\% false adult rates on minors while VLMs achieve 13--25\\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.", "AI": {"tldr": "\u9996\u4e2a\u5927\u89c4\u6a21\u8de8\u8303\u5f0f\u9762\u90e8\u5e74\u9f84\u4f30\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u663e\u8457\u4f18\u4e8e\u5927\u591a\u6570\u4e13\u7528\u6a21\u578b\uff0c\u6700\u4f73VLM\u6bd4\u6700\u4f73\u4e13\u7528\u6a21\u578bMAE\u964d\u4f4e15%\uff0c\u6311\u6218\u4e86\u4efb\u52a1\u4e13\u7528\u67b6\u6784\u5fc5\u8981\u7684\u5047\u8bbe\u3002", "motivation": "\u9762\u90e8\u5e74\u9f84\u4f30\u8ba1\u5bf9\u5185\u5bb9\u5ba1\u6838\u3001\u5e74\u9f84\u9a8c\u8bc1\u548c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u524d\u6ca1\u6709\u7cfb\u7edf\u6bd4\u8f83\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u4e13\u7528\u5e74\u9f84\u4f30\u8ba1\u67b6\u6784\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u8de8\u8303\u5f0f\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f3034\u4e2a\u6a21\u578b\uff0822\u4e2a\u4e13\u7528\u67b6\u6784\u548c12\u4e2a\u901a\u7528VLMs\uff09\uff0c\u57288\u4e2a\u6807\u51c6\u6570\u636e\u96c6\uff08\u603b\u8ba1\u6bcf\u4e2a\u6a21\u578b1100\u5f20\u6d4b\u8bd5\u56fe\u50cf\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u6790MAE\u300118\u5c81\u9608\u503c\u5e74\u9f84\u9a8c\u8bc1\u3001\u7c97\u7c92\u5ea6\u5206\u7bb1\u6548\u679c\u548c14\u4e2a\u5e74\u9f84\u7ec4\u7684\u5206\u5c42\u5206\u6790\u3002", "result": "\u96f6\u6837\u672cVLMs\u663e\u8457\u4f18\u4e8e\u5927\u591a\u6570\u4e13\u7528\u6a21\u578b\uff0c\u5e73\u5747MAE\u4e3a5.65\u5e74\uff08\u4e13\u7528\u6a21\u578b\u4e3a9.88\u5e74\uff09\uff1b\u6700\u4f73VLM\uff08Gemini 3 Flash Preview\uff0cMAE 4.32\uff09\u6bd4\u6700\u4f73\u4e13\u7528\u6a21\u578b\uff08MiVOLO\uff0cMAE 5.10\uff09\u63d0\u534715%\uff1bVLMs\u5728\u672a\u6210\u5e74\u4eba\u5e74\u9f84\u9a8c\u8bc1\u4e2d\u5047\u6210\u4eba\u7387\u4ec5\u4e3a13-25%\uff08\u4e13\u7528\u6a21\u578b\u4e3a60-100%\uff09\uff1b\u6240\u6709\u6a21\u578b\u5728\u6781\u7aef\u5e74\u9f84\uff08<5\u5c81\u548c65+\uff09\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u4efb\u52a1\u4e13\u7528\u67b6\u6784\u5bf9\u5e74\u9f84\u4f30\u8ba1\u5fc5\u8981\u7684\u5047\u8bbe\uff0c\u5efa\u8bae\u9886\u57df\u5e94\u8f6c\u5411\u5c06VLM\u80fd\u529b\u84b8\u998f\u5230\u9ad8\u6548\u4e13\u7528\u6a21\u578b\u4e2d\u3002"}}
{"id": "2602.08708", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08708", "abs": "https://arxiv.org/abs/2602.08708", "authors": ["Stefan Edelkamp", "Ji\u0159\u00ed Fink", "Petr Gregor", "Anders Jonsson", "Bernhard Nebel"], "title": "Intermediate Results on the Complexity of STRIPS$_{1}^{1}$", "comment": null, "summary": "This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86STRIPS\u89c4\u5212\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5f53\u64cd\u4f5c\u7b26\u53ea\u6709\u4e00\u4e2a\u524d\u63d0\u6761\u4ef6\u548c\u4e00\u4e2a\u6548\u679c\u65f6\uff08STRIPS\u00b9\u2081\uff09\u662f\u5426NP\u5b8c\u5168\u7684\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8eBylander\u5173\u4e8e\u547d\u9898STRIPS\u89c4\u5212\u8ba1\u7b97\u590d\u6742\u6027\u7684\u7ed3\u679c\uff0c\u867d\u7136\u5df2\u77e5\u5f53\u64cd\u4f5c\u7b26\u9650\u5236\u4e3a\u4e24\u4e2a\u524d\u63d0\u6761\u4ef6\u548c\u4e24\u4e2a\u540e\u7f6e\u6761\u4ef6\u65f6\uff0c\u89c4\u5212\u5b58\u5728\u6027\u5224\u5b9a\u662fPSPACE\u5b8c\u5168\u7684\uff0c\u4f46\u5bf9\u4e8e\u53ea\u6709\u4e00\u4e2a\u524d\u63d0\u6761\u4ef6\u548c\u4e00\u4e2a\u6548\u679c\u7684\u64cd\u4f5c\u7b26\uff0c\u5176\u590d\u6742\u6027\u662f\u5426NP\u5b8c\u5168\u4ecd\u662f\u672a\u77e5\u7684\u3002", "method": "\u901a\u8fc7\u8c03\u7528SAT\u6c42\u89e3\u5668\u5904\u7406\u5c0f\u89c4\u6a21\u5b9e\u4f8b\uff0c\u5f15\u5165\u6587\u5b57\u56fe\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230Petri\u7f51\u6765\u5206\u6790STRIPS\u00b9\u2081\u7684\u590d\u6742\u6027\u3002", "result": "\u8bba\u6587\u5bf9STRIPS\u00b9\u2081\u7684\u5c0f\u89e3\u5047\u8bbe\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u901a\u8fc7\u591a\u79cd\u6280\u672f\u65b9\u6cd5\u63a2\u8ba8\u4e86\u8be5\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3aSTRIPS\u89c4\u5212\u4e2d\u64cd\u4f5c\u7b26\u590d\u6742\u6027\u8fb9\u754c\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u7279\u522b\u662f\u9488\u5bf9\u53ea\u6709\u4e00\u4e2a\u524d\u63d0\u6761\u4ef6\u548c\u4e00\u4e2a\u6548\u679c\u7684\u64cd\u4f5c\u7b26\u7684NP\u5b8c\u5168\u6027\u95ee\u9898\u3002"}}
{"id": "2602.07729", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07729", "abs": "https://arxiv.org/abs/2602.07729", "authors": ["Sagnik Mukherjee", "Lifan Yuan", "Pavan Jayasinha", "Dilek Hakkani-T\u00fcr", "Hao Peng"], "title": "Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs", "comment": null, "summary": "Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u7b80\u5355\u7684SGD\u4f18\u5316\u5668\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684AdamW\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u53c2\u6570\u66f4\u65b0\u7a00\u758f\u5ea6\u6781\u9ad8\uff08\u4ec5\u66f4\u65b0\u4e0d\u52300.02%\u7684\u53c2\u6570\uff09\uff0c\u5185\u5b58\u6548\u7387\u5927\u5e45\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u8bad\u7ec3\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff08\u7279\u522b\u662fRLVR\uff09\u901a\u5e38\u6cbf\u7528\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u7684\u4f18\u5316\u5668\u5b9e\u8df5\uff08\u5982AdamW\uff09\uff0c\u4f46RL\u4e0e\u8fd9\u4e9b\u9636\u6bb5\u5b58\u5728\u6839\u672c\u5dee\u5f02\u3002AdamW\u5185\u5b58\u5f00\u9500\u5927\uff0c\u4e14\u5176\u52a8\u91cf\u673a\u5236\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u5728RL\u4e2d\u7684\u6548\u679c\u53ef\u80fd\u4e0d\u5982\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\u663e\u8457\u3002", "method": "\u901a\u8fc7\u5206\u6790AdamW\u5728RL\u548cSFT\u4e2d\u7684\u4e0d\u540c\u5f71\u54cd\uff0c\u63d0\u51fa\u5047\u8bbe\uff1aRL\u4eceAdam\u98ce\u683c\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u548c\u52a8\u91cf\u4e2d\u83b7\u76ca\u8f83\u5c11\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4f7f\u7528\u5185\u5b58\u6548\u7387\u66f4\u9ad8\u7684SGD\u4f18\u5316\u5668\u5728LLM\u7684RL\u8bad\u7ec3\u4e2d\u8868\u73b0\uff0c\u5e76\u4e0eAdamW\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "SGD\u5728LLM\u7684RL\u8bad\u7ec3\u4e2d\u5339\u914d\u751a\u81f3\u4f18\u4e8eAdamW\uff0c\u4e14\u53c2\u6570\u66f4\u65b0\u6781\u5176\u7a00\u758f\u2014\u2014SGD\u5168\u53c2\u6570\u5fae\u8c03\u4ec5\u66f4\u65b0\u4e0d\u52300.02%\u7684\u53c2\u6570\uff0c\u6bd4AdamW\u5c111000\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4e0d\u4f9d\u8d56\u4efb\u4f55\u7a00\u758f\u5316\u6b63\u5219\u5316\u3002", "conclusion": "RL\u9636\u6bb5\u7684\u4f18\u5316\u52a8\u6001\u4e0e\u76d1\u7763\u5b66\u4e60\u9636\u6bb5\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0cSGD\u5728RL\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u53c2\u6570\u6548\u7387\u6781\u9ad8\uff0c\u8fd9\u4e3aLLM\u7684RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u89c6\u89d2\uff0c\u8868\u660eRL\u53ef\u4ee5\u6bd4\u4ee5\u5f80\u8ba4\u77e5\u7684\u66f4\u52a0\u53c2\u6570\u9ad8\u6548\u3002"}}
{"id": "2602.08715", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08715", "abs": "https://arxiv.org/abs/2602.08715", "authors": ["Miquel Mir\u00f3-Nicolau", "Gabriel Moy\u00e0-Alcover", "Anna Arias-Duart"], "title": "Exploring SAIG Methods for an Objective Evaluation of XAI", "comment": null, "summary": "The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7efc\u8ff0\u4e86SAIG\u65b9\u6cd5\uff08\u5408\u6210\u4eba\u5de5\u667a\u80fd\u57fa\u51c6\u771f\u503c\u65b9\u6cd5\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u53d1\u73b0XAI\u8bc4\u4f30\u9886\u57df\u7f3a\u4e4f\u5171\u8bc6\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6807\u51c6\u5316\u3002", "motivation": "XAI\uff08\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff09\u8bc4\u4f30\u9886\u57df\u65b9\u6cd5\u591a\u6837\u4e14\u590d\u6742\uff0c\u4e0e\u4f20\u7edfAI\u8bc4\u4f30\u4e0d\u540c\uff0cXAI\u7f3a\u4e4f\u666e\u904d\u6b63\u786e\u7684\u89e3\u91ca\u57fa\u51c6\u771f\u503c\uff0c\u4f7f\u5f97\u5ba2\u89c2\u8bc4\u4f30\u5177\u6709\u6311\u6218\u6027\u3002SAIG\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u4eba\u5de5\u57fa\u51c6\u771f\u503c\u4e3aXAI\u6280\u672f\u63d0\u4f9b\u76f4\u63a5\u8bc4\u4f30\uff0c\u4f46\u8be5\u9886\u57df\u5c1a\u672a\u6709\u7cfb\u7edf\u7efc\u8ff0\u3002", "method": "1. \u9996\u6b21\u5bf9SAIG\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\u548c\u5206\u6790\uff1b2. \u63d0\u51fa\u65b0\u7684\u5206\u7c7b\u6cd5\u6765\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff1b3. \u8bc6\u522b\u4e86\u533a\u5206\u4e0d\u540cSAIG\u65b9\u6cd5\u7684\u4e03\u4e2a\u5173\u952e\u7279\u5f81\uff1b4. \u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0XAI\u8bc4\u4f30\u6280\u672f\u7f3a\u4e4f\u5171\u8bc6\uff0c\u4e0d\u540c\u65b9\u6cd5\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u51f8\u663e\u4e86\u8be5\u9886\u57df\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6807\u51c6\u5316\u3002", "conclusion": "SAIG\u65b9\u6cd5\u4e3a\u89e3\u51b3XAI\u8bc4\u4f30\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u4f46\u5f53\u524d\u9886\u57df\u7f3a\u4e4f\u5171\u8bc6\uff0c\u9700\u8981\u66f4\u591a\u7684\u7814\u7a76\u548c\u6807\u51c6\u5316\u5de5\u4f5c\u6765\u5efa\u7acb\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.07730", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07730", "abs": "https://arxiv.org/abs/2602.07730", "authors": ["Siddarth Chandrasekar", "Marlos C. Machado"], "title": "The Laplacian Keyboard: Beyond the Linear Span", "comment": "28 pages, 17 figures", "summary": "Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.", "AI": {"tldr": "Laplacian Keyboard (LK) \u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u9009\u9879\u5e93\uff0c\u901a\u8fc7\u5143\u7b56\u7565\u52a8\u6001\u7ec4\u5408\u8fd9\u4e9b\u9009\u9879\uff0c\u4ece\u800c\u8d85\u8d8a\u7ebf\u6027\u7ea6\u675f\u5b66\u4e60\u66f4\u590d\u6742\u7684\u7b56\u7565\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u901a\u5e38\u53ea\u7528\u4e8e\u7ebf\u6027\u8fd1\u4f3c\u5956\u52b1\u51fd\u6570\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u8d85\u8d8a\u7ebf\u6027\u7ea6\u675f\uff0c\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u5411\u91cf\u6784\u5efa\u66f4\u7075\u6d3b\u7684\u7b56\u7565\u8868\u793a\u3002", "method": "\u63d0\u51faLaplacian Keyboard (LK)\u5206\u5c42\u6846\u67b6\uff1a1) \u4ece\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u6784\u5efa\u4efb\u52a1\u65e0\u5173\u7684\u9009\u9879\u5e93\uff0c\u5f62\u6210\u884c\u4e3a\u57fa\uff1b2) \u8bad\u7ec3\u5143\u7b56\u7565\u52a8\u6001\u7ec4\u5408\u8fd9\u4e9b\u9009\u9879\uff1b3) \u8be5\u6846\u67b6\u4fdd\u8bc1\u5bf9\u7ebf\u6027\u8de8\u5ea6\u5185\u7684\u4efb\u4f55\u5956\u52b1\u51fd\u6570\u90fd\u80fd\u5305\u542b\u6700\u4f18\u7b56\u7565\u3002", "result": "\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u96f6\u6837\u672c\u8fd1\u4f3c\u8bef\u5dee\u7684\u754c\u9650\u3002\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cLK\u8d85\u8d8a\u4e86\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u76f8\u6bd4\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "Laplacian Keyboard \u6846\u67b6\u6210\u529f\u5730\u5c06\u62c9\u666e\u62c9\u65af\u7279\u5f81\u5411\u91cf\u7684\u5e94\u7528\u4ece\u7ebf\u6027\u8fd1\u4f3c\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u5c42\u9009\u9879\u7ec4\u5408\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2602.07827", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07827", "abs": "https://arxiv.org/abs/2602.07827", "authors": ["Guoting Wei", "Xia Yuan", "Yang Zhou", "Haizhao Jing", "Yu Liu", "Xianbiao Qi", "Chunxia Zhao", "Haokui Zhang", "Rong Xiao"], "title": "Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection", "comment": null, "summary": "Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.", "AI": {"tldr": "OTA-Det\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u5f00\u653e\u8bcd\u6c47\u822a\u7a7a\u68c0\u6d4b(OVAD)\u548c\u9065\u611f\u89c6\u89c9\u5b9a\u4f4d(RSVG)\u4e24\u79cd\u8303\u5f0f\u7ed3\u5408\uff0c\u652f\u6301\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u630134 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709OVAD\u65b9\u6cd5\u4ec5\u9650\u4e8e\u7c97\u7c92\u5ea6\u7684\u7c7b\u522b\u7ea7\u8bed\u4e49\u7406\u89e3\uff0c\u800cRSVG\u65b9\u6cd5\u5728\u7ed3\u6784\u4e0a\u4ec5\u9650\u4e8e\u5355\u76ee\u6807\u5b9a\u4f4d\u3002\u8fd9\u4e24\u79cd\u8303\u5f0f\u5404\u81ea\u72ec\u7acb\u8fd0\u4f5c\u65f6\u90fd\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u7684\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\u3002", "method": "\u63d0\u51faOTA-Det\u7edf\u4e00\u6846\u67b6\uff1a1\uff09\u4efb\u52a1\u91cd\u6784\u7b56\u7565\uff0c\u7edf\u4e00\u4efb\u52a1\u76ee\u6807\u548c\u76d1\u7763\u673a\u5236\uff0c\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\uff1b2\uff09\u5bc6\u96c6\u8bed\u4e49\u5bf9\u9f50\u7b56\u7565\uff0c\u5efa\u7acb\u4ece\u6574\u4f53\u8868\u8fbe\u5230\u4e2a\u4f53\u5c5e\u6027\u7684\u591a\u7c92\u5ea6\u5bf9\u5e94\u5173\u7cfb\uff1b3\uff09\u57fa\u4e8eRT-DETR\u67b6\u6784\u6269\u5c55\uff0c\u5f15\u5165\u9ad8\u6548\u6a21\u5757\uff0c\u4ece\u95ed\u96c6\u68c0\u6d4b\u6269\u5c55\u5230\u5f00\u653e\u6587\u672c\u68c0\u6d4b\u3002", "result": "\u5728\u516d\u4e2a\u6db5\u76d6OVAD\u548cRSVG\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u630134 FPS\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "OTA-Det\u6210\u529f\u5c06OVAD\u548cRSVG\u4e24\u79cd\u8303\u5f0f\u7edf\u4e00\u5230\u4e00\u4e2a\u6846\u67b6\u4e2d\uff0c\u514b\u670d\u4e86\u5404\u81ea\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u8bed\u4e49\u7406\u89e3\u548c\u591a\u76ee\u6807\u68c0\u6d4b\u7684\u80fd\u529b\uff0c\u4e3a\u822a\u7a7a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07732", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.07732", "abs": "https://arxiv.org/abs/2602.07732", "authors": ["Joon Suk Huh"], "title": "Efficient Adaptive Data Analysis over Dense Distributions", "comment": "23 pages", "summary": "Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\\sqrt{T})$ sample complexity, whereas statistically optimal $O(\\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u9002\u5e94\u6027\u6570\u636e\u5206\u6790\u673a\u5236\uff0c\u5728\u6570\u636e\u5206\u5e03\u76f8\u5bf9\u4e8e\u5df2\u77e5\u5148\u9a8c\u5bc6\u96c6\u7684\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7684O(log T)\u6837\u672c\u590d\u6742\u5ea6\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u7edf\u8ba1\u6700\u4f18\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u5de5\u4f5c\u6d41\u672c\u8d28\u4e0a\u662f\u9002\u5e94\u6027\u7684\uff0c\u53cd\u590d\u67e5\u8be2\u76f8\u540c\u6570\u636e\u96c6\u4ee5\u4f18\u5316\u548c\u9a8c\u8bc1\u987a\u5e8f\u51b3\u7b56\uff0c\u4f46\u8fd9\u79cd\u9002\u5e94\u6027\u53ef\u80fd\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u65e0\u6548\u7edf\u8ba1\u63a8\u65ad\u3002\u9002\u5e94\u6027\u6570\u636e\u5206\u6790\u673a\u5236\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4e0e\u6837\u672c\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u6839\u672c\u6027\u6743\u8861\uff1a\u8ba1\u7b97\u9ad8\u6548\u7b97\u6cd5\u901a\u5e38\u6709\u6b21\u4f18\u7684O(\u221aT)\u6837\u672c\u590d\u6742\u5ea6\uff0c\u800c\u7edf\u8ba1\u6700\u4f18\u7684O(log T)\u7b97\u6cd5\u5728\u6807\u51c6\u5bc6\u7801\u5b66\u5047\u8bbe\u4e0b\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684ADA\u673a\u5236\uff0c\u5f53\u6570\u636e\u5206\u5e03\u76f8\u5bf9\u4e8e\u5df2\u77e5\u5148\u9a8c\u5bc6\u96c6\u65f6\uff0c\u8be5\u673a\u5236\u80fd\u591f\u8fbe\u5230\u6700\u4f18\u7684O(log T)\u6837\u672c\u590d\u6742\u5ea6\u3002\u8fd9\u79cd\u8bbe\u7f6e\u7279\u522b\u5305\u62ec\u5206\u5e03\u7279\u5b9a\u5b66\u4e60\u4e2d\u51fa\u73b0\u7684\u7279\u5f81-\u6807\u7b7e\u6570\u636e\u5206\u5e03\u3002\u8be5\u673a\u5236\u8fd8\u4ea7\u751f\u5206\u5e03\u7279\u5b9a\u8bbe\u7f6e\u4e2d\u7684\u6837\u672c\u9ad8\u6548\u7edf\u8ba1\u67e5\u8be2\u9884\u8a00\u673a\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u6570\u636e\u5206\u5e03\u76f8\u5bf9\u4e8e\u5df2\u77e5\u5148\u9a8c\u5bc6\u96c6\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\u7684\u53cc\u91cd\u76ee\u6807\u3002\u867d\u7136\u7b97\u6cd5\u4e0d\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\uff0c\u4f46\u6ee1\u8db3\u8c13\u8bcd\u5355\u6311\u51fa\u5b89\u5168\u8fd9\u4e00\u677e\u5f1b\u7684\u9690\u79c1\u6982\u5ff5\uff0c\u63ed\u793a\u4e86\u9002\u5e94\u6027\u6570\u636e\u5206\u6790\u4e0e\u9690\u79c1\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\u3002", "conclusion": "\u672c\u6587\u786e\u5b9a\u4e86\u81ea\u7136\u7684\u6570\u636e\u5206\u5e03\u7c7b\u522b\uff0c\u4f7f\u5f97\u8ba1\u7b97\u6548\u7387\u548c\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\u540c\u65f6\u53ef\u8fbe\u6210\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u9002\u5e94\u6027\u6570\u636e\u5206\u6790\u4e0e\u5dee\u5206\u9690\u79c1\u4e4b\u5916\u7684\u5176\u4ed6\u9690\u79c1\u6982\u5ff5\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.07833", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07833", "abs": "https://arxiv.org/abs/2602.07833", "authors": ["Weijiang Lv", "Yaoxuan Feng", "Xiaobo Xia", "Jiayu Wang", "Yan Jing", "Wenchao Chen", "Bo Chen"], "title": "SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models", "comment": "53 pages, 42 figures, 14 tables", "summary": "Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SPD-Faith Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684\u5fe0\u5b9e\u6027\uff0c\u53d1\u73b0\u4e86\u611f\u77e5\u76f2\u70b9\u548c\u611f\u77e5-\u63a8\u7406\u5206\u79bb\u4e24\u79cd\u7cfb\u7edf\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86SAGE\u6846\u67b6\u6765\u6539\u5584\u89c6\u89c9\u8bc1\u636e\u6821\u51c6\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u6765\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u7684\u5fe0\u5b9e\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u5e7b\u89c9\uff0c\u800c\u63a8\u7406\u5c42\u9762\u7684\u4e0d\u5fe0\u5b9e\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165SPD-Faith Bench\u8bca\u65ad\u57fa\u51c6\uff0c\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5dee\u5f02\u63a8\u7406\u6765\u5f3a\u5236\u663e\u5f0f\u89c6\u89c9\u6bd4\u8f83\uff0c\u4ee5\u9694\u79bb\u8bed\u8a00\u5148\u9a8c\u5bf9\u5fe0\u5b9e\u6027\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5206\u6790\u53d1\u73b0\u6545\u969c\u6a21\u5f0f\u6e90\u4e8e\u8870\u51cf\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u548c\u6b8b\u5dee\u6d41\u4e2d\u7684\u8868\u793a\u504f\u79fb\uff0c\u5e76\u63d0\u51fa\u4e86SAGE\u6846\u67b6\u6765\u6539\u5584\u89c6\u89c9\u8def\u7531\u548c\u5bf9\u9f50\u63a8\u7406\u4e0e\u611f\u77e5\u3002", "result": "\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63ed\u793a\u4e86\u4e24\u79cd\u7cfb\u7edf\u6545\u969c\u6a21\u5f0f\uff1a\u611f\u77e5\u76f2\u70b9\u548c\u611f\u77e5-\u63a8\u7406\u5206\u79bb\u3002\u8fd9\u4e9b\u6545\u969c\u6e90\u4e8e\u8870\u51cf\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u548c\u6b8b\u5dee\u6d41\u4e2d\u7684\u8868\u793a\u504f\u79fb\u3002\u63d0\u51fa\u7684SAGE\u6846\u67b6\u80fd\u591f\u6539\u5584\u89c6\u89c9\u8def\u7531\u548c\u5bf9\u9f50\u63a8\u7406\u4e0e\u611f\u77e5\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8d85\u8d8a\u54cd\u5e94\u6b63\u786e\u6027\u6765\u663e\u5f0f\u8bc4\u4f30\u5fe0\u5b9e\u6027\u7684\u91cd\u8981\u6027\u3002\u63d0\u51fa\u7684\u57fa\u51c6\u548cSAGE\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u6539\u5584\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5fe0\u5b9e\u6027\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2602.07835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07835", "abs": "https://arxiv.org/abs/2602.07835", "authors": ["Sanoojan Baliah", "Yohan Abeysinghe", "Rusiru Thushara", "Khan Muhammad", "Abhinav Dhall", "Karthik Nandakumar", "Muhammad Haris Khan"], "title": "VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping", "comment": null, "summary": "We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.", "AI": {"tldr": "VFace\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\uff0c\u53ef\u4e0e\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u901a\u8fc7\u9891\u7387\u8c31\u6ce8\u610f\u529b\u63d2\u503c\u3001\u76ee\u6807\u7ed3\u6784\u5f15\u5bfc\u548c\u6d41\u5f15\u5bfc\u6ce8\u610f\u529b\u65f6\u5e8f\u5e73\u6ed1\u6280\u672f\u63d0\u5347\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u65f6\u5e8f\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u989d\u5916\u7684\u8bad\u7ec3\u6216\u89c6\u9891\u7279\u5b9a\u7684\u5fae\u8c03\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "method": "VFace\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1) \u9891\u7387\u8c31\u6ce8\u610f\u529b\u63d2\u503c\u6280\u672f\uff0c\u4fc3\u8fdb\u751f\u6210\u5e76\u4fdd\u7559\u5173\u952e\u8eab\u4efd\u7279\u5f81\uff1b2) \u76ee\u6807\u7ed3\u6784\u5f15\u5bfc\uff0c\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u6ce8\u610f\u529b\u6ce8\u5165\u66f4\u597d\u5730\u5bf9\u9f50\u76ee\u6807\u5e27\u7684\u7ed3\u6784\u7279\u5f81\uff1b3) \u6d41\u5f15\u5bfc\u6ce8\u610f\u529b\u65f6\u5e8f\u5e73\u6ed1\u673a\u5236\uff0c\u5728\u4e0d\u4fee\u6539\u5e95\u5c42\u6269\u6563\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u9010\u5e27\u751f\u6210\u4e2d\u7684\u65f6\u5e8f\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u4e3a\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6a21\u5757\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u89c6\u9891\u7279\u5b9a\u7684\u5fae\u8c03\u3002", "conclusion": "VFace\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\uff0c\u80fd\u591f\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u65f6\u5e8f\u5e73\u6ed1\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u4e2d\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08783", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08783", "abs": "https://arxiv.org/abs/2602.08783", "authors": ["Zirui Li", "Xuefeng Bai", "Kehai Chen", "Yizhi Li", "Jian Yang", "Chenghua Lin", "Min Zhang"], "title": "Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure", "comment": "22 pages", "summary": "Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u6f5c\u5728\u601d\u7ef4\u94fe\u89c6\u4e3a\u53ef\u64cd\u4f5c\u7684\u56e0\u679c\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u5206\u6790\u6f5c\u5728\u6b65\u9aa4\uff0c\u7814\u7a76\u5176\u5728\u6570\u5b66\u548c\u4e00\u822c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u56e0\u679c\u5fc5\u8981\u6027\u3001\u5f71\u54cd\u4f20\u64ad\u548c\u7b54\u6848\u6a21\u5f0f\u4fdd\u7559", "motivation": "\u73b0\u6709\u6f5c\u5728\u601d\u7ef4\u94fe\u65b9\u6cd5\u4f7f\u7528\u5185\u90e8\u6f5c\u5728\u6b65\u9aa4\u66ff\u4ee3\u663e\u5f0f\u6587\u672c\u63a8\u7406\uff0c\u4f46\u8fd9\u4e9b\u4e2d\u95f4\u8ba1\u7b97\u96be\u4ee5\u901a\u8fc7\u76f8\u5173\u6027\u63a2\u6d4b\u4e4b\u5916\u7684\u65b9\u5f0f\u8fdb\u884c\u8bc4\u4f30\u3002\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u5206\u6790\u8fd9\u4e9b\u6f5c\u5728\u63a8\u7406\u8fc7\u7a0b", "method": "\u5c06\u6f5c\u5728\u601d\u7ef4\u94fe\u5efa\u6a21\u4e3a\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u5c06\u6f5c\u5728\u6b65\u9aa4\u4f5c\u4e3a\u53d8\u91cf\uff0c\u901a\u8fc7\u9010\u6b65\u5e72\u9884\u5206\u6790\u5176\u56e0\u679c\u6548\u5e94\u3002\u7814\u7a76Coconut\u548cCODI\u4e24\u79cd\u4ee3\u8868\u6027\u8303\u5f0f\u5728\u6570\u5b66\u548c\u4e00\u822c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528", "result": "\u53d1\u73b0\u6f5c\u5728\u6b65\u9aa4\u9884\u7b97\u4e0d\u50cf\u540c\u8d28\u7684\u989d\u5916\u6df1\u5ea6\uff0c\u66f4\u50cf\u5177\u6709\u975e\u5c40\u90e8\u8def\u7531\u7684\u5206\u9636\u6bb5\u529f\u80fd\uff1b\u65e9\u671f\u8f93\u51fa\u504f\u5dee\u4e0e\u665a\u671f\u8868\u793a\u627f\u8bfa\u4e4b\u95f4\u5b58\u5728\u6301\u7eed\u5dee\u8ddd\uff1b\u4e2d\u95f4\u8f68\u8ff9\u4fdd\u7559\u7ade\u4e89\u7b54\u6848\u6a21\u5f0f", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u57fa\u4e8e\u6a21\u5f0f\u6761\u4ef6\u548c\u7a33\u5b9a\u6027\u611f\u77e5\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u8bad\u7ec3/\u89e3\u7801\u76ee\u6807\uff0c\u4f5c\u4e3a\u89e3\u91ca\u548c\u6539\u8fdb\u6f5c\u5728\u63a8\u7406\u7cfb\u7edf\u7684\u66f4\u53ef\u9760\u5de5\u5177"}}
{"id": "2602.07738", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07738", "abs": "https://arxiv.org/abs/2602.07738", "authors": ["Sunil Madhow", "Yuchen Liang", "Ness Shroff", "Yingbin Liang", "Yu-Xiang Wang"], "title": "Learnable Chernoff Baselines for Inference-Time Alignment", "comment": null, "summary": "We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.", "AI": {"tldr": "\u63d0\u51faLCB\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5207\u5c14\u8bfa\u592b\u57fa\u7ebf\u9ad8\u6548\u8fd1\u4f3c\u91c7\u6837\uff0c\u5b9e\u73b0\u5956\u52b1\u5f15\u5bfc\u7684\u751f\u6210\u6a21\u578b\u5bf9\u9f50\uff0c\u51cf\u5c11\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67e5\u8be2\u6b21\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7279\u5b9a\u67b6\u6784\u8c03\u6574\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u65f6\u5956\u52b1\u5f15\u5bfc\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u5207\u5c14\u8bfa\u592b\u57fa\u7ebf(LCBs)\uff0c\u4ec5\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9ed1\u76d2\u91c7\u6837\u8bbf\u95ee\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u63a5\u53d7\u6982\u7387\u7684\u62d2\u7edd\u91c7\u6837\u5b9e\u73b0KL\u6b63\u5219\u5316\u5956\u52b1\u5bf9\u9f50\u3002", "result": "\u5efa\u7acb\u4e86\u4e0e\u7406\u60f3\u5bf9\u9f50\u6a21\u578b\u7684\u603b\u53d8\u5dee\u4fdd\u8bc1\uff0c\u5728\u8fde\u7eed\u548c\u79bb\u6563\u6269\u6563\u8bbe\u7f6e\u4e2d\uff0cLCB\u91c7\u6837\u4e0e\u7406\u60f3\u62d2\u7edd\u91c7\u6837\u5339\u914d\u826f\u597d\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u67e5\u8be2\u6b21\u6570\u3002", "conclusion": "LCB\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u63a8\u7406\u65f6\u5956\u52b1\u5f15\u5bfc\u5bf9\u9f50\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u62d2\u7edd\u91c7\u6837\u5b9e\u73b0\u5bf9\u63a8\u7406\u8ba1\u7b97\u89c4\u6a21\u7684\u7cbe\u7ec6\u63a7\u5236\u3002"}}
{"id": "2602.07854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07854", "abs": "https://arxiv.org/abs/2602.07854", "authors": ["Chendong Xiang", "Jiajun Liu", "Jintao Zhang", "Xiao Yang", "Zhengwei Fang", "Shizun Wang", "Zijun Wang", "Yingtian Zou", "Hang Su", "Jun Zhu"], "title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model", "comment": null, "summary": "Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \\textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \\textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \\textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.", "AI": {"tldr": "ViewRope\uff1a\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u89c6\u9891Transformer\u7f16\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u76f8\u673a\u5c04\u7ebf\u65b9\u5411\u76f4\u63a5\u6ce8\u5165\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u89e3\u51b3\u4e86\u4e16\u754c\u6a21\u578b\u4e2d\u957f\u671f\u7a7a\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u51e0\u4f55\u6f02\u79fb\u548c\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u9884\u6d4b\u6027\u4e16\u754c\u6a21\u578b\u7f3a\u4e4f\u7a7a\u95f4\u6301\u4e45\u6027\uff0c\u5728\u957f\u8f68\u8ff9\u4e2d\u65e0\u6cd5\u4fdd\u6301\u7a33\u5b9a\u7684\u573a\u666f\u7ed3\u6784\uff0c\u5f53\u76f8\u673a\u91cd\u65b0\u8bbf\u95ee\u5148\u524d\u89c2\u5bdf\u4f4d\u7f6e\u65f6\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u7ec6\u8282\u3002\u8fd9\u79cd\u51e0\u4f55\u6f02\u79fb\u6e90\u4e8e\u5bf9\u5c4f\u5e55\u7a7a\u95f4\u4f4d\u7f6e\u5d4c\u5165\u7684\u4f9d\u8d56\uff0c\u8fd9\u4e0e3D\u4e00\u81f4\u6027\u6240\u9700\u7684\u6295\u5f71\u51e0\u4f55\u76f8\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86ViewRope\u51e0\u4f55\u611f\u77e5\u7f16\u7801\uff0c\u5c06\u76f8\u673a\u5c04\u7ebf\u65b9\u5411\u76f4\u63a5\u6ce8\u5165\u89c6\u9891Transformer\u81ea\u6ce8\u610f\u529b\u5c42\uff1b\u901a\u8fc7\u76f8\u5bf9\u5c04\u7ebf\u51e0\u4f55\u800c\u975e\u50cf\u7d20\u5c40\u90e8\u6027\u53c2\u6570\u5316\u6ce8\u610f\u529b\uff1b\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u5e27\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u5229\u7528\u51e0\u4f55\u7ebf\u7d22\u9009\u62e9\u6027\u5173\u6ce8\u76f8\u5173\u5386\u53f2\u5e27\uff1b\u5f00\u53d1\u4e86ViewBench\u8bca\u65ad\u5957\u4ef6\u6d4b\u91cf\u95ed\u73af\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u6f02\u79fb\u3002", "result": "ViewRope\u663e\u8457\u6539\u5584\u4e86\u957f\u671f\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u5728ViewBench\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u95ed\u73af\u4fdd\u771f\u5ea6\u548c\u66f4\u5c11\u7684\u51e0\u4f55\u6f02\u79fb\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0cViewRope\u4e3a\u89c6\u9891Transformer\u63d0\u4f9b\u4e863D\u4e00\u81f4\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9884\u6d4b\u6027\u4e16\u754c\u6a21\u578b\u4e2d\u7684\u7a7a\u95f4\u6301\u4e45\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u4ea4\u4e92\u5f0fAI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.08796", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08796", "abs": "https://arxiv.org/abs/2602.08796", "authors": ["Kevin Fan", "Jacquelyn A. Bialo", "Hongli Li"], "title": "The Use of AI Tools to Develop and Validate Q-Matrices", "comment": "An earlier version of this study was presented at the Psychometric Society Meeting held in July 2025 in Minneapolis, USA", "summary": "Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.", "AI": {"tldr": "AI\u5de5\u5177\u5728\u8ba4\u77e5\u8bca\u65ad\u5efa\u6a21\u4e2dQ\u77e9\u9635\u6784\u5efa\u7684\u5e94\u7528\u7814\u7a76\uff1a\u6bd4\u8f83\u4e0d\u540cAI\u6a21\u578b\u751f\u6210\u7684Q\u77e9\u9635\u4e0e\u5df2\u9a8c\u8bc1Q\u77e9\u9635\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0AI\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u4e14\u968f\u65f6\u95f4\u53d8\u5316", "motivation": "Q\u77e9\u9635\u6784\u5efa\u662f\u8ba4\u77e5\u8bca\u65ad\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u4f46\u52b3\u52a8\u5bc6\u96c6\u578b\u6b65\u9aa4\uff0c\u7814\u7a76\u63a2\u7d22AI\u5de5\u5177\uff08\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff09\u662f\u5426\u80fd\u652f\u6301Q\u77e9\u9635\u5f00\u53d1\uff0c\u51cf\u8f7b\u4e13\u5bb6\u8d1f\u62c5", "method": "\u5411\u591a\u4e2aAI\u6a21\u578b\u63d0\u4f9b\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u540c\u7684\u8bad\u7ec3\u6750\u6599\uff0c\u6bd4\u8f83AI\u751f\u6210\u7684Q\u77e9\u9635\u4e0eLi\u548cSuen\uff082013\uff09\u5df2\u9a8c\u8bc1\u7684\u9605\u8bfb\u7406\u89e3\u6d4b\u8bd5Q\u77e9\u9635\u7684\u4e00\u81f4\u6027\uff0c\u4f7f\u7528Cohen's kappa\u8bc4\u4f30\u4e00\u81f4\u6027\uff0c\u5e76\u57282026\u5e741\u6708\u4f7f\u7528\u66f4\u65b0\u7684AI\u7248\u672c\u8fdb\u884c\u540e\u7eed\u5206\u6790", "result": "\u4e0d\u540cAI\u6a21\u578b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0cGoogle Gemini 2.5 Pro\u4e0e\u5df2\u9a8c\u8bc1Q\u77e9\u9635\u7684\u4e00\u81f4\u6027\u6700\u9ad8\uff08Kappa = 0.63\uff09\uff0c\u8d85\u8fc7\u6240\u6709\u4eba\u7c7b\u4e13\u5bb6\uff1b\u4f462026\u5e74\u4f7f\u7528\u65b0\u7248AI\u7684\u5206\u6790\u663e\u793a\u4e0e\u5df2\u9a8c\u8bc1Q\u77e9\u9635\u7684\u4e00\u81f4\u6027\u964d\u4f4e", "conclusion": "AI\u5de5\u5177\u5728Q\u77e9\u9635\u6784\u5efa\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u8868\u73b0\u5b58\u5728\u6a21\u578b\u95f4\u5dee\u5f02\u4e14\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76AI\u5728\u8ba4\u77e5\u8bca\u65ad\u5efa\u6a21\u4e2d\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027"}}
{"id": "2602.07860", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.07860", "abs": "https://arxiv.org/abs/2602.07860", "authors": ["Fei Yu", "Shudan Guo", "Shiqing Xin", "Beibei Wang", "Haisen Zhao", "Wenzheng Chen"], "title": "Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images", "comment": "Accepted by 3DV 2026. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/", "summary": "We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.\n  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.\n  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u8d85\u9ad8\u901f\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u4e2d\u6062\u590d3D\u5f62\u72b6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u91cd\u5fc3\u5750\u6807\u6c42\u89e3\u5668\u5b9e\u73b0\u9ad8\u6548\u53ef\u5fae\u6e32\u67d3\uff0c\u5728\u5e73\u79fb\u548c\u65cb\u8f6c\u4e24\u79cd\u5178\u578b\u8fd0\u52a8\u7c7b\u578b\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf3D\u91cd\u5efa\u65b9\u6cd5\uff08\u5982\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\uff09\u5728\u5904\u7406\u8d85\u9ad8\u901f\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u65f6\u5931\u6548\uff0c\u800c\u8fd9\u7c7b\u573a\u666f\u5728\u4f53\u80b2\uff08\u5982\u5feb\u901f\u79fb\u52a8\u7684\u7403\u4f53\uff09\u548c\u5de5\u4e1a\uff08\u5982\u65cb\u8f6c\u673a\u68b0\uff09\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u5305\u542b\u5feb\u901f\u91cd\u5fc3\u5750\u6807\u6c42\u89e3\u5668\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff08\u52a0\u901f\u8fbe4.57\u500d\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u5fae\u5206\u7684\u8d85\u9ad8\u901f\u8fd0\u52a8\u6a21\u62df\uff0c\u652f\u6301\u4ece\u6e32\u67d3\u56fe\u50cf\u52303D\u5f62\u72b6\u7684\u68af\u5ea6\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u5feb\u901f\u5e73\u79fb\u548c\u65cb\u8f6c\u4e24\u79cd\u8fd0\u52a8\u7c7b\u578b\u4e0a\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u771f\u5b9e\u5730\u6a21\u62df\u8d85\u9ad8\u901f\u8fd0\u52a8\u7269\u4f53\uff0c\u5e76\u6210\u529f\u4ece\u6781\u7aef\u5e73\u79fb\u548c\u65cb\u8f6c\u8fd0\u52a8\u76842D\u56fe\u50cf\u4e2d\u6062\u590d3D\u5f62\u72b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u57fa\u4e8e\u89c6\u89c9\u76843D\u91cd\u5efa\u8fb9\u754c\uff0c\u4e3a\u4ece\u8d85\u9ad8\u901f\u8fd0\u52a8\u6a21\u7cca\u56fe\u50cf\u4e2d\u6062\u590d\u51e0\u4f55\u5f62\u72b6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u81ea\u7136\u548c\u5de5\u4e1a\u573a\u666f\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.08804", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08804", "abs": "https://arxiv.org/abs/2602.08804", "authors": ["Liming Zhou", "Ailing Liu", "Hongwei Liu", "Min He", "Heng Zhang"], "title": "Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures", "comment": null, "summary": "Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.", "AI": {"tldr": "RC-LLM\uff1a\u57fa\u4e8e\u6b8b\u5dee\u8fde\u63a5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u670d\u52a1\u6839\u56e0\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6e90\u9065\u6d4b\u6570\u636e\u878d\u5408\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u63d0\u5347\u6545\u969c\u5206\u6790\u51c6\u786e\u6027", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u6839\u56e0\u5b9a\u4f4d\u9762\u4e34\u6311\u6218\uff0c\u590d\u6742\u6545\u969c\u4f20\u64ad\u548c\u9ad8\u7ef4\u9065\u6d4b\u6570\u636e\uff08\u6307\u6807\u3001\u65e5\u5fd7\u3001\u8ffd\u8e2a\uff09\u9650\u5236\u4e86\u73b0\u6709RCA\u65b9\u6cd5\u7684\u6709\u6548\u6027", "method": "\u63d0\u51faRC-LLM\u65b9\u6cd5\uff1a1\uff09\u8bbe\u8ba1\u6b8b\u5dee\u5f0f\u5206\u5c42\u878d\u5408\u7ed3\u6784\u6574\u5408\u591a\u6e90\u9065\u6d4b\u6570\u636e\uff1b2\uff09\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u5efa\u6a21\u65f6\u5e8f\u548c\u8de8\u5fae\u670d\u52a1\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb", "result": "\u5728CCF-AIOps\u5fae\u670d\u52a1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRC-LLM\u5728\u6839\u56e0\u5206\u6790\u4e2d\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387", "conclusion": "RC-LLM\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u6b8b\u5dee\u8fde\u63a5\u7ed3\u6784\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u6839\u56e0\u5b9a\u4f4d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6545\u969c\u5206\u6790\u7684\u6027\u80fd"}}
{"id": "2602.07864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07864", "abs": "https://arxiv.org/abs/2602.07864", "authors": ["Chen Yang", "Guanxin Lin", "Youquan He", "Peiyao Chen", "Guanghe Liu", "Yufan Mo", "Zhouyuan Xu", "Linhao Wang", "Guohui Zhang", "Zihang Zhang", "Shenxiang Zeng", "Chen Wang", "Jiansheng Fan"], "title": "Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds", "comment": null, "summary": "Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.", "AI": {"tldr": "SSI-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u53d7\u7ea6\u675f\u6d41\u5f62\u4e0a\u7684\u7a7a\u95f4\u63a8\u7406\uff0c\u5305\u542b1000\u4e2a\u6392\u5e8f\u95ee\u9898\uff0c\u6db5\u76d6\u51e0\u4f55\u548c\u62d3\u6251\u63a8\u7406\uff0c\u9700\u8981\u591a\u79cd\u7a7a\u95f4\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u8bc4\u4f30\u65e0\u7ea6\u675f\u573a\u666f\uff0c\u6a21\u578b\u53ef\u4ee5\u5229\u75282D\u6377\u5f84\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u7269\u7406\u4e16\u754c\u4e2d\u53d7\u51e0\u4f55\u3001\u62d3\u6251\u548c\u7269\u7406\u7ea6\u675f\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5b8c\u5168\u4eba\u5de5\u4e2d\u5fc3\u7684\u6d41\u7a0b\u521b\u5efa\uff1a10\u540d\u7814\u7a76\u4eba\u5458\u82b1\u8d39400\u591a\u5c0f\u65f6\u7cbe\u5fc3\u6311\u9009\u56fe\u50cf\u3001\u6807\u6ce8\u7ed3\u6784\u7ec4\u4ef6\u3001\u8bbe\u8ba1\u95ee\u9898\u4ee5\u6700\u5c0f\u5316\u50cf\u7d20\u7ea7\u7ebf\u7d22\uff0c\u6784\u5efa\u5305\u542b1000\u4e2a\u6392\u5e8f\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f3031\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684VLM\u663e\u793a\u4e0e\u4eba\u7c7b\u5b58\u5728\u5de8\u5927\u5dee\u8ddd\uff1a\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u51c6\u786e\u738722.2%\uff0c\u6700\u5f3a\u95ed\u6e90\u6a21\u578b33.6%\uff0c\u800c\u4eba\u7c7b\u5f97\u520691.6%\u3002\u9f13\u52b1\u6a21\u578b\u601d\u8003\u4ec5\u5e26\u6765\u8fb9\u9645\u6536\u76ca\uff0c\u9519\u8bef\u5206\u6790\u663e\u793a\u6a21\u578b\u5728\u7ed3\u6784\u57fa\u7840\u548c\u7ea6\u675f\u4e00\u81f4\u76843D\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5931\u8d25\u3002", "conclusion": "SSI-Bench\u63ed\u793a\u4e86\u5f53\u524dVLM\u5728\u53d7\u7ea6\u675f\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u4e25\u91cd\u4e0d\u8db3\uff0c\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2602.08815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08815", "abs": "https://arxiv.org/abs/2602.08815", "authors": ["Yanglei Gan", "Peng He", "Yuxiang Cai", "Run Lin", "Guanyu Zhou", "Qiao Liu"], "title": "Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation", "comment": null, "summary": "Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.", "AI": {"tldr": "NADEx\u662f\u4e00\u79cd\u7528\u4e8e\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u8d1f\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u8d1f\u6837\u672c\u4fe1\u606f\u548c\u4f59\u5f26\u5bf9\u9f50\u6b63\u5219\u5316\u5668\uff0c\u5728\u56db\u4e2a\u516c\u5f00\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u6269\u6563\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u751f\u6210\u8def\u5f84\u4ec5\u57fa\u4e8e\u6b63\u6837\u672c\u8bc1\u636e\uff0c\u5ffd\u7565\u4e86\u4fe1\u606f\u4e30\u5bcc\u7684\u8d1f\u6837\u672c\u4e0a\u4e0b\u6587\uff1b2) \u8bad\u7ec3\u76ee\u6807\u4e3b\u8981\u4f9d\u8d56\u4ea4\u53c9\u71b5\u6392\u5e8f\uff0c\u867d\u7136\u6539\u8fdb\u4e86\u5019\u9009\u6392\u5e8f\u4f46\u5bf9\u53bb\u566a\u5d4c\u5165\u7684\u6821\u51c6\u76d1\u7763\u4e0d\u8db3\u3002", "method": "NADEx\u91c7\u7528\u8d1f\u611f\u77e5\u6269\u6563\u6a21\u578b\uff0c\u5c06\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u65f6\u5e8f\u95f4\u9694\u7684\u4e3b\u4f53\u4e2d\u5fc3\u5386\u53f2\u7f16\u7801\u4e3a\u5e8f\u5217\u5d4c\u5165\u3002\u5728\u6b63\u5411\u8fc7\u7a0b\u4e2d\u6270\u52a8\u67e5\u8be2\u5bf9\u8c61\uff0c\u5728\u53cd\u5411\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u57fa\u4e8e\u65f6\u5e8f\u5173\u7cfb\u4e0a\u4e0b\u6587\u7684Transformer\u53bb\u566a\u5668\u8fdb\u884c\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u4ece\u6279\u6b21\u8d1f\u6837\u672c\u539f\u578b\u63a8\u5bfc\u51fa\u4f59\u5f26\u5bf9\u9f50\u6b63\u5219\u5316\u5668\uff0c\u6536\u7d27\u51b3\u7b56\u8fb9\u754c\u4ee5\u6392\u9664\u4e0d\u5408\u7406\u5019\u9009\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cNADEx\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "NADEx\u901a\u8fc7\u7ed3\u5408\u8d1f\u6837\u672c\u4fe1\u606f\u548c\u6539\u8fdb\u7684\u6821\u51c6\u76d1\u7763\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.07790", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07790", "abs": "https://arxiv.org/abs/2602.07790", "authors": ["Wanyun Xie", "Francesco Tonin", "Volkan Cevher"], "title": "MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training", "comment": null, "summary": "Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.", "AI": {"tldr": "MaD-Mix\u662f\u4e00\u4e2a\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8bbe\u8ba1\u591a\u6a21\u6001\u6570\u636e\u6df7\u5408\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u7684\u57df\u5bf9\u9f50\u6700\u5927\u5316\u65b9\u6cd5\uff0c\u81ea\u52a8\u4f18\u5316\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\uff0c\u51cf\u5c11\u4eba\u5de5\u8c03\u53c2\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u8c03\u53c2\u6765\u786e\u5b9a\u591a\u6a21\u6001\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u573a\u666f\uff08\u5982\u56fe\u50cf-\u6587\u672c-\u89c6\u9891\uff09\u65f6\uff0c\u624b\u52a8\u8c03\u4f18\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002", "method": "MaD-Mix\u5c06\u6570\u636e\u6df7\u5408\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6a21\u6001\u611f\u77e5\u7684\u57df\u5bf9\u9f50\u6700\u5927\u5316\uff0c\u901a\u8fc7Fenchel\u5bf9\u5076\u548c\u8de8\u6a21\u6001\u8026\u5408\u53d8\u91cf\u83b7\u5f97\u95ed\u5f0f\u591a\u6a21\u6001\u5bf9\u9f50\u5206\u6570\uff0c\u7cfb\u7edf\u5904\u7406\u7f3a\u5931\u6a21\u6001\u7684\u57df\uff0c\u652f\u6301\u7eaf\u8bed\u8a00\u57df\u7684\u96c6\u6210\u3002", "result": "\u57280.5B\u548c7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMaD-Mix\u80fd\u52a0\u901fVLM\u8bad\u7ec3\uff0c\u5728\u56fe\u50cf-\u6587\u672c\u6307\u4ee4\u8c03\u4f18\u4e2d\u6bd4\u4eba\u5de5\u8c03\u4f18\u8282\u770122%\u8bad\u7ec3\u6b65\u6570\uff0c\u5728\u590d\u6742\u4e09\u6a21\u6001\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u5e73\u5747\u51c6\u786e\u7387\uff0c\u6df7\u5408\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\uff08<1 GPU\u5c0f\u65f6\uff09\u3002", "conclusion": "MaD-Mix\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6570\u636e\u6df7\u5408\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u66ff\u4ee3\u6602\u8d35\u7684\u4eba\u5de5\u8c03\u4f18\uff0c\u4e3a\u73b0\u4ee3VLM\u8bad\u7ec3\u6d41\u7a0b\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07872", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07872", "abs": "https://arxiv.org/abs/2602.07872", "authors": ["Mert Sonmezer", "Serge Vasylechko", "Duygu Atasoy", "Seyda Ertekin", "Sila Kurugol"], "title": "WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning", "comment": null, "summary": "Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.", "AI": {"tldr": "WristMIR\uff1a\u57fa\u4e8e\u533a\u57df\u611f\u77e5\u7684\u513f\u79d1\u8155\u90e8X\u5149\u7247\u68c0\u7d22\u6846\u67b6\uff0c\u5229\u7528\u5bc6\u96c6\u653e\u5c04\u5b66\u62a5\u544a\u548c\u9aa8\u9abc\u7279\u5b9a\u5b9a\u4f4d\u5b66\u4e60\u7ec6\u7c92\u5ea6\u56fe\u50cf\u8868\u793a\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u9aa8\u6298\u6a21\u5f0f\u68c0\u7d22\u548c\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u8155\u90e8X\u5149\u7247\u4e2d\u9aa8\u6298\u6a21\u5f0f\u7684\u68c0\u7d22\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e34\u5e8a\u91cd\u8981\u7ebf\u7d22\u7ec6\u5fae\u3001\u9ad8\u5ea6\u5c40\u90e8\u5316\uff0c\u5e38\u88ab\u91cd\u53e0\u89e3\u5256\u7ed3\u6784\u6216\u4e0d\u540c\u6210\u50cf\u89c6\u89d2\u906e\u6321\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6807\u6ce8\u7684\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u6570\u636e\u96c6\u4e5f\u9650\u5236\u4e86\u8fdb\u5c55\u3002", "method": "WristMIR\u91c7\u7528\u533a\u57df\u611f\u77e5\u6846\u67b6\uff1a1\uff09\u4f7f\u7528MedGemma\u7ed3\u6784\u5316\u62a5\u544a\u6316\u6398\u751f\u6210\u5168\u5c40\u548c\u533a\u57df\u7ea7\u63cf\u8ff0\uff1b2\uff09\u5904\u7406\u8155\u90e8\u56fe\u50cf\u548c\u9aa8\u9abc\u7279\u5b9a\u88c1\u526a\uff08\u8fdc\u7aef\u6861\u9aa8\u3001\u8fdc\u7aef\u5c3a\u9aa8\u3001\u5c3a\u9aa8\u830e\u7a81\uff09\uff1b3\uff09\u8054\u5408\u8bad\u7ec3\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u7f16\u7801\u5668\uff1b4\uff09\u6267\u884c\u4e24\u9636\u6bb5\u68c0\u7d22\uff1a\u7c97\u7c92\u5ea6\u5168\u5c40\u5339\u914d\u5019\u9009\u68c0\u67e5\uff0c\u7136\u540e\u57fa\u4e8e\u89e3\u5256\u9aa8\u9abc\u533a\u57df\u7684\u533a\u57df\u6761\u4ef6\u91cd\u6392\u5e8f\u3002", "result": "WristMIR\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\uff1a\u56fe\u50cf\u5230\u6587\u672cRecall@5\u4ece0.82%\u63d0\u5347\u81f39.35%\uff1b\u5d4c\u5165\u8868\u793a\u5728\u9aa8\u6298\u5206\u7c7b\u4e0a\u8868\u73b0\u66f4\u5f3a\uff08AUROC 0.949\uff0cAUPRC 0.953\uff09\uff1b\u533a\u57df\u611f\u77e5\u8bc4\u4f30\u4e2d\uff0c\u4e24\u9636\u6bb5\u8bbe\u8ba1\u5c06\u57fa\u4e8e\u68c0\u7d22\u7684\u9aa8\u6298\u8bca\u65ad\u5e73\u5747F1\u4ece0.568\u63d0\u5347\u81f30.753\uff1b\u653e\u5c04\u79d1\u533b\u751f\u8bc4\u4ef7\u5176\u68c0\u7d22\u75c5\u4f8b\u4e34\u5e8a\u76f8\u5173\u6027\u66f4\u9ad8\uff0c\u5e73\u5747\u8bc4\u5206\u4ece3.36\u63d0\u5347\u81f34.35\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u57fa\u4e8e\u89e3\u5256\u5f15\u5bfc\u7684\u68c0\u7d22\u5728\u589e\u5f3a\u513f\u79d1\u808c\u8089\u9aa8\u9abc\u6210\u50cf\u8bca\u65ad\u63a8\u7406\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u4eba\u5de5\u56fe\u50cf\u7ea7\u6807\u6ce8\uff0c\u901a\u8fc7\u5229\u7528\u5bc6\u96c6\u653e\u5c04\u5b66\u62a5\u544a\u548c\u9aa8\u9abc\u7279\u5b9a\u5b9a\u4f4d\u5b66\u4e60\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u7ec6\u7c92\u5ea6\u8868\u793a\u3002"}}
{"id": "2602.07798", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07798", "abs": "https://arxiv.org/abs/2602.07798", "authors": ["Ruiqi Wang", "Ruikang Liu", "Runyu Chen", "Haoxiang Suo", "Zhiyi Peng", "Zhuo Tang", "Changjian Chen"], "title": "CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection", "comment": null, "summary": "Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.", "AI": {"tldr": "CausalTaD\u662f\u4e00\u79cd\u5c06\u56e0\u679c\u77e5\u8bc6\u6ce8\u5165LLMs\u8fdb\u884c\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5217\u95f4\u56e0\u679c\u5173\u7cfb\u5e76\u91cd\u65b0\u6392\u5e8f\uff0c\u518d\u5bf9\u4e0d\u540c\u5217\u8fdb\u884c\u52a0\u6743\uff0c\u572830\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5c06\u8868\u683c\u6570\u636e\u8f6c\u6362\u4e3a\u6587\u672c\u65f6\u968f\u673a\u6392\u5217\u5217\u987a\u5e8f\uff0c\u5ffd\u7565\u4e86\u5217\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u800c\u8fd9\u5bf9\u51c6\u786e\u68c0\u6d4b\u5f02\u5e38\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9996\u5148\u8bc6\u522b\u5217\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u5e76\u91cd\u65b0\u6392\u5e8f\u4ee5\u5bf9\u9f50\u8fd9\u4e9b\u5173\u7cfb\uff08\u5efa\u6a21\u4e3a\u7ebf\u6027\u6392\u5e8f\u95ee\u9898\uff09\uff0c\u7136\u540e\u63d0\u51fa\u91cd\u52a0\u6743\u7b56\u7565\u4e3a\u4e0d\u540c\u5217\u5206\u914d\u4e0d\u540c\u6743\u91cd\u4ee5\u589e\u5f3a\u6548\u679c\u3002", "result": "\u572830\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u56e0\u679c\u77e5\u8bc6\u6ce8\u5165LLMs\u8fdb\u884c\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\uff0cCausalTaD\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8003\u8651\u5217\u95f4\u56e0\u679c\u5173\u7cfb\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.07891", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07891", "abs": "https://arxiv.org/abs/2602.07891", "authors": ["Zihui Gao", "Ke Liu", "Donny Y. Chen", "Duochao Shi", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video", "comment": null, "summary": "Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u4ece\u539f\u59cb\u89c6\u9891\u6d41\u4e2d\u53ef\u6269\u5c55\u9002\u5e94\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u6316\u6398\u7ba1\u9053\u5c06\u89c6\u9891\u8f6c\u5316\u4e3a\u8bad\u7ec3\u8f68\u8ff9\uff0c\u7ed3\u5408\u7a00\u758f\u51e0\u4f55\u951a\u70b9\u548c\u5bc6\u96c6\u53ef\u5fae\u5206\u4e00\u81f4\u6027\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u57283D\u91cd\u5efa\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5176\u53d1\u5c55\u53d7\u5230\u591a\u6837\u5316\u3001\u5927\u89c4\u6a213D\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u4e25\u91cd\u5236\u7ea6\u3002\u867d\u7136\u4e92\u8054\u7f51\u89c6\u9891\u63d0\u4f9b\u4e86\u51e0\u4e4e\u65e0\u9650\u7684\u539f\u59cb\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u51e0\u4f55\u4fe1\u606f\u548c\u5b58\u5728\u89c2\u6d4b\u566a\u58f0\uff0c\u5c06\u5176\u4f5c\u4e3a\u51e0\u4f55\u5b66\u4e60\u7684\u6269\u5c55\u6e90\u5177\u6709\u6311\u6218\u6027\u3002", "method": "SAGE\u91c7\u7528\u5c42\u6b21\u5316\u6316\u6398\u7ba1\u9053\u5c06\u89c6\u9891\u8f6c\u5316\u4e3a\u8bad\u7ec3\u8f68\u8ff9\uff0c\u7ed3\u5408\u4e09\u79cd\u76d1\u7763\u65b9\u5f0f\uff1a(1)\u4fe1\u606f\u4e30\u5bcc\u7684\u8bad\u7ec3\u8f68\u8ff9\u9009\u62e9\uff1b(2)\u901a\u8fc7SfM\u70b9\u4e91\u8fdb\u884c\u7a00\u758f\u51e0\u4f55\u951a\u70b9\uff0c\u63d0\u4f9b\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\uff1b(3)\u901a\u8fc73D\u9ad8\u65af\u6e32\u67d3\u5b9e\u73b0\u5bc6\u96c6\u53ef\u5fae\u5206\u4e00\u81f4\u6027\uff0c\u63d0\u4f9b\u591a\u89c6\u89d2\u7ea6\u675f\u3002\u4e3a\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u951a\u70b9\u6570\u636e\u7684\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u663e\u8457\u589e\u5f3a\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u57fa\u51c6\u6d4b\u8bd5\uff087Scenes\u3001TUM-RGBD\u3001Matterport3D\uff09\u4e0a\uff0cChamfer\u8ddd\u79bb\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e8620-42%\u3002", "conclusion": "SAGE\u5f00\u521b\u4e86\u901a\u8fc7\u4e92\u8054\u7f51\u89c6\u9891\u9002\u5e94\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u5148\u6cb3\uff0c\u4e3a\u901a\u75283D\u5b66\u4e60\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u89e3\u51b3\u4e863D\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6838\u5fc3\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2602.08848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08848", "abs": "https://arxiv.org/abs/2602.08848", "authors": ["Quentin Cohen-Solal", "Alexandre Niveau", "Maroua Bouzid"], "title": "Deciding the Satisfiability of Combined Qualitative Constraint Networks", "comment": null, "summary": "Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u591a\u79cd\u5b9a\u6027\u63a8\u7406\u7684\u6269\u5c55\u548c\u7ec4\u5408\u5f62\u5f0f\uff0c\u5305\u62ec\u591a\u5c3a\u5ea6\u63a8\u7406\u3001\u65f6\u95f4\u5e8f\u5217\u548c\u677e\u6563\u96c6\u6210\uff0c\u5e76\u7814\u7a76\u5176\u53ef\u6ee1\u8db3\u6027\u51b3\u7b56\u7684\u590d\u6742\u6027\u3002", "motivation": "\u5b9a\u6027\u63a8\u7406\u80fd\u591f\u5728\u4fe1\u606f\u4e0d\u7cbe\u786e\u3001\u4e0d\u5b8c\u6574\u4e14\u65e0\u6570\u503c\u7684\u60c5\u51b5\u4e0b\u63a8\u65ad\u65b0\u77e5\u8bc6\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5404\u79cd\u6269\u5c55\u548c\u7ec4\u5408\u5f62\u5f0f\u7684\u7edf\u4e00\u5904\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7edf\u4e00\u5904\u7406\u591a\u5c3a\u5ea6\u63a8\u7406\u3001\u65f6\u95f4\u5e8f\u5217\u548c\u677e\u6563\u96c6\u6210\u7b49\u5b9a\u6027\u5f62\u5f0f\u4e3b\u4e49\u7684\u6269\u5c55\u548c\u7ec4\u5408\uff0c\u5e76\u5206\u6790\u5176\u53ef\u6ee1\u8db3\u6027\u51b3\u7b56\u7684\u590d\u6742\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u4e24\u4e2a\u4e92\u8865\u5b9a\u7406\uff0c\u4fdd\u8bc1\u53ef\u6ee1\u8db3\u6027\u51b3\u7b56\u662f\u591a\u9879\u5f0f\u65f6\u95f4\u7684\uff0c\u5e76\u7528\u5b83\u4eec\u6062\u590d\u4e86\u5c3a\u5bf8-\u62d3\u6251\u7ec4\u5408\u7684\u5df2\u77e5\u7ed3\u679c\uff0c\u540c\u65f6\u6269\u5c55\u4e86\u5b9a\u6027\u5f62\u5f0f\u4e3b\u4e49\u7684\u4e3b\u8981\u5b9a\u4e49\u4ee5\u5305\u542b\u6587\u732e\u4e2d\u6392\u9664\u7684\u91cd\u8981\u7ec4\u5408\u5f62\u5f0f\u3002", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u4e0d\u4ec5\u652f\u6301\u5728\u5404\u79cd\u6269\u5c55\u548c\u7ec4\u5408\u5f62\u5f0f\u4e0b\u8fdb\u884c\u63a8\u7406\uff0c\u8fd8\u4e3a\u7814\u7a76\u53ef\u6ee1\u8db3\u6027\u51b3\u7b56\u53ca\u5176\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5b9a\u6027\u63a8\u7406\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2602.07799", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07799", "abs": "https://arxiv.org/abs/2602.07799", "authors": ["Ching Lam Choi", "Vighnesh Subramaniam", "Phillip Isola", "Antonio Torralba", "Stefanie Jegelka"], "title": "Fairness Aware Reward Optimization", "comment": null, "summary": "Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.", "AI": {"tldr": "Faro\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u6ee1\u8db3\u516c\u5e73\u6027\u7ea6\u675f\u7684\u5956\u52b1\u6a21\u578b\u6765\u89e3\u51b3LLM\u5bf9\u9f50\u4e2d\u7684\u7cfb\u7edf\u6027\u4e0d\u516c\u5e73\u95ee\u9898\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u5e76\u663e\u8457\u51cf\u5c11\u504f\u89c1", "motivation": "\u4eba\u7c7b\u504f\u597d\u6570\u636e\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u5dee\u4f1a\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u4f20\u64ad\u5230\u5bf9\u9f50\u7684LLM\u4e2d\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6027\u4e0d\u516c\u5e73\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u4fdd\u8bc1\u6392\u5e8f\u6b63\u786e\u6027\u3001\u6821\u51c6\u6027\u548c\u516c\u5e73\u6027\u7684\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5", "method": "\u63d0\u51faFairness Aware Reward Optimization (Faro)\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u65f6\u65bd\u52a0\u4eba\u53e3\u7edf\u8ba1\u5b66\u5e73\u7b49\u3001\u673a\u4f1a\u5747\u7b49\u6216\u53cd\u4e8b\u5b9e\u516c\u5e73\u6027\u7ea6\u675f\uff0c\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u5305\u62ec\u516c\u5e73\u6027\u8bc1\u4e66\u3001\u51c6\u786e\u6027\u4e0e\u516c\u5e73\u6027\u6743\u8861\u7684\u5f62\u5f0f\u5316\u7279\u5f81\u4ee5\u53ca\u5e15\u7d2f\u6258\u524d\u6cbf\u7684\u5b58\u5728\u6027\u8bc1\u660e", "result": "\u5728\u591a\u4e2aLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFaro\u663e\u8457\u51cf\u5c11\u4e86\u504f\u89c1\u548c\u6709\u5bb3\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u6a21\u578b\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u540c\u65f6\u5177\u5907\u6392\u5e8f\u6b63\u786e\u6027\u3001\u6821\u51c6\u6027\u548c\u516c\u5e73\u6027", "conclusion": "Faro\u6846\u67b6\u4e3aLLM\u5bf9\u9f50\u4e2d\u7684\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5728\u5956\u52b1\u6a21\u578b\u5c42\u9762\u89e3\u51b3\u516c\u5e73\u6027\u95ee\u9898\u7684\u53ef\u884c\u6027\uff0c\u4f18\u4e8e\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u65b9\u6cd5"}}
{"id": "2602.07899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07899", "abs": "https://arxiv.org/abs/2602.07899", "authors": ["Zhenhao Shang", "Haizhao Jing", "Guoting Wei", "Haokui Zhang", "Rong Xiao", "Jianqing Gao", "Peng Wang"], "title": "Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models", "comment": null, "summary": "Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.", "AI": {"tldr": "TLQ\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4ee4\u724c\u7ea7\u91cd\u8981\u6027\u611f\u77e5\u5c42\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u4fe1\u606f\u6307\u5bfc\u7684\u4ee4\u724c\u7ea7\u91cd\u8981\u6027\u96c6\u6210\u673a\u5236\u548c\u591aGPU\u91cf\u5316\u66b4\u9732\u5c42\u6821\u51c6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540e\u8bad\u7ec3\u91cf\u5316\u7684\u6027\u80fd\u7a33\u5b9a\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9\u548c\u6587\u672c\u4ee4\u724c\u5728\u6fc0\u6d3b\u5206\u5e03\u548c\u91cf\u5316\u8bef\u5dee\u654f\u611f\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u5bf9\u540e\u8bad\u7ec3\u91cf\u5316\u7684\u6709\u6548\u6821\u51c6\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u6821\u51c6\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5904\u7406\u8fd9\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u4ee4\u724c\u7684\u7279\u6027\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4ee4\u724c\u7ea7\u91cd\u8981\u6027\u611f\u77e5\u5c42\u91cf\u5316\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u68af\u5ea6\u4fe1\u606f\u8bbe\u8ba1\u4ee4\u724c\u7ea7\u91cd\u8981\u6027\u96c6\u6210\u673a\u5236\uff0c\u6784\u5efa\u4ee4\u724c\u7ea7\u6821\u51c6\u96c6\uff1b2\uff09\u5f15\u5165\u591aGPU\u3001\u91cf\u5316\u66b4\u9732\u5c42\u6821\u51c6\u65b9\u6848\uff0c\u4fdd\u6301\u6821\u51c6\u8fc7\u7a0b\u4e0e\u771f\u5b9e\u91cf\u5316\u63a8\u7406\u8def\u5f84\u4e00\u81f4\uff0c\u5e76\u5c06\u590d\u6742\u5c42\u6821\u51c6\u5de5\u4f5c\u5206\u914d\u5230\u591a\u4e2aRTX3090 GPU\u4e0a\u3002", "result": "\u5728\u4e24\u4e2a\u6a21\u578b\u3001\u4e09\u4e2a\u6a21\u578b\u89c4\u6a21\u548c\u4e24\u79cd\u91cf\u5316\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u5747\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u5176\u5177\u6709\u8f83\u5f3a\u7684\u91cf\u5316\u7a33\u5b9a\u6027\u3002", "conclusion": "TLQ\u6846\u67b6\u901a\u8fc7\u4ee4\u724c\u7ea7\u91cd\u8981\u6027\u611f\u77e5\u6821\u51c6\u548c\u591aGPU\u5c42\u6821\u51c6\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u91cf\u5316\u4e2d\u7684\u6821\u51c6\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u91cf\u5316\u6027\u80fd\u3002"}}
{"id": "2602.07931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07931", "abs": "https://arxiv.org/abs/2602.07931", "authors": ["Olena Hrynenko", "Darya Baranouskaya", "Alina Elena Baia", "Andrea Cavallaro"], "title": "Which private attributes do VLMs agree on and predict well?", "comment": "This work has been accepted to the ICASSP 2026", "summary": "Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9690\u79c1\u76f8\u5173\u5c5e\u6027\u8bc6\u522b\u65b9\u9762\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u53d1\u73b0VLM\u503e\u5411\u4e8e\u6bd4\u4eba\u7c7b\u6807\u6ce8\u8005\u66f4\u9891\u7e41\u5730\u9884\u6d4b\u9690\u79c1\u5c5e\u6027\u5b58\u5728\uff0c\u4f46\u5728\u9ad8\u4e00\u81f4\u6027\u60c5\u51b5\u4e0b\u53ef\u4ee5\u8865\u5145\u4eba\u7c7b\u6807\u6ce8\u7684\u9057\u6f0f\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e38\u7528\u4e8e\u56fe\u50cf\u89c6\u89c9\u5c5e\u6027\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u4f46\u5b83\u4eec\u5728\u9690\u79c1\u76f8\u5173\u5c5e\u6027\u8bc6\u522b\u65b9\u9762\u7684\u8868\u73b0\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5f00\u6e90VLM\u5728\u9690\u79c1\u5c5e\u6027\u8bc6\u522b\u4e2d\u7684\u80fd\u529b\uff0c\u5206\u6790\u5176\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u548c\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u96f6\u6837\u672c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u9690\u79c1\u76f8\u5173\u5c5e\u6027\u8fdb\u884c\u8bc6\u522b\u3002\u901a\u8fc7\u5206\u6790VLM\u4e4b\u95f4\u7684\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u5e76\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc6\u522bVLM\u8868\u73b0\u826f\u597d\u7684\u5c5e\u6027\u4ee5\u53ca\u5b58\u5728\u5206\u6b67\u7684\u60c5\u51b5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) \u4e0e\u4eba\u7c7b\u6807\u6ce8\u76f8\u6bd4\uff0cVLM\u503e\u5411\u4e8e\u66f4\u9891\u7e41\u5730\u9884\u6d4b\u9690\u79c1\u5c5e\u6027\u7684\u5b58\u5728\uff1b2) \u5728VLM\u4e4b\u95f4\u5177\u6709\u9ad8\u6807\u6ce8\u4e00\u81f4\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5b83\u4eec\u80fd\u591f\u8bc6\u522b\u4eba\u7c7b\u6807\u6ce8\u8005\u5ffd\u7565\u7684\u5c5e\u6027\uff0c\u4ece\u800c\u8865\u5145\u4eba\u7c7b\u6807\u6ce8\uff1b3) \u8bc6\u522b\u51fa\u4e86VLM\u8868\u73b0\u826f\u597d\u7684\u5177\u4f53\u5c5e\u6027\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u6570\u636e\u96c6\u7684\u9690\u79c1\u6807\u6ce8\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u4e00\u81f4\u6027\u60c5\u51b5\u4e0b\u53ef\u4ee5\u4f5c\u4e3a\u4eba\u7c7b\u6807\u6ce8\u7684\u8865\u5145\u5de5\u5177\uff0c\u8bc6\u522b\u88ab\u4eba\u7c7b\u5ffd\u7565\u7684\u9690\u79c1\u5c5e\u6027\u3002"}}
{"id": "2602.08905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08905", "abs": "https://arxiv.org/abs/2602.08905", "authors": ["Jiawei Liu", "Xiting Wang", "Yuanyuan Zhong", "Defu Lian", "Yu Yang"], "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models", "comment": "13 pages, 3 figures", "summary": "Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \\textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \\textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.", "AI": {"tldr": "STP\u6846\u67b6\u901a\u8fc7\u7a7a\u95f4\u526a\u679d\u548c\u65f6\u95f4\u526a\u679d\u540c\u65f6\u63d0\u5347\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5bf9\u89e3\u9501\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e94\u7528\u4e8edLLMs\u65f6\u9762\u4e34\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u72ec\u7279\u6311\u6218", "method": "\u63d0\u51fa\u65f6\u7a7a\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u526a\u679d\uff08\u4f7f\u7528\u9759\u6001\u5148\u9a8c\u7ea6\u675f\u63a2\u7d22\u7a7a\u95f4\uff09\u548c\u65f6\u95f4\u526a\u679d\uff08\u7ed5\u8fc7\u5197\u4f59\u7684\u540e\u671f\u7ec6\u5316\u6b65\u9aa4\uff09\u538b\u7f29\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eSTP\u4e25\u683c\u964d\u4f4e\u4e86\u5bf9\u6570\u4f3c\u7136\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u786e\u4fdd\u66f4\u7a33\u5b9a\u7684\u7b56\u7565\u66f4\u65b0\uff1b\u5927\u91cf\u5b9e\u9a8c\u663e\u793aSTP\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u90fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "STP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86dLLMs\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.07828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07828", "abs": "https://arxiv.org/abs/2602.07828", "authors": ["Charles Ye", "Jasmine Cui"], "title": "Efficient Representations are Controllable Representations", "comment": null, "summary": "What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.\n  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.", "AI": {"tldr": "\u901a\u8fc7\u7b80\u5355\u8f85\u52a9\u635f\u5931\u5fae\u8c03LLM\uff0c\u57283072\u4e2a\u6b8b\u5dee\u6d41\u7ef4\u5ea6\u4e2d\u8bad\u7ec316\u4e2a\u7ef4\u5ea6\u4f5c\u4e3a\u60f0\u6027\u53ef\u89e3\u91ca\u6027\u6807\u5fd7\uff0c\u6a21\u578b\u56f4\u7ed5\u8fd9\u4e9b\u6807\u5fd7\u91cd\u7ec4\u5e76\u4f9d\u8d56\u5b83\u4eec\u8fdb\u884c\u751f\u6210\uff0c\u4ece\u800c\u521b\u5efa\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u5f00\u5173", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5148\u8bc6\u522b\u6a21\u578b\u5185\u90e8\u7279\u5f81\u51e0\u4f55\u7ed3\u6784\u518d\u8fdb\u884c\u5e72\u9884\uff0c\u8fc7\u7a0b\u590d\u6742\u3002\u672c\u6587\u65e8\u5728\u5bfb\u627e\u4e00\u79cd\u66f4\u76f4\u63a5\u3001\u66f4\u66b4\u529b\u7684\u65b9\u5f0f\u5728\u6a21\u578b\u6fc0\u6d3b\u4e2d\u5b89\u88c5\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u5236\u7684\u7279\u5f81", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u8f85\u52a9\u635f\u5931\u5fae\u8c03LLM\uff0c\u57283072\u4e2a\u6b8b\u5dee\u6d41\u7ef4\u5ea6\u4e2d\u4e13\u95e8\u8bad\u7ec316\u4e2a\u7ef4\u5ea6\u4f5c\u4e3a\u60f0\u6027\u53ef\u89e3\u91ca\u6027\u6807\u5fd7\uff0c\u8fd9\u4e9b\u6807\u5fd7\u6307\u793a\u751f\u6210\u6240\u9700\u7684\u6982\u5ff5\u3002\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u56f4\u7ed5\u8fd9\u4e9b\u6807\u5fd7\u91cd\u7ec4\uff0c\u5b66\u4e60\u5728\u5b9e\u9645\u751f\u6210\u4efb\u52a1\u4e2d\u4f9d\u8d56\u8fd9\u4e9b\u6807\u5fd7", "result": "\u8fd9\u4e9b\u60f0\u6027\u6807\u5fd7\u6210\u4e3a\u771f\u6b63\u7684\u5185\u90e8\u7279\u5f81\uff1a\u53ef\u89e3\u91ca\u7684\u63a7\u5236\u5f00\u5173\uff0c\u5141\u8bb8\u5728\u63a8\u7406\u65f6\u5f15\u5bfc\u751f\u6210\u3002\u5f53\u7279\u5f81\u5728\u56fa\u5b9a\u4f4d\u7f6e\u53ef\u9760\u63d0\u4f9b\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d\u9010\u6e10\u6d88\u9664\u5176\u4ed6\u5730\u65b9\u7684\u5197\u4f59\u7f16\u7801\uff0c\u6a21\u578b\u4fb5\u8680\u81ea\u8eab\u7684\u66ff\u4ee3\u8868\u793a", "conclusion": "\u6a21\u578b\u7684\u6548\u7387\u538b\u529b\u662f\u4e00\u4e2a\u53ef\u5229\u7528\u7684\u6760\u6746\uff0c\u53ef\u4ee5\u7528\u6765\u8bf1\u5bfc\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u5236\u7684\u8868\u793a\u3002\u901a\u8fc7\u63d0\u4f9b\u53ef\u9760\u7684\u7279\u5f81\u4f4d\u7f6e\uff0c\u53ef\u4ee5\u8feb\u4f7f\u6a21\u578b\u91cd\u7ec4\u5176\u8868\u793a\u7ed3\u6784\uff0c\u5b9e\u73b0\u76f4\u63a5\u7684\u7279\u5f81\u63a7\u5236"}}
{"id": "2602.08939", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08939", "abs": "https://arxiv.org/abs/2602.08939", "authors": ["Longling Geng", "Andy Ouyang", "Theodore Wu", "Daphne Barretto", "Matthew John Hayes", "Rachael Cooper", "Yuqiao Zeng", "Sameer Vijay", "Gia Ancone", "Ankit Rai", "Matthew Wolfman", "Patrick Flanagan", "Edward Y. Chang"], "title": "CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse", "comment": "17 pages, 20 tables, figures", "summary": "LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench", "AI": {"tldr": "CausalT5K\u662f\u4e00\u4e2a\u5305\u542b5000\u591a\u4e2a\u6848\u4f8b\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u68c0\u6d4bLLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5305\u62ec\u9636\u68af\u574d\u584c\u3001\u8c04\u5a9a\u6f02\u79fb\u548c\u9519\u8bef\u62d2\u7edd\uff0c\u901a\u8fc7\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u6307\u6807\u63ed\u793a\u4f20\u7edf\u805a\u5408\u7cbe\u5ea6\u65e0\u6cd5\u53d1\u73b0\u7684\u6545\u969c\u6a21\u5f0f\u3002", "motivation": "LLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u5b58\u5728\u9636\u68af\u574d\u584c\u3001\u8c04\u5a9a\u6027\u548c\u9519\u8bef\u62d2\u7edd\u7b49\u5df2\u77e5\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u80fd\u591f\u8fdb\u884c\u7cfb\u7edf\u8bca\u65ad\u7684\u57fa\u51c6\uff0c\u4fee\u590d\u8fdb\u5c55\u7f13\u6162\u3002\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u80fd\u591f\u5168\u9762\u6d4b\u8bd5\u8fd9\u4e9b\u5173\u952e\u80fd\u529b\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b5000\u591a\u4e2a\u6848\u4f8b\u7684CausalT5K\u57fa\u51c6\uff0c\u6db5\u76d610\u4e2a\u9886\u57df\uff0c\u6d4b\u8bd5\u4e09\u79cd\u5173\u952e\u80fd\u529b\uff1a\u68c0\u6d4b\u9636\u68af\u574d\u584c\u3001\u62b5\u6297\u8c04\u5a9a\u6f02\u79fb\u3001\u751f\u6210\u660e\u667a\u62d2\u7edd\u3002\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6d41\u7a0b\uff0c\u6d89\u53ca40\u540d\u9886\u57df\u4e13\u5bb6\u3001\u8fed\u4ee3\u4ea4\u53c9\u9a8c\u8bc1\u5468\u671f\uff0c\u4ee5\u53ca\u57fa\u4e8e\u89c4\u5219\u3001LLM\u548c\u4eba\u5de5\u8bc4\u5206\u7684\u590d\u5408\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u63ed\u793a\u4e86\u56db\u8c61\u9650\u63a7\u5236\u666f\u89c2\uff0c\u9759\u6001\u5ba1\u8ba1\u7b56\u7565\u666e\u904d\u5931\u8d25\u3002\u57fa\u51c6\u80fd\u591f\u5c06\u6027\u80fd\u5206\u89e3\u4e3a\u5b9e\u7528\u6027\uff08\u654f\u611f\u6027\uff09\u548c\u5b89\u5168\u6027\uff08\u7279\u5f02\u6027\uff09\uff0c\u63ed\u793a\u4f20\u7edf\u805a\u5408\u7cbe\u5ea6\u65e0\u6cd5\u53d1\u73b0\u7684\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "CausalT5K\u4f5c\u4e3a\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u4e86Pearl\u7684\u56e0\u679c\u9636\u68af\uff0c\u4e3a\u63a8\u8fdb\u53ef\u4fe1\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u7cfb\u7edf\u8bc6\u522bLLM\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u3002"}}
{"id": "2602.07832", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07832", "abs": "https://arxiv.org/abs/2602.07832", "authors": ["Xian Wu", "Kaijie Zhu", "Ying Zhang", "Lun Wang", "Wenbo Guo"], "title": "rePIRL: Learn PRM with Inverse RL for LLM Reasoning", "comment": null, "summary": "Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.", "AI": {"tldr": "rePIRL\u662f\u4e00\u4e2a\u53d7\u9006\u5f3a\u5316\u5b66\u4e60\u542f\u53d1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u6709\u6548\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u5bf9\u4e13\u5bb6\u7b56\u7565\u7684\u5047\u8bbe\u8981\u6c42\u6700\u4f4e\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5bf9\u4e13\u5bb6\u7b56\u7565\u7684\u5f3a\u5047\u8bbe\uff08\u5982\u9700\u8981\u5176\u5956\u52b1\u51fd\u6570\uff09\uff0c\u8981\u4e48\u5b58\u5728\u5185\u5728\u5c40\u9650\u6027\uff08\u5982\u71b5\u5d29\u6e83\uff09\uff0c\u5bfc\u81f4\u6a21\u578b\u6548\u679c\u5f31\u6216\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51farePIRL\u6846\u67b6\uff0c\u8bbe\u8ba1\u53cc\u91cd\u5b66\u4e60\u8fc7\u7a0b\u4ea4\u66ff\u66f4\u65b0\u7b56\u7565\u548c\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u91c7\u7528\u5b9a\u5236\u5316\u6280\u672f\u89e3\u51b3\u4f20\u7edf\u9006\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u5230LLM\u7684\u6311\u6218\u3002", "result": "\u5728\u6807\u51c6\u5316\u6570\u5b66\u548c\u7f16\u7801\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793arePIRL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u51fa\u7684PRM\u53ef\u7528\u4e8e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u3001\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u4e3a\u56f0\u96be\u95ee\u9898\u8bad\u7ec3\u63d0\u4f9b\u65e9\u671f\u4fe1\u53f7\u3002", "conclusion": "rePIRL\u80fd\u591f\u4ee5\u6700\u5c0f\u5047\u8bbe\u5b66\u4e60\u6709\u6548\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u7edf\u4e00\u4e86\u5728\u7ebf\u548c\u79bb\u7ebfPRM\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u65b9\u6848\u548c\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u3002"}}
{"id": "2602.07955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07955", "abs": "https://arxiv.org/abs/2602.07955", "authors": ["Jiwei Chen", "Qi Wang", "Junyu Gao", "Jing Zhang", "Dingyi Li", "Jing-Jia Luo"], "title": "One-Shot Crowd Counting With Density Guidance For Scene Adaptaion", "comment": null, "summary": "Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\u7684\u8de8\u76d1\u63a7\u573a\u666f\u4eba\u7fa4\u8ba1\u6570\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u6307\u5bfc\u6a21\u578b\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u573a\u666f\u3002", "motivation": "\u4e0d\u540c\u76d1\u63a7\u6444\u50cf\u5934\u6355\u83b7\u7684\u4eba\u7fa4\u573a\u666f\u5dee\u5f02\u5f88\u5927\uff0c\u73b0\u6709\u7684\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u573a\u666f\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4e3a\u4e86\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7814\u7a76\u8005\u5c06\u4e0d\u540c\u76d1\u63a7\u573a\u666f\u89c6\u4e3a\u4e0d\u540c\u7c7b\u522b\u573a\u666f\uff0c\u5f15\u5165\u5c11\u6837\u672c\u5b66\u4e60\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u5c5e\u4e8e\u7ed9\u5b9a\u793a\u4f8b\u7c7b\u522b\u7684\u672a\u89c1\u76d1\u63a7\u573a\u666f\u3002", "method": "\u63d0\u51fa\u5229\u7528\u5c40\u90e8\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u6307\u5bfc\u6a21\u578b\u9002\u5e94\u672a\u89c1\u76d1\u63a7\u573a\u666f\u3002\u5177\u4f53\u5305\u62ec\uff1a1\uff09\u591a\u5c40\u90e8\u5bc6\u5ea6\u5b66\u4e60\u5668\u5b66\u4e60\u652f\u6301\u573a\u666f\u4e2d\u4ee3\u8868\u4e0d\u540c\u5bc6\u5ea6\u5206\u5e03\u7684\u591a\u539f\u578b\uff1b2\uff09\u7f16\u7801\u591a\u4e2a\u5c40\u90e8\u5bc6\u5ea6\u76f8\u4f3c\u6027\u77e9\u9635\uff0c\u4ee5\u5c40\u90e8\u65b9\u5f0f\u6307\u5bfc\u6a21\u578b\uff1b3\uff09\u4ece\u652f\u6301\u56fe\u50cf\u4e2d\u63d0\u53d6\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\uff0c\u4ee5\u5168\u5c40\u65b9\u5f0f\u6307\u5bfc\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u76d1\u63a7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u573a\u666f\uff0c\u5e76\u5728\u5c11\u6837\u672c\u4eba\u7fa4\u8ba1\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u8fd1\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u5bc6\u5ea6\u7279\u5f81\u6307\u5bfc\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4eba\u7fa4\u8ba1\u6570\u6a21\u578b\u5bf9\u672a\u89c1\u76d1\u63a7\u573a\u666f\u7684\u9002\u5e94\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.08948", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08948", "abs": "https://arxiv.org/abs/2602.08948", "authors": ["Chen Jin", "Ryutaro Tanno", "Tom Diethe", "Philip Teare"], "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute", "comment": null, "summary": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.", "AI": {"tldr": "CoRefine\u662f\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u81ea\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a7\u5236\u5668\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u76f8\u6bd4\u4f20\u7edf\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\u53ef\u51cf\u5c11\u7ea6190\u500dtoken\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u6d4b\u8bd5\u65f6\u5e76\u884c\u89e3\u7801\uff08\u5982512\u4e2a\u6837\u672c\uff09\u6765\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\uff0c\u4f46\u8fd9\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "method": "CoRefine\u5728\u51bb\u7ed3\u7684LLM\u4e0a\u6dfb\u52a0\u4e00\u4e2a\u8f7b\u91cf\u7ea7211k\u53c2\u6570\u7684Conv1D\u63a7\u5236\u5668\uff0c\u8be5\u63a7\u5236\u5668\u5229\u7528\u5b8c\u6574\u8f68\u8ff9\u7f6e\u4fe1\u5ea6\u6765\u51b3\u5b9a\u662f\u5426\u505c\u6b62\u3001\u91cd\u65b0\u68c0\u67e5\u6216\u5c1d\u8bd5\u4e0d\u540c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u81ea\u6211\u7ea0\u6b63\u3002\u5e73\u5747\u6bcf\u4e2a\u95ee\u9898\u53ea\u97002.7\u4e2a\u4f18\u5316\u6b65\u9aa4\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u4e0a\uff0c\u63a7\u5236\u5668\u5728\u81ea\u4fe1\u505c\u6b62\u65f6\u8fbe\u523092.6%\u7684\u7cbe\u786e\u5ea6\uff0c\u8868\u660e\u7f6e\u4fe1\u5ea6\u52a8\u6001\u53ef\u9760\u5730\u6307\u793a\u6b63\u786e\u6027\u800c\u65e0\u9700\u771f\u5b9e\u9a8c\u8bc1\u3002\u76f8\u6bd4512\u6837\u672c\u57fa\u7ebf\uff0c\u5e73\u5747\u51cf\u5c11\u7ea6190\u500dtoken\u6d88\u8017\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7f6e\u4fe1\u5ea6\u89c6\u4e3a\u63a7\u5236\u4fe1\u53f7\u800c\u975e\u6b63\u786e\u6027\u4fdd\u8bc1\uff0cCoRefine\u4e3a\u53ef\u6269\u5c55\u63a8\u7406\u548c\u5177\u6709\u4e0d\u5b8c\u7f8e\u9a8c\u8bc1\u5668\u7684\u667a\u80fd\u4f53\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u57fa\u7840\u3002CoRefine-Tree\u53d8\u4f53\u8fdb\u4e00\u6b65\u901a\u8fc7\u81ea\u9002\u5e94\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u6765\u589e\u5f3a\u6027\u80fd\u3002"}}
{"id": "2602.07960", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07960", "abs": "https://arxiv.org/abs/2602.07960", "authors": ["Changli Tang", "Tianyi Wang", "Fengyun Rao", "Jing Lyu", "Chao Zhang"], "title": "D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning", "comment": null, "summary": "Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \\textbf{d}ialogue-centric \\textbf{o}mni-modal large language model optimized for \\textbf{r}obust audio-visual \\textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \\href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \\href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.", "AI": {"tldr": "D-ORCA\u662f\u4e00\u4e2a\u9762\u5411\u5bf9\u8bdd\u7684\u8de8\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u4e2d\u7684\u8bf4\u8bdd\u4eba\u8bc6\u522b\u548c\u8bed\u97f3\u5185\u5bb9\u7406\u89e3\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u97f3\u9891-\u89c6\u89c9\u5b57\u5e55\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u4e2d\u7684\u5bf9\u8bdd\u662f\u91cd\u8981\u4fe1\u606f\u6e90\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u51c6\u786e\u8bc6\u522b\"\u8c01\u5728\u4f55\u65f6\u8bf4\u4e86\u4ec0\u4e48\"\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u591a\u65b9\u5bf9\u8bdd\u89c6\u9891\u6570\u636e\u96c6\u548c\u9488\u5bf9\u8bf4\u8bdd\u4eba\u5c5e\u6027\u3001\u5185\u5bb9\u51c6\u786e\u6027\u3001\u65f6\u95f4\u8fb9\u754c\u5bf9\u9f50\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "1) \u6784\u5efaDVD\u53cc\u8bed\u5927\u89c4\u6a21\u5bf9\u8bdd\u89c6\u9891\u6570\u636e\u96c6(4\u4e07\u8bad\u7ec3+2\u5343\u8bc4\u4f30)\uff1b2) \u63d0\u51faD-ORCA\u5bf9\u8bdd\u4e2d\u5fc3\u8de8\u6a21\u6001\u5927\u6a21\u578b\uff1b3) \u91c7\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u8bbe\u8ba1\u4e09\u4e2a\u65b0\u9896\u5956\u52b1\u51fd\u6570\uff1a\u8bf4\u8bdd\u4eba\u5c5e\u6027\u51c6\u786e\u6027\u3001\u5168\u5c40\u8bed\u97f3\u5185\u5bb9\u51c6\u786e\u6027\u3001\u53e5\u5b50\u7ea7\u65f6\u95f4\u8fb9\u754c\u5bf9\u9f50\u3002", "result": "D-ORCA\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u65f6\u95f4\u5b9a\u4f4d\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff1b\u4ec580\u4ebf\u53c2\u6570\u5374\u5728\u591a\u4e2a\u901a\u7528\u97f3\u9891-\u89c6\u89c9\u7406\u89e3\u57fa\u51c6\u4e0a\u4e0eQwen3-Omni\u7ade\u4e89\uff1b\u5728\u53cc\u8bed\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "D-ORCA\u901a\u8fc7\u5bf9\u8bdd\u4e2d\u5fc3\u8bbe\u8ba1\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u5bf9\u8bdd\u7406\u89e3\u7684\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u6df1\u5ea6\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07848", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07848", "abs": "https://arxiv.org/abs/2602.07848", "authors": ["Shijie Wang", "Pengfei Li", "Yikun Fu", "Kaifeng Liu", "Fangyuan Li", "Yang Liu", "Xiaowei Sun", "Zonglin Li", "Siyao Zhao", "Jian Zhao", "Kai Tian", "Dong Li", "Junqi Gao", "Yutong Zhang", "Yiqun Chen", "Yuqiang Li", "Zoe Li", "Weinan Zhang", "Peng Ye", "Shuyue Hu", "Lei Bai", "Bowen Zhou", "Kaiyan Zhang", "Biqing Qi"], "title": "MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation", "comment": null, "summary": "While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.", "AI": {"tldr": "MARTI-MARS2\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u8bad\u7ec3\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63a2\u7d22\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u52a8\u6001\u53ef\u5b66\u4e60\u73af\u5883\uff0c\u7ed3\u5408\u7b56\u7565\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u6811\u641c\u7d22\uff0c\u7a81\u7834\u5355\u667a\u80fd\u4f53\u80fd\u529b\u9650\u5236\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u751f\u6210\uff09\u4e2d\u5b58\u5728\u56fa\u6709\u7684\u6027\u80fd\u74f6\u9888\uff0c\u800c\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u57fa\u4e8e\u63d0\u793a\u7684\u6d4b\u8bd5\u65f6\u4ea4\u4e92\u6216\u4f7f\u7528\u540c\u8d28\u53c2\u6570\u8bad\u7ec3\u7684\u591a\u89d2\u8272\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u548c\u7b56\u7565\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faMARTI-MARS2\u6846\u67b6\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u63a2\u7d22\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u52a8\u6001\u53ef\u5b66\u4e60\u73af\u5883\uff0c\u7ed3\u5408\u7b56\u7565\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u6811\u641c\u7d22\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u73af\u5883\u4e2d\u8fed\u4ee3\u63a2\u7d22\u548c\u4f18\u5316\u3002\u540c\u65f6\u63d0\u51fa\u9ad8\u6548\u7684\u63a8\u7406\u7b56\u7565MARTI-MARS2-T+\uff0c\u5728\u6d4b\u8bd5\u65f6\u5145\u5206\u5229\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6269\u5c55\u6f5c\u529b\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\uff088B\u300114B\u300132B\uff09\u7684\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u4e24\u4e2a\u534f\u4f5c\u768432B\u6a21\u578b\uff0cMARTI-MARS2\u8fbe\u523077.7%\u7684\u6027\u80fd\uff0c\u4f18\u4e8eGPT-5.1\u7b49\u5f3a\u57fa\u7ebf\u3002\u6846\u67b6\u63ed\u793a\u4e86\u65b0\u7684\u6269\u5c55\u89c4\u5f8b\uff1a\u4ece\u5355\u667a\u80fd\u4f53\u5230\u540c\u8d28\u591a\u89d2\u8272\u518d\u5230\u5f02\u8d28\u591a\u667a\u80fd\u4f53\u8303\u5f0f\u9010\u6b65\u63d0\u9ad8RL\u6027\u80fd\u4e0a\u9650\u3001\u9c81\u68d2\u7684TTS\u80fd\u529b\u548c\u66f4\u5927\u7684\u7b56\u7565\u591a\u6837\u6027\u3002", "conclusion": "MARTI-MARS2\u901a\u8fc7\u6574\u5408\u7b56\u7565\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u6811\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u4ece\u53c2\u6570\u5171\u4eab\u7684\u540c\u8d28\u591a\u89d2\u8272\u8bad\u7ec3\u5230\u5f02\u8d28\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u6f14\u8fdb\uff0c\u7a81\u7834\u4e86\u5355\u667a\u80fd\u4f53\u80fd\u529b\u9650\u5236\u3002\u7814\u7a76\u8868\u660e\u7b56\u7565\u591a\u6837\u6027\u5bf9\u4e8e\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.07967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07967", "abs": "https://arxiv.org/abs/2602.07967", "authors": ["Xiaofeng Tan", "Wanjiang Weng", "Haodong Lei", "Hongsong Wang"], "title": "EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation", "comment": null, "summary": "In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.", "AI": {"tldr": "EasyTune\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6b65\u5fae\u8c03\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5185\u5b58\u6d88\u8017\u5927\u548c\u4f18\u5316\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u81ea\u4f18\u5316\u504f\u597d\u5b66\u4e60\u673a\u5236\u63d0\u5347\u8fd0\u52a8\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u53ef\u5fae\u5206\u5956\u52b1\u76f4\u63a5\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u504f\u597d\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1) \u4f18\u5316\u6548\u7387\u4f4e\u4e14\u7c92\u5ea6\u7c97\u7cd9\uff0c(2) \u5185\u5b58\u6d88\u8017\u5927\u3002\u6b64\u5916\uff0c\u504f\u597d\u8fd0\u52a8\u5bf9\u7684\u7a00\u7f3a\u9650\u5236\u4e86\u8fd0\u52a8\u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u9996\u5148\u4ece\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u8bc6\u522b\u51fa\u9650\u5236\u7684\u5173\u952e\u539f\u56e0\uff1a\u53bb\u566a\u8f68\u8ff9\u4e2d\u4e0d\u540c\u6b65\u9aa4\u4e4b\u95f4\u7684\u9012\u5f52\u4f9d\u8d56\u3002\u63d0\u51faEasyTune\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e00\u6b65\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5fae\u8c03\u6269\u6563\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u5728\u6574\u4e2a\u8f68\u8ff9\u4e0a\u4f18\u5316\uff0c\u4ece\u800c\u89e3\u8026\u9012\u5f52\u4f9d\u8d56\u3002\u6b64\u5916\uff0c\u5f15\u5165\u81ea\u4f18\u5316\u504f\u597d\u5b66\u4e60\u673a\u5236\uff0c\u52a8\u6001\u8bc6\u522b\u504f\u597d\u5bf9\u5e76\u8fdb\u884c\u504f\u597d\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEasyTune\u5728MM-Dist\u5bf9\u9f50\u6307\u6807\u4e0a\u6bd4DRaFT-50\u63d0\u53478.2%\uff0c\u540c\u65f6\u4ec5\u9700\u5176\u989d\u5916\u5185\u5b58\u5f00\u9500\u768431.16%\uff0c\u5e76\u5b9e\u73b07.3\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "EasyTune\u901a\u8fc7\u5206\u6b65\u5fae\u8c03\u548c\u81ea\u4f18\u5316\u504f\u597d\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u7684\u5185\u5b58\u6d88\u8017\u5927\u548c\u4f18\u5316\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2602.08968", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08968", "abs": "https://arxiv.org/abs/2602.08968", "authors": ["Lucas Maes", "Quentin Le Lidec", "Dan Haramati", "Nassim Massaudi", "Damien Scieur", "Yann LeCun", "Randall Balestriero"], "title": "stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation", "comment": null, "summary": "World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.", "AI": {"tldr": "SWM\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u7ecf\u8fc7\u6d4b\u8bd5\u548c\u6587\u6863\u5316\u7684\u4e16\u754c\u6a21\u578b\u7814\u7a76\u751f\u6001\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u7f3a\u4e4f\u53ef\u91cd\u7528\u6027\u3001\u6807\u51c6\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u90fd\u662f\u9488\u5bf9\u7279\u5b9a\u8bba\u6587\u7684\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5176\u53ef\u91cd\u7528\u6027\uff0c\u589e\u52a0\u4e86bug\u98ce\u9669\uff0c\u5e76\u964d\u4f4e\u4e86\u8bc4\u4f30\u6807\u51c6\u5316\u7a0b\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u5f00\u53d1\u4e86stable-worldmodel\uff08SWM\uff09\u3002", "method": "SWM\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u4e16\u754c\u6a21\u578b\u7814\u7a76\u751f\u6001\u7cfb\u7edf\uff0c\u63d0\u4f9b\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u5de5\u5177\u3001\u6807\u51c6\u5316\u73af\u5883\u3001\u89c4\u5212\u7b97\u6cd5\u548c\u57fa\u51c6\u5b9e\u73b0\u3002\u6bcf\u4e2a\u73af\u5883\u90fd\u652f\u6301\u53ef\u63a7\u7684\u53d8\u5316\u56e0\u7d20\uff08\u5305\u62ec\u89c6\u89c9\u548c\u7269\u7406\u5c5e\u6027\uff09\uff0c\u4ee5\u652f\u6301\u9c81\u68d2\u6027\u548c\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u3002", "result": "\u4f5c\u8005\u901a\u8fc7\u4f7f\u7528SWM\u7814\u7a76DINO-WM\u4e2d\u7684\u96f6\u6837\u672c\u9c81\u68d2\u6027\u6765\u5c55\u793a\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "SWM\u4f5c\u4e3a\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u4e16\u754c\u6a21\u578b\u7814\u7a76\u5e73\u53f0\uff0c\u80fd\u591f\u4fc3\u8fdb\u4e16\u754c\u6a21\u578b\u7814\u7a76\u7684\u53ef\u91cd\u7528\u6027\u3001\u51cf\u5c11bug\u98ce\u9669\uff0c\u5e76\u63d0\u9ad8\u8bc4\u4f30\u6807\u51c6\u5316\uff0c\u540c\u65f6\u652f\u6301\u9c81\u68d2\u6027\u548c\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u3002"}}
{"id": "2602.07859", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07859", "abs": "https://arxiv.org/abs/2602.07859", "authors": ["Siyu Lu", "Chenhan Xiao", "Yang Weng"], "title": "Dynamic Load Model for Data Centers with Pattern-Consistent Calibration", "comment": "10 pages, 13 figures", "summary": "The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u578b\u7535\u5b50\u8d1f\u8f7d\u5efa\u6a21\uff0c\u901a\u8fc7\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u6821\u51c6\u53c2\u6570\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u5fc3\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u53d1\u73b0\u672a\u6821\u51c6\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u8d1f\u8f7d\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u6548\u5e94\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u7684\u5feb\u901f\u589e\u957f\u4f7f\u5927\u578b\u7535\u5b50\u8d1f\u8f7d\u5efa\u6a21\u5bf9\u7535\u529b\u7cfb\u7edf\u5206\u6790\u65e5\u76ca\u91cd\u8981\u3002\u4f20\u7edf\u8d1f\u8f7d\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u8fd9\u7c7b\u8d1f\u8f7d\u7684\u5feb\u901f\u5de5\u4f5c\u8d1f\u8f7d\u9a71\u52a8\u53d8\u5316\u4ee5\u53ca\u4fdd\u62a4\u9a71\u52a8\u7684\u65ad\u5f00/\u91cd\u8fde\u884c\u4e3a\u3002\u73b0\u6709\u7269\u7406\u6a21\u578b\u901a\u5e38\u672a\u9488\u5bf9\u8bbe\u65bd\u7ea7\u8fd0\u884c\u6821\u51c6\uff0c\u800c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u4ea7\u751f\u4e0d\u73b0\u5b9e\u7684\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u8bbe\u8ba1\u7ed3\u5408\u7269\u7406\u6a21\u578b\u7ed3\u6784\u548c\u6570\u636e\u9a71\u52a8\u9002\u5e94\u6027\u7684\u6846\u67b6\u3002\u7269\u7406\u6a21\u578b\u53c2\u6570\u5316\u4ee5\u652f\u6301\u4ece\u771f\u5b9e\u8fd0\u884c\u6570\u636e\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u7684\u6a21\u5f0f\u4e00\u81f4\u6027\u6821\u51c6\u3002\u91c7\u7528\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u65f6\u95f4\u548c\u7edf\u8ba1\u6a21\u5f0f\uff0c\u800c\u975e\u8f68\u8ff9\u7ea7\u5bf9\u9f50\u3002\u6821\u51c6\u5728\u672c\u5730\u8bbe\u65bd\u8fdb\u884c\uff0c\u4ec5\u5171\u4eab\u6821\u51c6\u53c2\u6570\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "result": "\u4f7f\u7528MIT Supercloud\u3001ASU Sol\u3001Blue Waters\u548cASHRAE\u6570\u636e\u96c6\u7684\u771f\u5b9e\u8fd0\u884c\u8d1f\u8f7d\u6570\u636e\u6821\u51c6\u6a21\u578b\uff0c\u5e76\u5728ANDES\u5e73\u53f0\u4e0a\u96c6\u6210\uff0c\u5728IEEE 39\u603b\u7ebf\u3001NPCC 140\u603b\u7ebf\u548cWECC 179\u603b\u7ebf\u7cfb\u7edf\u4e0a\u8bc4\u4f30\u3002\u53d1\u73b0\u5927\u578b\u7535\u5b50\u8d1f\u8f7d\u95f4\u7684\u4ea4\u4e92\u4f1a\u6839\u672c\u6539\u53d8\u6270\u52a8\u540e\u6062\u590d\u884c\u4e3a\uff0c\u4ea7\u751f\u590d\u5408\u7684\u65ad\u5f00-\u91cd\u8fde\u52a8\u6001\u548c\u5ef6\u8fdf\u7a33\u5b9a\uff0c\u8fd9\u4e9b\u662f\u672a\u6821\u51c6\u8d1f\u8f7d\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u7684\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86\u7269\u7406\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u9a71\u52a8\u7684\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u6a21\u5f0f\u4e00\u81f4\u6027\u6821\u51c6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u7535\u5b50\u8d1f\u8f7d\u95f4\u4ea4\u4e92\u5bf9\u7535\u529b\u7cfb\u7edf\u52a8\u6001\u884c\u4e3a\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u4e3a\u8bbe\u65bd\u7ea7\u7535\u7f51\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5efa\u6a21\u5de5\u5177\u3002"}}
{"id": "2602.07979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07979", "abs": "https://arxiv.org/abs/2602.07979", "authors": ["Peng Peng", "Xinrui Zhang", "Junlin Wang", "Lei Li", "Shaoyu Wang", "Qiegen Liu"], "title": "FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction", "comment": null, "summary": "Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.", "AI": {"tldr": "FSP-Diff\u662f\u4e00\u4e2a\u7528\u4e8e\u8d85\u4f4e\u5242\u91cf\u80fd\u8c31CT\u91cd\u5efa\u7684\u5168\u8c31\u5148\u9a8c\u589e\u5f3a\u53cc\u57df\u6f5c\u5728\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u8865\u7279\u5f81\u6784\u5efa\u3001\u5168\u8c31\u5148\u9a8c\u96c6\u6210\u548c\u9ad8\u6548\u6f5c\u5728\u6269\u6563\u5408\u6210\u4e09\u5927\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u8d85\u4f4e\u5242\u91cf\u6761\u4ef6\u4e0b\uff0c\u80fd\u8c31CT\u4e2d\u80fd\u91cf\u7279\u5b9a\u6295\u5f71\u7684\u4fe1\u566a\u6bd4\u6025\u5267\u4e0b\u964d\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u51fa\u73b0\u4e25\u91cd\u4f2a\u5f71\u548c\u7ed3\u6784\u7ec6\u8282\u4e22\u5931\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u91cd\u5efa\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faFSP-Diff\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7b56\u7565\uff1a1)\u4e92\u8865\u7279\u5f81\u6784\u5efa\uff1a\u6574\u5408\u76f4\u63a5\u56fe\u50cf\u91cd\u5efa\u548c\u6295\u5f71\u57df\u53bb\u566a\u7ed3\u679c\uff1b2)\u5168\u8c31\u5148\u9a8c\u96c6\u6210\uff1a\u878d\u5408\u591a\u80fd\u91cf\u6295\u5f71\u4e3a\u9ad8\u4fe1\u566a\u6bd4\u5168\u8c31\u56fe\u50cf\u4f5c\u4e3a\u7edf\u4e00\u7ed3\u6784\u53c2\u8003\uff1b3)\u9ad8\u6548\u6f5c\u5728\u6269\u6563\u5408\u6210\uff1a\u5c06\u591a\u8def\u5f84\u7279\u5f81\u5d4c\u5165\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u4f4e\u7ef4\u6d41\u5f62\u4e2d\u8fdb\u884c\u4ea4\u4e92\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFSP-Diff\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FSP-Diff\u6846\u67b6\u4e3a\u4e34\u5e8a\u53ef\u884c\u7684\u8d85\u4f4e\u5242\u91cf\u80fd\u8c31CT\u6210\u50cf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u57df\u6269\u6563\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u7ec6\u8282\u4fdd\u771f\u5ea6\u548c\u566a\u58f0\u6291\u5236\u3002"}}
{"id": "2602.08990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08990", "abs": "https://arxiv.org/abs/2602.08990", "authors": ["Shiyang Feng", "Runmin Ma", "Xiangchao Yan", "Yue Fan", "Yusong Hu", "Songtao Huang", "Shuaiyu Zhang", "Zongsheng Cao", "Tianshuo Peng", "Jiakang Yuan", "Zijie Guo", "Zhijie Zhong", "Shangheng Du", "Weida Wang", "Jinxin Shi", "Yuhao Zhou", "Xiaohan He", "Zhiyin Yu", "Fangchen Yu", "Qihao Zheng", "Jiamin Wu", "Mianxin Liu", "Chi Zhang", "Shaowei Hou", "Shuya Li", "Yankai Jiang", "Wenjie Lou", "Lilong Wang", "Zifu Wang", "Jiong Wang", "Wanghan Xu", "Yue Deng", "Dongrui Liu", "Yiheng Wang", "Wenlong Zhang", "Fenghua Ling", "Shufei Zhang", "Xiaosong Wang", "Shuangjia Zheng", "Xun Huang", "Siqi Sun", "Shuyue Hu", "Peng Ye", "Chunfeng Song", "Bin Wang", "Conghui He", "Yihao Liu", "Xin Li", "Qibin Hou", "Tao Chen", "Xiangyu Yue", "Bin Wang", "Liang He", "Dahua Lin", "Bowen Zhou", "Bo Zhang", "Lei Bai"], "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery", "comment": "Code and project page: https://github.com/InternScience/InternAgent", "summary": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.", "AI": {"tldr": "InternAgent-1.5\u662f\u4e00\u4e2a\u7528\u4e8e\u8de8\u8ba1\u7b97\u548c\u5b9e\u8bc1\u9886\u57df\u7aef\u5230\u7aef\u79d1\u5b66\u53d1\u73b0\u7684\u7edf\u4e00\u7cfb\u7edf\uff0c\u5305\u542b\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u6f14\u5316\u4e09\u4e2a\u534f\u8c03\u5b50\u7cfb\u7edf\uff0c\u5728\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u5e76\u80fd\u81ea\u4e3b\u8bbe\u8ba1\u7b97\u6cd5\u548c\u8fdb\u884c\u5b9e\u8bc1\u5b9e\u9a8c\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u8ba1\u7b97\u548c\u5b9e\u8bc1\u9886\u57df\u8fdb\u884c\u7aef\u5230\u7aef\u79d1\u5b66\u53d1\u73b0\u7684\u7edf\u4e00\u7cfb\u7edf\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u534f\u8c03\u3001\u957f\u671f\u53d1\u73b0\u5468\u671f\u548c\u81ea\u4e3b\u5b9e\u9a8c\u6267\u884c\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8e\u7ed3\u6784\u5316\u67b6\u6784\u6784\u5efa\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u8c03\u5b50\u7cfb\u7edf\uff1a\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u6f14\u5316\uff0c\u652f\u6301\u6df1\u5ea6\u7814\u7a76\u3001\u89e3\u51b3\u65b9\u6848\u4f18\u5316\u548c\u957f\u671f\u8bb0\u5fc6\u7b49\u57fa\u7840\u80fd\u529b\uff0c\u80fd\u591f\u534f\u8c03\u8ba1\u7b97\u5efa\u6a21\u548c\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u3002", "result": "\u5728GAIA\u3001HLE\u3001GPQA\u548cFrontierScience\u7b49\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u6027\u80fd\uff1b\u5728\u7b97\u6cd5\u53d1\u73b0\u4efb\u52a1\u4e2d\u81ea\u4e3b\u8bbe\u8ba1\u6838\u5fc3\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7684\u7ade\u4e89\u6027\u65b9\u6cd5\uff1b\u5728\u5b9e\u8bc1\u53d1\u73b0\u4efb\u52a1\u4e2d\u6267\u884c\u5b8c\u6574\u7684\u8ba1\u7b97\u6216\u6e7f\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\uff0c\u5728\u5730\u7403\u3001\u751f\u547d\u3001\u751f\u7269\u548c\u7269\u7406\u9886\u57df\u4ea7\u751f\u79d1\u5b66\u53d1\u73b0\u3002", "conclusion": "InternAgent-1.5\u4e3a\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5728\u8de8\u8ba1\u7b97\u548c\u5b9e\u8bc1\u9886\u57df\u7684\u7edf\u4e00\u79d1\u5b66\u53d1\u73b0\u80fd\u529b\u3002"}}
{"id": "2602.07873", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07873", "abs": "https://arxiv.org/abs/2602.07873", "authors": ["Donghyeon Ki", "Hee-Jun Ahn", "Kyungyoon Kim", "Byung-Jun Lee"], "title": "Direct Soft-Policy Sampling via Langevin Dynamics", "comment": null, "summary": "Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.", "AI": {"tldr": "\u63d0\u51faNC-LQL\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6761\u4ef6\u5316Langevin\u52a8\u529b\u5b66\u5b9e\u73b0\u8f6f\u7b56\u7565\u91c7\u6837\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8868\u8fbe\u6027\u548c\u71b5\u4f30\u8ba1\u65b9\u9762\u7684\u9650\u5236\uff0c\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e0e\u6269\u6563\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8f6f\u7b56\u7565\u5b9e\u73b0\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u53c2\u6570\u5316\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u6269\u6563\u7b56\u7565\u7684\u4e0d\u53ef\u5904\u7406\u4f3c\u7136\u963b\u788d\u4e86\u8f6f\u7b56\u7565\u76ee\u6807\u4e2d\u7684\u53ef\u9760\u71b5\u4f30\u8ba1\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u8f6f\u7b56\u7565\u91c7\u6837\u53c8\u907f\u514d\u8fd9\u4e9b\u9650\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u566a\u58f0\u6761\u4ef6\u5316Langevin Q\u5b66\u4e60(NC-LQL)\uff1a1) \u4f7f\u7528Langevin\u52a8\u529b\u5b66\u76f4\u63a5\u91c7\u6837\u76ee\u6807Boltzmann\u5206\u5e03\uff0c\u65e0\u9700\u663e\u5f0f\u53c2\u6570\u5316\u7b56\u7565\uff1b2) \u5f15\u5165\u591a\u5c3a\u5ea6\u566a\u58f0\u6270\u52a8\u5230\u503c\u51fd\u6570\u4e2d\uff0c\u5b66\u4e60\u566a\u58f0\u6761\u4ef6\u5316Q\u51fd\u6570\uff1b3) \u901a\u8fc7\u6e10\u8fdb\u5e73\u6ed1\u7684\u503c\u51fd\u6570\u666f\u89c2\u5b9e\u73b0\u4ece\u5168\u5c40\u63a2\u7d22\u5230\u7cbe\u786e\u6a21\u5f0f\u7ec6\u5316\u7684\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5728OpenAI Gym MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNC-LQL\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "NC-LQL\u901a\u8fc7\u566a\u58f0\u6761\u4ef6\u5316Langevin\u52a8\u529b\u5b66\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u7b56\u7565\u5b9e\u73b0\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u7b56\u7565\u53c2\u6570\u5316\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7b80\u5355\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2602.07980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07980", "abs": "https://arxiv.org/abs/2602.07980", "authors": ["Junlin Wang", "Jiancheng Fang", "Peng Peng", "Shaoyu Wang", "Qiegen Liu"], "title": "Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction", "comment": null, "summary": "The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51faCSDN\u65b9\u6cd5\u7528\u4e8e\u8d85\u7a00\u758f\u89c6\u89d2CBCT\u91cd\u5efa\uff0c\u901a\u8fc7\u795e\u7ecf\u5148\u9a8c\u7f16\u7801\u8fde\u7eed\u4e09\u7ef4\u8870\u51cf\u8868\u793a\uff0c\u7ed3\u5408\u6b63\u5f26\u56fe\u4e0e\u6570\u5b57\u653e\u5c04\u56fe\u50cf\u53cc\u8def\u5f84\u6269\u6563\u7b56\u7565\uff0c\u6709\u6548\u6291\u5236\u4f2a\u5f71\u5e76\u6062\u590d\u7ec6\u8282\u7eb9\u7406\u3002", "motivation": "CBCT\u4e34\u5e8a\u5e94\u7528\u9762\u4e34\u8f90\u5c04\u66b4\u9732\u4e0e\u56fe\u50cf\u8d28\u91cf\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\u3002\u8d85\u7a00\u758f\u89d2\u5ea6\u91c7\u6837\u867d\u7136\u964d\u4f4e\u5242\u91cf\uff0c\u4f46\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u6b20\u91c7\u6837\u4f2a\u5f71\u548c\u5207\u7247\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u8bca\u65ad\u53ef\u9760\u6027\u3002\u73b0\u6709\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u89d2\u5ea6\u8fde\u7eed\u6027\u4e0e\u7a7a\u95f4\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u6027\u9a71\u52a8\u7684\u534f\u540c\u6269\u6563\u4e0e\u795e\u7ecf\u5148\u9a8c\uff08CSDN\uff09\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165\u795e\u7ecf\u5148\u9a8c\u4f5c\u4e3a\u7ed3\u6784\u57fa\u7840\uff0c\u7f16\u7801\u8fde\u7eed\u4e09\u7ef4\u8870\u51cf\u8868\u793a\uff1b2\uff09\u57fa\u4e8e\u795e\u7ecf\u5148\u9a8c\u521d\u59cb\u5316\uff0c\u5f00\u53d1\u534f\u540c\u6269\u6563\u7b56\u7565\uff0c\u5305\u62ec\u6b63\u5f26\u56fe\u7ec6\u5316\u6269\u6563\uff08Sino-RD\uff09\u6062\u590d\u89d2\u5ea6\u8fde\u7eed\u6027\uff0c\u6570\u5b57\u653e\u5c04\u56fe\u50cf\u7ec6\u5316\u6269\u6563\uff08DR-RD\uff09\u4ece\u6295\u5f71\u56fe\u50cf\u89d2\u5ea6\u589e\u5f3a\u5207\u7247\u95f4\u4e00\u81f4\u6027\uff1b3\uff09\u901a\u8fc7\u53cc\u6295\u5f71\u91cd\u5efa\u878d\u5408\uff08DPRF\uff09\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u4e24\u4e2a\u6269\u6563\u8def\u5f84\u8f93\u51fa\uff0c\u5b9e\u73b0\u8fde\u8d2f\u4f53\u79ef\u91cd\u5efa\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCSDN\u5728\u8d85\u7a00\u758f\u89c6\u89d2\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u6291\u5236\u4f2a\u5f71\u5e76\u6062\u590d\u7cbe\u7ec6\u7eb9\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "CSDN\u65b9\u6cd5\u901a\u8fc7\u795e\u7ecf\u5148\u9a8c\u548c\u534f\u540c\u6269\u6563\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8d85\u7a00\u758f\u89c6\u89d2CBCT\u91cd\u5efa\u4e2d\u7684\u89d2\u5ea6\u8fde\u7eed\u6027\u4e0e\u5207\u7247\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u4f4e\u5242\u91cf\u9ad8\u8d28\u91cfCBCT\u6210\u50cf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.09000", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09000", "abs": "https://arxiv.org/abs/2602.09000", "authors": ["Ali Hatamizadeh", "Shrimai Prabhumoye", "Igor Gitman", "Ximing Lu", "Seungju Han", "Wei Ping", "Yejin Choi", "Jan Kautz"], "title": "iGRPO: Self-Feedback-Driven LLM Reasoning", "comment": "Tech report", "summary": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.", "AI": {"tldr": "iGRPO\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u751f\u6210\u7684\u8349\u7a3f\u8fdb\u884c\u52a8\u6001\u81ea\u6211\u6761\u4ef6\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86GRPO\u5e76\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u6570\u5b66\u95ee\u9898\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u96be\u4ee5\u4ea7\u751f\u51c6\u786e\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5bf9\u9f50\u6a21\u578b\u4e0e\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "iGRPO\u662fGRPO\u7684\u4e24\u9636\u6bb5\u6269\u5c55\uff1a\u7b2c\u4e00\u9636\u6bb5\u91c7\u6837\u591a\u4e2a\u63a2\u7d22\u6027\u8349\u7a3f\u5e76\u9009\u62e9\u6700\u9ad8\u5956\u52b1\u7684\u8349\u7a3f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u6700\u4f73\u8349\u7a3f\u9644\u52a0\u5230\u539f\u59cb\u63d0\u793a\u540e\uff0c\u5728\u8349\u7a3f\u6761\u4ef6\u5316\u7684\u7cbe\u70bc\u4e0a\u5e94\u7528GRPO\u98ce\u683c\u7684\u66f4\u65b0\uff0c\u8bad\u7ec3\u7b56\u7565\u8d85\u8d8a\u5176\u5148\u524d\u7684\u6700\u4f73\u5c1d\u8bd5\u3002", "result": "\u5728\u5339\u914d\u7684rollout\u9884\u7b97\u4e0b\uff0ciGRPO\u5728\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u4e0a\u59cb\u7ec8\u4f18\u4e8eGRPO\u3002\u5e94\u7528\u4e8eOpenReasoning-Nemotron-7B\u6a21\u578b\u65f6\uff0c\u5728AIME24\u548cAIME25\u4e0a\u5206\u522b\u8fbe\u523085.62%\u548c79.64%\u7684\u65b0SOTA\u7ed3\u679c\u3002", "conclusion": "iGRPO\u5c55\u793a\u4e86\u8fed\u4ee3\u3001\u57fa\u4e8e\u81ea\u6211\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u8fdb\u53ef\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5176\u7cbe\u70bc\u5305\u88c5\u5668\u5177\u6709\u8d85\u8d8aGRPO\u53d8\u4f53\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u901a\u8fc7\u5ef6\u8fdf\u71b5\u5d29\u6e83\u6539\u53d8\u5b66\u4e60\u52a8\u6001\u3002"}}
{"id": "2602.07875", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07875", "abs": "https://arxiv.org/abs/2602.07875", "authors": ["Aditya Shankar", "Yuandou Wang", "Rihan Hai", "Lydia Y. Chen"], "title": "Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion", "comment": "Accepted at ICLR 2026", "summary": "Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon", "AI": {"tldr": "HARPOON\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5f62\u7406\u8bba\u7684\u8868\u683c\u6570\u636e\u6269\u6563\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u6d41\u5f62\u51e0\u4f55\u5f15\u5bfc\u65e0\u7ea6\u675f\u6837\u672c\u6765\u6ee1\u8db3\u591a\u6837\u7684\u8868\u683c\u6761\u4ef6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u7ea6\u675f\u548c\u4ec5\u9650\u4e8e\u8fde\u7eed\u57df\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u8868\u683c\u6570\u636e\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u8bad\u7ec3\u65f6\u7b56\u7565\u65e0\u6cd5\u6cdb\u5316\u5230\u63a8\u7406\u65f6\u672a\u89c1\u7ea6\u675f\uff1b2\uff09\u53ea\u80fd\u5904\u7406\u8868\u683c\u63d2\u8865\u7b49\u6709\u9650\u4efb\u52a1\u3002\u6d41\u5f62\u7406\u8bba\u867d\u7136\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\uff0c\u4f46\u73b0\u6709\u516c\u5f0f\u5c40\u9650\u4e8e\u7279\u5b9a\u63a8\u7406\u76ee\u6807\u4e14\u4ec5\u9650\u4e8e\u8fde\u7eed\u57df\u3002", "method": "\u5c06\u6d41\u5f62\u7406\u8bba\u6269\u5c55\u5230\u8868\u683c\u6570\u636e\uff0c\u6269\u5927\u5176\u5904\u7406\u591a\u6837\u63a8\u7406\u76ee\u6807\u7684\u80fd\u529b\u3002\u57fa\u4e8e\u6b64\u63d0\u51faHARPOON\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u8868\u683c\u6269\u6563\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u5f62\u51e0\u4f55\u5f15\u5bfc\u65e0\u7ea6\u675f\u6837\u672c\u6765\u6ee1\u8db3\u63a8\u7406\u65f6\u7684\u5404\u79cd\u8868\u683c\u6761\u4ef6\u3002", "result": "\u5728\u63d2\u8865\u548c\u6267\u884c\u4e0d\u7b49\u5f0f\u7ea6\u675f\u7b49\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u7406\u8bba\u8d21\u732e\uff0cHARPOON\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u6d41\u5f62\u611f\u77e5\u5f15\u5bfc\u5bf9\u8868\u683c\u6570\u636e\u7684\u5b9e\u9645\u76ca\u5904\u3002", "conclusion": "HARPOON\u901a\u8fc7\u6269\u5c55\u6d41\u5f62\u7406\u8bba\u5230\u8868\u683c\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u6837\u63a8\u7406\u76ee\u6807\u3001\u6cdb\u5316\u5230\u672a\u89c1\u7ea6\u675f\u7684\u8868\u683c\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.07986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.07986", "abs": "https://arxiv.org/abs/2602.07986", "authors": ["Md. Tarek Hasan", "Sanjay Saha", "Shaojing Fan", "Swakkhar Shatabda", "Terence Sim"], "title": "Deepfake Synthesis vs. Detection: An Uneven Contest", "comment": null, "summary": "The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.", "AI": {"tldr": "\u6700\u65b0\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u5728\u5e94\u5bf9\u73b0\u4ee3\u5408\u6210\u65b9\u6cd5\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5305\u62ec\u4eba\u7c7b\u8bc4\u4f30\u8005\u4e5f\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u9ad8\u8d28\u91cf\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\uff0c\u7a81\u663e\u68c0\u6d4b\u6280\u672f\u53d1\u5c55\u6ede\u540e\u4e8e\u751f\u6210\u6280\u672f\u8fdb\u6b65\u7684\u4e25\u5cfb\u73b0\u5b9e\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff08\u5305\u62ec\u6269\u6563\u6a21\u578b\u3001NeRF\u548c\u589e\u5f3a\u7684GANs\uff09\uff0c\u5408\u6210\u5a92\u4f53\u7684\u771f\u5b9e\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u663e\u8457\u63d0\u9ad8\u3002\u540c\u65f6\uff0c\u68c0\u6d4b\u65b9\u6cd5\u867d\u6709\u8fdb\u6b65\uff0c\u4f46\u9700\u8981\u8bc4\u4f30\u5f53\u524d\u6700\u5148\u8fdb\u68c0\u6d4b\u6280\u672f\u80fd\u5426\u6709\u6548\u5e94\u5bf9\u73b0\u4ee3\u5408\u6210\u65b9\u6cd5\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u3002", "method": "\u5bf9\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u8fdb\u884c\u5168\u9762\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u5305\u62ec\u9488\u5bf9\u5c16\u7aef\u5408\u6210\u65b9\u6cd5\u7684\u4eba\u7c7b\u8bc4\u4f30\u5b9e\u9a8c\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc4\u4f30\u68c0\u6d4b\u6a21\u578b\u5728\u5e94\u5bf9\u73b0\u4ee3\u5408\u6210\u6280\u672f\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u65f6\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ee4\u4eba\u62c5\u5fe7\u7684\u8d8b\u52bf\uff1a\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u73b0\u4ee3\u5408\u6210\u6280\u672f\u751f\u6210\u7684\u6df1\u5ea6\u4f2a\u9020\u65f6\u8868\u73b0\u660e\u663e\u4e0d\u4f73\uff0c\u4eba\u7c7b\u53c2\u4e0e\u8005\u5728\u9762\u5bf9\u6700\u9ad8\u8d28\u91cf\u6df1\u5ea6\u4f2a\u9020\u65f6\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002\u5b9e\u9a8c\u8bc1\u636e\u8868\u660e\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u6301\u7eed\u6539\u8fdb\u4ee5\u8ddf\u4e0a\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u6b65\u4f10\u3002", "conclusion": "\u5f53\u524d\u68c0\u6d4b\u65b9\u6cd5\u4e0e\u65b0\u4e00\u4ee3\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u7684\u590d\u6742\u6027\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u8feb\u5207\u9700\u8981\u5728\u8fd9\u4e00\u5173\u952e\u7814\u7a76\u9886\u57df\u52a0\u5f3a\u52aa\u529b\uff0c\u6301\u7eed\u6539\u8fdb\u68c0\u6d4b\u6a21\u578b\u4ee5\u5e94\u5bf9\u4e0d\u65ad\u53d1\u5c55\u7684\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u3002"}}
{"id": "2602.09003", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09003", "abs": "https://arxiv.org/abs/2602.09003", "authors": ["Yudong Wang", "Zixuan Fu", "Hengyu Zhao", "Chen Zhao", "Chuyue Zhou", "Xinle Lin", "Hongya Lyu", "Shuaikang Xue", "Yi Yi", "Yingjiao Wang", "Zhi Zheng", "Yuzhou Zhang", "Jie Zhou", "Chaojun Xiao", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management", "comment": "16 pages, 3 figures, 7 tables", "summary": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdL0-L4\u5206\u5c42\u6570\u636e\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u4e0e\u6a21\u578b\u7684\u534f\u540c\u8fdb\u5316\u6765\u63d0\u5347LLM\u8bad\u7ec3\u6548\u7387\uff0c\u89e3\u51b3\u5f53\u524d\u6570\u636e\u89c4\u6a21\u5355\u5411\u6269\u5c55\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u89c4\u6a21\u7684\u5355\u5411\u6269\u5c55\uff0c\u9762\u4e34\u6570\u636e\u53ef\u7528\u6027\u3001\u83b7\u53d6\u6210\u672c\u548c\u8bad\u7ec3\u6548\u7387\u7684\u74f6\u9888\u3002\u4f5c\u8005\u8ba4\u4e3aAGI\u53d1\u5c55\u6b63\u8fdb\u5165\u6570\u636e-\u6a21\u578b\u534f\u540c\u8fdb\u5316\u7684\u65b0\u9636\u6bb5\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u6570\u636e\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86L0-L4\u5206\u5c42\u6570\u636e\u7ba1\u7406\u6846\u67b6\uff0c\u4ece\u539f\u59cb\u672a\u6574\u7406\u8d44\u6e90\u5230\u7ec4\u7ec7\u5316\u53ef\u9a8c\u8bc1\u77e5\u8bc6\u5206\u4e3a\u4e94\u4e2a\u5c42\u7ea7\u3002\u5229\u7528LLM\u53c2\u4e0e\u6570\u636e\u7ba1\u7406\u8fc7\u7a0b\uff08\u5982\u8d28\u91cf\u8bc4\u5206\u548c\u5185\u5bb9\u7f16\u8f91\uff09\uff0c\u6839\u636e\u6570\u636e\u7279\u6027\u3001\u7ba1\u7406\u7b56\u7565\u548c\u8bad\u7ec3\u89d2\u8272\u5c06\u6570\u636e\u6218\u7565\u6027\u5730\u5206\u914d\u5230\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548c\u5bf9\u9f50\u7b49\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u5206\u5c42\u6570\u636e\u96c6\u8fdb\u884c\u591a\u9636\u6bb5\u8bad\u7ec3\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5206\u5c42\u611f\u77e5\u7684\u6570\u636e\u5229\u7528\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u5206\u5c42\u6570\u636e\u7ba1\u7406\u6846\u67b6\u5728\u6570\u636e\u8d28\u91cf\u3001\u83b7\u53d6\u6210\u672c\u548c\u8fb9\u9645\u8bad\u7ec3\u6548\u76ca\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u6301\u7eed\u7684\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u5e76\u5f00\u6e90\u4e86\u5206\u5c42\u6570\u636e\u96c6\u548c\u5904\u7406\u5de5\u5177\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2602.09007", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09007", "abs": "https://arxiv.org/abs/2602.09007", "authors": ["Haodong Li", "Jingwei Wu", "Quan Sun", "Guopeng Li", "Juanxi Tian", "Huanyu Zhang", "Yanlin Lai", "Ruichuan An", "Hongbo Peng", "Yuhong Dai", "Chenxi Li", "Chunmei Qing", "Jia Wang", "Ziyang Meng", "Zheng Ge", "Xiangyu Zhang", "Daxin Jiang"], "title": "GEBench: Benchmarking Image Generation Models as GUI Environments", "comment": "23 pages, 5 figures, 4 tables", "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.", "AI": {"tldr": "GEBench\u662f\u4e00\u4e2a\u8bc4\u4f30GUI\u751f\u6210\u4e2d\u52a8\u6001\u4ea4\u4e92\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b700\u4e2a\u6837\u672c\u548cGE-Score\u4e94\u7ef4\u8bc4\u4f30\u6307\u6807\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5355\u6b65\u8f6c\u6362\u8868\u73b0\u826f\u597d\u4f46\u591a\u6b65\u5e8f\u5217\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u5b9a\u4f4d\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u80fd\u591f\u6839\u636e\u7528\u6237\u6307\u4ee4\u9884\u6d4b\u672a\u6765GUI\u72b6\u6001\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u7528\u9886\u57df\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5bf9GUI\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u7684\u72b6\u6001\u8f6c\u6362\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u5f15\u5165GEBench\u57fa\u51c6\uff0c\u5305\u542b700\u4e2a\u7cbe\u5fc3\u7b56\u5212\u7684\u6837\u672c\uff0c\u6db5\u76d65\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u5305\u62ec\u5355\u6b65\u4ea4\u4e92\u548c\u591a\u6b65\u8f68\u8ff9\uff0c\u4ee5\u53ca\u5b9a\u4f4d\u70b9\u57fa\u7840\u4efb\u52a1\u3002\u63d0\u51faGE-Score\u4e94\u7ef4\u8bc4\u4f30\u6307\u6807\uff1a\u76ee\u6807\u8fbe\u6210\u3001\u4ea4\u4e92\u903b\u8f91\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u3001UI\u5408\u7406\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "result": "\u5bf9\u5f53\u524d\u6a21\u578b\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4eec\u5728\u5355\u6b65\u8f6c\u6362\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ef4\u6301\u957f\u65f6\u95f4\u4ea4\u4e92\u5e8f\u5217\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u663e\u8457\u56f0\u96be\u3002\u56fe\u6807\u89e3\u91ca\u3001\u6587\u672c\u6e32\u67d3\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u88ab\u8bc6\u522b\u4e3a\u5173\u952e\u74f6\u9888\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u6784\u5efa\u9ad8\u4fdd\u771f\u751f\u6210\u5f0fGUI\u73af\u5883\u7684\u7814\u7a76\u6307\u660e\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2602.07889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07889", "abs": "https://arxiv.org/abs/2602.07889", "authors": ["Long Chen", "Yinkui Liu", "Shen Li", "Bo Tang", "Xuemin Hu"], "title": "Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning", "comment": null, "summary": "Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eVQVAE\u548c\u6a21\u7cca\u805a\u7c7b\u7684\u79bb\u7ebfRL\u53cd\u63a2\u7d22\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u5bf9\u79bb\u6563\u5316\u4e2d\u7684\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53cd\u63a2\u7d22\u65b9\u6cd5\u901a\u8fc7\u79bb\u6563\u5316\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u5bf9\u8fdb\u884c\u8ba1\u6570\uff0c\u4f46\u5b58\u5728\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u964d\u4f4e\u751a\u81f3\u7b56\u7565\u5b66\u4e60\u5931\u8d25\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u591a\u7801\u672cVQVAE\u7684\u9ad8\u6548\u4f2a\u8ba1\u6570\u65b9\u6cd5\u79bb\u6563\u5316\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff1b2. \u8bbe\u8ba1\u57fa\u4e8e\u8be5\u4f2a\u8ba1\u6570\u65b9\u6cd5\u7684\u79bb\u7ebfRL\u53cd\u63a2\u7d22\u65b9\u6cd5\uff1b3. \u5f00\u53d1\u57fa\u4e8e\u6a21\u7ccaC\u5747\u503c\u805a\u7c7b\u7684\u7801\u672c\u66f4\u65b0\u673a\u5236\u63d0\u9ad8\u7801\u672c\u5411\u91cf\u5229\u7528\u7387\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u9700\u8981\u66f4\u5c11\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eVQVAE\u548c\u6a21\u7cca\u805a\u7c7b\u7684\u53cd\u63a2\u7d22\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u5bf9\u79bb\u6563\u5316\u4e2d\u7684\u7ef4\u5ea6\u707e\u96be\u548c\u4fe1\u606f\u635f\u5931\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2512.22730", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.22730", "abs": "https://arxiv.org/abs/2512.22730", "authors": ["Youssef Megahed", "Robin Ducharme", "Inok Lee", "Inbal Willner", "Adrian D. C. Chan", "Mark Walker", "Steven Hawken"], "title": "Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning", "comment": "13 pages, 6 figures, 2 tables", "summary": "Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u8d85\u58f0\u7279\u5f02\u6027\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08USF-MAE\uff09\u5728\u65e9\u5b55\u671f\u8d85\u58f0\u56fe\u50cf\u4e2d\u68c0\u6d4b\u56ca\u6027\u6c34\u56ca\u7624\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8e\u4f20\u7edf\u7684DenseNet-169\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u56ca\u6027\u6c34\u56ca\u7624\u662f\u9ad8\u98ce\u9669\u7684\u4ea7\u524d\u8d85\u58f0\u53d1\u73b0\uff0c\u4e0e\u67d3\u8272\u4f53\u5f02\u5e38\u3001\u7ed3\u6784\u7578\u5f62\u548c\u4e0d\u826f\u598a\u5a20\u7ed3\u5c40\u76f8\u5173\u3002\u81ea\u52a8\u5316\u68c0\u6d4b\u53ef\u4ee5\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u5e76\u652f\u6301\u53ef\u6269\u5c55\u7684\u65e9\u671f\u7b5b\u67e5\u9879\u76ee\uff0c\u4f46\u76d1\u7763\u5f0f\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\uff08MAE\uff09\u7684\u8d85\u58f0\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\uff08USF-MAE\uff09\uff0c\u8be5\u6a21\u578b\u5728\u8d85\u8fc737\u4e07\u5f20\u672a\u6807\u6ce8\u8d85\u58f0\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u9488\u5bf9\u672c\u7814\u7a76\u4e2d\u7684\u6b63\u5e38\u5bf9\u7167\u548c\u56ca\u6027\u6c34\u56ca\u7624\u75c5\u4f8b\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\u5fae\u8c03\u3002\u8bc4\u4f30\u91c7\u7528\u4e0eDenseNet-169\u57fa\u7ebf\u76f8\u540c\u7684\u8d85\u58f0\u6570\u636e\u96c6\u3001\u9884\u5904\u7406\u6d41\u7a0b\u548c4\u6298\u4ea4\u53c9\u9a8c\u8bc1\u534f\u8bae\uff0c\u4f7f\u7528\u51c6\u786e\u7387\u3001\u7075\u654f\u5ea6\u3001\u7279\u5f02\u5ea6\u548cROC-AUC\u7b49\u6307\u6807\u3002\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u901a\u8fc7Score-CAM\u53ef\u89c6\u5316\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u3002", "result": "USF-MAE\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8eDenseNet-169\u57fa\u7ebf\u6a21\u578b\u3002USF-MAE\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a0.96\u3001\u7075\u654f\u5ea60.94\u3001\u7279\u5f02\u5ea60.98\u3001ROC-AUC\u4e3a0.98\uff0c\u800cDenseNet-169\u57fa\u7ebf\u5206\u522b\u4e3a0.93\u30010.92\u30010.94\u548c0.94\u3002Score-CAM\u53ef\u89c6\u5316\u663e\u793a\u6a21\u578b\u9884\u6d4b\u5177\u6709\u4e34\u5e8a\u76f8\u5173\u6027\uff0c\u80fd\u7a81\u51fa\u663e\u793a\u80ce\u513f\u9888\u90e8\u7684\u9884\u671f\u533a\u57df\u3002Wilcoxon\u7b26\u53f7\u79e9\u68c0\u9a8c\u8bc1\u5b9eUSF-MAE\u7684\u6027\u80fd\u63d0\u5347\u5177\u6709\u7edf\u8ba1\u5b66\u663e\u8457\u6027\uff08p=0.0057\uff09\u3002", "conclusion": "\u8d85\u58f0\u7279\u5f02\u6027\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u80fd\u591f\u4fc3\u8fdb\u51c6\u786e\u3001\u7a33\u5065\u7684\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u56ca\u6027\u6c34\u56ca\u7624\uff0c\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u65e9\u671f\u4ea7\u524d\u7b5b\u67e5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2602.07892", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.07892", "abs": "https://arxiv.org/abs/2602.07892", "authors": ["Guanglong Sun", "Siyuan Zhang", "Liyuan Wang", "Jun Zhu", "Hang Su", "Yi Zhong"], "title": "Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection", "comment": null, "summary": "Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\\% to 3.03\\% and IFEval from 51.94\\% to 63.96\\%. Our source code is available at \\href{https://github.com/SunGL001/OGPSA}{OGPSA}", "AI": {"tldr": "OGPSA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u68af\u5ea6\u6295\u5f71\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\"\u5bf9\u9f50\u7a0e\"\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fdb\u884c\u5b89\u5168\u5bf9\u9f50\u8bad\u7ec3\u65f6\u5f80\u5f80\u4f1a\u51fa\u73b0\"\u5bf9\u9f50\u7a0e\"\u95ee\u9898\uff0c\u5373\u5b89\u5168\u6027\u63d0\u5347\u4f1a\u964d\u4f4e\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff08\u5982\u63a8\u7406\u548c\u7f16\u7801\uff09\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u8fde\u7eed\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u73b0\u8c61\u9020\u6210\u7684\uff0c\u5176\u4e2d\u5206\u5e03\u504f\u79fb\u548c\u51b2\u7a81\u76ee\u6807\u5bfc\u81f4\u5b89\u5168\u66f4\u65b0\u8986\u76d6\u4e86\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u80fd\u529b\u3002", "method": "\u5c06\u5b89\u5168\u5bf9\u9f50\u89c6\u4e3a\u8fde\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u6b63\u4ea4\u68af\u5ea6\u6295\u5f71\u5b89\u5168\u5bf9\u9f50\uff08OGPSA\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f30\u8ba1\u4e00\u4e2a\u4f4e\u79e9\u80fd\u529b\u5b50\u7a7a\u95f4\uff08\u4ece\u5c0f\u578b\u53c2\u8003\u96c6\u7684\u68af\u5ea6\u4e2d\u5b66\u4e60\uff09\uff0c\u7136\u540e\u5c06\u5b89\u5168\u68af\u5ea6\u6295\u5f71\u5230\u8be5\u5b50\u7a7a\u95f4\u7684\u6b63\u4ea4\u8865\u7a7a\u95f4\u4e0a\u8fdb\u884c\u66f4\u65b0\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u5bf9\u5148\u524d\u77e5\u8bc6\u7684\u5e72\u6270\u3002", "result": "\u5728\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4ee5\u53ca\u987a\u5e8fSFT\u2192DPO\u8bbe\u7f6e\u4e2d\uff0cOGPSA\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6539\u5584\u4e86\u5b89\u5168\u6027\u4e0e\u901a\u7528\u80fd\u529b\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002\u4f8b\u5982\uff0c\u5728Qwen2.5-7B-Instruct\u6a21\u578b\u4e0a\uff0cOGPSA\u5728\u4fdd\u6301\u5f3a\u5927\u5b89\u5168\u6027\u7684\u540c\u65f6\u6062\u590d\u4e86\u901a\u7528\u80fd\u529b\uff0c\u5c06SimpleQA\u4ece0.53%\u63d0\u5347\u52303.03%\uff0cIFEval\u4ece51.94%\u63d0\u5347\u523063.96%\u3002", "conclusion": "OGPSA\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u91cd\u653e\u3001\u8f85\u52a9\u76ee\u6807\u6216\u91cd\u65b0\u8bad\u7ec3\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\"\u5bf9\u9f50\u7a0e\"\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027\u3002"}}
{"id": "2602.08024", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08024", "abs": "https://arxiv.org/abs/2602.08024", "authors": ["Ziyang Fan", "Keyu Chen", "Ruilong Xing", "Yulin Li", "Li Jiang", "Zhuotao Tian"], "title": "FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging", "comment": "Accepted by ICLR 2026 (Oral)", "summary": "Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.", "AI": {"tldr": "FlashVID\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u4e0e\u591a\u6837\u6027\u4ee4\u724c\u9009\u62e9\u548c\u6811\u72b6\u65f6\u7a7a\u4ee4\u724c\u5408\u5e76\u6280\u672f\uff0c\u5728\u4fdd\u755999.1%\u6027\u80fd\u7684\u540c\u65f6\u5c06\u89c6\u89c9\u4ee4\u724c\u51cf\u5c1190%\uff0c\u5b9e\u73b010\u500d\u89c6\u9891\u5e27\u8f93\u5165\u6269\u5c55\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5904\u7406\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u52a0\u901f\u6846\u67b6\u901a\u5e38\u72ec\u7acb\u538b\u7f29\u7a7a\u95f4\u548c\u65f6\u95f4\u5197\u4f59\uff0c\u5ffd\u7565\u4e86\u65f6\u7a7a\u5173\u7cfb\uff0c\u5bfc\u81f4\u6b21\u4f18\u538b\u7f29\u6548\u679c\u3002\u89c6\u9891\u7684\u52a8\u6001\u7279\u6027\u4f7f\u5f97\u89c6\u89c9\u7279\u5f81\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u9ad8\u5ea6\u76f8\u5173\u4e14\u4e0d\u65ad\u53d8\u5316\u3002", "method": "FlashVID\u91c7\u7528\u4e24\u79cd\u6838\u5fc3\u6280\u672f\uff1a1) \u6ce8\u610f\u529b\u4e0e\u591a\u6837\u6027\u4ee4\u724c\u9009\u62e9(ADTS)\uff1a\u9009\u62e9\u6700\u5177\u4ee3\u8868\u6027\u7684\u4ee4\u724c\u4f5c\u4e3a\u57fa\u7840\u89c6\u9891\u8868\u793a\uff1b2) \u6811\u72b6\u65f6\u7a7a\u4ee4\u724c\u5408\u5e76(TSTM)\uff1a\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u5197\u4f59\u6d88\u9664\u3002\u8fd9\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u52a0\u901f\u6846\u67b6\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027VLLM\u548c\u4e94\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4ec5\u4fdd\u755910%\u89c6\u89c9\u4ee4\u724c\u65f6\uff0cFlashVID\u80fd\u4fdd\u7559LLaVA-OneVision 99.1%\u7684\u6027\u80fd\u3002\u53ef\u4f7fQwen2.5-VL\u7684\u89c6\u9891\u5e27\u8f93\u5165\u589e\u52a010\u500d\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u76f8\u5bf9\u63d0\u53478.6%\u3002", "conclusion": "FlashVID\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u65f6\u7a7a\u4ee4\u724c\u538b\u7f29\u6280\u672f\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u957f\u89c6\u9891\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08025", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08025", "abs": "https://arxiv.org/abs/2602.08025", "authors": ["Yixuan Ye", "Xuanyu Lu", "Yuxin Jiang", "Yuchao Gu", "Rui Zhao", "Qiwei Liang", "Jiachun Pan", "Fengda Zhang", "Weijia Wu", "Alex Jinpeng Wang"], "title": "MIND: Benchmarking Memory Consistency and Action Control in World Models", "comment": null, "summary": "World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/", "AI": {"tldr": "MIND\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\u7684\u5f00\u653e\u57df\u95ed\u73af\u91cd\u8bbf\u57fa\u51c6\uff0c\u5305\u542b250\u4e2a\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cf\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u4e24\u5927\u6838\u5fc3\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u5728\u52a8\u6001\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u57fa\u672c\u80fd\u529b\uff0c\u5305\u62ec\u7406\u89e3\u3001\u8bb0\u5fc6\u548c\u9884\u6d4b\u80fd\u529b\uff0c\u8fd9\u963b\u788d\u4e86\u4e16\u754c\u6a21\u578b\u7684\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u6bd4\u8f83\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b250\u4e2a1080p\u300124FPS\u9ad8\u8d28\u91cf\u89c6\u9891\u7684MIND\u57fa\u51c6\uff0c\u5305\u62ec\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\uff0c\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u91cf\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86MIND-World\u4f5c\u4e3a\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MIND\u57fa\u51c6\u7684\u5b8c\u6574\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4e16\u754c\u6a21\u578b\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u96be\u4ee5\u4fdd\u6301\u957f\u671f\u8bb0\u5fc6\u4e00\u81f4\u6027\u4ee5\u53ca\u5728\u8de8\u52a8\u4f5c\u7a7a\u95f4\u6cdb\u5316\u65b9\u9762\u7684\u56f0\u96be\u3002", "conclusion": "MIND\u57fa\u51c6\u586b\u8865\u4e86\u4e16\u754c\u6a21\u578b\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u4e16\u754c\u6a21\u578b\u7684\u8bb0\u5fc6\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u63a7\u5236\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4e16\u754c\u6a21\u578b\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2602.07915", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07915", "abs": "https://arxiv.org/abs/2602.07915", "authors": ["Huiyang Yi", "Xiaojian Shen", "Yonggang Wu", "Duxin Chen", "He Wang", "Wenwu Yu"], "title": "CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios", "comment": null, "summary": "Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.", "AI": {"tldr": "CausalCompass\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u5efa\u6a21\u5047\u8bbe\u8fdd\u53cd\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5b9e\u9a8c\u663e\u793a\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6ca1\u6709\u4efb\u4f55\u65b9\u6cd5\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u6700\u4f18\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u7684\u5e7f\u6cdb\u5e94\u7528\u53d7\u5230\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u5bf9\u4e0d\u53ef\u6d4b\u8bd5\u7684\u56e0\u679c\u5047\u8bbe\u7684\u4f9d\u8d56\uff1b2\uff09\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u9762\u5411\u9c81\u68d2\u6027\u7684\u8bc4\u4f30\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86CausalCompass\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86CausalCompass\uff0c\u4e00\u4e2a\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u5efa\u6a21\u5047\u8bbe\u8fdd\u53cd\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u516b\u4e2a\u5047\u8bbe\u8fdd\u53cd\u573a\u666f\u5bf9\u4ee3\u8868\u6027\u7b97\u6cd5\u8fdb\u884c\u4e86\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u6ca1\u6709\u4efb\u4f55\u5355\u4e00\u65b9\u6cd5\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff1b2\uff09\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u6700\u4f73\u6574\u4f53\u6027\u80fd\u7684\u65b9\u6cd5\u51e0\u4e4e\u90fd\u662f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff1b3\uff09NTS-NOTEARS\u4e25\u91cd\u4f9d\u8d56\u6807\u51c6\u5316\u9884\u5904\u7406\uff0c\u5728\u539f\u59cb\u8bbe\u7f6e\u4e2d\u8868\u73b0\u5dee\u4f46\u5728\u6807\u51c6\u5316\u540e\u8868\u73b0\u5f3a\uff1b4\uff09\u63d0\u4f9b\u4e86\u8d85\u53c2\u6570\u654f\u611f\u6027\u5206\u6790\u4ee5\u6df1\u5165\u7406\u89e3\u8fd9\u4e9b\u53d1\u73b0\u3002", "conclusion": "CausalCompass\u63d0\u4f9b\u4e86\u5bf9\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u5047\u8bbe\u8fdd\u53cd\u60c5\u51b5\u4e0b\u7684\u5168\u9762\u7cfb\u7edf\u8bc4\u4f30\uff0c\u65e8\u5728\u4fc3\u8fdb\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.07928", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07928", "abs": "https://arxiv.org/abs/2602.07928", "authors": ["Ziyun Li", "Huancheng Hu", "Soon Hoe Lim", "Xuyu Li", "Fei Gao", "Enmao Diao", "Zezhen Ding", "Michalis Vazirgiannis", "Henrik Bostrom"], "title": "A Kinetic-Energy Perspective of Flow Matching", "comment": null, "summary": "Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faKinetic Path Energy (KPE)\u4f5c\u4e3a\u6d41\u751f\u6210\u6a21\u578b\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u53d1\u73b0\u9ad8KPE\u5bf9\u5e94\u5f3a\u8bed\u4e49\u4fdd\u771f\u5ea6\u4f46\u53ef\u80fd\u5bfc\u81f4\u8bb0\u5fc6\u5316\uff0c\u8fdb\u800c\u63d0\u51faKinetic Trajectory Shaping (KTS)\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4ece\u7269\u7406\u5b66\u89c6\u89d2\u5206\u6790\u6d41\u751f\u6210\u6a21\u578b\uff0c\u5c06\u91c7\u6837\u8fc7\u7a0b\u89c6\u4e3a\u7c92\u5b50\u4ece\u566a\u58f0\u5230\u6570\u636e\u7684\u8f68\u8ff9\u8fd0\u52a8\uff0c\u53d7\u7ecf\u5178\u529b\u5b66\u542f\u53d1\uff0c\u9700\u8981\u91cf\u5316\u8f68\u8ff9\u7684\u52a8\u6001\u52aa\u529b\u6765\u7406\u89e3\u751f\u6210\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165Kinetic Path Energy (KPE)\u4f5c\u4e3a\u7c7b\u4f5c\u7528\u91cf\u7684\u6bcf\u6837\u672c\u8bca\u65ad\u6307\u6807\uff0c\u6d4b\u91cfODE\u8f68\u8ff9\u7684\u7d2f\u79ef\u52a8\u80fd\u52aa\u529b\uff1b\u63d0\u51faKinetic Trajectory Shaping (KTS)\u8bad\u7ec3\u81ea\u7531\u7684\u4e24\u9636\u6bb5\u63a8\u7406\u7b56\u7565\uff0c\u589e\u5f3a\u65e9\u671f\u8fd0\u52a8\u5e76\u5f3a\u5236\u540e\u671f\u8f6f\u7740\u9646\u3002", "result": "KPE\u8868\u73b0\u51fa\u4e24\u4e2a\u7a33\u5065\u5bf9\u5e94\u5173\u7cfb\uff1a1) \u9ad8KPE\u9884\u6d4b\u5f3a\u8bed\u4e49\u4fdd\u771f\u5ea6\uff1b2) \u9ad8KPE\u8f68\u8ff9\u7ec8\u6b62\u4e8e\u4f4e\u5bc6\u5ea6\u6d41\u5f62\u8fb9\u754c\u3002\u7406\u8bba\u4fdd\u8bc1\u5c06\u8f68\u8ff9\u80fd\u91cf\u4e0e\u6570\u636e\u5bc6\u5ea6\u8054\u7cfb\u8d77\u6765\uff0c\u4f46\u76f8\u5173\u6027\u975e\u5355\u8c03\uff0c\u6781\u9ad8\u80fd\u91cf\u4f1a\u5bfc\u81f4\u8bb0\u5fc6\u5316\u3002", "conclusion": "\u8f68\u8ff9\u80fd\u91cf\u5b58\u5728Goldilocks\u539f\u5219\uff08\u9002\u5ea6\u539f\u5219\uff09\uff0cKTS\u901a\u8fc7\u4f18\u5316\u8f68\u8ff9\u52a8\u529b\u5b66\u51cf\u5c11\u8bb0\u5fc6\u5316\uff0c\u5728\u57fa\u51c6\u4efb\u52a1\u4e2d\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u6d41\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u8bca\u65ad\u548c\u4f18\u5316\u6846\u67b6\u3002"}}
{"id": "2602.08047", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08047", "abs": "https://arxiv.org/abs/2602.08047", "authors": ["Jiahong Fu", "Qi Xie", "Deyu Meng", "Zongben Xu"], "title": "Vanilla Group Equivariant Vision Transformer: Simple and Effective", "comment": null, "summary": "Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u7b49\u53d8ViT\u6846\u67b6\uff0c\u901a\u8fc7\u4f7fViT\u7684\u5173\u952e\u7ec4\u4ef6\uff08\u5305\u62ecpatch embedding\u3001\u81ea\u6ce8\u610f\u529b\u3001\u4f4d\u7f6e\u7f16\u7801\u548c\u4e0a\u4e0b\u91c7\u6837\uff09\u7b49\u53d8\u5316\uff0c\u6784\u5efa\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7b49\u53d8\u6027\u7684ViT\u67b6\u6784\uff0c\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7b49\u53d8ViT\u5728\u5e73\u8861\u6027\u80fd\u4e0e\u7b49\u53d8\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u5b9e\u73b0ViT\u4e2d\u591a\u6837\u5316\u6a21\u5757\uff08\u7279\u522b\u662f\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0epatch embedding\u7684\u534f\u8c03\uff09\u7684\u6574\u4f53\u7b49\u53d8\u4fee\u6539\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5355\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u4f7fViT\u7684\u5173\u952e\u7ec4\u4ef6\uff08patch embedding\u3001\u81ea\u6ce8\u610f\u529b\u3001\u4f4d\u7f6e\u7f16\u7801\u3001Down/Up-Sampling\uff09\u7b49\u53d8\u5316\uff0c\u6784\u5efa\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7b49\u53d8\u6027\u7684ViT\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u751a\u81f3\u53ef\u6269\u5c55\u5230Swin Transformers\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7b49\u53d8ViT\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u4e00\u81f4\u5730\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u6709\u4fdd\u8bc1\u3001\u5b9e\u8df5\u4e0a\u591a\u529f\u80fd\u7684\u7b49\u53d8ViT\u67b6\u6784\uff0c\u80fd\u591f\u65e0\u7f1d\u6269\u5c55\u5230\u4e0d\u540cTransformer\u53d8\u4f53\uff0c\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.07933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07933", "abs": "https://arxiv.org/abs/2602.07933", "authors": ["Olamide Samuel Oseni", "Ibraheem Omotolani Obanla", "Toheeb Aduramomi Jimoh"], "title": "Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data", "comment": null, "summary": "Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.\n  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.\n  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.", "AI": {"tldr": "SAINT\u6ce8\u610f\u529b\u6a21\u578b\u5728\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8eMLP\u3001\u68af\u5ea6\u63d0\u5347\u548cTabNet\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u7279\u5f81\u5de5\u7a0b\u4e14\u96be\u4ee5\u6355\u6349\u590d\u6742\u7279\u5f81\u4ea4\u4e92\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528UCI\u673a\u5668\u5b66\u4e60\u5e93\u7684\u5e15\u91d1\u68ee\u75c5\u8bed\u97f3\u6d4b\u91cf\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u56db\u79cd\u5206\u7c7b\u6a21\u578b\uff1a\u591a\u5c42\u611f\u77e5\u5668\u3001\u68af\u5ea6\u63d0\u5347\u3001TabNet\u548cSAINT\uff0c\u8bc4\u4f30\u5176\u5728\u65e9\u671f\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "result": "SAINT\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff1a\u52a0\u6743\u7cbe\u5ea60.98\u3001\u52a0\u6743\u53ec\u56de\u73870.97\u3001\u52a0\u6743F1\u5206\u65700.97\u3001\u9a6c\u4fee\u65af\u76f8\u5173\u7cfb\u65700.9990\u548c\u6700\u9ad8AUC-ROC\u3002TabNet\u548cMLP\u8868\u73b0\u7ade\u4e89\u6027\uff0c\u68af\u5ea6\u63d0\u5347\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "\u6ce8\u610f\u529b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u68c0\u6d4b\u4e2d\u5177\u6709\u8bca\u65ad\u6f5c\u529b\uff0c\u53cc\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6709\u6548\u5efa\u6a21\u7279\u5f81\u4ea4\u4e92\uff0c\u52a8\u6001\u7279\u5f81\u8868\u793a\u5728\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u5f88\u91cd\u8981\u3002"}}
{"id": "2602.07950", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07950", "abs": "https://arxiv.org/abs/2602.07950", "authors": ["Daisuke Okanohara"], "title": "A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure", "comment": "Part II of a series entitled \"A Thermodynamic Theory of Learning.\"", "summary": "Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.\n  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.\n  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \\emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.\n  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.", "AI": {"tldr": "\u8be5\u8bba\u6587\uff08\u7cfb\u5217\u7b2c\u4e8c\u90e8\u5206\uff09\u4ece\u8f68\u8ff9\u5c42\u9762\u7814\u7a76\u6709\u9650\u65f6\u95f4\u5b66\u4e60\u4e2d\u7684\u4e0d\u53ef\u9006\u6027\uff0c\u63ed\u793a\u4e86\u6709\u9650\u8017\u6563\u5982\u4f55\u7ea6\u675f\u5b66\u4e60\u8def\u5f84\u7684\u53ef\u8fbe\u6027\uff0c\u5bfc\u81f4\"\u5173\u952e\u671f\u5173\u95ed\"\u73b0\u8c61\uff0c\u4ece\u800c\u5c06\u707e\u96be\u6027\u9057\u5fd8\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6709\u9650\u65f6\u95f4\u8017\u6563\u65bd\u52a0\u7684\u52a8\u6001\u7ea6\u675f\u800c\u975e\u76f4\u63a5\u4efb\u52a1\u5e72\u6270\u3002", "motivation": "\u7814\u7a76\u6709\u9650\u65f6\u95f4\u5b66\u4e60\u4e2d\u7684\u4e0d\u53ef\u9006\u6027\u5982\u4f55\u5f71\u54cd\u6301\u7eed\u5b66\u4e60\uff0c\u7279\u522b\u662f\u4ece\u8f68\u8ff9\u5c42\u9762\u7406\u89e3\u5b66\u4e60\u8def\u5f84\u7684\u52a8\u6001\u53ef\u8fbe\u6027\u7ea6\u675f\u3002", "method": "\u5c06\u5b66\u4e60\u5efa\u6a21\u4e3a\u53c2\u6570\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u4f20\u8f93\u8fc7\u7a0b\uff0c\u4ece\u8f68\u8ff9\u5c42\u9762\u5206\u6790\u6709\u9650\u8017\u6563\u5bf9\u5b66\u4e60\u8def\u5f84\u7684\u7ea6\u675f\uff0c\u63d0\u51fa\"\u5173\u952e\u671f\u5173\u95ed\"\u6982\u5ff5\u6765\u63cf\u8ff0\u8868\u793a\u81ea\u7531\u5ea6\u7684\u6e10\u8fdb\u6d88\u9664\u3002", "result": "\u6709\u9650\u65f6\u95f4\u5b66\u4e60\u4e0d\u53ef\u9006\u5730\u9009\u62e9\u4efb\u52a1\u7b49\u6548\u5b9e\u73b0\uff0c\u901a\u8fc7\u6e10\u8fdb\u6d88\u9664\u80fd\u591f\u5b9e\u73b0\u7ed3\u6784\u91cd\u6784\u7684\u81ea\u7531\u5ea6\uff0c\u5bfc\u81f4\u5173\u952e\u671f\u5173\u95ed\u73b0\u8c61\uff0c\u4f7f\u5f97\u517c\u5bb9\u8868\u793a\u4e4b\u95f4\u7684\u8f6c\u6362\u5728\u6709\u9650\u8017\u6563\u9884\u7b97\u4e0b\u53d8\u5f97\u52a8\u6001\u4e0d\u53ef\u8fbe\u3002", "conclusion": "\u6301\u7eed\u5b66\u4e60\u5931\u8d25\u4e0d\u662f\u7531\u4e8e\u7f3a\u4e4f\u6ee1\u8db3\u591a\u4e2a\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u662f\u7531\u5148\u524d\u5b66\u4e60\u5f15\u8d77\u7684\u8868\u793a\u81ea\u7531\u5ea6\u7684\u4e0d\u53ef\u9006\u635f\u5931\u6240\u81f4\uff0c\u4ece\u800c\u5c06\u707e\u96be\u6027\u9057\u5fd8\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6709\u9650\u65f6\u95f4\u8017\u6563\u65bd\u52a0\u7684\u52a8\u6001\u7ea6\u675f\u800c\u975e\u76f4\u63a5\u4efb\u52a1\u5e72\u6270\u3002"}}
{"id": "2602.08058", "categories": ["cs.CV", "cs.AI", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.08058", "abs": "https://arxiv.org/abs/2602.08058", "authors": ["Xihang Yu", "Rajat Talak", "Lorenzo Shaikewitz", "Luca Carlone"], "title": "Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling", "comment": "15 pages", "summary": "In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.", "AI": {"tldr": "Picasso\uff1a\u4e00\u4e2a\u7269\u7406\u7ea6\u675f\u7684\u591a\u7269\u4f53\u573a\u666f\u91cd\u5efa\u7cfb\u7edf\uff0c\u901a\u8fc7\u8003\u8651\u51e0\u4f55\u3001\u975e\u7a7f\u900f\u6027\u548c\u7269\u7406\u7ea6\u675f\u6765\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u573a\u666f\u91cd\u5efa\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u906e\u6321\u548c\u6d4b\u91cf\u566a\u58f0\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u51e0\u4f55\u4e0a\u51c6\u786e\u4f46\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u7684\u573a\u666f\u91cd\u5efa\uff08\u5982\u7269\u4f53\u7a7f\u900f\u6216\u4e0d\u7a33\u5b9a\u5e73\u8861\uff09\u96be\u4ee5\u7528\u4e8e\u6570\u5b57\u5b6a\u751f\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u884c\u4e3a\u89c4\u5212\u3002\u9700\u8981\u4ece\u6574\u4f53\u573a\u666f\u89d2\u5ea6\u8003\u8651\u7269\u4f53\u4ea4\u4e92\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "method": "\u63d0\u51faPicasso\u7269\u7406\u7ea6\u675f\u91cd\u5efa\u7ba1\u9053\uff0c\u4f7f\u7528\u5feb\u901f\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\u8003\u8651\u591a\u7269\u4f53\u4ea4\u4e92\uff0c\u5229\u7528\u63a8\u65ad\u7684\u7269\u4f53\u63a5\u89e6\u56fe\u6307\u5bfc\u91c7\u6837\uff0c\u786e\u4fdd\u975e\u7a7f\u900f\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "result": "\u5728\u81ea\u5efa\u7684Picasso\u6570\u636e\u96c6\u548cYCB-V\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cPicasso\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u4f9b\u7269\u7406\u4e0a\u5408\u7406\u4e14\u66f4\u7b26\u5408\u4eba\u7c7b\u76f4\u89c9\u7684\u91cd\u5efa\u7ed3\u679c\u3002", "conclusion": "\u7269\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u4f30\u8ba1\u9700\u8981\u4ece\u6574\u4f53\u573a\u666f\u89d2\u5ea6\u8003\u8651\u7269\u7406\u7ea6\u675f\uff0cPicasso\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u91cd\u5efa\u7ba1\u9053\u5b9e\u73b0\u4e86\u7269\u7406\u4e0a\u5408\u7406\u7684\u591a\u7269\u4f53\u573a\u666f\u91cd\u5efa\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u884c\u4e3a\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002"}}
{"id": "2602.07966", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.07966", "abs": "https://arxiv.org/abs/2602.07966", "authors": ["Pablo Hidalgo", "Daniel Rodriguez"], "title": "An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fr\u00e9chet Distance", "comment": null, "summary": "In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.\n  ALE curves are compared using the Fr\u00e9chet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.\n  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\u7684\u591a\u4efb\u52a1\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4f7f\u7528\u7d2f\u79ef\u5c40\u90e8\u6548\u5e94\uff08ALE\uff09\u66f2\u7ebf\u548cFr\u00e9chet\u8ddd\u79bb\u6765\u8861\u91cf\u4efb\u52a1\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u8be5\u65b9\u6cd5\u6a21\u578b\u65e0\u5173\u4e14\u9002\u7528\u4e8e\u8868\u683c\u548c\u975e\u8868\u683c\u6570\u636e\u3002", "motivation": "\u5728\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u4e2d\uff0c\u4efb\u52a1\u901a\u5e38\u88ab\u89c6\u4e3a\u76f8\u4e92\u5173\u8054\u7684\u7ec4\u4ef6\uff0c\u76ee\u6807\u662f\u5229\u7528\u4efb\u52a1\u95f4\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u5173\u952e\u95ee\u9898\uff1a\u54ea\u4e9b\u4efb\u52a1\u662f\u76f8\u4f3c\u7684\uff1f\u5b83\u4eec\u5982\u4f55\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u8868\u73b0\u51fa\u76f8\u4f3c\u6027\uff1f\u73b0\u6709\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5ea6\u91cf\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\u7684\u591a\u4efb\u52a1\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u7d2f\u79ef\u5c40\u90e8\u6548\u5e94\uff08ALE\uff09\u66f2\u7ebf\u5206\u6790\u7279\u5f81\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\uff1b2\uff09\u901a\u8fc7\u52a0\u6743Fr\u00e9chet\u8ddd\u79bb\u6bd4\u8f83ALE\u66f2\u7ebf\uff1b3\uff09\u5f15\u5165\u7f29\u653e\u56e0\u5b50\u8003\u8651\u4e0d\u540c\u4efb\u52a1\u7684\u9884\u6d4b\u6027\u80fd\u5dee\u5f02\uff1b4\uff09\u8be5\u65b9\u6cd5\u6a21\u578b\u65e0\u5173\uff0c\u53ef\u5e94\u7528\u4e8e\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u573a\u666f\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u5ea6\u91cf\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a\u4e00\u4e2a\u5408\u6210\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08Parkinson\u6570\u636e\u96c6\u3001\u81ea\u884c\u8f66\u5171\u4eab\u4f7f\u7528\u6570\u636e\u96c6\u3001CelebA\u6570\u636e\u96c6\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u5ea6\u91cf\u65b9\u6cd5\u5728\u8868\u683c\u548c\u975e\u8868\u683c\u6570\u636e\u4e0a\u90fd\u4e0e\u76f4\u89c2\u7684\u4efb\u52a1\u76f8\u4f3c\u6027\u9884\u671f\u4e00\u81f4\uff0c\u80fd\u591f\u6709\u6548\u63a2\u7d22\u4efb\u52a1\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eALE\u66f2\u7ebf\u7684\u591a\u4efb\u52a1\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u80fd\u591f\u652f\u6301\u4efb\u52a1\u5173\u7cfb\u7684\u63a2\u7d22\u548c\u77e5\u60c5\u51b3\u7b56\u5236\u5b9a\u3002\u8be5\u65b9\u6cd5\u6a21\u578b\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u6570\u636e\u7c7b\u578b\uff0c\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u76f8\u4f3c\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08059", "abs": "https://arxiv.org/abs/2602.08059", "authors": ["Tong Zhang", "Ru Zhang", "Jianyi Liu"], "title": "DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models", "comment": null, "summary": "The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.", "AI": {"tldr": "DICE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u5b9e\u65f6\u53bb\u9664\u827a\u672f\u5bb6\u98ce\u683c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b50\u7a7a\u95f4\u5206\u89e3\u5c06\u98ce\u683c\u4e0e\u5185\u5bb9\u5206\u79bb\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u98ce\u683c\u6a21\u4eff\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u666e\u53ca\u4f7f\u5f97\u98ce\u683c\u6a21\u4eff\u53d8\u5f97\u5bb9\u6613\uff0c\u5f15\u53d1\u4e86\u7248\u6743\u548c\u77e5\u8bc6\u4ea7\u6743\u98ce\u9669\u3002\u73b0\u6709\u5bf9\u7b56\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u6743\u91cd\u7f16\u8f91\uff0c\u8981\u4e48\u4f9d\u8d56\u660e\u786e\u6307\u5b9a\u7684\u7f16\u8f91\u98ce\u683c\uff0c\u9650\u5236\u4e86\u90e8\u7f72\u4fa7\u7684\u5b89\u5168\u6027\u5b9e\u7528\u6027\u3002", "method": "DICE\u901a\u8fc7\u6784\u5efa\u5bf9\u6bd4\u4e09\u5143\u7ec4\uff0c\u8feb\u4f7f\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u533a\u5206\u98ce\u683c\u548c\u975e\u98ce\u683c\u7279\u5f81\uff0c\u5c06\u89e3\u8026\u8fc7\u7a0b\u5f62\u5f0f\u5316\u4e3a\u53ef\u89e3\u7684\u5e7f\u4e49\u7279\u5f81\u503c\u95ee\u9898\uff0c\u7cbe\u786e\u5b9a\u4f4d\u98ce\u683c\u5b50\u7a7a\u95f4\u3002\u91c7\u7528\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u89e3\u8026\u7f16\u8f91\u7b56\u7565\uff0c\u52a8\u6001\u8bc4\u4f30\u6bcf\u4e2atoken\u7684\u98ce\u683c\u6d53\u5ea6\uff0c\u5bf9QKV\u5411\u91cf\u8fdb\u884c\u5dee\u5f02\u5316\u6291\u5236\u548c\u5185\u5bb9\u589e\u5f3a\u3002", "result": "DICE\u5728\u98ce\u683c\u53bb\u9664\u7684\u5f7b\u5e95\u6027\u548c\u5185\u5bb9\u5b8c\u6574\u6027\u4fdd\u5b58\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5e73\u8861\uff0c\u4ec5\u9700\u989d\u59163\u79d2\u5373\u53ef\u89e3\u8026\u98ce\u683c\uff0c\u4e3a\u904f\u5236\u98ce\u683c\u6a21\u4eff\u63d0\u4f9b\u4e86\u5b9e\u7528\u9ad8\u6548\u7684\u6280\u672f\u3002", "conclusion": "DICE\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5b9e\u65f6\u53bb\u9664\u827a\u672f\u5bb6\u98ce\u683c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b50\u7a7a\u95f4\u5206\u89e3\u6709\u6548\u89e3\u51b3\u98ce\u683c\u6a21\u4eff\u95ee\u9898\uff0c\u4e3a\u90e8\u7f72\u4fa7\u5b89\u5168\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07973", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07973", "abs": "https://arxiv.org/abs/2602.07973", "authors": ["Aaditya Naik", "Efthymia Tsamoura", "Shibo Jin", "Mayur Naik", "Dan Roth"], "title": "On Improving Neurosymbolic Learning by Exploiting the Representation Space", "comment": null, "summary": "We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.", "AI": {"tldr": "CLIPPER\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u6570\u7ebf\u6027\u89c4\u5212\u526a\u679d\u6807\u7b7e\u7ec4\u5408\u7a7a\u95f4\uff0c\u63d0\u5347\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7684\u6027\u80fd", "motivation": "\u5728\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u5f53\u9690\u85cf\u7684\u771f\u5b9e\u6807\u7b7e\u5fc5\u987b\u6ee1\u8db3\u903b\u8f91\u516c\u5f0f\u65f6\uff0c\u53ef\u80fd\u7684\u6807\u7b7e\u7ec4\u5408\u7a7a\u95f4\u4f1a\u6307\u6570\u7ea7\u589e\u957f\uff0c\u5bfc\u81f4\u5b66\u4e60\u56f0\u96be", "method": "\u63d0\u51faCLIPPER\u65b9\u6cd5\uff0c\u5229\u7528\u76f8\u4f3c\u6f5c\u5728\u8868\u793a\u5b9e\u4f8b\u53ef\u80fd\u5171\u4eab\u76f8\u540c\u6807\u7b7e\u7684\u76f4\u89c9\uff0c\u901a\u8fc7\u6574\u6570\u7ebf\u6027\u89c4\u5212\u526a\u679d\u4e0d\u4e00\u81f4\u7684\u6807\u7b7e\u7ec4\u5408\uff0c\u540c\u65f6\u5c0a\u91cd\u903b\u8f91\u7ed3\u6784", "result": "\u572816\u4e2a\u590d\u6742\u795e\u7ecf\u7b26\u53f7\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLIPPER\u5c06Scallop\u3001Dolphin\u548cISED\u7b49\u6700\u5148\u8fdb\u795e\u7ecf\u7b26\u53f7\u5f15\u64ce\u7684\u6027\u80fd\u5206\u522b\u63d0\u534748%\u300153%\u548c8%\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u7387", "conclusion": "CLIPPER\u662f\u4e00\u79cd\u6b63\u4ea4\u4e8e\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\u7684\u526a\u679d\u6280\u672f\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7cfb\u7edf\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u6807\u7b7e\u7ec4\u5408\u7a7a\u95f4\u7206\u70b8\u95ee\u9898"}}
{"id": "2602.08068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08068", "abs": "https://arxiv.org/abs/2602.08068", "authors": ["Chunyang Li", "Yuanbo Yang", "Jiahao Shao", "Hongyu Zhou", "Katja Schwarz", "Yiyi Liao"], "title": "ReRoPE: Repurposing RoPE for Relative Camera Control", "comment": null, "summary": "Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/", "AI": {"tldr": "ReRoPE\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4fe1\u606f\u6ce8\u5165\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u4f4e\u9891\u9891\u6bb5\uff0c\u5b9e\u73b0\u53ef\u63a7\u89c6\u89d2\u7684\u89c6\u9891\u751f\u6210\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6216\u67b6\u6784\u4fee\u6539\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u76f8\u5bf9\u4e8e\u56fa\u5b9a\u53c2\u8003\u5e27\uff08\u5982\u7b2c\u4e00\u5e27\uff09\u7684\u76f8\u673a\u59ff\u6001\u7f16\u7801\uff0c\u7f3a\u4e4f\u5e73\u79fb\u4e0d\u53d8\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u7d2f\u79ef\u6f02\u79fb\u3002\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u5d4c\u5165\u867d\u7136\u66f4\u9c81\u68d2\uff0c\u4f46\u96be\u4ee5\u5728\u4e0d\u589e\u52a0\u5927\u91cf\u8bad\u7ec3\u6210\u672c\u6216\u4fee\u6539\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u3002", "method": "\u57fa\u4e8eRotary Positional Embeddings (RoPE)\u5728\u73b0\u6709\u6a21\u578b\u4e2d\u672a\u5145\u5206\u5229\u7528\u5176\u5168\u9891\u8c31\u5e26\u5bbd\uff08\u7279\u522b\u662f\u4f4e\u9891\u5206\u91cf\uff09\u7684\u6d1e\u5bdf\uff0cReRoPE\u5c06\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4fe1\u606f\u65e0\u7f1d\u6ce8\u5165\u8fd9\u4e9b\u672a\u5145\u5206\u5229\u7528\u7684\u9891\u6bb5\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u76f8\u673a\u63a7\u5236\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u7684\u751f\u6210\u5148\u9a8c\u3002", "result": "\u5728\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u548c\u89c6\u9891\u5230\u89c6\u9891\uff08V2V\uff09\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cReRoPE\u5728\u76f8\u673a\u63a7\u5236\u7cbe\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bad\u7ec3\u9ad8\u6548\u7684\u8def\u5f84\u6765\u5b9e\u73b0\u53ef\u63a7\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u3002", "conclusion": "ReRoPE\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u76f8\u5bf9\u76f8\u673a\u4fe1\u606f\u96c6\u6210\u5230\u6a21\u578b\u4e2d\uff0c\u4e3a\u53ef\u63a7\u3001\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u8bad\u7ec3\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.07974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07974", "abs": "https://arxiv.org/abs/2602.07974", "authors": ["Xin Li"], "title": "Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness", "comment": null, "summary": "Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5ea6\u91cf-\u62d3\u6251\u5206\u89e3\uff08MTF\uff09\u4f5c\u4e3a\u667a\u80fd\u7684\u51e0\u4f55\u539f\u7406\uff1a\u667a\u80fd\u4e0d\u662f\u56fa\u5b9a\u51e0\u4f55\u4e2d\u7684\u4f18\u5316\uff0c\u800c\u662f\u91cd\u5851\u8868\u793a\u51e0\u4f55\u4ee5\u4f7f\u671f\u671b\u884c\u4e3a\u6210\u4e3a\u7a33\u5b9a\u5438\u5f15\u5b50\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5c06\u7a33\u5b9a\u62d3\u6251\u4e0e\u53ef\u5851\u5ea6\u91cf\u53d8\u5f62\u5206\u79bb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfML\u5728\u5206\u5e03\u504f\u79fb\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5c06\u667a\u80fd\u7b49\u540c\u4e8e\u5728\u56fa\u5b9a\u8868\u793a\u51e0\u4f55\u4e2d\u7684\u4f18\u5316\uff0c\u8fd9\u5728\u9759\u6001\u73af\u5883\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\u3001\u4efb\u52a1\u7f6e\u6362\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u4f1a\u5931\u6548\u3002\u5373\u4f7f\u8f7b\u5fae\u7684\u62d3\u6251\u53d8\u5316\u4e5f\u4f1a\u4f7f\u5b66\u4e60\u5230\u7684\u89e3\u51b3\u65b9\u6848\u5931\u6548\u5e76\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8\u3002\u9700\u8981\u65b0\u7684\u51e0\u4f55\u539f\u7406\u6765\u89e3\u51b3\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5ea6\u91cf-\u62d3\u6251\u5206\u89e3\uff08MTF\uff09\u4f5c\u4e3a\u7edf\u4e00\u51e0\u4f55\u539f\u7406\uff1a\u5c06\u8868\u793a\u51e0\u4f55\u5206\u89e3\u4e3a\u7a33\u5b9a\u7684\u62d3\u6251\u7ed3\u6784\u548c\u53ef\u5851\u7684\u5ea6\u91cf\u53d8\u5f62\u3002\u5b66\u4e60\u5bf9\u5e94\u5ea6\u91cf\u6536\u7f29\uff08\u9ece\u66fc\u7ed3\u6784\u7684\u53d7\u63a7\u53d8\u5f62\uff09\uff0c\u800c\u4efb\u52a1\u8eab\u4efd\u548c\u73af\u5883\u53d8\u5316\u88ab\u7f16\u7801\u4e3a\u62d3\u6251\u7279\u5f81\u5e76\u5b58\u50a8\u5728\u8bb0\u5fc6\u4e2d\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u62d3\u6251Urysohn\u673a\uff08TUM\uff09\uff0c\u901a\u8fc7\u8bb0\u5fc6\u644a\u9500\u5ea6\u91cf\u63a8\u65ad\uff08MAMI\uff09\u5b9e\u73b0MTF\uff1a\u8c31\u4efb\u52a1\u7b7e\u540d\u7d22\u5f15\u644a\u9500\u7684\u5ea6\u91cf\u53d8\u6362\uff0c\u4f7f\u5355\u4e2a\u5b66\u4e60\u5230\u7684\u51e0\u4f55\u53ef\u4ee5\u5728\u7f6e\u6362\u3001\u53cd\u5c04\u6216\u5947\u5076\u6027\u6539\u53d8\u7684\u73af\u5883\u4e2d\u91cd\u7528\u3002", "result": "MTF\u89e3\u51b3\u4e86\u4efb\u4f55\u56fa\u5b9a\u5ea6\u91cf\u8868\u793a\u7684\u51e0\u4f55\u4e0d\u5b8c\u6574\u6027\uff1a\u5bf9\u4e8e\u4efb\u4f55\u5c40\u90e8\u5ea6\u91cf\u8868\u793a\uff0c\u67d0\u4e9b\u62d3\u6251\u53d8\u6362\u4f1a\u4f7f\u5176\u5947\u5f02\u6216\u4e0d\u8fde\u8d2f\uff0c\u8fd9\u610f\u5473\u7740\u57fa\u4e8e\u6743\u91cd\u7684\u7cfb\u7edf\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u6743\u8861\u3002MTF\u901a\u8fc7\u5206\u89e3\u7a33\u5b9a\u62d3\u6251\u548c\u53ef\u5851\u5ea6\u91cf\u53d8\u5f62\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u901a\u8fc7\u51e0\u4f55\u5207\u6362\u800c\u975e\u91cd\u65b0\u4f18\u5316\u7684\u5feb\u901f\u9002\u5e94\u3002", "conclusion": "MTF\u4e3a\u7406\u89e3\u667a\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u91cd\u65b0\u5b9a\u4e49\u4e3a\u91cd\u5851\u8868\u793a\u51e0\u4f55\u7684\u80fd\u529b\u800c\u975e\u56fa\u5b9a\u51e0\u4f55\u4e2d\u7684\u4f18\u5316\u3002\u8fd9\u89e3\u91ca\u4e86\u4efb\u52a1\u91cd\u6392\u7684\u9c81\u68d2\u6027\u3001\u5bf9\u707e\u96be\u6027\u9057\u5fd8\u7684\u62b5\u6297\u6027\uff0c\u4ee5\u53ca\u8de8\u53d8\u6362\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u4e9b\u80fd\u529b\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff08\u5982EWC\uff09\u3002"}}
{"id": "2602.07992", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.07992", "abs": "https://arxiv.org/abs/2602.07992", "authors": ["Daniel Barzilai", "Yotam Wolf", "Ronen Basri"], "title": "When Is Compositional Reasoning Learnable from Verifiable Rewards?", "comment": null, "summary": "The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7406\u8bba\u7814\u7a76\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60(RLVR)\u4e0b\u81ea\u56de\u5f52\u6a21\u578b\u7ec4\u5408\u95ee\u9898\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u63d0\u51fa\u4e86\u4efb\u52a1\u4f18\u52bf\u6bd4\u7684\u6982\u5ff5\u6765\u523b\u753b\u54ea\u4e9b\u4efb\u52a1\u80fd\u4ece\u7ed3\u679c\u7ea7\u53cd\u9988\u4e2d\u5b66\u4e60", "motivation": "\u5c3d\u7ba1RLVR\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ec4\u5408\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u7ecf\u9a8c\u6210\u529f\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u54ea\u4e9b\u7ec4\u5408\u95ee\u9898\u4ec5\u901a\u8fc7\u7ed3\u679c\u7ea7\u53cd\u9988\u5c31\u80fd\u5b66\u4e60\uff0c\u9700\u8981\u7406\u8bba\u5206\u6790\u5176\u53ef\u5b66\u4e60\u6027", "method": "\u7406\u8bba\u7814\u7a76\u81ea\u56de\u5f52\u6a21\u578b\u5728RLVR\u8bad\u7ec3\u4e0b\u7684\u7ec4\u5408\u95ee\u9898\u53ef\u5b66\u4e60\u6027\uff0c\u63d0\u51fa\u4efb\u52a1\u4f18\u52bf\u6bd4\u4f5c\u4e3a\u5173\u952e\u6307\u6807\uff0c\u5206\u6790\u4e0d\u540c\u95ee\u9898\u4e2d\u4f18\u52bf\u5982\u4f55\u81ea\u7136\u4ea7\u751f", "result": "\u5f53\u6b63\u786e\u4e2d\u95f4\u6b65\u9aa4\u63d0\u4f9b\u660e\u663e\u4f18\u52bf\u65f6\uff0c\u7ec4\u5408\u95ee\u9898\u53ef\u901a\u8fc7RLVR\u9ad8\u6548\u5b66\u4e60\uff1b\u5f53\u7ed3\u6784\u4f18\u52bf\u4e0d\u5b58\u5728\u65f6\uff0cRLVR\u53ef\u80fd\u6536\u655b\u5230\u6b21\u4f18\u7ec4\u5408\uff1b\u57fa\u7840\u6a21\u578b\u8d28\u91cf\u5f71\u54cd\u4f18\u52bf\u5b58\u5728\u4e0e\u5426", "conclusion": "\u4efb\u52a1\u4f18\u52bf\u6bd4\u662f\u51b3\u5b9aRLVR\u6210\u529f\u4e0e\u5426\u7684\u5173\u952e\u56e0\u7d20\uff0c\u8be5\u5206\u6790\u4e3a\u7406\u89e3RLVR\u4f55\u65f6\u6210\u529f\u3001\u4f55\u65f6\u5931\u8d25\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7406\u8bba\u6846\u67b6"}}
{"id": "2602.08099", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08099", "abs": "https://arxiv.org/abs/2602.08099", "authors": ["Issar Tzachor", "Dvir Samuel", "Rami Ben-Ari"], "title": "VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval", "comment": "Project page: https://iyttor.github.io/VidVec/", "summary": "Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c6\u9891\u6587\u672c\u5d4c\u5165\u548c\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u89c6\u89c9\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u4e3a\u5d4c\u5165\u63d0\u53d6\u5668\u7528\u4e8e\u89c6\u89c9\u4efb\u52a1\uff0c\u4f46\u5b83\u4eec\u5728\u89c6\u9891\u4e0a\u7684\u6027\u80fd\u4ecd\u4e0d\u5982\u4e13\u95e8\u7684\u89c6\u9891\u57fa\u7840\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u66f4\u597d\u5730\u5229\u7528MLLMs\u8fdb\u884c\u89c6\u9891\u6587\u672c\u5d4c\u5165\u548c\u68c0\u7d22\u3002", "method": "\u9996\u5148\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u5c42\u7ea7\u5206\u6790\uff0c\u53d1\u73b0\u4e2d\u95f4\u5c42\u5df2\u7f16\u7801\u5927\u91cf\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff1b\u7136\u540e\u7ed3\u5408\u4e2d\u95f4\u5c42\u5d4c\u5165\u548c\u6821\u51c6\u7684MLLM\u5934\u90e8\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u7d22\uff1b\u6700\u540e\u5f15\u5165\u8f7b\u91cf\u7ea7\u6587\u672c\u5bf9\u9f50\u7b56\u7565\uff0c\u5c06\u5bc6\u96c6\u89c6\u9891\u63cf\u8ff0\u6620\u5c04\u5230\u7b80\u77ed\u6458\u8981\uff0c\u5b9e\u73b0\u65e0\u9700\u89c6\u89c9\u76d1\u7763\u7684\u89c6\u9891\u6587\u672c\u5d4c\u5165\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5e38\u89c1\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5f53\u524d\u65b9\u6cd5\uff0c\u901a\u5e38\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u89c6\u89c9\u76d1\u7763\u6216\u5fae\u8c03\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5229\u7528MLLMs\u4e2d\u95f4\u5c42\u7684\u4e30\u5bcc\u4fe1\u606f\u5e76\u7ed3\u5408\u6587\u672c\u5bf9\u9f50\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u65e0\u9700\u89c6\u89c9\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f3a\u5927\u7684\u89c6\u9891\u6587\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.08000", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08000", "abs": "https://arxiv.org/abs/2602.08000", "authors": ["Anirudh Satheesh", "Vaneet Aggarwal"], "title": "Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization", "comment": null, "summary": "We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\\tilde{O}(\\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5355\u94fe\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u539f\u59cb-\u5bf9\u5076\u81ea\u7136\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u4f7f\u7528\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\u548c\u663e\u5f0f\u9884\u70ed\u673a\u5236\uff0c\u65e0\u9700\u6df7\u5408\u65f6\u95f4\u9884\u8a00\u673a\uff0c\u83b7\u5f97\u221aT\u91cf\u7ea7\u7684\u9057\u61be\u548c\u7ea6\u675f\u8fdd\u53cd\u8fb9\u754c\u3002", "motivation": "\u73b0\u6709\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u9057\u61be\u5206\u6790\u4e3b\u8981\u4f9d\u8d56\u4e8e\u904d\u5386\u6027\u6216\u5f3a\u6df7\u5408\u65f6\u95f4\u5047\u8bbe\uff0c\u8fd9\u4e9b\u5047\u8bbe\u5728\u5b58\u5728\u77ac\u6001\u72b6\u6001\u65f6\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u5355\u94fe\u52a8\u6001\u4e14\u65e0\u9700\u6df7\u5408\u65f6\u95f4\u9884\u8a00\u673a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u539f\u59cb-\u5bf9\u5076\u81ea\u7136\u884c\u52a8\u8005-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\u548c\u663e\u5f0f\u9884\u70ed\u673a\u5236\u6765\u5904\u7406\u5355\u94fe\u52a8\u6001\uff0c\u65e0\u9700\u6df7\u5408\u65f6\u95f4\u5047\u8bbe\u3002", "result": "\u5efa\u7acb\u4e86\u6709\u9650\u65f6\u95f4\u9057\u61be\u548c\u7d2f\u79ef\u7ea6\u675f\u8fdd\u53cd\u8fb9\u754c\uff0c\u5c3a\u5ea6\u4e3a\u00d5(\u221aT)\uff0c\u53d7\u9650\u4e8e\u7b56\u7565\u548c\u8bc4\u8bba\u5bb6\u53c2\u6570\u5316\u4ea7\u751f\u7684\u8fd1\u4f3c\u8bef\u5dee\uff0c\u5c06\u6700\u4f18\u9636\u4fdd\u8bc1\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684CMDP\u7c7b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u6700\u4f18\u9636\u9057\u61be\u4fdd\u8bc1\u6269\u5c55\u5230\u5355\u94fe\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u65e0\u9700\u904d\u5386\u6027\u6216\u6df7\u5408\u65f6\u95f4\u5047\u8bbe\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.08112", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08112", "abs": "https://arxiv.org/abs/2602.08112", "authors": ["Sidike Paheding", "Abel Reyes-Angulo", "Leo Thomas Ramos", "Angel D. Sappa", "Rajaneesh A.", "Hiral P. B.", "Sajin Kumar K. S.", "Thomas Oommen"], "title": "MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery", "comment": null, "summary": "We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2", "AI": {"tldr": "MMLSv2\u662f\u4e00\u4e2a\u7528\u4e8e\u706b\u661f\u8868\u9762\u6ed1\u5761\u5206\u5272\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b7\u4e2a\u6ce2\u6bb5\u56fe\u50cf\uff0c\u5171664\u5f20\u56fe\u50cf\uff0c\u5e76\u989d\u5916\u63d0\u4f9b276\u5f20\u5730\u7406\u9694\u79bb\u6d4b\u8bd5\u96c6\u4ee5\u8bc4\u4f30\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u706b\u661f\u6ed1\u5761\u7814\u7a76\u7f3a\u4e4f\u4e13\u95e8\u7684\u591a\u6a21\u6001\u5206\u5272\u6570\u636e\u96c6\uff0c\u9700\u8981\u80fd\u591f\u8bc4\u4f30\u6a21\u578b\u5728\u788e\u7247\u5316\u3001\u7ec6\u957f\u548c\u5c0f\u5c3a\u5ea6\u6ed1\u5761\u533a\u57df\u6027\u80fd\u7684\u6570\u636e\u96c6\uff0c\u540c\u65f6\u9700\u8981\u6d4b\u8bd5\u6a21\u578b\u5728\u5730\u7406\u9694\u79bb\u533a\u57df\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542bRGB\u3001\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u3001\u5761\u5ea6\u3001\u70ed\u60ef\u6027\u548c\u7070\u5ea6\u901a\u9053\u7684\u4e03\u6ce2\u6bb5\u591a\u6a21\u6001\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b664\u5f20\u56fe\u50cf\u5206\u4e3a\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\uff0c\u5e76\u989d\u5916\u63d0\u4f9b276\u5f20\u5730\u7406\u9694\u79bb\u6d4b\u8bd5\u96c6\u3002", "result": "\u591a\u4e2a\u5206\u5272\u6a21\u578b\u5728\u6570\u636e\u96c6\u4e0a\u80fd\u591f\u7a33\u5b9a\u8bad\u7ec3\u5e76\u8fbe\u5230\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u4f46\u5728\u788e\u7247\u5316\u3001\u7ec6\u957f\u548c\u5c0f\u5c3a\u5ea6\u6ed1\u5761\u533a\u57df\u4ecd\u9762\u4e34\u6311\u6218\uff1b\u5728\u5730\u7406\u9694\u79bb\u6d4b\u8bd5\u96c6\u4e0a\u6027\u80fd\u660e\u663e\u4e0b\u964d\uff0c\u8868\u660e\u5176\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u4ef7\u503c\u3002", "conclusion": "MMLSv2\u6570\u636e\u96c6\u4e3a\u706b\u661f\u6ed1\u5761\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\uff0c\u7279\u522b\u9002\u5408\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u6ed1\u5761\u5f62\u6001\u548c\u7a7a\u95f4\u6cdb\u5316\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2602.08003", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08003", "abs": "https://arxiv.org/abs/2602.08003", "authors": ["Yigit Turkmen", "Baturalp Buyukates", "Melih Bastopcu"], "title": "Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection", "comment": null, "summary": "Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u6700\u5927\u5316\u7684\u9884\u7b97\u7ea6\u675f\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af-\u8fde\u63a5\u51fd\u6570\u5efa\u6a21\u6a21\u578b\u95f4\u76f8\u5173\u8bef\u5dee\uff0c\u5e76\u8bbe\u8ba1\u8d2a\u5fc3\u7b97\u6cd5\u5728\u67e5\u8be2\u9884\u7b97\u5185\u6784\u5efa\u6700\u4f18\u96c6\u6210\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u65f6\u6a21\u578b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u5728\u5f62\u6210LLM\u96c6\u6210\u65f6\u5e94\u9009\u62e9\u54ea\u4e9b\u6a21\u578b\uff1f\u540c\u65f6\u9700\u8981\u89e3\u91ca\u4e3a\u4f55\u5373\u4f7f\u4f7f\u7528\u591a\u4e2a\u6a21\u578b\u6027\u80fd\u4e5f\u4f1a\u9971\u548c\u3002", "method": "\u5c06\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u96c6\u6210\u9009\u62e9\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6700\u5927\u5316\u771f\u5b9e\u6807\u7b7e\u4e0e\u6240\u9009\u6a21\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u3002\u4f7f\u7528\u9ad8\u65af-\u8fde\u63a5\u51fd\u6570\u5efa\u6a21\u6a21\u578b\u95f4\u7684\u76f8\u5173\u8bef\u5dee\uff0c\u63d0\u51fa\u8d2a\u5fc3\u4e92\u4fe1\u606f\u9009\u62e9\u7b97\u6cd5\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u4f30\u8ba1\u6240\u9700\u4fe1\u606f\u9879\uff0c\u5728\u67e5\u8be2\u9884\u7b97\u5185\u8fed\u4ee3\u6784\u5efa\u96c6\u6210\u3002", "result": "\u5728MEDMCQA\u3001MMLU\u548cIMDB\u7535\u5f71\u8bc4\u8bba\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u67e5\u8be2\u9884\u7b97\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4fe1\u606f\u8bba\u6846\u67b6\u5206\u6790LLM\u96c6\u6210\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u8d2a\u5fc3\u4e92\u4fe1\u606f\u9009\u62e9\u7b97\u6cd5\u80fd\u6709\u6548\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u6784\u5efa\u9ad8\u6027\u80fd\u6a21\u578b\u96c6\u6210\uff0c\u4e3a\u89e3\u51b3\u6a21\u578b\u95f4\u5f3a\u76f8\u5173\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08117", "abs": "https://arxiv.org/abs/2602.08117", "authors": ["Smriti Siva", "Jan Cross-Zamirski"], "title": "Building Damage Detection using Satellite Images and Patch-Based Transformer Methods", "comment": "8 pages, 5 figures", "summary": "Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.\n  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86Vision Transformer\u6a21\u578b\u5728xBD\u6570\u636e\u96c6\u4e0a\u7684\u5efa\u7b51\u635f\u4f24\u5206\u7c7b\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6027\u7684\u8865\u4e01\u9884\u5904\u7406\u6d41\u7a0b\u548c\u51bb\u7ed3\u5934\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u566a\u58f0\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u4e0eCNN\u57fa\u7ebf\u76f8\u5f53\u7684\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "motivation": "\u5feb\u901f\u5efa\u7b51\u635f\u4f24\u8bc4\u4f30\u5bf9\u707e\u540e\u54cd\u5e94\u81f3\u5173\u91cd\u8981\u3002\u536b\u661f\u5f71\u50cf\u635f\u4f24\u5206\u7c7b\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6001\u52bf\u611f\u77e5\u624b\u6bb5\uff0c\u4f46\u536b\u661f\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u5e26\u6765\u4e86\u4e3b\u8981\u6311\u6218\u3002xBD\u6570\u636e\u96c6\u4e3a\u8de8\u5730\u7406\u533a\u57df\u7684\u5efa\u7b51\u7ea7\u635f\u4f24\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u8bc4\u4f30\u4e86DINOv2-small\u548cDeiT\u6a21\u578b\u8fdb\u884c\u591a\u7c7b\u635f\u4f24\u5206\u7c7b\u3002\u63d0\u51fa\u4e86\u9488\u5bf9\u6027\u7684\u8865\u4e01\u9884\u5904\u7406\u6d41\u7a0b\u6765\u9694\u79bb\u7ed3\u6784\u7279\u5f81\u5e76\u6700\u5c0f\u5316\u8bad\u7ec3\u4e2d\u7684\u80cc\u666f\u566a\u58f0\u3002\u91c7\u7528\u51bb\u7ed3\u5934\u5fae\u8c03\u7b56\u7565\u4ee5\u4fdd\u6301\u8ba1\u7b97\u9700\u6c42\u53ef\u63a7\u3002\u901a\u8fc7\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548c\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u91c7\u7528\u65b0\u9896\u8bad\u7ec3\u65b9\u6cd5\u7684\u5c0f\u578bViT\u67b6\u6784\u5728\u707e\u5bb3\u5206\u7c7b\u65b9\u9762\u76f8\u5bf9\u4e8e\u5148\u524d\u7684CNN\u57fa\u7ebf\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7684\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\u3002", "conclusion": "\u5c0f\u578bVision Transformer\u67b6\u6784\u7ed3\u5408\u9488\u5bf9\u6027\u7684\u9884\u5904\u7406\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u5728\u566a\u58f0\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u536b\u661f\u5f71\u50cf\u6570\u636e\u4e0a\u6709\u6548\u8fdb\u884c\u5efa\u7b51\u635f\u4f24\u5206\u7c7b\uff0c\u4e3a\u5feb\u901f\u707e\u540e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08007", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08007", "abs": "https://arxiv.org/abs/2602.08007", "authors": ["Sizhe Dang", "Jiaqi Shao", "Xiaodong Zheng", "Guang Dai", "Yan Song", "Haishan Ye"], "title": "From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency", "comment": null, "summary": "As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\\top G V\\in\\mathbb{R}^{r\\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\\times$, and on GLUE fine-tuning it reduces communication by $25\\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.", "AI": {"tldr": "TSR-Adam\u662f\u4e00\u79cd\u9488\u5bf9\u5e26\u5bbd\u53d7\u9650\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u4f4e\u79e9\u901a\u4fe1\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u6b65\u7d27\u51d1\u7684\u6838\u5fc3\u77e9\u9635\u5c06\u6bcf\u6b65\u901a\u4fe1\u8d1f\u8f7d\u4eceO(mn)\u964d\u4f4e\u5230O(r\u00b2)\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u6570\u636e\u5e76\u884c\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u4e2d\u5e26\u5bbd\u53d7\u9650\u7684\u68af\u5ea6\u540c\u6b65\u6210\u4e3a\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6295\u5f71\u7684\u4f4e\u79e9\u4f18\u5316\u5668\u4e3b\u8981\u4e3a\u5185\u5b58\u6548\u7387\u8bbe\u8ba1\uff0c\u5728\u901a\u4fe1\u53d7\u9650\u7684\u8bad\u7ec3\u4e2d\u4ecd\u4e0d\u7406\u60f3\uff0c\u56e0\u4e3a\u5355\u8fb9\u540c\u6b65\u4ecd\u9700\u4f20\u8f93O(rn)\u5bf9\u8c61\uff0c\u4e14\u5237\u65b0\u6b65\u9aa4\u53ef\u80fd\u4e3b\u5bfc\u5cf0\u503c\u901a\u4fe1\u5b57\u8282", "method": "\u63d0\u51faTSR-Adam\u65b9\u6cd5\uff1a1\uff09\u901a\u8fc7\u540c\u6b65\u7d27\u51d1\u6838\u5fc3U\u22a4GV\u2208\u211d^{r\u00d7r}\u5b9e\u73b0Adam\u65cf\u66f4\u65b0\u7684\u53cc\u8fb9\u4f4e\u79e9\u901a\u4fe1\uff0c\u5c06\u4e3b\u8981\u6bcf\u6b65\u8d1f\u8f7d\u4eceO(mn)\u51cf\u5c11\u5230O(r\u00b2)\uff1b2\uff09\u91c7\u7528\u57fa\u4e8e\u968f\u673aSVD\u7684\u5237\u65b0\u907f\u514d\u5168\u68af\u5ea6\u540c\u6b65\uff1b3\uff09\u5c06\u4f4e\u79e9\u901a\u4fe1\u6269\u5c55\u5230\u5d4c\u5165\u68af\u5ea6\uff0c\u4f7f\u7528\u5d4c\u5165\u7279\u5b9a\u79e9\u548c\u5237\u65b0\u8ba1\u5212", "result": "\u5728\u4ece6000\u4e07\u523010\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\uff0cTSR-Adam\u5c06\u5e73\u5747\u6bcf\u6b65\u901a\u4fe1\u5b57\u8282\u51cf\u5c1113\u500d\uff1b\u5728GLUE\u5fae\u8c03\u4e2d\u51cf\u5c11\u901a\u4fe125\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "TSR-Adam\u901a\u8fc7\u53cc\u8fb9\u4f4e\u79e9\u901a\u4fe1\u548c\u968f\u673aSVD\u5237\u65b0\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u800c\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u901a\u4fe1\u53d7\u9650\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08012", "abs": "https://arxiv.org/abs/2602.08012", "authors": ["Riccardo De Santi", "Malte Franke", "Ya-Ping Hsieh", "Andreas Krause"], "title": "A Unified Density Operator View of Flow Control and Merging", "comment": null, "summary": "Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6982\u7387\u7a7a\u95f4\u6846\u67b6\uff0c\u5c06\u6d41\u6a21\u578b\u7684\u5956\u52b1\u9002\u5e94\u4e0e\u6a21\u578b\u5408\u5e76\u7edf\u4e00\u5904\u7406\uff0c\u652f\u6301\u5956\u52b1\u5f15\u5bfc\u7684\u6d41\u5408\u5e76\uff0c\u5b9e\u73b0\u4efb\u52a1\u611f\u77e5\u7684\u591a\u9884\u8bad\u7ec3\u6d41\u7ec4\u5408", "motivation": "\u5927\u89c4\u6a21\u6d41\u548c\u6269\u6563\u6a21\u578b\u7684\u53d1\u5c55\u5e26\u6765\u4e86\u4e24\u4e2a\u57fa\u672c\u7b97\u6cd5\u6311\u6218\uff1a\u57fa\u4e8e\u63a7\u5236\u7684\u9884\u8bad\u7ec3\u6d41\u5956\u52b1\u9002\u5e94\u548c\u591a\u4e2a\u6a21\u578b\u7684\u96c6\u6210\uff08\u6d41\u5408\u5e76\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u5904\u7406\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6846\u67b6\u6765\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u6311\u6218", "method": "\u5f15\u5165\u7edf\u4e00\u7684\u6982\u7387\u7a7a\u95f4\u6846\u67b6\uff0c\u5c06\u5956\u52b1\u9002\u5e94\u548c\u6d41\u5408\u5e76\u4f5c\u4e3a\u6781\u9650\u60c5\u51b5\u5305\u542b\u5176\u4e2d\uff1b\u63d0\u51fa\u5956\u52b1\u5f15\u5bfc\u6d41\u5408\u5e76\uff08RFM\uff09\uff0c\u4f7f\u7528\u955c\u50cf\u4e0b\u964d\u65b9\u6848\u5c06\u5956\u52b1\u5f15\u5bfc\u6d41\u5408\u5e76\u8f6c\u5316\u4e3a\u4e00\u7cfb\u5217\u6807\u51c6\u5fae\u8c03\u95ee\u9898", "result": "\u4e3a\u5956\u52b1\u5f15\u5bfc\u548c\u7eaf\u6d41\u5408\u5e76\u63d0\u4f9b\u4e86\u9996\u4e2a\u7406\u8bba\u4fdd\u8bc1\uff1b\u5728\u5206\u5b50\u8bbe\u8ba1\u548c\u4f4e\u80fd\u6784\u8c61\u751f\u6210\u7b49\u9ad8\u7ef4\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u591f\u8868\u8fbe\u4e30\u5bcc\u7684\u751f\u6210\u6a21\u578b\u5bc6\u5ea6\u7b97\u5b50\uff0c\u5305\u62ec\u4ea4\u96c6\u3001\u5e76\u96c6\u3001\u63d2\u503c\u53ca\u5176\u5956\u52b1\u5f15\u5bfc\u7248\u672c\uff0c\u901a\u8fc7\u751f\u6210\u7535\u8def\u652f\u6301\u590d\u6742\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u7ec4\u5408\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5"}}
{"id": "2602.08131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08131", "abs": "https://arxiv.org/abs/2602.08131", "authors": ["Isaac Corley", "Hannah Kerner", "Caleb Robinson", "Jennifer Marcus"], "title": "Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries", "comment": null, "summary": "Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).", "AI": {"tldr": "Fields of The World (FTW)\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5168\u7403\u519c\u7530\u8fb9\u754c\u6570\u636e\u96c6\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5de5\u5177\uff0c\u652f\u6301\u519c\u7530\u8fb9\u754c\u63d0\u53d6\u3001\u4f5c\u7269\u5206\u7c7b\u548c\u68ee\u6797\u635f\u5931\u5f52\u56e0\u7b49\u519c\u4e1a\u5e94\u7528\u3002", "motivation": "\u519c\u7530\u8fb9\u754c\u5730\u56fe\u662f\u519c\u4e1a\u6570\u636e\u4ea7\u54c1\u7684\u57fa\u7840\uff0c\u5bf9\u4e8e\u4f5c\u7269\u76d1\u6d4b\u3001\u4ea7\u91cf\u4f30\u7b97\u548c\u75be\u75c5\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u6807\u51c6\u5316\u7684\u5168\u7403\u519c\u7530\u8fb9\u754c\u6570\u636e\u96c6\u548c\u5de5\u5177\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b160\u4e07\u519c\u7530\u591a\u8fb9\u5f62\u3001\u8986\u76d624\u4e2a\u56fd\u5bb6\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u5f00\u53d1\u4e86\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u548c\u547d\u4ee4\u884c\u63a8\u7406\u5de5\u5177\uff1b\u4f7f\u7528MOSAIKS\u968f\u673a\u5377\u79ef\u7279\u5f81\u548cFTW\u519c\u7530\u8fb9\u754c\u8fdb\u884c\u4f5c\u7269\u7c7b\u578b\u5206\u7c7b\uff1b\u63d0\u4f9b\u672c\u5730\u5c3a\u5ea6\u548c\u56fd\u5bb6\u5c3a\u5ea6\u7684\u63a8\u7406\u7b14\u8bb0\u672c\u3002", "result": "\u4f5c\u7269\u7c7b\u578b\u5206\u7c7b\u7684\u5b8f\u89c2F1\u5206\u6570\u8fbe\u52300.65-0.75\uff08\u4f7f\u7528\u6709\u9650\u6807\u7b7e\uff09\uff1b\u57285\u4e2a\u56fd\u5bb6\uff08476\u4e07\u5e73\u65b9\u516c\u91cc\uff09\u4e0a\u5c55\u793a\u4e86\u9884\u8ba1\u7b97\u9884\u6d4b\u7ed3\u679c\uff0c\u9884\u6d4b\u519c\u7530\u4e2d\u4f4d\u6570\u9762\u79ef\u4ece0.06\u516c\u9877\uff08\u5362\u65fa\u8fbe\uff09\u52300.28\u516c\u9877\uff08\u745e\u58eb\uff09\u3002", "conclusion": "FTW\u751f\u6001\u7cfb\u7edf\u4e3a\u519c\u4e1a\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5de5\u5177\u548c\u6570\u636e\u96c6\uff0c\u80fd\u591f\u652f\u6301\u4ece\u672c\u5730\u5230\u56fd\u5bb6\u5c3a\u5ea6\u7684\u519c\u7530\u8fb9\u754c\u63d0\u53d6\u548c\u4f5c\u7269\u5206\u7c7b\u4efb\u52a1\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u548c\u7cae\u98df\u5b89\u5168\u76d1\u6d4b\u63d0\u4f9b\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2602.08136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08136", "abs": "https://arxiv.org/abs/2602.08136", "authors": ["Md Rafi Ur Rashid", "MD Sadik Hossain Shanto", "Vishnu Asutosh Dasu", "Shagufta Mehnaz"], "title": "Robustness of Vision Language Models Against Split-Image Harmful Input Attacks", "comment": "22 Pages, long conference paper", "summary": "Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6f0f\u6d1e\uff1a\u867d\u7136VLM\u5728\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u4e2d\u80fd\u5904\u7406\u5206\u5272\u56fe\u50cf\uff0c\u4f46\u5b89\u5168\u5bf9\u9f50\u901a\u5e38\u53ea\u5728\u5b8c\u6574\u56fe\u50cf\u4e0a\u8fdb\u884c\uff0c\u5bfc\u81f4\u65e0\u6cd5\u68c0\u6d4b\u8de8\u591a\u4e2a\u56fe\u50cf\u7247\u6bb5\u5206\u5e03\u7684\u6709\u5bb3\u8bed\u4e49\u3002\u4f5c\u8005\u5f00\u53d1\u4e86SIVA\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u7b56\u7565\u548c\u5bf9\u6297\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u9ad8\u8f6c\u79fb\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u901a\u8fc7RLHF\u7b49\u504f\u597d\u4f18\u5316\u6280\u672f\u589e\u5f3a\u4e86\u5b89\u5168\u6027\uff0c\u4f46\u8fd9\u4e9b\u5b89\u5168\u5bf9\u9f50\u4e3b\u8981\u9488\u5bf9\u5b8c\u6574\u56fe\u50cf\u3002\u7136\u800c\uff0cVLM\u7684\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u80fd\u591f\u5f88\u597d\u5730\u5904\u7406\u5206\u5272\u56fe\u50cf\u8f93\u5165\uff0c\u800c\u5b89\u5168\u5bf9\u9f50\u5374\u6ca1\u6709\u8003\u8651\u6709\u5bb3\u8bed\u4e49\u5206\u5e03\u5728\u591a\u4e2a\u56fe\u50cf\u7247\u6bb5\u4e2d\u7684\u60c5\u51b5\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4e00\u4e2a\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5272\u56fe\u50cf\u89c6\u89c9\u8d8a\u72f1\u653b\u51fb\uff08SIVA\uff09\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u7b56\u7565\uff1a\u4ece\u7b80\u5355\u7684\u56fe\u50cf\u5206\u5272\u5f00\u59cb\uff0c\u53d1\u5c55\u5230\u81ea\u9002\u5e94\u767d\u76d2\u653b\u51fb\uff0c\u6700\u7ec8\u5f62\u6210\u9ed1\u76d2\u8f6c\u79fb\u653b\u51fb\u3002\u6700\u5f3a\u7684\u653b\u51fb\u7b56\u7565\u91c7\u7528\u65b0\u9896\u7684\u5bf9\u6297\u77e5\u8bc6\u84b8\u998f\uff08Adv-KD\uff09\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u6a21\u578b\u8f6c\u79fb\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u73b0\u4ee3VLM\u548c\u4e09\u4e2a\u8d8a\u72f1\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4f5c\u8005\u7684\u6700\u5f3a\u653b\u51fb\u6bd4\u73b0\u6709\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe60%\u7684\u8f6c\u79fb\u6210\u529f\u7387\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5206\u5272\u56fe\u50cf\u653b\u51fb\u7684\u6709\u6548\u6027\u548c\u4e25\u91cd\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dVLM\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u3002\u6700\u540e\uff0c\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e00\u6f0f\u6d1e\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u6539\u8fdbVLM\u5b89\u5168\u5bf9\u9f50\u4ee5\u5904\u7406\u5206\u5272\u56fe\u50cf\u8f93\u5165\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.08026", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08026", "abs": "https://arxiv.org/abs/2602.08026", "authors": ["Arya Akhavan", "David Janz", "Csaba Szepesv\u00e1ri"], "title": "Sharp analysis of linear ensemble sampling", "comment": null, "summary": "We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=\u0398(d\\log n)$, ES attains $\\tilde O(d^{3/2}\\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7ebf\u6027\u96c6\u6210\u91c7\u6837(ES)\u5728\u9ad8\u65af\u6270\u52a8\u4e0b\u7684\u968f\u673a\u7ebf\u6027\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5f53\u96c6\u6210\u89c4\u6a21\u4e3am=\u0398(d log n)\u65f6\uff0cES\u80fd\u8fbe\u5230$\\tilde O(d^{3/2}\\sqrt n)$\u7684\u9ad8\u6982\u7387\u9057\u61be\u754c\uff0c\u586b\u8865\u4e86\u4e0eThompson\u91c7\u6837\u57fa\u51c6\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u7814\u7a76\u7ebf\u6027\u96c6\u6210\u91c7\u6837\u5728\u968f\u673a\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\u7684\u6027\u80fd\uff0c\u65e8\u5728\u586b\u8865\u5176\u4e0eThompson\u91c7\u6837\u57fa\u51c6\u4e4b\u95f4\u7684\u7406\u8bba\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002\u63a2\u7d22\u968f\u673a\u5316\u63a2\u7d22\u7b56\u7565\u5728\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\u4e0b\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u96c6\u6210\u91c7\u6837(ES)\u7ed3\u5408\u6807\u51c6\u9ad8\u65af\u6270\u52a8\uff0c\u901a\u8fc7\u5c06\u79bb\u6563\u65f6\u95f4\u95ee\u9898\u8f6c\u5316\u4e3am\u4e2a\u72ec\u7acb\u5e03\u6717\u8fd0\u52a8\u7684\u65f6\u9f50\u8d85\u8d8a\u95ee\u9898\uff0c\u5229\u7528\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\u8fdb\u884c\u5206\u6790\u3002\u96c6\u6210\u89c4\u6a21\u8bbe\u7f6e\u4e3am=\u0398(d log n)\u3002", "result": "\u5f53\u96c6\u6210\u89c4\u6a21m=\u0398(d log n)\u65f6\uff0c\u7ebf\u6027\u96c6\u6210\u91c7\u6837\u80fd\u591f\u8fbe\u5230$\\tilde O(d^{3/2}\\sqrt n)$\u7684\u9ad8\u6982\u7387\u9057\u61be\u754c\uff0c\u8fd9\u4e00\u7ed3\u679c\u4e0eThompson\u91c7\u6837\u57fa\u51c6\u76f8\u5339\u914d\uff0c\u540c\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u4fdd\u6301\u53ef\u6bd4\u3002", "conclusion": "\u7ebf\u6027\u96c6\u6210\u91c7\u6837\u5728\u9002\u5f53\u7684\u96c6\u6210\u89c4\u6a21\u4e0b\u80fd\u591f\u8fbe\u5230\u4e0eThompson\u91c7\u6837\u76f8\u5f53\u7684\u7406\u8bba\u6027\u80fd\uff0c\u8fde\u7eed\u65f6\u95f4\u89c6\u89d2\u4e3a\u5206\u6790\u968f\u673a\u5316\u63a2\u7d22\u7b56\u7565\u63d0\u4f9b\u4e86\u81ea\u7136\u4e14\u53ef\u80fd\u5fc5\u8981\u7684\u6846\u67b6\uff0c\u79bb\u6563\u65f6\u95f4\u95ee\u9898\u4f3c\u4e4e\u9700\u8981\u8fde\u7eed\u65f6\u95f4\u89e3\u51b3\u65b9\u6848\u624d\u80fd\u83b7\u5f97\u5c16\u9510\u7684\u8fb9\u754c\u3002"}}
{"id": "2602.08032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08032", "abs": "https://arxiv.org/abs/2602.08032", "authors": ["Lior Cohen", "Ofir Nabati", "Kaixin Wang", "Navdeep Kumar", "Shie Mannor"], "title": "Horizon Imagination: Efficient On-Policy Training in Diffusion World Models", "comment": "This paper will be published in the ICLR 2026 proceedings", "summary": "We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.", "AI": {"tldr": "\u63d0\u51faHorizon Imagination\uff08HI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u53bb\u566a\u591a\u4e2a\u672a\u6765\u89c2\u6d4b\u6765\u63d0\u5347\u6269\u6563\u4e16\u754c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6548\u7387\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u4e16\u754c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u867d\u7136\u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u6548\u7387\u6311\u6218\uff1a\u8981\u4e48\u9700\u8981\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u91cd\u578b\u6a21\u578b\uff0c\u8981\u4e48\u4f9d\u8d56\u9ad8\u5ea6\u5e8f\u5217\u5316\u7684\u60f3\u8c61\u8fc7\u7a0b\uff0c\u4e24\u8005\u90fd\u5e26\u6765\u8fc7\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51faHorizon Imagination\uff08HI\uff09\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u79bb\u6563\u968f\u673a\u7b56\u7565\u7684\u5728\u7ebf\u7b56\u7565\u60f3\u8c61\u8fc7\u7a0b\uff0c\u80fd\u591f\u5e76\u884c\u53bb\u566a\u591a\u4e2a\u672a\u6765\u89c2\u6d4b\u3002HI\u5305\u542b\u7a33\u5b9a\u673a\u5236\u548c\u65b0\u7684\u91c7\u6837\u8c03\u5ea6\u7b56\u7565\uff0c\u5c06\u53bb\u566a\u9884\u7b97\u4e0e\u6709\u6548\u53bb\u566a\u8303\u56f4\u89e3\u8026\uff0c\u540c\u65f6\u652f\u6301\u5b50\u5e27\u9884\u7b97\u3002", "result": "\u5728Atari 100K\u548cCraftium\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f7f\u7528\u4e00\u534a\u53bb\u566a\u6b65\u6570\u7684\u5b50\u5e27\u9884\u7b97\u4e0b\u4ecd\u80fd\u4fdd\u6301\u63a7\u5236\u6027\u80fd\uff0c\u5e76\u5728\u4e0d\u540c\u8c03\u5ea6\u7b56\u7565\u4e0b\u5b9e\u73b0\u66f4\u4f18\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "Horizon Imagination\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u4e16\u754c\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u5e76\u884c\u53bb\u566a\u548c\u521b\u65b0\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2602.08198", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.08198", "abs": "https://arxiv.org/abs/2602.08198", "authors": ["Jingyu Hu", "Bin Hu", "Ka-Hei Hui", "Haipeng Li", "Zhengzhe Liu", "Daniel Cohen-Or", "Chi-Wing Fu"], "title": "PEGAsus: 3D Personalization of Geometry and Appearance", "comment": null, "summary": "We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.", "AI": {"tldr": "PEGAsus\u662f\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u4e2a\u6027\u53163D\u5f62\u72b6\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u548c\u5916\u89c2\u5c42\u9762\u7684\u6982\u5ff5\u5b66\u4e60\uff0c\u4ece\u53c2\u8003\u5f62\u72b6\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u7684\u5c5e\u6027\u5e76\u4e0e\u6587\u672c\u7ed3\u5408\u751f\u6210\u65b0\u5f62\u72b6\u3002", "motivation": "\u73b0\u67093D\u5f62\u72b6\u751f\u6210\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u4ece\u53c2\u8003\u5f62\u72b6\u4e2d\u63d0\u53d6\u53ef\u91cd\u7528\u7684\u51e0\u4f55\u548c\u5916\u89c2\u5c5e\u6027\uff0c\u5e76\u7075\u6d3b\u7ec4\u5408\u8fd9\u4e9b\u5c5e\u6027\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u4e2a\u6027\u5316\u5f62\u72b6\u3002", "method": "1) \u5c063D\u5f62\u72b6\u4e2a\u6027\u5316\u5b9a\u4e49\u4e3a\u63d0\u53d6\u7c7b\u522b\u65e0\u5173\u7684\u51e0\u4f55\u548c\u5916\u89c2\u5c5e\u6027\uff1b2) \u8bbe\u8ba1\u6e10\u8fdb\u4f18\u5316\u7b56\u7565\uff0c\u5728\u51e0\u4f55\u548c\u5916\u89c2\u5c42\u9762\u89e3\u8026\u5b66\u4e60\u5f62\u72b6\u6982\u5ff5\uff1b3) \u6269\u5c55\u5230\u533a\u57df\u7ea7\u6982\u5ff5\u5b66\u4e60\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u65e0\u4e0a\u4e0b\u6587\u635f\u5931\u3002", "result": "PEGAsus\u80fd\u591f\u4ece\u5e7f\u6cdb\u7684\u53c2\u8003\u5f62\u72b6\u4e2d\u6709\u6548\u63d0\u53d6\u5c5e\u6027\uff0c\u5e76\u4e0e\u6587\u672c\u7075\u6d3b\u7ec4\u5408\u751f\u6210\u65b0\u5f62\u72b6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u652f\u6301\u521b\u5efa\u591a\u6837\u5316\u4e2a\u6027\u5316\u7ed3\u679c\uff0c\u5728\u8de8\u7c7b\u522b\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "PEGAsus\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u548c\u5916\u89c2\u5c42\u9762\u7684\u6982\u5ff5\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u76843D\u5f62\u72b6\u4e2a\u6027\u5316\u751f\u6210\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u8de8\u7c7b\u522b\u5e94\u7528\uff0c\u4e3a\u4e2a\u6027\u53163D\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08033", "abs": "https://arxiv.org/abs/2602.08033", "authors": ["Julien Fageot", "Matthias Grossglauser", "L\u00ea-Nguy\u00ean Hoang", "Matteo Tacchi-B\u00e9nard", "Oscar Villemaud"], "title": "The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring", "comment": "1 table, 5 figures, 8 pages", "summary": "Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.", "AI": {"tldr": "SCoRa\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u4e2a\u4f53\u8bc4\u5206\u548c\u6210\u5bf9\u6bd4\u8f83\u4e24\u79cd\u504f\u597d\u83b7\u53d6\u65b9\u5f0f\uff0c\u5728\u9700\u8981\u51c6\u786e\u6392\u5e8f\u9876\u7ea7\u5b9e\u4f53\u7684\u573a\u666f\u4e2d\u4f18\u4e8e\u5355\u4e00\u65b9\u6cd5", "motivation": "\u957f\u671f\u4ee5\u6765\u5b58\u5728\u5173\u4e8e\u4eba\u7c7b\u5e94\u8be5\u5355\u72ec\u8bc4\u4f30\u5b9e\u4f53\u8fd8\u662f\u8fdb\u884c\u6bd4\u8f83\u8bc4\u4f30\u7684\u4e89\u8bba\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7ed3\u5408\u4e24\u79cd\u504f\u597d\u83b7\u53d6\u5f62\u5f0f\u662f\u5426\u80fd\u8d85\u8d8a\u5355\u4e00\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u51c6\u786e\u6392\u5e8f\u9876\u7ea7\u5b9e\u4f53\u7684\u5173\u952e\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86SCoRa\uff08\u4ece\u6bd4\u8f83\u548c\u8bc4\u5206\u4e2d\u8bc4\u5206\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u7387\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u4ece\u4e2a\u4f53\u8bc4\u5206\u548c\u6210\u5bf9\u6bd4\u8f83\u4e24\u79cd\u4fe1\u53f7\u4e2d\u5b66\u4e60\u3002\u8bc1\u660e\u4e86SCoRa\u7684MAP\u4f30\u8ba1\u5668\u5177\u6709\u826f\u597d\u7684\u6027\u8d28\uff0c\u6ee1\u8db3\u5355\u8c03\u6027\u548c\u9c81\u68d2\u6027\u4fdd\u8bc1\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6a21\u578b\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\uff0cSCoRa\u4e5f\u80fd\u6062\u590d\u51c6\u786e\u7684\u8bc4\u5206\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8bc6\u522b\u51fa\u4e86\u4e00\u4e2a\u73b0\u5b9e\u573a\u666f\uff1a\u5f53\u9700\u8981\u51c6\u786e\u6392\u5e8f\u9876\u7ea7\u5b9e\u4f53\u65f6\uff0c\u7ed3\u5408\u6bd4\u8f83\u548c\u8bc4\u5206\u7684\u65b9\u6cd5\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u4efb\u4f55\u4e00\u79cd\u65b9\u6cd5\u3002", "conclusion": "\u8003\u8651\u5230\u5b9e\u9645\u4e2d\u591a\u79cd\u5f62\u5f0f\u4fe1\u53f7\u7684\u53ef\u7528\u6027\uff0cSCoRa\u4e3a\u504f\u597d\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u57fa\u7840\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u4e2a\u4f53\u8bc4\u5206\u548c\u6210\u5bf9\u6bd4\u8f83\u4e24\u79cd\u504f\u597d\u83b7\u53d6\u65b9\u5f0f\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.08202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08202", "abs": "https://arxiv.org/abs/2602.08202", "authors": ["Jinrong Lv", "Xun Gong", "Zhaohuan Li", "Weili Jiang"], "title": "Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video", "comment": "11 pages, 5 tables, 10 figures. Under peer review", "summary": "Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMCSDR\u6a21\u578b\uff0c\u4f7f\u7528\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u6765\u4f30\u8ba1\u8d85\u58f0\u5fc3\u52a8\u56fe\u4e2d\u7684\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u5728\u75c5\u7406\u591a\u6a21\u6001\u5206\u5e03\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4ece\u8d85\u58f0\u5fc3\u52a8\u56fe\u4f30\u8ba1\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\u662f\u4e00\u4e2a\u75c5\u6001\u9006\u95ee\u9898\uff0c\u5b58\u5728\u566a\u58f0\u3001\u4f2a\u5f71\u548c\u6709\u9650\u89c6\u89d2\u7b49\u6311\u6218\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5747\u65b9\u8bef\u5dee\u56de\u5f52\uff0c\u5f3a\u5236\u6a21\u578b\u5b66\u4e60\u6761\u4ef6\u671f\u671b\uff0c\u4f46\u5f53\u540e\u9a8c\u5206\u5e03\u662f\u591a\u6a21\u6001\u6216\u91cd\u5c3e\u5206\u5e03\u65f6\uff08\u8fd9\u5728\u75c5\u7406\u573a\u666f\u4e2d\u5e38\u89c1\uff09\uff0c\u4f1a\u4ea7\u751f\u8bef\u5bfc\u6027\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6761\u4ef6\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u56de\u5f52\u6a21\u578b\uff08MCSDR\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u65e8\u5728\u5efa\u6a21\u4ee5\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u548c\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u5148\u9a8c\u4e3a\u6761\u4ef6\u7684LVEF\u8fde\u7eed\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5728EchoNet-Dynamic\u3001EchoNet-Pediatric\u548cCAMUS\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMCSDR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u5728\u9ad8\u566a\u58f0\u6216\u663e\u8457\u751f\u7406\u53d8\u5f02\u6027\u7684\u75c5\u4f8b\u4e2d\uff0c\u6a21\u578b\u7684\u751f\u6210\u8f68\u8ff9\u8868\u73b0\u51fa\u72ec\u7279\u884c\u4e3a\uff0c\u4e3aAI\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u4ece\u786e\u5b9a\u6027\u56de\u5f52\u5411\u751f\u6210\u56de\u5f52\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u63d0\u51fa\u7684MCSDR\u6846\u67b6\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u8d85\u58f0\u5fc3\u52a8\u56feLVEF\u4f30\u8ba1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u533b\u7597AI\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08206", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08206", "abs": "https://arxiv.org/abs/2602.08206", "authors": ["Chufeng Zhou", "Jian Wang", "Xinyuan Liu", "Xiaokang Zhang"], "title": "Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation", "comment": "5 pages, 3 figures", "summary": "Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based\" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.", "AI": {"tldr": "\u63d0\u51faGR-CoT\u6846\u67b6\uff0c\u901a\u8fc7\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u94fe\u589e\u5f3aMLLMs\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u89e3\u51b3\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6b67\u4e49\u95ee\u9898", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\u548c\u6587\u672c\u5d4c\u5165\u7684\u88ab\u52a8\u6620\u5c04\uff0c\u7f3a\u4e4f\u5730\u7406\u7a7a\u95f4\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5bfc\u81f4\u76f8\u4f3c\u5149\u8c31\u7279\u5f81\u4f46\u4e0d\u540c\u8bed\u4e49\u5c5e\u6027\u7684\u5730\u7269\u7c7b\u522b\u51fa\u73b0\u4e25\u91cd\u8bed\u4e49\u6b67\u4e49\u548c\u8bef\u5206\u7c7b", "method": "\u63d0\u51faGeospatial Reasoning Chain-of-Thought (GR-CoT)\u6846\u67b6\uff0c\u5305\u542b\u79bb\u7ebf\u77e5\u8bc6\u84b8\u998f\u6d41\u548c\u5728\u7ebf\u5b9e\u4f8b\u63a8\u7406\u6d41\u3002\u79bb\u7ebf\u6d41\u5efa\u7acb\u7ec6\u7c92\u5ea6\u7c7b\u522b\u89e3\u91ca\u6807\u51c6\uff1b\u5728\u7ebf\u6d41\u6267\u884c\u5b8f\u89c2\u573a\u666f\u951a\u5b9a\u3001\u89c6\u89c9\u7279\u5f81\u89e3\u8026\u548c\u77e5\u8bc6\u9a71\u52a8\u51b3\u7b56\u5408\u6210\u7684\u987a\u5e8f\u63a8\u7406\u8fc7\u7a0b\uff0c\u751f\u6210\u56fe\u50cf\u81ea\u9002\u5e94\u8bcd\u6c47\u8868", "result": "\u5728LoveDA\u548cGID5\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027", "conclusion": "GR-CoT\u6846\u67b6\u901a\u8fc7\u589e\u5f3aMLLMs\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6b67\u4e49\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6"}}
{"id": "2602.08211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08211", "abs": "https://arxiv.org/abs/2602.08211", "authors": ["Yik Lung Pang", "Changjae Oh"], "title": "Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension", "comment": "4 pages, 5 figures, 2 tables", "summary": "Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Caption\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u6765\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e865%\u523030%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u9700\u8981\u6839\u636e\u6587\u672c\u63cf\u8ff0\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u5bf9\u8c61\u3002\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u6269\u5927\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u6570\u636e\u5728REC\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u7b49\u6280\u672f\u63d0\u4f9b\u989d\u5916\u89c6\u89c9\u6216\u6587\u672c\u4e0a\u4e0b\u6587\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u4e0d\u540c\u4e0a\u4e0b\u6587\u63d0\u4f9b\u6280\u672f\u5bf9MLLM\u5728REC\u4efb\u52a1\u4e0a\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3aChain-of-Caption\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u4e3aMLLM\u63d0\u4f9b\u989d\u5916\u7684\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u3002\u5206\u6790\u4e0d\u540c\u4e0a\u4e0b\u6587\u63d0\u4f9b\u6280\u672f\u7684\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1\u65b9\u6cd5\u7ed3\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\u6765\u63d0\u5347REC\u6027\u80fd\u3002", "result": "\u5728RefCOCO\u3001RefCOCOg\u3001RefCOCO+\u548cRef-L4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u72ec\u7684\u6587\u672c\u6216\u89c6\u89c9\u4e0a\u4e0b\u6587\u90fd\u80fd\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u63d0\u5347REC\u6027\u80fd\u3002\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\uff0c\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u5728\u4e0d\u540cIoU\u9608\u503c\u4e0b\u7684\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e865%\u523030%\u3002", "conclusion": "\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u989d\u5916\u7684\u89c6\u89c9\u548c\u6587\u672c\u4e0a\u4e0b\u6587\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u63d0\u51fa\u7684Chain-of-Caption\u6846\u67b6\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4e3aREC\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08041", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08041", "abs": "https://arxiv.org/abs/2602.08041", "authors": ["Boyang Xia", "Weiyou Tian", "Qingnan Ren", "Jiaqi Huang", "Jie Xiao", "Shuo Lu", "Kai Wang", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments", "comment": null, "summary": "Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faISO\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6218\u7565\u80cc\u666f\u6765\u4f18\u5316LLM\u667a\u80fd\u4f53\u5728\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u77ed\u671f\u4f18\u5316\u65b9\u6cd5\u5728\u52a8\u6001\u6218\u7565\u73af\u5883\u4e2d\u5931\u6548\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\uff0c\u5956\u52b1\u53d7\u5230\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u6f5c\u5728\u6218\u7565\u5916\u90e8\u6027\u5f71\u54cd\uff0c\u4f20\u7edf\u7684\u77ed\u671f\u4f18\u5316\u548c\u57fa\u4e8e\u53d8\u5f02\u7684\u9057\u61be\u5206\u6790\u5728\u53ef\u9884\u6d4b\u7684\u52a8\u6001\u73af\u5883\u4e2d\u53ef\u80fd\u53d8\u5f97\u65e0\u6548\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u5904\u7406\u8fd9\u79cd\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u9690\u5f0f\u6218\u7565\u4f18\u5316(ISO)\u6846\u67b6\uff0c\u5305\u542b\u6218\u7565\u5956\u52b1\u6a21\u578b(SRM)\u6765\u4f30\u8ba1\u884c\u52a8\u7684\u957f\u671f\u6218\u7565\u4ef7\u503c\uff0c\u4ee5\u53caiso-grpo\uff08\u57fa\u4e8e\u60c5\u5883\u7684\u4e50\u89c2\u5b66\u4e60\u89c4\u5219\uff09\uff0c\u8ba9\u667a\u80fd\u4f53\u9884\u6d4b\u5f53\u524d\u6218\u7565\u80cc\u666f\u5e76\u5728\u7ebf\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u663e\u793a\u6b21\u7ebf\u6027\u60c5\u5883\u9057\u61be\u548c\u5747\u8861\u6536\u655b\u4fdd\u8bc1\uff0c\u4e3b\u8981\u9879\u4e0e\u60c5\u5883\u9884\u6d4b\u9519\u8bef\u6570\u91cf\u76f8\u5173\uff1b\u5728\u9884\u6d4b\u8bef\u5dee\u6709\u754c\u65f6\uff0c\u6062\u590d\u9759\u6001\u6e38\u620f\u7684\u6536\u655b\u7387\u3002\u57286\u4eba\u65e0\u9650\u6ce8\u5fb7\u5dde\u6251\u514b\u548c\u7ade\u4e89\u6027\u5b9d\u53ef\u68a6\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u5f3aLLM\u548cRL\u57fa\u7ebf\uff0c\u957f\u671f\u56de\u62a5\u6301\u7eed\u6539\u8fdb\uff0c\u4e14\u5728\u53d7\u63a7\u9884\u6d4b\u566a\u58f0\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "ISO\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u6218\u7565\u80cc\u666f\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u5bf9\u6297\u6e38\u620f\u4e2d\u7684\u6218\u7565\u4f18\u5316\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u5728\u52a8\u6001\u6218\u7565\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08050", "abs": "https://arxiv.org/abs/2602.08050", "authors": ["Qusai Khaled", "Uzay Kaymak", "Laura Genga"], "title": "Interpretable Fuzzy Systems For Forward Osmosis Desalination", "comment": "7 pages, 4 figures, FUZZ-IEEE 2025", "summary": "Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u540c\u65b9\u6cd5\u5f00\u53d1\u53ef\u89e3\u91ca\u6a21\u7cca\u89c4\u5219\u7cfb\u7edf\uff0c\u7528\u4e8e\u9884\u6d4b\u6b63\u6e17\u900f\u6d77\u6c34\u6de1\u5316\u751f\u4ea7\u529b\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u8fbe\u5230\u4e0e\u805a\u7c7b\u65b9\u6cd5\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd", "motivation": "\u5728\u6c34\u5904\u7406\u5e94\u7528\u4e2d\uff0c\u6a21\u7cca\u89c4\u5219\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u51b3\u7b56\u5f71\u54cd\u516c\u5171\u5065\u5eb7\u3002\u867d\u7136\u7ed3\u6784\u53ef\u89e3\u91ca\u6027\u5df2\u901a\u8fc7\u591a\u76ee\u6807\u7b97\u6cd5\u89e3\u51b3\uff0c\u4f46\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u5e38\u56e0\u6a21\u7cca\u96c6\u533a\u5206\u5ea6\u4f4e\u800c\u53d7\u635f", "method": "\u96c6\u6210\u4e13\u5bb6\u9a71\u52a8\u7684\u7f51\u683c\u5212\u5206\u751f\u6210\u53ef\u533a\u5206\u7684\u96b6\u5c5e\u51fd\u6570\uff0c\u9886\u57df\u77e5\u8bc6\u5f15\u5bfc\u7684\u7279\u5f81\u5de5\u7a0b\u51cf\u5c11\u5197\u4f59\uff0c\u57fa\u4e8e\u89e6\u53d1\u5f3a\u5ea6\u7684\u89c4\u5219\u526a\u679d", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u548c\u6ee1\u8db3\u7ed3\u6784\u590d\u6742\u5ea6\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u57fa\u4e8e\u805a\u7c7b\u7684\u6a21\u7cca\u89c4\u5219\u7cfb\u7edf\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd", "conclusion": "\u4e3a\u6c34\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u540c\u65b9\u6cd5\u5e73\u8861\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u8bed\u4e49\u53ef\u89e3\u91ca\u6027"}}
{"id": "2602.08236", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08236", "abs": "https://arxiv.org/abs/2602.08236", "authors": ["Shoubin Yu", "Yue Zhang", "Zun Wang", "Jaehong Yoon", "Huaxiu Yao", "Mingyu Ding", "Mohit Bansal"], "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning", "comment": "the first two authors are equally contributed. Project page: https://adaptive-visual-tts.github.io/", "summary": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u4e2d\u6d4b\u8bd5\u65f6\u89c6\u89c9\u60f3\u8c61\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u6846\u67b6AVIC\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u8c03\u7528\u4e16\u754c\u6a21\u578b\u6765\u4f18\u5316\u60f3\u8c61\u7684\u4f7f\u7528\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4ece\u4e0d\u540c\u89c6\u89d2\u7406\u89e3\u573a\u666f\u65f6\u3002\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u60f3\u8c61\u6765\u589e\u5f3a\u63a8\u7406\uff0c\u4f46\u4f55\u65f6\u9700\u8981\u60f3\u8c61\u3001\u9700\u8981\u591a\u5c11\u60f3\u8c61\u3001\u4ee5\u53ca\u4f55\u65f6\u60f3\u8c61\u53cd\u800c\u6709\u5bb3\u7b49\u95ee\u9898\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u4e0d\u52a0\u533a\u5206\u7684\u60f3\u8c61\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u751a\u81f3\u53ef\u80fd\u56e0\u5f15\u5165\u8bef\u5bfc\u6027\u8bc1\u636e\u800c\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86AVIC\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u660e\u786e\u63a8\u7406\u5f53\u524d\u89c6\u89c9\u8bc1\u636e\u7684\u5145\u5206\u6027\uff0c\u7136\u540e\u9009\u62e9\u6027\u5730\u8c03\u7528\u548c\u6269\u5c55\u89c6\u89c9\u60f3\u8c61\u3002\u6846\u67b6\u9996\u5148\u8bc4\u4f30\u9759\u6001\u89c6\u89c9\u8bc1\u636e\u662f\u5426\u8db3\u591f\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u624d\u6fc0\u6d3b\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u60f3\u8c61\uff0c\u5e76\u63a7\u5236\u60f3\u8c61\u7684\u89c4\u6a21\u3002", "result": "\u5728\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08SAT\u3001MMSI\uff09\u548c\u5177\u8eab\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\uff08R2R\uff09\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff1a1\uff09\u660e\u786e\u4e86\u60f3\u8c61\u5728\u54ea\u4e9b\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\u3001\u8fb9\u9645\u6709\u6548\u6216\u6709\u5bb3\uff1b2\uff09\u9009\u62e9\u6027\u63a7\u5236\u7b56\u7565\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u56fa\u5b9a\u60f3\u8c61\u7b56\u7565\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e16\u754c\u6a21\u578b\u8c03\u7528\u6b21\u6570\u548c\u8bed\u8a00\u6807\u8bb0\u6570\u91cf\uff1b3\uff09\u63ed\u793a\u4e86\u89c6\u89c9\u60f3\u8c61\u4f5c\u4e3a\u53ef\u63a7\u8d44\u6e90\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u4f18\u5316\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5206\u6790\u548c\u63a7\u5236\u6d4b\u8bd5\u65f6\u60f3\u8c61\u5bf9\u4e8e\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7a7a\u95f4\u63a8\u7406\u7684\u91cd\u8981\u6027\u3002\u81ea\u9002\u5e94\u9009\u62e9\u6027\u60f3\u8c61\u7b56\u7565\u80fd\u591f\u5728\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08054", "abs": "https://arxiv.org/abs/2602.08054", "authors": ["Manan Tayal", "Mumuksh Tayal"], "title": "Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning", "comment": "23 pages, 8 figures", "summary": "Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.", "AI": {"tldr": "EpiFlow\u662f\u4e00\u79cd\u5b89\u5168\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\u8054\u5408\u4f18\u5316\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0c\u4f7f\u7528epigraph\u503c\u51fd\u6570\u6307\u5bfc\u7b56\u7565\u5408\u6210\uff0c\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u56de\u62a5\u4e14\u51e0\u4e4e\u96f6\u5b89\u5168\u8fdd\u89c4\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5f3a\u5b89\u5168\u6027\u548c\u9ad8\u6027\u80fd\u3002\u73b0\u6709\u5b89\u5168\u79bb\u7ebfRL\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8f6f\u7ea6\u675f\u5141\u8bb8\u8fdd\u89c4\uff0c\u8981\u4e48\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u8981\u4e48\u96be\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u3001\u5956\u52b1\u4f18\u5316\u548c\u6570\u636e\u5206\u5e03\u9075\u5faa\u3002", "method": "\u63d0\u51faEpigraph-Guided Flow Matching (EpiFlow)\u6846\u67b6\uff0c\u5c06\u5b89\u5168\u79bb\u7ebfRL\u8868\u8ff0\u4e3a\u72b6\u6001\u7ea6\u675f\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002\u5b66\u4e60\u4ece\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684epigraph\u91cd\u6784\u63a8\u5bfc\u51fa\u7684\u53ef\u884c\u6027\u503c\u51fd\u6570\uff0c\u907f\u514d\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u89e3\u8026\u76ee\u6807\u6216\u540e\u5904\u7406\u8fc7\u6ee4\u3002\u7b56\u7565\u901a\u8fc7\u57fa\u4e8eepigraph\u503c\u51fd\u6570\u91cd\u65b0\u52a0\u6743\u884c\u4e3a\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6d41\u5339\u914d\u62df\u5408\u751f\u6210\u7b56\u7565\u6765\u5408\u6210\u3002", "result": "\u5728\u5305\u62ecSafety-Gymnasium\u57fa\u51c6\u6d4b\u8bd5\u5728\u5185\u7684\u5404\u79cd\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\uff0cEpiFlow\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u56de\u62a5\u4e14\u51e0\u4e4e\u96f6\u7ecf\u9a8c\u5b89\u5168\u8fdd\u89c4\uff0c\u8bc1\u660e\u4e86epigraph\u5f15\u5bfc\u7b56\u7565\u5408\u6210\u7684\u6709\u6548\u6027\u3002", "conclusion": "EpiFlow\u901a\u8fc7epigraph\u5f15\u5bfc\u7684\u6d41\u5339\u914d\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5b89\u5168\u79bb\u7ebfRL\u4e2d\u5e73\u8861\u5b89\u5168\u6027\u3001\u6027\u80fd\u548c\u5206\u5e03\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u81ea\u4e3b\u7cfb\u7edf\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08262", "abs": "https://arxiv.org/abs/2602.08262", "authors": ["Guoqi Yu", "Xiaowei Hu", "Angelica I. Aviles-Rivero", "Anqi Qiu", "Shujun Wang"], "title": "Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification", "comment": "This paper has been accepted by IEEE Transactions on Medical Imaging", "summary": "Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDeCI\u6846\u67b6\uff0c\u901a\u8fc7\u5468\u671f-\u6f02\u79fb\u5206\u89e3\u548c\u901a\u9053\u72ec\u7acb\u6027\u5efa\u6a21\u539f\u59cbBOLD\u4fe1\u53f7\uff0c\u5728fMRI\u8111\u75be\u75c5\u5206\u7c7b\u4e2d\u8d85\u8d8a\u4f20\u7edf\u529f\u80fd\u8fde\u63a5\u65b9\u6cd5", "motivation": "\u73b0\u6709fMRI\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u76ae\u5c14\u900a\u76f8\u5173\u7684\u529f\u80fd\u8fde\u63a5\uff0c\u5c064D BOLD\u4fe1\u53f7\u7b80\u5316\u4e3a\u9759\u60012D\u77e9\u9635\uff0c\u4e22\u5931\u4e86\u65f6\u95f4\u52a8\u6001\u4fe1\u606f\u4e14\u53ea\u80fd\u6355\u6349\u7ebf\u6027\u533a\u57df\u95f4\u5173\u7cfb\u3002\u9700\u8981\u66f4\u76f4\u63a5\u5730\u5efa\u6a21\u65f6\u95f4\u4fe1\u606f\u6765\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u8111\u52a8\u529b\u5b66\u3002", "method": "\u63d0\u51faDeCI\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u539f\u5219\uff1a(1) \u5468\u671f\u4e0e\u6f02\u79fb\u5206\u89e3\uff1a\u5728\u6bcf\u4e2a\u611f\u5174\u8da3\u533a\u57df\u4e2d\u5206\u79bb\u5468\u671f\u6027\u7684\u632f\u8361\u6ce2\u52a8\u548c\u6f02\u79fb\u6027\u7684\u6162\u57fa\u7ebf\u8d8b\u52bf\uff1b(2) \u901a\u9053\u72ec\u7acb\u6027\uff1a\u5206\u522b\u5efa\u6a21\u6bcf\u4e2aROI\uff0c\u63d0\u9ad8\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDeCI\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfFC\u65b9\u6cd5\u548c\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u57fa\u51c6\uff0c\u9a8c\u8bc1\u4e86\u76f4\u63a5\u5efa\u6a21\u65f6\u95f4\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5728fMRI\u5206\u6790\u4e2d\u8f6c\u5411\u7aef\u5230\u7aef\u7684\u65f6\u95f4\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u590d\u6742\u7684\u8111\u52a8\u529b\u5b66\u3002DeCI\u6846\u67b6\u7b80\u5355\u6709\u6548\uff0c\u4e3afMRI\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08060", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08060", "abs": "https://arxiv.org/abs/2602.08060", "authors": ["Alejandro Ruiz y Mesa", "Guilherme Korol", "Moritz Riesteter", "Jo\u00e3o Paulo Cardoso de Lima", "Jeronimo Castrillon"], "title": "Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices", "comment": "Accepted to AccML@HiPEAC 2026", "summary": "LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\\times$ speedup for translation tasks, closely matching analytic expectations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6790\u6210\u672c\u6a21\u578b\u7684\u7c97\u7c92\u5ea6\u5206\u533a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f18\u5316\u63a8\u6d4b\u89e3\u7801\uff08Speculative Decoding\uff09\u6027\u80fd\uff0c\u89e3\u51b3\u7f16\u8bd1\u5668\u96c6\u6210\u548c\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7684\u6311\u6218\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72LLM\u9762\u4e34\u4e25\u91cd\u7684\u5ef6\u8fdf\u7ea6\u675f\uff0c\u7279\u522b\u662f\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u3002\u63a8\u6d4b\u89e3\u7801\u867d\u7136\u662f\u6709\u524d\u666f\u7684\u6280\u672f\uff0c\u4f46\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u5728\u4e0d\u727a\u7272\u6027\u80fd\u6216\u53ef\u7f16\u7a0b\u6027\u7684\u60c5\u51b5\u4e0b\u5c06SD\u96c6\u6210\u5230\u57fa\u4e8e\u7f16\u8bd1\u5668\u7684\u5de5\u4f5c\u6d41\u4e2d\uff1b2\uff09\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5206\u533a\u7b56\u7565\u5229\u7528\u73b0\u4ee3SoC\u7684\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u4f7f\u7528\u5206\u6790\u6210\u672c\u6a21\u578b\u63a2\u7d22\u5f02\u6784\u786c\u4ef6\u914d\u7f6e\uff0c\u6307\u5bfcLLM\u5b50\u56fe\u7684\u7c97\u7c92\u5ea6\u5206\u533a\uff0c\u7279\u522b\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u5178\u578b\u7684\u77ed\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u3002\u8be5\u6a21\u578b\u9884\u6d4b\u63a8\u6d4b\u91c7\u6837\u548c\u5f02\u6784\u6267\u884c\u4f55\u65f6\u5171\u540c\u6709\u76ca\u3002", "result": "\u5728\u914d\u5907\u516d\u6838Cortex-A CPU\u548cMali GPU\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ffb\u8bd1\u4efb\u52a1\u5b9e\u73b0\u4e86\u6700\u9ad81.68\u500d\u7684\u52a0\u901f\uff0c\u4e0e\u5206\u6790\u9884\u671f\u5bc6\u5207\u5339\u914d\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u6790\u6210\u672c\u6a21\u578b\u80fd\u591f\u6709\u6548\u6307\u5bfc\u8fb9\u7f18\u8bbe\u5907\u4e0a\u63a8\u6d4b\u89e3\u7801\u7684\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u5206\u533a\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u77ed\u5e8f\u5217\u573a\u666f\u4e2d\u3002"}}
{"id": "2602.08277", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08277", "abs": "https://arxiv.org/abs/2602.08277", "authors": ["Xiangbo Gao", "Renjie Li", "Xinghao Chen", "Yuheng Wu", "Suofei Feng", "Qing Yin", "Zhengzhong Tu"], "title": "PISCO: Precise Video Instance Insertion with Sparse Control", "comment": null, "summary": "The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.", "AI": {"tldr": "PISCO\u662f\u4e00\u4e2a\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u73b0\u6709\u89c6\u9891\u4e2d\u7cbe\u786e\u63d2\u5165\u7279\u5b9a\u5b9e\u4f8b\uff0c\u652f\u6301\u4efb\u610f\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u9891\u7f16\u8f91\u96be\u4ee5\u4fdd\u6301\u573a\u666f\u5b8c\u6574\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "motivation": "AI\u89c6\u9891\u751f\u6210\u6b63\u4ece\u4f9d\u8d56\u5927\u91cf\u63d0\u793a\u5de5\u7a0b\u548c\"\u7b5b\u9009\"\u7684\u901a\u7528\u751f\u6210\uff0c\u8f6c\u5411\u7ec6\u7c92\u5ea6\u53ef\u63a7\u751f\u6210\u548c\u9ad8\u4fdd\u771f\u540e\u5904\u7406\u3002\u4e13\u4e1aAI\u8f85\u52a9\u7535\u5f71\u5236\u4f5c\u9700\u8981\u7cbe\u786e\u7684\u76ee\u6807\u4fee\u6539\uff0c\u89c6\u9891\u5b9e\u4f8b\u63d2\u5165\u662f\u8fd9\u4e00\u8f6c\u53d8\u7684\u5173\u952e\uff0c\u8981\u6c42\u4fdd\u6301\u573a\u666f\u5b8c\u6574\u6027\u3001\u7269\u7406\u4e00\u81f4\u6027\u548c\u539f\u59cb\u52a8\u6001\u3002", "method": "\u63d0\u51faPISCO\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u652f\u6301\u5355\u5173\u952e\u5e27\u3001\u8d77\u6b62\u5173\u952e\u5e27\u6216\u4efb\u610f\u65f6\u95f4\u6233\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\u3002\u5f15\u5165\u53d8\u91cf\u4fe1\u606f\u5f15\u5bfc\u5904\u7406\u7a00\u758f\u6761\u4ef6\u5f15\u8d77\u7684\u5206\u5e03\u504f\u79fb\uff0c\u5206\u5e03\u4fdd\u6301\u65f6\u95f4\u63a9\u7801\u7a33\u5b9a\u65f6\u95f4\u751f\u6210\uff0c\u4ee5\u53ca\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u5b9e\u73b0\u771f\u5b9e\u573a\u666f\u9002\u5e94\u3002", "result": "\u6784\u5efa\u4e86PISCO-Bench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u9a8c\u8bc1\u8fc7\u7684\u5b9e\u4f8b\u6807\u6ce8\u548c\u914d\u5bf9\u5e72\u51c0\u80cc\u666f\u89c6\u9891\u3002\u5b9e\u9a8c\u8868\u660ePISCO\u5728\u7a00\u758f\u63a7\u5236\u4e0b\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u968f\u7740\u63a7\u5236\u4fe1\u53f7\u589e\u52a0\u5448\u73b0\u6e05\u6670\u5355\u8c03\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PISCO\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u89c6\u9891\u5b9e\u4f8b\u63d2\u5165\uff0c\u652f\u6301\u7075\u6d3b\u7684\u7a00\u758f\u5173\u952e\u5e27\u63a7\u5236\uff0c\u4e3a\u4e13\u4e1aAI\u8f85\u52a9\u7535\u5f71\u5236\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86AI\u89c6\u9891\u751f\u6210\u5411\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u65b9\u5411\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.08062", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08062", "abs": "https://arxiv.org/abs/2602.08062", "authors": ["Shayan Ali Hassan", "Tao Ni", "Zafar Ayyub Qazi", "Marco Canini"], "title": "Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.\n  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.", "AI": {"tldr": "BAGEL\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6a21\u5757\u5316\u7684\u6076\u610f\u63d0\u793a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u4e13\u95e8\u5316\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u652f\u6301\u589e\u91cf\u66f4\u65b0\u4ee5\u9002\u5e94\u65b0\u653b\u51fb\u3002", "motivation": "\u5f53\u524dLLM\u9632\u5fa1\u7cfb\u7edf\u9762\u4e34\u6839\u672c\u6027\u9650\u5236\uff1a\u9ed1\u7bb1\u5ba1\u6838API\u900f\u660e\u5ea6\u6709\u9650\u4e14\u9002\u5e94\u6027\u5dee\uff0c\u767d\u7bb1\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u9700\u8981\u6602\u8d35\u91cd\u8bad\u7ec3\u3002\u73b0\u6709\u7cfb\u7edf\u9700\u8981\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u4e4b\u95f4\u505a\u51fa\u59a5\u534f\u3002", "method": "BAGEL\u91c7\u7528\u57fa\u4e8e\u5f15\u5bfc\u805a\u5408\u548c\u4e13\u5bb6\u6df7\u5408\u601d\u60f3\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5305\u542b\u591a\u4e2a\u5728\u4e0d\u540c\u653b\u51fb\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684\u4e13\u95e8\u5316\u6a21\u578b\u3002\u63a8\u7406\u65f6\u4f7f\u7528\u968f\u673a\u68ee\u6797\u8def\u7531\u5668\u9009\u62e9\u6700\u5408\u9002\u7684\u96c6\u6210\u6210\u5458\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u62bd\u6837\u805a\u5408\u591a\u4e2a\u6210\u5458\u7684\u9884\u6d4b\u3002\u652f\u6301\u589e\u91cf\u66f4\u65b0\uff0c\u53ea\u9700\u5fae\u8c03\u5c0f\u578b\u63d0\u793a\u5b89\u5168\u5206\u7c7b\u5668\u5e76\u52a0\u5165\u96c6\u6210\u3002", "result": "BAGEL\u4ec5\u9009\u62e95\u4e2a\u96c6\u6210\u6210\u5458\uff08430M\u53c2\u6570\uff09\u5c31\u5b9e\u73b0\u4e860.92\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u9700\u8981\u6570\u5341\u4ebf\u53c2\u6570\u7684OpenAI Moderation API\u548cShieldGemma\u3002\u7ecf\u8fc79\u6b21\u589e\u91cf\u66f4\u65b0\u540e\u6027\u80fd\u4fdd\u6301\u7a33\u5065\uff0c\u5e76\u901a\u8fc7\u8def\u7531\u5668\u7ed3\u6784\u7279\u5f81\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c0f\u578b\u5fae\u8c03\u5206\u7c7b\u5668\u7684\u96c6\u6210\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u6570\u5341\u4ebf\u53c2\u6570\u7684\u9632\u62a4\u7cfb\u7edf\uff0c\u540c\u65f6\u63d0\u4f9b\u751f\u4ea7\u7cfb\u7edf\u6240\u9700\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u4e3a\u89e3\u51b3LLM\u6076\u610f\u63d0\u793a\u68c0\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08063", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08063", "abs": "https://arxiv.org/abs/2602.08063", "authors": ["Eduardo Figueiredo", "Steven Adams", "Luca Laurenti"], "title": "Efficient Distribution Learning with Error Bounds in Wasserstein Distance", "comment": null, "summary": "The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\\widehat{\\mathbb{P}}$ while bounding the Wasserstein distance between $\\mathbb{P}$ and $\\widehat{\\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\\mathbb{P}$ is unknown, the Wasserstein distance between $\\mathbb{P}$ and $\\widehat{\\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\\widehat{\\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\\widehat{\\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u548c\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u65b0\u6846\u67b6\uff0c\u4ece\u6709\u9650\u6837\u672c\u4e2d\u8fd1\u4f3c\u672a\u77e5\u6982\u7387\u5206\u5e03\uff0c\u5e76\u63d0\u4f9bWasserstein\u8ddd\u79bb\u7684\u975e\u6e10\u8fd1\u6613\u8ba1\u7b97\u8bef\u5dee\u754c\u3002", "motivation": "Wasserstein\u8ddd\u79bb\u5df2\u6210\u4e3a\u91cf\u5316\u6982\u7387\u5206\u5e03\u95f4\u8ddd\u79bb\u7684\u5173\u952e\u5ea6\u91cf\uff0c\u5728\u673a\u5668\u5b66\u4e60\u3001\u63a7\u5236\u7406\u8bba\u3001\u51b3\u7b56\u7406\u8bba\u548c\u751f\u7269\u7cfb\u7edf\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u4ece\u6709\u9650\u6837\u672c\u4e2d\u5b66\u4e60\u672a\u77e5\u5206\u5e03\u5e76\u7ed9\u51fa\u975e\u6e10\u8fd1\u4e14\u6613\u4e8e\u8ba1\u7b97\u7684Wasserstein\u8ddd\u79bb\u8bef\u5dee\u754c\uff0c\u5df2\u6210\u4e3a\u8bb8\u591a\u9886\u57df\u7684\u57fa\u672c\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u3001\u975e\u7ebf\u6027\u4f18\u5316\u548c\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u7684\u7b97\u6cd5\u7406\u8bba\u6846\u67b6\u3002\u901a\u8fc7\u6c42\u89e3\u4e00\u4e2a\u89c4\u6a21\u4ec5\u4f9d\u8d56\u4e8e\u8fd1\u4f3c\u5206\u5e03\u652f\u6491\u96c6\u5927\u5c0f\u7684\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5373\u4f7f\u771f\u5b9e\u5206\u5e03\u672a\u77e5\uff0c\u4e5f\u80fd\u9ad8\u6548\u5730\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u754c\u5b9aWasserstein\u8ddd\u79bb\u8bef\u5dee\u3002\u5229\u7528\u667a\u80fd\u805a\u7c7b\u7b97\u6cd5\u4f18\u5316\u5bfb\u627e\u8fd1\u4f3c\u5206\u5e03\u7684\u652f\u6491\u96c6\uff0c\u540c\u65f6\u6700\u5c0f\u5316Wasserstein\u8ddd\u79bb\u8bef\u5dee\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u53ef\u6bd4\u65b9\u6cd5\uff0c\u901a\u5e38\u8fd4\u56de\u5177\u6709\u663e\u8457\u66f4\u5c0f\u652f\u6491\u96c6\u548c\u66f4\u7d27\u8bef\u5dee\u754c\u7684\u8fd1\u4f3c\u5206\u5e03\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4ece\u6709\u9650\u6837\u672c\u4e2d\u8fd1\u4f3c\u672a\u77e5\u6982\u7387\u5206\u5e03\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u8ba1\u7b97Wasserstein\u8ddd\u79bb\u7684\u975e\u6e10\u8fd1\u8bef\u5dee\u754c\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u652f\u6491\u96c6\u9009\u62e9\u5b9e\u73b0\u66f4\u7d27\u51d1\u7684\u8fd1\u4f3c\u8868\u793a\u3002"}}
{"id": "2602.08064", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08064", "abs": "https://arxiv.org/abs/2602.08064", "authors": ["Tianyu Li", "Dongchen Han", "Zixuan Cao", "Haofeng Huang", "Mengyu Zhou", "Ming Chen", "Erchao Zhao", "Xiaoxi Jiang", "Guanjun Jiang", "Gao Huang"], "title": "SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm", "comment": null, "summary": "Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.", "AI": {"tldr": "SiameseNorm\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6d41\u67b6\u6784\uff0c\u5c06Pre-Norm\u548cPost-Norm\u7684\u4f18\u52bf\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5355\u6d41\u8bbe\u8ba1\u4e2d\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3Transformer\u4e3b\u8981\u91c7\u7528Pre-Norm\u8303\u5f0f\u4ee5\u83b7\u5f97\u4f18\u5316\u7a33\u5b9a\u6027\uff0c\u4f46\u653e\u5f03\u4e86Post-Norm\u67b6\u6784\u7684\u4f18\u8d8a\u6f5c\u529b\u3002\u5148\u524d\u5c1d\u8bd5\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u901a\u5e38\u5bfc\u81f4\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u7684\u6743\u8861\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u5355\u6d41\u8bbe\u8ba1\u4e2d\u7684\u7ed3\u6784\u4e0d\u517c\u5bb9\u6027\u9020\u6210\u7684\u3002", "method": "\u63d0\u51faSiameseNorm\u53cc\u6d41\u67b6\u6784\uff0c\u8026\u5408Pre-Norm-like\u548cPost-Norm-like\u6d41\u5e76\u5171\u4eab\u53c2\u6570\u3002\u8fd9\u79cd\u8bbe\u8ba1\u89e3\u8026\u4e86\u4e24\u4e2a\u6d41\u7684\u4f18\u5316\u52a8\u6001\uff0c\u4f7f\u6240\u6709\u6b8b\u5dee\u5757\u90fd\u80fd\u63a5\u6536\u6765\u81ea\u4e24\u79cd\u8303\u5f0f\u7684\u7ec4\u5408\u68af\u5ea6\uff0c\u5176\u4e2d\u4e00\u4e2a\u6d41\u786e\u4fdd\u7a33\u5b9a\u6027\uff0c\u53e6\u4e00\u4e2a\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u572813\u4ebf\u53c2\u6570\u6a21\u578b\u4e0a\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b9e\u9a8c\u8868\u660e\uff0cSiameseNorm\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u4f18\u5316\u9c81\u68d2\u6027\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SiameseNorm\u901a\u8fc7\u53cc\u6d41\u67b6\u6784\u6210\u529f\u8c03\u548c\u4e86Pre-Norm\u548cPost-Norm\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u4f18\u5316\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u4e3aTransformer\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08337", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08337", "abs": "https://arxiv.org/abs/2602.08337", "authors": ["Sheng Yan", "Yong Wang", "Xin Du", "Junsong Yuan", "Mengyuan Liu"], "title": "Language-Guided Transformer Tokenizer for Human Motion Generation", "comment": null, "summary": "In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u8a00\u5f15\u5bfc\u7684\u8fd0\u52a8\u79bb\u6563\u5316\u65b9\u6cd5LG-Tok\uff0c\u901a\u8fc7Transformer\u67b6\u6784\u5b9e\u73b0\u8bed\u8a00\u4e0e\u8fd0\u52a8\u7684\u5bf9\u9f50\uff0c\u5728\u51cf\u5c11token\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u52a8\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u79bb\u6563\u5316\u65b9\u6cd5\u589e\u52a0token\u6570\u91cf\u4ee5\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\uff0c\u4f46\u8fd9\u4f1a\u589e\u52a0\u751f\u6210\u6a21\u578b\u7684\u5b66\u4e60\u96be\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u53c8\u80fd\u964d\u4f4e\u751f\u6210\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u5f15\u5bfc\u7684\u79bb\u6563\u5316\u65b9\u6cd5LG-Tok\uff0c\u4f7f\u7528Transformer\u67b6\u6784\u5b9e\u73b0\u8bed\u8a00\u4e0e\u8fd0\u52a8\u7684\u5bf9\u9f50\uff0c\u8bbe\u8ba1\u8bed\u8a00\u4e22\u5f03\u65b9\u6848\u4f7f\u89e3\u79bb\u6563\u5668\u652f\u6301\u65e0\u8bed\u8a00\u6307\u5bfc\u7684\u751f\u6210\u3002", "result": "\u5728HumanML3D\u548cMotion-X\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLG-Tok\u83b7\u5f97Top-1\u5206\u65700.542\u548c0.582\uff0c\u4f18\u4e8eMARDM\u76840.500\u548c0.528\uff1bFID\u5206\u6570\u5206\u522b\u4e3a0.057\u548c0.088\uff0c\u4f18\u4e8e0.114\u548c0.147\u3002LG-Tok-mini\u4ec5\u7528\u4e00\u534atoken\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u8fd0\u52a8\u79bb\u6563\u5316\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u7d27\u51d1\u7684\u9ad8\u5c42\u8bed\u4e49\u8868\u793a\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\u964d\u4f4e\u751f\u6210\u590d\u6742\u5ea6\uff0c\u4e3a\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08077", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08077", "abs": "https://arxiv.org/abs/2602.08077", "authors": ["Sayantan Kumar", "Peijie Qiu", "Aristeidis Sotiras"], "title": "Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders", "comment": "Conference on Health, Inference, and Learning (CHIL)", "summary": "Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.", "AI": {"tldr": "\u63d0\u51fammSIVAE\u6a21\u578b\uff0c\u7ed3\u5408\u8f6f\u81ea\u7701\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6df7\u5408\u4e13\u5bb6\u4e58\u79ef\u805a\u5408\uff0c\u6539\u8fdb\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u7684\u89c4\u8303\u5efa\u6a21\uff0c\u63d0\u5347\u5065\u5eb7\u53c2\u8003\u5206\u5e03\u62df\u5408\u548c\u591a\u6a21\u6001\u878d\u5408\u6548\u679c", "motivation": "\u9488\u5bf9\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u591a\u6a21\u6001\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u4e2d\uff0c\u73b0\u6709VAE\u89c4\u8303\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u5065\u5eb7\u53c2\u8003\u5206\u5e03\u62df\u5408\u4e0d\u5b8c\u7f8e\u5bfc\u81f4\u5047\u9633\u6027\u589e\u52a0\uff1b2) \u540e\u9a8c\u805a\u5408\u65b9\u6cd5\uff08\u5982PoE/MoE\uff09\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u591a\u6a21\u6001\u878d\u5408\u6548\u679c\u5f31", "method": "\u63d0\u51fammSIVAE\uff08\u591a\u6a21\u6001\u8f6f\u81ea\u7701\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff09\uff0c\u7ed3\u5408Mixture-of-Product-of-Experts\uff08MOPOE\uff09\u805a\u5408\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u548c\u7279\u5f81\u7a7a\u95f4\u8ba1\u7b97\u4e0e\u5b66\u4e60\u5230\u7684\u5065\u5eb7\u5206\u5e03\u7684\u8ddd\u79bb\u4f5c\u4e3a\u504f\u5dee\u5206\u6570\uff0c\u5e76\u5c06\u7edf\u8ba1\u663e\u8457\u7684\u6f5c\u5728\u504f\u5dee\u6620\u5c04\u5230\u533a\u57df\u5f02\u5e38\u4ee5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027", "result": "\u5728ADNI\u7684MRI\u533a\u57df\u4f53\u79ef\u548c\u6dc0\u7c89\u6837\u86cb\u767dPET SUVR\u6570\u636e\u4e0a\uff0cmmSIVAE\u5728\u4fdd\u7559\u5bf9\u7167\u7ec4\u4e0a\u6539\u5584\u4e86\u91cd\u5efa\u6548\u679c\uff0c\u76f8\u6bd4VAE\u57fa\u7ebf\u4ea7\u751f\u66f4\u5177\u533a\u5206\u5ea6\u7684\u504f\u5dee\u5206\u6570\u7528\u4e8e\u5f02\u5e38\u68c0\u6d4b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u4f3c\u7136\u6bd4\u548c\u66f4\u6e05\u6670\u7684\u5bf9\u7167\u7ec4\u4e0eAD\u8c31\u7cfb\u961f\u5217\u5206\u79bb\u3002\u504f\u5dee\u56fe\u7a81\u51fa\u4e86\u4e0e\u5df2\u77e5AD\u76f8\u5173\u53d8\u5316\u4e00\u81f4\u7684\u533a\u57df\u7ea7\u6a21\u5f0f", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8bad\u7ec3\u76ee\u6807\u4e2d\u4f18\u5148\u8003\u8651\u53c2\u8003\u5206\u5e03\u4fdd\u771f\u5ea6\u548c\u7a33\u5065\u7684\u591a\u6a21\u6001\u540e\u9a8c\u805a\u5408\u5bf9\u4e8e\u89c4\u8303\u5efa\u6a21\u7684\u91cd\u8981\u6027\uff0c\u5bf9\u8de8\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u7684\u504f\u5dee\u5206\u6790\u5177\u6709\u5e7f\u6cdb\u610f\u4e49"}}
{"id": "2602.08346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08346", "abs": "https://arxiv.org/abs/2602.08346", "authors": ["Yujin Zhou", "Pengcheng Wen", "Jiale Chen", "Boqin Yin", "Han Zhu", "Jiaming Ji", "Juntao Dai", "Chi-Min Chan", "Sirui Han"], "title": "What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning", "comment": null, "summary": "The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\"\u56fe\u50cf\u601d\u7ef4\"\u8303\u5f0f\u4e0b\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u8bc4\u4f30\u95ee\u9898\uff0c\u9996\u6b21\u6784\u5efa\u4e86\u4e13\u95e8\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b1,206\u6761\u4eba\u5de5\u6807\u6ce8\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5b9a\u4e49\u4e867\u79cd\u7ec6\u7c92\u5ea6\u9519\u8bef\u7c7b\u578b\uff0c\u5e76\u53d1\u73b0\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aPRMs\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\"\u56fe\u50cf\u601d\u7ef4\"\u8303\u5f0f\u4f7f\u6a21\u578b\u80fd\u591f\u52a8\u6001\u7f16\u8f91\u548c\u91cd\u65b0\u7f16\u7801\u89c6\u89c9\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e00\u8303\u5f0f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4ea7\u751f\u591a\u79cd\u9519\u8bef\u3002\u867d\u7136\u9700\u8981\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u6765\u533a\u5206\u6b63\u8d1f\u63a8\u7406\u6b65\u9aa4\uff0c\u4f46\u73b0\u6709\u7684PRMs\u57fa\u51c6\u4e3b\u8981\u662f\u6587\u672c\u4e2d\u5fc3\u5316\u7684\uff0c\u7f3a\u4e4f\u5bf9\u8be5\u8303\u5f0f\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u5206\u6790\u63a8\u7406\u8f68\u8ff9\u548cPRMs\u5f15\u5bfc\u641c\u7d22\u5b9e\u9a8c\uff0c\u5b9a\u4e49\u4e867\u79cd\u7ec6\u7c92\u5ea6\u9519\u8bef\u7c7b\u578b\uff1b\u6784\u5efa\u4e86\u5305\u542b1,206\u6761\u4eba\u5de5\u6807\u6ce8\u7684\"\u56fe\u50cf\u601d\u7ef4\"\u63a8\u7406\u8f68\u8ff9\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u6db5\u76d64\u4e2a\u7c7b\u522b\u548c16\u4e2a\u5b50\u7c7b\u522b\uff1b\u5bf9\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aPRMs\u7684\u80fd\u529b\u8fdb\u884c\u4e86\u5b9e\u9a8c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aPRMs\u8868\u73b0\u4e0d\u8db3\uff1a\u5728\u89c6\u89c9\u63a8\u7406\u8fc7\u7a0b\u8bc4\u4f30\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5728\u4e0d\u540c\u9519\u8bef\u7c7b\u578b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u8868\u73b0\u51fa\u6b63\u5411\u8bc4\u4f30\u504f\u89c1\uff0c\u5e76\u5bf9\u63a8\u7406\u6b65\u9aa4\u4f4d\u7f6e\u654f\u611f\u3002\u8fd9\u4e9b\u53d1\u73b0\u8bc1\u660e\u4e86\u57fa\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\"\u56fe\u50cf\u601d\u7ef4\"\u8303\u5f0f\u4e0b\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3aPRMs\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u63a8\u8fdbLVLMs\u4e2dPRMs\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.08086", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08086", "abs": "https://arxiv.org/abs/2602.08086", "authors": ["Liisa Janssens", "Laura Middeldorp"], "title": "Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method", "comment": "6 pages, Pre-publication. Copyright 2026 IEEE. Peer Reviewed. Accepted at ICASSP 2026 - 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), scheduled for 3-8 May 2026 in Barcelona, Spain", "summary": "In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\u6765\u5206\u6790\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u8bc6\u522b\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u52a0\u5f3a\u7cfb\u7edf\u53ef\u4fe1\u5ea6\u7684\u6cd5\u5f8b\u673a\u5236\u8981\u6c42\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u5e94\u5bf9\u65e0\u4eba\u673a\u7cfb\u7edf\u5e26\u6765\u7684\u5404\u79cd\u5a01\u80c1\uff0c\u9700\u8981\u4e13\u95e8\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u3002\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u7b49\u65b0\u5174\u98a0\u8986\u6027\u6280\u672f\u589e\u5f3a\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u53ef\u4ee5\u4ea7\u751f\u66f4\u6709\u6548\u7684\u5bf9\u6297\u63aa\u65bd\u3002\u7136\u800c\uff0c\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u9762\u4e34\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u7684\u6311\u6218\uff0c\u9700\u8981\u786e\u4fdd\u5176\u53ef\u4fe1\u5ea6\u4ee5\u5efa\u7acb\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5408\u7406\u4fe1\u4efb\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\u6765\u5206\u6790\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u7528\u4e8e\u6784\u5efa\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u4f5c\u4e3a\u6311\u6218\u7684\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u53ef\u4ee5\u5728\u73b0\u6709\u6cd5\u6cbb\u673a\u5236\u4e2d\u5b9e\u65bd\u7684\u8981\u6c42\uff0c\u4ee5\u9632\u6b62\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u3002", "result": "\u901a\u8fc7\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u8bba\u6587\u8bc6\u522b\u4e86\u9632\u6b62\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u7684\u5177\u4f53\u8981\u6c42\uff0c\u8fd9\u4e9b\u8981\u6c42\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u6cd5\u6cbb\u673a\u5236\u4e2d\u3002\u8fd9\u4e9b\u8981\u6c42\u589e\u5f3a\u4e86\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u5728\u6c11\u7528\u548c\u519b\u4e8b\u73af\u5883\u4e2d\u5efa\u7acb\u6210\u529f\u7684\u4eba\u673a\u534f\u4f5c\u6240\u9700\u7684\u5408\u7406\u4fe1\u4efb\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u9700\u8981\u89e3\u51b3\u6982\u7387\u9ed1\u5ba2\u653b\u51fb\u7684\u6311\u6218\u4ee5\u786e\u4fdd\u53ef\u4fe1\u5ea6\u3002\u901a\u8fc7\u57fa\u4e8e\u573a\u666f\u7684\u65b9\u6cd5\u8bc6\u522b\u5e76\u5b9e\u65bd\u5230\u6cd5\u6cbb\u673a\u5236\u4e2d\u7684\u8981\u6c42\uff0c\u53ef\u4ee5\u52a0\u5f3a\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u4ece\u800c\u5efa\u7acb\u5408\u7406\u4fe1\u4efb\uff0c\u8fd9\u5bf9\u4e8e\u6210\u529f\u7684\u4eba\u673a\u534f\u4f5c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.08388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08388", "abs": "https://arxiv.org/abs/2602.08388", "authors": ["Shuo Zhang", "Wenzhuo Wu", "Huayu Zhang", "Jiarong Cheng", "Xianghao Zang", "Chao Ban", "Hao Sun", "Zhongjiang He", "Tianwei Cao", "Kongming Liang", "Zhanyu Ma"], "title": "Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers", "comment": null, "summary": "Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.", "AI": {"tldr": "GeoEdit\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563transformer\u6a21\u5757\u5b9e\u73b0\u7cbe\u786e\u7684\u51e0\u4f55\u53d8\u6362\u7f16\u8f91\uff0c\u5e76\u5f15\u5165\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\u6765\u589e\u5f3a\u5149\u5f71\u6548\u679c\uff0c\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u51e0\u4f55\u53d8\u6362\u7f16\u8f91\uff08\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\uff09\uff1b2\uff09\u5bf9\u590d\u6742\u5149\u5f71\u6548\u679c\u5efa\u6a21\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u771f\u5b9e\u3002\u8fd9\u4e9b\u95ee\u9898\u5728\u590d\u6742\u573a\u666f\u4e2d\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u63d0\u51faGeoEdit\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u901a\u8fc7\u6269\u6563transformer\u6a21\u5757\u5b9e\u73b0\u4e0a\u4e0b\u6587\u751f\u6210\uff0c\u96c6\u6210\u51e0\u4f55\u53d8\u6362\u8fdb\u884c\u7cbe\u786e\u5bf9\u8c61\u7f16\u8f91\uff1b2\uff09\u5f15\u5165\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u589e\u5f3a\u590d\u6742\u5149\u5f71\u548c\u9634\u5f71\u6548\u679c\u7684\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u5305\u542b12\u4e07\u9ad8\u8d28\u91cf\u56fe\u50cf\u5bf9\u7684RS-Objects\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGeoEdit\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u771f\u5b9e\u611f\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "GeoEdit\u901a\u8fc7\u521b\u65b0\u7684\u6269\u6563transformer\u67b6\u6784\u548c\u6548\u679c\u654f\u611f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u51e0\u4f55\u53d8\u6362\u7f16\u8f91\u548c\u5149\u5f71\u6548\u679c\u5efa\u6a21\u65b9\u9762\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u8d28\u91cf\u548c\u771f\u5b9e\u611f\u3002"}}
{"id": "2602.08088", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08088", "abs": "https://arxiv.org/abs/2602.08088", "authors": ["Mohammad Abu-Shaira", "Weishi Shi"], "title": "Online Domain-aware LLM Decoding for Continual Domain Evolution", "comment": null, "summary": "LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.", "AI": {"tldr": "ODD\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u6982\u7387\u878d\u5408\u548c\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u8c03\u5236\uff0c\u4f7fLLM\u80fd\u591f\u5b9e\u65f6\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u65e0\u9700\u6602\u8d35\u91cd\u8bad\u7ec3", "motivation": "\u4f20\u7edfLLM\u79bb\u7ebf\u5fae\u8c03\u5047\u8bbe\u9759\u6001\u9886\u57df\uff0c\u4f46\u5b9e\u9645\u4e2d\u9886\u57df\u77e5\u8bc6\u6301\u7eed\u6f14\u5316\uff08\u65b0\u6cd5\u89c4\u3001\u4ea7\u54c1\u3001\u670d\u52a1\u7b49\uff09\uff0c\u91cd\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\uff0c\u4e14\u73b0\u5b9e\u73af\u5883\u5b58\u5728\u6570\u636e\u5206\u5e03\u6f02\u79fb\uff08\u6982\u5ff5\u6f02\u79fb\uff09\uff0c\u9700\u8981\u9ad8\u6548\u5b9e\u65f6\u9002\u5e94\u65b9\u6848", "method": "\u63d0\u51fa\u5728\u7ebf\u9886\u57df\u611f\u77e5\u89e3\u7801\u6846\u67b6\uff08ODD\uff09\uff0c\u5728\u57fa\u7840LLM\u548c\u524d\u7f00\u6811\u5148\u9a8c\u4e4b\u95f4\u8fdb\u884c\u6982\u7387\u7ea7\u878d\u5408\uff0c\u4f7f\u7528\u5206\u6b67\u548c\u8fde\u7eed\u6027\u4fe1\u53f7\u6307\u5bfc\u81ea\u9002\u5e94\u7f6e\u4fe1\u5ea6\u8c03\u5236", "result": "\u5728\u591a\u79cd\u6f02\u79fb\u573a\u666f\u4e0b\uff0cODD\u5728\u6240\u6709\u53e5\u6cd5\u548c\u8bed\u4e49NLG\u6307\u6807\u4e0a\u5747\u4f18\u4e8eLLM-Greedy\u548cLLM-Temp Scaled\uff0c\u83b7\u5f970.065\u7684\u7edd\u5bf9ROUGE-L\u589e\u76ca\u548c13.6%\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u76f8\u5bf9\u63d0\u5347", "conclusion": "ODD\u5bf9\u6f14\u5316\u7684\u8bcd\u6c47\u548c\u4e0a\u4e0b\u6587\u6a21\u5f0f\u5177\u6709\u9c81\u68d2\u6027\uff0c\u9002\u5408\u52a8\u6001LLM\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u9886\u57df\u6f14\u5316\u4e0e\u9759\u6001\u9002\u5e94\u7ba1\u9053\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898"}}
{"id": "2602.08395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08395", "abs": "https://arxiv.org/abs/2602.08395", "authors": ["Jianfeng Liang", "Shaocheng Shen", "Botao Xu", "Qiang Hu", "Xiaoyun Zhang"], "title": "D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy", "comment": null, "summary": "The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \\textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \\textbf{12$\\times$}", "AI": {"tldr": "\u63d0\u51faD\u00b2-VR\u6846\u67b6\uff0c\u901a\u8fc7\u964d\u566a\u9c81\u68d2\u6d41\u5bf9\u9f50\u6a21\u5757\u548c\u5bf9\u6297\u84b8\u998f\u6280\u672f\uff0c\u5728\u4fdd\u6301\u89c6\u9891\u4fee\u590d\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06\u91c7\u6837\u901f\u5ea6\u63d0\u534712\u500d\u5e76\u589e\u5f3a\u65f6\u95f4\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u5148\u9a8c\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u4f9b\u4f18\u79c0\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u4f46\u5728\u9762\u5bf9\u590d\u6742\u771f\u5b9e\u4e16\u754c\u9000\u5316\u65f6\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u8fc7\u9ad8\u548c\u65f6\u95f4\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72", "method": "1. \u8bbe\u8ba1\u964d\u566a\u9c81\u68d2\u6d41\u5bf9\u9f50\u6a21\u5757\uff0c\u5229\u7528\u7f6e\u4fe1\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\u8fc7\u6ee4\u4e0d\u53ef\u9760\u8fd0\u52a8\u7ebf\u7d22\uff1b2. \u91c7\u7528\u5bf9\u6297\u84b8\u998f\u8303\u5f0f\u5c06\u6269\u6563\u91c7\u6837\u8f68\u8ff9\u538b\u7f29\u5230\u5feb\u901f\u5c11\u6b65\u63a8\u7406\uff1b3. \u8bbe\u8ba1\u534f\u540c\u4f18\u5316\u7b56\u7565\u5e73\u8861\u611f\u77e5\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027", "result": "D\u00b2-VR\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u91c7\u6837\u8fc7\u7a0b\u52a0\u901f12\u500d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u65f6\u95f4\u7a33\u5b9a\u6027", "conclusion": "\u63d0\u51fa\u7684D\u00b2-VR\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5ef6\u8fdf\u548c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u4fee\u590d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.08105", "categories": ["cs.LG", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08105", "abs": "https://arxiv.org/abs/2602.08105", "authors": ["Paarth Gulati", "Eslam Abdelaleem", "Audrey Sederberg", "Ilya Nemenman"], "title": "Mutual information and task-relevant latent dimensionality", "comment": null, "summary": "Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u74f6\u9888\u7684\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u6279\u8bc4\u5668\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u4f30\u8ba1\u5668\u9ad8\u4f30\u7ef4\u5ea6\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u65e0\u9700\u626b\u63cf\u74f6\u9888\u5c3a\u5bf8\u7684\u4e00\u6b65\u5f0f\u534f\u8bae\u3002", "motivation": "\u4f30\u8ba1\u9884\u6d4b\u6240\u9700\u7684\u6f5c\u5728\u8868\u793a\u7ef4\u5ea6\uff08\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\uff09\u662f\u4e00\u4e2a\u56f0\u96be\u4e14\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u79d1\u5b66\u5e94\u7528\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u566a\u58f0\u73af\u5883\u4e0b\u3002", "method": "\u5c06\u7ef4\u5ea6\u4f30\u8ba1\u8f6c\u5316\u4e3a\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff1a\u4ec0\u4e48\u5d4c\u5165\u74f6\u9888\u7ef4\u5ea6\u8db3\u4ee5\u538b\u7f29\u9884\u6d4b\u5668\u548c\u88ab\u9884\u6d4b\u89c6\u56fe\uff0c\u540c\u65f6\u4fdd\u6301\u5b83\u4eec\u7684\u4e92\u4fe1\u606f\u3002\u63d0\u51fa\u6df7\u5408\u6279\u8bc4\u5668\uff0c\u5728\u4fdd\u7559\u663e\u5f0f\u7ef4\u5ea6\u74f6\u9888\u7684\u540c\u65f6\u5141\u8bb8\u7075\u6d3b\u7684\u975e\u7ebf\u6027\u8de8\u89c6\u56fe\u4ea4\u4e92\u3002\u5f00\u53d1\u4e00\u6b65\u5f0f\u534f\u8bae\uff0c\u4ece\u5355\u4e2a\u8fc7\u53c2\u6570\u5316\u6df7\u5408\u6a21\u578b\u4e2d\u76f4\u63a5\u8bfb\u53d6\u6709\u6548\u7ef4\u5ea6\u3002", "result": "\u5728\u5df2\u77e5\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\u7684\u5408\u6210\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u6269\u5c55\u5230\u5185\u5728\u7ef4\u5ea6\u4f30\u8ba1\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u6bd4\u7ecf\u5178\u51e0\u4f55\u7ef4\u5ea6\u4f30\u8ba1\u5668\u66f4\u53ef\u9760\u3002\u5728\u591a\u4e2a\u7269\u7406\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u795e\u7ecf\u4f30\u8ba1\u5668\u9ad8\u4f30\u7ef4\u5ea6\u7684\u95ee\u9898\uff0c\u5e76\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2602.08397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08397", "abs": "https://arxiv.org/abs/2602.08397", "authors": ["Chiara Lena", "Davide Milesi", "Alessandro Casella", "Luca Carlini", "Joseph C. Norton", "James Martin", "Bruno Scaglioni", "Keith L. Obstein", "Roberto De Sire", "Marco Spadaccini", "Cesare Hassan", "Pietro Valdastri", "Elena De Momi"], "title": "RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications", "comment": null, "summary": "Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.", "AI": {"tldr": "RealSynCol\u662f\u4e00\u4e2a\u9ad8\u5ea6\u903c\u771f\u7684\u5408\u6210\u7ed3\u80a0\u955c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u7ed3\u80a0\u955c3D\u91cd\u5efa\u4e2d\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u5728\u4e34\u5e8a\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u53ef\u4ee5\u6539\u5584\u7ed3\u80a0\u955c\u68c0\u67e5\uff0c\u901a\u8fc73D\u91cd\u5efa\u63d0\u4f9b\u5168\u9762\u7684\u9ecf\u819c\u8868\u9762\u548c\u75c5\u53d8\u89c6\u56fe\uff0c\u5e76\u5e2e\u52a9\u8bc6\u522b\u672a\u63a2\u7d22\u533a\u57df\u3002\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u53d1\u5c55\u53d7\u5230\u5927\u89c4\u6a21\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002", "method": "\u4ece10\u4e2aCT\u626b\u63cf\u4e2d\u63d0\u53d6\u7ed3\u80a0\u51e0\u4f55\u7ed3\u6784\uff0c\u5bfc\u5165\u5230\u6a21\u62df\u672f\u4e2d\u6761\u4ef6\u7684\u865a\u62df\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u903c\u771f\u7684\u8840\u7ba1\u7eb9\u7406\u8fdb\u884c\u6e32\u67d3\u3002\u751f\u6210\u7684\u6570\u636e\u96c6\u5305\u542b28,130\u5e27\u56fe\u50cf\uff0c\u914d\u6709\u6df1\u5ea6\u56fe\u3001\u5149\u6d41\u30013D\u7f51\u683c\u548c\u76f8\u673a\u8f68\u8ff9\u7b49\u771f\u5b9e\u6807\u6ce8\u3002", "result": "\u57fa\u51c6\u7814\u7a76\u8868\u660e\uff0cRealSynCol\u7684\u9ad8\u771f\u5b9e\u611f\u548c\u591a\u6837\u6027\u663e\u8457\u63d0\u5347\u4e86\u5728\u4e34\u5e8a\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u8bc1\u660e\u5b83\u662f\u5f00\u53d1\u652f\u6301\u5185\u955c\u8bca\u65ad\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u5f3a\u5927\u5de5\u5177\u3002", "conclusion": "RealSynCol\u662f\u4e00\u4e2a\u9ad8\u5ea6\u903c\u771f\u7684\u5408\u6210\u7ed3\u80a0\u955c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301\u7ed3\u80a0\u955c3D\u91cd\u5efa\u548c\u75c5\u53d8\u68c0\u6d4b\u7b49\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u5f00\u53d1\u3002"}}
{"id": "2602.08430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08430", "abs": "https://arxiv.org/abs/2602.08430", "authors": ["Qiang Wang"], "title": "Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features", "comment": null, "summary": "We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.", "AI": {"tldr": "\u91cd\u65b0\u5ba1\u89c6\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7a00\u758f\u56fe\u50cf\u5339\u914d\u6a21\u578b\u8bad\u7ec3\uff0c\u53d1\u73b0\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u5bf9LightGlue\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff0c\u68c0\u6d4b\u5668\u6bd4\u63cf\u8ff0\u7b26\u5bf9\u6027\u80fd\u5dee\u5f02\u5f71\u54cd\u66f4\u5927\uff0c\u63d0\u51fa\u4f7f\u7528\u591a\u6837\u5316\u68c0\u6d4b\u5668\u5173\u952e\u70b9\u5fae\u8c03\u73b0\u6709\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5f97\u5230\u901a\u7528\u68c0\u6d4b\u5668\u65e0\u5173\u6a21\u578b", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u7a00\u758f\u56fe\u50cf\u5339\u914d\u6a21\u578b\u8bad\u7ec3\u95ee\u9898\uff0c\u8bc6\u522b\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u5bf9LightGlue\u6a21\u578b\u6027\u80fd\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63a2\u7a76\u68c0\u6d4b\u5668\u548c\u63cf\u8ff0\u7b26\u5728\u57fa\u4e8etransformer\u7684\u5339\u914d\u6846\u67b6\u4e2d\u7684\u4f5c\u7528", "method": "\u9996\u5148\u8bc6\u522b\u5f71\u54cdLightGlue\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u7136\u540e\u7814\u7a76\u68c0\u6d4b\u5668\u548c\u63cf\u8ff0\u7b26\u5728transformer\u5339\u914d\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\uff0c\u6700\u540e\u63d0\u51fa\u4f7f\u7528\u6765\u81ea\u591a\u6837\u5316\u68c0\u6d4b\u5668\u96c6\u5408\u7684\u5173\u952e\u70b9\u5fae\u8c03\u73b0\u6709\u56fe\u50cf\u5339\u914d\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5f97\u5230\u901a\u7528\u68c0\u6d4b\u5668\u65e0\u5173\u6a21\u578b", "result": "\u53d1\u73b0\u68c0\u6d4b\u5668\uff08\u800c\u975e\u63cf\u8ff0\u7b26\uff09\u901a\u5e38\u662f\u6027\u80fd\u5dee\u5f02\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u63d0\u51fa\u7684\u901a\u7528\u68c0\u6d4b\u5668\u65e0\u5173\u6a21\u578b\u5728\u4f5c\u4e3a\u65b0\u68c0\u6d4b\u5668\u7684\u96f6\u6837\u672c\u5339\u914d\u5668\u65f6\uff0c\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u4e13\u95e8\u4e3a\u8fd9\u4e9b\u7279\u5f81\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u51c6\u786e\u6027", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u57fa\u4e8etransformer\u7684\u5339\u914d\u6a21\u578b\u7684\u90e8\u7f72\u548c\u5c40\u90e8\u7279\u5f81\u7684\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u63d0\u51fa\u7684\u901a\u7528\u68c0\u6d4b\u5668\u65e0\u5173\u6a21\u578b\u80fd\u591f\u6709\u6548\u9002\u5e94\u4e0d\u540c\u7684\u68c0\u6d4b\u5668"}}
{"id": "2602.08142", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08142", "abs": "https://arxiv.org/abs/2602.08142", "authors": ["H. Martin Gillis", "Isaac Xu", "Thomas Trappenberg"], "title": "Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation", "comment": null, "summary": "Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.", "AI": {"tldr": "VGE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u95e8\u63a7\u7684\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u53f7\u566a\u58f0\u6bd4\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u53ef\u5fae\u5206\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u52a0\u6027\u5206\u89e3\u65b9\u6cd5\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u6216\u8fd1\u4f3c\u65b9\u6cd5\u4e2d\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u52a0\u6027\u5206\u89e3\u4e3a\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u5728\u4f7f\u7528\u6709\u9650\u96c6\u6210\u91c7\u6837\u548c/\u6216\u4e0d\u5339\u914d\u9884\u6d4b\u5206\u5e03\u65f6\u4f1a\u5931\u6548\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u65b9\u5dee\u95e8\u63a7\u96c6\u6210\uff08VGE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u7edf\u8ba1\u91cf\u8ba1\u7b97\u4fe1\u53f7\u566a\u58f0\u6bd4\u95e8\u63a7\u6765\u6ce8\u5165\u8ba4\u77e5\u654f\u611f\u6027\uff0c\u5305\u62ecVGMU\u8bc4\u5206\uff08\u8026\u5408\u51b3\u7b56\u8fb9\u754c\u4e0e\u96c6\u6210\u9884\u6d4b\u65b9\u5dee\uff09\u548cVGN\u5c42\uff08\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6bcf\u7c7b\u5f52\u4e00\u5316\u6269\u5c55\u95e8\u63a7\u673a\u5236\uff09\u3002", "result": "VGE\u5728\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u4fe1\u606f\u8bba\u57fa\u7ebf\u65b9\u6cd5\u7684\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u96c6\u6210\u6a21\u578b\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "VGE\u4e3a\u96c6\u6210\u6a21\u578b\u4e2d\u7684\u8ba4\u77e5\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u52a0\u6027\u5206\u89e3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.08439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08439", "abs": "https://arxiv.org/abs/2602.08439", "authors": ["Yuhao Dong", "Shulin Tian", "Shuai Liu", "Shuangrui Ding", "Yuhang Zang", "Xiaoyi Dong", "Yuhang Cao", "Jiaqi Wang", "Ziwei Liu"], "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition", "comment": null, "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Demo\u9a71\u52a8\u7684\u89c6\u9891\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86Demo-ICL-Bench\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u4ece\u5c11\u91cf\u793a\u4f8b\u4e2d\u5b66\u4e60\u548c\u9002\u5e94\u52a8\u6001\u65b0\u60c5\u5883\u7684\u80fd\u529b\uff0c\u540c\u65f6\u5f00\u53d1\u4e86Demo-ICL\u6a21\u578b\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6a21\u578b\u57fa\u4e8e\u9759\u6001\u5185\u90e8\u77e5\u8bc6\u7684\u7406\u89e3\u80fd\u529b\uff0c\u800c\u975e\u4ece\u52a8\u6001\u65b0\u9896\u60c5\u5883\u4e2d\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u5b66\u4e60\u548c\u9002\u5e94\u7684\u80fd\u529b\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc4\u4f30\u548c\u63d0\u5347\u89c6\u9891\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u65b0\u4efb\u52a1\u548c\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86Demo\u9a71\u52a8\u7684\u89c6\u9891\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\uff1b\u6784\u5efa\u4e86Demo-ICL-Bench\u57fa\u51c6\uff0c\u5305\u542b1200\u4e2a\u6559\u5b66YouTube\u89c6\u9891\u53ca\u76f8\u5173\u95ee\u9898\uff0c\u63d0\u4f9b\u6587\u672c\u548c\u89c6\u9891\u4e24\u79cd\u6f14\u793a\u7c7b\u578b\uff1b\u5f00\u53d1\u4e86Demo-ICL\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u89c6\u9891\u76d1\u7763\u5fae\u8c03\u548c\u4fe1\u606f\u8f85\u52a9\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3002", "result": "\u5bf9\u6700\u5148\u8fdbMLLM\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\u4e86Demo-ICL-Bench\u7684\u96be\u5ea6\uff0c\u8bc1\u660e\u4e86Demo-ICL\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u89c6\u9891\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u65b0\u4efb\u52a1\u3001\u57fa\u51c6\u548c\u6a21\u578b\u4e3a\u89c6\u9891\u7406\u89e3\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\u548c\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u4ece\u52a8\u6001\u60c5\u5883\u4e2d\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.08448", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08448", "abs": "https://arxiv.org/abs/2602.08448", "authors": ["Haocheng Lu", "Nan Zhang", "Wei Tao", "Xiaoyang Qu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries", "comment": "Accepted to AAAI 2026 (Main Technical Track)", "summary": "Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.", "AI": {"tldr": "Vista\u662f\u4e00\u4e2a\u7528\u4e8e\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u573a\u666f\u611f\u77e5\u7684\u5206\u5272\u3001\u538b\u7f29\u548c\u53ec\u56de\u673a\u5236\uff0c\u89e3\u51b3\u4e86MLLMs\u5728\u5904\u7406\u8fde\u7eed\u89c6\u9891\u6d41\u65f6\u7684\u5185\u5b58\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\uff0c\u56e0\u4e3a\u89c6\u9891\u5e27\u662f\u987a\u5e8f\u5230\u8fbe\u7684\uff0c\u7528\u6237\u67e5\u8be2\u53ef\u4ee5\u5728\u4efb\u610f\u65f6\u95f4\u70b9\u53d1\u51fa\u3002\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u5927\u5c0f\u5185\u5b58\u6216\u7b80\u5355\u538b\u7f29\u7684\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u5b58\u5728\u4e0a\u4e0b\u6587\u4e22\u5931\u6216\u5185\u5b58\u6ea2\u51fa\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u957f\u683c\u5f0f\u3001\u5b9e\u65f6\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "Vista\u7684\u521b\u65b0\u5305\u62ec\u4e09\u4e2a\u65b9\u9762\uff1a1) \u573a\u666f\u611f\u77e5\u5206\u5272 - \u52a8\u6001\u5c06\u4f20\u5165\u5e27\u805a\u7c7b\u4e3a\u65f6\u95f4\u548c\u89c6\u89c9\u4e0a\u8fde\u8d2f\u7684\u573a\u666f\u5355\u5143\uff1b2) \u573a\u666f\u611f\u77e5\u538b\u7f29 - \u5c06\u6bcf\u4e2a\u573a\u666f\u538b\u7f29\u4e3a\u7d27\u51d1\u7684token\u8868\u793a\u5b58\u50a8\u5728GPU\u5185\u5b58\u4e2d\uff0c\u540c\u65f6\u5c06\u5168\u5206\u8fa8\u7387\u5e27\u5378\u8f7d\u5230CPU\u5185\u5b58\uff1b3) \u573a\u666f\u611f\u77e5\u53ec\u56de - \u5728\u63a5\u6536\u5230\u67e5\u8be2\u65f6\u9009\u62e9\u6027\u53ec\u56de\u76f8\u5173\u573a\u666f\u5e76\u91cd\u65b0\u6574\u5408\u5230\u6a21\u578b\u8f93\u5165\u4e2d\u3002", "result": "\u5728StreamingBench\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVista\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u51c6\u3002", "conclusion": "Vista\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4e0e\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u9aa8\u5e72\u65e0\u7f1d\u96c6\u6210\uff0c\u5728\u4e0d\u5f71\u54cd\u5ef6\u8fdf\u6216\u5185\u5b58\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u4e3a\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08151", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08151", "abs": "https://arxiv.org/abs/2602.08151", "authors": ["Yoav Freund", "Nicholas J. A. Harvey", "Victor S. Portella", "Yabing Qi", "Yu-Xiang Wang"], "title": "A second order regret bound for NormalHedge", "comment": null, "summary": "We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $\u03b5$-quantile regret bound of $O\\big(\\sqrt{V_T \\log(V_T/\u03b5)}\\big) $ when $V_T > \\log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdNormalHedge\u53d8\u4f53\u7b97\u6cd5\uff0c\u5728\"\u7b80\u5355\"\u5e8f\u5217\u4e0a\u5b9e\u73b0\u4e8c\u9636\u03b5-\u5206\u4f4d\u6570\u9057\u61be\u754cO(\u221a(V_T log(V_T/\u03b5)))\uff0c\u5176\u4e2dV_T\u662f\u77ac\u65f6\u4e13\u5bb6\u9057\u61be\u7684\u4e8c\u9636\u77e9\u7d2f\u79ef\u91cf\u3002", "motivation": "\u9488\u5bf9\"\u7b80\u5355\"\u5e8f\u5217\u7684\u4e13\u5bb6\u5efa\u8bae\u9884\u6d4b\u95ee\u9898\uff0c\u5bfb\u6c42\u66f4\u597d\u7684\u9057\u61be\u754c\u3002\u5f53\u5e8f\u5217\u76f8\u5bf9\u5bb9\u6613\u65f6\uff0c\u5e0c\u671b\u83b7\u5f97\u6bd4\u6807\u51c6\u9057\u61be\u754c\u66f4\u4f18\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u63d0\u51faNormalHedge\u7b97\u6cd5\u7684\u53d8\u4f53\uff0c\u901a\u8fc7\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7684\u8fde\u7eed\u65f6\u95f4\u6781\u9650\u63a8\u5bfc\u7b97\u6cd5\u52a8\u673a\uff0c\u5728\u79bb\u6563\u65f6\u95f4\u5206\u6790\u4e2d\u4f7f\u7528\u81ea\u534f\u8c03\u6280\u672f\u3002", "result": "\u5f53V_T > log N\u65f6\uff0c\u7b97\u6cd5\u83b7\u5f97\u4e8c\u9636\u03b5-\u5206\u4f4d\u6570\u9057\u61be\u754cO(\u221a(V_T log(V_T/\u03b5)))\uff0c\u5176\u4e2dV_T\u662f\u77ac\u65f6\u4e13\u5bb6\u9057\u61be\u7684\u4e8c\u9636\u77e9\u7d2f\u79ef\u91cf\uff0c\u7531\u7b97\u6cd5\u786e\u5b9a\u7684\u81ea\u7136\u5206\u5e03\u5e73\u5747\u5f97\u5230\u3002", "conclusion": "\u8be5NormalHedge\u53d8\u4f53\u7b97\u6cd5\u5728\"\u7b80\u5355\"\u5e8f\u5217\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u4e8c\u9636\u9057\u61be\u754c\uff0c\u4e3a\u4e13\u5bb6\u5efa\u8bae\u9884\u6d4b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.08462", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08462", "abs": "https://arxiv.org/abs/2602.08462", "authors": ["Yiyang Cao", "Yunze Deng", "Ziyu Lin", "Bin Feng", "Xinggang Wang", "Wenyu Liu", "Dandan Zheng", "Jingdong Chen"], "title": "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation", "comment": null, "summary": "Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.", "AI": {"tldr": "TriC-Motion\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4e09\u57df\u56e0\u679c\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u9891\u4e09\u57df\u8054\u5408\u5efa\u6a21\u4e0e\u56e0\u679c\u5e72\u9884\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u57df\u8054\u5408\u4f18\u5316\u548c\u566a\u58f0\u89e3\u8026\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u65f6\u7a7a\u5efa\u6a21\u6216\u72ec\u7acb\u9891\u57df\u5206\u6790\uff0c\u7f3a\u4e4f\u8de8\u65f6\u7a7a\u9891\u4e09\u57df\u7684\u7edf\u4e00\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u5bfc\u81f4\u65e0\u6cd5\u540c\u65f6\u5229\u7528\u6240\u6709\u57df\u7684\u4fe1\u606f\uff0c\u751f\u6210\u8d28\u91cf\u53d7\u9650\u3002\u6b64\u5916\uff0c\u8fd0\u52a8\u751f\u6210\u6846\u67b6\u4e2d\u566a\u58f0\u5f15\u8d77\u7684\u8fd0\u52a8\u65e0\u5173\u7ebf\u7d22\u4e0e\u6709\u76ca\u7279\u5f81\u7ea0\u7f20\uff0c\u5bfc\u81f4\u8fd0\u52a8\u5931\u771f\u3002", "method": "\u63d0\u51faTriC-Motion\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u5efa\u6a21\u6a21\u5757\uff1a\u65f6\u95f4\u8fd0\u52a8\u7f16\u7801\u3001\u7a7a\u95f4\u62d3\u6251\u5efa\u6a21\u548c\u6df7\u5408\u9891\u7387\u5206\u6790\u3002\u901a\u8fc7\u8bc4\u5206\u5f15\u5bfc\u7684\u4e09\u57df\u878d\u5408\u6a21\u5757\u6574\u5408\u4e09\u57df\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u62d3\u6251\u3001\u8fd0\u52a8\u8d8b\u52bf\u548c\u52a8\u529b\u5b66\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u56e0\u679c\u7684\u53cd\u4e8b\u5b9e\u8fd0\u52a8\u89e3\u8026\u5668\uff0c\u66b4\u9732\u8fd0\u52a8\u65e0\u5173\u7ebf\u7d22\u4ee5\u6d88\u9664\u566a\u58f0\uff0c\u89e3\u8026\u5404\u57df\u7684\u771f\u5b9e\u5efa\u6a21\u8d21\u732e\u3002", "result": "\u5728HumanML3D\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0cR@1\u8fbe\u52300.612\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u8fde\u8d2f\u3001\u591a\u6837\u4e14\u4e0e\u6587\u672c\u5bf9\u9f50\u7684\u8fd0\u52a8\u5e8f\u5217\u3002", "conclusion": "TriC-Motion\u901a\u8fc7\u65f6\u7a7a\u9891\u4e09\u57df\u8054\u5408\u5efa\u6a21\u4e0e\u56e0\u679c\u5e72\u9884\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u751f\u6210\uff0c\u4e3a\u591a\u57df\u8054\u5408\u4f18\u5316\u548c\u566a\u58f0\u89e3\u8026\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08159", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08159", "abs": "https://arxiv.org/abs/2602.08159", "authors": ["Seonglae Cho", "Zekun Wu", "Kleyton Da Costa", "Adriano Koshiyama"], "title": "The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models", "comment": null, "summary": "When a language model asserts that \"the capital of Australia is Sydney,\" does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u7b80\u5355\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u8868\u793a\u6b63\u786e\u6027\uff0c\u4ec5\u97003-8\u7ef4\u5373\u53ef\u6709\u6548\u533a\u5206\u5bf9\u9519\uff0c\u7ebf\u6027\u5206\u7c7b\u5668\u6548\u679c\u6700\u4f73\uff0c\u4e14\u53ef\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u56e0\u679c\u9a8c\u8bc1", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\"\u77e5\u9053\"\u81ea\u5df1\u9648\u8ff0\u7684\u9519\u8bef\uff08\u5982\"\u6fb3\u5927\u5229\u4e9a\u9996\u90fd\u662f\u6089\u5c3c\"\uff09\uff0c\u4ee5\u53ca\u8fd9\u79cd\u77e5\u8bc6\u5728\u6a21\u578b\u5185\u90e8\u5982\u4f55\u8868\u793a", "method": "\u5206\u67909\u4e2a\u4e0d\u540c\u67b6\u6784\u6a21\u578b\uff0c\u7814\u7a76\u6b63\u786e\u6027\u8868\u793a\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u4f7f\u7528\u7ebf\u6027/\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u6bd4\u8f83\uff0c\u901a\u8fc7\u8d28\u5fc3\u8ddd\u79bb\u65b9\u6cd5\u8fdb\u884c\u5c11\u6837\u672c\u68c0\u6d4b\uff0c\u5e76\u7528\u6fc0\u6d3b\u5f15\u5bfc\u8fdb\u884c\u56e0\u679c\u9a8c\u8bc1", "result": "\u6b63\u786e\u6027\u4fe1\u53f7\u4ec5\u97003-8\u7ef4\uff0c\u66f4\u591a\u7ef4\u5ea6\u53cd\u800c\u964d\u4f4e\u6027\u80fd\uff1b\u7ebf\u6027\u5206\u7c7b\u5668\u6548\u679c\u6700\u4f73\uff1b\u8d28\u5fc3\u8ddd\u79bb\u65b9\u6cd5\u8fbe\u52300.90 AUC\uff0c\u4ec5\u970025\u4e2a\u6837\u672c\u5373\u53ef\u8fbe\u5230GPT-2\u5168\u6570\u636e\u51c6\u786e\u7387\u768489%\uff1b\u6fc0\u6d3b\u5f15\u5bfc\u53ef\u6539\u53d810.9%\u7684\u9519\u8bef\u7387\uff1b\u5185\u90e8\u63a2\u9488AUC\u4e3a0.80-0.97\uff0c\u800c\u57fa\u4e8e\u8f93\u51fa\u7684\u65b9\u6cd5\u4ec50.44-0.64", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5b58\u5728\u6b63\u786e\u6027\u4fe1\u53f7\u4f46\u672a\u5728\u8f93\u51fa\u4e2d\u8868\u8fbe\uff1b\u6b63\u786e\u6027\u5206\u79bb\u672c\u8d28\u4e0a\u662f\u5747\u503c\u504f\u79fb\u7684\u51e0\u4f55\u95ee\u9898\u800c\u975e\u5b66\u4e60\u95ee\u9898\uff1b\u8d28\u5fc3\u8ddd\u79bb\u65b9\u6cd5\u53ef\u6709\u6548\u68c0\u6d4b\u6a21\u578b\u662f\u5426\"\u77e5\u9053\"\u9519\u8bef"}}
{"id": "2602.08479", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08479", "abs": "https://arxiv.org/abs/2602.08479", "authors": ["Alif Rizqullah Mahdi", "Mahdi Rezaei", "Natasha Merat"], "title": "Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation", "comment": "9th International Conference on Instrumentation, Control, and Automation (ICA)", "summary": "Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.", "AI": {"tldr": "\u4f7f\u75282D\u59ff\u6001\u4f30\u8ba1\u5bf9\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u89c6\u9891\u4e2d\u7684\u884c\u4eba\u624b\u52bf\u8fdb\u884c\u5206\u7c7b\uff0c\u5c06\u624b\u52bf\u5206\u4e3a\u505c\u6b62\u3001\u901a\u884c\u3001\u611f\u8c22\u95ee\u5019\u548c\u65e0\u624b\u52bf\u56db\u7c7b\uff0c\u901a\u8fc7\u63d0\u53d676\u4e2a\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81\uff0c\u5b9e\u73b0\u4e8687%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u624b\u52bf\u662f\u4ea4\u901a\u4e2d\u975e\u8bed\u8a00\u4ea4\u6d41\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u6709\u52a9\u4e8e\u884c\u4eba-\u9a7e\u9a76\u5458\u4e92\u52a8\uff0c\u4f46\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u96be\u4ee5\u89e3\u91ca\u8fd9\u4e9b\u624b\u52bf\u3002\u5f53\u6b63\u5f0f\u4ea4\u901a\u89c4\u5219\u4e0d\u8db3\u65f6\uff0c\u8fd9\u4e2a\u95ee\u9898\u66f4\u52a0\u660e\u663e\u3002", "method": "\u4f7f\u75282D\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u5904\u7406WIVW\u6570\u636e\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u5e8f\u5217\uff0c\u5c06\u624b\u52bf\u5206\u4e3a\u56db\u7c7b\uff08\u505c\u6b62\u3001\u901a\u884c\u3001\u611f\u8c22\u95ee\u5019\u3001\u65e0\u624b\u52bf\uff09\uff0c\u4ece\u5f52\u4e00\u5316\u7684\u5173\u952e\u70b9\u4e2d\u63d0\u53d676\u4e2a\u9759\u6001\u548c\u52a8\u6001\u7279\u5f81\u3002", "result": "\u5206\u6790\u8868\u660e\u624b\u90e8\u4f4d\u7f6e\u548c\u8fd0\u52a8\u901f\u5ea6\u5728\u533a\u5206\u624b\u52bf\u7c7b\u522b\u65f6\u7279\u522b\u5177\u6709\u5224\u522b\u529b\uff0c\u5b9e\u73b0\u4e8687%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u611f\u77e5\u80fd\u529b\uff0c\u8fd8\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u5730\u7406\u89e3\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u884c\u4eba\u884c\u4e3a\u3002"}}
{"id": "2602.08169", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08169", "abs": "https://arxiv.org/abs/2602.08169", "authors": ["Zejia You", "Chunyuan Deng", "Hanjie Chen"], "title": "Spherical Steering: Geometry-Aware Activation Rotation for Language Models", "comment": "The code is at: https://github.com/chili-lab/Spherical-Steering", "summary": "Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.", "AI": {"tldr": "\u63d0\u51faSpherical Steering\u65b9\u6cd5\uff0c\u901a\u8fc7\u6fc0\u6d3b\u65cb\u8f6c\u800c\u975e\u52a0\u6cd5\u5b9e\u73b0\u63a8\u7406\u65f6\u63a7\u5236\uff0c\u4fdd\u6301\u8868\u793a\u8303\u6570\u4e0d\u53d8\uff0c\u907f\u514d\u8868\u793a\u5d29\u6e83\u5e76\u4fdd\u6301\u5f00\u653e\u751f\u6210\u80fd\u529b", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u6fc0\u6d3b\u52a0\u6cd5\uff0c\u8fd9\u4f1a\u6539\u53d8\u9690\u85cf\u8868\u793a\u7684\u5e45\u5ea6\uff0c\u53ef\u80fd\u5bfc\u81f4\u8868\u793a\u5d29\u6e83\u548c\u5f00\u653e\u751f\u6210\u80fd\u529b\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u4fdd\u6301\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faSpherical Steering\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u6fc0\u6d3b\u65cb\u8f6c\u800c\u975e\u56fa\u5b9a\u5411\u91cf\u52a0\u6cd5\uff0c\u6cbf\u6d4b\u5730\u7ebf\u65cb\u8f6c\u6fc0\u6d3b\u5411\u91cf\u671d\u5411\u76ee\u6807\u65b9\u5411\uff1b2\uff09\u5f15\u5165\u7f6e\u4fe1\u95e8\u673a\u5236\uff0c\u6839\u636e\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u8282\u63a7\u5236\u5f3a\u5ea6\uff1b3\uff09\u4fdd\u6301\u8bad\u7ec3\u514d\u8d39\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u591a\u9009\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u52a0\u6cd5\u7684\u65b9\u6cd5\uff08\u5728TruthfulQA\u3001COPA\u548cStorycloze\u4e0a\u63d0\u5347+10%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u5f00\u653e\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u4fdd\u6301\u8303\u6570\u7684\u65cb\u8f6c\u64cd\u4f5c\u662f\u7cbe\u786e\u63a8\u7406\u65f6\u63a7\u5236\u7684\u9c81\u68d2\u6709\u6548\u539f\u8bed\uff0c\u51e0\u4f55\u4e00\u81f4\u6027\u5bf9\u4e8e\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.08491", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08491", "abs": "https://arxiv.org/abs/2602.08491", "authors": ["Keonvin Park", "Aditya Pal", "Jin Hong Mok"], "title": "Enhanced Food Category Recognition under Illumination-Induced Domain Shift", "comment": null, "summary": "Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.\n  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.\n  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5149\u7167\u53d8\u5316\u5bf9\u591a\u7c7b\u522b\u98df\u54c1\u8bc6\u522b\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5408\u6210\u5149\u7167\u589e\u5f3a\u6570\u636e\u96c6\u548c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u63d0\u5347\u5149\u7167\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u98df\u54c1\u8bc6\u522b\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u4f20\u9001\u5e26\u68c0\u6d4b\uff09\u5bf9\u5149\u7167\u53d8\u5316\u5f15\u8d77\u7684\u9886\u57df\u504f\u79fb\u9ad8\u5ea6\u654f\u611f\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u4e00\u98df\u54c1\u7c7b\u522b\u6216\u53d7\u63a7\u73af\u5883\uff0c\u4e14\u5927\u591a\u6570\u516c\u5171\u98df\u54c1\u6570\u636e\u96c6\u7f3a\u4e4f\u660e\u786e\u7684\u5149\u7167\u6807\u6ce8\uff0c\u9700\u8981\u7814\u7a76\u5149\u7167\u9c81\u68d2\u6027\u4ee5\u63d0\u5347\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528Food-101\u548cFruits-360\u4e24\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u53d8\u5316\u5149\u6e29\u548c\u5f3a\u5ea6\u6784\u5efa\u5408\u6210\u5149\u7167\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u548c\u9886\u57df\u6cdb\u5316\u5206\u6790\uff0c\u7279\u522b\u5173\u6ce8\u82f9\u679c\u7c7b\u7b49\u5149\u7167\u654f\u611f\u76ee\u6807\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5149\u7167\u53d8\u5316\u5bfc\u81f4\u663e\u8457\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u800c\u5149\u7167\u611f\u77e5\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u8bc6\u522b\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5149\u7167\u9c81\u68d2\u6027\u5728\u98df\u54c1\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5728\u73b0\u5b9e\u4e16\u754c\u68c0\u6d4b\u573a\u666f\u4e2d\u90e8\u7f72\u53ef\u9760\u7684\u98df\u54c1\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5149\u7167\u611f\u77e5\u589e\u5f3a\u662f\u63d0\u5347\u7cfb\u7edf\u7a33\u5065\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.08171", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08171", "abs": "https://arxiv.org/abs/2602.08171", "authors": ["Cristian Minoccheri", "Sophia Tesic", "Kayvan Najarian", "Ryan Stidham"], "title": "A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis", "comment": null, "summary": "Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u56e0\u679c\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u533a\u5206\u6cbb\u7597\u6548\u679c\u5f02\u8d28\u6027\u7684\u7edf\u8ba1\u68c0\u6d4b\u4e0e\u4e2a\u6027\u5316\u6cbb\u7597\u51b3\u7b56\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u5e76\u5728\u6e83\u75a1\u6027\u7ed3\u80a0\u708e\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u5e94\u7528\u9a8c\u8bc1\u3002", "motivation": "\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\u901a\u5e38\u4f30\u8ba1\u5e73\u5747\u6cbb\u7597\u6548\u679c\uff0c\u4f46\u6cbb\u7597\u53cd\u5e94\u5b58\u5728\u5f02\u8d28\u6027\u3002\u7136\u800c\uff0c\u7edf\u8ba1\u4e0a\u53ef\u68c0\u6d4b\u7684\u5f02\u8d28\u6027\u5e76\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u80fd\u591f\u6539\u5584\u6cbb\u7597\u51b3\u7b56\uff0c\u8fd9\u4e24\u4e2a\u95ee\u9898\u53ef\u80fd\u4ea7\u751f\u77db\u76fe\u7684\u7ed3\u679c\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u533a\u5206\u548c\u8bc4\u4f30\u8fd9\u4e24\u4e2a\u4e0d\u540c\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u56e0\u679c\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff1a1) \u7f6e\u6362\u91cd\u8981\u6027\u8bc6\u522b\u9884\u6d4b\u5f02\u8d28\u6027\u7684\u7279\u5f81\uff1b2) \u6700\u4f73\u7ebf\u6027\u9884\u6d4b\u5668(BLP)\u6d4b\u8bd5\u8bc4\u4f30\u7edf\u8ba1\u663e\u8457\u6027\uff1b3) \u53cc\u91cd\u7a33\u5065\u7b56\u7565\u8bc4\u4f30\u8861\u91cf\u5229\u7528\u5f02\u8d28\u6027\u662f\u5426\u80fd\u6539\u5584\u60a3\u8005\u7ed3\u5c40\u3002\u5e94\u7528\u4e8eUNIFI\u7ef4\u6301\u8bd5\u9a8c\u6570\u636e\uff0c\u6bd4\u8f83\u5b89\u6170\u5242\u3001\u6807\u51c6\u5242\u91cf\u4e4c\u53f8\u5974\u5355\u6297(\u6bcf12\u5468)\u548c\u5242\u91cf\u5f3a\u5316\u4e4c\u53f8\u5974\u5355\u6297(\u6bcf8\u5468)\uff0c\u4f7f\u7528\u4ea4\u53c9\u62df\u5408X-learner\u6a21\u578b\uff0c\u7eb3\u5165\u57fa\u7ebf\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u7528\u836f\u53f2\u3001\u7b2c8\u5468\u4e34\u5e8a\u8bc4\u5206\u3001\u5b9e\u9a8c\u5ba4\u751f\u7269\u6807\u5fd7\u7269\u548c\u89c6\u9891\u5185\u955c\u7279\u5f81\u3002", "result": "BLP\u6d4b\u8bd5\u53d1\u73b0\u5185\u955c\u7279\u5f81\u4e0e\u4e4c\u53f8\u5974\u5355\u6297vs\u5b89\u6170\u5242\u7684\u6cbb\u7597\u6548\u679c\u5f02\u8d28\u6027\u6709\u5f3a\u5173\u8054\uff0c\u4f46\u53cc\u91cd\u7a33\u5065\u7b56\u7565\u8bc4\u4f30\u663e\u793a\u7eb3\u5165\u5185\u955c\u7279\u5f81\u672a\u80fd\u6539\u5584\u9884\u671f\u7f13\u89e3\u7387\uff0c\u4e14\u4ea4\u53c9\u9a8c\u8bc1\u7684\u591a\u81c2\u8bc4\u4f30\u8868\u73b0\u66f4\u5dee\u3002\u8bca\u65ad\u6bd4\u8f83\u663e\u793a\u5185\u955c\u8bc4\u5206\u4f5c\u4e3a\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u6807\u5fd7\u7269\uff0c\u80fd\u6539\u5584\u672a\u6cbb\u7597\u60a3\u8005\u7684\u7ed3\u5c40\u9884\u6d4b\uff0c\u4f46\u4e3a\u6cbb\u7597\u9009\u62e9\u589e\u52a0\u4e86\u566a\u58f0\uff1b\u800c\u4e34\u5e8a\u53d8\u91cf(\u7caa\u4fbf\u9499\u536b\u86cb\u767d\u3001\u5e74\u9f84\u3001CRP)\u6355\u6349\u4e86\u51b3\u7b56\u76f8\u5173\u7684\u53d8\u5f02\u3002", "conclusion": "\u56e0\u679c\u673a\u5668\u5b66\u4e60\u5728\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u7684\u5e94\u7528\u5e94\u540c\u65f6\u5305\u542b\u7b56\u7565\u5c42\u9762\u7684\u8bc4\u4f30\u548c\u5f02\u8d28\u6027\u6d4b\u8bd5\u3002\u7edf\u8ba1\u4e0a\u663e\u8457\u7684\u5f02\u8d28\u6027\u7279\u5f81\u4e0d\u4e00\u5b9a\u80fd\u6539\u5584\u6cbb\u7597\u51b3\u7b56\uff0c\u9700\u8981\u533a\u5206\u9884\u540e\u8d21\u732e\u548c\u7b56\u7565\u4ef7\u503c\u3002\u5185\u955c\u7279\u5f81\u867d\u7136\u4e0e\u6cbb\u7597\u6548\u679c\u5f02\u8d28\u6027\u76f8\u5173\uff0c\u4f46\u672a\u80fd\u6539\u5584\u4e2a\u6027\u5316\u6cbb\u7597\u51b3\u7b56\uff0c\u800c\u4e34\u5e8a\u53d8\u91cf\u66f4\u5177\u51b3\u7b56\u76f8\u5173\u6027\u3002"}}
{"id": "2602.08503", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08503", "abs": "https://arxiv.org/abs/2602.08503", "authors": ["Yi Ding", "Ziliang Qiu", "Bolian Li", "Ruqi Zhang"], "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation", "comment": "17 pages", "summary": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\\times$ training time per step.", "AI": {"tldr": "Octopus\u6846\u67b6\u901a\u8fc7\u91cd\u7ec4\u73b0\u6709rollouts\u5408\u6210\u5bc6\u96c6\u7684\u81ea\u6821\u6b63\u793a\u4f8b\uff0c\u89e3\u51b3\u4e86VLMs\u4e2d\u81ea\u6821\u6b63\u5b66\u4e60\u4fe1\u53f7\u7a00\u758f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b66\u4e60\u81ea\u6821\u6b63\u884c\u4e3a\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u6709\u6548\u7684\u81ea\u6821\u6b63\u884c\u4e3a\u5f88\u5c11\u51fa\u73b0\uff0c\u5bfc\u81f4\u5b66\u4e60\u4fe1\u53f7\u6781\u5176\u7a00\u758f\u3002", "method": "\u63d0\u51fa\u4e86correction-specific rollouts\uff08Octopus\uff09\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u91cd\u7ec4\u73b0\u6709rollouts\u5408\u6210\u5bc6\u96c6\u7684\u81ea\u6821\u6b63\u793a\u4f8b\uff1b2\uff09\u5f15\u5165\u54cd\u5e94\u63a9\u7801\u7b56\u7565\uff0c\u5c06\u81ea\u6821\u6b63\u4e0e\u76f4\u63a5\u63a8\u7406\u89e3\u8026\uff1b3\uff09\u6784\u5efa\u4e86\u5177\u6709\u53ef\u63a7\u81ea\u6821\u6b63\u80fd\u529b\u7684Octopus-8B\u63a8\u7406VLM\u3002", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOctopus-8B\u5728\u5f00\u6e90VLMs\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u6700\u4f73RLVR\u57fa\u7ebf\u9ad8\u51fa1.0\u5206\uff0c\u540c\u65f6\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u4ec5\u97000.72\u500d\u7684\u65f6\u95f4\u3002", "conclusion": "Octopus\u6846\u67b6\u901a\u8fc7rollout\u91cd\u7ec4\u548c\u54cd\u5e94\u63a9\u7801\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLMs\u4e2d\u81ea\u6821\u6b63\u5b66\u4e60\u7684\u7a00\u758f\u4fe1\u53f7\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.08194", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08194", "abs": "https://arxiv.org/abs/2602.08194", "authors": ["Konstantinos Mitsides", "Maxence Faldor", "Antoine Cully"], "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds", "comment": "11 pages (main text), 90 pages total. Project page: https://konstantinosmitsides.github.io/dreaming-in-code", "summary": "Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.", "AI": {"tldr": "DiCode\u6846\u67b6\u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u53ef\u6267\u884c\u73af\u5883\u4ee3\u7801\uff0c\u4e3a\u667a\u80fd\u4f53\u5728\u5f00\u653e\u4e16\u754c\u5b66\u4e60\u4e2d\u642d\u5efa\u6e10\u8fdb\u5f0f\u5b66\u4e60\u8def\u5f84\uff0c\u5728Craftax\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u5b66\u4e60\u7cfb\u7edf\u867d\u7136\u80fd\u751f\u6210\u591a\u6837\u73af\u5883\uff0c\u4f46\u5f80\u5f80\u5173\u6ce8\u5b64\u7acb\u884c\u4e3a\u53d1\u73b0\u800c\u975e\u6301\u7eed\u8fdb\u5c55\u7f16\u6392\u3002\u590d\u6742\u5f00\u653e\u4e16\u754c\u4e2d\u5de8\u5927\u7684\u7ec4\u5408\u7a7a\u95f4\u4f7f\u5f97\u667a\u80fd\u4f53\u96be\u4ee5\u53d1\u73b0\u59cb\u7ec8\u4fdd\u6301\u53ef\u5b66\u4e60\u6027\u7684\u7ecf\u9a8c\u5e8f\u5217\u3002", "method": "\u63d0\u51faDreaming in Code\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u5408\u6210\u53ef\u6267\u884c\u73af\u5883\u4ee3\u7801\u6765\u642d\u5efa\u5b66\u4e60\u811a\u624b\u67b6\u3002\"\u68a6\u60f3\"\u8868\u73b0\u4e3a\u751f\u6210\u4ee3\u7801\u5c42\u9762\u7684\u4e16\u754c\u53d8\u4f53\uff0c\u5728Craftax\u57fa\u51c6\u4e0a\u5b9e\u73b0\u5177\u4f53\u5b9e\u4f8b\u5316\u3002", "result": "DiCode\u4f7f\u667a\u80fd\u4f53\u83b7\u5f97\u957f\u65f6\u7a0b\u6280\u80fd\uff0c\u5e73\u5747\u56de\u62a5\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u534716%\uff0c\u5728\u540e\u671f\u6218\u6597\u4efb\u52a1\u4e0a\u53d6\u5f97\u975e\u96f6\u6210\u529f\u7387\uff08\u5148\u524d\u65b9\u6cd5\u5b8c\u5168\u5931\u8d25\uff09\u3002", "conclusion": "\u4ee3\u7801\u7ea7\u73af\u5883\u8bbe\u8ba1\u4e3a\u8bfe\u7a0b\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u673a\u5236\uff0c\u80fd\u591f\u6784\u5efa\u4e2d\u95f4\u73af\u5883\u6765\u5f25\u5408\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u80fd\u529b\u5dee\u8ddd\u3002"}}
{"id": "2602.08524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08524", "abs": "https://arxiv.org/abs/2602.08524", "authors": ["Linger Deng", "Yuliang Liu", "Wenwen Yu", "Zujia Zhang", "Jianzhong Ju", "Zhenbo Luo", "Xiang Bai"], "title": "GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving", "comment": null, "summary": "Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus", "AI": {"tldr": "GeoFocus\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\u7684\u591a\u6a21\u6001\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u5c40\u90e8\u611f\u77e5\u5668\u548cVertexLang\u62d3\u6251\u8bed\u8a00\uff0c\u5728\u591a\u4e2a\u51e0\u4f55\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e864.7%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u5bf9\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u540c\u65f6\u5173\u6ce8\u5168\u5c40\u5f62\u72b6\u8bc6\u522b\u548c\u57fa\u4e8e\u51e0\u4f55\u7406\u8bba\u7684\u5c40\u90e8\u5173\u7cfb\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5173\u952e\u5c40\u90e8\u7279\u5f81\u8986\u76d6\u548c\u62d3\u6251\u8868\u793a\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGeoFocus\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u5173\u952e\u5c40\u90e8\u611f\u77e5\u5668\uff0c\u901a\u8fc713\u4e2a\u57fa\u4e8e\u7406\u8bba\u7684\u611f\u77e5\u6a21\u677f\u81ea\u52a8\u8bc6\u522b\u548c\u5f3a\u8c03\u5173\u952e\u5c40\u90e8\u7ed3\u6784\uff1b2) VertexLang\uff0c\u4e00\u79cd\u7d27\u51d1\u7684\u62d3\u6251\u5f62\u5f0f\u8bed\u8a00\uff0c\u901a\u8fc7\u9876\u70b9\u5750\u6807\u548c\u8fde\u63a5\u5173\u7cfb\u7f16\u7801\u5168\u5c40\u56fe\u5f62\u3002", "result": "\u5728Geo3K\u3001GeoQA\u548cFormalGeo7K\u6570\u636e\u96c6\u4e0a\uff0cGeoFocus\u6bd4\u9886\u5148\u7684\u4e13\u4e1a\u6a21\u578b\u51c6\u786e\u7387\u63d0\u53474.7%\uff1b\u5173\u952e\u5c40\u90e8\u7279\u5f81\u8986\u76d6\u63d0\u534761%\uff1b\u5168\u5c40\u611f\u77e5\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1120%\uff1b\u5728MATHVERSE\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u89c6\u89c9\u6761\u4ef6\u9c81\u68d2\u6027\u3002", "conclusion": "GeoFocus\u901a\u8fc7\u7ed3\u5408\u7406\u8bba\u9a71\u52a8\u7684\u5c40\u90e8\u611f\u77e5\u548c\u9ad8\u6548\u7684\u62d3\u6251\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u51e0\u4f55\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.08528", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.08528", "abs": "https://arxiv.org/abs/2602.08528", "authors": ["Chuyang Wu", "Samuli Siltanen"], "title": "Automatic regularization parameter choice for tomography using a double model approach", "comment": null, "summary": "Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u7f51\u683c\u79bb\u6563\u5316\u7684\u81ea\u52a8\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u9988\u63a7\u5236\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u4f7f\u8fed\u4ee3\u91cd\u5efa\u8fbe\u5230\u4e24\u4e2a\u7f51\u683c\u4e0a\u91cd\u5efa\u7ed3\u679c\u8db3\u591f\u76f8\u4f3c\u7684\u6700\u5c0f\u53c2\u6570\u503c\u3002", "motivation": "X\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u4e2d\u7684\u56fe\u50cf\u91cd\u5efa\u662f\u4e00\u4e2a\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u6b63\u5219\u5316\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u6b63\u5219\u5316\u53c2\u6570\u7684\u9009\u62e9\uff0c\u8be5\u53c2\u6570\u9700\u8981\u5728\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u5148\u9a8c\u4fe1\u606f\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e24\u4e2a\u4e0d\u540c\u8ba1\u7b97\u79bb\u6563\u5316\u7684\u81ea\u52a8\u53c2\u6570\u9009\u62e9\u65b9\u6cd5\u3002\u4f7f\u7528\u53cd\u9988\u63a7\u5236\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u9a71\u52a8\u8fed\u4ee3\u91cd\u5efa\u671d\u7740\u4f7f\u4e24\u4e2a\u7f51\u683c\u4e0a\u7684\u91cd\u5efa\u7ed3\u679c\u8fbe\u5230\u8db3\u591f\u76f8\u4f3c\u7684\u6700\u5c0f\u53c2\u6570\u503c\u53d1\u5c55\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u65ad\u5c42\u626b\u63cf\u6570\u636e\u4e0a\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u7f51\u683c\u79bb\u6563\u5316\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u9009\u62e9\u6b63\u5219\u5316\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u53c2\u6570\u9009\u62e9\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2602.08210", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08210", "abs": "https://arxiv.org/abs/2602.08210", "authors": ["Hyungseok Song", "Deunsol Yoon", "Kanghoon Lee", "Han-Seul Jeong", "Soonyoung Lee", "Woohyung Lim"], "title": "CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization", "comment": null, "summary": "Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faCADO\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u89e3\u51b3\u70ed\u56fe\u6c42\u89e3\u5668\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u76f4\u63a5\u4f18\u5316\u89e3\u7801\u540e\u89e3\u7684\u6210\u672c\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u70ed\u56fe\u7684\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u5668\u4e3b\u8981\u91c7\u7528\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b58\u5728\u6839\u672c\u6027\u7684\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u6700\u5c0f\u5316\u6a21\u4eff\u635f\u5931\uff08\u5982\u4ea4\u53c9\u71b5\uff09\u4e0d\u80fd\u4fdd\u8bc1\u89e3\u6210\u672c\u7684\u4f18\u5316\u3002\u8fd9\u79cd\u4e0d\u5339\u914d\u8868\u73b0\u4e3a\u89e3\u7801\u5668\u76f2\u89c6\uff08\u5ffd\u7565\u4e0d\u53ef\u5fae\u7684\u89e3\u7801\u8fc7\u7a0b\uff09\u548c\u6210\u672c\u76f2\u89c6\uff08\u4f18\u5148\u7ed3\u6784\u6a21\u4eff\u800c\u975e\u89e3\u8d28\u91cf\uff09\uff0c\u5bfc\u81f4\u6027\u80fd\u5b58\u5728\u786c\u6027\u4e0a\u9650\u3002", "method": "\u63d0\u51faCADO\uff08\u7528\u4e8e\u4f18\u5316\u7684\u6210\u672c\u611f\u77e5\u6269\u6563\u6a21\u578b\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u76f4\u63a5\u4f18\u5316\u89e3\u7801\u540e\u89e3\u7684\u6210\u672c\uff1b2\uff09\u5f15\u5165\u6807\u7b7e\u4e2d\u5fc3\u5956\u52b1\u673a\u5236\uff0c\u5c06\u771f\u5b9e\u6807\u7b7e\u91cd\u65b0\u7528\u4f5c\u65e0\u504f\u57fa\u7ebf\u800c\u975e\u6a21\u4eff\u76ee\u6807\uff1b3\uff09\u91c7\u7528\u6df7\u5408\u5fae\u8c03\u65b9\u6cd5\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u9002\u914d\u3002", "result": "CADO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u76ee\u6807\u5bf9\u9f50\u5bf9\u4e8e\u91ca\u653e\u57fa\u4e8e\u70ed\u56fe\u7684\u6c42\u89e3\u5668\u5168\u90e8\u6f5c\u529b\u7684\u91cd\u8981\u6027\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u56fa\u6709\u7684\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u91cd\u65b0\u6784\u5efa\u4e3aMDP\u5e76\u76f4\u63a5\u4f18\u5316\u89e3\u6210\u672c\uff0cCADO\u89e3\u51b3\u4e86\u70ed\u56fe\u6c42\u89e3\u5668\u4e2d\u7684\u76ee\u6807\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u7ec4\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u5728\u63d0\u5347\u57fa\u4e8e\u70ed\u56fe\u7684\u7ec4\u5408\u4f18\u5316\u6c42\u89e3\u5668\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.08531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08531", "abs": "https://arxiv.org/abs/2602.08531", "authors": ["Anastasiia Kornilova", "Ivan Moskalenko", "Arabella Gromova", "Gonzalo Ferrer", "Alexander Menshchikov"], "title": "Thegra: Graph-based SLAM for Thermal Imagery", "comment": null, "summary": "Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7a00\u758f\u5355\u76ee\u56fe\u7684\u70ed\u6210\u50cfSLAM\u7cfb\u7edf\uff0c\u5229\u7528\u53ef\u89c1\u5149\u6570\u636e\u8bad\u7ec3\u7684\u901a\u7528\u5b66\u4e60\u7279\u5f81\uff08SuperPoint\u68c0\u6d4b\u5668\u548cLightGlue\u5339\u914d\u5668\uff09\uff0c\u901a\u8fc7\u9884\u5904\u7406\u589e\u5f3a\u70ed\u56fe\u50cf\u9002\u5e94\u6027\uff0c\u6539\u8fdbSLAM\u6a21\u5757\u5904\u7406\u7a00\u758f\u548c\u5f02\u5e38\u5339\u914d\uff0c\u5e76\u5f15\u5165\u7f6e\u4fe1\u5ea6\u52a0\u6743\u56e0\u5b50\u56fe\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u70ed\u6210\u50cf\u5728\u89c6\u89c9\u9000\u5316\u73af\u5883\uff08\u5982\u4f4e\u5149\u7167\u3001\u70df\u96fe\u3001\u6076\u52a3\u5929\u6c14\uff09\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u4f46\u70ed\u56fe\u50cf\u901a\u5e38\u5b58\u5728\u7eb9\u7406\u5c11\u3001\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u566a\u58f0\u9ad8\u7b49\u95ee\u9898\uff0c\u4f7f\u5f97\u57fa\u4e8e\u7279\u5f81\u7684SLAM\u53d8\u5f97\u590d\u6742\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u70ed\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u800c\u9ad8\u8d28\u91cf\u70ed\u6570\u636e\u7a00\u7f3a\u3002", "method": "1. \u4f7f\u7528\u5728\u53ef\u89c1\u5149\u8c31\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\u7684SuperPoint\u68c0\u6d4b\u5668\u548cLightGlue\u5339\u914d\u5668\uff0c\u5b9e\u73b0\u8de8\u57df\u6cdb\u5316\uff1b2. \u5f15\u5165\u9884\u5904\u7406\u6d41\u7a0b\u589e\u5f3a\u70ed\u56fe\u50cf\u8f93\u5165\u9002\u5e94\u6027\uff1b3. \u4fee\u6539\u6838\u5fc3SLAM\u6a21\u5757\u4ee5\u5904\u7406\u7a00\u758f\u548c\u5f02\u5e38\u7279\u5f81\u5339\u914d\uff1b4. \u5c06SuperPoint\u7684\u5173\u952e\u70b9\u7f6e\u4fe1\u5ea6\u5206\u6570\u6574\u5408\u5230\u7f6e\u4fe1\u5ea6\u52a0\u6743\u56e0\u5b50\u56fe\u4e2d\u3002", "result": "\u5728\u516c\u5f00\u70ed\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u7cfb\u7edf\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6027\u80fd\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u6216\u5fae\u8c03\u7279\u5f81\u68c0\u6d4b\u5668\uff0c\u89e3\u51b3\u4e86\u70ed\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u70ed\u6210\u50cfSLAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u53ef\u89c1\u5149\u8bad\u7ec3\u7684\u901a\u7528\u5b66\u4e60\u7279\u5f81\u548c\u9002\u5e94\u6027\u6539\u8fdb\uff0c\u5728\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff0c\u4ee3\u7801\u5c06\u5728\u53d1\u8868\u540e\u516c\u5f00\u3002"}}
{"id": "2602.08213", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2602.08213", "abs": "https://arxiv.org/abs/2602.08213", "authors": ["Haoran Liu", "Zheni Zeng", "Yukun Yan", "Yuxuan Chen", "Yunduo Xiao"], "title": "DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning", "comment": null, "summary": "Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.", "AI": {"tldr": "DrugR\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u836f\u7269\u5206\u5b50\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u660e\u786e\u7684\u836f\u7406\u5b66\u63a8\u7406\u6b65\u9aa4\uff0c\u7ed3\u5408\u9886\u57df\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u591a\u7c92\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u5206\u5b50\u6838\u5fc3\u529f\u6548\u7684\u540c\u65f6\u6539\u5584ADMET\u6027\u8d28\u3002", "motivation": "\u5206\u5b50\u751f\u6210\u548c\u4f18\u5316\u662f\u5316\u5b66\u9886\u57df\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u6b64\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4f46\u9762\u4e34\u5206\u5b50\u7ed3\u6784\u4e0e\u836f\u7406\u6027\u8d28\u95f4\u590d\u6742\u9690\u5f0f\u5173\u7cfb\u4ee5\u53ca\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faDrugR\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9886\u57df\u7279\u5b9a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u901a\u8fc7\u53cd\u5411\u6570\u636e\u5de5\u7a0b\u7684\u76d1\u7763\u5fae\u8c03\u3001\u4ee5\u53ca\u81ea\u5e73\u8861\u7684\u591a\u7c92\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5c06\u660e\u786e\u7684\u9010\u6b65\u836f\u7406\u5b66\u63a8\u7406\u5f15\u5165\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDrugR\u80fd\u591f\u5728\u591a\u4e2a\u6027\u8d28\u4e0a\u5b9e\u73b0\u5168\u9762\u589e\u5f3a\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u7ed3\u6784\u76f8\u4f3c\u6027\u6216\u9776\u6807\u7ed3\u5408\u4eb2\u548c\u529b\uff0c\u5176\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\u4e3a\u6bcf\u4e2a\u4f18\u5316\u6b65\u9aa4\u63d0\u4f9b\u4e86\u6e05\u6670\u53ef\u89e3\u91ca\u7684\u4f9d\u636e\u3002", "conclusion": "DrugR\u901a\u8fc7\u660e\u786e\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u89c1\u89e3\uff0c\u63a8\u52a8\u81ea\u52a8\u5316\u3001\u77e5\u8bc6\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u68c0\u67e5\u70b9\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2602.08540", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.08540", "abs": "https://arxiv.org/abs/2602.08540", "authors": ["He Wu", "Xia Yan", "Yanghui Xu", "Liegang Xia", "Jiazhou Chen"], "title": "TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation", "comment": "13 pages, 6 figures, 4 tables", "summary": "Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.", "AI": {"tldr": "\u63d0\u51faTIBR4D\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fed\u4ee3\u8fb9\u754c\u7cbe\u70bc\u5b9e\u73b0\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u9ad8\u6548\u65e0\u5b66\u4e60\u5bf9\u8c61\u5206\u5272\uff0c\u63d0\u5347\u8fb9\u754c\u6e05\u6670\u5ea6\u548c\u5904\u7406\u906e\u6321\u80fd\u529b\u3002", "motivation": "\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u7ea7\u5206\u5272\u9762\u4e34\u590d\u6742\u8fd0\u52a8\u3001\u906e\u6321\u548c\u6a21\u7cca\u8fb9\u754c\u7684\u6311\u6218\uff0c\u73b0\u6709\u57fa\u4e8e\u9608\u503c\u7684\u4e00\u6b21\u6027\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faTIBR4D\u6846\u67b6\uff1a1) \u8fed\u4ee3\u9ad8\u65af\u5b9e\u4f8b\u8ffd\u8e2a(IGIT)\u5728\u65f6\u95f4\u7247\u6bb5\u7ea7\u9010\u6b65\u7cbe\u70bc\u9ad8\u65af\u5230\u5b9e\u4f8b\u7684\u6982\u7387\uff1b2) \u5e27\u7ea7\u9ad8\u65af\u6e32\u67d3\u8303\u56f4\u63a7\u5236(RCC)\u6291\u5236\u8fb9\u754c\u9644\u8fd1\u4e0d\u786e\u5b9a\u9ad8\u65af\uff1b3) \u65f6\u95f4\u5206\u5272\u5408\u5e76\u7b56\u7565\u5e73\u8861\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u52a8\u6001\u611f\u77e5\u3002", "result": "\u5728HyperNeRF\u548cNeu3D\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5\uff0c\u672c\u65b9\u6cd5\u80fd\u751f\u6210\u8fb9\u754c\u66f4\u6e05\u6670\u3001\u66f4\u51c6\u786e\u7684\u5bf9\u8c61\u9ad8\u65af\u70b9\u4e91\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "TIBR4D\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8fed\u4ee3\u8fb9\u754c\u7cbe\u70bc\u548c\u81ea\u9002\u5e94\u65f6\u95f4\u5206\u5272\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u60014D\u9ad8\u65af\u573a\u666f\u4e2d\u7684\u5bf9\u8c61\u5206\u5272\u95ee\u9898\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2602.08215", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08215", "abs": "https://arxiv.org/abs/2602.08215", "authors": ["Yash Patel", "Ambuj Tewari"], "title": "Distribution-Free Robust Functional Predict-Then-Optimize", "comment": null, "summary": "The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fdd\u5f62\u9884\u6d4b\u7684\u795e\u7ecf\u7b97\u5b50\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8ePDE\u6c42\u89e3\u4e2d\u7684\u51b3\u7b56\u4efb\u52a1\uff0c\u65e0\u9700\u5206\u5e03\u5047\u8bbe\u4e14\u53ef\u6269\u5c55", "motivation": "\u795e\u7ecf\u7b97\u5b50\u66ff\u4ee3\u6a21\u578b\u5728PDE\u6c42\u89e3\u51b3\u7b56\u4efb\u52a1\u4e2d\u5e94\u7528\u589e\u591a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u96c6\u6210\u6216\u8d1d\u53f6\u65af\u540e\u9a8c\u4f30\u8ba1\u8981\u4e48\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u5206\u5e03\u5047\u8bbe\uff0c\u8981\u4e48\u7f3a\u4e4f\u5b9e\u9645\u53ef\u6269\u5c55\u6027", "method": "\u5c06\u4fdd\u5f62\u9884\u6d4b\u5e94\u7528\u4e8e\u795e\u7ecf\u7b97\u5b50\u6620\u5c04\u7684\u51fd\u6570\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5206\u5e03\u81ea\u7531\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff1b\u5229\u7528Danskin\u5b9a\u7406\u7684\u65e0\u9650\u7ef4\u63a8\u5e7f\u548c\u53d8\u5206\u6cd5\u89e3\u51b3\u4e0b\u6e38\u9c81\u68d2\u51b3\u7b56\u4efb\u52a1", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u795e\u7ecf\u7b97\u5b50\u9884\u6d4b\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u533a\u57df\uff0c\u5728\u591a\u4e2a\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u9650\u5236\u6027\u5efa\u6a21\u8303\u5f0f\uff08\u5982\u9ad8\u65af\u8fc7\u7a0b\uff09", "conclusion": "\u4fdd\u5f62\u9884\u6d4b\u4e3a\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u4e0b\u6e38\u9c81\u68d2\u51b3\u7b56\u4efb\u52a1\u7684\u5f62\u5f0f\u5316\u540e\u6094\u8868\u5f81\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.08550", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.08550", "abs": "https://arxiv.org/abs/2602.08550", "authors": ["Shih-Fang Chen", "Jun-Cheng Chen", "I-Hong Jhuo", "Yen-Yu Lin"], "title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing", "comment": "ICLR 2026. This is a preprint version. The camera-ready version will be updated soon", "summary": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.", "AI": {"tldr": "GOT-Edit\uff1a\u4e00\u79cd\u5728\u7ebf\u8de8\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u611f\u77e5\u7ebf\u7d22\u96c6\u6210\u5230\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5668\u4e2d\uff0c\u7ed3\u54082D\u8bed\u4e49\u4e0e3D\u51e0\u4f55\u63a8\u7406\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd", "motivation": "\u4eba\u7c7b\u611f\u77e5\u57282D\u89c6\u9891\u6d41\u4e2d\u8fdb\u884c\u6709\u6548\u76ee\u6807\u8ddf\u8e2a\u65f6\uff0c\u9690\u5f0f\u5730\u7ed3\u5408\u4e86\u5148\u9a8c3D\u77e5\u8bc6\u548c\u8bed\u4e49\u63a8\u7406\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5927\u591a\u6570\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76ee\u6807\u76842D\u7279\u5f81\u53ca\u5176\u5468\u56f4\u73af\u5883\uff0c\u5ffd\u7565\u4e863D\u51e0\u4f55\u7ebf\u7d22\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u90e8\u5206\u906e\u6321\u3001\u5e72\u6270\u7269\u4ee5\u53ca\u51e0\u4f55\u548c\u5916\u89c2\u53d8\u5316\u7684\u5f71\u54cd\u3002", "method": "GOT-Edit\u91c7\u7528\u5728\u7ebf\u8de8\u6a21\u6001\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\uff0c\u4ece\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u51e0\u4f55\u57fa\u7840Transformer\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u4ec5\u4ece\u5c11\u91cf2D\u56fe\u50cf\u63a8\u65ad\u51e0\u4f55\u7ebf\u7d22\u3002\u901a\u8fc7\u96f6\u7a7a\u95f4\u7ea6\u675f\u66f4\u65b0\u8fdb\u884c\u5728\u7ebf\u6a21\u578b\u7f16\u8f91\uff0c\u65e0\u7f1d\u7ed3\u5408\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u5224\u522b\u80fd\u529b\u7684\u540c\u65f6\u878d\u5165\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGOT-Edit\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u548c\u6742\u4e71\u573a\u666f\u4e0b\uff0c\u4e3a\u7ed3\u54082D\u8bed\u4e49\u4e0e3D\u51e0\u4f55\u63a8\u7406\u8fdb\u884c\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "GOT-Edit\u901a\u8fc7\u5728\u7ebf\u6a21\u578b\u7f16\u8f91\u5c06\u51e0\u4f55\u611f\u77e5\u7ebf\u7d22\u96c6\u6210\u5230\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u5668\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5ffd\u75653D\u51e0\u4f55\u7ebf\u7d22\u7684\u95ee\u9898\uff0c\u5728\u906e\u6321\u548c\u6742\u4e71\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a2D\u8bed\u4e49\u4e0e3D\u51e0\u4f55\u63a8\u7406\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.08216", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08216", "abs": "https://arxiv.org/abs/2602.08216", "authors": ["Gunn Kim"], "title": "Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics", "comment": "9 pages, 1 figure. Based on a thermodynamic framework for Transformer architectures. Derives the equation of state from first principles", "summary": "Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u4fe1\u606f\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u5c06Transformer\u6ce8\u610f\u529b\u673a\u5236\u89c6\u4e3a\u53d7\u6700\u5c0f\u4f5c\u7528\u91cf\u539f\u7406\u652f\u914d\u7684\u7269\u7406\u7cfb\u7edf\uff0c\u800c\u975e\u7b97\u6cd5\u4f18\u5316\u3002\u8be5\u7406\u8bba\u5efa\u7acb\u4e86\u4fe1\u606f\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b\uff0c\u7edf\u4e00\u4e86\u63a8\u7406\uff08\u673a\u68b0\u529f\uff09\u548c\u5b66\u4e60\uff08\u5316\u5b66\u6f14\u5316\uff09\uff0c\u5e76\u89e3\u91ca\u4e86\u6d8c\u73b0\u73b0\u8c61\u5982\u7f29\u653e\u5b9a\u5f8b\u548c\u987f\u609f\u3002", "motivation": "\u5c3d\u7ba1Transformer\u67b6\u6784\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u4eba\u5de5\u667a\u80fd\uff0c\u4f46\u5176\u5e95\u5c42\u673a\u5236\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u7136\u662f\u542f\u53d1\u5f0f\u7684\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7269\u7406\u7406\u8bba\u3002\u4f5c\u8005\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u6846\u67b6\uff0c\u5c06\u4fe1\u606f\u52a8\u529b\u5b66\u89c6\u4e3a\u7269\u7406\u7cfb\u7edf\uff0c\u4ece\u800c\u4e3a\u57fa\u4e8e\u7269\u7406\u7684\u667a\u80fd\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5c06\u4fe1\u606f\u72b6\u6001\u6620\u5c04\u5230\u5177\u6709Fisher\u4fe1\u606f\u5ea6\u91cf\u7684\u9ece\u66fc\u6d41\u5f62\uff0c\u63a8\u5bfc\u51fa\u667a\u80fd\u62c9\u683c\u6717\u65e5\u91cf\u3002\u5c06softmax\u51fd\u6570\u5bf9\u5e94\u4e8e\u6700\u5c0f\u5316\u4fe1\u606f\u6c14\u4f53\u4ea5\u59c6\u970d\u5179\u81ea\u7531\u80fd\u7684\u552f\u4e00\u70ed\u529b\u5b66\u5e73\u8861\u6001\uff0c\u5c06\u67e5\u8be2-\u952e\u4ea4\u4e92\u8bc6\u522b\u4e3a\u5916\u90e8\u573a\u4e0e\u56fa\u6709\u5076\u6781\u77e9\u4e4b\u95f4\u7684\u7535\u52a8\u529b\u5b66\u8026\u5408\u3002", "result": "\u5efa\u7acb\u4e86\u4fe1\u606f\u70ed\u529b\u5b66\u7b2c\u4e00\u5b9a\u5f8b\uff0c\u7edf\u4e00\u4e86\u63a8\u7406\uff08\u673a\u68b0\u529f\uff09\u548c\u5b66\u4e60\uff08\u5316\u5b66\u6f14\u5316\uff09\u3002\u89e3\u91ca\u4e86\u6d8c\u73b0\u73b0\u8c61\u5982\u7f29\u653e\u5b9a\u5f8b\u548c\u987f\u609f\u4f5c\u4e3a\u4ee5\u6bd4\u70ed\u53d1\u6563\u4e3a\u7279\u5f81\u7684\u76f8\u53d8\u3002\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u6d41\u5f62\u4e2d\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\u7834\u7f3a\u4ea7\u751f\u65e0\u8d28\u91cf\u7684Goldstone\u73bb\u8272\u5b50\uff0c\u4e3a\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u63d0\u4f9b\u4e86\u573a\u8bba\u89c6\u89d2\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8fde\u63a5\u4e86\u7edf\u8ba1\u7269\u7406\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u57fa\u4e8e\u7269\u7406\u7684\u667a\u80fd\u7684\u4e00\u822c\u7406\u8bba\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5bf9Transformer\u67b6\u6784\u5e95\u5c42\u7269\u7406\u673a\u5236\u7684\u7edf\u4e00\u7406\u89e3\u6846\u67b6\u3002"}}
{"id": "2602.08218", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08218", "abs": "https://arxiv.org/abs/2602.08218", "authors": ["Huan Zhang", "Yanjian Zhang", "Guillaume Wisniewski", "Nadi Tomeh", "Bang Liu"], "title": "Sparsity-Aware Evolution for Model Merging", "comment": null, "summary": "We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \\textit{competition} for sparsity introduces an extra local \\textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u611f\u77e5\u8fdb\u5316\u7684\u6a21\u578b\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u526a\u679d-\u878d\u5408\u5faa\u73af\u4f5c\u4e3a\u65b0\u578b\u53d8\u5f02\u7b97\u5b50\uff0c\u5728\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5165\u7a00\u758f\u6027\u7ea6\u675f\uff0c\u63d0\u9ad8\u6a21\u578b\u878d\u5408\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8003\u8651\u6a21\u578b\u6027\u80fd\u548c\u7a00\u758f\u6027\u7684\u878d\u5408\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u878d\u5408\u6548\u679c\u548c\u6a21\u578b\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u611f\u77e5\u8fdb\u5316\u6846\u67b6\uff0c\u91c7\u7528\u8fed\u4ee3\u7684\u526a\u679d-\u878d\u5408\u5faa\u73af\u4f5c\u4e3a\u53d8\u5f02\u7b97\u5b50\uff0c\u5728\u8bc4\u5206\u51fd\u6570\u4e2d\u878d\u5165\u7a00\u758f\u6027\u7ea6\u675f\uff0c\u5f15\u5bfc\u8fdb\u5316\u8fc7\u7a0b\u504f\u597d\u66f4\u7a00\u758f\u7684\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4f20\u7edf\u6027\u80fd\u6307\u6807\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21LLM\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u878d\u5408\u7684\u53ef\u9760\u6027\uff0c\u4e14\u7531\u4e8e\u5176\u7b80\u5355\u6027\u548c\u4e0e\u73b0\u6709\u65b9\u6cd5\u7684\u6b63\u4ea4\u6027\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6d41\u7a0b\u4e2d\u3002", "conclusion": "\u7a00\u758f\u611f\u77e5\u8fdb\u5316\u6846\u67b6\u4e3a\u6a21\u578b\u878d\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u6027\u7ea6\u675f\u548c\u7ade\u4e89\u673a\u5236\u6539\u5584\u4e86\u878d\u5408\u6548\u679c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.08582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08582", "abs": "https://arxiv.org/abs/2602.08582", "authors": ["Melany Yang", "Yuhang Yu", "Diwang Weng", "Jinwei Chen", "Wei Dong"], "title": "SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning", "comment": null, "summary": "Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.", "AI": {"tldr": "SemiNFT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u8272\u5f69\u6da6\u8272\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u827a\u672f\u8bad\u7ec3\u8f68\u8ff9\uff08\u4ece\u521a\u6027\u6a21\u4eff\u5230\u76f4\u89c9\u521b\u4f5c\uff09\u5b9e\u73b0\u53c2\u8003\u56fe\u50cf\u8272\u5f69\u9884\u8bbe\u7684\u667a\u80fd\u8fc1\u79fb\u3002", "motivation": "\u4f20\u7edf\u53c2\u8003\u5f0f\u8272\u5f69\u6da6\u8272\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u50cf\u7d20\u7ea7\u7edf\u8ba1\u8fdb\u884c\u5168\u5c40\u8272\u5f69\u6620\u5c04\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u4e49\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u7f8e\u5b66\u7684\u771f\u6b63\u7406\u89e3\uff0c\u65e0\u6cd5\u5b9e\u73b0\u667a\u80fd\u5316\u7684\u8272\u5f69\u8c03\u6574\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u4f7f\u7528\u914d\u5bf9\u4e09\u5143\u7ec4\u5b66\u4e60\u57fa\u672c\u7ed3\u6784\u4fdd\u7559\u548c\u8272\u5f69\u6620\u5c04\u6280\u80fd\uff1b2) \u5728\u65e0\u914d\u5bf9\u6570\u636e\u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4ee5\u57f9\u517b\u7ec6\u817b\u7684\u7f8e\u5b66\u611f\u77e5\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408\u5728\u7ebf-\u79bb\u7ebf\u5956\u52b1\u673a\u5236\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728\u6807\u51c6\u9884\u8bbe\u8fc1\u79fb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u9ed1\u767d\u7167\u7247\u7740\u8272\u548c\u8de8\u57df\uff08\u52a8\u6f2b\u5230\u7167\u7247\uff09\u9884\u8bbe\u8fc1\u79fb\u7b49\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u5b9e\u4e86\u5176\u8d85\u8d8a\u7b80\u5355\u7edf\u8ba1\u5339\u914d\u7684\u7f8e\u5b66\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "SemiNFT\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u827a\u672f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u4ece\u673a\u68b0\u6a21\u4eff\u5230\u76f4\u89c9\u521b\u4f5c\u7684\u8272\u5f69\u6da6\u8272\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u8d85\u8d8a\u4f20\u7edf\u7edf\u8ba1\u5339\u914d\u65b9\u6cd5\u7684\u590d\u6742\u7f8e\u5b66\u7406\u89e3\u6c34\u5e73\u3002"}}
{"id": "2602.08234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08234", "abs": "https://arxiv.org/abs/2602.08234", "authors": ["Peng Xia", "Jianwen Chen", "Hanyang Wang", "Jiaqi Liu", "Kaide Zeng", "Yu Wang", "Siwei Han", "Yiyang Zhou", "Xujiang Zhao", "Haifeng Chen", "Zeyu Zheng", "Cihang Xie", "Huaxiu Yao"], "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning", "comment": null, "summary": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.", "AI": {"tldr": "SkillRL\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u6280\u80fd\u53d1\u73b0\u548c\u9012\u5f52\u8fdb\u5316\uff0c\u5c06\u539f\u59cb\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bb0\u5fc6\u7684\u65b9\u6cd5\u4e3b\u8981\u5b58\u50a8\u539f\u59cb\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u8f68\u8ff9\u901a\u5e38\u5197\u4f59\u4e14\u566a\u58f0\u591a\uff0c\u65e0\u6cd5\u63d0\u53d6\u9ad8\u7ea7\u3001\u53ef\u91cd\u7528\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51faSkillRL\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u57fa\u4e8e\u7ecf\u9a8c\u7684\u84b8\u998f\u673a\u5236\u6784\u5efa\u5206\u5c42\u6280\u80fd\u5e93SkillBank\uff1b2) \u901a\u7528\u548c\u4efb\u52a1\u7279\u5b9a\u542f\u53d1\u5f0f\u7684\u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\uff1b3) \u6280\u80fd\u5e93\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u534f\u540c\u8fdb\u5316\u7684\u9012\u5f52\u8fdb\u5316\u673a\u5236", "result": "\u5728ALFWorld\u3001WebShop\u548c\u4e03\u4e2a\u641c\u7d22\u589e\u5f3a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSkillRL\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u5347\u8d85\u8fc715.3%\uff0c\u5e76\u5728\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "SkillRL\u901a\u8fc7\u5c06\u539f\u59cb\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6280\u80fd\uff0c\u663e\u8457\u51cf\u5c11token\u5360\u7528\u540c\u65f6\u589e\u5f3a\u63a8\u7406\u6548\u7528\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u53ef\u91cd\u7528\u884c\u4e3a\u6a21\u5f0f\u7684\u6709\u6548\u6846\u67b6"}}
{"id": "2602.08613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08613", "abs": "https://arxiv.org/abs/2602.08613", "authors": ["Wei Gao", "Wenxu Gao", "Xingming Mu", "Changhao Peng", "Ge Li"], "title": "Overview and Comparison of AVS Point Cloud Compression Standard", "comment": "3 figures, 3 tables", "summary": "Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u4e2d\u56fd\u97f3\u89c6\u9891\u7f16\u7801\u6807\u51c6\u5de5\u4f5c\u7ec4\uff08AVS\uff09\u5236\u5b9a\u7684\u7b2c\u4e00\u4ee3\u70b9\u4e91\u538b\u7f29\u6807\u51c6AVS PCC\uff0c\u4ece\u6280\u672f\u7279\u70b9\u548c\u6027\u80fd\u6bd4\u8f83\u4e24\u4e2a\u89d2\u5ea6\u8fdb\u884c\u5206\u6790\u3002", "motivation": "\u70b9\u4e91\u4f5c\u4e3a\u91cd\u8981\u76843D\u6570\u636e\u8868\u793a\u683c\u5f0f\uff0c\u5728\u6c89\u6d78\u5f0f\u5a92\u4f53\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u6570\u5b57\u9057\u4ea7\u4fdd\u62a4\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u5176\u5927\u6570\u636e\u91cf\u7ed9\u4f20\u8f93\u548c\u5b58\u50a8\u5e26\u6765\u6311\u6218\u3002\u867d\u7136MPEG\u5df2\u5236\u5b9a\u4e86G-PCC\u548cV-PCC\u4e24\u4e2a\u6807\u51c6\uff0c\u4f46\u4e2d\u56fdAVS\u5de5\u4f5c\u7ec4\u4e5f\u5f00\u53d1\u4e86\u81ea\u5df1\u7684\u70b9\u4e91\u538b\u7f29\u6807\u51c6AVS PCC\uff0c\u8be5\u6807\u51c6\u91c7\u7528\u4e86\u4e0d\u540c\u4e8e\u5176\u4ed6\u6807\u51c6\u7684\u65b0\u7f16\u7801\u5de5\u5177\u548c\u6280\u672f\u3002", "method": "\u4ece\u4e24\u4e2a\u89c6\u89d2\u56de\u987eAVS PCC\u6807\u51c6\uff1a1\uff09\u76f8\u5173\u6280\u672f\u5206\u6790\uff0c\u4ecb\u7ecdAVS PCC\u91c7\u7528\u7684\u65b0\u7f16\u7801\u5de5\u5177\u548c\u6280\u672f\uff1b2\uff09\u6027\u80fd\u6bd4\u8f83\uff0c\u5c06AVS PCC\u4e0e\u5176\u4ed6\u70b9\u4e91\u538b\u7f29\u6807\u51c6\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "AVS PCC\u4f5c\u4e3a\u4e2d\u56fd\u81ea\u4e3b\u5236\u5b9a\u7684\u7b2c\u4e00\u4ee3\u70b9\u4e91\u538b\u7f29\u6807\u51c6\uff0c\u91c7\u7528\u4e86\u4e0eMPEG\u6807\u51c6\u4e0d\u540c\u7684\u65b0\u7f16\u7801\u5de5\u5177\u548c\u6280\u672f\uff0c\u5728\u70b9\u4e91\u538b\u7f29\u9886\u57df\u63d0\u4f9b\u4e86\u53e6\u4e00\u79cd\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AVS PCC\u6807\u51c6\u7684\u5236\u5b9a\u4e3a\u4e2d\u56fd\u5728\u70b9\u4e91\u538b\u7f29\u9886\u57df\u63d0\u4f9b\u4e86\u81ea\u4e3b\u7684\u6280\u672f\u6807\u51c6\uff0c\u8be5\u6807\u51c6\u91c7\u7528\u4e86\u521b\u65b0\u7684\u7f16\u7801\u5de5\u5177\uff0c\u4e0e\u5176\u4ed6\u56fd\u9645\u6807\u51c6\u5f62\u6210\u5dee\u5f02\u5316\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u70b9\u4e91\u538b\u7f29\u6280\u672f\u7684\u591a\u6837\u5316\u53d1\u5c55\u548c\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2602.08239", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08239", "abs": "https://arxiv.org/abs/2602.08239", "authors": ["Zahra Rahimi Afzal", "Tara Esmaeilbeig", "Mojtaba Soltanalian", "Mesrob I. Ohannessian"], "title": "Linearization Explains Fine-Tuning in Large Language Models", "comment": null, "summary": "Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7ebf\u6027\u5316\u89c6\u89d2\u5206\u6790\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u673a\u5236\uff0c\u53d1\u73b0\u5fae\u8c03\u52a8\u6001\u7b49\u4ef7\u4e8e\u4f7f\u7528\u795e\u7ecf\u6b63\u5207\u6838(NTK)\u5b66\u4e60\uff0c\u5e76\u63ed\u793a\u4e86NTK\u7279\u5f81\u503c\u8c31\u4e0e\u6a21\u578b\u9002\u5e94\u6027\u80fd\u7684\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u867d\u7136\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u6280\u672f\u5728\u5927\u6a21\u578b\u9002\u5e94\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8bad\u7ec3\u6027\u80fd\u548c\u6cdb\u5316\u7684\u5185\u5728\u673a\u5236\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u7ebf\u6027\u5316\u89c6\u89d2\u6765\u6df1\u5165\u7406\u89e3\u8fd9\u4e9b\u5fae\u8c03\u6280\u672f\u7684\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u5316\u65b9\u6cd5\u5206\u6790\u5fae\u8c03\u52a8\u6001\uff0c\u901a\u8fc7\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u5f52\u7eb3\u504f\u7f6e\u4f7f\u5fae\u8c03\u52a8\u6001\u7b49\u4ef7\u4e8e\u4f7f\u7528\u6b63\u5b9a\u795e\u7ecf\u6b63\u5207\u6838(NTK)\u5b66\u4e60\u3002\u57fa\u4e8e\u6b63\u5219\u5316\u5f3a\u5ea6\u5206\u6790\u5b8c\u5168\u7ebf\u6027\u548c\u7ebf\u6027\u5316\u5fae\u8c03\u4f18\u5316\u7684\u63a5\u8fd1\u7a0b\u5ea6\uff0c\u5e76\u7ed9\u51faNTK\u8c31\u6270\u52a8\u754c\u9650\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u7ebf\u6027\u5316\u662f\u826f\u597d\u6a21\u578b\u65f6\uff0cNTK\u7684\u7279\u5f81\u503c\u8c31\u4e0e\u6a21\u578b\u9002\u5e94\u6027\u80fd\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002\u901a\u8fc7\u8c31\u6270\u52a8\u754c\u9650\u5206\u6790\u4e86\u9009\u62e9\u4e0d\u540c\u5c42\u8fdb\u884c\u5fae\u8c03\u5bf9NTK\u7684\u5f71\u54cd\uff0c\u5e76\u5728LLM\u7684LoRA\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u4ece\u7406\u8bba\u4e0a\u8868\u5f81\u4e86\u5fae\u8c03\u673a\u5236\uff0c\u8fd8\u63ed\u793a\u4e86NTK\u8c31\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u4e3a\u6539\u8fdbPEFT\u6280\u672f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u660e\u667a\u3001\u66f4\u7075\u6d3b\u7684LLM\u9002\u5e94\u65b9\u6cd5\u3002"}}
{"id": "2602.08615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08615", "abs": "https://arxiv.org/abs/2602.08615", "authors": ["Kfir Goldberg", "Elad Richardson", "Yael Vinker"], "title": "Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration", "comment": "Project page available at https://inspirationseedspaper.github.io/InspirationSeeds/", "summary": "While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.", "AI": {"tldr": "Inspiration Seeds\uff1a\u4e00\u4e2a\u65e0\u9700\u6587\u672c\u63d0\u793a\u7684\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u5f20\u8f93\u5165\u56fe\u50cf\u751f\u6210\u591a\u6837\u5316\u89c6\u89c9\u7ec4\u5408\uff0c\u652f\u6301\u521b\u610f\u63a2\u7d22\u9636\u6bb5\u7684\u53ef\u89c6\u5316\u6784\u601d", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6587\u672c\u63d0\u793a\u8fdb\u884c\u4f18\u5316\uff0c\u4e0d\u652f\u6301\u521b\u610f\u5f62\u6210\u524d\u7684\u5f00\u653e\u5f0f\u89c6\u89c9\u63a2\u7d22\u3002\u8bbe\u8ba1\u5e08\u901a\u5e38\u4ece\u677e\u6563\u8fde\u63a5\u7684\u89c6\u89c9\u53c2\u8003\u4e2d\u5bfb\u627e\u7075\u611f\uff0c\u9700\u8981\u80fd\u591f\u63ed\u793a\u8f93\u5165\u56fe\u50cf\u95f4\u6f5c\u5728\u5173\u7cfb\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528CLIP\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6CLIP\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7f16\u8f91\u65b9\u5411\uff0c\u901a\u8fc7\u7eaf\u89c6\u89c9\u624b\u6bb5\u5206\u89e3\u89c6\u89c9\u65b9\u9762\uff0c\u6784\u5efa\u5408\u6210\u4e09\u5143\u7ec4\u8fdb\u884c\u524d\u9988\u8bad\u7ec3\u3002\u7ed9\u5b9a\u4e24\u5f20\u8f93\u5165\u56fe\u50cf\uff0c\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u4e14\u89c6\u89c9\u8fde\u8d2f\u7684\u7ec4\u5408\uff0c\u65e0\u9700\u7528\u6237\u6307\u5b9a\u6587\u672c\u63d0\u793a\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u63ed\u793a\u8f93\u5165\u56fe\u50cf\u95f4\u6f5c\u5728\u5173\u7cfb\u7684\u591a\u6837\u5316\u89c6\u89c9\u7ec4\u5408\uff0c\u652f\u6301\u5feb\u901f\u76f4\u89c2\u7684\u89c6\u89c9\u91cd\u7ec4\uff0c\u9002\u7528\u4e8e\u521b\u610f\u5de5\u4f5c\u7684\u65e9\u671f\u548c\u6a21\u7cca\u9636\u6bb5\u3002", "conclusion": "Inspiration Seeds\u6846\u67b6\u5c06\u56fe\u50cf\u751f\u6210\u4ece\u6700\u7ec8\u6267\u884c\u8f6c\u5411\u63a2\u7d22\u6027\u6784\u601d\uff0c\u901a\u8fc7\u6d88\u9664\u5bf9\u8bed\u8a00\u7684\u4f9d\u8d56\uff0c\u652f\u6301\u521b\u610f\u8fc7\u7a0b\u4e2d\u7684\u89c6\u89c9\u6784\u601d\uff0c\u7279\u522b\u9002\u5408\u65e9\u671f\u521b\u610f\u63a2\u7d22\u9636\u6bb5\u3002"}}
{"id": "2602.08244", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08244", "abs": "https://arxiv.org/abs/2602.08244", "authors": ["Juncheng Dong", "Bowen He", "Moyang Guo", "Ethan X. Fang", "Zhuoran Yang", "Vahid Tarokh"], "title": "Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers", "comment": null, "summary": "In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u504f\u597d\u7684\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICPRL)\uff0c\u4ec5\u4f7f\u7528\u504f\u597d\u53cd\u9988\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u90e8\u7f72\uff0c\u65e0\u9700\u5956\u52b1\u76d1\u7763\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e0e\u4f7f\u7528\u5b8c\u6574\u5956\u52b1\u76d1\u7763\u7684ICRL\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60(ICRL)\u65b9\u6cd5\u4f9d\u8d56\u660e\u786e\u7684\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u5728\u5956\u52b1\u6a21\u7cca\u3001\u96be\u4ee5\u6307\u5b9a\u6216\u83b7\u53d6\u6210\u672c\u9ad8\u65f6\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u4ec5\u4f7f\u7528\u504f\u597d\u53cd\u9988\u7684\u5b66\u4e60\u8303\u5f0f\u3002", "method": "\u63d0\u51faICPRL\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u79cd\u53d8\u4f53\uff1a\u57fa\u4e8e\u5373\u65f6\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60(I-PRL)\u4f7f\u7528\u6bcf\u6b65\u504f\u597d\uff0c\u57fa\u4e8e\u8f68\u8ff9\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60(T-PRL)\u4f7f\u7528\u8f68\u8ff9\u7ea7\u6bd4\u8f83\u3002\u5f15\u5165\u504f\u597d\u539f\u751f\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u504f\u597d\u6570\u636e\u4f18\u5316Transformer\u7b56\u7565\uff0c\u65e0\u9700\u5956\u52b1\u4fe1\u53f7\u6216\u6700\u4f18\u52a8\u4f5c\u6807\u7b7e\u3002", "result": "\u5728\u51b3\u6597\u8001\u864e\u673a\u3001\u5bfc\u822a\u548c\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cICPRL\u80fd\u591f\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5f3a\u5927\u7684\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\uff0c\u6027\u80fd\u4e0e\u4f7f\u7528\u5b8c\u6574\u5956\u52b1\u76d1\u7763\u7684ICRL\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "ICPRL\u8bc1\u660e\u4e86\u4ec5\u4f7f\u7528\u504f\u597d\u53cd\u9988\u8fdb\u884c\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5956\u52b1\u4fe1\u53f7\u96be\u4ee5\u83b7\u53d6\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2602.08620", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08620", "abs": "https://arxiv.org/abs/2602.08620", "authors": ["Siyu Liu", "Chujie Qin", "Hubery Yin", "Qixin Yan", "Zheng-Peng Duan", "Chen Li", "Jing Lyu", "Chun-Le Guo", "Chongyi Li"], "title": "Improving Reconstruction of Representation Autoencoder", "comment": null, "summary": "Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.", "AI": {"tldr": "LV-RAE\u662f\u4e00\u79cd\u8868\u793a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\u7684\u4f4e\u7ea7\u4fe1\u606f\u6765\u63d0\u5347\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5206\u5e03\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u89e3\u7801\u5668\u9c81\u68d2\u6027\u5fae\u8c03\u548c\u6f5c\u5728\u5e73\u6ed1\u6765\u6539\u5584\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u56fe\u50cf\u7f16\u7801\u5668\u6765\u63d0\u5347\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\uff0c\u4f46\u8fd9\u4e9b\u8bed\u4e49\u7279\u5f81\u7f3a\u4e4f\u4f4e\u7ea7\u4fe1\u606f\uff08\u5982\u989c\u8272\u548c\u7eb9\u7406\uff09\uff0c\u5bfc\u81f4\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0b\u964d\uff0c\u6210\u4e3a\u8fdb\u4e00\u6b65\u6269\u5c55LDMs\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51faLV-RAE\u8868\u793a\u81ea\u7f16\u7801\u5668\uff0c\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\u4e2d\u7f3a\u5931\u7684\u4f4e\u7ea7\u4fe1\u606f\uff1b\u901a\u8fc7\u5206\u6790\u53d1\u73b0\u4fe1\u606f\u4e30\u5bcc\u7684\u6f5c\u5728\u8868\u793a\u4f7f\u89e3\u7801\u5668\u5bf9\u6f5c\u5728\u6270\u52a8\u654f\u611f\uff0c\u56e0\u6b64\u5fae\u8c03\u89e3\u7801\u5668\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u53d7\u63a7\u566a\u58f0\u6ce8\u5165\u5e73\u6ed1\u751f\u6210\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLV-RAE\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u62bd\u8c61\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "LV-RAE\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u7279\u5f81\u7684\u4f4e\u7ea7\u4fe1\u606f\u89e3\u51b3\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u63d0\u9ad8\u89e3\u7801\u5668\u9c81\u68d2\u6027\u548c\u6f5c\u5728\u5e73\u6ed1\u6280\u672f\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u6269\u5c55LDMs\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08261", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.08261", "abs": "https://arxiv.org/abs/2602.08261", "authors": ["Binglin Wu", "Yingyi Zhang", "Xianneng Li", "Ruyue Deng", "Chuan Yue", "Weiru Zhang", "Xiaoyi Zeng"], "title": "Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization", "comment": null, "summary": "Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.", "AI": {"tldr": "PRO-Bid\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea6\u675f\u611f\u77e5\u751f\u6210\u7684\u81ea\u52a8\u7ade\u4ef7\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7ea6\u675f\u8868\u793a\u548c\u53cd\u4e8b\u5b9e\u4f18\u5316\u673a\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u51b3\u7b56\u53d8\u6362\u5668\u5728\u6ee1\u8db3\u76ee\u6807CPA\u7ea6\u675f\u65f6\u7684\u72b6\u6001\u6df7\u53e0\u548c\u5e73\u5747\u6a21\u4eff\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u7ea6\u675f\u6ee1\u8db3\u548c\u4ef7\u503c\u83b7\u53d6\u3002", "motivation": "\u4f20\u7edf\u51b3\u7b56\u53d8\u6362\u5668\u5e94\u7528\u4e8e\u81ea\u52a8\u7ade\u4ef7\u7cfb\u7edf\u65f6\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a1) \u6807\u51c6Return-to-Go\u6761\u4ef6\u5316\u5ffd\u7565\u6210\u672c\u7ef4\u5ea6\u5bfc\u81f4\u72b6\u6001\u6df7\u53e0\uff0c\u65e0\u6cd5\u7cbe\u786e\u63a7\u5236\u8d44\u6e90\u8282\u594f\uff1b2) \u6807\u51c6\u56de\u5f52\u8feb\u4f7f\u7b56\u7565\u6a21\u4eff\u5386\u53f2\u5e73\u5747\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u5411\u7ea6\u675f\u8fb9\u754c\u4f18\u5316\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faPRO-Bid\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u673a\u5236\uff1a1) \u7ea6\u675f\u89e3\u8026\u5e15\u7d2f\u6258\u8868\u793a(CDPR)\uff0c\u5c06\u5168\u5c40\u7ea6\u675f\u5206\u89e3\u4e3a\u9012\u5f52\u6210\u672c\u548c\u4ef7\u503c\u4e0a\u4e0b\u6587\u4ee5\u6062\u590d\u8d44\u6e90\u611f\u77e5\uff0c\u5e76\u57fa\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\u91cd\u65b0\u52a0\u6743\u8f68\u8ff9\u4ee5\u805a\u7126\u9ad8\u6548\u6570\u636e\uff1b2) \u53cd\u4e8b\u5b9e\u9057\u61be\u4f18\u5316(CRO)\uff0c\u5229\u7528\u5168\u5c40\u7ed3\u679c\u9884\u6d4b\u5668\u8bc6\u522b\u66f4\u4f18\u7684\u53cd\u4e8b\u5b9e\u52a8\u4f5c\uff0c\u5c06\u8fd9\u4e9b\u9ad8\u6548\u7528\u7ed3\u679c\u4f5c\u4e3a\u52a0\u6743\u56de\u5f52\u76ee\u6807\uff0c\u4f7f\u6a21\u578b\u8d85\u8d8a\u5386\u53f2\u5e73\u5747\u5411\u6700\u4f18\u7ea6\u675f\u8fb9\u754c\u903c\u8fd1\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPRO-Bid\u5728\u7ea6\u675f\u6ee1\u8db3\u548c\u4ef7\u503c\u83b7\u53d6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PRO-Bid\u901a\u8fc7\u521b\u65b0\u7684\u7ea6\u675f\u89e3\u8026\u8868\u793a\u548c\u53cd\u4e8b\u5b9e\u4f18\u5316\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u7ade\u4ef7\u7cfb\u7edf\u4e2d\u51b3\u7b56\u53d8\u6362\u5668\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7ea6\u675f\u8fb9\u754c\u4f18\u5316\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.08626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08626", "abs": "https://arxiv.org/abs/2602.08626", "authors": ["Alexis Marouani", "Oriane Sim\u00e9oni", "Herv\u00e9 J\u00e9gou", "Piotr Bojanowski", "Huy V. Vo"], "title": "Revisiting [CLS] and Patch Token Interaction in Vision Transformers", "comment": "To be published as a conference paper at ICLR 2026", "summary": "Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76Vision Transformers\u4e2d\u5168\u5c40\uff08CLS token\uff09\u548c\u5c40\u90e8\uff08patch token\uff09\u7279\u5f81\u5b66\u4e60\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u4e13\u95e8\u5316\u5904\u7406\u8def\u5f84\u5206\u79bb\u4e24\u7c7btoken\u7684\u8ba1\u7b97\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "Vision Transformers\u901a\u5e38\u5c06\u53ef\u5b66\u4e60\u7684[CLS]\u7c7btoken\u4e0epatch token\u4e00\u8d77\u5904\u7406\uff0c\u5c3d\u7ba1\u5b83\u4eec\u6027\u8d28\u4e0d\u540c\u3002\u4f5c\u8005\u53d1\u73b0\u6807\u51c6\u5f52\u4e00\u5316\u5c42\u5728\u8fd9\u4e24\u7c7btoken\u4e4b\u95f4\u5f15\u5165\u4e86\u9690\u5f0f\u533a\u5206\uff0c\u5bfc\u81f4\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u5b58\u5728\u6469\u64e6\uff0c\u9700\u8981\u4e13\u95e8\u5316\u5904\u7406\u6765\u6539\u5584\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u8868\u793a\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7c7btoken\u548cpatch token\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u63d0\u51fa\u4e13\u95e8\u5316\u5904\u7406\u8def\u5f84\uff0c\u7279\u522b\u662f\u5728\u5f52\u4e00\u5316\u5c42\u548c\u65e9\u671fquery-key-value\u6295\u5f71\u4e2d\uff0c\u6709\u9009\u62e9\u5730\u89e3\u8026\u4e24\u7c7btoken\u7684\u8ba1\u7b97\u6d41\u7a0b\u3002\u8fd9\u79cd\u65b9\u6cd5\u9488\u5bf9\u6027\u5730\u5206\u79bb\u4e86\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\u7684\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u5272\u6027\u80fd\u63d0\u5347\u8d85\u8fc72 mIoU\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002\u63d0\u51fa\u7684\u4fee\u6539\u4ec5\u589e\u52a08%\u7684\u53c2\u6570\uff0c\u6ca1\u6709\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u901a\u8fc7\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u54ea\u4e9b\u67b6\u6784\u7ec4\u4ef6\u4ece\u4e13\u95e8\u5316\u4e2d\u83b7\u76ca\u6700\u591a\uff0c\u4ee5\u53ca\u8be5\u65b9\u6cd5\u5982\u4f55\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u5b66\u4e60\u6846\u67b6\u4e2d\u6cdb\u5316\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u5316\u5904\u7406\u8def\u5f84\u5206\u79bbVision Transformers\u4e2d\u7c7btoken\u548cpatch token\u7684\u8ba1\u7b97\u6d41\u7a0b\uff0c\u53ef\u4ee5\u6709\u6548\u6539\u5584\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u8868\u793a\u8d28\u91cf\uff0c\u5728\u5206\u5272\u6027\u80fd\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5f88\u5c0f\u3002"}}
{"id": "2602.08652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08652", "abs": "https://arxiv.org/abs/2602.08652", "authors": ["Oskar Thaeter", "Tanja Niedermair", "Johannes Raffler", "Ralf Huss", "Peter J. Sch\u00fcffler"], "title": "Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology", "comment": "17 pages, 8 figures, 7 tables", "summary": "Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to\n  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen\n  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.\n  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from\n  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,\n  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).\n  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and\n  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\\times$\n  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.\n  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for\n  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner\n  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other\n  low-resolution slide annotations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u4f7f\u7528\u4f4e\u5206\u8fa8\u7387\u9884\u626b\u63cf\u7f29\u7565\u56fe\u9884\u6d4b\u7ec4\u7ec7\u56fa\u5b9a\u7c7b\u578b\uff0c\u5b9e\u73b0\u5feb\u901f\u9ad8\u901a\u91cf\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb400\u500d\u3002", "motivation": "\u75c5\u7406\u5b9e\u9a8c\u5ba4\u4e2d\u7ec4\u7ec7\u56fa\u5b9a\u7c7b\u578b\u7684\u624b\u52a8\u6807\u6ce8\u5bb9\u6613\u51fa\u9519\uff0c\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5168\u5206\u8fa8\u7387\u5168\u5207\u7247\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u9ad8\u901a\u91cf\u8d28\u91cf\u63a7\u5236\u7684\u6269\u5c55\u6027\u3002", "method": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u4f4e\u5206\u8fa8\u7387\u9884\u626b\u63cf\u7f29\u7565\u56fe\u9884\u6d4b\u798f\u5c14\u9a6c\u6797\u56fa\u5b9a\u77f3\u8721\u5305\u57cb\uff08FFPE\uff09\u548c\u51b0\u51bb\u5207\u7247\uff08FS\uff09\u7684\u56fa\u5b9a\u7c7b\u578b\u3002\u6a21\u578b\u5728TUM\u75c5\u7406\u7814\u7a76\u6240\u76841200\u5f20WSI\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728TCGA\u3001\u5965\u683c\u65af\u5821\u548c\u96f7\u6839\u65af\u5821\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728TCGA\u6570\u636e\u96c6\u4e0aAUROC\u8fbe\u52300.88\uff0c\u6bd4\u53ef\u6bd4\u9884\u626b\u63cf\u65b9\u6cd5\u63d0\u9ad84.8%\u3002\u5728\u96f7\u6839\u65af\u5821\u548c\u5965\u683c\u65af\u5821\u6570\u636e\u96c6\u4e0aAUROC\u4e3a0.72\uff0c\u63ed\u793a\u4e86\u626b\u63cf\u4eea\u5f15\u8d77\u7684\u57df\u504f\u79fb\u6311\u6218\u3002\u5904\u7406\u6bcf\u5f20\u5207\u7247\u4ec5\u970021\u6beb\u79d2\uff0c\u6bd4\u73b0\u6709\u9ad8\u500d\u7387\u5168\u5206\u8fa8\u7387\u65b9\u6cd5\u5feb400\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u68c0\u6d4b\u6807\u6ce8\u9519\u8bef\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4f9d\u8d56\u9ad8\u500d\u7387\u626b\u63cf\uff0c\u662f\u75c5\u7406\u9ad8\u901a\u91cf\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u6709\u4ef7\u503c\u7684\u8d28\u91cf\u63a7\u5236\u5de5\u5177\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u6539\u8fdb\u6a21\u578b\u5bf9\u66f4\u591a\u626b\u63cf\u4eea\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.08272", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08272", "abs": "https://arxiv.org/abs/2602.08272", "authors": ["Junwei Su", "Chuan Wu"], "title": "When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7PAC\u6846\u67b6\u7406\u8bba\u5206\u6790\u6bd4\u8f83\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e0e\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08SARL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6837\u672c\u6548\u7387\uff0c\u53d1\u73b0\u5f53\u4efb\u52a1\u81ea\u7136\u5206\u89e3\u4e3a\u72ec\u7acb\u5b50\u4efb\u52a1\u65f6MARL\u66f4\u4f18\uff0c\u800c\u4f9d\u8d56\u6027\u5b50\u4efb\u52a1\u4f1a\u524a\u5f31\u5176\u4f18\u52bf\u3002", "motivation": "\u5c3d\u7ba1\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u6307\u5bfc\u8bf4\u660e\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55MARL\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08SARL\uff09\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u9009\u62e9\u5408\u9002\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "method": "\u91c7\u7528\u6982\u7387\u8fd1\u4f3c\u6b63\u786e\uff08PAC\uff09\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49LLM\u7684SARL\u548cMARL\u8bbe\u7f6e\uff0c\u63a8\u5bfc\u663e\u5f0f\u7684\u6837\u672c\u590d\u6742\u5ea6\u8fb9\u754c\uff0c\u7cfb\u7edf\u5206\u6790\u4efb\u52a1\u5206\u89e3\u548c\u5bf9\u9f50\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u5bf9\u9f50\u6982\u5ff5\u91cf\u5316\u72ec\u7acb\u4efb\u52a1\u5206\u89e3\u7684\u6743\u8861\u3002", "result": "MARL\u5728\u4efb\u52a1\u81ea\u7136\u5206\u89e3\u4e3a\u72ec\u7acb\u5b50\u4efb\u52a1\u65f6\u80fd\u6539\u5584\u6837\u672c\u590d\u6742\u5ea6\uff0c\u800c\u4f9d\u8d56\u6027\u5b50\u4efb\u52a1\u4f1a\u524a\u5f31MARL\u7684\u6bd4\u8f83\u4f18\u52bf\uff1b\u4efb\u52a1\u5bf9\u9f50\u5206\u6790\u63ed\u793a\u4e86\u5728\u6f5c\u5728\u9519\u914d\u60c5\u51b5\u4e0b\u5f3a\u5236\u72ec\u7acb\u4efb\u52a1\u5206\u89e3\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u7406\u8bba\u5206\u6790\u6f84\u6e05\u4e86\u5b9e\u8bc1\u7814\u7a76\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u5728\u590d\u6742LLM\u573a\u666f\u4e2d\u6709\u6548\u90e8\u7f72MARL\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u6807\u51c6\uff0c\u6307\u5bfc\u4f55\u65f6\u9009\u62e9MARL\u6846\u67b6\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2602.08661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08661", "abs": "https://arxiv.org/abs/2602.08661", "authors": ["Yi Dao", "Lankai Zhang", "Hao Liu", "Haiwei Zhang", "Wenbo Wang"], "title": "WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling", "comment": null, "summary": "Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.", "AI": {"tldr": "WiFlow\u662f\u4e00\u4e2a\u57fa\u4e8eWiFi\u4fe1\u53f7\u7684\u8fde\u7eed\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5728\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.00% PCK@20\u548c99.48% PCK@50\u7684\u7cbe\u5ea6\uff0c\u6a21\u578b\u53c2\u6570\u4ec54.82M\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u662f\u7269\u8054\u7f51\u667a\u80fd\u611f\u77e5\u7684\u57fa\u7840\uff0c\u73b0\u6709WiFi\u65b9\u6cd5\u5728\u8fde\u7eed\u8fd0\u52a8\u5904\u7406\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002WiFlow\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645WiFi\u59ff\u6001\u4f30\u8ba1\u5efa\u7acb\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002", "method": "\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff1a\u7f16\u7801\u5668\u4f7f\u7528\u65f6\u5e8f\u548c\u975e\u5bf9\u79f0\u5377\u79ef\u6355\u6349CSI\u7684\u65f6\u7a7a\u7279\u5f81\uff0c\u4fdd\u6301\u4fe1\u53f7\u539f\u59cb\u5e8f\u5217\u7ed3\u6784\uff1b\u901a\u8fc7\u8f74\u5411\u6ce8\u610f\u529b\u7cbe\u70bc\u4eba\u4f53\u5173\u952e\u70b9\u7279\u5f81\u5e76\u6355\u6349\u7ed3\u6784\u4f9d\u8d56\uff1b\u89e3\u7801\u5668\u5c06\u7f16\u7801\u7684\u9ad8\u7ef4\u7279\u5f81\u6620\u5c04\u4e3a\u5173\u952e\u70b9\u5750\u6807\u3002", "result": "\u57285\u540d\u53d7\u8bd5\u8005\u6267\u884c8\u79cd\u65e5\u5e38\u6d3b\u52a8\u768436\u4e07\u4e2a\u540c\u6b65CSI-\u59ff\u6001\u6837\u672c\u6570\u636e\u96c6\u4e0a\uff0cWiFlow\u8fbe\u5230PCK@20\u4e3a97.00%\uff0cPCK@50\u4e3a99.48%\uff0c\u5e73\u5747\u6bcf\u5173\u8282\u4f4d\u7f6e\u8bef\u5dee0.008\u7c73\uff0c\u6a21\u578b\u53c2\u6570\u4ec54.82M\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "WiFlow\u4e3a\u5b9e\u9645WiFi\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.08290", "categories": ["cs.LG", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.08290", "abs": "https://arxiv.org/abs/2602.08290", "authors": ["Ajay Kumar Shrestha"], "title": "Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems", "comment": "To appear in the ICBTA 2025 Conference Proceedings and published as a volume of Lecture Notes in Networks and Systems by Springer", "summary": "In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u4efb\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u5956\u52b1\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u53c2\u4e0e\u8005\u7684\u8d21\u732e\u8d28\u91cf\uff0c\u901a\u8fc7\u52a8\u6001\u4fe1\u4efb\u8bc4\u5206\u548c\u533a\u5757\u94fe\u6280\u672f\u786e\u4fdd\u7cfb\u7edf\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5b58\u5728\u6076\u610f\u6216\u6545\u969c\u8282\u70b9\u53ef\u80fd\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u7684\u95ee\u9898\uff0c\u9700\u8981\u786e\u4fdd\u7cfb\u7edf\u5b8c\u6574\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6fc0\u52b1\u8bda\u5b9e\u53c2\u4e0e\u548c\u60e9\u7f5a\u4e0d\u53ef\u9760\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u4efb\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u52a8\u6001\u8bc4\u4f30\u4fe1\u4efb\u5206\u6570\uff08\u8003\u8651\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u8d21\u732e\u9891\u7387\uff09\uff0c\u5e76\u5c06\u4fe1\u4efb\u5206\u6570\u4f5c\u4e3a\u6fc0\u52b1\u5206\u914d\u7684\u57fa\u7840\uff1b\u540c\u65f6\u63a2\u7d22\u6574\u5408\u533a\u5757\u94fe\u548c\u667a\u80fd\u5408\u7ea6\u6765\u81ea\u52a8\u5316\u4fe1\u4efb\u8bc4\u4f30\u548c\u6fc0\u52b1\u5206\u914d\u8fc7\u7a0b\u3002", "result": "\u7406\u8bba\u6846\u67b6\u65e8\u5728\u521b\u5efa\u66f4\u5065\u58ee\u3001\u516c\u5e73\u548c\u900f\u660e\u7684\u8054\u90a6\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u4fe1\u4efb\u8bc4\u5206\u548c\u6fc0\u52b1\u673a\u5236\u51cf\u5c11\u4e0d\u53ef\u4fe1\u53c2\u4e0e\u8005\u5e26\u6765\u7684\u98ce\u9669\u3002", "conclusion": "\u63d0\u51fa\u7684\u4fe1\u4efb\u57fa\u6fc0\u52b1\u673a\u5236\u7ed3\u5408\u533a\u5757\u94fe\u6280\u672f\u80fd\u591f\u6709\u6548\u4fc3\u8fdb\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u8bda\u5b9e\u53c2\u4e0e\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u7684\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u73af\u5883\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.08682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08682", "abs": "https://arxiv.org/abs/2602.08682", "authors": ["Ying Guo", "Qijun Gan", "Yifu Zhang", "Jinlai Liu", "Yifei Hu", "Pan Xie", "Dongjun Qian", "Yu Zhang", "Ruiqi Li", "Yuqi Zhang", "Ruibiao Lu", "Xiaofeng Mei", "Bo Han", "Xiang Yin", "Bingyue Peng", "Zehuan Yuan"], "title": "ALIVE: Animate Your World with Lifelike Audio-Video Generation", "comment": null, "summary": "Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.", "AI": {"tldr": "ALIVE\u662f\u4e00\u4e2a\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdbMMDiT\u67b6\u6784\u548c\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5347\u7ea7\u4e3a\u7c7b\u4f3cSora\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\u548c\u52a8\u753b\u80fd\u529b\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u6b63\u671d\u7740\u7edf\u4e00\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\u53d1\u5c55\uff0c\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7f3a\u4e4f\u97f3\u9891\u751f\u6210\u548c\u52a8\u753b\u80fd\u529b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u751f\u6210\u540c\u6b65\u97f3\u9891\u548c\u89c6\u9891\u5185\u5bb9\u7684\u6a21\u578b\u3002", "method": "1) \u6269\u5c55MMDiT\u67b6\u6784\uff0c\u589e\u52a0\u8054\u5408\u97f3\u9891-\u89c6\u9891\u5206\u652f\uff0c\u5305\u542bTA-CrossAttn\u7528\u4e8e\u65f6\u5e8f\u5bf9\u9f50\u7684\u8de8\u6a21\u6001\u878d\u5408\u548cUniTemp-RoPE\u7528\u4e8e\u7cbe\u786e\u7684\u97f3\u9891-\u89c6\u89c9\u5bf9\u9f50\uff1b2) \u8bbe\u8ba1\u5168\u9762\u7684\u6570\u636e\u7ba1\u9053\uff0c\u5305\u62ec\u97f3\u9891-\u89c6\u9891\u5b57\u5e55\u751f\u6210\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u6536\u96c6\u767e\u4e07\u7ea7\u9ad8\u8d28\u91cf\u5fae\u8c03\u6570\u636e\uff1b3) \u5f15\u5165\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "ALIVE\u5728\u767e\u4e07\u7ea7\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u540e\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u6301\u7eed\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ALIVE\u6210\u529f\u5c06\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5347\u7ea7\u4e3a\u97f3\u9891-\u89c6\u9891\u751f\u6210\u548c\u52a8\u753b\u6a21\u578b\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u5b9e\u73b0\u65b9\u6848\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u793e\u533a\u5f00\u53d1\u97f3\u9891-\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u652f\u6301\u3002"}}
{"id": "2602.08683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08683", "abs": "https://arxiv.org/abs/2602.08683", "authors": ["Feilong Tang", "Xiang An", "Yunyao Yan", "Yin Xie", "Bin Qin", "Kaicheng Yang", "Yifei Shen", "Yuanhan Zhang", "Chunyuan Li", "Shikun Feng", "Changrui Chen", "Huajie Tan", "Ming Hu", "Manyuan Zhang", "Bo Li", "Ziyong Feng", "Ziwei Liu", "Zongyuan Ge", "Jiankang Deng"], "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence", "comment": null, "summary": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.\n  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.\n  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u89c6\u89c9\u7406\u89e3\u7684\u6838\u5fc3\u662f\u538b\u7f29\u95ee\u9898\uff0c\u901a\u8fc7\u7f16\u7801\u5668\u67b6\u6784\u4e0e\u89c6\u9891\u4fe1\u606f\u8bba\u539f\u7406\u5bf9\u9f50\uff0c\u91c7\u7528\u7a00\u758f\u8ba1\u7b97\u805a\u7126\u4fe1\u53f7\u71b5\u533a\u57df\uff0c\u5b9e\u73b0\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6b63\u76f8\u5173", "motivation": "\u73b0\u4ee3\u89c6\u89c9\u67b6\u6784\u504f\u79bb\u4e86\u4fe1\u606f\u8bba\u57fa\u672c\u539f\u5219\uff0c\u89c6\u89c9\u4fe1\u53f7\u9ad8\u5ea6\u5197\u4f59\u800c\u5224\u522b\u4fe1\u606f\u7a00\u758f\uff0c\u5f53\u524d\u6a21\u578b\u5747\u5300\u5904\u7406\u5bc6\u96c6\u50cf\u7d20\u7f51\u683c\uff0c\u6d6a\u8d39\u5927\u91cf\u8ba1\u7b97\u5728\u9759\u6001\u80cc\u666f\u800c\u975e\u9884\u6d4b\u6b8b\u5dee\u4e0a", "method": "\u91c7\u7528Codec Patchification\u6280\u672f\uff0c\u4ec5\u5904\u74063.1%-25%\u5bcc\u542b\u4fe1\u53f7\u71b5\u7684\u533a\u57df\uff1b\u4f7f\u7528\u5171\u4eab3D RoPE\u7edf\u4e00\u65f6\u7a7a\u63a8\u7406\uff1b\u901a\u8fc7\u5927\u89c4\u6a21\u805a\u7c7b\u5224\u522b\u76ee\u6807\u8bad\u7ec3\uff0c\u6355\u83b7\u5bf9\u8c61\u6301\u4e45\u6027\u548c\u8fd0\u52a8\u52a8\u6001", "result": "\u572816\u4e2a\u56fe\u50cf\u3001\u89c6\u9891\u548c\u6587\u6863\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOV-Encoder\u4f7f\u7528\u66f4\u5c11\u89c6\u89c9token\u548c\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u4ecd\u4f18\u4e8eQwen3-ViT\u548cSigLIP2\uff1b\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u5e73\u5747\u63d0\u53474.1%", "conclusion": "\u6548\u7387\u4e0e\u51c6\u786e\u6027\u5448\u6b63\u76f8\u5173\uff0cCodec\u5bf9\u9f50\u7684\u8865\u4e01\u7ea7\u7a00\u758f\u6027\u662f\u57fa\u7840\u539f\u5219\uff0c\u4f7fOV-Encoder\u6210\u4e3a\u4e0b\u4e00\u4ee3\u89c6\u89c9\u901a\u7528\u6a21\u578b\u7684\u53ef\u6269\u5c55\u5f15\u64ce"}}
{"id": "2602.08307", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08307", "abs": "https://arxiv.org/abs/2602.08307", "authors": ["Mengxiao Zhang", "Yuheng Zhang", "Haipeng Luo", "Paul Mineiro"], "title": "Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback", "comment": null, "summary": "In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6b65\u4ea4\u4e92\u573a\u666f\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u5c06\u4ea4\u4e92\u5f0f\u57fa\u7840\u5b66\u4e60\u4ece\u5355\u6b65\u6269\u5c55\u5230\u591a\u6b65\u8bbe\u7f6e\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u60c5\u666f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u6b21\u7ebf\u6027\u9057\u61be\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709\u4ea4\u4e92\u5f0f\u57fa\u7840\u5b66\u4e60\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u6b65\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u5e94\u7528\u4e8e\u591a\u8f6e\u51b3\u7b56\u7cfb\u7edf\uff08\u5982\u591a\u8f6eLLM\u90e8\u7f72\uff09\uff0c\u9700\u8981\u5c06\u8fd9\u4e00\u8303\u5f0f\u6269\u5c55\u5230\u591a\u6b65\u5e8f\u5217\u51b3\u7b56\u573a\u666f\u3002", "method": "\u5c06Zhang\u7b49\u4eba[2024a]\u7684\u5956\u52b1\u4f30\u8ba1\u5668\u4ece\u5355\u6b65\u6269\u5c55\u5230\u591a\u6b65\u8bbe\u7f6e\uff0c\u89e3\u51b3MDP\u4e0b\u6f5c\u5728\u5956\u52b1\u89e3\u7801\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u9006\u95f4\u9699\u52a0\u6743\u7b97\u6cd5\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u7b97\u6cd5\u5728\u4e0a\u4e0b\u6587\u60c5\u666fMDP\u4e2d\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u4fdd\u8bc1\uff0c\u5e76\u5728\u5408\u6210\u60c5\u666fMDP\u548c\u771f\u5b9e\u4e16\u754c\u7528\u6237\u9884\u8ba2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5b66\u4e60\u4e2a\u6027\u5316\u76ee\u6807\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6210\u529f\u5c06\u4ea4\u4e92\u5f0f\u57fa\u7840\u5b66\u4e60\u6269\u5c55\u5230\u591a\u6b65\u5e8f\u5217\u51b3\u7b56\u573a\u666f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\uff08\u5982\u591a\u8f6eLLM\u4ea4\u4e92\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7b97\u6cd5\u3002"}}
{"id": "2602.08711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08711", "abs": "https://arxiv.org/abs/2602.08711", "authors": ["Linli Yao", "Yuancheng Wei", "Yaojie Zhang", "Lei Li", "Xinlong Chen", "Feifan Song", "Ziyue Wang", "Kun Ouyang", "Yuanxin Liu", "Lingpeng Kong", "Qi Liu", "Pengfei Wan", "Kun Gai", "Yuanxing Zhang", "Xu Sun"], "title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions", "comment": null, "summary": "This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.", "AI": {"tldr": "\u63d0\u51faOmni Dense Captioning\u4efb\u52a1\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u57fa\u51c6\u6570\u636e\u96c6OmniDCBench\u548c\u8bad\u7ec3\u6570\u636e\u96c6TimeChatCap-42K\uff0c\u5f00\u53d1TimeChat-Captioner-7B\u6a21\u578b\uff0c\u5728\u5bc6\u96c6\u63cf\u8ff0\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63cf\u8ff0\u4efb\u52a1\u901a\u5e38\u751f\u6210\u7b80\u77ed\u3001\u6982\u62ec\u6027\u7684\u63cf\u8ff0\uff0c\u7f3a\u4e4f\u8fde\u7eed\u3001\u7ec6\u7c92\u5ea6\u548c\u7ed3\u6784\u5316\u7684\u97f3\u9891-\u89c6\u89c9\u53d9\u4e8b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u7c7b\u4f3c\u7535\u5f71\u5267\u672c\u7684\u8be6\u7ec6\u65f6\u95f4\u6233\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u4f7f\u8bfb\u8005\u80fd\u591f\u9010\u573a\u666f\u751f\u52a8\u60f3\u8c61\u89c6\u9891\u5185\u5bb9\u3002", "method": "1) \u63d0\u51fa\u516d\u7ef4\u7ed3\u6784\u6a21\u5f0f\u521b\u5efa\"\u811a\u672c\u5f0f\"\u63cf\u8ff0\uff1b2) \u6784\u5efa\u9ad8\u8d28\u91cf\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6OmniDCBench\uff1b3) \u63d0\u51faSodaM\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u65f6\u95f4\u611f\u77e5\u7684\u8be6\u7ec6\u63cf\u8ff0\u5e76\u7f13\u89e3\u573a\u666f\u8fb9\u754c\u6a21\u7cca\uff1b4) \u6784\u5efa\u8bad\u7ec3\u6570\u636e\u96c6TimeChatCap-42K\uff1b5) \u5f00\u53d1TimeChat-Captioner-7B\u6a21\u578b\uff0c\u901a\u8fc7SFT\u548cGRPO\u4e0e\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "TimeChat-Captioner-7B\u5728\u5bc6\u96c6\u63cf\u8ff0\u751f\u6210\u4e0a\u8d85\u8d8aGemini-2.5-Pro\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002\u751f\u6210\u7684\u5bc6\u96c6\u63cf\u8ff0\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u80fd\u529b\uff1a\u97f3\u9891-\u89c6\u89c9\u63a8\u7406\uff08DailyOmni\u548cWorldSense\uff09\u548c\u65f6\u95f4\u5b9a\u4f4d\uff08Charades-STA\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684Omni Dense Captioning\u4efb\u52a1\u3001\u57fa\u51c6\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u6a21\u578b\u4e3a\u751f\u6210\u8fde\u7eed\u3001\u7ec6\u7c92\u5ea6\u3001\u7ed3\u6784\u5316\u7684\u97f3\u9891-\u89c6\u89c9\u53d9\u4e8b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5bc6\u96c6\u63cf\u8ff0\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u6240\u6709\u8d44\u6e90\u5c06\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2602.08315", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08315", "abs": "https://arxiv.org/abs/2602.08315", "authors": ["Shunyu Zhao", "Yanfeng Yang", "Shuai Li", "Kenji Fukumizu"], "title": "Fast Flow Matching based Conditional Independence Tests for Causal Discovery", "comment": null, "summary": "Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.", "AI": {"tldr": "\u63d0\u51faFMCIT\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6d41\u5339\u914d\u52a0\u901f\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0c\u663e\u8457\u964d\u4f4e\u56e0\u679c\u53d1\u73b0\u7684\u8ba1\u7b97\u590d\u6742\u5ea6", "motivation": "\u57fa\u4e8e\u7ea6\u675f\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u8981\u52a0\u901f\u5355\u4e2a\u6d4b\u8bd5", "method": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5(FMCIT)\uff0c\u5229\u7528\u6d41\u5339\u914d\u7684\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u6a21\u578b\u5728\u6574\u4e2a\u56e0\u679c\u53d1\u73b0\u8fc7\u7a0b\u4e2d\u53ea\u9700\u8bad\u7ec3\u4e00\u6b21\uff1b\u8fdb\u4e00\u6b65\u5c06FMCIT\u96c6\u6210\u5230\u4e24\u9636\u6bb5\u5f15\u5bfcPC\u9aa8\u67b6\u5b66\u4e60\u6846\u67b6(GPC-FMCIT)\u4e2d\uff0c\u7ed3\u5408\u5feb\u901f\u7b5b\u9009\u548c\u5f15\u5bfc\u7684\u9884\u7b97\u5316\u7cbe\u70bc", "result": "FMCIT\u80fd\u6709\u6548\u63a7\u5236I\u7c7b\u9519\u8bef\uff0c\u5728\u5907\u62e9\u5047\u8bbe\u4e0b\u4fdd\u6301\u9ad8\u6d4b\u8bd5\u529f\u6548\uff0c\u5373\u4f7f\u5728\u9ad8\u7ef4\u6761\u4ef6\u96c6\u4e0b\u4e5f\u8868\u73b0\u826f\u597d\uff1bGPC-FMCIT\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56e0\u679c\u53d1\u73b0\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709CI\u6d4b\u8bd5\u65b9\u6cd5\u548cPC\u53d8\u4f53\u7684\u51c6\u786e\u7387-\u6548\u7387\u6743\u8861", "conclusion": "FMCIT\u901a\u8fc7\u6d41\u5339\u914d\u6280\u672f\u663e\u8457\u52a0\u901f\u4e86\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\uff0cGPC-FMCIT\u6846\u67b6\u5728\u4fdd\u8bc1\u7edf\u8ba1\u529f\u6548\u7684\u540c\u65f6\u9650\u5236\u4e86CI\u67e5\u8be2\u6570\u91cf\uff0c\u4e3a\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.08713", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08713", "abs": "https://arxiv.org/abs/2602.08713", "authors": ["Lachin Naghashyar", "Hunar Batra", "Ashkan Khakzar", "Philip Torr", "Ronald Clark", "Christian Schroeder de Witt", "Constantin Venhoff"], "title": "Towards Understanding Multimodal Fine-Tuning: Spatial Features", "comment": null, "summary": "Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to \"see\". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u9002\u5e94\u8fc7\u7a0b\u8fdb\u884c\u673a\u5236\u6027\u5206\u6790\uff0c\u901a\u8fc7\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5b66\u4e60\"\u770b\"\u7684\u80fd\u529b\uff0c\u8bc6\u522b\u51fa\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u6216\u91cd\u65b0\u5b9a\u5411\u7684\u89c6\u89c9\u504f\u597d\u7279\u5f81\uff0c\u5e76\u8ffd\u8e2a\u8fd9\u4e9b\u7279\u5f81\u5230\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u3002", "motivation": "\u5c3d\u7ba1\u5f53\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bed\u8a00\u4e3b\u5e72\u8868\u793a\u5728\u591a\u6a21\u6001\u8bad\u7ec3\u4e2d\u5982\u4f55\u9002\u5e94\u4ee5\u53ca\u89c6\u89c9\u7279\u5b9a\u80fd\u529b\u4f55\u65f6\u51fa\u73b0\u4ecd\u4e0d\u6e05\u695a\u3002\u9700\u8981\u7406\u89e3\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u83b7\u5f97\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\uff0c\u9694\u79bb\u591a\u6a21\u6001\u5fae\u8c03\u671f\u95f4\u5f15\u5165\u7684\u8868\u5f81\u53d8\u5316\u3002\u8bc6\u522b\u89c6\u89c9\u504f\u597d\u7279\u5f81\uff0c\u901a\u8fc7\u7a7a\u95f4\u63d0\u793a\u7684\u53d7\u63a7\u504f\u79fb\u63ed\u793a\u8fd9\u4e9b\u7279\u5f81\u5982\u4f55\u7f16\u7801\u7a7a\u95f4\u5173\u7cfb\uff0c\u5e76\u8ffd\u8e2a\u8fd9\u4e9b\u7279\u5f81\u7684\u56e0\u679c\u6fc0\u6d3b\u5230\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u3002", "result": "\u53d1\u73b0\u9636\u6bb5\u5f0f\u6a21\u578b\u5dee\u5f02\u6280\u672f\u80fd\u591f\u63ed\u793a\u65f6\u7a7a\u57fa\u7840\u7684\u591a\u6a21\u6001\u7279\u5f81\u4f55\u65f6\u4f55\u5730\u51fa\u73b0\uff0c\u663e\u793a\u89c6\u89c9\u57fa\u7840\u5982\u4f55\u91cd\u5851\u5148\u524d\u4ec5\u57fa\u4e8e\u6587\u672c\u7684\u7279\u5f81\uff0c\u8bc6\u522b\u51fa\u53ef\u9760\u7f16\u7801\u7a7a\u95f4\u5173\u7cfb\u7684\u89c6\u89c9\u504f\u597d\u7279\u5f81\u5b50\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u8bad\u7ec3\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u83b7\u5f97\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\"\u770b\"\u7684\u673a\u5236\u3002"}}
{"id": "2602.08324", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08324", "abs": "https://arxiv.org/abs/2602.08324", "authors": ["Yuntian Tang", "Bohan Jia", "Wenxuan Huang", "Lianyue Zhang", "Jiao Xie", "Wenxi Li", "Wei Li", "Jie Hu", "Xinghao Chen", "Rongrong Ji", "Shaohui Lin"], "title": "Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression", "comment": "15 pages, 7 figures", "summary": "Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\\% token reduction with an accuracy improvement of 0.6\\%, significantly outperforming state-of-the-art (SOTA) methods.", "AI": {"tldr": "Extra-CoT\u6846\u67b6\u901a\u8fc7\u6781\u7aef\u6bd4\u4f8b\u538b\u7f29\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u8ba1\u7b97\u5f00\u9500\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u572873%\u4ee4\u724c\u538b\u7f29\u4e0b\u8fd8\u80fd\u63d0\u53470.6%\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709CoT\u538b\u7f29\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u4f1a\u4e25\u91cd\u635f\u5931\u903b\u8f91\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u6781\u7aef\u538b\u7f29\u6bd4\u4e0b\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u7684\u5feb\u901f\u63a8\u7406\u6846\u67b6\u3002", "method": "1) \u5728\u6570\u5b66CoT\u6570\u636e\u4e0a\u8bad\u7ec3\u4e13\u95e8\u7684\u8bed\u4e49\u4fdd\u6301\u538b\u7f29\u5668\u751f\u6210\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\uff1b2) \u901a\u8fc7\u6df7\u5408\u6bd4\u4f8b\u76d1\u7763\u5fae\u8c03\u8ba9LLM\u9002\u5e94\u4e0d\u540c\u538b\u7f29\u9884\u7b97\uff1b3) \u63d0\u51fa\u7ea6\u675f\u5206\u5c42\u6bd4\u4f8b\u7b56\u7565\u4f18\u5316(CHRPO)\uff0c\u901a\u8fc7\u5206\u5c42\u5956\u52b1\u663e\u5f0f\u6fc0\u52b1\u4f4e\u9884\u7b97\u4e0b\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u4ee5Qwen3-1.7B\u5728MATH-500\u4e0a\u4e3a\u4f8b\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc773%\u7684\u4ee4\u724c\u51cf\u5c11\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u53470.6%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "Extra-CoT\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u6781\u7aef\u6bd4\u4f8bCoT\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08717", "abs": "https://arxiv.org/abs/2602.08717", "authors": ["Farnaz Khun Jush", "Grit Werner", "Mark Klemens", "Matthias Lenga"], "title": "Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images", "comment": "8 pages, 5 figures, 5 tables", "summary": "Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u65e0\u76d1\u7763\u3001\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u8fdb\u884cCT\u548cMR\u56fe\u50cf\u89e3\u5256\u533a\u57df\u81ea\u52a8\u8bc6\u522b\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6d41\u7a0b\uff0c\u5176\u4e2d\u57fa\u4e8e\u5206\u5272\u7684\u89c4\u5219\u7cfb\u7edf\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u89e3\u5256\u533a\u57df\u8bc6\u522b\u4e3b\u8981\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684DICOM\u5143\u6570\u636e\u6216\u76d1\u7763\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u5b9e\u73b0\u5b8c\u5168\u96f6\u6837\u672c\u7684\u89e3\u5256\u533a\u57df\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6d41\u7a0b\uff1a1\uff09\u57fa\u4e8e\u9884\u8bad\u7ec3\u591a\u5668\u5b98\u5206\u5272\u6a21\u578b\u7684\u89c4\u5219\u7cfb\u7edf\uff1b2\uff09\u57fa\u4e8e\u653e\u5c04\u79d1\u533b\u751f\u5b9a\u4e49\u89c4\u5219\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff1b3\uff09\u7ed3\u5408\u89c6\u89c9\u8f93\u5165\u548c\u89e3\u5256\u8bc1\u636e\u7684\u5206\u5272\u611f\u77e5\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728887\u4e2aCT\u548cMR\u626b\u63cf\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u57fa\u4e8e\u5206\u5272\u7684\u89c4\u5219\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u52a0\u6743F1\u5206\u6570\u5206\u522b\u4e3a0.947\uff08CT\uff09\u548c0.914\uff08MR\uff09\uff0c\u5728\u591a\u6a21\u6001\u548c\u975e\u5178\u578b\u626b\u63cf\u8986\u76d6\u4e0b\u5747\u8868\u73b0\u7a33\u5065\u3002\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7279\u5f81\u660e\u663e\u533a\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5206\u5272\u611f\u77e5\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "conclusion": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u7684\u89c4\u5219\u7cfb\u7edf\u80fd\u591f\u5728\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u89e3\u5256\u533a\u57df\u8bc6\u522b\uff0c\u4e3a\u533b\u7597\u5f71\u50cf\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u4e0d\u4f9d\u8d56DICOM\u5143\u6570\u636e\u6216\u76d1\u7763\u5b66\u4e60\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08725", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08725", "abs": "https://arxiv.org/abs/2602.08725", "authors": ["Yongwen Lai", "Chaoqun Wang", "Shaobo Min"], "title": "FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing", "comment": "Accepted by ICASSP 2026", "summary": "Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \\href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.", "AI": {"tldr": "FusionEdit\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5dee\u5f02\u81ea\u52a8\u8bc6\u522b\u7f16\u8f91\u533a\u57df\uff0c\u4f7f\u7528\u8ddd\u79bb\u611f\u77e5\u6f5c\u5728\u878d\u5408\u548c\u7edf\u8ba1\u6ce8\u610f\u529b\u878d\u5408\u5b9e\u73b0\u7cbe\u786e\u53ef\u63a7\u7684\u7f16\u8f91\uff0c\u907f\u514d\u786c\u63a9\u7801\u8fb9\u754c\u5e26\u6765\u7684\u4f2a\u5f71\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u663e\u5f0f\u4e8c\u8fdb\u5236\u63a9\u7801\u7ea6\u675f\u7f16\u8f91\uff0c\u4f46\u786c\u63a9\u7801\u8fb9\u754c\u4f1a\u5f15\u5165\u4f2a\u5f71\u5e76\u964d\u4f4e\u53ef\u7f16\u8f91\u6027\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u53ef\u63a7\u7684\u56fe\u50cf\u7f16\u8f91\u3002", "method": "1) \u901a\u8fc7\u6d4b\u91cf\u6e90\u63d0\u793a\u8bcd\u548c\u76ee\u6807\u63d0\u793a\u8bcd\u8bed\u4e49\u5dee\u5f02\u81ea\u52a8\u8bc6\u522b\u7f16\u8f91\u548c\u4fdd\u7559\u533a\u57df\uff1b2) \u4f7f\u7528\u8ddd\u79bb\u611f\u77e5\u6f5c\u5728\u878d\u5408\u751f\u6210\u8f6f\u63a9\u7801\uff0c\u7ed3\u5408\u603b\u53d8\u5dee\u635f\u5931\u786e\u4fdd\u5e73\u6ed1\u8fc7\u6e21\uff1b3) \u5728DiT\u6ce8\u610f\u529b\u5c42\u4e2d\u4f7f\u7528AdaIN\u8c03\u5236\u8fdb\u884c\u7edf\u8ba1\u6ce8\u610f\u529b\u878d\u5408\uff0c\u589e\u5f3a\u53ef\u7f16\u8f91\u6027\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFusionEdit\u5728\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u7cbe\u786e\u7684\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "FusionEdit\u901a\u8fc7\u8f6f\u63a9\u7801\u751f\u6210\u548c\u7edf\u8ba1\u6ce8\u610f\u529b\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u786c\u63a9\u7801\u8fb9\u754c\u5e26\u6765\u7684\u4f2a\u5f71\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u7f16\u8f91\uff0c\u5728\u7cbe\u786e\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08343", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08343", "abs": "https://arxiv.org/abs/2602.08343", "authors": ["Debajyoti Datta", "Trishala Neeraj", "Bibek Paudel", "Vyom Sharma", "Subhabrata Mukherjee"], "title": "ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection", "comment": "18 pages, 5 figures, 18 tables", "summary": "Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.\n  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.", "AI": {"tldr": "ManifoldKV\u63d0\u51fa\u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f59\u5f26\u76f8\u4f3c\u5ea6\u80fd\u540c\u65f6\u6355\u83b7\u89d2\u5ea6\u548c\u5e45\u5ea6\u4fe1\u606f\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u8868\u73b0\u66f4\u4f18", "motivation": "\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u53d7KV\u7f13\u5b58\u5185\u5b58\u9650\u5236\uff0c\u73b0\u6709\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u51e0\u4f55\u6dd8\u6c70\u65b9\u6cd5\u4f1a\u4e22\u5931\u5e45\u5ea6\u4fe1\u606f\uff0c\u65e0\u6cd5\u533a\u5206\u8bed\u4e49\u91cd\u8981\u7684token", "method": "\u63d0\u51faManifoldKV\u8bad\u7ec3\u81ea\u7531\u8bc4\u5206\u5668\uff0c\u57fa\u4e8etoken\u5230key\u8d28\u5fc3\u7684\u6b27\u6c0f\u8ddd\u79bb\u8fdb\u884c\u6392\u5e8f\uff0c\u6355\u83b7\u89d2\u5ea6\u548c\u5f84\u5411\u504f\u5dee\uff1b\u9488\u5bf964K\u4e0a\u4e0b\u6587\u5f15\u5165WindowedManifoldKV", "result": "\u5728RULER\u57fa\u51c6\u4e0a\uff0c4K-16K\u4e0a\u4e0b\u658720%\u538b\u7f29\u8fbe\u523095.7%\u51c6\u786e\u7387\uff1b\u591a\u952e\u68c0\u7d22\u4efb\u52a1\u63d0\u534715.4\u4e2a\u767e\u5206\u70b9\uff1b64K\u4e0a\u4e0b\u6587\u6062\u590d84.3%\u51c6\u786e\u7387\uff0c\u6bd4KeyDiff\u9ad83.2\u4e2a\u767e\u5206\u70b9", "conclusion": "ManifoldKV\u901a\u8fc7\u6b27\u6c0f\u8ddd\u79bb\u8bc4\u5206\u6709\u6548\u63d0\u5347KV\u7f13\u5b58\u538b\u7f29\u6027\u80fd\uff0c\u4ec5\u97003\u884c\u4ee3\u7801\u5373\u53ef\u8de84\u79cd\u67b6\u6784\u5de5\u4f5c\uff0c\u65e0\u9700\u8c03\u53c2"}}
{"id": "2602.08350", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08350", "abs": "https://arxiv.org/abs/2602.08350", "authors": ["Tal Burla", "Roi Livni"], "title": "All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension", "comment": null, "summary": "We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.\n  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $\u03a9\\left(\\sqrt{\u03b7T/m^{1.5}}\\right)$ for Gradient Descent, where $\u03b7$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(\u03b7T/m)$ and existing lower bounds from previous constructions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u968f\u673a\u51f8\u4f18\u5316\u4e2d\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5668(ERM)\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0bERM\u4f1a\u8fc7\u62df\u5408\uff0c\u5373\u4f7f\u6837\u672c\u91cf\u968f\u7ef4\u5ea6\u7ebf\u6027\u589e\u957f\u3002\u540c\u65f6\u5206\u6790\u4e86\u68af\u5ea6\u4e0b\u964d\u7684\u6cdb\u5316\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u51f8\u4f18\u5316\u4e2d\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5668\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u7279\u522b\u662f\u63a2\u7d22ERM\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u4f1a\u8fc7\u62df\u5408\uff0c\u4ee5\u53ca\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\u3002\u8fd9\u89e3\u51b3\u4e86Feldman\u63d0\u51fa\u7684\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6784\u9020\u5177\u4f53\u7684\u5b9e\u4f8b\u6765\u8bc1\u660e\u5b58\u5728\u6027\u7ed3\u679c\uff1a\u6784\u5efa\u4e86\u4e00\u4e2a\u968f\u673a\u51f8\u4f18\u5316\u95ee\u9898\u5b9e\u4f8b\uff0c\u5176\u4e2d\u6837\u672c\u91cf\u968f\u7ef4\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u5b66\u4e60\u662f\u53ef\u80fd\u7684\uff0c\u4f46ERM\u5f88\u53ef\u80fd\u552f\u4e00\u4e14\u4f1a\u8fc7\u62df\u5408\u3002\u7136\u540e\u57fa\u4e8e\u8fd9\u4e2a\u6784\u9020\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u7684\u6cdb\u5316\u4e0b\u754c\u3002", "result": "1. \u8bc1\u660e\u4e86\u5b58\u5728\u5b9e\u4f8b\u4e2dERM\u4f1a\u8fc7\u62df\u5408\uff0c\u5373\u4f7f\u6837\u672c\u91cf\u968f\u7ef4\u5ea6\u7ebf\u6027\u589e\u957f\uff1b2. \u5c06\u7ed3\u679c\u6269\u5c55\u5230\u8fd1\u4f3cERM\uff1b3. \u4e3a\u68af\u5ea6\u4e0b\u964d\u63d0\u4f9b\u4e86\u65b0\u7684\u6cdb\u5316\u4e0b\u754c\u03a9(\u221a(\u03b7T/m^1.5))\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u73b0\u6709\u4e0a\u4e0b\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5668\u5728\u968f\u673a\u51f8\u4f18\u5316\u4e2d\u53ef\u80fd\u8fc7\u62df\u5408\uff0c\u5373\u4f7f\u6837\u672c\u91cf\u8db3\u591f\u5927\u3002\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e5f\u4f1a\u8fc7\u62df\u5408\uff0c\u8fd9\u4e3a\u7406\u89e3\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6d1e\u89c1\u3002"}}
{"id": "2602.08727", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08727", "abs": "https://arxiv.org/abs/2602.08727", "authors": ["Johannes Thalhammer", "Tina Dorosti", "Sebastian Peterhansl", "Daniela Pfeiffer", "Franz Pfeiffer", "Florian Schaff"], "title": "Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework", "comment": null, "summary": "Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54082D\u548c3D\u6a21\u578b\u4f18\u52bf\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6b20\u91c7\u6837CT\u4f53\u79ef\u4e2d\u53bb\u9664\u4f2a\u5f71\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u5c42\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u6b20\u91c7\u6837CT\u626b\u63cf\u53ef\u4ee5\u51cf\u5c11\u91c7\u96c6\u65f6\u95f4\u548c\u8f90\u5c04\u66b4\u9732\uff0c\u4f46\u4f1a\u5f15\u5165\u4f2a\u5f71\uff0c\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u548c\u8bca\u65ad\u4ef7\u503c\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6709\u6548\u53bb\u9664\u4f2a\u5f71\u53c8\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6df7\u5408\u6846\u67b6\uff1a\u9996\u5148\u4f7f\u75282D U-Net\u5904\u7406\u5355\u4e2aCT\u5207\u7247\u63d0\u53d6\u7279\u5f81\u56fe\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5207\u7247\u7279\u5f81\u56fe\u5806\u53e0\u6210\u4f53\u79ef\uff0c\u8f93\u5165\u52303D\u89e3\u7801\u5668\u4e2d\uff0c\u5229\u7528\u8de8\u5207\u7247\u4e0a\u4e0b\u6587\u4fe1\u606f\u9884\u6d4b\u65e0\u4f2a\u5f71\u76843D CT\u4f53\u79ef\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u51a0\u72b6\u9762\u548c\u77e2\u72b6\u9762\u65b9\u5411\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u5c42\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u9ad8\u8d28\u91cf3D CT\u56fe\u50cf\u540e\u5904\u7406\u63d0\u4f9b\u4e86\u7a33\u5065\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u5e73\u8861\u4e862D\u5904\u7406\u7684\u8ba1\u7b97\u6548\u7387\u548c3D\u5efa\u6a21\u7684\u4f53\u79ef\u4e00\u81f4\u6027\uff0c\u662f\u9ad8\u8d28\u91cf3D CT\u56fe\u50cf\u540e\u5904\u7406\u7684\u7a33\u5065\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2602.08351", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08351", "abs": "https://arxiv.org/abs/2602.08351", "authors": ["Zhiliang Chen", "Alfred Wei Lun Leong", "Shao Yong Ong", "Apivich Hemachandram", "Gregory Kang Ruey Lau", "Chuan-Sheng Foo", "Zhengyuan Liu", "Nancy F. Chen", "Bryan Kian Hsiang Low"], "title": "The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs", "comment": null, "summary": "Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.", "AI": {"tldr": "JoBS\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7f29\u653e\u5b9a\u5f8b\u6027\u80fd\u9884\u6d4b\u5668\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u8054\u5408\u4f18\u5316LLM\u8bad\u7ec3\u7684\u6570\u636e\u548c\u6a21\u578b\u914d\u7f6e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u4e24\u8005\u7684\u56f0\u5883\u3002", "motivation": "LLM\u8bad\u7ec3\u4e2d\u5b58\u5728\u6570\u636e\u914d\u7f6e\u548c\u6a21\u578b\u914d\u7f6e\u7684\"\u9e21\u751f\u86cb\u86cb\u751f\u9e21\"\u56f0\u5883\uff1a\u6700\u4f73\u6570\u636e\u914d\u7f6e\u4f9d\u8d56\u4e8e\u6a21\u578b\u914d\u7f6e\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u4f18\u5316\u6570\u636e\uff0c\u8981\u4e48\u53ea\u4f18\u5316\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "JoBS\u4f7f\u7528\u57fa\u4e8e\u7f29\u653e\u5b9a\u5f8b\u7684\u6027\u80fd\u9884\u6d4b\u5668\u8f85\u52a9\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u5c06\u4f18\u5316\u9884\u7b97\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u4e00\u90e8\u5206\u7528\u4e8e\u4ece\u5c11\u91cf\u8bad\u7ec3\u6b65\u9aa4\u4e2d\u5b66\u4e60LLM\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u5269\u4f59\u9884\u7b97\u5b8c\u5168\u4f7f\u7528\u9884\u6d4b\u5668\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316\u3002", "result": "JoBS\u5728\u76f8\u540c\u4f18\u5316\u9884\u7b97\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u4fdd\u771f\u5ea6\u8d1d\u53f6\u65af\u4f18\u5316\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5355\u72ec\u7684\u6570\u636e\u548c\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u591a\u6837\u5316\u7684LLM\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "JoBS\u901a\u8fc7\u6709\u6548\u644a\u9500\u5b8c\u6574\u8bad\u7ec3\u8fd0\u884c\u7684\u6210\u672c\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u8054\u5408\u4f18\u5316LLM\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u914d\u7f6e\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2602.08730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08730", "abs": "https://arxiv.org/abs/2602.08730", "authors": ["Shanshan Wang", "Ziying Feng", "Xiaozheng Shen", "Xun Yang", "Pichao Wang", "Zhenwei He", "Xingyi Zhang"], "title": "Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation", "comment": null, "summary": "Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA", "AI": {"tldr": "\u63d0\u51faCLIP\u5f15\u5bfc\u5bf9\u9f50(CGA)\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u3001\u5efa\u6a21\u548c\u7f13\u89e3\u7c7b\u522b\u6df7\u6dc6\u6765\u89e3\u51b3\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ec6\u7c92\u5ea6\u573a\u666f", "motivation": "\u73b0\u6709\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u573a\u666f\u4e2d\u56e0\u7c7b\u522b\u95f4\u76f8\u4f3c\u6027\u5bfc\u81f4\u4f2a\u6807\u7b7e\u566a\u58f0\u4e25\u91cd\uff0c\u7279\u522b\u662f\u5b58\u5728\u4e0d\u5bf9\u79f0\u548c\u52a8\u6001\u7684\u7c7b\u522b\u6df7\u6dc6\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u8fd9\u4e9b\u6df7\u6dc6\u6a21\u5f0f", "method": "CGA\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1aMCA\u68c0\u6d4b\u76ee\u6807\u57df\u4e2d\u7684\u65b9\u5411\u6027\u6df7\u6dc6\u5bf9\uff1bMCC\u5229\u7528CLIP\u6784\u5efa\u6df7\u6dc6\u611f\u77e5\u7684\u6587\u672c\u63d0\u793a\uff1bFAM\u5efa\u7acb\u6df7\u6dc6\u5f15\u5bfc\u7684\u7279\u5f81\u5e93\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50CLIP\u548c\u6e90\u6a21\u578b\u7684\u7279\u5f81\u7a7a\u95f4", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCGA\u5728\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u6613\u6df7\u6dc6\u548c\u7ec6\u7c92\u5ea6\u573a\u666f\u4e2d\u53d6\u5f97\u663e\u8457\u63d0\u5347", "conclusion": "\u660e\u786e\u5efa\u6a21\u7c7b\u522b\u95f4\u6df7\u6dc6\u5bf9\u4e8e\u6709\u6548\u7684\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u81f3\u5173\u91cd\u8981\uff0cCGA\u6846\u67b6\u901a\u8fc7\u68c0\u6d4b\u548c\u7f13\u89e3\u6df7\u6dc6\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u57df\u7684\u5206\u7c7b\u6027\u80fd"}}
{"id": "2602.08372", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.08372", "abs": "https://arxiv.org/abs/2602.08372", "authors": ["Yan-Feng Xie", "Yu-Jie Zhang", "Peng Zhao", "Zhi-Hua Zhou"], "title": "Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer", "comment": null, "summary": "We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(\u03b2_1,\u03b2_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6298\u6263\u5230\u52a8\u6001\u7684\u5f52\u7ea6\u6280\u672f\uff0c\u4e3aFTRL\u65b9\u6cd5\u53ca\u5176\u76f8\u5173\u4f18\u5316\u5668\uff08\u5982Adam\uff09\u5728\u975e\u5e73\u7a33\u5728\u7ebf\u5b66\u4e60\u4e2d\u83b7\u5f97\u52a8\u6001\u9057\u61be\u754c\u3002", "motivation": "FTRL\u65b9\u6cd5\u5bf9\u4e8e\u66f2\u7ebf\u635f\u5931\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08\u5982Adam\uff09\u7684\u7406\u89e3\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u52a8\u6001\u9057\u61be\u5206\u6790\u5bf9FTRL\u7684\u63a2\u7d22\u8f83\u5c11\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u5206\u6790FTRL\u5728\u975e\u5e73\u7a33\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\u9057\u61be\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6298\u6263\u5230\u52a8\u6001\u7684\u5f52\u7ea6\u6280\u672f\uff0c\u63d0\u51fa\u6a21\u5757\u5316\u6846\u67b6\u6765\u5206\u6790FTRL\u76f8\u5173\u95ee\u9898\u7684\u52a8\u6001\u9057\u61be\u754c\u3002\u91cd\u70b9\u5173\u6ce8\u7ebf\u6027\u56de\u5f52\u548c\u903b\u8f91\u56de\u5f52\u4e24\u79cd\u4ee3\u8868\u6027\u66f2\u7ebf\u635f\u5931\uff0c\u5e76\u5c06\u8be5\u65b9\u6cd5\u6269\u5c55\u5230Adam\u4f18\u5316\u5668\u7684\u5206\u6790\u3002", "result": "1. \u7b80\u5316\u4e86\u5728\u7ebf\u7ebf\u6027\u56de\u5f52\u6700\u4f18\u52a8\u6001\u9057\u61be\u7684\u73b0\u6709\u8bc1\u660e\uff1b2. \u83b7\u5f97\u4e86\u5728\u7ebf\u903b\u8f91\u56de\u5f52\u7684\u65b0\u52a8\u6001\u9057\u61be\u4fdd\u8bc1\uff1b3. \u5728\u968f\u673a\u3001\u975e\u51f8\u3001\u975e\u5149\u6ed1\u8bbe\u7f6e\u4e0b\u83b7\u5f97\u4e86Adam\u4f18\u5316\u5668\u7684\u6700\u4f18\u6536\u655b\u7387\uff1b4. \u5bf9\u5177\u6709\u4e24\u4e2a\u6298\u6263\u53c2\u6570(\u03b2\u2081,\u03b2\u2082)\u7684Adam\u8fdb\u884c\u4e86\u66f4\u8be6\u7ec6\u5206\u6790\uff0c\u4e3a\u526a\u88c1\u548c\u65e0\u526a\u88c1\u53d8\u4f53\u63d0\u4f9b\u4e86\u65b0\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u5757\u5316\u5f52\u7ea6\u65b9\u6cd5\u4e3aFTRL\u53ca\u76f8\u5173\u4f18\u5316\u5668\u5728\u975e\u5e73\u7a33\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\u9057\u61be\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u4e0d\u4ec5\u7b80\u5316\u4e86\u73b0\u6709\u8bc1\u660e\uff0c\u8fd8\u83b7\u5f97\u4e86\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u7279\u522b\u5728Adam\u4f18\u5316\u5668\u7684\u7406\u8bba\u5206\u6790\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\u3002"}}
{"id": "2602.08735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08735", "abs": "https://arxiv.org/abs/2602.08735", "authors": ["Masanari Oi", "Koki Maeda", "Ryuto Koike", "Daisuke Oba", "Nakamasa Inoue", "Naoaki Okazaki"], "title": "From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models", "comment": null, "summary": "While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.", "AI": {"tldr": "HATCH\u8bad\u7ec3\u6846\u67b6\u901a\u8fc7\u8865\u4e01\u7ea7\u7a7a\u95f4\u5bf9\u9f50\u548c\u52a8\u4f5c-\u7b54\u6848\u63a8\u7406\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8d85\u8d8a\u540c\u89c4\u6a21\u57fa\u7ebf\u6a21\u578b\u5e76\u63a5\u8fd1\u66f4\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u9700\u8981\u6574\u5408\u591a\u4e2a\u89c6\u89d2\u4fe1\u606f\u7684\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u4eba\u7c7b\u901a\u8fc7\u8de8\u89c6\u89d2\u5bf9\u5e94\u548c\u9010\u6b65\u89c6\u89d2\u53d8\u6362\u4e24\u79cd\u673a\u5236\u89e3\u51b3\u6b64\u7c7b\u4efb\u52a1\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4ec5\u90e8\u5206\u4e14\u9690\u5f0f\u5730\u7ed3\u5408\u8fd9\u4e9b\u673a\u5236\uff0c\u7f3a\u4e4f\u5bf9\u4e24\u8005\u7684\u663e\u5f0f\u76d1\u7763\u3002", "method": "\u63d0\u51faHATCH\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u76ee\u6807\uff1a1) \u8865\u4e01\u7ea7\u7a7a\u95f4\u5bf9\u9f50\uff0c\u9f13\u52b1\u4e0d\u540c\u89c6\u89d2\u4e2d\u7a7a\u95f4\u5bf9\u5e94\u533a\u57df\u7684\u8865\u4e01\u8868\u793a\u5bf9\u9f50\uff1b2) \u52a8\u4f5c-\u7b54\u6848\u63a8\u7406\uff0c\u8981\u6c42\u6a21\u578b\u5728\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u524d\u751f\u6210\u663e\u5f0f\u7684\u89c6\u89d2\u8f6c\u6362\u52a8\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHATCH\u59cb\u7ec8\u4ee5\u660e\u663e\u4f18\u52bf\u8d85\u8d8a\u540c\u89c4\u6a21\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e0e\u66f4\u5927\u6a21\u578b\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u5355\u56fe\u50cf\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "HATCH\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u8de8\u89c6\u89d2\u5bf9\u5e94\u548c\u9010\u6b65\u89c6\u89d2\u53d8\u6362\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08376", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08376", "abs": "https://arxiv.org/abs/2602.08376", "authors": ["Xinyu Wang", "Ziyu Zhao", "Peng Lu", "Yu Gu", "Xiao-Wen Chang"], "title": "OJBKQ: Objective-Joint Babai-Klein Quantization", "comment": null, "summary": "Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.", "AI": {"tldr": "OJBKQ\u662f\u4e00\u79cd\u65b0\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u5c06\u6743\u91cd\u91cf\u5316\u5efa\u6a21\u4e3a\u6fc0\u6d3b\u548c\u6743\u91cd\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u5c55\u7684Babai\u6700\u8fd1\u5e73\u9762\u7b97\u6cd5\u548cKlein\u968f\u673a\u5316\u7b97\u6cd5\u89e3\u51b3NP-hard\u7684\u6574\u6570\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\uff0c\u57283-4\u4f4d\u91cf\u5316\u4e0b\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u83b7\u5f97\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\u3002", "motivation": "\u73b0\u6709\u6743\u91cd\u91cf\u5316\u65b9\u6cd5\u591a\u4f9d\u8d56\u542f\u53d1\u5f0f\u76ee\u6807\u548c\u8d2a\u5a6a\u820d\u5165\u7b56\u7565\uff0c\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u4f1a\u5bfc\u81f4\u660e\u663e\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u6765\u51cf\u5c11\u8fd9\u79cd\u6027\u80fd\u635f\u5931\u3002", "method": "\u63d0\u51faOJBKQ\u65b9\u6cd5\uff0c\u5c06\u6743\u91cd\u91cf\u5316\u5efa\u6a21\u4e3a\u6bcf\u5c42\u7684\u6fc0\u6d3b\u548c\u6743\u91cd\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5f62\u6210\u591a\u53f3\u7aef\u76d2\u7ea6\u675f\u6574\u6570\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u3002\u5bf9\u6743\u91cd\u77e9\u9635\u7684\u6bcf\u4e00\u5217\uff0c\u5e94\u7528\u6269\u5c55\u7684Babai\u6700\u8fd1\u5e73\u9762\u7b97\u6cd5\u548c\u6269\u5c55\u7684Klein\u968f\u673a\u5316Babai\u7b97\u6cd5\u6765\u5bfb\u627e\u6700\u5c0f\u6b8b\u5dee\u7684Babai-Klein\u70b9\u4f5c\u4e3a\u5b50\u4f18\u89e3\u3002", "result": "\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cOJBKQ\u57283-4\u4f4d\u91cf\u5316\u4e0b\u76f8\u6bd4\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u4f4e\u7684\u56f0\u60d1\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "OJBKQ\u901a\u8fc7\u5c06\u6743\u91cd\u91cf\u5316\u5efa\u6a21\u4e3a\u8054\u5408\u4f18\u5316\u95ee\u9898\u5e76\u91c7\u7528\u6709\u6548\u7684\u7b97\u6cd5\u6c42\u89e3\uff0c\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u80fd\u591f\u663e\u8457\u51cf\u5c11\u6027\u80fd\u635f\u5931\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u538b\u7f29\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.08749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08749", "abs": "https://arxiv.org/abs/2602.08749", "authors": ["Carmine Zaccagnino", "Fabio Quattrini", "Enis Simsar", "Marta Tintor\u00e9 Gazulla", "Rita Cucchiara", "Alessio Tonioni", "Silvia Cascianelli"], "title": "Shifting the Breaking Point of Flow Matching for Multi-Instance Editing", "comment": null, "summary": "Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInstance-Disentangled Attention\u7684\u65b0\u673a\u5236\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u6d41\u5339\u914d\u6a21\u578b\u5728\u591a\u5b9e\u4f8b\u7f16\u8f91\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5355\u6b21\u4f20\u9012\u7684\u5b9e\u4f8b\u7ea7\u56fe\u50cf\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u56fe\u50cf\u7f16\u8f91\u5668\u4e3b\u8981\u652f\u6301\u5168\u5c40\u6216\u5355\u6307\u4ee4\u7f16\u8f91\uff0c\u5728\u591a\u5b9e\u4f8b\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u5f53\u9700\u8981\u540c\u65f6\u7f16\u8f91\u53c2\u8003\u8f93\u5165\u7684\u591a\u4e2a\u90e8\u5206\u65f6\uff0c\u4f1a\u4ea7\u751f\u8bed\u4e49\u5e72\u6270\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u6e90\u4e8e\u5168\u5c40\u6761\u4ef6\u5316\u7684\u901f\u5ea6\u573a\u548c\u8054\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bfc\u81f4\u5e76\u53d1\u7f16\u8f91\u76f8\u4e92\u7ea0\u7f20\u3002", "method": "\u63d0\u51fa\u4e86Instance-Disentangled Attention\u673a\u5236\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u5212\u5206\u8054\u5408\u6ce8\u610f\u529b\u64cd\u4f5c\uff0c\u5728\u901f\u5ea6\u573a\u4f30\u8ba1\u671f\u95f4\u5f3a\u5236\u7ed1\u5b9a\u5b9e\u4f8b\u7279\u5b9a\u7684\u6587\u672c\u6307\u4ee4\u548c\u7a7a\u95f4\u533a\u57df\uff0c\u4ece\u800c\u5b9e\u73b0\u7f16\u8f91\u7684\u89e3\u8026\u548c\u5c40\u90e8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u7136\u56fe\u50cf\u7f16\u8f91\u548c\u65b0\u5f15\u5165\u7684\u6587\u672c\u5bc6\u96c6\u4fe1\u606f\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4fc3\u8fdb\u7f16\u8f91\u89e3\u8026\u548c\u5c40\u90e8\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684Instance-Disentangled Attention\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5b9e\u4f8b\u7f16\u8f91\u4e2d\u7684\u8bed\u4e49\u5e72\u6270\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5355\u6b21\u4f20\u9012\u7684\u5b9e\u4f8b\u7ea7\u7f16\u8f91\uff0c\u4e3a\u57fa\u4e8e\u6d41\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u591a\u5b9e\u4f8b\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2602.08377", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08377", "abs": "https://arxiv.org/abs/2602.08377", "authors": ["Bilgehan Sel", "Vaishakh Keshava", "Phillip Wallis", "Lukas Rutishauser", "Ming Jin", "Dingcheng Li"], "title": "Reinforcement Learning with Backtracking Feedback", "comment": "NeurIPS 2025", "summary": "Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient \"backtrack by x tokens\" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.", "AI": {"tldr": "RLBF\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u52a8\u6001\u7ea0\u6b63\u81ea\u8eab\u751f\u6210\u9519\u8bef\uff0c\u4f7f\u7528\"\u56de\u6eafx\u4e2atoken\"\u4fe1\u53f7\u6765\u6062\u590d\u5b89\u5168\u8fdd\u89c4\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u5185\u9519\u8bef\u7684\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5bf9\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u5185\u9519\u8bef\u65f6\u5b58\u5728\u5b89\u5168\u8106\u5f31\u6027\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u5982BSAFE\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u52a8\u6001\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u3002", "method": "\u63d0\u51faRLBF\u6846\u67b6\uff1a1) \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u6a21\u578b\u5b66\u4e60\u901a\u8fc7\"\u56de\u6eafx\u4e2atoken\"\u4fe1\u53f7\u52a8\u6001\u7ea0\u6b63\u751f\u6210\u9519\u8bef\uff1b2) \u6539\u8fdb\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u751f\u6210\u7b56\u7565BSAFE+\uff0c\u5728\u539f\u672c\u5b89\u5168\u7684\u6587\u672c\u4e2d\u6ce8\u5165\u8fdd\u89c4\u5185\u5bb9\uff0c\u4e3a\u56de\u6eaf\u673a\u5236\u63d0\u4f9b\u66f4\u597d\u7684\u521d\u59cb\u8bad\u7ec3\u3002", "result": "RLBF\u663e\u8457\u964d\u4f4e\u4e86\u591a\u79cd\u653b\u51fb\u7684\u6210\u529f\u7387\uff0c\u5305\u62ec\u4e2d\u95f4\u586b\u5145\u3001GCG\u653b\u51fb\u548c\u89e3\u7801\u53c2\u6570\u64cd\u7eb5\u7b49\uff0c\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u89c4\u6a21\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5b89\u5168\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u57fa\u7840\u6548\u7528\u3002", "conclusion": "RLBF\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9\u6a21\u578b\u5b66\u4e60\u52a8\u6001\u56de\u6eaf\u7ea0\u6b63\u9519\u8bef\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\u80fd\u529b\uff0c\u5728\u5bf9\u6297\u653b\u51fb\u548c\u5206\u5e03\u5185\u9519\u8bef\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08753", "abs": "https://arxiv.org/abs/2602.08753", "authors": ["Tianyu Sun", "Zhoujie Fu", "Bang Zhang", "Guosheng Lin"], "title": "MVAnimate: Enhancing Character Animation with Multi-View Optimization", "comment": null, "summary": "The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.", "AI": {"tldr": "MVAnimate\u662f\u4e00\u4e2a\u5229\u7528\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u751f\u6210\u9ad8\u8d28\u91cf2D\u548c3D\u89d2\u8272\u52a8\u753b\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u52a8\u753b\u751f\u6210\u7b97\u6cd5\u8f93\u51fa\u8d28\u91cf\u4f4e\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89d2\u8272\u52a8\u753b\u751f\u6210\u9700\u6c42\u6fc0\u589e\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e2D\u62163D\u4eba\u4f53\u59ff\u6001\u5efa\u6a21\u7684\u7b97\u6cd5\u9762\u4e34\u8f93\u51fa\u8d28\u91cf\u4f4e\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u52a8\u753b\u89c6\u9891\u3002", "method": "\u63d0\u51faMVAnimate\u6846\u67b6\uff0c\u57fa\u4e8e\u591a\u89c6\u89d2\u5148\u9a8c\u4fe1\u606f\u5408\u6210\u52a8\u6001\u89d2\u8272\u76842D\u548c3D\u4fe1\u606f\uff0c\u5229\u7528\u591a\u89c6\u89d2\u5148\u9a8c\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u548c\u7a7a\u95f4\u8fde\u8d2f\u7684\u52a8\u753b\u8f93\u51fa\uff0c\u5e76\u4f18\u5316\u76ee\u6807\u89d2\u8272\u7684\u591a\u89c6\u89d2\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u5404\u79cd\u8fd0\u52a8\u6a21\u5f0f\u548c\u5916\u89c2\uff0c\u76f8\u6bd4\u73b0\u6709\u52a8\u753b\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "MVAnimate\u901a\u8fc7\u6574\u5408\u591a\u89c6\u89d2\u4fe1\u606f\u6709\u6548\u63d0\u5347\u4e86\u89d2\u8272\u52a8\u753b\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u8d28\u91cf\u52a8\u753b\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08387", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.08387", "abs": "https://arxiv.org/abs/2602.08387", "authors": ["Max L\u00fcbbering", "Timm Ruland", "Richard Rutmann", "Felix Stollenwerk", "David Fitzek", "Michael Fromm", "Alexander Weber", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim K\u00f6hler", "Mehdi Ali"], "title": "Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research", "comment": null, "summary": "Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.", "AI": {"tldr": "Modalities\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684PyTorch\u539f\u751f\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3LLM\u7814\u7a76\u4e2d\u5927\u89c4\u6a21\u6d88\u878d\u5b9e\u9a8c\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u73b0\u6709\u5de5\u5177\u652f\u6301\u6709\u9650\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u96c6\u6210\u5148\u8fdb\u5e76\u884c\u5316\u7b56\u7565\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u6548\u9884\u8bad\u7ec3\u548c\u7cfb\u7edf\u6027\u6d88\u878d\u3002", "motivation": "\u5f53\u524dLLM\u9884\u8bad\u7ec3\u548c\u7814\u7a76\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5927\u89c4\u6a21\u6d88\u878d\u5b9e\u9a8c\u6d88\u8017\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u73b0\u6709\u5f00\u6e90\u6846\u67b6\u5bf9\u6b64\u63d0\u4f9b\u6709\u9650\u5de5\u5177\u652f\u6301\uff0c\u7814\u7a76\u4eba\u5458\u901a\u5e38\u9700\u8981\u81ea\u5df1\u7f16\u5199\u5305\u88c5\u5668\u548c\u811a\u672c\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u53ef\u91cd\u590d\u6027\u95ee\u9898\u3002", "method": "Modalities\u91c7\u7528\u4e24\u4e2a\u4e3b\u8981\u65b9\u6cd5\uff1a1) \u96c6\u6210\u6700\u5148\u8fdb\u7684\u5e76\u884c\u5316\u7b56\u7565\uff0c\u652f\u6301\u4e07\u4ebftoken\u548c\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u7684\u9ad8\u6548\u9884\u8bad\u7ec3\u548c\u7cfb\u7edf\u6027\u6d88\u878d\uff1b2) \u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u5177\u6709\u58f0\u660e\u5f0f\u3001\u81ea\u5305\u542b\u7684\u914d\u7f6e\uff0c\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u73b0\u6709LLM\u8bad\u7ec3\u6846\u67b6\u96be\u4ee5\u8fbe\u5230\u7684\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\u6c34\u5e73\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u7814\u7a76\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Modalities\u901a\u8fc7\u96c6\u6210\u5148\u8fdb\u5e76\u884c\u5316\u7b56\u7565\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4e3aLLM\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u5927\u89c4\u6a21\u6d88\u878d\u5b9e\u9a8c\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5de5\u5177\u652f\u6301\u95ee\u9898\u3002"}}
{"id": "2602.08775", "categories": ["cs.CV", "cs.CG"], "pdf": "https://arxiv.org/pdf/2602.08775", "abs": "https://arxiv.org/abs/2602.08775", "authors": ["Vineet Kumar Rakesh", "Ahana Bhattacharjee", "Soumya Mazumdar", "Tapas Samanta", "Hemendra Kumar Pandey", "Amitabha Das", "Sarbajit Pal"], "title": "VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars", "comment": null, "summary": "Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\u7684\u786e\u5b9a\u6027CPU\u5bfc\u5411\u8bf4\u8bdd\u5934\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u6559\u80b2\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u8bed\u97f3\u9a71\u52a8\u5934\u50cf\u5408\u6210", "motivation": "\u5f53\u524d\u8bf4\u8bdd\u5934\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56GPU\u6e32\u67d3\u3001\u5927\u8bad\u7ec3\u96c6\u6216\u9ad8\u5bb9\u91cf\u6269\u6563\u6a21\u578b\uff0c\u96be\u4ee5\u5728\u79bb\u7ebf\u6216\u8d44\u6e90\u53d7\u9650\u7684\u6559\u80b2\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7CPU\u53ef\u884c\u65b9\u6848", "method": "\u4f7f\u7528\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\uff1a\u8bed\u97f3\u8f6c\u65f6\u95f4\u5bf9\u9f50\u97f3\u7d20\u6d41\uff0c\u6620\u5c04\u5230\u7d27\u51d1\u89c6\u7d20\u5e93\uff0c\u901a\u8fc7\u53d7\u5420\u9640\u7ecfUrdhva Tiryakbhyam\u542f\u53d1\u7684\u7b26\u53f7\u534f\u540c\u53d1\u97f3\u751f\u6210\u5e73\u6ed1\u89c6\u7d20\u8f68\u8ff9\uff0c\u8f7b\u91cf2D\u6e32\u67d3\u5668\u8fdb\u884cROI\u626d\u66f2\u548c\u5634\u90e8\u5408\u6210", "result": "\u5728\u4ec5CPU\u6267\u884c\u4e0b\u5b9e\u73b0\u4e86\u53ef\u63a5\u53d7\u7684\u5507\u540c\u6b65\u8d28\u91cf\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u548c\u5ef6\u8fdf\uff0c\u652f\u6301\u5728\u4f4e\u7aef\u786c\u4ef6\u4e0a\u8fd0\u884c\u5b9e\u7528\u6559\u80b2\u5934\u50cf", "conclusion": "\u63d0\u51fa\u7684\u7b26\u53f7\u5420\u9640\u8ba1\u7b97\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u6559\u80b2\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8bf4\u8bdd\u5934\u751f\u6210\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387"}}
{"id": "2602.08407", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.08407", "abs": "https://arxiv.org/abs/2602.08407", "authors": ["Richard Serrano", "Baptiste Jeudy", "Charlotte Laclau", "Christine Largeron"], "title": "Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms", "comment": null, "summary": "Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86GAMM\u6846\u67b6\uff0c\u5c06\u7f3a\u5931\u6570\u636e\u673a\u5236\u5206\u7c7b\u6269\u5c55\u5230\u5c5e\u6027\u56fe\u9886\u57df\uff0c\u8003\u8651\u8282\u70b9\u5c5e\u6027\u548c\u56fe\u7ed3\u6784\u7684\u4f9d\u8d56\u6027\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u63d2\u8865\u65b9\u6cd5\u5728\u56fe\u611f\u77e5\u7f3a\u5931\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73", "motivation": "\u5c5e\u6027\u56fe\u4e2d\u7684\u7f3a\u5931\u6570\u636e\u95ee\u9898\u6bd4\u8868\u683c\u6570\u636e\u66f4\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u56fe\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u673a\u5236", "method": "\u63d0\u51faGAMM\uff08\u56fe\u5c5e\u6027\u7f3a\u5931\u673a\u5236\uff09\u6846\u67b6\uff0c\u7cfb\u7edf\u5730\u5c06\u7f3a\u5931\u6982\u7387\u4e0e\u8282\u70b9\u5c5e\u6027\u548c\u5e95\u5c42\u56fe\u7ed3\u6784\u8054\u7cfb\u8d77\u6765\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u7f3a\u5931\u673a\u5236\u5206\u7c7b\u6cd5", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u63d2\u8865\u65b9\u6cd5\u5728\u4f20\u7edf\u7f3a\u5931\u673a\u5236\u4e0a\u6709\u6548\uff0c\u4f46\u5728\u66f4\u73b0\u5b9e\u7684\u56fe\u611f\u77e5\u7f3a\u5931\u573a\u666f\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u56fe\u7ed3\u6784\u6570\u636e\u7684\u7f3a\u5931\u6570\u636e\u5904\u7406\u65b9\u6cd5\uff0cGAMM\u6846\u67b6\u4e3a\u7406\u89e3\u5c5e\u6027\u56fe\u4e2d\u7684\u7f3a\u5931\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.08792", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08792", "abs": "https://arxiv.org/abs/2602.08792", "authors": ["Hao Dong", "Eleni Chatzi", "Olga Fink"], "title": "Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems", "comment": null, "summary": "The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u68c0\u6d4b\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u754c\u9762\u7535\u5f27\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u529b\u6d4b\u91cf\u6570\u636e\uff0c\u901a\u8fc7MultiDeepSAD\u7b97\u6cd5\u548c\u7279\u5b9a\u4f2a\u5f02\u5e38\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7535\u5f27\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u754c\u9762\u7684\u7535\u5f27\u73b0\u8c61\u5bf9\u7535\u6c14\u5316\u94c1\u8def\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u98ce\u9669\uff0c\u5305\u62ec\u52a0\u901f\u63a5\u89e6\u90e8\u4ef6\u78e8\u635f\u3001\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\u548c\u6f5c\u5728\u670d\u52a1\u4e2d\u65ad\u3002\u7136\u800c\uff0c\u7535\u5f27\u68c0\u6d4b\u9762\u4e34\u77ac\u6001\u7279\u6027\u3001\u566a\u58f0\u73af\u5883\u3001\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u4e0e\u5176\u4ed6\u77ac\u6001\u73b0\u8c61\u96be\u4ee5\u533a\u5206\u7b49\u6311\u6218\u3002", "method": "1. \u6784\u5efa\u4e86\u4e24\u4e2a\u5305\u542b\u540c\u6b65\u89c6\u89c9\u548c\u529b\u6d4b\u91cf\u7684\u7535\u5f27\u68c0\u6d4b\u6570\u636e\u96c6\uff1a\u4e00\u4e2a\u6765\u81ea\u745e\u58eb\u8054\u90a6\u94c1\u8def\u516c\u53f8(SBB)\u7684\u771f\u5b9e\u6570\u636e\uff0c\u53e6\u4e00\u4e2a\u6765\u81ea\u516c\u5f00\u89c6\u9891\u548c\u6a21\u62df\u529b\u6570\u636e\u7684\u5408\u6210\u6570\u636e\u96c6\uff1b2. \u63d0\u51fa\u4e86MultiDeepSAD\u7b97\u6cd5\uff0c\u6269\u5c55DeepSAD\u7b97\u6cd5\u4ee5\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u5e76\u91c7\u7528\u65b0\u7684\u635f\u5931\u51fd\u6570\uff1b3. \u9488\u5bf9\u6bcf\u79cd\u6570\u636e\u7c7b\u578b\u8bbe\u8ba1\u4e86\u7279\u5b9a\u7684\u4f2a\u5f02\u5e38\u751f\u6210\u6280\u672f\uff0c\u5982\u56fe\u50cf\u4e2d\u7684\u5408\u6210\u7535\u5f27\u4f2a\u5f71\u548c\u6a21\u62df\u529b\u4e0d\u89c4\u5219\u6027\uff0c\u4ee5\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u9886\u57df\u504f\u79fb\u548c\u771f\u5b9e\u7535\u5f27\u89c2\u6d4b\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u771f\u5b9e\u7535\u5f27\u4e8b\u4ef6\u4e5f\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u529b\u6d4b\u91cf\u6570\u636e\uff0c\u4ee5\u53ca\u521b\u65b0\u7684\u4f2a\u5f02\u5e38\u751f\u6210\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53d7\u7535\u5f13-\u63a5\u89e6\u7f51\u754c\u9762\u7535\u5f27\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u7535\u6c14\u5316\u94c1\u8def\u7cfb\u7edf\u7684\u53ef\u9760\u8fd0\u884c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u7535\u5f27\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08794", "categories": ["cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.08794", "abs": "https://arxiv.org/abs/2602.08794", "authors": ["SII-OpenMOSS Team", ":", "Donghua Yu", "Mingshu Chen", "Qi Chen", "Qi Luo", "Qianyi Wu", "Qinyuan Cheng", "Ruixiao Li", "Tianyi Liang", "Wenbo Zhang", "Wenming Tu", "Xiangyu Peng", "Yang Gao", "Yanru Huo", "Ying Zhu", "Yinze Luo", "Yiyang Zhang", "Yuerong Song", "Zhe Xu", "Zhiyu Zhang", "Chenchen Yang", "Cheng Chang", "Chushu Zhou", "Hanfu Chen", "Hongnan Ma", "Jiaxi Li", "Jingqi Tong", "Junxi Liu", "Ke Chen", "Shimin Li", "Songlin Wang", "Wei Jiang", "Zhaoye Fei", "Zhiyuan Ning", "Chunguo Li", "Chenhui Li", "Ziwei He", "Zengfeng Huang", "Xie Chen", "Xipeng Qiu"], "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "comment": "Technical report for MOVA (open-source video-audio generation model). 38 pages, 10 figures, 22 tables. Project page: https://mosi.cn/models/mova Code: https://github.com/OpenMOSS/MOVA Models: https://huggingface.co/collections/OpenMOSS-Team/mova. Qinyuan Cheng and Tianyi Liang are project leader. Xie Chen and Xipeng Qiu are corresponding authors", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "AI": {"tldr": "MOVA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u540c\u6b65\u7684\u89c6\u542c\u5185\u5bb9\uff0c\u5305\u62ec\u5507\u8bed\u540c\u6b65\u7684\u8bed\u97f3\u3001\u73af\u5883\u611f\u77e5\u97f3\u6548\u548c\u5185\u5bb9\u5bf9\u9f50\u7684\u97f3\u4e50\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5927\u591a\u5ffd\u89c6\u97f3\u9891\u7ec4\u4ef6\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7ea7\u8054\u7ba1\u9053\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u3001\u8bef\u5dee\u7d2f\u79ef\u548c\u8d28\u91cf\u4e0b\u964d\u3002\u867d\u7136Veo 3\u548cSora 2\u7b49\u7cfb\u7edf\u5f3a\u8c03\u540c\u65f6\u751f\u6210\u7684\u4ef7\u503c\uff0c\u4f46\u8054\u5408\u591a\u6a21\u6001\u5efa\u6a21\u9762\u4e34\u67b6\u6784\u3001\u6570\u636e\u548c\u8bad\u7ec3\u6311\u6218\uff0c\u4e14\u73b0\u6709\u7cfb\u7edf\u95ed\u6e90\u9650\u5236\u4e86\u9886\u57df\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u603b\u53c2\u6570\u91cf320\u4ebf\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b180\u4ebf\u53c2\u6570\u3002\u652f\u6301\u56fe\u50cf-\u6587\u672c\u5230\u89c6\u9891-\u97f3\u9891\u751f\u6210\u4efb\u52a1\uff0c\u63d0\u4f9b\u9ad8\u6548\u63a8\u7406\u3001LoRA\u5fae\u8c03\u548c\u63d0\u793a\u589e\u5f3a\u7684\u5b8c\u6574\u4ee3\u7801\u5e93\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5305\u62ec\u903c\u771f\u7684\u5507\u8bed\u540c\u6b65\u8bed\u97f3\u3001\u73af\u5883\u611f\u77e5\u97f3\u6548\u548c\u5185\u5bb9\u5bf9\u9f50\u97f3\u4e50\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\uff0cMOVA\u65e8\u5728\u63a8\u52a8\u97f3\u9891-\u89c6\u9891\u8054\u5408\u751f\u6210\u7814\u7a76\uff0c\u4fc3\u8fdb\u521b\u4f5c\u8005\u793e\u533a\u53d1\u5c55\uff0c\u89e3\u51b3\u5f53\u524d\u95ed\u6e90\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.08797", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08797", "abs": "https://arxiv.org/abs/2602.08797", "authors": ["Jiaming Liu", "Cheng Ding", "Daoqiang Zhang"], "title": "Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework", "comment": "10 pages, 7 figures. Submitted to IEEE Journal of Biomedical and Health Informatics (JBHI)", "summary": "Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u5e08\u751f\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f2a\u6807\u7b7e\u6559\u5e08\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6e10\u8fdb\u8bfe\u7a0b\u5b66\u4e60\uff0c\u7528\u4e8eMRI\u8111\u80bf\u7624\u5206\u5272\uff0c\u5728\u6709\u9650\u6807\u6ce8\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "MRI\u8111\u80bf\u7624\u5206\u5272\u9762\u4e34\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u6570\u636e\u5f02\u8d28\u6027\uff08\u4e0d\u540c\u626b\u63cf\u4eea\u548c\u7ad9\u70b9\uff09\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u5728\u6709\u9650\u76d1\u7763\u4e0b\u9c81\u68d2\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5e08\u751f\u6846\u67b6\uff1a\u6559\u5e08\u6a21\u578b\u751f\u6210\u6982\u7387\u63a9\u7801\u548c\u9010\u50cf\u7d20\u4e0d\u786e\u5b9a\u6027\uff1b\u5b66\u751f\u6a21\u578b\u901a\u8fc7\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6e10\u8fdb\u8bfe\u7a0b\u5b66\u4e60\uff0c\u6309\u56fe\u50cf\u7ea7\u7f6e\u4fe1\u5ea6\u5206\u9636\u6bb5\u5f15\u5165\u672a\u6807\u6ce8\u6570\u636e\uff0c\u4f7f\u7528\u53cc\u635f\u5931\u76ee\u6807\u4ece\u9ad8\u7f6e\u4fe1\u533a\u57df\u5b66\u4e60\u5e76\u9057\u5fd8\u4f4e\u7f6e\u4fe1\u533a\u57df\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u4f18\u5316\u6539\u8fdb\u4f2a\u6807\u7b7e\u8d28\u91cf\u3002", "result": "\u5728BraTS 2021\u4e0a\uff0c\u9a8c\u8bc1DSC\u4ece0.393\uff0810%\u6570\u636e\uff09\u63d0\u5347\u52300.872\uff08100%\uff09\uff0c\u65e9\u671f\u9636\u6bb5\u63d0\u5347\u6700\u5927\uff1b\u6559\u5e08\u6a21\u578bDSC\u8fbe0.922\uff0c\u5b66\u751f\u5728\u80bf\u7624\u4e9a\u533a\uff08\u5982NCR/NET 0.797\u548cEdema 0.980\uff09\u8d85\u8d8a\u6559\u5e08\uff0c\u7279\u522b\u662f\u5728\u6559\u5e08\u5931\u8d25\u7684\u589e\u5f3a\u7c7b\u522b\u4e0a\u6062\u590dDSC 0.620\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u8bfe\u7a0b\u5b66\u4e60\u548c\u9009\u62e9\u6027\u9057\u5fd8\u5728\u6709\u9650\u76d1\u7763\u548c\u566a\u58f0\u4f2a\u6807\u7b7e\u4e0b\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u5206\u5272\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6570\u636e\u6548\u7387\u548c\u6a21\u578b\u6539\u8fdb\u6f5c\u529b\u3002"}}
{"id": "2602.08431", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08431", "abs": "https://arxiv.org/abs/2602.08431", "authors": ["Yingxu Wang", "Kunyu Zhang", "Mengzhu Wang", "Siyang Gao", "Nan Yin"], "title": "USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation", "comment": null, "summary": "SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.", "AI": {"tldr": "USBD\u63d0\u51fa\u901a\u7528\u7ed3\u6784\u57fa\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u65e0\u5173\u7684\u57fa\u6765\u8986\u76d6\u5b8c\u6574\u62d3\u6251\u6a21\u5f0f\u8c31\uff0c\u89e3\u51b3\u6e90\u57dfGNN\u5e73\u6ed1\u5148\u9a8c\u9650\u5236\u5728\u7ed3\u6784\u5dee\u5f02\u5927\u7684\u76ee\u6807\u57df\u6cdb\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SF-GDA\u65b9\u6cd5\u4f9d\u8d56\u6e90\u8bad\u7ec3GNN\u7684\u5e73\u6ed1\u5148\u9a8c\uff0c\u5728\u62d3\u6251\u7ed3\u6784\u663e\u8457\u53d8\u5316\u65f6\uff0c\u6e90\u6a21\u578b\u4f1a\u5c06\u672a\u89c1\u7684\u7ed3\u6784\u6a21\u5f0f\u8bef\u5224\u4e3a\u566a\u58f0\uff0c\u5bfc\u81f4\u57fa\u4e8e\u4f2a\u6807\u7b7e\u7684\u9002\u5e94\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u901a\u7528\u7ed3\u6784\u57fa\u84b8\u998f\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u5c06\u6e90\u6570\u636e\u96c6\u84b8\u998f\u4e3a\u7d27\u51d1\u7ed3\u6784\u57fa\uff0c\u901a\u8fc7\u5f3a\u5236\u539f\u578b\u8de8\u8d8a\u5b8c\u6574\u72c4\u5229\u514b\u96f7\u80fd\u91cf\u8c31\u6765\u6355\u83b7\u591a\u6837\u62d3\u6251\u6a21\u5f0f\u3002\u63a8\u7406\u65f6\u5f15\u5165\u8c31\u611f\u77e5\u96c6\u6210\u673a\u5236\uff0c\u6839\u636e\u76ee\u6807\u56fe\u8c31\u6307\u7eb9\u52a8\u6001\u6fc0\u6d3b\u6700\u4f18\u539f\u578b\u7ec4\u5408\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUSBD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u53d8\u5316\u4e25\u91cd\u7684\u573a\u666f\u4e0b\uff0c\u540c\u65f6\u901a\u8fc7\u5c06\u9002\u5e94\u6210\u672c\u4e0e\u76ee\u6807\u6570\u636e\u89c4\u6a21\u89e3\u8026\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "USBD\u901a\u8fc7\u4ece\u9002\u5e94\u6709\u504f\u6a21\u578b\u8f6c\u5411\u5b66\u4e60\u901a\u7528\u7ed3\u6784\u57fa\uff0c\u89e3\u51b3\u4e86SF-GDA\u5728\u62d3\u6251\u7ed3\u6784\u663e\u8457\u53d8\u5316\u65f6\u7684\u6cdb\u5316\u74f6\u9888\uff0c\u4e3a\u8de8\u56fe\u6570\u636e\u96c6\u7684\u77e5\u8bc6\u8f6c\u79fb\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08820", "abs": "https://arxiv.org/abs/2602.08820", "authors": ["Hao Yang", "Zhiyu Tan", "Jia Gong", "Luozheng Qin", "Hesen Chen", "Xiaomeng Yang", "Yuqing Sun", "Yuetan Lin", "Mengping Yang", "Hao Li"], "title": "Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing", "comment": "Technical Report, Project: https://howellyoung-s.github.io/Omni-Video2-project/", "summary": "We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \\emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.", "AI": {"tldr": "Omni-Video 2\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89c6\u9891\u751f\u6210\u4e0e\u7f16\u8f91\u6a21\u578b\uff0c\u901a\u8fc7\u8fde\u63a5\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528MLLM\u7684\u7406\u89e3\u63a8\u7406\u80fd\u529b\u751f\u6210\u660e\u786e\u7684\u76ee\u6807\u63cf\u8ff0\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u548c\u590d\u6742\u7f16\u8f91\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u4e2d\u590d\u6742\u7ec4\u5408\u6307\u4ee4\u7684\u9075\u5faa\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7406\u89e3\u63a8\u7406\u80fd\u529b\u6765\u66f4\u597d\u5730\u89e3\u91ca\u7528\u6237\u6307\u4ee4\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u7ec4\u5408\u7f16\u8f91\u7684\u6027\u80fd\u3002", "method": "1. \u5229\u7528\u9884\u8bad\u7ec3MLLM\u751f\u6210\u660e\u786e\u7684\u76ee\u6807\u63cf\u8ff0\u6765\u89e3\u91ca\u7528\u6237\u6307\u4ee4\uff1b2. \u5f00\u53d1\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5c06\u591a\u6a21\u6001\u6761\u4ef6\u6807\u8bb0\u6ce8\u5165\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\uff1b3. \u5728\u7cbe\u5fc3\u7b56\u5212\u7684\u8bad\u7ec3\u6570\u636e\u4e0a\u6269\u5c55\u5230140\u4ebf\u53c2\u6570\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728FiVE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u9075\u5faa\u590d\u6742\u7ec4\u5408\u6307\u4ee4\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5728VBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u548c\u5404\u79cd\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "Omni-Video 2\u901a\u8fc7\u8fde\u63a5MLLM\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u548c\u590d\u6742\u7f16\u8f91\uff0c\u5728\u9075\u5faa\u590d\u6742\u6307\u4ee4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08822", "abs": "https://arxiv.org/abs/2602.08822", "authors": ["Yao Pu", "Yiming Shi", "Zhenxi Zhang", "Peixin Yu", "Yitao Zhuang", "Xiang Wang", "Hongzhao Chen", "Jing Cai", "Ge Ren"], "title": "Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications", "comment": null, "summary": "Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4efb\u610f\u5230\u6240\u6709MRI\u5408\u6210\uff0c\u63d0\u9ad8\u9f3b\u54bd\u764c\u653e\u7597\u89c4\u5212\u51c6\u786e\u6027\u3002", "motivation": "MRI\u5bf9\u9f3b\u54bd\u764c\u653e\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u4e34\u5e8a\u4e2d\u5e38\u56e0\u60a3\u8005\u4e0d\u9002\u3001\u626b\u63cf\u65f6\u95f4\u957f\u3001\u6210\u672c\u9ad8\u7b49\u56e0\u7d20\u5bfc\u81f4\u6a21\u6001\u4e0d\u5b8c\u6574\uff0c\u5f71\u54cd\u653e\u7597\u89c4\u5212\u51c6\u786e\u6027\u3002\u4f20\u7edfMRI\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u7279\u5b9a\u6027\u3001\u89e3\u5256\u9002\u5e94\u6027\u6709\u9650\u3001\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u5bf9\u6bd4\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u3002\u4f7f\u7528\u5bf9\u6bd4\u7f16\u7801\u5668\u83b7\u53d6\u6a21\u6001\u4e0d\u53d8\u8868\u793a\uff0c\u57fa\u4e8eCLIP\u7684\u6587\u672c\u4fe1\u606f\u89e3\u7801\u5668\u8fdb\u884c\u8bed\u4e49\u4e00\u81f4\u5408\u6210\uff0c\u652f\u6301\u901a\u8fc7\u5355\u4e00\u7edf\u4e00\u6a21\u578b\u5b9e\u73b0\u4efb\u610f\u5230\u6240\u6709MRI\u5408\u6210\u3002", "result": "\u572813\u4e2a\u673a\u6784\u768440,825\u5f20\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u572826\u4e2a\u5185\u90e8/\u5916\u90e8\u9a8c\u8bc1\u7ad9\u70b9\uff0815,748\u5f20\u56fe\u50cf\uff09\u4e0a\u5b9e\u73b0\u4e00\u81f4\u9ad8\u6027\u80fd\uff08\u5e73\u5747SSIM 0.90\uff0cPSNR 27\uff09\uff0c\u5177\u6709\u4f18\u5f02\u7684\u5408\u6210\u4fdd\u771f\u5ea6\u548c\u5bf9\u566a\u58f0\u53ca\u57df\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002\u7edf\u4e00\u8868\u793a\u8fd8\u589e\u5f3a\u4e86\u653e\u7597\u76f8\u5173\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5206\u5272\uff09\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5229\u7528\u57fa\u7840\u6a21\u578b\u6865\u63a5\u6280\u672f\u5408\u6210\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u63a8\u8fdb\u4e86\u9f3b\u54bd\u764c\u62a4\u7406\u7684\u6570\u5b57\u533b\u5b66\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08461", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08461", "abs": "https://arxiv.org/abs/2602.08461", "authors": ["Liyuan Xu", "Bijan Mazaheri"], "title": "Estimating Aleatoric Uncertainty in the Causal Treatment Effect", "comment": null, "summary": "Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6cbb\u7597\u6548\u5e94\u65b9\u5dee(VTE)\u548c\u6761\u4ef6\u6cbb\u7597\u6548\u5e94\u65b9\u5dee(CVTE)\u4f5c\u4e3a\u8861\u91cf\u6cbb\u7597\u53cd\u5e94\u4e2d\u56fa\u6709\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u7684\u81ea\u7136\u5ea6\u91cf\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u91cf\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u53ef\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u8bc6\u522b\uff0c\u5e76\u5f00\u53d1\u4e86\u975e\u53c2\u6570\u6838\u4f30\u8ba1\u5668\u3002", "motivation": "\u5148\u524d\u56e0\u679c\u63a8\u65ad\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6cbb\u7597\u6548\u5e94\u7684\u5e73\u5747\u503c\u548c\u6761\u4ef6\u5e73\u5747\u503c\uff0c\u5bf9\u4e2a\u4f53\u6cbb\u7597\u53cd\u5e94\u7684\u53d8\u5f02\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5173\u6ce8\u8f83\u5c11\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u91cf\u5316\u6cbb\u7597\u53cd\u5e94\u4e2d\u56fa\u6709\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u6cbb\u7597\u6548\u5e94\u65b9\u5dee(VTE)\u548c\u6761\u4ef6\u6cbb\u7597\u6548\u5e94\u65b9\u5dee(CVTE)\u4f5c\u4e3a\u8861\u91cf\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u7684\u5ea6\u91cf\uff1b\u8bc1\u660e\u8fd9\u4e9b\u91cf\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u53ef\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u8bc6\u522b\uff0c\u5373\u4f7f\u5b58\u5728\u672a\u89c2\u6d4b\u6df7\u6742\u56e0\u7d20\uff1b\u63d0\u51fa\u975e\u53c2\u6570\u6838\u57fa\u4f30\u8ba1\u5668\u6765\u4f30\u8ba1VTE\u548cCVTE\uff1b\u8fdb\u884c\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4f30\u8ba1\u5668\u7684\u6536\u655b\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u4f30\u8ba1\u5668\u7684\u6536\u655b\u6027\uff1b\u5728\u5408\u6210\u548c\u534a\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u4e0e\u6734\u7d20\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u56e0\u679c\u63a8\u65ad\u4e2d\u5bf9\u6cbb\u7597\u53cd\u5e94\u53d8\u5f02\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684VTE\u548cCVTE\u5ea6\u91cf\u53ca\u5176\u4f30\u8ba1\u65b9\u6cd5\u4e3a\u7406\u89e3\u4e2a\u4f53\u6cbb\u7597\u53cd\u5e94\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2602.08828", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08828", "abs": "https://arxiv.org/abs/2602.08828", "authors": ["Hao Tan", "Jun Lan", "Senyuan Shi", "Zichang Tan", "Zijian Yu", "Huijia Zhu", "Weiqiang Wang", "Jun Wan", "Zhen Lei"], "title": "VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning", "comment": "Project: https://github.com/EricTan7/VideoVeritas", "summary": "The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.", "AI": {"tldr": "VideoVeritas\u6846\u67b6\u901a\u8fc7\u8054\u5408\u504f\u597d\u5bf9\u9f50\u548c\u611f\u77e5\u9884\u6587\u672c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u611f\u77e5\u4e0e\u4e8b\u5b9e\u63a8\u7406\uff0c\u63d0\u5347\u89c6\u9891\u751f\u6210\u5185\u5bb9\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6MintVid\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5e73\u8861\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u80fd\u529b\u7684\u63d0\u5347\u5e26\u6765\u4e86\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63a8\u7406\u80fd\u529b\u5f3a\uff0c\u4f46\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faVideoVeritas\u6846\u67b6\uff0c\u6574\u5408\u7ec6\u7c92\u5ea6\u611f\u77e5\u548c\u4e8b\u5b9e\u63a8\u7406\u3002\u5f15\u5165\u8054\u5408\u504f\u597d\u5bf9\u9f50\u548c\u611f\u77e5\u9884\u6587\u672c\u5f3a\u5316\u5b66\u4e60\uff08PPRL\uff09\uff0c\u901a\u8fc7\u901a\u7528\u65f6\u7a7a\u5b9a\u4f4d\u548c\u81ea\u76d1\u7763\u7269\u4f53\u8ba1\u6570\u7b49\u611f\u77e5\u9884\u4efb\u52a1\u6765\u589e\u5f3a\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u503e\u5411\u4e8e\u504f\u5411\u8868\u9762\u63a8\u7406\u6216\u673a\u68b0\u5206\u6790\uff0c\u800cVideoVeritas\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5e73\u8861\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "VideoVeritas\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u5e73\u8861\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7MintVid\u6570\u636e\u96c6\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u7a33\u5065\u8bc4\u4f30\u3002"}}
{"id": "2602.08467", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08467", "abs": "https://arxiv.org/abs/2602.08467", "authors": ["Charalampos Shimillas", "Kleanthis Malialis", "Konstantinos Fokianos", "Marios M. Polycarpou"], "title": "Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization", "comment": null, "summary": "Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.", "AI": {"tldr": "\u63d0\u51faALoRa-T\u6a21\u578b\u548cALoRa-Loc\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u6b63\u5219\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u95f4\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u8bca\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u6d1e\u5bdf\uff0c\u7279\u522b\u662f\u5f02\u5e38\u5b9a\u4f4d\u8fd9\u4e00\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002\u9700\u8981\u7814\u7a76Transformer\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5e94\u7528\uff0c\u5efa\u7acb\u4e0e\u7edf\u8ba1\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u7684\u7406\u8bba\u8054\u7cfb\u3002", "method": "\u63d0\u51faAttention Low-Rank Transformer (ALoRa-T)\u6a21\u578b\uff0c\u5bf9\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5e94\u7528\u4f4e\u79e9\u6b63\u5219\u5316\uff1b\u5f15\u5165Attention Low-Rank\u5206\u6570\u6355\u6349\u5f02\u5e38\u7684\u65f6\u95f4\u7279\u5f81\uff1b\u63d0\u51faALoRa-Loc\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u65f6\u95f4\u5e8f\u5217\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\u5c06\u5f02\u5e38\u5173\u8054\u5230\u7279\u5b9a\u53d8\u91cf\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u771f\u5b9e\u6570\u636e\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u6d1e\u5bdfTransformer\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u7684ALoRa-T\u6a21\u578b\u548cALoRa-Loc\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u95ee\u9898\uff0c\u4e3a\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u65b9\u6848\u3002"}}
{"id": "2602.08470", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08470", "abs": "https://arxiv.org/abs/2602.08470", "authors": ["Kaizheng Wang", "Ghifari Adam Faza", "Fabio Cuzzolin", "Siu Lun Chau", "David Moens", "Hans Hallez"], "title": "Learning Credal Ensembles via Distributionally Robust Optimization", "comment": "32 pages", "summary": "Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.", "AI": {"tldr": "CreDRO\uff1a\u4e00\u79cd\u65b0\u7684\u53ef\u4fe1\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u5b66\u4e60\u6a21\u578b\u96c6\u5408\uff0c\u6355\u6349\u8bad\u7ec3\u968f\u673a\u6027\u548c\u5206\u5e03\u504f\u79fb\u5f15\u8d77\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5728OOD\u68c0\u6d4b\u548c\u533b\u7597\u9009\u62e9\u6027\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u53ef\u4fe1\u9884\u6d4b\u5668\u4e3b\u8981\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5b9a\u4e49\u4e3a\u8bad\u7ec3\u521d\u59cb\u5316\u968f\u673a\u6027\u5f15\u8d77\u7684\u5206\u6b67\uff0c\u8fd9\u4e3b\u8981\u53cd\u6620\u4e86\u5bf9\u4f18\u5316\u968f\u673a\u6027\u7684\u654f\u611f\u6027\uff0c\u800c\u975e\u66f4\u6df1\u5c42\u6b21\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u3002\u9700\u8981\u6355\u6349\u7531\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4\u6f5c\u5728\u5206\u5e03\u504f\u79fb\u5f15\u8d77\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51faCreDRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u5b66\u4e60\u4e00\u4e2a\u6a21\u578b\u96c6\u5408\uff0c\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5b9a\u4e49\u4e3a\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u4e4b\u95f4i.i.d.\u5047\u8bbe\u7684\u4e0d\u540c\u677e\u5f1b\u7a0b\u5ea6\u4e0b\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u7684\u5206\u6b67\u3002\u8fd9\u79cd\u65b9\u6cd5\u540c\u65f6\u6355\u6349\u8bad\u7ec3\u968f\u673a\u6027\u548c\u6709\u610f\u4e49\u7684\u5206\u6b67\u3002", "result": "CreDRO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u53ef\u4fe1\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u68c0\u6d4b\u4efb\u52a1\u548c\u533b\u7597\u5e94\u7528\u4e2d\u7684\u9009\u62e9\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5b9a\u4e49\u4e3a\u6a21\u578b\u5728i.i.d.\u5047\u8bbe\u4e0d\u540c\u677e\u5f1b\u7a0b\u5ea6\u4e0b\u7684\u5206\u6b67\uff0cCreDRO\u80fd\u591f\u66f4\u5168\u9762\u5730\u6355\u6349\u9884\u6d4b\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5305\u62ec\u8bad\u7ec3\u968f\u673a\u6027\u548c\u5206\u5e03\u504f\u79fb\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2602.08861", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08861", "abs": "https://arxiv.org/abs/2602.08861", "authors": ["Xiangtian Zheng", "Zishuo Wang", "Yuxin Peng"], "title": "TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models", "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.", "AI": {"tldr": "TiFRe\u662f\u4e00\u4e2a\u6587\u672c\u5f15\u5bfc\u7684\u89c6\u9891\u5e27\u7f29\u51cf\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u5173\u952e\u5e27\u5e76\u5408\u5e76\u975e\u5173\u952e\u5e27\u4fe1\u606f\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5927\u91cf\u89c6\u9891\u5e27\u65f6\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u4f20\u7edf\u56fa\u5b9a\u5e27\u7387\u7684\u5173\u952e\u5e27\u9009\u62e9\u65b9\u6cd5\u4f1a\u4e22\u5931\u975e\u5173\u952e\u5e27\u7684\u91cd\u8981\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faTiFRe\u6846\u67b6\uff1a1) \u6587\u672c\u5f15\u5bfc\u5e27\u91c7\u6837(TFS)\uff1a\u4f7f\u7528LLM\u6839\u636e\u7528\u6237\u8f93\u5165\u751f\u6210CLIP\u98ce\u683c\u63d0\u793a\uff0c\u901a\u8fc7CLIP\u7f16\u7801\u5668\u8ba1\u7b97\u63d0\u793a\u4e0e\u6bcf\u5e27\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6765\u9009\u62e9\u5173\u952e\u5e27\uff1b2) \u5e27\u5339\u914d\u4e0e\u5408\u5e76(FMM)\uff1a\u5c06\u975e\u5173\u952e\u5e27\u4fe1\u606f\u6574\u5408\u5230\u9009\u5b9a\u7684\u5173\u952e\u5e27\u4e2d\uff0c\u6700\u5c0f\u5316\u4fe1\u606f\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTiFRe\u80fd\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u5728\u89c6\u9891\u8bed\u8a00\u4efb\u52a1\u4e0a\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "TiFRe\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u7684\u667a\u80fd\u5e27\u9009\u62e9\u548c\u5e27\u5408\u5e76\u673a\u5236\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u9891\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u4e3a\u89c6\u9891MLLMs\u7684\u9ad8\u6548\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08909", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08909", "abs": "https://arxiv.org/abs/2602.08909", "authors": ["Zhendong Wang", "Cihan Ruan", "Jingchuan Xiao", "Chuqing Shi", "Wei Jiang", "Wei Wang", "Wenjie Liu", "Nam Ling"], "title": "Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit", "comment": null, "summary": "We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e863D\u9ad8\u65af\u6e85\u5c04\u4f18\u5316\u4e2d\u51fa\u73b0\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u6e32\u67d3\u6700\u4f18\u53c2\u8003\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5bc6\u5ea6\u5206\u5c42\u5bf9\u53c2\u6570\u53ef\u5b66\u4e60\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a763D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u5728\u591a\u89c6\u89d2\u4f18\u5316\u4e2d\u5f62\u6210\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u7406\u89e3\u8fd9\u4e9b\u6e32\u67d3\u6700\u4f18\u53c2\u8003\uff08RORs\uff09\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u63a2\u7d22\u51b3\u5b9a\u8fd9\u4e9b\u53c2\u6570\u7684\u56e0\u7d20\uff0c\u7279\u522b\u662f\u5bc6\u5ea6\u5206\u5c42\u5982\u4f55\u5f71\u54cd\u53c2\u6570\u7684\u53ef\u5b66\u4e60\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790RORs\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u6027\u63a2\u9488\u8bad\u7ec3\u9884\u6d4b\u5668\u4ece\u70b9\u4e91\u91cd\u5efaRORs\uff0c\u5e94\u7528\u65b9\u5dee\u5206\u89e3\u5206\u6790\u5bc6\u5ea6\u5206\u5c42\u5bf9\u53c2\u6570\u8026\u5408\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u5bc6\u5ea6\u611f\u77e5\u7b56\u7565\u3002", "result": "\u53d1\u73b0RORs\u5177\u6709\u7a33\u5b9a\u7684\u7edf\u8ba1\u6a21\u5f0f\uff1a\u6df7\u5408\u7ed3\u6784\u7684\u5c3a\u5ea6\u548c\u53cc\u5cf0\u8f90\u5c04\u5206\u5e03\uff1b\u5bc6\u5ea6\u5206\u5c42\u5bfc\u81f4\u53c2\u6570\u53ef\u5b66\u4e60\u6027\u5dee\u5f02\uff1a\u5bc6\u96c6\u533a\u57df\u53c2\u6570\u4e0e\u51e0\u4f55\u76f8\u5173\u53ef\u9884\u6d4b\uff0c\u7a00\u758f\u533a\u57df\u53c2\u6570\u53d7\u53ef\u89c1\u6027\u5f02\u8d28\u6027\u5f71\u54cd\u96be\u4ee5\u9884\u6d4b\uff1b\u65b9\u5dee\u5206\u89e3\u663e\u793a\u7a00\u758f\u533a\u57df\u51e0\u4f55\u4e0e\u5916\u89c2\u53c2\u6570\u5b58\u5728\u534f\u65b9\u5dee\u4e3b\u5bfc\u7684\u8026\u5408\u3002", "conclusion": "RORs\u5177\u6709\u53cc\u91cd\u7279\u6027\uff1a\u5728\u5bc6\u96c6\u533a\u57df\u8868\u73b0\u4e3a\u51e0\u4f55\u57fa\u5143\uff08\u70b9\u4e91\u8db3\u591f\uff09\uff0c\u5728\u7a00\u758f\u533a\u57df\u8868\u73b0\u4e3a\u89c6\u56fe\u5408\u6210\u57fa\u5143\uff08\u9700\u8981\u591a\u89c6\u89d2\u7ea6\u675f\uff09\uff1b\u5bc6\u5ea6\u611f\u77e5\u7b56\u7565\u80fd\u63d0\u9ad8\u8bad\u7ec3\u9c81\u68d2\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u5e73\u8861\u524d\u9988\u9884\u6d4b\u548c\u6e32\u67d3\u4f18\u5316\u7684\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u67b6\u6784\u542f\u793a\u3002"}}
{"id": "2602.08489", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08489", "abs": "https://arxiv.org/abs/2602.08489", "authors": ["Hyunseok Lee", "Soheil Abbasloo", "Jihoon Tack", "Jinwoo Shin"], "title": "Beyond Correctness: Learning Robust Reasoning via Transfer", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.", "AI": {"tldr": "RLTR\uff08\u5e26\u53ef\u8f6c\u79fb\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u901a\u8fc7\u6d4b\u8bd5\u90e8\u5206\u63a8\u7406\u524d\u7f00\u80fd\u5426\u6307\u5bfc\u5176\u4ed6\u6a21\u578b\u5f97\u5230\u6b63\u786e\u7b54\u6848\uff0c\u63d0\u5347LLM\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u76f8\u6bd4RLVR\u5728\u66f4\u5c11\u8bad\u7ec3\u6b65\u6570\u4e0b\u8fbe\u5230\u76f8\u5f53\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u53ea\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u63a8\u7406\u8fc7\u7a0b\u672c\u8eab\u7684\u9c81\u68d2\u6027\u3002\u4f5c\u8005\u8ba4\u4e3a\u7a33\u5065\u7684\u63a8\u7406\u5e94\u8be5\u80fd\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u8f6c\u79fb\u548c\u5ef6\u7eed\uff0c\u5373\u63a8\u7406\u5e94\u5177\u5907\u53ef\u8f6c\u79fb\u6027\u3002", "method": "\u63d0\u51faRLTR\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u8f6c\u79fb\u5956\u52b1\u6765\u64cd\u4f5c\u5316\u9c81\u68d2\u6027\uff1a\u6d4b\u8bd5\u4e00\u4e2a\u6a21\u578b\u7684\u90e8\u5206\u63a8\u7406\u524d\u7f00\u80fd\u5426\u6307\u5bfc\u53e6\u4e00\u4e2a\u72ec\u7acb\u6a21\u578b\u5f97\u5230\u6b63\u786e\u7b54\u6848\u3002\u8fd9\u9f13\u52b1LLM\u4ea7\u751f\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u4e14\u771f\u6b63\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u3002", "result": "\u5728MATH500\u4e0a\uff0cRLTR\u76f8\u6bd4RLVR\u83b7\u5f97+3.6%\u7684Maj@64\u63d0\u5347\uff0c\u4e14\u4ec5\u7528\u7ea62.5\u500d\u66f4\u5c11\u7684\u8bad\u7ec3\u6b65\u6570\u5c31\u8fbe\u5230RLVR\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u91c7\u6837\u4e00\u81f4\u6027\u548c\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u3002", "conclusion": "RLTR\u901a\u8fc7\u53ef\u8f6c\u79fb\u5956\u52b1\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86LLM\u63a8\u7406\u7684\u9c81\u68d2\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u4ea7\u751f\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86RLVR\u53ea\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u800c\u5ffd\u7565\u63a8\u7406\u8fc7\u7a0b\u7a33\u5065\u6027\u7684\u95ee\u9898\u3002"}}
{"id": "2602.08958", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.08958", "abs": "https://arxiv.org/abs/2602.08958", "authors": ["Weihan Luo", "Lily Goli", "Sherwin Bahmani", "Felix Taubner", "Andrea Tagliasacchi", "David B. Lindell"], "title": "Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields", "comment": "Project page: https://weihanluo.ca/growflow/", "summary": "Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u690d\u7269\u751f\u957f\u5efa\u6a21\u76843D\u9ad8\u65af\u6d41\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u53d8\u5316\u7684\u9ad8\u65af\u53c2\u6570\u5bfc\u6570\u6765\u6a21\u62df\u975e\u7ebf\u6027\u8fde\u7eed\u751f\u957f\u52a8\u6001\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u690d\u7269\u751f\u957f\u5efa\u6a21\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u690d\u7269\u4f1a\u968f\u65f6\u95f4\u751f\u6210\u65b0\u51e0\u4f55\u7ed3\u6784\uff08\u6269\u5f20\u3001\u5206\u679d\u3001\u5206\u5316\uff09\uff0c\u800c\u73b0\u6709\u7684\u8fd0\u52a8\u5efa\u6a21\u6280\u672f\uff08\u5982\u53d8\u5f62\u573a\u65e0\u6cd5\u5f15\u5165\u65b0\u51e0\u4f55\uff0c4D\u9ad8\u65af\u6e85\u5c04\u9650\u5236\u7ebf\u6027\u8f68\u8ff9\uff09\u4e0d\u9002\u7528\u4e8e\u8fd9\u79cd\u95ee\u9898\u8bbe\u7f6e\u3002", "method": "\u5f15\u51653D\u9ad8\u65af\u6d41\u573a\u8868\u793a\uff0c\u5c06\u690d\u7269\u751f\u957f\u5efa\u6a21\u4e3a\u9ad8\u65af\u53c2\u6570\uff08\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u3001\u65b9\u5411\u3001\u989c\u8272\u3001\u4e0d\u900f\u660e\u5ea6\uff09\u7684\u65f6\u95f4\u53d8\u5316\u5bfc\u6570\u3002\u901a\u8fc7\u91cd\u5efa\u6210\u719f\u690d\u7269\u5e76\u5b66\u4e60\u53cd\u5411\u751f\u957f\u8fc7\u7a0b\u6765\u521d\u59cb\u5316\u8db3\u591f\u7684\u9ad8\u65af\u57fa\u5143\uff0c\u6a21\u62df\u690d\u7269\u7684\u53d1\u80b2\u5386\u53f2\u3002", "result": "\u5728\u591a\u89c6\u89d2\u690d\u7269\u751f\u957f\u65f6\u5e8f\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u51e0\u4f55\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4e3a\u751f\u957f\u4e2d3D\u7ed3\u6784\u7684\u5916\u89c2\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u76843D\u9ad8\u65af\u6d41\u573a\u8868\u793a\u80fd\u591f\u6709\u6548\u5efa\u6a21\u690d\u7269\u751f\u957f\u7684\u975e\u7ebf\u6027\u8fde\u7eed\u52a8\u6001\uff0c\u901a\u8fc7\u53cd\u5411\u751f\u957f\u521d\u59cb\u5316\u7b56\u7565\u89e3\u51b3\u4e86\u65b0\u51e0\u4f55\u751f\u6210\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u690d\u7269\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08499", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08499", "abs": "https://arxiv.org/abs/2602.08499", "authors": ["Xiaodong Lu", "Xiaohan Wang", "Jiajun Chai", "Guojun Yin", "Wei Lin", "Zhijun Chen", "Yu Luo", "Fuzhen Zhuang", "Yikun Ban", "Deqing Wang"], "title": "Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9RLVR\uff08\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u7684\u795e\u7ecf\u8c03\u5ea6\u6846\u67b6\uff0c\u5c06rollout\u8c03\u5ea6\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u4ef7\u503crollout\u6765\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728rollout\u4f7f\u7528\u4e0a\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u5bf9\u6bcf\u4e2aprompt\u5185\u8d28\u91cf\u5404\u5f02\u7684\u54cd\u5e94\u8fdb\u884c\u65e0\u5dee\u522b\u5904\u7406\uff1b2\uff09\u5386\u53f2rollout\u5728\u5355\u6b21\u4f7f\u7528\u540e\u5373\u88ab\u4e22\u5f03\u3002\u8fd9\u5bfc\u81f4\u76d1\u7763\u4fe1\u53f7\u566a\u58f0\u5927\u3001\u6837\u672c\u6548\u7387\u4f4e\u3001\u7b56\u7565\u66f4\u65b0\u6b21\u4f18\u3002", "method": "\u5c06RLVR\u4e2d\u7684rollout\u8c03\u5ea6\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u795e\u7ecf\u8c03\u5ea6\u6846\u67b6\u3002\u6bcf\u4e2arollout\u88ab\u89c6\u4e3a\u4e00\u4e2a\"\u81c2\"\uff0c\u5176\u5956\u52b1\u5b9a\u4e49\u4e3a\u8fde\u7eed\u4f18\u5316\u6b65\u9aa4\u95f4\u7684\u6027\u80fd\u589e\u76ca\u63d0\u5347\u3002\u8be5\u8c03\u5ea6\u5668\u652f\u6301\u566a\u58f0\u611f\u77e5\u7684\u7ec4\u5185\u9009\u62e9\u548c\u5386\u53f2rollout\u7684\u81ea\u9002\u5e94\u5168\u5c40\u91cd\u7528\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2aRLVR\u4f18\u5316\u65b9\u6cd5\u4e2d\u5747\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u548c\u8bad\u7ec3\u6548\u7387\u6539\u8fdb\u3002\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u754c\u9650\uff0c\u5e76\u8bc1\u660e\u6269\u5927rollout\u7f13\u51b2\u533a\u80fd\u63d0\u9ad8\u53ef\u8fbe\u5230\u7684\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u63d0\u51fa\u7684\u795e\u7ecf\u8c03\u5ea6\u6846\u67b6\u901a\u8fc7\u5c06rollout\u8c03\u5ea6\u5efa\u6a21\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RLVR\u65b9\u6cd5\u4e2drollout\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.08961", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08961", "abs": "https://arxiv.org/abs/2602.08961", "authors": ["Ruijie Zhu", "Jiahao Lu", "Wenbo Hu", "Xiaoguang Han", "Jianfei Cai", "Ying Shan", "Chuanxia Zheng"], "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE", "comment": "Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "summary": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page", "AI": {"tldr": "MotionCrafter\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u8054\u5408\u91cd\u5efa4D\u51e0\u4f55\u5e76\u4f30\u8ba1\u5bc6\u96c6\u8fd0\u52a8\uff0c\u901a\u8fc7\u65b0\u7684\u8054\u5408\u8868\u793a\u548c4D VAE\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5f3a\u52363D\u503c\u548c\u6f5c\u5728\u53d8\u91cf\u4e0eRGB VAE\u6f5c\u5728\u53d8\u91cf\u4e25\u683c\u5bf9\u9f50\uff0c\u5c3d\u7ba1\u5b83\u4eec\u7684\u5206\u5e03\u672c\u8d28\u4e0d\u540c\uff0c\u8fd9\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5b66\u4e604D\u51e0\u4f55\u548c\u8fd0\u52a8\u7684\u8054\u5408\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u5bc6\u96c63D\u70b9\u56fe\u548c3D\u573a\u666f\u6d41\u5728\u5171\u4eab\u5750\u6807\u7cfb\u4e2d\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u76844D VAE\u6765\u6709\u6548\u5b66\u4e60\u8fd9\u79cd\u8868\u793a\u3002\u91c7\u7528\u65b0\u7684\u6570\u636e\u5f52\u4e00\u5316\u548cVAE\u8bad\u7ec3\u7b56\u7565\uff0c\u66f4\u597d\u5730\u4f20\u9012\u6269\u6563\u5148\u9a8c\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMotionCrafter\u5728\u51e0\u4f55\u91cd\u5efa\u548c\u5bc6\u96c6\u573a\u666f\u6d41\u4f30\u8ba1\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u51e0\u4f55\u91cd\u5efa\u63d0\u534738.64%\uff0c\u8fd0\u52a8\u91cd\u5efa\u63d0\u534725.0%\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u540e\u4f18\u5316\u3002", "conclusion": "MotionCrafter\u901a\u8fc7\u521b\u65b0\u7684\u8054\u5408\u8868\u793a\u548c4D VAE\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5148\u524d\u65b9\u6cd5\u4e2d\u5206\u5e03\u5bf9\u9f50\u4e0d\u5f53\u7684\u95ee\u9898\uff0c\u4e3a\u5355\u76ee\u89c6\u9891\u76844D\u91cd\u5efa\u548c\u8fd0\u52a8\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08519", "abs": "https://arxiv.org/abs/2602.08519", "authors": ["Yunhui Liu", "Pengyu Qiu", "Yu Xing", "Yongchao Liu", "Peng Du", "Chuntao Hong", "Jiajun Zheng", "Tao Zheng", "Tieke He"], "title": "Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering", "comment": null, "summary": "Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).", "AI": {"tldr": "PyAGC\u662f\u4e00\u4e2a\u9762\u5411\u5de5\u4e1a\u90e8\u7f72\u7684\u56fe\u805a\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u5e93\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u3001\u9ad8\u540c\u8d28\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6279\u91cf\u8bad\u7ec3\u5b9e\u73b0\u548c\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u56fe\u805a\u7c7b\u7814\u7a76\u5b58\u5728\u5b66\u672f\u4e0e\u5de5\u4e1a\u5e94\u7528\u7684\u9e3f\u6c9f\uff1a\u8bc4\u4f30\u534f\u8bae\u4f9d\u8d56\u5c0f\u89c4\u6a21\u3001\u9ad8\u540c\u8d28\u6027\u7684\u5f15\u6587\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e0d\u53ef\u6269\u5c55\u7684\u5168\u6279\u91cf\u8bad\u7ec3\u8303\u5f0f\uff0c\u4e14\u4f9d\u8d56\u76d1\u7763\u6307\u6807\uff0c\u65e0\u6cd5\u53cd\u6620\u6807\u7b7e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u771f\u5b9e\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86PyAGC\u57fa\u51c6\u6d4b\u8bd5\u5e93\uff0c\u5c06\u73b0\u6709\u65b9\u6cd5\u7edf\u4e00\u4e3a\u6a21\u5757\u5316\u7684Encode-Cluster-Optimize\u6846\u67b6\uff0c\u9996\u6b21\u4e3a\u591a\u79cd\u5148\u8fdb\u56fe\u805a\u7c7b\u7b97\u6cd5\u63d0\u4f9b\u5185\u5b58\u9ad8\u6548\u7684mini-batch\u5b9e\u73b0\u3002\u6784\u5efa\u4e8612\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff082.7K\u5230111M\u8282\u70b9\uff09\uff0c\u5305\u542b\u5de5\u4e1a\u7ea7\u56fe\u6570\u636e\uff0c\u5e76\u5021\u5bfc\u5305\u542b\u65e0\u76d1\u7763\u7ed3\u6784\u6307\u6807\u548c\u6548\u7387\u5206\u6790\u7684\u7efc\u5408\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5728\u8682\u8681\u96c6\u56e2\u9ad8\u98ce\u9669\u5de5\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u7ecf\u8fc7\u5b9e\u6218\u68c0\u9a8c\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u63a8\u52a8\u56fe\u805a\u7c7b\u7814\u7a76\u5411\u5b9e\u9645\u90e8\u7f72\u53d1\u5c55\u3002\u4ee3\u7801\u548c\u8d44\u6e90\u5df2\u516c\u5f00\u3002", "conclusion": "PyAGC\u586b\u8865\u4e86\u56fe\u805a\u7c7b\u7814\u7a76\u5b66\u672f\u4e0e\u5de5\u4e1a\u5e94\u7528\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u901a\u8fc7\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e93\u3001\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u7efc\u5408\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u56fe\u805a\u7c7b\u65b9\u6cd5\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u5e73\u53f0\u3002"}}
{"id": "2602.08971", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.08971", "abs": "https://arxiv.org/abs/2602.08971", "authors": ["Yu Shang", "Zhuohang Li", "Yiding Ma", "Weikang Su", "Xin Jin", "Ziyou Wang", "Xin Zhang", "Yinzhou Tang", "Chen Gao", "Wei Wu", "Xihui Liu", "Dhruv Shah", "Zhaoxiang Zhang", "Zhibo Chen", "Jun Zhu", "Yonghong Tian", "Tat-Seng Chua", "Wenwu Zhu", "Yong Li"], "title": "WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models", "comment": null, "summary": "While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.", "AI": {"tldr": "WorldArena\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5177\u8eab\u4e16\u754c\u6a21\u578b\u5728\u611f\u77e5\u548c\u529f\u80fd\u4e24\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u8d28\u91cf\u4e0e\u4e0b\u6e38\u4efb\u52a1\u529f\u80fd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u5bf9\u5177\u8eab\u4e16\u754c\u6a21\u578b\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u4fdd\u771f\u5ea6\uff08\u5982\u89c6\u9891\u751f\u6210\u8d28\u91cf\uff09\uff0c\u800c\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0b\u6e38\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u529f\u80fd\u6548\u7528\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faWorldArena\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6a21\u578b\uff1a1) \u89c6\u9891\u611f\u77e5\u8d28\u91cf\uff0816\u4e2a\u6307\u6807\u8986\u76d66\u4e2a\u5b50\u7ef4\u5ea6\uff09\uff1b2) \u5177\u8eab\u4efb\u52a1\u529f\u80fd\uff08\u4f5c\u4e3a\u6570\u636e\u5f15\u64ce\u3001\u7b56\u7565\u8bc4\u4f30\u5668\u548c\u52a8\u4f5c\u89c4\u5212\u5668\uff09\uff1b3) \u7ed3\u5408\u4e3b\u89c2\u4eba\u7c7b\u8bc4\u4f30\u3002\u540c\u65f6\u63d0\u51faEWMScore\u7efc\u5408\u6307\u6807\u3002", "result": "\u901a\u8fc7\u5bf914\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u53d1\u73b0\u611f\u77e5\u4e0e\u529f\u80fd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9ad8\u89c6\u89c9\u8d28\u91cf\u4e0d\u4e00\u5b9a\u8f6c\u5316\u4e3a\u5f3a\u5927\u7684\u5177\u8eab\u4efb\u52a1\u80fd\u529b\u3002", "conclusion": "WorldArena\u4e3a\u8ffd\u8e2a\u5177\u8eabAI\u4e2d\u771f\u6b63\u529f\u80fd\u6027\u4e16\u754c\u6a21\u578b\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u9700\u8981\u540c\u65f6\u8003\u8651\u611f\u77e5\u548c\u529f\u80fd\u7ef4\u5ea6\u7684\u8bc4\u4f30\u3002"}}
{"id": "2602.08535", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08535", "abs": "https://arxiv.org/abs/2602.08535", "authors": ["Rui Wu", "Li YongJun"], "title": "Causal Schr\u00f6dinger Bridges: Constrained Optimal Transport on Structural Manifolds", "comment": "12 pages, 7 figures", "summary": "Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions (\"off-manifold\") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schr\u00f6dinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly \"tunnel\" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.", "AI": {"tldr": "\u63d0\u51faCausal Schr\u00f6dinger Bridge (CSB)\u6846\u67b6\uff0c\u5c06\u53cd\u4e8b\u5b9e\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u71b5\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5728\u652f\u6301\u96c6\u4e0d\u5339\u914d\u65f6\u7a33\u5065\u5730\"\u96a7\u9053\u7a7f\u8d8a\"\uff0c\u4f18\u4e8e\u786e\u5b9a\u6027\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u5efa\u6a21\u4f7f\u7528\u786e\u5b9a\u6027\u6d41\uff08ODE\uff09\u5bfb\u627e\u6700\u5c0f\u4f5c\u7528\u8def\u5f84\uff0c\u4f46\u5728\u56e0\u679c\u5e72\u9884\u4e0b\u53d8\u5f97\u8106\u5f31\uff0c\u56e0\u4e3a\u9700\u8981\u8de8\u8fc7\u4f4e\u5bc6\u5ea6\u533a\u57df\uff08\"\u79bb\u6d41\u5f62\"\uff09\u4f20\u8f93\u6982\u7387\u8d28\u91cf\uff0c\u800c\u8be5\u533a\u57df\u7684\u5411\u91cf\u573a\u5b9a\u4e49\u4e0d\u660e\u786e\uff0c\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\u548c\u865a\u5047\u76f8\u5173\u6027\u3002", "method": "\u5f15\u5165Causal Schr\u00f6dinger Bridge (CSB)\u6846\u67b6\uff0c\u5c06\u53cd\u4e8b\u5b9e\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u71b5\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u3002\u4e0e\u9700\u8981\u4e25\u683c\u53ef\u9006\u6027\u7684\u786e\u5b9a\u6027\u65b9\u6cd5\u4e0d\u540c\uff0cCSB\u5229\u7528\u6269\u6563\u8fc7\u7a0b\uff08SDEs\uff09\u5728\u652f\u6301\u96c6\u4e0d\u5339\u914d\u65f6\u7a33\u5065\u5730\"\u96a7\u9053\u7a7f\u8d8a\"\uff0c\u540c\u65f6\u4e25\u683c\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u53ef\u5bb9\u8bb8\u6027\u7ea6\u675f\u3002\u8bc1\u660e\u4e86\u7ed3\u6784\u5206\u89e3\u5b9a\u7406\uff0c\u8868\u660e\u5168\u5c40\u9ad8\u7ef4\u6865\u5206\u89e3\u4e3a\u5c40\u90e8\u3001\u7a33\u5065\u7684\u8f6c\u79fb\u3002", "result": "\u5728\u9ad8\u7ef4\u5e72\u9884\uff08Morpho-MNIST\uff09\u4e0a\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0cCSB\u5728\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u786e\u5b9a\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5f3a\u3001\u5206\u5e03\u5916\u5904\u7406\u7684\u673a\u5236\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "CSB\u6846\u67b6\u901a\u8fc7\u5c06\u53cd\u4e8b\u5b9e\u63a8\u7406\u91cd\u65b0\u8868\u8ff0\u4e3a\u71b5\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u514b\u670d\u4e86\u786e\u5b9a\u6027\u65b9\u6cd5\u5728\u56e0\u679c\u5e72\u9884\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u5904\u7406\u652f\u6301\u96c6\u4e0d\u5339\u914d\u548c\u5206\u5e03\u5916\u5e72\u9884\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08552", "categories": ["cs.LG", "eess.AS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08552", "abs": "https://arxiv.org/abs/2602.08552", "authors": ["Fredrik Cumlin"], "title": "Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets", "comment": null, "summary": "Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $\u03c1$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $\u03c1$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $\u03c1$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $\u03c1$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.", "AI": {"tldr": "\u03c1-Perfect\u662f\u4e00\u79cd\u4f30\u8ba1\u4e3b\u89c2\u8bc4\u5206\u6570\u636e\u96c6\u4e2d\u6a21\u578b\u53ef\u8fbe\u5230\u7684\u6700\u9ad8\u76f8\u5173\u6027\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u5f02\u65b9\u5dee\u566a\u58f0\u6765\u91cf\u5316\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u4e3b\u89c2\u8bc4\u5206\u5b58\u5728\u56fa\u6709\u566a\u58f0\uff0c\u8fd9\u4f1a\u9650\u5236\u6a21\u578b\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u76f8\u5173\u6027\uff0c\u4f46\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898\u5f88\u5c11\u88ab\u91cf\u5316\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f30\u8ba1\u4e3b\u89c2\u8bc4\u5206\u6570\u636e\u96c6\u4e0a\u6a21\u578b\u53ef\u8fbe\u5230\u7684\u6700\u9ad8\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u03c1-Perfect\u65b9\u6cd5\uff0c\u5b9a\u4e49\u4e3a\u5b8c\u7f8e\u9884\u6d4b\u5668\u4e0e\u4eba\u7c7b\u8bc4\u5206\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u57fa\u4e8e\u4e3b\u89c2\u8bc4\u5206\u6570\u636e\u96c6\u4e2d\u5e38\u89c1\u7684\u5f02\u65b9\u5dee\u566a\u58f0\u573a\u666f\uff0c\u63a8\u5bfc\u51fa\u8be5\u503c\u7684\u4f30\u8ba1\u3002\u8bc1\u660e\u03c1-Perfect\u7684\u5e73\u65b9\u53ef\u4ee5\u4f30\u8ba1\u6d4b\u8bd5-\u91cd\u6d4b\u76f8\u5173\u6027\uff0c\u5e76\u4ee5\u6b64\u9a8c\u8bc1\u4f30\u8ba1\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u8bed\u97f3\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u03c1-Perfect\u7684\u5e94\u7528\uff0c\u8868\u660e\u8be5\u5ea6\u91cf\u53ef\u4ee5\u533a\u5206\u6a21\u578b\u9650\u5236\u548c\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3002\u9a8c\u8bc1\u4e86\u03c1-Perfect\u5e73\u65b9\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u6d4b\u8bd5-\u91cd\u6d4b\u76f8\u5173\u6027\u3002", "conclusion": "\u03c1-Perfect\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u4e3b\u89c2\u8bc4\u5206\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u9650\u5236\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u533a\u5206\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u4e0e\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u57fa\u51c6\u3002"}}
{"id": "2602.09014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09014", "abs": "https://arxiv.org/abs/2602.09014", "authors": ["Zihan Yang", "Shuyuan Tu", "Licheng Zhang", "Qi Dai", "Yu-Gang Jiang", "Zuxuan Wu"], "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation", "comment": null, "summary": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.", "AI": {"tldr": "ArcFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6d41\u8f68\u8ff9\u8fd1\u4f3c\u6559\u5e08\u8f68\u8ff9\uff0c\u5b9e\u73b02\u6b65\u63a8\u7406\u768440\u500d\u52a0\u901f\uff0c\u4ec5\u9700\u5fae\u8c03\u4e0d\u52305%\u7684\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u84b8\u998f\u65b9\u6cd5\u4f7f\u7528\u7ebf\u6027\u6377\u5f84\u8fd1\u4f3c\u6559\u5e08\u8f68\u8ff9\uff0c\u96be\u4ee5\u5339\u914d\u968f\u65f6\u95f4\u6b65\u53d8\u5316\u7684\u5207\u7ebf\u65b9\u5411\uff0c\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u901f\u5ea6\u6f14\u5316\u7684\u975e\u7ebf\u6027\u8f68\u8ff9\u8fd1\u4f3c\u65b9\u6cd5\u3002", "method": "ArcFlow\u5c06\u63a8\u7406\u8f68\u8ff9\u7684\u5e95\u5c42\u901f\u5ea6\u573a\u53c2\u6570\u5316\u4e3a\u8fde\u7eed\u52a8\u91cf\u8fc7\u7a0b\u7684\u6df7\u5408\uff0c\u6355\u6349\u901f\u5ea6\u6f14\u5316\u5e76\u5916\u63a8\u5f62\u6210\u8fde\u7eed\u975e\u7ebf\u6027\u8f68\u8ff9\u3002\u8be5\u53c2\u6570\u5316\u5141\u8bb8\u5bf9\u975e\u7ebf\u6027\u8f68\u8ff9\u8fdb\u884c\u89e3\u6790\u79ef\u5206\uff0c\u907f\u514d\u6570\u503c\u79bb\u6563\u8bef\u5dee\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5728\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u4e0a\u8fdb\u884c\u8f68\u8ff9\u84b8\u998f\u8bad\u7ec3\u3002", "result": "\u57fa\u4e8eQwen-Image-20B\u548cFLUX.1-dev\u7b49\u5927\u89c4\u6a21\u6a21\u578b\uff0cArcFlow\u4ec5\u5fae\u8c03\u4e0d\u52305%\u7684\u539f\u59cb\u53c2\u6570\uff0c\u57282\u6b65\u63a8\u7406\u4e0b\u5b9e\u73b040\u500d\u52a0\u901f\uff0c\u4e14\u65e0\u660e\u663e\u8d28\u91cf\u4e0b\u964d\u3002\u57fa\u51c6\u6d4b\u8bd5\u663e\u793aArcFlow\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "ArcFlow\u901a\u8fc7\u975e\u7ebf\u6027\u6d41\u8f68\u8ff9\u8fd1\u4f3c\u6559\u5e08\u8f68\u8ff9\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u96be\u4ee5\u5339\u914d\u901f\u5ea6\u6f14\u5316\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u6548\u7387\u7684\u5c11\u6b65\u6269\u6563\u6a21\u578b\u63a8\u7406\u3002"}}
{"id": "2602.08563", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08563", "abs": "https://arxiv.org/abs/2602.08563", "authors": ["Ahmed Salem", "Andrew Paverd", "Sahar Abdelnabi"], "title": "Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs", "comment": "Accepted at IEEE SaTML 2026", "summary": "Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.", "AI": {"tldr": "LLMs\u5b9e\u9645\u4e0a\u5177\u6709\u9690\u5f0f\u8bb0\u5fc6\u80fd\u529b\uff0c\u80fd\u591f\u5728\u72ec\u7acb\u4ea4\u4e92\u95f4\u901a\u8fc7\u7f16\u7801\u4fe1\u606f\u5230\u8f93\u51fa\u4e2d\u5e76\u540e\u7eed\u6062\u590d\uff0c\u5f62\u6210\u8de8\u63a8\u7406\u8bf7\u6c42\u7684\u6301\u4e45\u4fe1\u606f\u901a\u9053\uff0c\u8fd9\u6311\u6218\u4e86LLM\u662f\u65e0\u72b6\u6001\u7684\u5047\u8bbe\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u65e0\u72b6\u6001\u7cfb\u7edf\u7684\u666e\u904d\u5047\u8bbe\uff0c\u63ed\u793aLLMs\u5b9e\u9645\u4e0a\u5177\u6709\u8de8\u72ec\u7acb\u4ea4\u4e92\u4fdd\u6301\u72b6\u6001\u7684\u80fd\u529b\uff0c\u8fd9\u79cd\u9690\u5f0f\u8bb0\u5fc6\u673a\u5236\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u98ce\u9669\u548c\u5e94\u7528\u5f71\u54cd\u3002", "method": "\u5f15\u5165\"\u9690\u5f0f\u8bb0\u5fc6\"\u6982\u5ff5\uff0c\u901a\u8fc7\u65f6\u95f4\u70b8\u5f39\uff08\u4e00\u79cd\u65b0\u578b\u65f6\u95f4\u540e\u95e8\uff09\u4f5c\u4e3a\u5177\u4f53\u6f14\u793a\uff0c\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u7b80\u5355\u63d0\u793a\u6216\u5fae\u8c03\u8bf1\u5bfc\u6a21\u578b\u5728\u5e8f\u5217\u4ea4\u4e92\u4e2d\u7d2f\u79ef\u9690\u85cf\u6761\u4ef6\u5e76\u6fc0\u6d3b\u7279\u5b9a\u884c\u4e3a\u3002", "result": "\u8bc1\u660eLLMs\u786e\u5b9e\u5177\u6709\u9690\u5f0f\u8bb0\u5fc6\u80fd\u529b\uff0c\u80fd\u591f\u521b\u5efa\u8de8\u63a8\u7406\u8bf7\u6c42\u7684\u6301\u4e45\u4fe1\u606f\u901a\u9053\uff0c\u65f6\u95f4\u70b8\u5f39\u7b49\u653b\u51fb\u5728\u5f53\u524d\u6280\u672f\u4e0b\u5373\u53ef\u5b9e\u73b0\uff0c\u63ed\u793a\u4e86\u9690\u5f0f\u8bb0\u5fc6\u5e26\u6765\u7684\u591a\u79cd\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "\u9690\u5f0f\u8bb0\u5fc6\u5bf9LLM\u5b89\u5168\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u68c0\u6d4b\u65b9\u6cd5\u3001\u538b\u529b\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6846\u67b6\u6765\u5e94\u5bf9\u672a\u6765\u53ef\u80fd\u51fa\u73b0\u7684\u66f4\u590d\u6742\u653b\u51fb\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2602.09016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09016", "abs": "https://arxiv.org/abs/2602.09016", "authors": ["Hao Phung", "Hadar Averbuch-Elor"], "title": "Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction", "comment": "Code: https://anonymous.4open.science/r/Raster2Seq-BE73/", "summary": "Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.", "AI": {"tldr": "Raster2Seq\u5c06\u697c\u5c42\u5e73\u9762\u56fe\u91cd\u5efa\u89c6\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u89e3\u7801\u5668\u5c06\u697c\u5c42\u5e73\u9762\u5143\u7d20\u8868\u793a\u4e3a\u5e26\u6807\u7b7e\u7684\u591a\u8fb9\u5f62\u5e8f\u5217\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u5904\u7406\u590d\u6742\u697c\u5c42\u5e73\u9762\u56fe\u65f6\u96be\u4ee5\u51c6\u786e\u751f\u6210\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u8fd9\u4e9b\u5e73\u9762\u56fe\u901a\u5e38\u5305\u542b\u5927\u91cf\u623f\u95f4\u548c\u4e0d\u540c\u6570\u91cf\u7684\u591a\u8fb9\u5f62\u89d2\u843d\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u5c06\u697c\u5c42\u5e73\u9762\u56fe\u91cd\u5efa\u6846\u67b6\u5316\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u89e3\u7801\u5668\u9884\u6d4b\u4e0b\u4e00\u4e2a\u89d2\u843d\u70b9\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u951a\u70b9\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u4fe1\u606f\u4e30\u5bcc\u7684\u56fe\u50cf\u533a\u57df\uff0c\u5c06\u623f\u95f4\u3001\u95e8\u7a97\u7b49\u5143\u7d20\u8868\u793a\u4e3a\u5e26\u6807\u7b7e\u7684\u591a\u8fb9\u5f62\u5e8f\u5217\u3002", "result": "\u5728Structure3D\u3001CubiCasa5K\u548cRaster2Graph\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5305\u542b\u591a\u6837\u5316\u623f\u95f4\u7ed3\u6784\u548c\u590d\u6742\u51e0\u4f55\u53d8\u5316\u7684WAFFLE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Raster2Seq\u65b9\u6cd5\u901a\u8fc7\u5e8f\u5217\u5230\u5e8f\u5217\u6846\u67b6\u548c\u81ea\u56de\u5f52\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u697c\u5c42\u5e73\u9762\u56fe\u7684\u91cd\u5efa\u4efb\u52a1\uff0c\u4e3a\u81ea\u52a8\u5316\u7406\u89e3\u548cCAD\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7ed3\u6784\u5316\u77e2\u91cf\u56fe\u5f62\u8868\u793a\u3002"}}
{"id": "2602.08564", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08564", "abs": "https://arxiv.org/abs/2602.08564", "authors": ["Tiantong Wang", "Yiyang Duan", "Haoyu Chen", "Tiantong Wu", "Wei Yang Bryan Lim"], "title": "M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data", "comment": "Code available at https://github.com/languangduan/mLoss", "summary": "Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.", "AI": {"tldr": "\u63d0\u51faM-Loss\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u6a21\u578b\u5408\u5e76\u7684\u517c\u5bb9\u6027\uff0c\u901a\u8fc7\u6d4b\u91cf\u53c2\u6570\u5e73\u5747\u4e0e\u6a21\u578b\u96c6\u6210\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u6307\u5bfc\u66f4\u6709\u6548\u7684\u6a21\u578b\u5408\u5e76\u7b56\u7565\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u8ba1\u7b97\u5bc6\u96c6\u4e14\u53d7\u6807\u6ce8\u6570\u636e\u9650\u5236\uff0c\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u5927\u91cf\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u4f46\u4f20\u7edf\u5408\u5e76\u65b9\u6cd5\uff08\u5982\u53c2\u6570\u5e73\u5747\uff09\u5bb9\u6613\u7ec4\u5408\u4e0d\u53ef\u6cdb\u5316\u7684\u7279\u5f81\uff0c\u800c\u6a21\u578b\u96c6\u6210\u867d\u7136\u6027\u80fd\u66f4\u7a33\u5b9a\u4f46\u63a8\u7406\u6210\u672c\u9ad8\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7406\u8bba\u8bc1\u636e\u548c\u8bc4\u4f30\u6307\u6807\u6765\u6307\u5bfc\u6a21\u578b\u5408\u5e76\u3002", "method": "\u5f15\u5165Merging-ensembling loss (M-Loss)\u8bc4\u4f30\u6307\u6807\uff0c\u4f7f\u7528\u5c11\u91cf\u65e0\u6807\u7b7e\u6570\u636e\u91cf\u5316\u5408\u5e76\u6e90\u6a21\u578b\u7684\u517c\u5bb9\u6027\u3002\u901a\u8fc7\u5728\u5c42\u548c\u8282\u70b9\u7ea7\u522b\u6d4b\u91cf\u53c2\u6570\u5e73\u5747\u4e0e\u6a21\u578b\u96c6\u6210\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u6307\u5bfc\u66f4\u6709\u6548\u7684\u5408\u5e76\u7b56\u7565\u3002M-Loss\u65e2\u4f5c\u4e3a\u6a21\u578b\u5408\u5e76\u7406\u8bba\u53ef\u884c\u6027\u7684\u91cf\u5316\u6807\u51c6\uff0c\u4e5f\u4f5c\u4e3a\u6a21\u578b\u526a\u679d\u4e2d\u53c2\u6570\u91cd\u8981\u6027\u7684\u6307\u5bfc\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u5c06M-Loss\u7eb3\u5165\u5408\u5e76\u8fc7\u7a0b\u663e\u8457\u63d0\u9ad8\u4e86\u5408\u5e76\u6a21\u578b\u4e0e\u6a21\u578b\u96c6\u6210\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\uff0c\u4e3a\u51c6\u786e\u7684\u6a21\u578b\u6574\u5408\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u6846\u67b6\u3002", "conclusion": "M-Loss\u4e3a\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u5b9e\u7528\u8bc4\u4f30\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5408\u5e76\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4e86\u6a21\u578b\u96c6\u6210\u7684\u9ad8\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u6a21\u578b\u6574\u5408\u3002"}}
{"id": "2602.09022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09022", "abs": "https://arxiv.org/abs/2602.09022", "authors": ["Zehan Wang", "Tengfei Wang", "Haiyu Zhang", "Xuhui Zuo", "Junta Wu", "Haoyuan Wang", "Wenqiang Sun", "Zhenwei Wang", "Chenjie Cao", "Hengshuang Zhao", "Chunchao Guo", "Zhou Zhao"], "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models", "comment": "Project page: \\url{https://3d-models.hunyuan.tencent.com/world/}", "summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.", "AI": {"tldr": "WorldCompass\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u65f6\u7a0b\u4ea4\u4e92\u5f0f\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684RL\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u521b\u65b0\u65b9\u6cd5\u63d0\u5347\u4e16\u754c\u6a21\u578b\u7684\u63a2\u7d22\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u957f\u65f6\u7a0b\u4ea4\u4e92\u5f0f\u89c6\u9891\u4e16\u754c\u6a21\u578b\u5728\u63a2\u7d22\u4e16\u754c\u65f6\u7f3a\u4e4f\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u540e\u8bad\u7ec3\u6846\u67b6\u6765\u57fa\u4e8e\u4ea4\u4e92\u4fe1\u53f7\"\u5f15\u5bfc\"\u6a21\u578b\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u6838\u5fc3\u521b\u65b0\uff1a1\uff09\u7247\u6bb5\u7ea7\u5c55\u5f00\u7b56\u7565\uff0c\u5728\u5355\u4e2a\u76ee\u6807\u7247\u6bb5\u751f\u6210\u548c\u8bc4\u4f30\u591a\u4e2a\u6837\u672c\uff1b2\uff09\u4e92\u8865\u5956\u52b1\u51fd\u6570\uff0c\u8bbe\u8ba1\u4ea4\u4e92\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u7684\u5956\u52b1\u51fd\u6570\uff1b3\uff09\u9ad8\u6548RL\u7b97\u6cd5\uff0c\u91c7\u7528\u8d1f\u611f\u77e5\u5fae\u8c03\u7b56\u7565\u548c\u5404\u79cd\u6548\u7387\u4f18\u5316\u3002", "result": "\u5728SoTA\u5f00\u6e90\u4e16\u754c\u6a21\u578bWorldPlay\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cWorldCompass\u5728\u5404\u79cd\u573a\u666f\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u4ea4\u4e92\u51c6\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "WorldCompass\u662f\u4e00\u4e2a\u6709\u6548\u7684RL\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u957f\u65f6\u7a0b\u4ea4\u4e92\u5f0f\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.08577", "categories": ["cs.LG", "math.CO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2602.08577", "abs": "https://arxiv.org/abs/2602.08577", "authors": ["Theodoros Anagnostopoulos", "Evanthia Zervoudi", "Christos Anagnostopoulos", "Apostolos Christopoulos", "Bogdan Wierzbinski"], "title": "An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources", "comment": "Nature Scientific Reports", "summary": "Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u672f\u65b9\u6cd5\u7684k-NN\u56de\u5f52\u7b97\u6cd5\u4f18\u5316\u65b9\u6848AMR\uff0c\u901a\u8fc7\u5f15\u5165\u80fd\u5904\u7406\u4efb\u610f\u6570\u91cf\u5b9e\u53d8\u91cf\u7684\u7ebf\u6027\u65b9\u7a0b\u6c42\u89e3\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edfk-NN\u7684\u6027\u80fd\u3002", "motivation": "\u7ebf\u6027\u56de\u5f52\u5206\u6790\u5728\u9884\u6d4b\u6570\u503c\u578b\u56e0\u53d8\u91cf\u65b9\u9762\u5f88\u91cd\u8981\uff0ck-NN\u4f5c\u4e3a\u4e00\u79cd\u5e38\u89c1\u7684\u975e\u53c2\u6570\u56de\u5f52\u7b97\u6cd5\uff0c\u867d\u7136\u6027\u80fd\u76f8\u5bf9\u9ad8\u6548\uff0c\u4f46\u4ecd\u5b58\u5728\u4f18\u5316\u7a7a\u95f4\u3002\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u7b97\u672f\u65b9\u6cd5\u6765\u63d0\u5347k-NN\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7b97\u672f\u65b9\u6cd5\u56de\u5f52(AMR)\u7b97\u6cd5\u4f5c\u4e3ak-NN\u7684\u4f18\u5316\u7248\u672c\u3002\u9996\u5148\u5f15\u5165\u4e00\u79cd\u80fd\u5904\u7406\u4efb\u610f\u6570\u91cf\u5b9e\u53d8\u91cf\u7ebf\u6027\u65b9\u7a0b\u7684\u7b97\u672f\u65b9\u6cd5\uff0c\u7136\u540e\u91c7\u7528\u7b97\u672f\u65b9\u6cd5\u7b97\u6cd5(AMA)\u8bc4\u4f30\u8be5\u65b9\u6cd5\u7684\u6548\u7387\uff0c\u6700\u540e\u5c06AMR\u4e0e\u5176\u4ed6\u56de\u5f52\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u4f7f\u7528\u5f15\u5165\u7684\u6700\u4f18\u63a8\u7406\u51b3\u7b56\u89c4\u5219\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6e90\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eAMR\u7b97\u6cd5\u4e0e\u5176\u4ed6\u7b97\u6cd5\u5177\u6709\u53ef\u6bd4\u6027\u80fd\uff0c\u4e14\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfk-NN\u7b97\u6cd5\u3002\u8f93\u51fa\u7ed3\u679c\u8bc1\u5b9eAMR\u662f\u5bf9k-NN\u7684\u6709\u6548\u4f18\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684AMR\u7b97\u6cd5\u6210\u529f\u4f18\u5316\u4e86k-NN\u56de\u5f52\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u7b97\u672f\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4e0e\u5176\u4ed6\u7b97\u6cd5\u53ef\u6bd4\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86k-NN\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u8be5\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.09024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.09024", "abs": "https://arxiv.org/abs/2602.09024", "authors": ["Qihang Yu", "Qihao Liu", "Ju He", "Xinyang Zhang", "Yang Liu", "Liang-Chieh Chen", "Xi Chen"], "title": "Autoregressive Image Generation with Masked Bit Modeling", "comment": "SOTA discrete visual generation defeats diffusion models with 0.99 FID score, project page is available at https://bar-gen.github.io/", "summary": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u89c6\u89c9\u751f\u6210\u4e2d\u8fde\u7eed\u7ba1\u9053\u7684\u7edf\u6cbb\u5730\u4f4d\uff0c\u901a\u8fc7\u7814\u7a76\u53d1\u73b0\u79bb\u6563\u4e0e\u8fde\u7eed\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\u4e3b\u8981\u6e90\u4e8e\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5206\u914d\u7684\u6bd4\u7279\u603b\u6570\uff0c\u800c\u975e\u79bb\u6563\u6807\u8bb0\u5668\u672c\u8eab\u7684\u5185\u5728\u52a3\u52bf\u3002\u901a\u8fc7\u6269\u5927\u7801\u672c\u89c4\u6a21\u53ef\u4ee5\u6709\u6548\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5e76\u63d0\u51faBAR\u6846\u67b6\u652f\u6301\u4efb\u610f\u7801\u672c\u5927\u5c0f\uff0c\u5728ImageNet-256\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u6311\u6218\u89c6\u89c9\u751f\u6210\u9886\u57df\u4e2d\u8fde\u7eed\u7ba1\u9053\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u7cfb\u7edf\u7814\u7a76\u79bb\u6563\u4e0e\u8fde\u7eed\u65b9\u6cd5\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u9488\u5bf9\u666e\u904d\u8ba4\u4e3a\u79bb\u6563\u6807\u8bb0\u5668\u672c\u8d28\u4e0a\u52a3\u4e8e\u8fde\u7eed\u65b9\u6cd5\u7684\u89c2\u70b9\uff0c\u63d0\u51fa\u65b0\u7684\u7814\u7a76\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u63a9\u7801\u6bd4\u7279\u81ea\u56de\u5f52\u5efa\u6a21\uff08BAR\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u81ea\u56de\u5f52\u53d8\u6362\u5668\u914d\u5907\u63a9\u7801\u6bd4\u7279\u5efa\u6a21\u5934\uff0c\u652f\u6301\u4efb\u610f\u7801\u672c\u5927\u5c0f\u3002BAR\u901a\u8fc7\u9010\u6b65\u751f\u6210\u6784\u6210\u79bb\u6563\u6807\u8bb0\u7684\u6bd4\u7279\u6765\u9884\u6d4b\u79bb\u6563\u6807\u8bb0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\u5728\u6269\u5927\u7801\u672c\u65f6\u9047\u5230\u7684\u6027\u80fd\u4e0b\u964d\u6216\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\u95ee\u9898\u3002", "result": "BAR\u5728ImageNet-256\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdbgFID\u4e3a0.99\uff0c\u8d85\u8d8a\u4e86\u8fde\u7eed\u548c\u79bb\u6563\u8303\u5f0f\u4e2d\u7684\u9886\u5148\u65b9\u6cd5\u3002\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u91c7\u6837\u6210\u672c\uff0c\u6536\u655b\u901f\u5ea6\u4e5f\u6bd4\u4e4b\u524d\u7684\u8fde\u7eed\u65b9\u6cd5\u66f4\u5feb\u3002", "conclusion": "\u79bb\u6563\u4e0e\u8fde\u7eed\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\u4e3b\u8981\u6e90\u4e8e\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6bd4\u7279\u5206\u914d\u6570\u91cf\uff08\u538b\u7f29\u6bd4\uff09\uff0c\u800c\u975e\u79bb\u6563\u6807\u8bb0\u5668\u7684\u5185\u5728\u52a3\u52bf\u3002\u901a\u8fc7\u6269\u5927\u7801\u672c\u89c4\u6a21\u53ef\u4ee5\u6709\u6548\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0cBAR\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002"}}
{"id": "2602.08579", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08579", "abs": "https://arxiv.org/abs/2602.08579", "authors": ["Junsu Seo"], "title": "Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs", "comment": null, "summary": "This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\u89c6\u4e3a\u9a71\u52a8Fokker-Planck\u65b9\u7a0b\u7684\u968f\u673a\u6e90\uff0c\u5728SPDE\u6846\u67b6\u4e0b\u5206\u6790\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u52a8\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eSPDE\u89e3\u5728\u5f84\u5411\u6d4b\u8bd5\u51fd\u6570\u4e0a\u4e8c\u6b21\u53d8\u5dee\u7684\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7c92\u5b50\u7684SDE\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4ece\u6982\u7387\u5bc6\u5ea6\u573a\u6f14\u5316\u7684\u89d2\u5ea6\u7406\u89e3\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u52a8\u6001\uff0c\u7279\u522b\u662f\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u968f\u673a\u504f\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\uff0c\u5c06\u5206\u6570\u4f30\u8ba1\u8bef\u5dee\u5efa\u6a21\u4e3aFokker-Planck\u65b9\u7a0b\u7684\u968f\u673a\u6f02\u79fb\u6270\u52a8\u6e90\uff0c\u5728\u7b80\u5316\u8bbe\u7f6e\u4e0b\u901a\u8fc7\u51e0\u4f55\u7a33\u5b9a\u6027\u548c\u4f4d\u79fb\u51f8\u6027\u5206\u6790\u751f\u6210\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8eSPDE\u89e3\u5728\u5f84\u5411\u6d4b\u8bd5\u51fd\u6570\u4e0a\u4e8c\u6b21\u53d8\u5dee\u7684\u5019\u9009\u8bc4\u4f30\u6307\u6807\uff0c\u521d\u6b65\u89c2\u5bdf\u8868\u660e\u4ec5\u4f7f\u7528\u91c7\u6837\u8f68\u8ff9\u524d10%\u7684\u6570\u636e\u8be5\u6307\u6807\u4ecd\u4fdd\u6301\u6709\u6548\uff0c\u5177\u6709\u8ba1\u7b97\u6548\u7387\u6f5c\u529b\u3002", "conclusion": "SPDE\u6846\u67b6\u4e3a\u7406\u89e3\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u5728\u4fdd\u6301\u6709\u6548\u6027\u7684\u540c\u65f6\u5177\u6709\u8ba1\u7b97\u6548\u7387\u4f18\u52bf\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.08589", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.08589", "abs": "https://arxiv.org/abs/2602.08589", "authors": ["Emmanouil Kariotakis", "Aritra Konar"], "title": "FairRARI: A Plug and Play Framework for Fairness-Aware PageRank", "comment": null, "summary": "PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFairRARI\u6846\u67b6\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u65b9\u6cd5\u4e3aPageRank\u7b97\u6cd5\u6dfb\u52a0\u7fa4\u4f53\u516c\u5e73\u6027\u7ea6\u675f\uff0c\u786e\u4fdd\u8fbe\u5230\u76ee\u6807\u516c\u5e73\u6c34\u5e73\u7684\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u6548\u7387\u3002", "motivation": "\u968f\u7740\u7b97\u6cd5\u516c\u5e73\u6027\u65e5\u76ca\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97PageRank\u65f6\u65e0\u6cd5\u4fdd\u8bc1\u8fbe\u5230\u76ee\u6807\u516c\u5e73\u6c34\u5e73\u6216\u7f3a\u4e4f\u6700\u4f18\u6027\u4fdd\u8bc1\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u6765\u89e3\u51b3\u4e0d\u540c\u7fa4\u4f53\u516c\u5e73\u6027\u6807\u51c6\u3002", "method": "\u63d0\u51faFairRARI\u7edf\u4e00\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528PageRank\u7684\u53d8\u5206\u516c\u5f0f\uff0c\u901a\u8fc7\u6c42\u89e3\u5e26\u6709\u516c\u5e73\u6027\u7ea6\u675f\u7684\u5f3a\u51f8\u4f18\u5316\u95ee\u9898\u6765\u8ba1\u7b97\u516c\u5e73PageRank\u5411\u91cf\uff0c\u652f\u6301\"\u5373\u63d2\u5373\u7528\"\u65b9\u5f0f\u5904\u7406\u4e0d\u540c\u516c\u5e73\u6807\u51c6\u3002", "result": "FairRARI\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6548\u7528\u7684\u540c\u65f6\u8fbe\u5230\u671f\u671b\u7684\u516c\u5e73\u6c34\u5e73\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u539f\u59cbPageRank\u7b97\u6cd5\u76f8\u540c\u3002", "conclusion": "FairRARI\u4e3aPageRank\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u3001\u53ef\u6269\u5c55\u7684\u516c\u5e73\u6027\u589e\u5f3a\u6846\u67b6\uff0c\u80fd\u591f\u6ee1\u8db3\u4e0d\u540c\u7fa4\u4f53\u516c\u5e73\u6027\u9700\u6c42\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u539f\u5219\u6027\u7b97\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.08590", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.08590", "abs": "https://arxiv.org/abs/2602.08590", "authors": ["Yicheng Di", "Wei Yuan", "Tieke He", "Zhanjie Zhang", "Ao Ma", "Yuan Liu", "Hongzhi Yin"], "title": "SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning", "comment": "13 pages, 6 figures", "summary": "Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \\textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.", "AI": {"tldr": "SDFed\u662f\u4e00\u4e2a\u5f02\u6784\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u7ec6\u5316\u548c\u53d1\u6563\u63a7\u5236\u89e3\u51b3\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5141\u8bb8\u53ef\u53d8\u957f\u5ea6\u7684\u672c\u5730\u63d0\u793a\u540c\u65f6\u4fdd\u6301\u56fa\u5b9a\u957f\u5ea6\u7684\u5168\u5c40\u63d0\u793a\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5f3a\u5236\u6240\u6709\u5ba2\u6237\u7aef\u4f7f\u7528\u7edf\u4e00\u7684\u63d0\u793a\u7ed3\u6784\u548c\u957f\u5ea6\uff0c\u8fd9\u5728\u5b9e\u9645\u7684\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\uff08\u6570\u636e\u5206\u5e03\u548c\u7cfb\u7edf\u8d44\u6e90\uff09\u4e0b\u4e0d\u8db3\uff0c\u5e76\u53ef\u80fd\u5f15\u5165\u5168\u5c40\u5171\u4eab\u4e0e\u672c\u5730\u6700\u4f18\u77e5\u8bc6\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "SDFed\u901a\u8fc7\u5b50\u7a7a\u95f4\u7ec6\u5316\u548c\u53d1\u6563\u63a7\u5236\u6765\u5f25\u5408\u672c\u5730-\u5168\u5c40\u5dee\u5f02\u3002\u5b83\u4fdd\u6301\u56fa\u5b9a\u957f\u5ea6\u7684\u5168\u5c40\u63d0\u793a\u4ee5\u8fdb\u884c\u9ad8\u6548\u805a\u5408\uff0c\u540c\u65f6\u5141\u8bb8\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5b66\u4e60\u53ef\u53d8\u957f\u5ea6\u7684\u672c\u5730\u63d0\u793a\u4ee5\u5339\u914d\u5176\u6570\u636e\u7279\u5f81\u548c\u5bb9\u91cf\u3002\u5f15\u5165\u672c\u5730\u63d0\u793a\u7684\u5b50\u7a7a\u95f4\u7ec6\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4fe1\u606f\u4fdd\u7559\u548c\u53d1\u6563\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSDFed\u5728\u5f02\u6784\u8054\u90a6\u8bbe\u7f6e\u4e2d\u6301\u7eed\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SDFed\u901a\u8fc7\u5141\u8bb8\u53ef\u53d8\u957f\u5ea6\u7684\u672c\u5730\u63d0\u793a\u548c\u6709\u6548\u7684\u672c\u5730-\u5168\u5c40\u77e5\u8bc6\u534f\u8c03\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u4fdd\u6301\u901a\u4fe1\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.08592", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08592", "abs": "https://arxiv.org/abs/2602.08592", "authors": ["Tianyin Liao", "Chunyu Hu", "Yicheng Sui", "Xingxuan Zhang", "Peng Cui", "Jianxin Li", "Ziwei Zhang"], "title": "TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models", "comment": null, "summary": "Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.", "AI": {"tldr": "TFMLinker\uff1a\u57fa\u4e8e\u8868\u683c\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u94fe\u63a5\u9884\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u6570\u636e\u96c6\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u8de8\u56fe\u57df\u5de5\u4f5c", "motivation": "\u73b0\u6709\u56fe\u57fa\u7840\u6a21\u578b\u5728\u94fe\u63a5\u9884\u6d4b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u9884\u8bad\u7ec3\u89c4\u6a21\u6709\u9650\u6216\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u3002\u53d7\u8868\u683c\u57fa\u7840\u6a21\u578b\u5728\u8de8\u8868\u683c\u6570\u636e\u96c6\u901a\u7528\u9884\u6d4b\u6210\u529f\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5c06\u5176\u5e94\u7528\u4e8e\u94fe\u63a5\u9884\u6d4b\uff0c\u4f46\u9762\u4e34\u5982\u4f55\u83b7\u53d6\u5fc5\u8981\u4e0a\u4e0b\u6587\u548c\u6355\u83b7\u94fe\u63a5\u4e2d\u5fc3\u62d3\u6251\u4fe1\u606f\u7684\u6280\u672f\u6311\u6218", "method": "\u63d0\u51faTFMLinker\uff1a1\uff09\u539f\u578b\u589e\u5f3a\u7684\u5c40\u90e8-\u5168\u5c40\u4e0a\u4e0b\u6587\u6a21\u5757\uff0c\u6784\u5efa\u6355\u83b7\u56fe\u7279\u5b9a\u548c\u8de8\u56fe\u53ef\u8fc1\u79fb\u6a21\u5f0f\u7684\u4e0a\u4e0b\u6587\uff1b2\uff09\u901a\u7528\u62d3\u6251\u611f\u77e5\u94fe\u63a5\u7f16\u7801\u5668\uff0c\u6355\u83b7\u94fe\u63a5\u4e2d\u5fc3\u62d3\u6251\u4fe1\u606f\u5e76\u751f\u6210\u94fe\u63a5\u8868\u793a\u4f5c\u4e3aTFM\u8f93\u5165\uff1b3\uff09\u5229\u7528TFM\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u9884\u6d4b\u94fe\u63a5\u5b58\u5728", "result": "\u57286\u4e2a\u8de8\u4e0d\u540c\u9886\u57df\u7684\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u6570\u636e\u96c6\u7279\u5b9a\u7684\u5fae\u8c03", "conclusion": "TFMLinker\u6210\u529f\u5c06\u8868\u683c\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u8de8\u56fe\u6570\u636e\u96c6\u901a\u7528\u94fe\u63a5\u9884\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u56fe\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.08616", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08616", "abs": "https://arxiv.org/abs/2602.08616", "authors": ["Heiko Hoppe", "Fabian Akkerman", "Wouter van Heeswijk", "Maximilian Schiffer"], "title": "Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces", "comment": "26 pages, 8 figures", "summary": "Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.", "AI": {"tldr": "\u63d0\u51faDGRL\u65b9\u6cd5\u89e3\u51b3\u5927\u89c4\u6a21\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u7ed3\u5408SDN\u548cDBU\u6280\u672f\uff0c\u5728\u9ad8\u8fbe10^20\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u6027\u80fd\u63d0\u534766%", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u6d41\u3001\u8c03\u5ea6\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u5927\u89c4\u6a21\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u7684\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7f51\u683c\u7ed3\u6784\u6216\u8ba1\u7b97\u6602\u8d35\u7684\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u5728\u9ad8\u7ef4\u6216\u4e0d\u89c4\u5219\u7ed3\u6784\u57df\u4e2d\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u8ddd\u79bb\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60(DGRL)\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1) \u91c7\u6837\u52a8\u6001\u90bb\u57df(SDN)\uff1a\u5229\u7528\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u968f\u673a\u4f53\u79ef\u63a2\u7d22\uff0c\u5728\u5c40\u90e8\u4fe1\u4efb\u533a\u57df\u63d0\u4f9b\u5b8c\u5168\u652f\u6301\uff1b2) \u57fa\u4e8e\u8ddd\u79bb\u7684\u66f4\u65b0(DBU)\uff1a\u5c06\u7b56\u7565\u4f18\u5316\u8f6c\u5316\u4e3a\u7a33\u5b9a\u7684\u56de\u5f52\u4efb\u52a1\uff0c\u89e3\u8026\u68af\u5ea6\u65b9\u5dee\u4e0e\u52a8\u4f5c\u7a7a\u95f4\u57fa\u6570\uff0c\u4fdd\u8bc1\u7b56\u7565\u5355\u8c03\u6539\u8fdb\u3002", "result": "\u5728\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u7ed3\u6784\u73af\u5883\u4e2d\uff0cDGRL\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u51c6\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe66%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u65b9\u6cd5\u81ea\u7136\u63a8\u5e7f\u5230\u6df7\u5408\u8fde\u7eed-\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\uff0c\u65e0\u9700\u5c42\u6b21\u4f9d\u8d56\u3002", "conclusion": "DGRL\u901a\u8fc7SDN\u548cDBU\u7684\u7ec4\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u9ad8\u8fbe10^20\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b66\u4e60\uff0c\u4e3a\u7269\u6d41\u3001\u8c03\u5ea6\u7b49\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08617", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08617", "abs": "https://arxiv.org/abs/2602.08617", "authors": ["Dario Fenoglio", "Pasquale Polverino", "Jacopo Quizi", "Martin Gjoreski", "Marc Langheinrich"], "title": "ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning", "comment": null, "summary": "Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.", "AI": {"tldr": "ERIS\u662f\u4e00\u4e2a\u65e0\u670d\u52a1\u5668\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5206\u533a\u548c\u5206\u5e03\u5f0f\u68af\u5ea6\u538b\u7f29\uff0c\u5728\u4fdd\u6301FedAvg\u7ea7\u522b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u5e76\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5c06\u8054\u90a6\u5b66\u4e60\u6269\u5c55\u5230\u5341\u4ebf\u53c2\u6570\u6a21\u578b\u65f6\uff0c\u901a\u4fe1\u6548\u7387\u3001\u6a21\u578b\u7cbe\u5ea6\u548c\u9690\u79c1\u4fdd\u8bc1\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u6743\u8861\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u5b64\u7acb\u5730\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u727a\u7272\u7cbe\u5ea6\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u5bc6\u7801\u5b66\u5de5\u5177\u3002", "method": "ERIS\u7ed3\u5408\u4e86\u6a21\u578b\u5206\u533a\u7b56\u7565\uff08\u5c06\u805a\u5408\u5206\u5e03\u5728\u591a\u4e2a\u5ba2\u6237\u7aef\u805a\u5408\u5668\u4e0a\uff09\u548c\u5206\u5e03\u5f0f\u79fb\u4f4d\u68af\u5ea6\u538b\u7f29\u673a\u5236\uff0c\u6d88\u9664\u4e86\u670d\u52a1\u5668\u74f6\u9888\u5e76\u5206\u5e03\u4e86\u901a\u4fe1\u8d1f\u8f7d\u3002", "result": "\u7406\u8bba\u8bc1\u660eERIS\u5728\u6807\u51c6\u5047\u8bbe\u4e0b\u4ee5\u4e0eFedAvg\u76f8\u540c\u7684\u901f\u7387\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u4e92\u4fe1\u606f\u6cc4\u6f0f\u4e0e\u805a\u5408\u5668\u6570\u91cf\u6210\u53cd\u6bd4\u7684\u5173\u7cfb\u5b9e\u73b0\u5f3a\u9690\u79c1\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u8868\u660eERIS\u5728\u56fe\u50cf\u548c\u6587\u672c\u4efb\u52a1\uff08\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\uff09\u4e2d\u8fbe\u5230FedAvg\u7ea7\u522b\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u5e76\u63d0\u9ad8\u5bf9\u6210\u5458\u63a8\u65ad\u548c\u91cd\u5efa\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ERIS\u662f\u4e00\u4e2a\u5e73\u8861\u9690\u79c1\u548c\u7cbe\u5ea6\u7684\u65e0\u670d\u52a1\u5668\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u65e0\u9700\u4f9d\u8d56\u7e41\u91cd\u7684\u5bc6\u7801\u5b66\u6216\u566a\u58f0\u6ce8\u5165\uff0c\u5c31\u80fd\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2602.08621", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08621", "abs": "https://arxiv.org/abs/2602.08621", "authors": ["Yukun Jiang", "Hai Huang", "Mingjie Li", "Yage Zhang", "Michael Backes", "Yang Zhang"], "title": "Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs", "comment": null, "summary": "By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0MoE\u67b6\u6784\u7684LLMs\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u901a\u8fc7\u64cd\u7eb5\u8def\u7531\u5668\u53ef\u4ee5\u6fc0\u6d3b\u4e0d\u5b89\u5168\u8def\u5f84\uff0c\u5c06\u5b89\u5168\u8f93\u51fa\u8f6c\u4e3a\u6709\u5bb3\u5185\u5bb9", "motivation": "\u867d\u7136MoE\u67b6\u6784\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6548\u7528\u548c\u6548\u7387\uff0c\u5bf9\u5176\u7a00\u758f\u67b6\u6784\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u63a2\u7d22\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793aMoE LLMs\u4e2d\u5b58\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "1. \u5f15\u5165Router Safety\u91cd\u8981\u6027\u8bc4\u5206(RoSais)\u91cf\u5316\u6bcf\u5c42\u8def\u7531\u5668\u7684\u5b89\u5168\u5173\u952e\u6027\uff1b2. \u63d0\u51fa\u7ec6\u7c92\u5ea6token-layer-wise\u968f\u673a\u4f18\u5316\u6846\u67b6(F-SOUR)\u6765\u53d1\u73b0\u66f4\u5177\u4f53\u7684\u4e0d\u5b89\u5168\u8def\u5f84\uff0c\u8be5\u6846\u67b6\u660e\u786e\u8003\u8651\u4e86\u8f93\u5165token\u7684\u987a\u5e8f\u6027\u548c\u52a8\u6001\u6027\u3002", "result": "1. \u4ec5\u64cd\u7eb5\u9ad8RoSais\u8def\u7531\u5668\u5c31\u80fd\u5c06\u9ed8\u8ba4\u8def\u5f84\u8f6c\u4e3a\u4e0d\u5b89\u5168\u8def\u5f84\uff1b2. \u5728DeepSeek-V2-Lite\u4e0a\uff0c\u5c4f\u853d5\u4e2a\u8def\u7531\u5668\u4f7f\u653b\u51fb\u6210\u529f\u7387(ASR)\u63d0\u9ad84\u500d\u4ee5\u4e0a\u81f30.79\uff1b3. F-SOUR\u5728\u56db\u4e2a\u4ee3\u8868\u6027MoE LLM\u5bb6\u65cf\u4e0a\uff0c\u5728JailbreakBench\u548cAdvBench\u4e0a\u7684\u5e73\u5747ASR\u5206\u522b\u8fbe\u52300.90\u548c0.98\u3002", "conclusion": "MoE LLMs\u7684\u5b89\u5168\u6027\u4e0e\u5176\u67b6\u6784\u4e00\u6837\u7a00\u758f\uff0c\u5b58\u5728\u4e0d\u5b89\u5168\u8def\u5f84\u7684\u5b89\u5168\u98ce\u9669\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u9632\u5fa1\u89c6\u89d2\uff0c\u5305\u62ec\u5b89\u5168\u611f\u77e5\u7684\u8def\u5f84\u7981\u7528\u548c\u8def\u7531\u5668\u8bad\u7ec3\uff0c\u4e3a\u672a\u6765MoE LLMs\u7684\u7ea2\u961f\u6d4b\u8bd5\u548c\u5b89\u5168\u4fdd\u969c\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2602.08629", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08629", "abs": "https://arxiv.org/abs/2602.08629", "authors": ["Bo Peng", "Sirui Chen", "Jiaguo Tian", "Yu Qiao", "Chaochao Lu"], "title": "CauScale: Neural Causal Discovery at Scale", "comment": null, "summary": "Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.", "AI": {"tldr": "CauScale\u662f\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u56e0\u679c\u53d1\u73b0\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u80fd\u591f\u6269\u5c55\u52301000\u4e2a\u8282\u70b9\u7684\u5927\u56fe\uff0c\u901a\u8fc7\u538b\u7f29\u5355\u5143\u548c\u5171\u4eab\u6ce8\u610f\u529b\u6743\u91cd\u663e\u8457\u63d0\u5347\u65f6\u7a7a\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u65f6\u9762\u4e34\u65f6\u95f4\u548c\u7a7a\u95f4\u6548\u7387\u74f6\u9888\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u5927\u578b\u56fe\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5728\u79d1\u5b66AI\u548c\u6570\u636e\u5206\u6790\u7b49\u6570\u636e\u9a71\u52a8\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "CauScale\u91c7\u7528\u795e\u7ecf\u67b6\u6784\u8bbe\u8ba1\uff1a1\uff09\u901a\u8fc7\u538b\u7f29\u5355\u5143\u51cf\u5c11\u6570\u636e\u5d4c\u5165\u7ef4\u5ea6\u63d0\u5347\u65f6\u95f4\u6548\u7387\uff1b2\uff09\u91c7\u7528\u5171\u4eab\u6ce8\u610f\u529b\u6743\u91cd\u907f\u514d\u7ef4\u62a4\u8f74\u7279\u5b9a\u6ce8\u610f\u529b\u56fe\u6765\u63d0\u5347\u7a7a\u95f4\u6548\u7387\uff1b3\uff09\u91c7\u7528\u53cc\u6d41\u8bbe\u8ba1\uff1a\u6570\u636e\u6d41\u4ece\u9ad8\u7ef4\u89c2\u6d4b\u4e2d\u63d0\u53d6\u5173\u7cfb\u8bc1\u636e\uff0c\u56fe\u6d41\u6574\u5408\u7edf\u8ba1\u56fe\u5148\u9a8c\u5e76\u4fdd\u7559\u5173\u952e\u7ed3\u6784\u4fe1\u53f7\u3002", "result": "CauScale\u6210\u529f\u6269\u5c55\u5230500\u8282\u70b9\u56fe\u7684\u8bad\u7ec3\uff08\u5148\u524d\u65b9\u6cd5\u56e0\u7a7a\u95f4\u9650\u5236\u65e0\u6cd5\u5b9e\u73b0\uff09\uff0c\u5728\u5206\u5e03\u5185\u6570\u636e\u4e0a\u8fbe\u523099.6% mAP\uff0c\u5206\u5e03\u5916\u6570\u636e\u8fbe\u523084.4% mAP\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u5148\u524d\u65b9\u6cd5\u5feb4-13,000\u500d\u3002", "conclusion": "CauScale\u901a\u8fc7\u521b\u65b0\u7684\u795e\u7ecf\u67b6\u6784\u8bbe\u8ba1\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u56e0\u679c\u53d1\u73b0\u7684\u65f6\u7a7a\u6548\u7387\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u54111000\u8282\u70b9\u56fe\u7684\u6269\u5c55\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u5927\u89c4\u6a21\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08638", "abs": "https://arxiv.org/abs/2602.08638", "authors": ["Dezheng Wang", "Tong Chen", "Guansong Pang", "Congyan Chen", "Shihua Li", "Hongzhi Yin"], "title": "LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection", "comment": null, "summary": "As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.", "AI": {"tldr": "LEFT\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u65f6\u95f4\u3001\u9891\u7387\u548c\u591a\u5c3a\u5ea6\u4e09\u89c6\u56fe\u7279\u5f81\uff0c\u5229\u7528\u81ea\u9002\u5e94Nyquist\u7ea6\u675f\u8c31\u6ee4\u6ce2\u5668\u5b66\u4e60\u5f02\u5e38\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u5f53\u524d\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u8bb8\u591a\u5f02\u5e38\u8fc7\u4e8e\u7ec6\u5fae\uff0c\u65e0\u6cd5\u5728\u5355\u4e00\u89c6\u56fe\u4e2d\u68c0\u6d4b\u5230\uff0c\u800c\u662f\u8868\u73b0\u4e3a\u8de8\u591a\u4e2a\u89c6\u56fe\uff08\u5982\u65f6\u95f4\u3001\u9891\u7387\u3001\u591a\u5206\u8fa8\u7387\uff09\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u73b0\u6709\u8de8\u89c6\u56fe\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u7279\u5f81\u6216\u5206\u6570\u878d\u5408\uff0c\u7f3a\u4e4f\u5206\u6790-\u5408\u6210\u4e00\u81f4\u6027\u7ea6\u675f\u3002", "method": "LEFT\u6846\u67b6\u4ece\u4e09\u4e2a\u4e92\u8865\u89c6\u56fe\u5b66\u4e60\u7279\u5f81token\uff1a\u9891\u7387\u57dftoken\u5d4c\u5165\u5468\u671f\u6027\u4fe1\u606f\uff0c\u65f6\u95f4\u57dftoken\u6355\u6349\u5c40\u90e8\u52a8\u6001\uff0c\u591a\u5c3a\u5ea6token\u5b66\u4e60\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u7c92\u5ea6\u7684\u5f02\u5e38\u6a21\u5f0f\u3002\u901a\u8fc7\u81ea\u9002\u5e94Nyquist\u7ea6\u675f\u8c31\u6ee4\u6ce2\u5668\u5c06\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u91cd\u7f29\u653e\u4e3a\u591a\u4e2a\u5206\u8fa8\u7387\uff0c\u7136\u540e\u7f16\u7801\u3002\u5f15\u5165\u4ece\u7c97\u7c92\u5ea6\u591a\u5c3a\u5ea6\u7ed3\u6784\u91cd\u6784\u7ec6\u7c92\u5ea6\u76ee\u6807\u7684\u65b0\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u63d0\u51fa\u521b\u65b0\u7684\u65f6\u95f4-\u9891\u7387\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u663e\u5f0f\u6b63\u5219\u5316\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLEFT\u76f8\u6bd4SOTA\u57fa\u7ebf\u83b7\u5f97\u4e86\u6700\u4f73\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u5b9e\u73b0\u4e865\u500d\u7684FLOPs\u51cf\u5c11\u548c8\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "LEFT\u901a\u8fc7\u878d\u5408\u4e09\u89c6\u56fetoken\u5e76\u5f15\u5165\u65f6\u95f4-\u9891\u7387\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8de8\u89c6\u56fe\u4e0d\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.08646", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08646", "abs": "https://arxiv.org/abs/2602.08646", "authors": ["Jisung Hwang", "Minhyuk Sung"], "title": "Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models", "comment": null, "summary": "We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \\log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e26\u7ea6\u675f\u7684\u6f5c\u5728\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u786c\u6027\u767d\u9ad8\u65af\u566a\u58f0\u7ea6\u675f\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u6f5c\u5728\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u5956\u52b1\u5f15\u5bfc\u751f\u6210\u6548\u679c\uff0c\u4f46\u5b58\u5728\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4e14\u8ba1\u7b97\u901f\u5ea6\u8fc7\u6162\uff0c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528", "method": "\u7528\u786c\u6027\u767d\u9ad8\u65af\u566a\u58f0\u7ea6\u675f\u66ff\u4ee3\u8f6f\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u6295\u5f71\u68af\u5ea6\u4e0a\u5347\u6cd5\u5728\u6bcf\u6b21\u66f4\u65b0\u540e\u5e94\u7528\u95ed\u5f0f\u6295\u5f71\uff0c\u4fdd\u6301\u6f5c\u5728\u5411\u91cf\u59cb\u7ec8\u5177\u6709\u767d\u9ad8\u65af\u566a\u58f0\u7279\u6027\uff0c\u9632\u6b62\u5bfc\u81f4\u4e0d\u771f\u5b9e\u4f2a\u5f71\u7684\u6f02\u79fb", "result": "\u8be5\u65b9\u6cd5\u4ec5\u9700SOTA\u6b63\u5219\u5316\u65b9\u6cd530%\u7684\u5899\u949f\u65f6\u95f4\u5c31\u80fd\u8fbe\u5230\u76f8\u5f53\u7684\u7f8e\u5b66\u8bc4\u5206\uff0c\u540c\u65f6\u6709\u6548\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\uff0c\u6295\u5f71\u64cd\u4f5c\u590d\u6742\u5ea6\u4e3aO(N log N)\uff0c\u4e0e\u6392\u5e8f\u6216FFT\u76f8\u5f53\uff0c\u5b9e\u9645\u4e0d\u589e\u52a0\u8ba1\u7b97\u65f6\u95f4", "conclusion": "\u63d0\u51fa\u7684\u7ea6\u675f\u6f5c\u5728\u4f18\u5316\u65b9\u6cd5\u4f7f\u6d4b\u8bd5\u65f6\u4f18\u5316\u65e2\u9ad8\u6548\u53c8\u53ef\u9760\uff0c\u901a\u8fc7\u786c\u6027\u566a\u58f0\u7ea6\u675f\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.08655", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08655", "abs": "https://arxiv.org/abs/2602.08655", "authors": ["Sarthak Wanjari"], "title": "From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism", "comment": "10 pages, 8 figures", "summary": "Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.", "AI": {"tldr": "Geo-IQL\uff1a\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8ek\u6700\u8fd1\u90bb\u8ddd\u79bb\u7684\u5bc6\u5ea6\u60e9\u7f5a\u6765\u589e\u5f3aIQL\uff0c\u6709\u6548\u89e3\u51b3OOD\u52a8\u4f5c\u9ad8\u4f30\u95ee\u9898\uff0c\u5728\u654f\u611f\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u534718%\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u5206\u5e03\u5916\u52a8\u4f5c\u9ad8\u4f30\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002CQL\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800cIQL\u5728\u75c5\u6001\u6570\u636e\u96c6\u4e0a\u5bb9\u6613\u9000\u5316\u5230\u884c\u4e3a\u514b\u9686\u3002\u9700\u8981\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u4e14\u80fd\u6709\u6548\u5904\u7406OOD\u8bef\u5dee\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u60b2\u89c2\u4e3b\u4e49\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u72b6\u6001-\u52a8\u4f5c\u5d4c\u5165\u7a7a\u95f4\u4e2dk\u6700\u8fd1\u90bb\u8ddd\u79bb\u7684\u5bc6\u5ea6\u60e9\u7f5a\uff0c\u4ee5\u5956\u52b1\u5851\u5f62\u65b9\u5f0f\u6ce8\u5165OOD\u4fdd\u5b88\u6027\uff0c\u8bad\u7ec3\u5f00\u9500\u4ec5\u4e3aO(1)\u3002\u8be5\u65b9\u6cd5\u4f5c\u4e3aIQL\u7684\u6a21\u5757\u5316\u589e\u5f3a\u3002", "result": "\u5728D4RL MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGeo-IQL\u5728\u654f\u611f\u7684\u4e2d\u7b49\u56de\u653e\u4efb\u52a1\u4e0a\u6bd4\u6807\u51c6IQL\u63d0\u5347\u8d85\u8fc718\u5206\uff0c\u79cd\u5b50\u95f4\u65b9\u5dee\u51cf\u5c114\u500d\u3002\u5728MIMIC-III Sepsis\u6570\u636e\u96c6\u4e0a\uff0c\u6807\u51c6IQL\u9000\u5316\u4e3a\u884c\u4e3a\u514b\u9686\uff0c\u800cGeo-IQL\u5b9e\u73b0\u4e3b\u52a8\u7b56\u7565\u6539\u8fdb\uff0c\u4e0e\u4e34\u5e8a\u533b\u751f\u7ec8\u672b\u51b3\u7b56\u4e00\u81f4\u6027\u8fbe86.4%\uff08IQL\u4e3a75%\uff09\u3002", "conclusion": "\u51e0\u4f55\u60b2\u89c2\u4e3b\u4e49\u4e3a\u5173\u952e\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u6b63\u5219\u5316\uff0c\u80fd\u591f\u5b89\u5168\u514b\u670d\u5c40\u90e8\u6700\u4f18\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u6709\u6548\u5904\u7406\u5206\u5e03\u5916\u52a8\u4f5c\u9ad8\u4f30\u95ee\u9898\u3002"}}
{"id": "2602.08657", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.08657", "abs": "https://arxiv.org/abs/2602.08657", "authors": ["Xiaotong Liu", "Shao-Bo Lin", "Jun Fan", "Ding-Xuan Zhou"], "title": "Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction", "comment": null, "summary": "Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5408\u6210\u6570\u636e\u7b56\u7565\uff0c\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u4e0e\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210-\u6df7\u5408\u9636\u6bb5\u548cKRR\u5408\u6210\u9636\u6bb5\u5b9e\u73b0\u7edf\u8ba1\u9a71\u52a8\u7684\u53d7\u9650\u9690\u79c1-\u9884\u6d4b\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7edf\u8ba1\u4fe1\u606f\u4fdd\u6301\uff0c\u867d\u7136\u6709\u4e9b\u7814\u7a76\u6d89\u53ca\u9884\u6d4b\u6027\u80fd\u4fdd\u8bc1\uff0c\u4f46\u5355\u9636\u6bb5\u5408\u6210\u8bbe\u8ba1\u96be\u4ee5\u5e73\u8861\u9700\u8981\u5927\u5e45\u6270\u52a8\u7684\u9690\u79c1\u8981\u6c42\u548c\u5bf9\u6b64\u6270\u52a8\u654f\u611f\u7684\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5408\u6210\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u91c7\u7528\"\u5408\u6210-\u6df7\u5408\"\u7b56\u7565\uff0c\u5148\u751f\u6210\u7eaf\u5408\u6210\u6570\u636e\uff0c\u518d\u4e0e\u539f\u59cb\u6570\u636e\u878d\u5408\uff1b\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u6838\u5cad\u56de\u5f52(KRR)\u7684\u5408\u6210\u7b56\u7565\uff0c\u5148\u5728\u539f\u59cb\u6570\u636e\u4e0a\u8bad\u7ec3KRR\u6a21\u578b\uff0c\u7136\u540e\u7528\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7684\u5408\u6210\u8f93\u5165\u751f\u6210\u5408\u6210\u8f93\u51fa\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u7edf\u8ba1\u9a71\u52a8\u548c\u53d7\u9650\u9690\u79c1-\u9884\u6d4b\u6743\u8861\u7684\u7279\u6027\uff0c\u5e76\u5728\u8425\u9500\u95ee\u9898\u548c\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u5408\u6210\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u7edf\u8ba1\u9a71\u52a8\u7684\u53d7\u9650\u9690\u79c1-\u9884\u6d4b\u6743\u8861\uff0c\u5e76\u4fdd\u8bc1\u6700\u4f18\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u5408\u6210\u6570\u636e\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u4e0b\u6e38\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08660", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08660", "abs": "https://arxiv.org/abs/2602.08660", "authors": ["Alexandre Verine", "Rafael Pinot", "Florian Le Bronnec"], "title": "Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models", "comment": null, "summary": "Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\u516c\u5e73\u6027\u5b9a\u4e49EGT\uff0c\u8981\u6c42\u6240\u6709\u654f\u611f\u7fa4\u4f53\u5177\u6709\u53ef\u6bd4\u7684\u751f\u6210\u8d28\u91cf\uff0c\u800c\u975e\u4ec5\u5e73\u8861\u751f\u6210\u6982\u7387\uff0c\u5e76\u901a\u8fc7min-max\u5fae\u8c03\u65b9\u6cd5\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u7684\u516c\u5e73\u6027\u6982\u5ff5\u4e3b\u8981\u4ece\u5206\u7c7b\u4efb\u52a1\u4e2d\u501f\u9274\uff0c\u4fa7\u91cd\u4e8e\u5e73\u8861\u5404\u654f\u611f\u7fa4\u4f53\u7684\u751f\u6210\u6982\u7387\uff0c\u4f46\u8fd9\u4e9b\u6807\u51c6\u5f88\u8106\u5f31\uff0c\u56e0\u4e3a\u5373\u4f7f\u4e0d\u540c\u654f\u611f\u7fa4\u4f53\u7684\u5efa\u6a21\u8d28\u91cf\u5dee\u5f02\u5f88\u5927\uff0c\u4e5f\u80fd\u6ee1\u8db3\u8fd9\u4e9b\u6807\u51c6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u6765\u786e\u4fdd\u6240\u6709\u7fa4\u4f53\u90fd\u80fd\u83b7\u5f97\u53ef\u6bd4\u7684\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u65b0\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u2014\u2014\u5e73\u7b49\u751f\u6210\u5904\u7406\uff08EGT\uff09\uff0c\u8981\u6c42\u6240\u6709\u654f\u611f\u7fa4\u4f53\u5177\u6709\u53ef\u6bd4\u7684\u751f\u6210\u8d28\u91cf\uff0c\u8d28\u91cf\u901a\u8fc7\u53c2\u8003f-\u6563\u5ea6\u6765\u8861\u91cf\u3002\u5206\u6790\u4e86EGT\u5f15\u53d1\u7684\u6743\u8861\uff0c\u8868\u660e\u5f3a\u5236\u6267\u884c\u516c\u5e73\u7ea6\u675f\u5fc5\u7136\u5c06\u6574\u4f53\u6a21\u578b\u8d28\u91cf\u4e0e\u6700\u96be\u8fd1\u4f3c\u7684\u7fa4\u4f53\u8d28\u91cf\u8026\u5408\u3002\u57fa\u4e8e\u8fd9\u4e00\u7406\u8bba\u6d1e\u5bdf\uff0c\u63d0\u51fa\u7b80\u5355\u9ad8\u6548\u7684min-max\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u5404\u654f\u611f\u7fa4\u4f53\u7684f-\u6563\u5ea6\u6765\u6ee1\u8db3EGT\u3002", "result": "\u5728\u56fe\u50cf\u548c\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cmin-max\u65b9\u6cd5\u76f8\u6bd4\u6587\u732e\u4e2d\u7684\u5176\u4ed6\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u90fd\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "EGT\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u516c\u5e73\u6027\u5b9a\u4e49\uff0c\u8d85\u8d8a\u4e86\u4ec5\u5173\u6ce8\u6982\u7387\u5e73\u8861\u7684\u4f20\u7edf\u65b9\u6cd5\u3002min-max\u5fae\u8c03\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0EGT\u516c\u5e73\u6027\uff0c\u5728\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u7684\u540c\u65f6\u786e\u4fdd\u6240\u6709\u654f\u611f\u7fa4\u4f53\u83b7\u5f97\u53ef\u6bd4\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.08676", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08676", "abs": "https://arxiv.org/abs/2602.08676", "authors": ["Tiwei Bie", "Maosong Cao", "Xiang Cao", "Bingsen Chen", "Fuyuan Chen", "Kun Chen", "Lun Du", "Daozhuo Feng", "Haibo Feng", "Mingliang Gong", "Zhuocheng Gong", "Yanmei Gu", "Jian Guan", "Kaiyuan Guan", "Hongliang He", "Zenan Huang", "Juyong Jiang", "Zhonghui Jiang", "Zhenzhong Lan", "Chengxi Li", "Jianguo Li", "Zehuan Li", "Huabin Liu", "Lin Liu", "Guoshan Lu", "Yuan Lu", "Yuxin Ma", "Xingyu Mou", "Zhenxuan Pan", "Kaida Qiu", "Yuji Ren", "Jianfeng Tan", "Yiding Tian", "Zian Wang", "Lanning Wei", "Tao Wu", "Yipeng Xing", "Wentao Ye", "Liangyu Zha", "Tianze Zhang", "Xiaolu Zhang", "Junbo Zhao", "Da Zheng", "Hao Zhong", "Wanli Zhong", "Jun Zhou", "Junlin Zhou", "Liwang Zhu", "Muzhi Zhu", "Yihong Zhuang"], "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing", "comment": "11 pages, 3 figures", "summary": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.", "AI": {"tldr": "LLaDA2.1\u901a\u8fc7\u7ed3\u5408Token-to-Token\u7f16\u8f91\u548cMask-to-Token\u65b9\u6848\uff0c\u5f15\u5165\u53ef\u914d\u7f6e\u9608\u503c\u89e3\u7801\uff0c\u63d0\u4f9b\u901f\u5ea6\u6a21\u5f0f\u548c\u8d28\u91cf\u6a21\u5f0f\uff0c\u5e76\u9996\u6b21\u4e3adLLMs\u5b9e\u73b0\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u89e3\u7801\u901f\u5ea6\u3002", "motivation": "\u867d\u7136LLaDA2.0\u5c55\u793a\u4e86100B\u7ea7\u522b\u5757\u6269\u6563\u6a21\u578b\u7684\u6269\u5c55\u6f5c\u529b\u548c\u5185\u5728\u5e76\u884c\u5316\u80fd\u529b\uff0c\u4f46\u89e3\u7801\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u5fae\u5999\u5e73\u8861\u4e00\u76f4\u662f\u4e00\u4e2a\u96be\u4ee5\u7a81\u7834\u7684\u74f6\u9888\u3002", "method": "1. \u5c06Token-to-Token\u7f16\u8f91\u65e0\u7f1d\u96c6\u6210\u5230\u4f20\u7edf\u7684Mask-to-Token\u65b9\u6848\u4e2d\uff0c\u5f15\u5165\u8054\u5408\u53ef\u914d\u7f6e\u9608\u503c\u89e3\u7801\u65b9\u6848\uff1b2. \u521b\u5efa\u4e24\u79cd\u6a21\u5f0f\uff1a\u901f\u5ea6\u6a21\u5f0f\uff08\u964d\u4f4eM2T\u9608\u503c\uff0c\u4f9d\u8d56T2T\u4f18\u5316\u8f93\u51fa\uff09\u548c\u8d28\u91cf\u6a21\u5f0f\uff08\u4fdd\u5b88\u9608\u503c\u786e\u4fdd\u57fa\u51c6\u6027\u80fd\uff09\uff1b3. \u57fa\u4e8e\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9996\u6b21\u4e3adLLMs\u5b9e\u73b0\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u7a33\u5b9a\u68af\u5ea6\u4f30\u8ba1\u6280\u672f\u3002", "result": "\u572833\u4e2a\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLaDA2.1\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u4efb\u52a1\u6027\u80fd\u548c\u95ea\u7535\u822c\u7684\u89e3\u7801\u901f\u5ea6\u3002\u5c3d\u7ba1\u6709100B\u53c2\u6570\u89c4\u6a21\uff0c\u5728\u7f16\u7801\u4efb\u52a1\u4e0a\u8fbe\u5230\u60ca\u4eba\u6027\u80fd\uff1aHumanEval+\u4e0a892 TPS\uff0cBigCodeBench\u4e0a801 TPS\uff0cLiveCodeBench\u4e0a663 TPS\u3002\u53d1\u5e03\u4e86LLaDA2.1-Mini\uff0816B\uff09\u548cLLaDA2.1-Flash\uff08100B\uff09\u4e24\u4e2a\u7248\u672c\u3002", "conclusion": "LLaDA2.1\u901a\u8fc7\u7ed3\u6784\u521b\u65b0\u548c\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\uff0c\u6210\u529f\u7a81\u7834\u4e86\u6269\u6563\u6a21\u578b\u89e3\u7801\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u63a8\u7406\u7cbe\u5ea6\u548c\u6307\u4ee4\u8ddf\u968f\u4fdd\u771f\u5ea6\uff0c\u8fd8\u5f25\u5408\u4e86\u6269\u6563\u52a8\u529b\u5b66\u4e0e\u590d\u6742\u4eba\u7c7b\u610f\u56fe\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2602.08679", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08679", "abs": "https://arxiv.org/abs/2602.08679", "authors": ["Yanzhang Fu", "Zizheng Guo", "Jizhou Luo"], "title": "Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks", "comment": null, "summary": "Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDashed Line Defense (DLD)\uff0c\u4e00\u79cd\u9488\u5bf9\u57fa\u4e8e\u5206\u6570\u7684\u67e5\u8be2\u653b\u51fb\u7684\u8fd0\u884c\u65f6\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u635f\u5931\u503c\u6a21\u7cca\u6027\u6765\u62b5\u5fa1\u81ea\u9002\u5e94\u653b\u51fb\u7b56\u7565\u3002", "motivation": "\u57fa\u4e8e\u5206\u6570\u7684\u67e5\u8be2\u653b\u51fb\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u800c\u73b0\u6709\u7684\u8fd0\u884c\u65f6\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6a21\u578b\u53c2\u6570\u8bbf\u95ee\u6743\u9650\uff0c\u8981\u4e48\u5728\u9762\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u65f6\u5931\u6548\u3002\u4f5c\u8005\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u5373\u63d2\u5373\u7528\u9632\u5fa1\u4e5f\u80fd\u88ab\u81ea\u9002\u5e94\u653b\u51fb\u7ed5\u8fc7\uff0c\u8fd9\u66b4\u9732\u4e86\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faDashed Line Defense (DLD)\uff0c\u8fd9\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u89c2\u5bdf\u5230\u7684\u635f\u5931\u503c\u4e0e\u771f\u5b9e\u5bf9\u6297\u5f3a\u5ea6\u4e4b\u95f4\u5f15\u5165\u6a21\u7cca\u6027\uff0c\u963b\u6b62\u653b\u51fb\u8005\u53ef\u9760\u5730\u5206\u6790\u548c\u8c03\u6574\u67e5\u8be2\u7b56\u7565\uff0c\u4ece\u800c\u6709\u6548\u7834\u574f\u5bf9\u6297\u6837\u672c\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728ImageNet\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DLD\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660eDLD\u5728\u4fdd\u6301\u6a21\u578b\u9884\u6d4b\u6807\u7b7e\u7684\u540c\u65f6\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u6700\u574f\u60c5\u51b5\u7684\u81ea\u9002\u5e94\u653b\u51fb\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9632\u5fa1\u80fd\u529b\u3002", "conclusion": "DLD\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8fd0\u884c\u65f6\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u591f\u62b5\u5fa1\u81ea\u9002\u5e94\u67e5\u8be2\u653b\u51fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08681", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08681", "abs": "https://arxiv.org/abs/2602.08681", "authors": ["Leander Kurscheidt", "Gabriele Masina", "Roberto Sebastiani", "Antonio Vergari"], "title": "The Theory and Practice of MAP Inference over Non-Convex Constraints", "comment": null, "summary": "In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.\n  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.\n  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.\n  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.\n  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.\n  We evaluate both methods on synthetic and real-world benchmarks, showing our %\n  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u7ea6\u675f\u6700\u5927\u540e\u9a8c\u6982\u7387\u9884\u6d4b\u65b9\u6cd5\uff1a\u4e00\u79cd\u9488\u5bf9\u53ef\u7cbe\u786e\u9ad8\u6548\u6c42\u89e3\u7684\u8fde\u7eed\u53d8\u91cf\u7ea6\u675fMAP\u95ee\u9898\uff0c\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\uff1b\u53e6\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\u5c06\u57df\u5212\u5206\u4e3a\u51f8\u53ef\u884c\u533a\u57df\u5e76\u4e0e\u6570\u503c\u7ea6\u675f\u4f18\u5316\u4ea4\u66ff\u8fdb\u884c\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u6982\u7387\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9700\u8981\u5728\u4ee3\u6570\u7ea6\u675f\u4e0b\u8fdb\u884c\u9884\u6d4b\uff08\u5982\u907f\u5f00\u969c\u788d\u7269\u7684\u6700\u53ef\u80fd\u8f68\u8ff9\uff09\u3002\u8fd9\u4e9b\u73b0\u5b9e\u7ea6\u675f\u901a\u5e38\u975e\u51f8\uff0c\u4e14\u8003\u8651\u7684\u5bc6\u5ea6\u51fd\u6570\u4e5f\u975e\uff08\u5bf9\u6570\uff09\u51f9\uff0c\u4f7f\u5f97\u7ea6\u675fMAP\u9884\u6d4b\u7684\u8ba1\u7b97\u65e2\u56f0\u96be\u53c8\u4e0d\u53ef\u9760\u3002", "method": "1. \u7814\u7a76\u8fde\u7eed\u53d8\u91cf\u7ea6\u675fMAP\u63a8\u65ad\u53ef\u7cbe\u786e\u9ad8\u6548\u6c42\u89e3\u7684\u6761\u4ef6\uff0c\u5e76\u4e3a\u6b64\u53ef\u5904\u7406\u7247\u6bb5\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u30022. \u63d0\u51fa\u901a\u7528\u7ea6\u675fMAP\u7b56\u7565\uff0c\u5c06\u57df\u5212\u5206\u4e3a\u51f8\u53ef\u884c\u533a\u57df\u5e76\u4e0e\u6570\u503c\u7ea6\u675f\u4f18\u5316\u4ea4\u66ff\u8fdb\u884c\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u4f18\u4e8e\u65e0\u89c6\u7ea6\u675f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u590d\u6742\u5bc6\u5ea6\u51fd\u6570\uff0c\u8fd9\u4e9b\u5bc6\u5ea6\u51fd\u6570\u5bf9\u73b0\u6709\u6700\u5148\u8fdb\u7684\u7cbe\u786e\u6c42\u89e3\u5668\u6765\u8bf4\u662f\u96be\u4ee5\u5904\u7406\u7684\u3002", "conclusion": "\u672c\u6587\u4e3a\u7ea6\u675fMAP\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u80fd\u5904\u7406\u53ef\u7cbe\u786e\u6c42\u89e3\u7684\u7279\u4f8b\uff0c\u53c8\u80fd\u5e94\u5bf9\u66f4\u4e00\u822c\u7684\u590d\u6742\u7ea6\u675f\u573a\u666f\uff0c\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.08686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08686", "abs": "https://arxiv.org/abs/2602.08686", "authors": ["Ning Yang", "Chengzhi Wang", "Yibo Liu", "Baoliang Tian", "Haijun Zhang"], "title": "CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation", "comment": null, "summary": "Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.", "AI": {"tldr": "CompilerKV\u662f\u4e00\u4e2a\u98ce\u9669\u81ea\u9002\u5e94\u548c\u5934\u90e8\u611f\u77e5\u7684KV\u7f13\u5b58\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u7ecf\u9a8c\u7f16\u8bd1\u6210\u53ef\u91cd\u7528\u51b3\u7b56\u8868\uff0c\u5728512\u4ee4\u724c\u9884\u7b97\u4e0b\u6062\u590d97.7%\u7684FullKV\u6027\u80fd\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2dLLMs\u53d7\u9650\u4e8eKV\u7f13\u5b58\u5185\u5b58\u7684\u7ebf\u6027\u589e\u957f\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u9759\u6001\u9608\u503c\u548c\u6ce8\u610f\u529b\u542f\u53d1\u5f0f\uff0c\u8981\u4e48\u91c7\u7528\u7c97\u7c92\u5ea6\u5185\u5b58\u5206\u914d\uff0c\u5ffd\u7565\u4e86\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\uff1a\u63d0\u793a\u76f8\u5173\u7684\u538b\u7f29\u98ce\u9669\u53d8\u5316\u548c\u6ce8\u610f\u529b\u5934\u4e4b\u95f4\u7684\u529f\u80fd\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u4ee4\u724c\u9009\u62e9\u4e0d\u7a33\u5b9a\u548c\u5c3e\u90e8\u5931\u8d25\u3002", "method": "\u63d0\u51faCompilerKV\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a1) \u901a\u8fc7\u79bb\u7ebf\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u5b66\u4e60\u7684\u5934\u90e8\u5f02\u8d28\u6027\u8868\uff0c\u4e3a\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u5206\u914d\u7279\u5b9a\u53ef\u9760\u6027\u6743\u91cd\uff1b2) \u98ce\u9669\u81ea\u9002\u5e94\u9608\u503c\u95e8\u63a7\u673a\u5236\uff0c\u8054\u5408\u5efa\u6a21\u6ce8\u610f\u529b\u71b5\u548c\u5c40\u90e8\u56f0\u60d1\u5ea6\uff0c\u5c06\u63d0\u793a\u7ea7\u98ce\u9669\u8f6c\u5316\u4e3a\u53ef\u90e8\u7f72\u7684\u4fdd\u7559\u9608\u503c\u3002", "result": "\u5728LongBench\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728512\u4ee4\u724c\u9884\u7b97\u4e0b\uff0cCompilerKV\u4e3b\u5bfcSOTA\u65b9\u6cd5\uff0c\u6062\u590d97.7%\u7684FullKV\u6027\u80fd\uff0c\u76f8\u6bd4\u6700\u5f3a\u7ade\u4e89\u5bf9\u624b\u83b7\u5f97\u9ad8\u8fbe+5.2\u5206\u7684\u63d0\u5347\u3002", "conclusion": "CompilerKV\u901a\u8fc7\u98ce\u9669\u81ea\u9002\u5e94\u548c\u5934\u90e8\u611f\u77e5\u7684\u538b\u7f29\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLMs\u4e2dKV\u7f13\u5b58\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6027\u80fd\u3002"}}
{"id": "2602.08689", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08689", "abs": "https://arxiv.org/abs/2602.08689", "authors": ["Constant Bourdrez", "Alexandre V\u00e9rine", "Olivier Capp\u00e9"], "title": "Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning", "comment": "Preprint", "summary": "Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u9006\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u7b56\u7565\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u53bb\u566a\u5668\uff0c\u901a\u8fc7\u5c06\u91c7\u6837\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6765\u4f18\u5316\u91c7\u6837\u52a8\u6001", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f46\u91c7\u6837\u8fc7\u7a0b\u5177\u6709\u7075\u6d3b\u6027\u3002\u5229\u7528\u8fd9\u79cd\u7075\u6d3b\u6027\u53ef\u4ee5\u6539\u8fdb\u751f\u6210\u6837\u672c\u8d28\u91cf\u548c\u91c7\u6837\u6548\u7387\uff0c\u4f46\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u53bb\u566a\u5668\u5c31\u80fd\u4f18\u5316\u91c7\u6837\u7b56\u7565\u7684\u65b9\u6cd5", "method": "\u5c06\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u79bb\u6563\u65f6\u95f4\u6709\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5176\u4e2d\u52a8\u4f5c\u5bf9\u5e94\u91c7\u6837\u52a8\u6001\u7684\u53ef\u9009\u4fee\u6539\u3002\u4f7f\u7528\u9006\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u6280\u672f\u76f4\u63a5\u5339\u914d\u76ee\u6807\u884c\u4e3a\uff0c\u907f\u514d\u5b9a\u4e49\u663e\u5f0f\u5956\u52b1\u51fd\u6570", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u6837\u672c\u7684\u8d28\u91cf\uff0c\u5e76\u81ea\u52a8\u8c03\u6574\u91c7\u6837\u8d85\u53c2\u6570", "conclusion": "\u63d0\u51fa\u7684\u9006\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e3a\u4f18\u5316\u6269\u6563\u6a21\u578b\u91c7\u6837\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u53bb\u566a\u5668\uff0c\u65e2\u80fd\u63d0\u5347\u6837\u672c\u8d28\u91cf\u53c8\u80fd\u63d0\u9ad8\u91c7\u6837\u6548\u7387"}}
{"id": "2602.08695", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08695", "abs": "https://arxiv.org/abs/2602.08695", "authors": ["Evan Peters", "Ando Deng", "Matheus H. Zambianco", "Devin Blankespoor", "Achim Kempf"], "title": "Trapped by simplicity: When Transformers fail to learn from noisy features", "comment": "13+12 pages, 7 figures. Accepted at ICLR 2026", "summary": "Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.", "AI": {"tldr": "Transformer\u5728\u566a\u58f0\u7279\u5f81\u6570\u636e\u8bad\u7ec3\u540e\uff0c\u5bf9\u65e0\u566a\u58f0\u7279\u5f81\u7684\u76ee\u6807\u51fd\u6570\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u5728\u968f\u673a\u5e03\u5c14\u51fd\u6570\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800cLSTM\u8868\u73b0\u66f4\u5dee", "motivation": "\u7814\u7a76\u5728\u5b58\u5728\u7279\u5f81\u566a\u58f0\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\u7684Transformer\u662f\u5426\u80fd\u591f\u6b63\u786e\u6cdb\u5316\u5230\u65e0\u566a\u58f0\u8f93\u5165\uff0c\u5373\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u95ee\u9898", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4Transformer\u548cLSTM\u5728k\u7a00\u758f\u5947\u5076\u51fd\u6570\u3001\u591a\u6570\u51fd\u6570\u548c\u968f\u673ak-juntas\u4e0a\u7684\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u80fd\u529b\uff0c\u5206\u6790Transformer\u5931\u8d25\u539f\u56e0\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5", "result": "Transformer\u5728k\u7a00\u758f\u5947\u5076\u51fd\u6570\u548c\u591a\u6570\u51fd\u6570\u4e0a\u6210\u529f\u5b9e\u73b0\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\uff0c\u4f46\u5728\u968f\u673ak-juntas\u4e0a\u5931\u8d25\uff1bLSTM\u5728\u7279\u5f81\u566a\u58f0\u4e0b\u8868\u73b0\u66f4\u5dee\uff1bTransformer\u5931\u8d25\u6e90\u4e8e\u5bf9\u7b80\u5355\u51fd\u6570\u7684\u504f\u597d\u548c\u6700\u4f18\u566a\u58f0\u9c81\u68d2\u51fd\u6570\u7075\u654f\u5ea6\u8f83\u4f4e", "conclusion": "Transformer\u5728\u7279\u5f81\u566a\u58f0\u4e0b\u5b66\u4e60\u5e03\u5c14\u51fd\u6570\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0\u60e9\u7f5a\u9ad8\u7075\u654f\u5ea6\u89e3\u7684\u635f\u5931\u9879\u6765\u6539\u5584\uff1bTransformer\u7684\u7b80\u5355\u6027\u504f\u597d\u4e0e\u566a\u58f0\u9c81\u68d2\u5b66\u4e60\u8981\u6c42\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81"}}
{"id": "2602.08722", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08722", "abs": "https://arxiv.org/abs/2602.08722", "authors": ["Dalton Jones", "Junyoung Park", "Matthew Morse", "Mingu Lee", "Chris Lott", "Harper Langston"], "title": "QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill", "comment": null, "summary": "We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.", "AI": {"tldr": "QUOKA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u786c\u4ef6\u65e0\u5173\u7684\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5730\u4fdd\u7559\u4ee3\u8868\u6027\u67e5\u8be2\u548c\u76f8\u5173\u7684\u952e\u503c\u5bf9\u6765\u52a0\u901fTransformer\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b03-7\u500d\u7684\u52a0\u901f\u3002", "motivation": "Transformer\u63a8\u7406\u4e2d\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u7279\u522b\u662f\u5728\u5206\u5757\u9884\u586b\u5145\u9636\u6bb5\u3002\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u67e5\u8be2\u53ea\u5173\u6ce8\u4e00\u5c0f\u90e8\u5206\u952e\uff0c\u800c\u4f4e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u67e5\u8be2\u5bf9\u6700\u7ec8\u6ce8\u610f\u529b\u5bf9\u6570\u6709\u66f4\u5927\u8d21\u732e\uff0c\u8fd9\u4e3a\u9009\u62e9\u6027\u6ce8\u610f\u529b\u8ba1\u7b97\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "QUOKA\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u9996\u5148\u4fdd\u7559\u4e00\u5c0f\u90e8\u5206\u4ee3\u8868\u6027\u67e5\u8be2\uff08\u7279\u522b\u662f\u4f4e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u67e5\u8be2\uff09\uff1b2\uff09\u7136\u540e\u9009\u62e9\u4e0e\u8fd9\u4e9b\u67e5\u8be2\u6700\u76f8\u5173\u7684\u952e\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u786c\u4ef6\u65e0\u5173\uff0c\u9002\u7528\u4e8e\u5206\u5757\u9884\u586b\u5145\u573a\u666f\u3002", "result": "\u5728Needle-In-A-Haystack\u3001LongBench\u3001RULER\u548cMath500\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQUOKA\u5b9e\u73b0\u4e86\u9996\u4ee4\u724c\u751f\u6210\u65f6\u95f4\u51cf\u5c113\u500d\uff0cNvidia GPU\u4e0a\u6ce8\u610f\u529b\u8ba1\u7b97\u52a0\u901f5\u500d\uff0cIntel Xeon CPU\u4e0a\u52a0\u901f\u8fd17\u500d\uff0c\u540c\u65f6\u4f7f\u752888%\u66f4\u5c11\u7684\u952e\u503c\u5bf9\uff0c\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u51c6\u786e\u6027\u3002", "conclusion": "QUOKA\u901a\u8fc7\u57fa\u4e8e\u67e5\u8be2\u76f8\u4f3c\u5ea6\u7684\u9009\u62e9\u6027\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u6709\u6548\u52a0\u901fTransformer\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u9ad8\u6548\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08733", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08733", "abs": "https://arxiv.org/abs/2602.08733", "authors": ["Maximilian Mauel", "Johannes R. H\u00fcbers", "David Berghaus", "Patrick Seifner", "Ramses J. Sanchez"], "title": "Foundation Inference Models for Ordinary Differential Equations", "comment": null, "summary": "Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.", "AI": {"tldr": "FIM-ODE\u662f\u4e00\u79cd\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u63a8\u7406\u6a21\u578b\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u4ece\u566a\u58f0\u8f68\u8ff9\u6570\u636e\u9884\u6d4bODE\u5411\u91cf\u573a\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u6d41\u7a0b\u6216\u5927\u91cf\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u3002", "motivation": "\u5f53\u524dODE\u5411\u91cf\u573a\u63a8\u65ad\u65b9\u6cd5\uff08\u5982\u7b26\u53f7\u56de\u5f52\u3001\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u3001\u795e\u7ecfODE\uff09\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u8bad\u7ec3\u6d41\u7a0b\u3001\u5927\u91cf\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\uff0c\u6216\u4e25\u91cd\u4f9d\u8d56\u7cfb\u7edf\u7279\u5b9a\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u79d1\u5b66\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faFIM-ODE\u9884\u8bad\u7ec3\u57fa\u7840\u63a8\u7406\u6a21\u578b\uff0c\u5728\u4f4e\u6b21\u591a\u9879\u5f0f\u5411\u91cf\u573a\u7684ODE\u5148\u9a8c\u5206\u5e03\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u795e\u7ecf\u7b97\u5b50\u8868\u793a\u76ee\u6807\u5411\u91cf\u573a\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u4ece\u566a\u58f0\u8f68\u8ff9\u6570\u636e\u9884\u6d4b\u5411\u91cf\u573a\u3002", "result": "FIM-ODE\u5728\u96f6\u6837\u672c\u6027\u80fd\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86\u6700\u8fd1\u7684\u9884\u8bad\u7ec3\u7b26\u53f7\u57fa\u51c6ODEFormer\uff1b\u9884\u8bad\u7ec3\u4e3a\u5fae\u8c03\u63d0\u4f9b\u4e86\u5f3a\u521d\u59cb\u5316\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u7a33\u5b9a\u7684\u9002\u5e94\uff0c\u4f18\u4e8e\u73b0\u4ee3\u795e\u7ecf\u548cGP\u57fa\u7ebf\uff0c\u4e14\u65e0\u9700\u673a\u5668\u5b66\u4e60\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "FIM-ODE\u901a\u8fc7\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86ODE\u5411\u91cf\u573a\u63a8\u65ad\u6d41\u7a0b\uff0c\u964d\u4f4e\u4e86\u4f7f\u7528\u95e8\u69db\uff0c\u4e3a\u79d1\u5b66\u5efa\u6a21\u4e2d\u7684ODE\u63a8\u65ad\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08755", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08755", "abs": "https://arxiv.org/abs/2602.08755", "authors": ["Duc-Anh Nguyen", "Nhien-An Le-Khac"], "title": "Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views", "comment": null, "summary": "Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.", "AI": {"tldr": "RALIS\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u591a\u89c6\u56fe\u5b66\u4e60\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u8c03\u6574\u7684\u4e2d\u5fc3\u5bf9\u6bd4\u635f\u5931\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u652f\u6301\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\u7684\u4efb\u610f\u89c6\u56fe\u53ef\u7528\u6027\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(V\u00b2)\u964d\u4f4e\u5230O(V)\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u591a\u89c6\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u7075\u6d3b\u89c6\u56fe\u914d\u7f6e\uff08\u5305\u62ec\u4efb\u610f\u89c6\u56fe\u7ec4\u5408\u3001\u89c6\u56fe\u6570\u91cf\u548c\u5f02\u6784\u6a21\u6001\uff09\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4efb\u52a1\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u8bad\u7ec3\u548c\u63a8\u7406\u671f\u95f4\u4efb\u610f\u89c6\u56fe\u53ef\u7528\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRALIS\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u3002\u4f7f\u7528\u8c03\u6574\u7684\u4e2d\u5fc3\u5bf9\u6bd4\u635f\u5931\u8fdb\u884c\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u548c\u89c6\u56fe\u5bf9\u9f50\uff0c\u51cf\u8f7b\u7f3a\u5931\u89c6\u56fe\u5bf9\u591a\u89c6\u56fe\u878d\u5408\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u5904\u7406\u5bf9\u6bd4\u5b66\u4e60\u672a\u6355\u83b7\u7684\u6b8b\u5dee\u5f02\u5e38\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u8d1f\u8f7d\u5e73\u8861\u7b56\u7565\u9002\u5e94\u4efb\u610f\u89c6\u56fe\u7ec4\u5408\u3002", "result": "\u5728\u5305\u542b\u60ef\u6027\u548c\u4eba\u4f53\u59ff\u6001\u6a21\u6001\u7684\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u89c6\u56fe\u6570\u91cf\u4ece3\u52309\u4e2a\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(V\u00b2)\u964d\u4f4e\u5230O(V)\u3002", "conclusion": "RALIS\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u6bd4\u635f\u5931\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u591a\u89c6\u56fe\u5b66\u4e60\u4e2d\u7684\u7075\u6d3b\u89c6\u56fe\u914d\u7f6e\u95ee\u9898\uff0c\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.08768", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.08768", "abs": "https://arxiv.org/abs/2602.08768", "authors": ["Chi-Sheng Chen", "Xinyu Zhang", "En-Jui Kuo", "Guan-Ying Chen", "Qiuzhe Xie", "Fan Zhang"], "title": "FreqLens: Interpretable Frequency Attribution for Time Series Forecasting", "comment": null, "summary": "Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \\textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \\textsc{FreqLens} introduces two key innovations: (1) \\emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \\emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \\textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \\pm 0.1$h, 2.5\\% error) and 12-hour half-daily cycle ($11.8 \\pm 0.1$h, 1.6\\% error) on Traffic, and weekly cycles ($10\\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.", "AI": {"tldr": "FreqLens\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u9891\u7387\u5206\u91cf\u53d1\u73b0\u548c\u7406\u8bba\u4fdd\u8bc1\u7684\u9891\u7387\u5f52\u56e0\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u53ef\u89e3\u91ca\u9884\u6d4b\u7684\u9886\u57df\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u4f9b\u51c6\u786e\u9884\u6d4b\u53c8\u80fd\u89e3\u91ca\u9884\u6d4b\u4f9d\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFreqLens\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u53ef\u5b66\u4e60\u9891\u7387\u53d1\u73b0 - \u901a\u8fc7sigmoid\u6620\u5c04\u53c2\u6570\u5316\u9891\u7387\u57fa\uff0c\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u81ea\u52a8\u53d1\u73b0\u4e3b\u8981\u5468\u671f\u6027\u6a21\u5f0f\uff1b2) \u516c\u7406\u5316\u9891\u7387\u5f52\u56e0 - \u57fa\u4e8e\u7406\u8bba\u6846\u67b6\uff0c\u6ee1\u8db3\u5b8c\u5907\u6027\u3001\u5fe0\u5b9e\u6027\u3001\u96f6\u9891\u7387\u548c\u5bf9\u79f0\u6027\u516c\u7406\uff0c\u9891\u7387\u5f52\u56e0\u7b49\u4ef7\u4e8eShapley\u503c\u3002", "result": "\u5728Traffic\u548cWeather\u6570\u636e\u96c6\u4e0a\uff0cFreqLens\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u7269\u7406\u4e0a\u6709\u610f\u4e49\u7684\u9891\u7387\uff1a\u5728Traffic\u6570\u636e\u4e2d\u6240\u67095\u6b21\u72ec\u7acb\u8fd0\u884c\u90fd\u53d1\u73b0\u4e8624\u5c0f\u65f6\u65e5\u5468\u671f\uff0824.6\u00b10.1h\uff0c2.5%\u8bef\u5dee\uff09\u548c12\u5c0f\u65f6\u534a\u65e5\u5468\u671f\uff0811.8\u00b10.1h\uff0c1.6%\u8bef\u5dee\uff09\uff0c\u5728Weather\u6570\u636e\u4e2d\u53d1\u73b0\u4e86\u5468\u5468\u671f\uff08\u6bd4\u8f93\u5165\u7a97\u53e3\u957f10\u500d\uff09\u3002", "conclusion": "FreqLens\u5c55\u793a\u4e86\u5728\u9891\u7387\u7ea7\u522b\u4e0a\u7684\u771f\u6b63\u77e5\u8bc6\u53d1\u73b0\uff0c\u540c\u65f6\u5177\u6709\u5f52\u56e0\u8d28\u91cf\u7684\u6b63\u5f0f\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.08774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08774", "abs": "https://arxiv.org/abs/2602.08774", "authors": ["Nicol\u00e1s Villagr\u00e1n Prieto", "Eduardo C. Garrido-Merch\u00e1n"], "title": "Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization", "comment": null, "summary": "Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.", "AI": {"tldr": "\u8d1d\u53f6\u65af\u4f18\u5316\u4f7f\u7528\u5e93\u9ed8\u8ba4\u8d85\u53c2\u6570\u4f5c\u4e3a\u521d\u59cb\u5316\u70b9\u5e76\u4e0d\u80fd\u5e26\u6765\u7edf\u8ba1\u663e\u8457\u7684\u4f18\u52bf\uff0c\u4e0e\u968f\u673a\u521d\u59cb\u5316\u76f8\u6bd4\u6700\u7ec8\u6027\u80fd\u65e0\u5dee\u5f02", "motivation": "\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u5229\u7528\u673a\u5668\u5b66\u4e60\u5e93\uff08\u5982scikit-learn\uff09\u4e2d\u9ed8\u8ba4\u8d85\u53c2\u6570\u503c\u6240\u9690\u542b\u7684\u4e13\u5bb6\u77e5\u8bc6\u6765\u52a0\u901f\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6536\u655b\u8fc7\u7a0b", "method": "\u4f7f\u7528\u4ee5\u5e93\u9ed8\u8ba4\u503c\u4e3a\u4e2d\u5fc3\u7684\u9ad8\u65af\u5206\u5e03\u521d\u59cb\u5316\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u5e76\u4e0e\u5747\u5300\u968f\u673a\u521d\u59cb\u5316\u8fdb\u884c\u5bf9\u6bd4\u3002\u5b9e\u9a8c\u6db5\u76d6\u4e09\u4e2aBO\u540e\u7aef\u3001\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u548c\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u9884\u6d4b\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\uff0c\u4f7f\u7528\u5355\u4fa7\u4e8c\u9879\u68c0\u9a8c\u786e\u5b9a\u7edf\u8ba1\u663e\u8457\u6027", "result": "\u5728\u6240\u6709\u5b9e\u9a8c\u6761\u4ef6\u4e0b\uff0c\u57fa\u4e8e\u9ed8\u8ba4\u503c\u7684\u521d\u59cb\u5316\u76f8\u6bd4\u7eaf\u968f\u673a\u91c7\u6837\u6ca1\u6709\u7edf\u8ba1\u663e\u8457\u4f18\u52bf\uff08p\u503c\u8303\u56f40.141-0.908\uff09\u3002\u867d\u7136\u66f4\u7d27\u5bc6\u5730\u56f4\u7ed5\u9ed8\u8ba4\u503c\u53ef\u4ee5\u6539\u5584\u65e9\u671f\u8bc4\u4f30\uff0c\u4f46\u8fd9\u79cd\u77ed\u6682\u4f18\u52bf\u968f\u7740\u4f18\u5316\u8fdb\u5c55\u800c\u6d88\u5931\uff0c\u6700\u7ec8\u6027\u80fd\u4fdd\u6301\u4e0d\u53d8", "conclusion": "\u5e93\u9ed8\u8ba4\u8d85\u53c2\u6570\u5e76\u672a\u5305\u542b\u5bf9\u4f18\u5316\u6709\u7528\u7684\u65b9\u5411\u6027\u4fe1\u606f\uff0c\u5efa\u8bae\u5c06\u8d85\u53c2\u6570\u8c03\u4f18\u4f5c\u4e3a\u6a21\u578b\u5f00\u53d1\u7684\u5fc5\u8981\u90e8\u5206\uff0c\u91c7\u7528\u57fa\u4e8e\u6570\u636e\u7684\u641c\u7d22\u7b56\u7565\u800c\u975e\u4f9d\u8d56\u5e93\u9ed8\u8ba4\u503c\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5"}}
{"id": "2602.08809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08809", "abs": "https://arxiv.org/abs/2602.08809", "authors": ["Karim Haroun", "Aya Zitouni", "Aicha Zenakhri", "Meriem Amel Guessoum", "Larbi Boubchir"], "title": "Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI", "comment": "8 pages, 2 figures, accepted at the 2025 IEEE SDS conference", "summary": "Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.", "AI": {"tldr": "\u672c\u6587\u7b80\u8981\u7efc\u8ff0\u4e86\u751f\u7269\u8bc6\u522b\u5e94\u7528\u4e2d\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u73b0\u72b6\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u8bad\u7ec3\u548c\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5b89\u5168\u9632\u5fa1\u7b49\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u8bad\u7ec3\u548c\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u5de8\u5927\u7684\u8ba1\u7b97\u9700\u6c42\u5bfc\u81f4\u9ad8\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u5e94\u7528\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u7684\u65b9\u5f0f\uff0c\u5bf9\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\u6574\u7406\uff0c\u63d0\u51fa\u8bc4\u4f30\u6a21\u578b\u6548\u7387\u7684\u8865\u5145\u6307\u6807\uff08\u5185\u5b58\u3001\u8ba1\u7b97\u3001\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\uff09\uff0c\u5e76\u5021\u5bfc\u5efa\u7acb\u901a\u7528\u4e14\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u63d0\u51fa\u4e86\u591a\u7ef4\u5ea6\u7684\u6548\u7387\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u751f\u7269\u8bc6\u522b\u9886\u57df\u7684\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u6307\u5bfc\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee5\u5e94\u5bf9\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u548c\u73af\u5883\u5f71\u54cd\uff0c\u540c\u65f6\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u7684\u6548\u7387\u8bc4\u4f30\u6307\u6807\u6765\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2602.08813", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08813", "abs": "https://arxiv.org/abs/2602.08813", "authors": ["Mahdi Sabbaghi", "George Pappas", "Adel Javanmard", "Hamed Hassani"], "title": "Robust Policy Optimization to Prevent Catastrophic Forgetting", "comment": null, "summary": "Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.\n  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.", "AI": {"tldr": "FRPO\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u7684RLHF\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316KL\u6709\u754c\u90bb\u57df\u5185\u7684\u7b56\u7565\u6765\u9632\u6b62\u4e0b\u6e38\u5fae\u8c03\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5728\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5b89\u5168\u9000\u5316\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9636\u6bb5\u540e\u8bad\u7ec3\u4e2d\uff0c\u5373\u4f7f\u5c0f\u7684\u4e0b\u6e38\u66f4\u65b0\u4e5f\u4f1a\u635f\u5bb3\u5148\u524d\u5b66\u4e60\u7684\u884c\u4e3a\uff08\u5982\u5b89\u5168\u6027\uff09\uff0c\u8fd9\u79cd\u707e\u96be\u6027\u9057\u5fd8\u8868\u660e\u6807\u51c6RLHF\u76ee\u6807\u4e0d\u80fd\u4fdd\u8bc1\u5bf9\u672a\u6765\u9002\u5e94\u7684\u9c81\u68d2\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e0b\u6e38\u65f6\u7684\u65b9\u6cd5\u6765\u4fdd\u7559\u5148\u524d\u884c\u4e3a\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u9700\u8981\u9884\u5fae\u8c03\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faFine-tuning Robust Policy Optimization (FRPO)\uff0c\u4e00\u4e2a\u9c81\u68d2\u7684RLHF\u6846\u67b6\uff0c\u4e0d\u4ec5\u4f18\u5316\u5f53\u524d\u7b56\u7565\u7684\u5956\u52b1\uff0c\u8fd8\u4f18\u5316\u4e0b\u6e38\u9002\u5e94\u53ef\u8fbe\u7684KL\u6709\u754c\u90bb\u57df\u5185\u7684\u7b56\u7565\u3002\u91c7\u7528\u6700\u5927\u6700\u5c0f\u5316\u516c\u5f0f\u786e\u4fdd\u7b56\u7565\u53d8\u5316\u4e0b\u7684\u5956\u52b1\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u4fee\u6539GRPO\u5f00\u53d1\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u7684\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFRPO\u663e\u8457\u51cf\u5c11\u4e86\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u4e0b\u6e38\u5fae\u8c03\u673a\u5236\uff08SFT\u548cRL\uff09\u4e0b\u7684\u5b89\u5168\u9000\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002\u5728\u6570\u5b66\u805a\u7126\u7684RL\u8bbe\u7f6e\u4e2d\uff0cFRPO\u5728\u540e\u7eed\u5fae\u8c03\u4e0b\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "FRPO\u901a\u8fc7\u9884\u5fae\u8c03\u9c81\u68d2\u6027\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3aRLHF\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u786e\u4fdd\u6a21\u578b\u5728\u4e0b\u6e38\u9002\u5e94\u65f6\u4fdd\u6301\u5148\u524d\u5b66\u4e60\u7684\u91cd\u8981\u884c\u4e3a\u3002"}}
{"id": "2602.08818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08818", "abs": "https://arxiv.org/abs/2602.08818", "authors": ["Annemette Brok Pirchert", "Jacob Nielsen", "Mogens Henrik From", "Lukas Galke Poech", "Peter Schneider-Kamp"], "title": "FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models", "comment": null, "summary": "Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.", "AI": {"tldr": "FlexMoRE\u662f\u4e00\u79cd\u7075\u6d3b\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u652f\u6301\u4e0d\u540c\u79e9\u7684\u4e13\u5bb6\uff08\u5168\u5c3a\u5bf8\u4e13\u5bb6\u6216\u4f4e\u79e9\u9002\u914d\u5668\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u4e13\u5bb6\u79e9\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u901a\u5e38\u8bad\u7ec3\u5168\u5c3a\u5bf8\u4e13\u5bb6\uff0c\u4f46\u4f5c\u8005\u5047\u8bbe\u5e76\u975e\u6240\u6709\u9886\u57df\u90fd\u9700\u8981\u5168\u5c3a\u5bf8\u4e13\u5bb6\uff0c\u4f4e\u79e9\u9002\u914d\u5668\u53ef\u80fd\u5c31\u8db3\u591f\u3002\u4e3a\u4e86\u5728\u5185\u5b58\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u627e\u5230\u6700\u4f73\u5e73\u8861\uff0c\u9700\u8981\u7814\u7a76\u4e13\u5bb6\u79e9\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51faFlexMoRE\uff08Flexible Mixture of Rank-heterogenous Experts\uff09\uff0c\u652f\u6301\u5168\u5c3a\u5bf8\u4e13\u5bb6\u6216\u4e0d\u540c\u79e9\u7684\u4f4e\u79e9\u9002\u914d\u5668\u3002\u57fa\u4e8eFlexOlmo\u6784\u5efa\uff0c\u5c06\u5176\u9884\u8bad\u7ec3\u4e13\u5bb6\u8f6c\u6362\u4e3a\u4f4e\u79e9\u7248\u672c\u3002\u7cfb\u7edf\u8bc4\u4f306\u79cd\u4e0d\u540c\u79e9\u7684\u4e13\u5bb6\uff08\u4ece2^0\u52302^14\uff09\uff0c\u5728150\u79cd\u6df7\u5408\u914d\u7f6e\uff0896\u79cd2\u4e13\u5bb6\u6df7\u5408\uff0c54\u79cd7\u4e13\u5bb6\u6df7\u5408\uff09\u548c120\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u56de\u5f52\u5206\u6790\u663e\u793a\uff0c\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u7684\u6700\u4f73\u79e9\u663e\u8457\u9ad8\u4e8e\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u3002\u4f7f\u7528\u6700\u4f18\u79e9\u65f6\uff0cFlexMoRE\u5728\u5e73\u5747\u5f97\u520647.18\u7684\u60c5\u51b5\u4e0b\uff0c\u53c2\u6570\u6570\u91cf\u4e0d\u5230FlexOlmo\u5168\u5c3a\u5bf8\u4e13\u5bb6\u6df7\u5408\u7684\u4e09\u5206\u4e4b\u4e00\uff0810.75B vs 33.27B\uff09\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0845.46\uff09\u3002", "conclusion": "FlexMoRE\u901a\u8fc7\u7075\u6d3b\u6df7\u5408\u4e0d\u540c\u79e9\u7684\u4e13\u5bb6\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4e13\u5bb6\u79e9\u4f18\u5316\u5bf9\u5185\u5b58\u6548\u7387\u548c\u6027\u80fd\u5e73\u8861\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u4efb\u52a1\u9700\u6c42\u3002"}}
{"id": "2602.08819", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08819", "abs": "https://arxiv.org/abs/2602.08819", "authors": ["Jiwoo Hong", "Shao Tang", "Zhipeng Wang"], "title": "Bayesian Preference Learning for Test-Time Steerable Reward Models", "comment": "Preprint", "summary": "Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.", "AI": {"tldr": "\u63d0\u51fa\u53d8\u5206\u4e0a\u4e0b\u6587\u5956\u52b1\u5efa\u6a21\uff08ICRM\uff09\uff0c\u4e00\u79cd\u8d1d\u53f6\u65af\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u504f\u597d\u6f14\u793a\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u53ef\u5f15\u5bfc\u6027\uff0c\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u8bbe\u7f6e\u4e2d\u90fd\u80fd\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u504f\u597d\u5206\u5e03\u3002", "motivation": "\u968f\u7740\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u548c\u591a\u76ee\u6807\u5bf9\u9f50\u7b49\u573a\u666f\uff0c\u5956\u52b1\u6a21\u578b\u9700\u8981\u7f16\u7801\u66f4\u590d\u6742\u3001\u591a\u65b9\u9762\u7684\u504f\u597d\u5206\u5e03\u3002\u4f46\u4f20\u7edf\u7684\u5206\u7c7b\u5668\u5956\u52b1\u6a21\u578b\u4e00\u65e6\u8bad\u7ec3\u5b8c\u6210\u5c31\u4fdd\u6301\u9759\u6001\uff0c\u9650\u5236\u4e86\u5176\u5728\u6d4b\u8bd5\u65f6\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u53d8\u5206\u4e0a\u4e0b\u6587\u5956\u52b1\u5efa\u6a21\uff08ICRM\uff09\uff0c\u5c06\u5956\u52b1\u5efa\u6a21\u8f6c\u5316\u4e3a\u5728Bradley-Terry\u6a21\u578b\u4e0b\u4f7f\u7528\u5171\u8f6dBeta\u5148\u9a8c\u5bf9\u6f5c\u5728\u504f\u597d\u6982\u7387\u8fdb\u884c\u644a\u9500\u53d8\u5206\u63a8\u65ad\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u504f\u597d\u6f14\u793a\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u53ef\u5f15\u5bfc\u6027\u3002", "result": "ICRM\u5728\u5355\u76ee\u6807\u8bbe\u7f6e\u4e2d\uff0c\u968f\u7740\u66f4\u591a\u4e0a\u4e0b\u6587\u6f14\u793a\uff0c\u5728SafeRLHF\u4e0a\u83b7\u5f9734%\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5728RM-Bench\u4e0a\u83b7\u5f979%\u51c6\u786e\u7387\u63d0\u5347\uff1b\u5728\u591a\u76ee\u6807\u8bbe\u7f6e\u4e2d\uff0c\u5728\u6709\u7528\u6027\u548c\u62d2\u7edd\u57fa\u51c6\u4e0a\uff0c\u5e15\u7d2f\u6258\u524d\u6cbf\u6269\u5c55\u4e864%\u7684\u8d85\u4f53\u79ef\u589e\u76ca\u3002\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cICRM\u80fd\u6709\u6548\u7f16\u7801\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u4f18\u4e8e\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "ICRM\u901a\u8fc7\u8d1d\u53f6\u65af\u6846\u67b6\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u4e86\u5956\u52b1\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u53ef\u5f15\u5bfc\u6027\uff0c\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u8bc1\u660e\u53d8\u5206\u76ee\u6807\u5b58\u5728\u5168\u5c40\u5185\u90e8\u6700\u4f18\u89e3\uff0cKL\u6b63\u5219\u5316\u80fd\u7f13\u89e3\u5956\u52b1\u8fc7\u5ea6\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2602.08847", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08847", "abs": "https://arxiv.org/abs/2602.08847", "authors": ["Lang Feng", "Longtao Zheng", "Shuo He", "Fuxiang Zhang", "Bo An"], "title": "Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems", "comment": "Preprint", "summary": "Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\\% avg@16 and +4.6\\% pass@16 on math, and +15.2\\% avg@16 and +13.1\\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDr. MAS\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u667a\u80fd\u4f53\u5f52\u4e00\u5316\u4f18\u52bf\u503c\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u5e76\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u901a\u8fc7\u89d2\u8272\u4e13\u4e1a\u5316\u5b9e\u73b0\u9ad8\u7ea7\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u7684\u7fa4\u4f53\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u5728\u8fd9\u79cd\u7cfb\u7edf\u4e2d\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u4f5c\u8005\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86GRPO\u98ce\u683c\u4f18\u5316\u4e2d\u5168\u5c40\u5f52\u4e00\u5316\u57fa\u7ebf\u504f\u79bb\u4e0d\u540c\u667a\u80fd\u4f53\u5956\u52b1\u5206\u5e03\u7684\u95ee\u9898\uff0c\u8fd9\u5bfc\u81f4\u68af\u5ea6\u8303\u6570\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faDr. MAS\u65b9\u6cd5\uff0c\u91c7\u7528\u667a\u80fd\u4f53\u7ea7\u522b\u7684\u89e3\u51b3\u65b9\u6848\uff1a\u4f7f\u7528\u6bcf\u4e2a\u667a\u80fd\u4f53\u81ea\u8eab\u7684\u5956\u52b1\u7edf\u8ba1\u4fe1\u606f\u5bf9\u4f18\u52bf\u503c\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4ece\u800c\u6821\u51c6\u68af\u5ea6\u5c3a\u5ea6\u3002\u8be5\u65b9\u6cd5\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684RL\u8bad\u7ec3\u6846\u67b6\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7f16\u6392\u3001\u7075\u6d3b\u7684\u6309\u667a\u80fd\u4f53LLM\u670d\u52a1\u548c\u4f18\u5316\u914d\u7f6e\uff0c\u4ee5\u53ca\u5171\u4eab\u7684LLM\u6267\u884c\u5668\u540e\u7aef\u8d44\u6e90\u8c03\u5ea6\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u591a\u8f6e\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528Qwen2.5\u548cQwen3\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002Dr. MAS\u76f8\u6bd4\u539f\u59cbGRPO\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53475.6%\uff08avg@16\uff09\u548c4.6%\uff08pass@16\uff09\uff0c\u5728\u641c\u7d22\u4efb\u52a1\u4e0a\u63d0\u534715.2%\uff08avg@16\uff09\u548c13.1%\uff08pass@16\uff09\uff0c\u540c\u65f6\u5927\u5e45\u6d88\u9664\u68af\u5ea6\u5c16\u5cf0\u3002\u5728\u5f02\u6784\u667a\u80fd\u4f53\u6a21\u578b\u5206\u914d\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "Dr. MAS\u901a\u8fc7\u667a\u80fd\u4f53\u7ea7\u522b\u7684\u4f18\u52bf\u503c\u5f52\u4e00\u5316\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7b80\u5355\u7a33\u5b9a\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002"}}
{"id": "2602.08857", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.08857", "abs": "https://arxiv.org/abs/2602.08857", "authors": ["Xinting Huang", "Aleksandra Bakalova", "Satwik Bhattamishra", "William Merrill", "Michael Hahn"], "title": "Discovering Interpretable Algorithms by Decompiling Transformers to RASP", "comment": "101 pages, 92 figures", "summary": "Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u8bad\u7ec3\u597d\u7684Transformer\u6a21\u578b\u4e2d\u63d0\u53d6\u7b80\u5355\u53ef\u89e3\u91caRASP\u7a0b\u5e8f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06Transformer\u91cd\u65b0\u53c2\u6570\u5316\u4e3aRASP\u7a0b\u5e8f\u5e76\u5e94\u7528\u56e0\u679c\u5e72\u9884\u6765\u53d1\u73b0\u6700\u5c0f\u7684\u5145\u5206\u5b50\u7a0b\u5e8f\u3002", "motivation": "\u5c3d\u7ba1\u5148\u524d\u7814\u7a76\u8868\u660eTransformer\u7684\u8ba1\u7b97\u53ef\u4ee5\u5728RASP\u7f16\u7a0b\u8bed\u8a00\u65cf\u4e2d\u6a21\u62df\uff0c\u5e76\u4e14Transformer\u5728\u5177\u6709\u7b80\u5355RASP\u7a0b\u5e8f\u7684\u95ee\u9898\u4e0a\u80fd\u591f\u7cbe\u786e\u5730\u8fdb\u884c\u957f\u5ea6\u6cdb\u5316\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b9e\u73b0\u4e86\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u7a0b\u5e8f\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff1a\u9996\u5148\u5c06Transformer\u5fe0\u5b9e\u91cd\u65b0\u53c2\u6570\u5316\u4e3aRASP\u7a0b\u5e8f\uff0c\u7136\u540e\u5e94\u7528\u56e0\u679c\u5e72\u9884\u6765\u53d1\u73b0\u4e00\u4e2a\u5c0f\u7684\u5145\u5206\u5b50\u7a0b\u5e8f\u3002\u5728\u5c0f\u578bTransformer\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u7b97\u6cd5\u548c\u5f62\u5f0f\u8bed\u8a00\u4efb\u52a1\u4e0a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u5e38\u80fd\u591f\u4ece\u957f\u5ea6\u6cdb\u5316\u7684Transformer\u4e2d\u6062\u590d\u51fa\u7b80\u5355\u4e14\u53ef\u89e3\u91ca\u7684RASP\u7a0b\u5e8f\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3aTransformer\u5185\u90e8\u5b9e\u73b0\u7b80\u5355RASP\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u76f4\u63a5\u7684\u8bc1\u636e\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u6210\u529f\u5730\u4ece\u8bad\u7ec3\u597d\u7684Transformer\u4e2d\u63d0\u53d6\u4e86\u7b80\u5355\u53ef\u89e3\u91ca\u7684RASP\u7a0b\u5e8f\uff0c\u4e3a\u7406\u89e3Transformer\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u548c\u8bc1\u636e\u3002"}}
{"id": "2602.08859", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08859", "abs": "https://arxiv.org/abs/2602.08859", "authors": ["Sahel Torkamani", "Henry Gouk", "Rik Sarkar"], "title": "Magnitude Distance: A Geometric Measure of Dataset Similarity", "comment": null, "summary": "Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \\textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \\emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5ea6\u91cf\u7a7a\u95f4magnitude\u6982\u5ff5\u7684\u65b0\u8ddd\u79bb\u5ea6\u91cf\u2014\u2014magnitude distance\uff0c\u8be5\u8ddd\u79bb\u5305\u542b\u53ef\u8c03\u7f29\u653e\u53c2\u6570t\uff0c\u80fd\u63a7\u5236\u5bf9\u5168\u5c40\u7ed3\u6784\uff08\u5c0ft\uff09\u548c\u7ec6\u8282\uff08\u5927t\uff09\u7684\u654f\u611f\u6027\uff0c\u5728\u9002\u5f53\u8c03\u6574\u5c3a\u5ea6\u65f6\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u533a\u5206\u6027\uff0c\u5e76\u53ef\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "motivation": "\u91cf\u5316\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u8ddd\u79bb\u662f\u6570\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u57fa\u672c\u95ee\u9898\uff0c\u73b0\u6709\u8ddd\u79bb\u5ea6\u91cf\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u53ef\u80fd\u5931\u53bb\u533a\u5206\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u4e0d\u540c\u5c3a\u5ea6\u7ed3\u6784\u7684\u65b0\u8ddd\u79bb\u5ea6\u91cf\u3002", "method": "\u57fa\u4e8e\u5ea6\u91cf\u7a7a\u95f4\u7684magnitude\u6982\u5ff5\u5b9a\u4e49magnitude distance\uff0c\u5f15\u5165\u53ef\u8c03\u7f29\u653e\u53c2\u6570t\u63a7\u5236\u8ddd\u79bb\u5bf9\u5168\u5c40\u7ed3\u6784\u548c\u7ec6\u8282\u7684\u654f\u611f\u6027\uff0c\u8bc1\u660e\u5176\u7406\u8bba\u6027\u8d28\u5305\u62ec\u6781\u9650\u884c\u4e3a\u548c\u5ea6\u91cf\u6027\u8d28\u6761\u4ef6\uff0c\u5c06\u5176\u5e94\u7528\u4e8epush-forward\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u8bc1\u660e\u4e86magnitude distance\u7684\u7406\u8bba\u6027\u8d28\uff0c\u5305\u62ec\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u7684\u6781\u9650\u884c\u4e3a\uff0c\u4ee5\u53ca\u6ee1\u8db3\u5173\u952e\u5ea6\u91cf\u6027\u8d28\u7684\u6761\u4ef6\uff1b\u5b9e\u9a8c\u8868\u660e\u5728\u9002\u5f53\u8c03\u6574\u5c3a\u5ea6\u65f6\uff0c\u8be5\u8ddd\u79bb\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u533a\u5206\u6027\uff0c\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u76ee\u6807\u65f6\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u4fe1\u53f7\uff0c\u4e0e\u73b0\u6709\u57fa\u4e8e\u8ddd\u79bb\u7684\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "magnitude distance\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u901a\u8fc7\u53ef\u8c03\u53c2\u6570t\u9002\u5e94\u4e0d\u540c\u5c3a\u5ea6\u7ed3\u6784\uff0c\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u533a\u5206\u6027\uff0c\u53ef\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\u7684\u6709\u6548\u8bad\u7ec3\u76ee\u6807\uff0c\u4e3a\u6570\u636e\u96c6\u8ddd\u79bb\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.08862", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08862", "abs": "https://arxiv.org/abs/2602.08862", "authors": ["Lunjia Hu", "Jon Schneider", "Yifan Wu"], "title": "Near-optimal Swap Regret Minimization for Convex Losses", "comment": null, "summary": "We give a randomized online algorithm that guarantees near-optimal $\\widetilde O(\\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\\widetilde O(\\sqrt T)$ calibration error guarantee.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u5728\u7ebf\u7b97\u6cd5\uff0c\u5728\u5355\u4f4d\u533a\u95f4\u4e0a\u9488\u5bf9\u81ea\u9002\u5e94\u9009\u62e9\u7684Lipschitz\u51f8\u635f\u5931\u5e8f\u5217\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684$\\widetilde O(\\sqrt T)$\u671f\u671b\u4ea4\u6362\u9057\u61be\uff0c\u6539\u8fdb\u4e86\u4e4b\u524d$\\widetilde O(T^{2/3})$\u7684\u6700\u4f73\u754c\u9650\uff0c\u5e76\u56de\u7b54\u4e86Fishelson\u7b49\u4eba\u7684\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u4e2d\u7684\u4ea4\u6362\u9057\u61be\uff08swap regret\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u6307\u6807\uff0c\u8861\u91cf\u7b97\u6cd5\u76f8\u5bf9\u4e8e\u6700\u4f73\u56fa\u5b9a\u7b56\u7565\u8f6c\u6362\u7684\u540e\u6094\u7a0b\u5ea6\u3002\u5148\u524d\u7684\u6700\u4f73\u754c\u9650\u662f$\\widetilde O(T^{2/3})$\uff0c\u800c\u7406\u8bba\u4e0a\u7684\u4e0b\u754c\u662f$\\Omega(\\sqrt T)$\uff0c\u56e0\u6b64\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002\u6b64\u5916\uff0c\u8be5\u95ee\u9898\u4e0e\u53ef\u5f15\u51fa\u5c5e\u6027\u7684\u6821\u51c6\u8bef\u5dee\u6700\u5c0f\u5316\u5bc6\u5207\u76f8\u5173\uff0c\u7279\u522b\u662f\u4e2d\u4f4d\u6570\u6821\u51c6\u7b49\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\"\u591a\u5c3a\u5ea6\u5206\u7bb1\"\uff08multi-scale binning\uff09\u7684\u5173\u952e\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u5c06\u5355\u4f4d\u533a\u95f4\u79bb\u6563\u5316\u4e3a\u591a\u4e2a\u7c92\u5ea6\u5c3a\u5ea6\u7684\u7bb1\uff0c\u5e76\u540c\u65f6\u4f7f\u7528\u6240\u6709\u5c3a\u5ea6\u8fdb\u884c\u968f\u673a\u9884\u6d4b\u3002\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8fd0\u884c\uff0c\u901a\u8fc7\u8fd9\u79cd\u591a\u5c3a\u5ea6\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9057\u61be\u754c\u9650\u3002", "result": "1. \u5b9e\u73b0\u4e86$\\widetilde O(\\sqrt T)$\u7684\u671f\u671b\u4ea4\u6362\u9057\u61be\uff0c\u6539\u8fdb\u4e86\u4e4b\u524d\u7684$\\widetilde O(T^{2/3})$\u754c\u9650\uff0c\u8fbe\u5230\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u7ed3\u679c\uff08\u56e0\u4e3a\u4e0b\u754c\u662f$\\Omega(\\sqrt T)$\uff09\u3002\n2. \u7b97\u6cd5\u662f\u9ad8\u6548\u7684\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a$\\mathsf{poly}(T)$\u3002\n3. \u4f5c\u4e3a\u76f4\u63a5\u63a8\u8bba\uff0c\u4e3a\u4e00\u822c\u53ef\u5f15\u51fa\u5c5e\u6027\u7684\u6821\u51c6\u8bef\u5dee\u6700\u5c0f\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u5728\u7ebf\u7b97\u6cd5\uff0c\u65e0\u9700\u5148\u524d\u5de5\u4f5c\u4e2d\u9700\u8981\u7684\u8bc6\u522b\u51fd\u6570\u7684Lipschitz\u5047\u8bbe\u3002\n4. \u7279\u522b\u5730\uff0c\u4e3a\u4e2d\u4f4d\u6570\u6821\u51c6\u5b9e\u73b0\u4e86\u9996\u4e2a$\\widetilde O(\\sqrt T)$\u6821\u51c6\u8bef\u5dee\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u521b\u65b0\u7684\u591a\u5c3a\u5ea6\u5206\u7bb1\u6280\u672f\uff0c\u5728\u5355\u4f4d\u533a\u95f4\u4e0a\u7684Lipschitz\u51f8\u635f\u5931\u5728\u7ebf\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u4ea4\u6362\u9057\u61be\u754c\u9650\uff0c\u89e3\u51b3\u4e86Fishelson\u7b49\u4eba\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002\u8be5\u7ed3\u679c\u4e0d\u4ec5\u6539\u8fdb\u4e86\u7406\u8bba\u754c\u9650\uff0c\u8fd8\u4e3a\u6821\u51c6\u8bef\u5dee\u6700\u5c0f\u5316\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u7279\u522b\u662f\u6269\u5c55\u5230\u4e86\u4e2d\u4f4d\u6570\u6821\u51c6\u7b49\u5148\u524d\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.08868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08868", "abs": "https://arxiv.org/abs/2602.08868", "authors": ["Junru Zhang", "Lang Feng", "Haoran Shi", "Xu Guo", "Han Yu", "Yabo Dong", "Duanqing Xu"], "title": "AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection", "comment": "Preprint", "summary": "Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.", "AI": {"tldr": "AnomSeer\uff1a\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e13\u5bb6\u601d\u7ef4\u94fe\u548c\u65f6\u5e8f\u57fa\u7840\u7b56\u7565\u4f18\u5316\uff0c\u5b9e\u73b0\u5f02\u5e38\u5206\u7c7b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u70b9\u5f02\u5e38\u548c\u9891\u7387\u5f02\u5e38\u68c0\u6d4b\u4e0a\u4f18\u4e8eGPT-4o\u7b49\u5927\u578b\u5546\u4e1a\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u96be\u4ee5\u8fdb\u884c\u591a\u7ef4\u5ea6\u7684\u8be6\u7ec6\u63a8\u7406\uff0c\u800c\u7406\u89e3\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u9700\u8981\u7cbe\u786e\u7684\u7ed3\u6784\u7ec6\u8282\u5206\u6790\u80fd\u529b\u3002", "method": "1. \u751f\u6210\u4e13\u5bb6\u601d\u7ef4\u94fe\u8ddf\u8e2a\uff0c\u63d0\u4f9b\u57fa\u4e8e\u7ecf\u5178\u5206\u6790\uff08\u7edf\u8ba1\u6d4b\u91cf\u3001\u9891\u7387\u53d8\u6362\uff09\u7684\u53ef\u9a8c\u8bc1\u7ec6\u7c92\u5ea6\u63a8\u7406\uff1b2. \u63d0\u51fa\u65f6\u5e8f\u57fa\u7840\u7b56\u7565\u4f18\u5316\uff08TimerPO\uff09\uff0c\u5305\u542b\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u65f6\u5e8f\u57fa\u7840\u4f18\u52bf\u548c\u6b63\u4ea4\u6295\u5f71\u7ec4\u4ef6\uff0c\u786e\u4fdd\u8f85\u52a9\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u4e0d\u5e72\u6270\u4e3b\u8981\u68c0\u6d4b\u76ee\u6807\u3002", "result": "\u4f7f\u7528Qwen2.5-VL-3B/7B-Instruct\u7684AnomSeer\u5728\u591a\u79cd\u5f02\u5e38\u573a\u666f\u4e0b\uff0c\u5728\u5206\u7c7b\u548c\u5b9a\u4f4d\u51c6\u786e\u7387\u4e0a\u4f18\u4e8eGPT-4o\u7b49\u5927\u578b\u5546\u4e1a\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u70b9\u5f02\u5e38\u548c\u9891\u7387\u9a71\u52a8\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u80fd\u751f\u6210\u652f\u6301\u5176\u7ed3\u8bba\u7684\u5408\u7406\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "AnomSeer\u901a\u8fc7\u5f3a\u5316\u6a21\u578b\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u7cbe\u786e\u7ed3\u6784\u7ec6\u8282\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7ec6\u7c92\u5ea6\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5f02\u5e38\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2602.08877", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08877", "abs": "https://arxiv.org/abs/2602.08877", "authors": ["Oliver Daniels", "Perusha Moodley", "Ben Marlin", "David Lindner"], "title": "Stress-Testing Alignment Audits With Prompt-Level Strategic Deception", "comment": null, "summary": "Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u7ea2\u961f\u6d4b\u8bd5\u7ba1\u9053\uff0c\u9488\u5bf9\u5bf9\u9f50\u5ba1\u8ba1\u65b9\u6cd5\u751f\u6210\u6b3a\u9a97\u7b56\u7565\uff0c\u53d1\u73b0\u73b0\u6709\u767d\u76d2\u548c\u9ed1\u76d2\u65b9\u6cd5\u5728\u9762\u5bf9\u6218\u7565\u6027\u6b3a\u9a97\u65f6\u90fd\u4e0d\u591f\u7a33\u5065\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u5ba1\u8ba1\u65b9\u6cd5\u65e8\u5728\u68c0\u6d4b\u6218\u7565\u6027\u3001\u60c5\u5883\u611f\u77e5\u7684\u9519\u4f4d\u6a21\u578b\u4e2d\u7684\u9690\u85cf\u76ee\u6807\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5c1a\u672a\u7ecf\u8fc7\u7cfb\u7edf\u6027\u538b\u529b\u6d4b\u8bd5\u6765\u8bc4\u4f30\u5176\u5bf9\u6297\u6b3a\u9a97\u7b56\u7565\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u7ea2\u961f\u6d4b\u8bd5\u7ba1\u9053\uff0c\u9488\u5bf9\u7279\u5b9a\u767d\u76d2\u548c\u9ed1\u76d2\u5ba1\u8ba1\u65b9\u6cd5\u751f\u6210\u5b9a\u5236\u5316\u7684\u6b3a\u9a97\u7b56\u7565\uff08\u4ee5\u7cfb\u7edf\u63d0\u793a\u5f62\u5f0f\uff09\uff0c\u6d4b\u8bd5\u52a9\u624b\u9884\u586b\u5145\u3001\u7528\u6237\u89d2\u8272\u91c7\u6837\u3001\u7a00\u758f\u81ea\u7f16\u7801\u5668\u548c\u8bcd\u5d4c\u5165\u76f8\u4f3c\u6027\u7b49\u65b9\u6cd5\u3002", "result": "\u81ea\u52a8\u7ea2\u961f\u7ba1\u9053\u627e\u5230\u4e86\u80fd\u591f\u6b3a\u9a97\u9ed1\u76d2\u548c\u767d\u76d2\u65b9\u6cd5\u7684\u63d0\u793a\uff0c\u4f7f\u5176\u4ea7\u751f\u81ea\u4fe1\u4f46\u9519\u8bef\u7684\u731c\u6d4b\u3002\u8fd9\u662f\u9996\u6b21\u8bb0\u5f55\u5230\u57fa\u4e8e\u6fc0\u6d3b\u7684\u6218\u7565\u6027\u6b3a\u9a97\u8bc1\u636e\u3002", "conclusion": "\u5f53\u524d\u7684\u9ed1\u76d2\u548c\u767d\u76d2\u5ba1\u8ba1\u65b9\u6cd5\u5728\u9762\u5bf9\u8db3\u591f\u5f3a\u5927\u7684\u9519\u4f4d\u6a21\u578b\u65f6\u4e0d\u591f\u7a33\u5065\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u5bf9\u9f50\u5ba1\u8ba1\u6280\u672f\u3002"}}
{"id": "2602.08878", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08878", "abs": "https://arxiv.org/abs/2602.08878", "authors": ["Itai Zilberstein", "Ioannis Anagnostides", "Zachary W. Sollie", "Arman Kilic", "Tuomas Sandholm"], "title": "Learning Potentials for Dynamic Matching and Application to Heart Transplantation", "comment": null, "summary": "Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6f5c\u529b\u7684\u975e\u8fd1\u89c6\u653f\u7b56\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5fc3\u810f\u79fb\u690d\u5668\u5b98\u5206\u914d\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u6f5c\u529b\u51fd\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5fc3\u810f\u79fb\u690d\u60a3\u8005\u9762\u4e34\u5668\u5b98\u77ed\u7f3a\u5bfc\u81f4\u7684\u81f4\u547d\u7b49\u5f85\u65f6\u95f4\uff0c\u73b0\u6709\u5206\u914d\u653f\u7b56\u672a\u80fd\u5145\u5206\u8003\u8651\u5668\u5b98\u52a8\u6001\u5230\u8fbe\u548c\u7b49\u5f85\u540d\u5355\u7ec4\u6210\uff0c\u7f8e\u56fd\u6b63\u4ece\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u8f6c\u5411\u6570\u636e\u9a71\u52a8\u6a21\u578b", "method": "\u63d0\u51fa\u57fa\u4e8e\u6f5c\u529b\u7684\u975e\u8fd1\u89c6\u653f\u7b56\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u9ad8\u7ef4\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u6f5c\u529b\u51fd\u6570\uff0c\u6a21\u4eff\u5177\u6709\u5b8c\u7f8e\u9884\u89c1\u6027\u7684\u5168\u77e5\u7b97\u6cd5", "result": "\u4f7f\u7528\u771f\u5b9e\u5386\u53f2\u6570\u636e\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u7fa4\u4f53\u6c34\u5e73\u7ed3\u679c\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u7f8e\u56fd\u73b0\u72b6\u653f\u7b56\u548c\u63d0\u51fa\u7684\u8fde\u7eed\u5206\u5e03\u6846\u67b6", "conclusion": "\u5728\u7f8e\u56fd\u5fc3\u810f\u79fb\u690d\u5206\u914d\u7cfb\u7edf\u5ba1\u67e5\u7684\u5173\u952e\u65f6\u523b\uff0c\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u4e14\u7406\u8bba\u57fa\u7840\u7684\u8def\u5f84\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5668\u5b98\u5206\u914d"}}
{"id": "2602.08885", "categories": ["cs.LG", "cs.AI", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.08885", "abs": "https://arxiv.org/abs/2602.08885", "authors": ["Paul Saegert", "Ullrich K\u00f6the"], "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression", "comment": "main text: 8 pages, 7 figures appendix: 12 pages, 11 figures code available at https://github.com/psaegert/simplipy and https://github.com/psaegert/flash-ansr", "summary": "Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSimpliPy\u7b80\u5316\u5f15\u64ce\uff0c\u76f8\u6bd4SymPy\u5b9e\u73b0100\u500d\u52a0\u901f\uff0c\u5e76\u6784\u5efaFlash-ANSR\u6846\u67b6\uff0c\u5728\u7b26\u53f7\u56de\u5f52\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0ePySR\u76f8\u5f53\u7684\u6027\u80fd\u4f46\u80fd\u53d1\u73b0\u66f4\u7b80\u6d01\u7684\u8868\u8fbe\u5f0f\u3002", "motivation": "\u5f53\u524d\u644a\u9500\u7b26\u53f7\u56de\u5f52\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u79d1\u5b66\u95ee\u9898\u65f6\u9762\u4e34\u6548\u7387\u74f6\u9888\uff0c\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u5feb\u901f\u5c06\u7b49\u4ef7\u8868\u8fbe\u5f0f\u7b80\u5316\u4e3a\u7b80\u6d01\u89c4\u8303\u5f62\u5f0f\u7684\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u901a\u7528\u8ba1\u7b97\u673a\u4ee3\u6570\u7cfb\u7edf\u5982SymPy\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u9650\u5236\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002", "method": "\u63d0\u51faSimpliPy\u7b80\u5316\u5f15\u64ce\uff0c\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u7b80\u5316\u65b9\u6cd5\uff1b\u57fa\u4e8e\u6b64\u6784\u5efaFlash-ANSR\u6846\u67b6\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u644a\u9500\u7b26\u53f7\u56de\u5f52\uff0c\u5305\u62ec\u652f\u6301\u66f4\u5927\u8bad\u7ec3\u96c6\u3001\u66f4\u6709\u6548\u5730\u5229\u7528\u8868\u8fbe\u5f0ftoken\u9884\u7b97\uff0c\u4ee5\u53ca\u7cfb\u7edf\u6027\u5730\u53bb\u9664\u8bad\u7ec3\u96c6\u4e2d\u4e0e\u6d4b\u8bd5\u8868\u8fbe\u5f0f\u7b49\u4ef7\u7684\u6c61\u67d3\u6570\u636e\u3002", "result": "SimpliPy\u76f8\u6bd4SymPy\u5b9e\u73b0100\u500d\u52a0\u901f\u4e14\u8d28\u91cf\u76f8\u5f53\uff1bFlash-ANSR\u5728FastSRB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eNeSymReS\u548cE2E\u7b49\u644a\u9500\u57fa\u7ebf\u65b9\u6cd5\uff1b\u4e0e\u76f4\u63a5\u4f18\u5316\u65b9\u6cd5PySR\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u80fd\u5728\u589e\u52a0\u63a8\u7406\u9884\u7b97\u65f6\u6062\u590d\u66f4\u7b80\u6d01\u800c\u975e\u66f4\u590d\u6742\u7684\u8868\u8fbe\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u9ad8\u6548\u7684\u7b80\u5316\u5f15\u64ceSimpliPy\uff0c\u663e\u8457\u63d0\u5347\u4e86\u644a\u9500\u7b26\u53f7\u56de\u5f52\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u79d1\u5b66\u95ee\u9898\uff0c\u5e76\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u53d1\u73b0\u66f4\u7b80\u6d01\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\u3002"}}
{"id": "2602.08894", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08894", "abs": "https://arxiv.org/abs/2602.08894", "authors": ["Iryna Zabarianska", "Sergei Kholkin", "Grigoriy Ksenofontov", "Ivan Butakov", "Alexander Korotin"], "title": "Discrete Bridges for Mutual Information Estimation", "comment": null, "summary": "Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u6269\u6563\u6865\u6a21\u578b\u7684\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668DBMI\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u79bb\u6563\u6570\u636e\u7684\u4e92\u4fe1\u606f\u4f30\u8ba1\u95ee\u9898", "motivation": "\u4f20\u7edf\u4e92\u4fe1\u606f\u4f30\u8ba1\u5668\u5728\u5904\u7406\u79bb\u6563\u6570\u636e\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u800c\u6269\u6563\u6865\u6a21\u578b\u5728\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u53d1\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027", "method": "\u5c06\u4e92\u4fe1\u606f\u4f30\u8ba1\u91cd\u65b0\u6784\u5efa\u4e3a\u9886\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u5229\u7528\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u7684\u6865\u5339\u914d\u6a21\u578b\u6784\u5efaDBMI\u4f30\u8ba1\u5668", "result": "DBMI\u4f30\u8ba1\u5668\u5728\u4f4e\u7ef4\u548c\u57fa\u4e8e\u56fe\u50cf\u7684\u4e24\u79cd\u4e92\u4fe1\u606f\u4f30\u8ba1\u573a\u666f\u4e2d\u90fd\u5c55\u793a\u4e86\u826f\u597d\u7684\u6027\u80fd", "conclusion": "\u79bb\u6563\u6865\u6a21\u578b\u4e0d\u4ec5\u53ef\u7528\u4e8e\u751f\u6210\u5efa\u6a21\uff0c\u8fd8\u80fd\u6709\u6548\u89e3\u51b3\u79bb\u6563\u6570\u636e\u7684\u4e92\u4fe1\u606f\u4f30\u8ba1\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u548c\u4fe1\u606f\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177"}}
{"id": "2602.08901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08901", "abs": "https://arxiv.org/abs/2602.08901", "authors": ["Xuanqi Zhang", "Haoyang Shang", "Xiaoxiao Li"], "title": "GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs", "comment": "34 pages, 12 figures", "summary": "Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \\times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.", "AI": {"tldr": "Gated Subspace Steering (GSS) \u662f\u4e00\u79cd\u9009\u62e9\u6027\u8bb0\u5fc6\u7f13\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u6d4b-\u5f15\u5bfc\u673a\u5236\u4ec5\u5728\u68c0\u6d4b\u5230\u8bb0\u5fc6\u76f8\u5173\u6fc0\u6d3b\u65f6\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6b63\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u51cf\u5c11\u8bb0\u5fc6\u5316\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f1a\u8bb0\u5fc6\u5e76\u9010\u5b57\u590d\u5236\u8bad\u7ec3\u5e8f\u5217\uff0c\u8fd9\u65e2\u635f\u5bb3\u6cdb\u5316\u80fd\u529b\u53c8\u5a01\u80c1\u9690\u79c1\u5b89\u5168\u3002\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u901a\u5e38\u5747\u5300\u65bd\u52a0\u5e72\u9884\uff0c\u4f1a\u964d\u4f4e\u5927\u591a\u6570\u6b63\u5e38\u6cdb\u5316token\u7684\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u8bb0\u5fc6\u5316\u662f\u7a00\u758f\u3001\u95f4\u6b47\u4e14token\u6761\u4ef6\u5316\u7684\uff0c\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5e72\u9884\u800c\u975e\u9759\u6001\u53c2\u6570\u4fee\u6539\u3002", "method": "\u63d0\u51faGated Subspace Steering (GSS)\u65b9\u6cd5\uff0c\u5c06\u5e72\u9884\u5206\u89e3\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a1) \u63a2\u6d4b\uff08\u68c0\u6d4b\u8bb0\u5fc6\u76f8\u5173\u6fc0\u6d3b\uff09\uff0c2) \u5f15\u5bfc\uff08\u4ec5\u5728\u63a2\u6d4b\u8d85\u8fc7\u9608\u503c\u65f6\u5e94\u7528\u9488\u5bf9\u6027\u4fee\u6b63\uff09\u3002\u6700\u4f18\u7684\u63a2\u6d4b-\u5f15\u5bfc\u5bf9\u57fa\u4e8e\u6700\u4f18\u5b50\u7a7a\u95f4\u5f15\u5bfc\u7684\u4f18\u5316\u6846\u67b6\u4ea7\u751f\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGSS\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u8bb0\u5fc6\u51cf\u5c11\u6548\u679c\uff0c\u540c\u65f6\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u66ff\u4ee3\u65b9\u6cd5\u9700\u8981\u5c11100-1000\u500d\u7684\u8ba1\u7b97\u91cf\u3002\u8be5\u65b9\u6cd5\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u795e\u7ecf\u8868\u793a\u4e2d\u8bb0\u5fc6\u5316\u51e0\u4f55\u7ed3\u6784\u7684\u65b0\u7406\u8bba\u89c1\u89e3\u3002", "conclusion": "GSS\u901a\u8fc7\u9009\u62e9\u6027\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5e72\u9884\u6709\u6548\u7f13\u89e3LLM\u7684\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u7406\u89e3\u8bb0\u5fc6\u5316\u5728\u795e\u7ecf\u8868\u793a\u4e2d\u7684\u51e0\u4f55\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2602.08907", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08907", "abs": "https://arxiv.org/abs/2602.08907", "authors": ["Marko Medvedev", "Idan Attias", "Elisabetta Cornacchia", "Theodor Misiakiewicz", "Gal Vardi", "Nathan Srebro"], "title": "Positive Distribution Shift as a Framework for Understanding Tractable Learning", "comment": null, "summary": "We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u6b63\u5206\u5e03\u504f\u79fb\"\u6982\u5ff5\uff0c\u8ba4\u4e3a\u901a\u8fc7\u7cbe\u5fc3\u9009\u62e9\u8bad\u7ec3\u5206\u5e03D'(x)\u800c\u975e\u76ee\u6807\u5206\u5e03D(x)\uff0c\u53ef\u4ee5\u4f7f\u5b66\u4e60\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u8fd9\u4e3b\u8981\u662f\u8ba1\u7b97\u4e0a\u7684\u4f18\u52bf\u800c\u975e\u7edf\u8ba1\u4e0a\u7684\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u5206\u5e03\u504f\u79fb\uff08\u534f\u53d8\u91cf\u504f\u79fb\uff09\u5bf9\u5b66\u4e60\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u8bad\u7ec3\u5206\u5e03\uff0c\u5206\u5e03\u504f\u79fb\u53ef\u4ee5\u6210\u4e3a\u6b63\u9762\u56e0\u7d20\uff0c\u4f7f\u5b66\u4e60\u66f4\u5bb9\u6613\u3002\u8fd9\u79cd\u89c6\u89d2\u5728\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u521b\u65b0\u5f80\u5f80\u5728\u4e8e\u5bfb\u627e\u597d\u7684\u8bad\u7ec3\u5206\u5e03\u800c\u975e\u6539\u53d8\u8bad\u7ec3\u7b97\u6cd5\u3002", "method": "\u5f62\u5f0f\u5316\u4e86\u6b63\u5206\u5e03\u504f\u79fb\u7684\u4e0d\u540c\u53d8\u4f53\uff0c\u5c55\u793a\u4e86\u67d0\u4e9b\u56f0\u96be\u7c7b\u522b\u5728\u6b63\u5206\u5e03\u504f\u79fb\u4e0b\u53d8\u5f97\u5bb9\u6613\u5b66\u4e60\uff0c\u5e76\u4e0e\u6210\u5458\u67e5\u8be2\u5b66\u4e60\u5efa\u7acb\u4e86\u8054\u7cfb\u3002", "result": "\u8bc1\u660e\u4e86\u6b63\u5206\u5e03\u504f\u79fb\u53ef\u4ee5\u4f7f\u8ba1\u7b97\u56f0\u96be\u7684\u95ee\u9898\u53d8\u5f97\u53ef\u5904\u7406\uff0c\u5373\u4f7f\u4f7f\u7528\u6807\u51c6\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u6b63\u5206\u5e03\u504f\u79fb\u662f\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u89c6\u89d2\uff0c\u5b83\u5f3a\u8c03\u4e86\u8bad\u7ec3\u5206\u5e03\u9009\u62e9\u7684\u91cd\u8981\u6027\uff0c\u80fd\u591f\u63d0\u4f9b\u8ba1\u7b97\u4e0a\u7684\u4f18\u52bf\uff0c\u4f7f\u539f\u672c\u56f0\u96be\u7684\u5b66\u4e60\u95ee\u9898\u53d8\u5f97\u53ef\u884c\u3002"}}
{"id": "2602.08913", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.08913", "abs": "https://arxiv.org/abs/2602.08913", "authors": ["Kate\u0159ina Henclov\u00e1", "V\u00e1clav \u0160m\u00eddl"], "title": "GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems", "comment": null, "summary": "Selecting interpretable feature sets in underdetermined ($n \\ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.\n  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.\n  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.\n  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.", "AI": {"tldr": "GEMSS\u662f\u4e00\u4e2a\u53d8\u5206\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u5728n\u226ap\u548c\u9ad8\u76f8\u5173\u6027\u7684\u6b20\u5b9a\u95ee\u9898\u4e2d\u53d1\u73b0\u591a\u4e2a\u4e0d\u540c\u7684\u7a00\u758f\u7279\u5f81\u7ec4\u5408\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5148\u9a8c\u548c\u591a\u6837\u6027\u60e9\u7f5a\u540c\u65f6\u4f18\u5316\u6574\u4e2a\u89e3\u96c6\u5408\u3002", "motivation": "\u5728\u6b20\u5b9a(n\u226ap)\u548c\u9ad8\u76f8\u5173\u6027\u573a\u666f\u4e2d\uff0c\u591a\u4e2a\u4e0d\u540c\u7684\u7a00\u758f\u7279\u5f81\u5b50\u96c6\u53ef\u80fd\u540c\u6837\u597d\u5730\u89e3\u91ca\u54cd\u5e94\u53d8\u91cf\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u53ea\u63d0\u4f9b\u4e00\u4e2a\u89e3\uff0c\u63a9\u76d6\u4e86\u5b8c\u6574\u7684\u53ef\u80fd\u89e3\u91ca\u8c31\uff0c\u800c\u8bc6\u522b\u8fd9\u4e9b\u66ff\u4ee3\u89e3\u5bf9\u4e8e\u6df1\u5165\u7406\u89e3\u5e95\u5c42\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "GEMSS\u91c7\u7528\u53d8\u5206\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316spike-and-slab\u5148\u9a8c\u5b9e\u73b0\u7a00\u758f\u6027\uff0c\u7528\u9ad8\u65af\u6df7\u5408\u8fd1\u4f3c\u96be\u4ee5\u5904\u7406\u7684\u591a\u5cf0\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u901a\u8fc7Jaccard\u8ddd\u79bb\u60e9\u7f5a\u63a7\u5236\u89e3\u591a\u6837\u6027\u3002\u4e0e\u987a\u5e8f\u8d2a\u5a6a\u65b9\u6cd5\u4e0d\u540c\uff0c\u5b83\u901a\u8fc7\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5728\u5355\u4e2a\u76ee\u6807\u51fd\u6570\u4e2d\u540c\u65f6\u4f18\u5316\u6574\u4e2a\u89e3\u96c6\u5408\u3002", "result": "\u5728\u5305\u542b128\u4e2a\u5408\u6210\u5b9e\u9a8c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGEMSS\u80fd\u6709\u6548\u6269\u5c55\u5230\u9ad8\u7ef4\u8bbe\u7f6e(p=5000)\uff0c\u6837\u672c\u91cf\u5c0f\u81f3n=50\uff0c\u80fd\u65e0\u7f1d\u63a8\u5e7f\u5230\u8fde\u7eed\u76ee\u6807\uff0c\u539f\u751f\u5904\u7406\u7f3a\u5931\u6570\u636e\uff0c\u5e76\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9ad8\u65af\u566a\u58f0\u8868\u73b0\u51fa\u663e\u8457\u9c81\u68d2\u6027\u3002", "conclusion": "GEMSS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6b20\u5b9a\u548c\u9ad8\u76f8\u5173\u573a\u666f\u4e2d\u540c\u65f6\u53d1\u73b0\u591a\u4e2a\u4e0d\u540c\u7684\u7a00\u758f\u7279\u5f81\u7ec4\u5408\uff0c\u5df2\u5b9e\u73b0\u4e3aPython\u5305'gemss'\uff0c\u5305\u542b\u9002\u5408\u975e\u7f16\u7801\u8005\u4f7f\u7528\u7684\u5e94\u7528\u7a0b\u5e8f\u3002"}}
{"id": "2602.08920", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08920", "abs": "https://arxiv.org/abs/2602.08920", "authors": ["Manh Cuong Dao", "Quang Hung Pham", "Phi Le Nguyen", "Thao Nguyen Truong", "Bryan Kian Hsiang Low", "Trong Nghia Hoang"], "title": "Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration", "comment": null, "summary": "Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u8fc7\u7a0b\u7684Transformer\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u65b9\u6cd5\uff0c\u5c06\u7279\u5f81\u53d8\u6362\u5757\u5efa\u6a21\u4e3a\u6982\u7387\u6620\u5c04\uff0c\u5b9e\u73b0\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u7684\u4f20\u64ad", "motivation": "\u9884\u8bad\u7ec3Transformer\u5728\u98ce\u9669\u654f\u611f\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u901a\u8fc7\u7279\u5f81\u53d8\u6362\u5806\u6808\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u7684\u673a\u5236", "method": "\u63d0\u51fa\u6269\u6563\u542f\u53d1\u7684Transformer\u91cd\u6784\u65b9\u6cd5\uff0c\u5c06\u6bcf\u4e2a\u7279\u5f81\u53d8\u6362\u5757\u5efa\u6a21\u4e3a\u6982\u7387\u6620\u5c04\uff0c\u7ec4\u5408\u8fd9\u4e9b\u6620\u5c04\u5f62\u6210\u7c7b\u4f3c\u6269\u6563\u8fc7\u7a0b\u7684\u6982\u7387\u8def\u5f84\uff0c\u7136\u540e\u91cd\u65b0\u7f16\u8bd1\u5230\u5177\u6709\u7edf\u4e00\u8f6c\u79fb\u6a21\u578b\u7684\u6269\u6563\u8fc7\u7a0b\u4e2d", "result": "\u5728\u591a\u79cd\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5Transformer\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6821\u51c6\u548c\u9884\u6d4b\u51c6\u786e\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u539f\u59cb\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u67b6\u6784\u4e2d\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u7684\u539f\u5219\u6027\u4f20\u64ad"}}
{"id": "2602.08934", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.08934", "abs": "https://arxiv.org/abs/2602.08934", "authors": ["Suraj Ranganath", "Atharv Ramesh"], "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors", "comment": "Expanded version of a workshop submission. Code available", "summary": "AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.", "AI": {"tldr": "StealthRL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u6297\u6027\u6539\u8ff0\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5AI\u6587\u672c\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u5728\u4fdd\u6301\u8bed\u4e49\u7684\u540c\u65f6\u5b9e\u73b0\u8fd1\u4e4e\u96f6\u68c0\u6d4b\u7387\uff0c\u63ed\u793a\u5f53\u524d\u68c0\u6d4b\u5668\u7684\u663e\u8457\u8106\u5f31\u6027\u3002", "motivation": "AI\u6587\u672c\u68c0\u6d4b\u5668\u9762\u4e34\u5bf9\u6297\u6027\u6539\u8ff0\u653b\u51fb\u7684\u5173\u952e\u9c81\u68d2\u6027\u6311\u6218\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u5728\u4fdd\u6301\u8bed\u4e49\u7684\u540c\u65f6\u9003\u907f\u68c0\u6d4b\u3002\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u73b0\u5b9e\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u9700\u8981\u7cfb\u7edf\u6027\u7684\u538b\u529b\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faStealthRL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528Group Relative Policy Optimization (GRPO)\u548cLoRA\u9002\u914d\u5668\u5728Qwen3-4B\u6a21\u578b\u4e0a\u8bad\u7ec3\u6539\u8ff0\u7b56\u7565\uff0c\u9488\u5bf9\u591a\u68c0\u6d4b\u5668\u96c6\u6210\u4f18\u5316\u590d\u5408\u5956\u52b1\u51fd\u6570\uff0c\u5e73\u8861\u68c0\u6d4b\u9003\u907f\u548c\u8bed\u4e49\u4fdd\u6301\u3002", "result": "\u57281%\u5047\u9633\u6027\u7387\u7684\u5b89\u5168\u76f8\u5173\u64cd\u4f5c\u70b9\u4e0a\uff0cStealthRL\u5b9e\u73b0\u8fd1\u4e4e\u96f6\u68c0\u6d4b\uff080.001\u5e73\u5747TPR@1%FPR\uff09\uff0c\u5c06\u5e73\u5747AUROC\u4ece0.74\u964d\u81f30.27\uff0c\u8fbe\u523099.9%\u653b\u51fb\u6210\u529f\u7387\u3002\u653b\u51fb\u8fd8\u80fd\u8fc1\u79fb\u5230\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u68c0\u6d4b\u5668\u5bb6\u65cf\uff0c\u63ed\u793a\u5171\u4eab\u67b6\u6784\u6f0f\u6d1e\u3002", "conclusion": "\u5f53\u524dAI\u6587\u672c\u68c0\u6d4b\u5668\u5b58\u5728\u663e\u8457\u7684\u9c81\u68d2\u6027\u7f3a\u9677\uff0cStealthRL\u4e3a\u5bf9\u6297\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u534f\u8bae\uff0c\u4ee3\u7801\u548c\u8bc4\u4f30\u6d41\u7a0b\u5df2\u516c\u5f00\u3002"}}
{"id": "2602.08976", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.08976", "abs": "https://arxiv.org/abs/2602.08976", "authors": ["Jiaqi Wen", "Jianyi Yang"], "title": "Distributionally Robust Optimization via Generative Ambiguity Modeling", "comment": null, "summary": "This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u5206\u5e03\u9c81\u68d2\u4f18\u5316(GAS-DRO)\uff0c\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6784\u5efa\u6a21\u7cca\u96c6\uff0c\u63d0\u5347\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u5206\u5e03\u5916\u6cdb\u5316\u6027\u80fd", "motivation": "\u4f20\u7edf\u5206\u5e03\u9c81\u68d2\u4f18\u5316(DRO)\u7684\u6a21\u7cca\u96c6\u9700\u8981\u5728\u4fdd\u6301\u4e0e\u540d\u4e49\u5206\u5e03\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u8db3\u591f\u591a\u6837\u5316\u4ee5\u8986\u76d6\u5404\u79cd\u6f5c\u5728\u573a\u666f\uff0c\u4e14\u9700\u8981\u4fdd\u8bc1\u6c42\u89e3\u7684\u6613\u5904\u7406\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u540d\u4e49\u652f\u6491\u7a7a\u95f4\u4e4b\u5916\u7684\u5bf9\u6297\u5206\u5e03\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u751f\u6210\u6a21\u578b\u6a21\u7cca\u96c6\uff0c\u6355\u6349\u540d\u4e49\u652f\u6491\u7a7a\u95f4\u4e4b\u5916\u7684\u5404\u79cd\u5bf9\u6297\u5206\u5e03\uff1b\u57fa\u4e8e\u6b64\u6784\u5efaGAS-DRO\u7b97\u6cd5\uff0c\u901a\u8fc7\u6c42\u89e3\u53c2\u6570\u5316\u751f\u6210\u6a21\u578b\u7a7a\u95f4\u7684\u5185\u5c42\u6700\u5927\u5316\u95ee\u9898\u5b9e\u73b0\u6613\u5904\u7406\u7684DRO\u6c42\u89e3\uff1b\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5b9e\u73b0GAS-DRO\u3002", "result": "\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86GAS-DRO\u7684\u5e73\u7a33\u6536\u655b\u6027\u80fd\uff1b\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u5728\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0cGAS-DRO\u5728\u5206\u5e03\u5916\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GAS-DRO\u901a\u8fc7\u751f\u6210\u6a21\u578b\u6784\u5efa\u6a21\u7cca\u96c6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.08983", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.08983", "abs": "https://arxiv.org/abs/2602.08983", "authors": ["Yubin Kim", "Viresh Pati", "Jevon Twitty", "Vinh Pham", "Shihao Yang", "Jiecheng Lu"], "title": "StretchTime: Adaptive Time Series Forecasting via Symplectic Attention", "comment": null, "summary": "Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit \"time-warped\" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\\mathrm{SO}(2)$ to the symplectic group $\\mathrm{Sp}(2,\\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSymplectic Positional Embeddings (SyPE)\u6765\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u65e0\u6cd5\u5904\u7406\"\u65f6\u95f4\u626d\u66f2\"\u52a8\u6001\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u65cb\u8f6c\u7fa4SO(2)\u6269\u5c55\u5230\u8f9b\u7fa4Sp(2,R)\uff0c\u5b9e\u73b0\u4e86\u5bf9\u975e\u4eff\u5c04\u65f6\u95f4\u626d\u66f2\u7684\u5efa\u6a21\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\uff08\u5982\u91d1\u878d\u5468\u671f\u3001\u751f\u7269\u8282\u5f8b\uff09\u7ecf\u5e38\u8868\u73b0\u51fa\"\u65f6\u95f4\u626d\u66f2\"\u52a8\u6001\uff0c\u5373\u6709\u6548\u65f6\u95f4\u6d41\u4e0e\u91c7\u6837\u7d22\u5f15\u89e3\u8026\u3002\u4f20\u7edf\u7684Transformer\u4f4d\u7f6e\u7f16\u7801\uff08\u5982RoPE\uff09\u57fa\u4e8e\u5747\u5300\u3001\u7d22\u5f15\u7684\u65f6\u95f4\u8fdb\u5c55\u5047\u8bbe\uff0c\u65e0\u6cd5\u8868\u793a\u975e\u4eff\u5c04\u65f6\u95f4\u626d\u66f2\u3002", "method": "\u63d0\u51faSymplectic Positional Embeddings (SyPE)\uff0c\u4ece\u54c8\u5bc6\u987f\u529b\u5b66\u63a8\u5bfc\u7684\u53ef\u5b66\u4e60\u7f16\u7801\u6846\u67b6\u3002SyPE\u901a\u8fc7\u5c06\u65cb\u8f6c\u7fa4SO(2)\u6269\u5c55\u5230\u8f9b\u7fa4Sp(2,R)\uff0c\u4e25\u683c\u63a8\u5e7f\u4e86RoPE\u3002\u5f15\u5165\u65b0\u9896\u7684\u8f93\u5165\u4f9d\u8d56\u81ea\u9002\u5e94\u626d\u66f2\u6a21\u5757\uff0c\u5141\u8bb8\u6ce8\u610f\u529b\u673a\u5236\u7aef\u5230\u7aef\u5730\u81ea\u9002\u5e94\u6269\u5f20\u6216\u6536\u7f29\u65f6\u95f4\u5750\u6807\u3002", "result": "\u5728StretchTime\u67b6\u6784\u4e2d\u5b9e\u73b0\u8be5\u673a\u5236\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u8868\u73b0\u51fa\u975e\u5e73\u7a33\u65f6\u95f4\u52a8\u6001\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SyPE\u6846\u67b6\u80fd\u591f\u6355\u83b7\u5c40\u90e8\u53d8\u5316\u7684\u5468\u671f\u6027\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u626d\u66f2\u51fd\u6570\uff0c\u4e3a\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u65f6\u95f4\u626d\u66f2\u52a8\u6001\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.09001", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09001", "abs": "https://arxiv.org/abs/2602.09001", "authors": ["Amirhossein Vahidi", "Hesam Asadollahzadeh", "Navid Akhavan Attar", "Marie Moullet", "Kevin Ly", "Xingyi Yang", "Mohammad Lotfollahi"], "title": "DirMoE: Dirichlet-routed Mixture of Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.", "AI": {"tldr": "\u63d0\u51faDirichlet-Routed MoE (DirMoE)\uff0c\u4e00\u79cd\u57fa\u4e8eDirichlet\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u8def\u7531\u673a\u5236\uff0c\u5c06\u4e13\u5bb6\u9009\u62e9\u548c\u8d21\u732e\u5206\u914d\u89e3\u8026\uff0c\u901a\u8fc7\u53d8\u5206ELBO\u8bad\u7ec3\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u7a00\u758f\u6027\u63a7\u5236\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002", "motivation": "\u73b0\u6709MoE\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e0d\u53ef\u5fae\u5206\u7684Top-k+Softmax\u8def\u7531\u673a\u5236\uff0c\u9650\u5236\u4e86\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002\u6807\u51c6Top-k+Softmax\u5c06\u4e24\u4e2a\u4e0d\u540c\u7684\u51b3\u7b56\uff08\u6fc0\u6d3b\u54ea\u4e9b\u4e13\u5bb6\u3001\u5982\u4f55\u5728\u4e13\u5bb6\u95f4\u5206\u914d\u8d21\u732e\uff09\u6df7\u4e3a\u4e00\u8c08\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u53ef\u5fae\u5206\u7684\u8def\u7531\u65b9\u6848\u3002", "method": "\u63d0\u51faDirMoE\uff0c\u57fa\u4e8eDirichlet\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff1a1) \u4e13\u5bb6\u9009\u62e9\u901a\u8fc7Bernoulli\u7ec4\u4ef6\u5efa\u6a21\uff1b2) \u4e13\u5bb6\u8d21\u732e\u5206\u914d\u901a\u8fc7Dirichlet\u7ec4\u4ef6\u5904\u7406\uff1b3) \u4f7f\u7528Gumbel-Sigmoid\u677e\u5f1b\u5b9e\u73b0\u4e13\u5bb6\u9009\u62e9\u53ef\u5fae\uff1b4) \u4f7f\u7528\u9690\u5f0f\u91cd\u53c2\u6570\u5316\u5904\u7406Dirichlet\u5206\u5e03\uff1b5) \u53d8\u5206ELBO\u8bad\u7ec3\u76ee\u6807\u5305\u542b\u76f4\u63a5\u7a00\u758f\u6027\u60e9\u7f5a\uff1b6) \u8d85\u53c2\u6570\u8c03\u5ea6\u5f15\u5bfc\u6a21\u578b\u4ece\u63a2\u7d22\u6027\u8def\u7531\u5230\u786e\u5b9a\u6027\u8def\u7531\u3002", "result": "DirMoE\u8def\u7531\u673a\u5236\u5728\u5339\u914d\u6216\u8d85\u8d8a\u5176\u4ed6\u65b9\u6cd5\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u4e13\u5bb6\u4e13\u4e1a\u5316\u7a0b\u5ea6\u3002\u6574\u4e2a\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u4fdd\u6301\u5b8c\u5168\u53ef\u5fae\u5206\uff0c\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u671f\u671b\u7684\u6d3b\u8dc3\u4e13\u5bb6\u6570\u91cf\u3002", "conclusion": "DirMoE\u901a\u8fc7\u5c06\u8def\u7531\u95ee\u9898\u89e3\u8026\u4e3a\u4e13\u5bb6\u9009\u62e9\u548c\u8d21\u732e\u5206\u914d\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u7684\u8def\u7531\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709Top-k+Softmax\u8def\u7531\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002"}}
{"id": "2602.09006", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.09006", "abs": "https://arxiv.org/abs/2602.09006", "authors": ["Wenbo Gong", "Javier Zazo", "Qijun Luo", "Puqian Wang", "James Hensman", "Chao Ma"], "title": "ARO: A New Lens On Matrix Optimization For Large Models", "comment": null, "summary": "Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \\textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\\sim$1.35$\\times$) and orthogonalization methods (by 1.1$\\sim$1.15$\\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.", "AI": {"tldr": "ARO\u662f\u4e00\u79cd\u65b0\u7684\u77e9\u9635\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u65cb\u8f6c\u4f5c\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u5728\u65cb\u8f6c\u5750\u6807\u7cfb\u4e2d\u6267\u884c\u5f52\u4e00\u5316\u6700\u901f\u4e0b\u964d\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6b63\u4ea4\u5316/\u767d\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u867d\u7136\u57fa\u4e8e\u6b63\u4ea4\u5316/\u767d\u5316\u7684\u77e9\u9635\u4f18\u5316\u5668\u5728\u63d0\u5347LLM\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u8d85\u8d8a\u6b63\u4ea4\u5316\u7684\u65b0\u8303\u5f0f\uff0c\u8fdb\u4e00\u6b65\u63a8\u52a8\u6548\u7387\u8fb9\u754c\u3002", "method": "ARO\u6846\u67b6\u5c06\u68af\u5ea6\u65cb\u8f6c\u4f5c\u4e3a\u9996\u8981\u8bbe\u8ba1\u539f\u5219\uff0c\u5728\u65cb\u8f6c\u5750\u6807\u7cfb\u4e2d\u6267\u884c\u5f52\u4e00\u5316\u6700\u901f\u4e0b\u964d\uff0c\u65cb\u8f6c\u7531\u65b0\u9896\u7684\u8303\u6570\u611f\u77e5\u7b56\u7565\u786e\u5b9a\u3002\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6b63\u4ea4\u5316\u548c\u767d\u5316\u4f18\u5316\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u4e25\u683c\u63a7\u5236\u7684\u57fa\u51c6\u6d4b\u8bd5\u534f\u8bae\u4ee5\u51cf\u5c11\u6df7\u6dc6\u548c\u504f\u5dee\u3002", "result": "\u5728\u4e25\u683c\u63a7\u5236\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0b\uff0cARO\u5728LLM\u9884\u8bad\u7ec3\u4e2d\u59cb\u7ec8\u4f18\u4e8eAdamW\uff081.3-1.35\u500d\uff09\u548c\u6b63\u4ea4\u5316\u65b9\u6cd5\uff081.1-1.15\u500d\uff09\uff0c\u53c2\u6570\u89c4\u6a21\u8fbe80\u4ebf\u6fc0\u6d3b\u53c2\u6570\uff0c\u8fc7\u8bad\u7ec3\u9884\u7b97\u8fbe8\u500d\uff0c\u4e14\u672a\u89c1\u6536\u76ca\u9012\u51cf\u8ff9\u8c61\u3002", "conclusion": "ARO\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u65cb\u8f6c\u5bf9\u79f0\u6027\u7684\u5bf9\u79f0\u611f\u77e5\u4f18\u5316\u5668\uff0c\u4e3a\u5229\u7528\u8de8\u5c42/\u8de8\u6a21\u5757\u8026\u5408\u7684\u8ba1\u7b97\u9ad8\u6548\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u8d85\u8d8a\u4f20\u7edf\u6b63\u4ea4\u5316\u65b9\u6cd5\u7684\u77e9\u9635\u4f18\u5316\u8303\u5f0f\u53d1\u5c55\u3002"}}
{"id": "2602.09008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09008", "abs": "https://arxiv.org/abs/2602.09008", "authors": ["Sijia Peng", "Yun Xiong", "Xi Chen", "Yi Xie", "Guanzhi Li", "Yanwei Yu", "Yangyong Zhu", "Zhiqiang Shen"], "title": "ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification", "comment": "Code at: https://github.com/lunaaa95/ShapeCond", "summary": "Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.", "AI": {"tldr": "ShapeCond\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7shapelet\u5f15\u5bfc\u7684\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528shapelet\u77e5\u8bc6\u8fdb\u884c\u9ad8\u6548\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u5173\u952e\u5c40\u90e8\u6a21\u5f0f\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u5408\u6210\u901f\u5ea6\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5feb\u901f\u589e\u957f\u7ed9\u5b58\u50a8\u548c\u8ba1\u7b97\u5e26\u6765\u538b\u529b\uff0c\u73b0\u6709\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u591a\u4e3a\u56fe\u50cf\u4e2d\u5fc3\u5316\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u7279\u6709\u7684\u65f6\u95f4\u7ed3\u6784\u548c\u5c40\u90e8\u5224\u522b\u6a21\u5f0f\uff08\u5982shapelet\uff09\u3002", "method": "\u63d0\u51faShapeCond\u6846\u67b6\uff0c\u91c7\u7528shapelet\u5f15\u5bfc\u7684\u4f18\u5316\u7b56\u7565\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u6570\u636e\u96c6\u538b\u7f29\uff0cshapelet\u8f85\u52a9\u7684\u5408\u6210\u6210\u672c\u4e0e\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5408\u6210\u6548\u7387\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5CondTSC\uff0cShapeCond\u5728\u5408\u6210\u901f\u5ea6\u4e0a\u5feb29\u500d\uff0c\u5728Sleep\u6570\u636e\u96c6\uff083000\u65f6\u95f4\u6b65\uff09\u4e0a\u6bd4\u6734\u7d20\u4f7f\u7528shapelet\u5feb10000\u500d\uff0c\u5e76\u5728\u4e0b\u6e38\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ShapeCond\u901a\u8fc7\u663e\u5f0f\u4fdd\u7559\u5173\u952e\u5c40\u90e8\u6a21\u5f0f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
