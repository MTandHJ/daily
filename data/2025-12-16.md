<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 151]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.IR](#cs.IR) [Total: 13]
- [cs.CY](#cs.CY) [Total: 38]
- [cs.LG](#cs.LG) [Total: 99]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出基于OpenVLA-OFT框架的可解释对抗鲁棒视觉-语言-动作模型，通过Evidence-3模块检测光度扰动并生成自然语言解释，在对抗条件下提升动作预测准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 智能农业中基于RGB相机和机械臂的系统容易受到色调、光照、噪声等光度扰动的对抗攻击，导致系统故障，需要提高系统的对抗鲁棒性和可解释性

Method: 基于OpenVLA-OFT框架构建可解释对抗鲁棒视觉-语言-动作模型，集成Evidence-3模块来检测光度扰动并生成自然语言解释，说明扰动的原因和影响

Result: 相比基线模型，当前动作L1损失降低21.7%，下一动作L1损失降低18.4%，在对抗条件下显著提升了动作预测准确性和可解释性

Conclusion: 提出的可解释对抗鲁棒模型有效解决了智能农业系统中光度扰动导致的对抗攻击问题，为智能农业系统提供了更可靠和可解释的解决方案

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [2] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: Temporal-Anchor3DLane通过改进损失函数和添加轻量级LSTM时序融合模块，显著提升了单目3D车道线检测的准确性和时序稳定性


<details>
  <summary>Details</summary>
Motivation: 单目3D车道线检测面临深度模糊、遮挡和时序不稳定等挑战。Anchor3DLane等基于锚点的方法在多摄像头环绕视图上表现出色，但基线模型仍存在回归异常值敏感、全局曲线几何监督弱、多损失项平衡困难以及时序连续性利用有限等问题

Method: 提出Temporal-Anchor3DLane框架，包含三个关键改进：(1)多任务损失改进：平衡L1回归、Chamfer点集距离、基于不确定性的损失加权，以及分类和可见性的focal和Dice组件；(2)轻量级时序LSTM融合模块：跨帧聚合每个锚点的特征，替代较重的Transformer风格时序融合；(3)ESCOP风格训练优化：将曲线级监督与时序一致性相结合

Result: 在OpenLane数据集上，Temporal-Anchor3DLane将F1分数提升了+6.2，并产生更平滑的时序轨迹，表明小的架构和损失改进显著增强了3D车道线鲁棒性，无需额外传感器或扩展

Conclusion: 通过改进损失函数和添加轻量级时序融合模块，Temporal-Anchor3DLane显著提升了单目3D车道线检测的性能和时序稳定性，证明了小的架构和损失优化可以在不增加额外传感器或计算复杂度的情况下显著改善模型鲁棒性

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [3] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 研究人员为埃塞俄比亚提格雷地区开发了一个离线作物病害检测系统，专注于仙人掌无花果病害识别，在移动设备上实现了高精度诊断，支持当地语言，增强粮食安全。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚提格雷地区80%以上人口依赖农业，但基础设施中断限制了专业作物病害诊断的获取。需要为冲突后边缘环境开发离线、移动高效的病害检测系统。

Method: 创建了包含3,587张田间图像的本地仙人掌无花果数据集，评估了三种移动高效架构：定制轻量CNN、EfficientNet-Lite1和CNN-Transformer混合模型MobileViT-XS。系统包含独立模块用于马铃薯、苹果和玉米，但本研究专注于仙人掌无花果模型性能评估。

Result: EfficientNet-Lite1达到90.7%测试准确率；轻量CNN达到89.5%准确率，具有最佳部署特性（42ms推理延迟，4.8MB模型大小）；MobileViT-XS达到97.3%平均交叉验证准确率，显示基于多头自注意力的全局推理比局部纹理CNN核更可靠地区分害虫集群和二维真菌病变。

Conclusion: 成功开发了ARM兼容模型，部署在支持提格里尼亚语和阿姆哈拉语的Flutter应用中，可在Cortex-A53类设备上完全离线推理，增强了粮食安全关键诊断的包容性。MobileViT-XS的全局推理能力在区分复杂病害模式方面表现优异。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [4] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: 本文提出了一种用于小麦全语义分割竞赛的系统化自训练框架，结合两阶段混合训练策略和大量数据增强，使用SegFormer模型，通过迭代师生循环提升精度，在开发集和测试集上都取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 参加全球小麦全语义分割竞赛，需要开发一个能够准确分割小麦图像的系统化解决方案，以应对农业图像分析中的挑战。

Method: 采用系统化自训练框架，结合两阶段混合训练策略和大量数据增强，核心模型为基于Mix Transformer（MiT-B4）骨干的SegFormer，通过迭代师生循环逐步提升模型精度并最大化数据利用率。

Result: 在开发阶段和测试阶段的数据集上都取得了有竞争力的性能表现。

Conclusion: 提出的系统化自训练框架结合两阶段混合训练策略和迭代师生循环，能够有效提升小麦语义分割的精度，在竞赛中表现出色。

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [5] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: 该研究系统比较了零样本分割的SAM3模型与微调YOLO11模型在密集苹果实例分割任务上的表现，发现IoU阈值选择对性能评估有显著影响，揭示了SAM3在边界稳定性上的优势与YOLO在检测完整性上的优势。


<details>
  <summary>Details</summary>
Motivation: 深度学习实例分割存在两种范式：通过任务特定微调优化的专用模型和能够零样本分割的通用基础模型。本研究旨在全面比较这两种范式在密集实例分割任务上的表现，为实际应用提供指导。

Method: 使用MinneApple数据集（包含670张果园图像和28,179个苹果实例标注）作为密集基准，评估SAM3在零样本模式下的表现，并与三种不同规模的Ultralytics YOLO11模型（nano、medium、large）进行对比。研究特别关注IoU阈值选择对性能评估的影响。

Result: 在合适的IoU=0.15阈值下，YOLO模型达到68.9%、72.2%和71.9%的F1分数，而SAM3在纯零样本模式下达到59.8%。但YOLO模型在不同IoU范围内的性能下降48-50个百分点，而SAM3仅下降4个百分点，显示出SAM3的边界稳定性是YOLO的12倍。

Conclusion: 研究揭示了SAM3在掩码精度和边界稳定性方面的优势，以及YOLO在检测完整性方面的优势。IoU阈值选择对性能评估有显著影响（可达30%的性能差距）。研究提供了开源代码、评估流程和方法论建议，帮助理解在密集实例分割任务中何时选择专用微调模型或通用基础模型更合适。

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [6] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: 该论文提供了基于无人机采集的开放微观车辆轨迹数据集，用于研究异质化、区域型城市交通环境。


<details>
  <summary>Details</summary>
Motivation: 传统路边视频采集在密集混合交通中存在遮挡、视角有限和车辆运动不规则等问题，需要更好的数据采集方法来研究复杂城市交通环境。

Method: 使用无人机从俯视角度采集交通数据，通过Data from Sky平台提取车辆轨迹信息，在印度首都地区的六个路段收集数据，包含时间戳、位置、速度、加速度和车辆分类等信息。

Result: 创建了包含丰富时空动态信息的开放数据集，分辨率达30帧/秒，覆盖不同交通组成和密度水平，揭示了车道保持偏好、速度分布和横向机动等行为模式。

Conclusion: 这些开放数据集为全球研究社区提供了独特资源，支持仿真建模、安全评估和行为研究，有助于开发更准确反映复杂城市交通环境的模型。

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [7] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 论文提出RIO-VQA任务和RIO-Bench基准，解决大型视觉语言模型在对抗性文本攻击下的选择性文本使用问题，要求模型根据上下文决定何时读取文本、何时忽略文本。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型容易受到排版攻击，现有评估和防御方法主要关注物体识别，鼓励忽略文本以获得鲁棒性，但现实场景需要同时处理物体和文本的联合推理。

Method: 提出RIO-VQA任务形式化选择性文本使用，创建RIO-Bench基准数据集和评估协议，为每个真实图像提供相同场景的反事实（读取/忽略），仅改变文本内容和问题类型。

Result: 使用RIO-Bench评估显示，现有强LVLMs和防御方法无法平衡排版鲁棒性和文本阅读能力，突显改进方法的必要性。

Conclusion: 这项工作揭示了现有评估范围与现实需求之间的根本错位，为可靠的大型视觉语言模型提供了原则性路径，并启发了基于数据驱动的自适应选择性文本使用防御方法。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [8] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: 通过智能手机被动捕捉的自然微笑强度可作为主观幸福感的客观行为指标，与全国幸福感调查数据高度相关


<details>
  <summary>Details</summary>
Motivation: 传统主观幸福感测量依赖自我报告方法，存在回忆偏差和参与者负担重的问题，需要更客观、可扩展的日常幸福感测量方法

Method: 分析233名参与者一周内被动记录的405,448个视频片段，使用深度学习模型量化微笑强度，研究其昼夜和日常模式，并与身体活动、光照暴露和智能手机使用等变量关联分析

Result: 微笑强度日常模式与全国幸福感调查数据高度相关(r=0.92)，昼夜节律与日重建方法结果高度一致(r=0.80)；更高的日均微笑强度与更多身体活动和更大光照暴露显著相关，但与智能手机使用无显著关联

Conclusion: 被动智能手机传感可作为研究情感行为动态的强大、生态有效的方法，为在人群尺度上理解这种行为打开了大门

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [9] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: MPath是一个轻量级多模态框架，通过视觉前缀提示机制将WSI视觉嵌入注入预训练生物医学语言模型，用于从全切片图像自动生成病理诊断报告。


<details>
  <summary>Details</summary>
Motivation: 从全切片图像自动生成病理诊断报告是计算病理学的新方向，但由于组织形态的高度变异性和病理叙述的复杂结构，将高分辨率组织模式转化为临床连贯文本仍然困难。

Method: MPath采用轻量级多模态框架，通过学习的视觉前缀提示机制，将基础模型提取的WSI特征（CONCH + Titan）注入冻结的BioBART语言模型中，使用紧凑投影模块而非端到端的视觉语言预训练。

Result: 在RED 2025 Grand Challenge数据集上开发评估，在Test Phase 2中排名第4，尽管提交机会有限。结果显示了基于提示的多模态条件化作为可扩展且可解释的病理报告生成策略的潜力。

Conclusion: 基于提示的多模态条件化是一种可扩展且可解释的病理报告生成策略，MPath框架展示了将WSI视觉特征有效注入生物医学语言模型的能力。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [10] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: FloraForge是一个LLM辅助框架，让领域专家通过自然语言交互生成参数化3D植物模型，无需编程专业知识，结合了学习方法和程序化建模的优点。


<details>
  <summary>Details</summary>
Motivation: 当前3D植物建模方法存在局限性：基于学习的方法需要大量物种特定训练数据且缺乏可编辑性；程序化建模需要几何建模专业知识和复杂规则理解，对领域科学家不友好。

Method: 利用LLM辅助协同设计，通过迭代自然语言植物精炼（PR）生成Python脚本，创建参数化植物几何体作为分层B样条曲面表示，具有植物学约束、显式控制点和参数变形函数。

Result: 在玉米、大豆和绿豆上演示了该框架，通过手动精炼植物描述符（PD）文件将程序化模型拟合到经验点云数据，生成用于可视化的三角网格和用于定量分析的带参数元数据的三角网格。

Conclusion: FloraForge独特地结合了LLM辅助模板创建、支持表型分析和渲染的数学连续表示，以及通过PD的直接参数控制，为植物科学民主化了复杂几何建模，同时保持数学严谨性。

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [11] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: 提出基于检索增强生成(RAG)的零样本交通标志识别框架，无需特定任务训练即可实现高精度识别


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法面临交通标志类别繁多、创建详尽标注数据集不切实际的挑战，需要开发无需特定训练的可扩展识别系统

Method: 采用RAG范式：1) 使用视觉语言模型(VLM)从输入图像生成文本描述；2) 从参考设计向量数据库中检索最相关的候选标志；3) 使用大语言模型(LLM)对检索结果进行推理，实现细粒度识别

Result: 在俄亥俄州MUTCD的303个监管标志上验证，理想参考图像准确率达95.58%，具有挑战性的真实道路数据准确率达82.45%

Conclusion: 基于RAG的架构为创建无需特定任务训练的可扩展、准确的道路标志识别系统提供了可行方案

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [12] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: 研究人员开发了MONET扩散模型，可从明场图像预测细胞染色图像，解决了传统细胞染色技术劳动密集且无法研究细胞动态的问题。


<details>
  <summary>Details</summary>
Motivation: 传统细胞染色技术存在两个主要问题：一是劳动密集型，需要大量人工操作；二是需要化学固定，无法研究细胞动态变化。研究人员希望开发一种替代方法来解决这些问题。

Method: 训练了一个扩散模型（MONET），使用大规模数据集从明场图像预测细胞染色通道。模型采用一致性架构，能够生成时间序列视频，尽管没有细胞染色视频训练数据。该架构还支持上下文学习，使模型能够部分适应分布外的细胞系和成像协议。

Result: 模型质量随着规模扩大而提高。一致性架构能够生成时间序列视频，并且支持上下文学习，使模型能够部分适应分布外的细胞系和成像协议。

Conclusion: 虚拟细胞染色并非完全替代物理细胞染色，而是作为补充工具，为生物学研究提供新的工作流程可能性。

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [13] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

TL;DR: 本文提出了一种新的HEMC-CPS模型，将上下文Peano扫描与证据隐马尔可夫链结合，用于无监督图像分割，相比传统方法具有更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统隐马尔可夫场（HMFs）图像分割方法计算复杂且耗时，而基于Peano扫描的隐马尔可夫链（HMCs）方法虽然更快，但仍有改进空间。上下文Peano扫描（CPS）和证据隐马尔可夫链（HEMCs）各自显示出改进潜力，因此需要探索将它们结合的可能性。

Method: 提出HEMC-CPS模型，将上下文Peano扫描与证据隐马尔可夫链相结合。采用贝叶斯最大后验概率（MPM）分割，使用随机期望最大化（SEM）方法进行无监督参数估计。

Result: 在合成图像和真实图像上的实验表明，HEMC-CPS模型在图像分割方面表现有效。该模型不仅限于二维图像分割，还可扩展到三维或多传感器多分辨率图像。

Conclusion: HEMC-CPS模型结合了上下文Peano扫描和证据隐马尔可夫链的优势，为复杂图像建模和分割提供了新方法，且不限于图像分割，可应用于任何空间相关数据。

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [14] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: DynaPURLS是一个用于零样本骨架动作识别的统一框架，通过建立多尺度视觉-语义对应关系并在推理时动态优化，显著提升了未见类别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本骨架动作识别方法主要依赖骨架特征与静态类别级语义的对齐，这种粗粒度对齐无法有效弥合可见类别与未见类别之间的领域偏移，限制了细粒度视觉知识的迁移。

Method: 1) 使用大语言模型生成包含全局运动和局部身体部位动态的分层文本描述；2) 自适应分区模块通过语义分组骨架关节生成细粒度视觉表示；3) 动态优化模块在推理时通过轻量级可学习投影将文本特征适配到输入视觉流；4) 置信感知的类别平衡记忆库稳定优化过程，减少噪声伪标签的错误传播。

Result: 在NTU RGB+D 60/120和PKU-MMD三个大规模基准数据集上的广泛实验表明，DynaPURLS显著优于现有方法，创造了新的最先进记录。

Conclusion: DynaPURLS通过建立鲁棒的多尺度视觉-语义对应关系并在推理时动态优化，有效解决了零样本骨架动作识别中的领域偏移问题，显著提升了模型对未见类别的泛化能力。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [15] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: TARA是一种无需视频数据、基于多模态大语言模型的时间感知视频-文本嵌入模型适配方法，在时间感知检索、否定理解和动词副词理解方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建通用的时间感知视频-文本嵌入模型，解决现有模型在时间感知检索方面的不足，特别是对时间顺序敏感的动作识别。

Method: 提出TARA（时间感知检索适配）方法，无需视频数据即可将多模态大语言模型适配为时间感知视频-文本嵌入模型。同时提出新的基准测试，使用时间相反（手性）动作作为困难负样本。

Result: TARA在手性基准测试中优于所有现有视频-文本模型，在标准基准测试中也表现强劲。此外，TARA嵌入具有否定感知能力，在动词和副词理解方面达到最先进水平。

Conclusion: TARA产生了一个强大、通用、时间感知的视频-文本嵌入模型，具有最先进的零样本性能，在时间感知检索、否定理解和动作理解方面均有显著优势。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [16] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

TL;DR: CARI4D：首个从单目RGB视频中重建4D人-物交互的类别无关方法，通过姿态假设选择算法和渲染比较范式实现空间、时间和像素对齐，在未见数据集上表现优于先前方法36%


<details>
  <summary>Details</summary>
Motivation: 从单目RGB视频准确捕捉人-物交互对于人类理解、游戏和机器人学习应用很重要，但由于未知物体和人体信息、深度模糊、遮挡和复杂运动等因素，从单视角推断4D交互极具挑战性。先前方法通常需要真实物体模板或局限于有限物体类别。

Method: 提出CARI4D方法：1）姿态假设选择算法，鲁棒地整合基础模型的个体预测；2）通过学习的渲染-比较范式联合优化，确保空间、时间和像素对齐；3）推理复杂接触点进行进一步细化，满足物理约束。这是首个类别无关的4D人-物交互重建方法。

Result: 在分布内数据集上重建误差优于先前方法38%，在未见数据集上优于36%。模型能够泛化到训练类别之外，可零样本应用于野外互联网视频。代码和预训练模型将公开发布。

Conclusion: CARI4D是首个从单目RGB视频中重建空间和时间一致的4D人-物交互的类别无关方法，通过整合基础模型预测和渲染比较优化，显著提升了重建精度和泛化能力，为实际应用提供了有效解决方案。

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [17] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: V-REX是一个评估视觉语言模型多步探索推理能力的评测套件，包含挑战性视觉推理任务和评估协议，将多步探索转化为问题链，分别评估规划和执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理需要多步探索和推理的复杂开放任务时表现不佳，但这类任务的中间步骤难以评估。需要开发专门的评估框架来填补这一空白。

Method: 开发V-REX评估套件，包含跨多个领域的挑战性视觉推理任务。将多步探索推理转化为问题链（CoQ），分别评估模型的规划能力（分解任务并选择探索性问题链）和执行能力（按顺序回答问题链以收集信息）。通过为每个步骤策划有限的问题和答案选项，实现可靠的定量和细粒度分析。

Result: 通过评估最先进的专有和开源视觉语言模型，揭示了模型能力的一致扩展趋势，规划能力和执行能力之间存在显著差异，以及多步探索推理仍有巨大的改进空间。

Conclusion: V-REX提供了一个有效的框架来评估视觉语言模型的多步探索推理能力，揭示了当前模型的局限性，并为未来改进指明了方向。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [18] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: Semantic-Drive是一个本地优先的神经符号框架，用于从自动驾驶车辆的视频日志中挖掘罕见的安全关键事件，通过解耦感知为符号定位和认知分析两阶段，显著提高了召回率并降低了风险评估错误。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的发展受到"长尾"训练数据稀缺的瓶颈限制。虽然车队收集了海量视频日志，但识别罕见的安全关键事件（如乱穿马路、施工改道）仍然是一个手动、成本高昂的过程。现有解决方案要么依赖缺乏精度的粗粒度元数据搜索，要么使用侵犯隐私且昂贵的基于云的视觉语言模型。

Method: 提出Semantic-Drive框架，将感知解耦为两个阶段：(1) 符号定位：使用实时开放词汇检测器（YOLOE）锚定注意力；(2) 认知分析：通过推理视觉语言模型进行法医场景分析。为缓解幻觉问题，实施了"系统2"推理时对齐策略，采用多模型"法官-侦察员"共识机制。

Result: 在nuScenes数据集上使用Waymo开放数据集（WOD-E2E）分类法进行基准测试，Semantic-Drive实现了0.966的召回率（CLIP为0.475），与单模型相比将风险评估错误降低了40%。系统完全在消费级硬件（NVIDIA RTX 3090）上运行。

Conclusion: Semantic-Drive提供了一个隐私保护的云替代方案，能够高效地从自动驾驶车辆视频日志中挖掘罕见的安全关键事件，解决了长尾数据稀缺问题，显著提高了事件检测的召回率和准确性。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [19] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

TL;DR: 联邦学习在船舶检测中的应用研究，比较了四种FL模型与本地训练基线的性能，发现FL能显著提升小数据集上的检测精度，接近使用全部数据的全局训练效果。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习在船舶检测中的应用，旨在提供一种隐私保护的解决方案，避免数据共享或集中收集，特别适用于处理商业卫星图像或敏感船舶标注数据。

Method: 使用YOLOv8船舶检测模型，评估了四种联邦学习模型（FedAvg、FedProx、FedOpt、FedMedian），并与本地训练基线进行比较。本地训练基线是在每个数据集上独立训练而不共享学习参数。

Result: 联邦学习模型显著提高了在较小本地数据集上的检测精度，性能接近使用所有数据集进行训练的全局训练效果。研究还强调了选择合适的FL配置（如通信轮数和本地训练轮数）对优化检测精度和保持计算效率的重要性。

Conclusion: 联邦学习为卫星图像船舶检测提供了一种有效的隐私保护解决方案，能够在保护数据隐私的同时实现接近全局训练的性能，但需要仔细配置FL参数以获得最佳效果。

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [20] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: 本研究提出了一种监督式语义分割工作流，旨在提高野火后烧毁区域（BA）划分的性能和效率，针对SPOT-6/7高分辨率影像，评估了U-Net和SegFormer模型在紧急管理场景下的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前烧毁区域划分方法主要依赖计算机视觉模型处理灾后遥感影像，但往往忽视了其在时间紧迫的应急管理场景中的实际适用性。需要开发既高效又准确的BA划分方法以支持灾害评估和生态系统恢复。

Method: 提出监督式语义分割工作流，使用SPOT-6/7高分辨率影像。对比评估U-Net和SegFormer模型性能，引入土地覆盖数据作为辅助任务增强模型鲁棒性，并采用测试时增强（TTA）技术。使用Dice分数、IoU和推理时间作为评估指标。

Result: U-Net和SegFormer在有限训练数据下表现相似，但SegFormer需要更多计算资源，在紧急情况下实用性受限。加入土地覆盖辅助任务能提高模型鲁棒性且不增加推理时间。测试时增强能提升划分性能但会增加推理时间，可通过混合精度等优化方法缓解。

Conclusion: 在紧急管理场景中，U-Net模型因其资源效率更具实用性。辅助任务和测试时增强能有效提升烧毁区域划分性能，但需平衡性能提升与推理时间增加的关系。混合精度优化等技术可帮助实现这一平衡。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [21] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: CreativeVR是一个针对AI生成视频和真实视频中严重结构/时序伪影的修复框架，通过单一精度控制旋钮在精确修复和结构校正间平滑权衡，在AIGC54基准上取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频扩散模型在细粒度结构上表现脆弱，常产生扭曲的面部、手部、背景和时序不一致的运动；传统视频修复方法主要针对合成退化（如模糊、下采样），而扩散先验修复器通常针对光度噪声训练，缺乏对感知质量与保真度权衡的控制。

Method: 提出CreativeVR框架，基于深度适配器的方法暴露单一精度控制旋钮；关键创新是训练时使用的时序一致性退化模块，应用精心设计的变换来产生真实的结构失效；针对AIGC伪影修复提出了AIGC54基准。

Result: 在严重伪影视频上取得最先进结果，在标准视频修复基准上表现有竞争力；实际吞吐量约为13 FPS（720p分辨率，单张80GB A100显卡）。

Conclusion: CreativeVR为AI生成内容和真实视频中的严重结构及时序伪影提供了有效的修复解决方案，通过单一控制旋钮实现了修复精度与结构校正的灵活权衡。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [22] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: BAgger是一种自监督训练方案，通过构建从模型自身生成轨迹的纠正路径，解决自回归视频模型中的曝光偏差问题，提高长期视频生成的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型在通过下一帧预测进行世界建模时存在曝光偏差问题：训练时使用干净上下文，而推理时使用自生成帧，导致误差累积和质量随时间漂移。

Method: 提出Backwards Aggregation (BAgger)自监督方案，从模型自身的生成轨迹构建纠正路径，教导模型从自身错误中恢复。该方法使用标准得分或流匹配目标进行训练，避免使用大型教师模型和长时间链的反向传播。

Result: 在因果扩散变换器上实例化BAgger，在文本到视频、视频扩展和多提示生成任务中评估，观察到更稳定的长期运动、更好的视觉一致性，并减少了漂移现象。

Conclusion: BAgger通过自监督纠正轨迹有效解决了自回归视频模型的曝光偏差问题，提高了长期视频生成的质量和稳定性，避免了传统方法可能带来的质量和多样性损失。

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [23] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

TL;DR: RePack框架通过将高维视觉基础模型特征压缩到低维流形，解决了信息过载问题，显著加速了扩散变换器的收敛并提升了图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉基础模型的高维特征虽然能增强潜在扩散模型，但可能导致信息过载，特别是当特征维度超过原始图像解码所需时。需要一种方法既能保留VFM特征的语义信息，又能避免高维度带来的负面影响。

Method: 提出RePack（Representation Packing）框架，将高维VFM表示通过投影变换到低维流形上，形成更紧凑、解码器友好的表示，有效过滤非语义噪声同时保留核心结构信息。

Result: 在DiT-XL/2上，RePack仅用64个epoch就达到了3.66的FID分数，比现有最先进方法收敛速度快35%，显著优于直接将原始VFM特征注入解码器的方法。

Conclusion: RePack成功提取了VFM表示的核心语义信息，同时避免了高维度带来的副作用，为高效利用预训练视觉基础模型增强扩散模型提供了有效解决方案。

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [24] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

TL;DR: VEGAS：通过将视觉编码器的注意力图注入语言模型中间层来减少LVLM幻觉的推理时方法


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）虽然能联合推理视觉和文本输入，但经常产生与视觉证据事实不一致的输出（幻觉）。现有研究未能有效回答：何种形式的视觉注意力能在解码过程中有效抑制幻觉？

Method: 提出VEGAS方法：1）发现视觉编码器自身的注意力图能有效减少幻觉；2）分析解码过程中的视觉-文本冲突，发现冲突在语言模型中间层达到峰值；3）将视觉编码器的注意力图注入语言模型的中间层，自适应地引导未能聚焦关键图像对象的token

Result: 在多个基准测试上的广泛实验表明，VEGAS在减少幻觉方面始终达到最先进的性能

Conclusion: 视觉编码器的注意力图是抑制LVLM幻觉的有效形式，VEGAS通过简单而有效的推理时方法成功减少了模型输出中的事实不一致问题

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [25] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: SPDMark是一种基于选择性参数位移的视频生成水印框架，通过修改生成模型参数子集嵌入水印，利用低秩适配实现参数效率，支持高精度水印恢复并抵抗多种视频修改。


<details>
  <summary>Details</summary>
Motivation: 高质量视频生成模型的兴起增加了对鲁棒水印方案的需求，现有视频水印方法无法同时实现不可感知性、鲁棒性和计算效率。

Method: 基于选择性参数位移的视频扩散模型水印框架，将位移建模为层间基移的加性组合，利用低秩适配实现参数效率，联合训练基移和水印提取器，使用加密哈希函数生成帧特定水印消息，通过最大二分图匹配恢复帧顺序。

Result: 在文本到视频和图像到视频生成模型上的评估表明，SPDMark能够生成不可感知的水印，并以高精度恢复水印，同时对多种常见视频修改具有鲁棒性。

Conclusion: SPDMark为视频生成模型提供了一种高效、鲁棒且不可感知的水印方案，能够可靠地检测和跟踪生成视频的来源。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [26] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 该研究探索了在传统光学显微镜和数字同轴全息显微镜(DIHM)图像上实现全自动花粉识别的性能差异，并通过GAN生成合成DIHM图像来改善检测效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决DIHM图像中花粉识别面临的挑战，包括散斑噪声、孪生像伪影以及与明场图像的显著外观差异，这些因素使得在未重建的全息图像中视觉识别花粉变得困难。

Method: 研究方法包括：1) 使用YOLOv8s进行目标检测和MobileNetV3L进行分类；2) 在双模态数据集（自动标注的光学图像和仿射对齐的DIHM图像）上训练；3) 扩展DIHM图像中花粉的边界框；4) 使用带谱归一化的Wasserstein GAN(WGAN-SN)生成合成DIHM图像；5) 混合真实和合成数据进行训练。

Result: 在光学数据上，检测mAP50达到91.3%，分类准确率达到97%；而在DIHM数据上，检测mAP50仅为8.15%，分类准确率为50%。扩展边界框后，DIHM检测mAP50提升至13.3%，分类准确率提升至54%。使用GAN生成合成图像(FID分数58.246)并以1:1.5比例混合真实和合成数据后，目标检测性能提升至15.4%。

Conclusion: 研究表明，基于GAN的数据增强可以缩小光学和DIHM图像识别性能之间的差距，为兽医成像领域的全自动DIHM工作流程向实际应用迈出了重要一步。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [27] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

TL;DR: 本文提出了EchoGround-MIMIC数据集和EchoVLM模型，这是首个基于测量的多模态超声心动图数据集和视觉语言模型，显著提升了超声心动图自动解读的性能。


<details>
  <summary>Details</summary>
Motivation: 超声心动图是心脏病学中最广泛使用的成像方式，但其解读仍然劳动密集且本质上是多模态的，需要视图识别、定量测量、定性评估和基于指南的推理。现有的视觉语言模型在超声心动图领域的应用受到缺乏大规模临床基础数据集和测量推理能力的限制。

Method: 1. 创建EchoGround-MIMIC数据集：包含19,065个图像-文本对，来自1,572名患者，具有标准化视图、结构化测量、基于测量的描述和指南衍生的疾病标签。2. 提出EchoVLM模型：引入两种新的预训练目标：(i)视图感知对比损失，编码超声心动图成像的视图依赖结构；(ii)否定感知对比损失，区分临床关键的阴性与阳性发现。

Result: 在涵盖多模态疾病分类、图像-文本检索、视图分类、腔室分割和地标检测的36个任务中，EchoVLM实现了最先进的性能：零样本疾病分类AUC为86.5%，视图分类准确率为95.1%。临床基础的多模态预训练产生了可迁移的视觉表示。

Conclusion: EchoVLM被确立为端到端超声心动图解读的基础模型。EchoGround-MIMIC数据集和数据处理代码将公开发布，促进多模态超声心动图解读的可重复性和进一步研究。

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [28] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: 该研究提出了一种针对3D CT医学影像的基于分块的拓扑数据分析方法，相比传统的3D立方体复形方法，在分类性能和计算效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于3D立方体复形过滤的拓扑数据分析方法在处理高分辨率CT图像时存在性能不足和计算复杂度高的问题，需要更高效的方法来提取CT图像中的拓扑特征。

Method: 提出了一种新颖的基于分块的持久同调构建方法，专门针对体积医学成像数据（特别是CT模态）设计。该方法将3D CT图像分割成小块进行处理，避免了传统立方体复形方法的计算复杂度问题。

Result: 在多个3D CT数据集上的实验表明，基于分块的TDA方法在分类性能和计算效率上都优于传统的3D立方体复形算法。具体来说，在准确率、AUC、敏感性、特异性和F1分数上分别平均提升了10.38%、6.94%、2.06%、11.58%和8.51%。

Conclusion: 基于分块的拓扑数据分析方法在3D CT图像处理中具有显著优势，既提高了分类性能又提升了计算效率。研究还提供了方便的Python软件包Patch-TDA来促进该方法的实际应用。

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [29] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: 该研究创建了最大的道路损坏评估基准数据集CRASAR-U-DRIODs，包含10次联邦灾害后的无人机图像，标注了657.25公里道路，并提供18个基线模型，解决了现有数据集规模小、分辨率低、缺乏操作验证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有灾害道路损坏评估数据集存在三个主要问题：1) 规模小或依赖低分辨率图像，无法检测应急管理者关心的现象；2) 缺乏操作验证的机器学习系统；3) 实践中发现道路线错位问题严重影响模型性能。

Method: 1) 创建CRASAR-U-DRIODs数据集，包含10次联邦灾害后的无人机图像；2) 按照10类标注方案标注657.25公里道路；3) 提供9,184个道路线调整用于空间对齐；4) 训练18个基线模型并在2024年飓风Debby和Helene的实际应急响应中部署验证。

Result: 1) 当18个基线模型部署到实际错位的道路线时，模型性能平均下降5.596% Macro IoU；2) 如果不考虑空间对齐，约8%（11公里）的不良道路条件会被错误标注；3) 约9%（59公里）的道路线与实际道路错位。

Conclusion: 道路线空间错位是影响灾害道路损坏评估模型性能的关键因素，ML、CV和机器人社区需要解决这一差距，以实现灾害期间更有效和明智的决策制定。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [30] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 该研究开发了一个深度学习模型，通过融合遥感观测和物理模型数据，生成格陵兰冰盖每日100米分辨率的地表融水分布图，解决了现有融水地图在时间和空间分辨率上的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰盖加速融化，但相关过程尚未完全理解且难以测量。现有融水地图面临时间分辨率与空间分辨率的权衡：要么时间分辨率高但空间分辨率低，要么空间分辨率高但时间分辨率低。

Method: 开发深度学习模型，融合区域气候模型（RCM）、合成孔径雷达（SAR）、被动微波（PMW）和数字高程模型（DEM）数据，对2017-2023年东格陵兰Helheim冰川区域进行时空降尺度处理。使用SAR衍生的融水数据作为"地面真值"，评估了UNet和DeepLabv3+等标准深度学习方法。

Result: 融合所有数据流的深度学习方法在研究区域比现有非深度学习方法准确率高10个百分点以上：相比仅依赖区域气候模型的方法（83% vs. 95%），以及仅依赖被动微波观测的方法（72% vs. 95%）。基于SAR数据的滑动窗口计算方法虽然低估极端融化事件，但也达到了90%的准确率。

Conclusion: 深度学习融合多源数据的方法能显著提高地表融水地图的时空分辨率，为理解格陵兰冰盖融化过程提供更精确的工具。研究发布了时空对齐的数据集MeltwaterBench作为基准，促进更复杂数据驱动降尺度方法的比较。

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [31] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

TL;DR: 提出首个利用音频信息进行真实世界视频中相对相机姿态估计的方法，通过音频-视觉框架在视觉信息退化时仍能保持鲁棒性


<details>
  <summary>Details</summary>
Motivation: 视觉方法在运动模糊、遮挡等视觉退化条件下表现不佳，而被动场景声音提供了互补线索，可用于增强相机姿态估计

Method: 提出简单的音频-视觉框架，将到达方向谱和双耳化嵌入集成到最先进的纯视觉姿态估计模型中

Result: 在两个大型数据集上相比强视觉基线获得一致提升，在视觉信息受损时表现出鲁棒性

Conclusion: 这是首个成功利用音频进行真实世界视频中相对相机姿态估计的工作，确立了日常音频作为经典空间挑战的意外但有前景的信号

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [32] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级周界跟踪方法，用于在有限带宽条件下在野火环境中操作的微型无人机编队。通过热成像和RGB图像融合处理，结合周期性信标和惯性反馈，实现了低延迟、稳定的边界跟踪。


<details>
  <summary>Details</summary>
Motivation: 针对野火环境下的紧急侦察应用，需要微型无人机编队在有限带宽条件下进行快速部署，要求鲁棒感知和最小化通信，同时保持轨迹稳定性。

Method: 1. 热成像帧通过自适应阈值和形态学细化生成粗略的热区域掩码；2. RGB帧提供边缘线索，并使用基于梯度的滤波抑制纹理相关的误检测；3. 规则级融合策略选择边界候选，通过Ramer-Douglas-Peucker算法简化；4. 系统包含周期性信标和惯性反馈回路，在GPS降级时保持轨迹稳定性；5. 在嵌入式SoC平台上通过限制每帧像素操作和预计算梯度表实现亚50ms延迟。

Result: 小规模模拟显示：与纯边缘跟踪基线相比，平均路径长度和边界抖动减少，同时通过交集合并分析保持环境覆盖；电池消耗和计算利用率证实了在标准微型平台上实现10-15 m/s前向运动的可行性。

Conclusion: 该方法实现了快速现场部署，为紧急侦察应用提供了鲁棒感知和最小化通信的解决方案，在有限带宽条件下有效支持微型无人机编队在野火环境中的周界跟踪任务。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [33] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

TL;DR: 英国布里斯托尔部署22个固定角度摄像头，采集2021-2025年城市路灯图像数据集，包含52.6万张图像及丰富元数据，用于研究视觉漂移、异常检测和MLOps策略。


<details>
  <summary>Details</summary>
Motivation: 为智能城市部署提供真实世界、大规模、长期的视觉数据集，支持研究视觉漂移、模型稳定性、异常检测等实际问题。

Method: 使用22个固定摄像头每小时采集图像，提供丰富元数据（时间戳、GPS坐标、设备ID）。基于卷积变分自编码器（CNN-VAEs）构建自监督框架，为每个摄像头节点和日/夜图像集分别训练模型。

Result: 创建了包含52.6万张图像的数据集，涵盖不同光照、天气和季节条件。定义了两个漂移度量指标：相对质心漂移（捕捉潜在空间偏离）和相对重建误差（测量图像域退化）。

Conclusion: 该数据集为评估长期模型稳定性、漂移感知学习和部署就绪的视觉系统提供了真实、细粒度的基准，支持街道照明监控、天气推断和城市场景理解等下游应用。

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [34] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 本文提出ISA-ViT框架和ALERT数据集，解决基于UWB雷达的驾驶员分心行为识别中缺乏大规模真实数据集和ViT模型输入尺寸固定的问题。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶导致全球致命事故，基于IR-UWB雷达的驾驶员活动识别具有抗干扰、低功耗和隐私保护优势，但面临两大挑战：缺乏覆盖多种分心驾驶行为的大规模真实UWB数据集，以及固定输入尺寸的Vision Transformers难以适应非标准维度的UWB雷达数据。

Method: 提出输入尺寸无关的Vision Transformer（ISA-ViT）框架，通过调整补丁配置和利用预训练位置嵌入向量，在满足ViT输入要求的同时保留雷达特定信息（多普勒频移和相位特征）。采用域融合策略结合距离域和频域特征提升分类性能，并发布包含10,220个雷达样本的ALERT数据集。

Result: ISA-ViT在基于UWB的驾驶员活动识别任务中，相比现有ViT方法实现了22.68%的准确率提升。ALERT数据集为真实驾驶条件下七种分心驾驶活动提供了大规模雷达样本。

Conclusion: 通过公开ALERT数据集和详细描述输入尺寸无关策略，这项工作促进了更鲁棒、可扩展的分心驾驶检测系统的开发，为实际部署奠定了基础。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [35] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: CineLOG是一个包含5000个高质量平衡视频片段的数据集，带有详细场景描述、基于标准电影分类的明确摄像机指令和类型标签，解决了现有数据集的数据不平衡、噪声标签和模拟到真实差距问题。


<details>
  <summary>Details</summary>
Motivation: 当前可控视频合成模型在超越文本提示的细粒度控制方面存在困难，特别是在摄像机轨迹和电影类型等电影属性方面。现有数据集通常存在严重的数据不平衡、噪声标签或显著的模拟到真实差距。

Method: 提出了CineLOG数据集创建的新流程，将复杂的文本到视频生成任务解耦为四个更简单的阶段，使用更成熟的技术。引入了轨迹引导过渡模块来生成平滑的时空插值，实现连贯的多镜头序列。

Result: 广泛的人类评估表明，该流程在遵循特定摄像机和剧本指令方面显著优于最先进的端到端文本到视频模型，同时保持专业的视觉质量。

Conclusion: CineLOG数据集和提出的流程为可控视频合成提供了高质量、平衡的数据资源和有效的技术方案，解决了现有方法的局限性。

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [36] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: 本文提出评估视觉语言模型推理链视觉忠实性的新方法，通过分解感知与推理步骤，使用现成VLM评估步骤级忠实性，并提出轻量级自反思程序检测和修复不忠实的感知步骤。


<details>
  <summary>Details</summary>
Motivation: 当前推理增强型视觉语言模型虽然生成显式思维链，但存在新失败模式：模型可能通过视觉不忠实的中间步骤得出正确答案，或忠实推理却最终预测失败。仅评估最终答案准确性的标准方法无法区分这些行为。

Method: 提出无需训练和参考的框架，将推理链分解为感知与推理步骤，使用现成VLM评估步骤级视觉忠实性；提出轻量级自反思程序，检测并局部重新生成不忠实的感知步骤。

Result: 在多个推理训练VLM和感知密集型基准测试中，该方法降低了不忠实感知率，同时保持了最终答案准确性，提高了多模态推理的可靠性。

Conclusion: 视觉推理链的忠实性应作为独立评估维度，提出的框架和自反思程序能有效提升视觉语言模型推理的可靠性和透明度。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [37] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

TL;DR: 该论文提出了一种属性中心表示（ACR）的零样本学习框架，通过两个专家混合组件（MoPE和MoAE）实现属性解缠，以解决细粒度类别识别中的属性纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 识别未见过的细粒度类别需要模型能够区分细微的视觉差异。传统方法将颜色、形状、纹理等不同属性压缩到单一视觉嵌入中，导致属性纠缠问题，掩盖了关键区别。现有的事后解决方案不足，因为它们处理的是已经混合的表征。

Method: 提出AttributeCentric Representations（ACR）框架，通过两个专家混合组件实现属性解缠：1）Mixture of Patch Experts（MoPE）：使用双级路由机制将图像块有条件地分配到专门的专家进行处理；2）Mixture of Attribute Experts（MoAE）：将专家精炼的特征投影到稀疏的、部分感知的属性映射中，用于鲁棒的零样本分类。

Result: 在零样本学习基准数据集CUB、AwA2和SUN上，ACR框架取得了持续的最先进结果。

Conclusion: 通过在表示学习过程中施加属性解缠，提出的ACR框架有效解决了属性纠缠问题，在细粒度零样本学习任务中表现出色。

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [38] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: 该研究提出了ProImage-Bench基准，用于评估专业图像生成模型在科学精确图示方面的能力，通过基于规则的评估和迭代编辑提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在开放领域表现良好，但在生成信息密集、科学精确的专业图示方面存在不足，需要专门的评估基准来量化这一差距。

Method: 构建ProImage-Bench基准，包含654个真实教科书和技术报告中的图示，创建详细图像指令和层次化评估规则（6,076个标准，44,131个二元检查）。使用大型多模态模型从文本和参考图像中推导规则，并通过自动化LMM评估器进行评分。

Result: 基准测试显示，即使在开放领域表现优秀的模型，在ProImage-Bench上也只能达到0.791的规则准确率和0.553的标准分数，表明在科学保真度方面存在显著差距。通过将失败的检查反馈给编辑模型进行迭代优化，可以将规则准确率从0.653提升到0.865，标准分数从0.388提升到0.697。

Conclusion: ProImage-Bench为专业图像生成提供了严格的诊断工具，并为改进规范忠实的科学图示提供了可扩展的监督信号，有助于推动模型在专业领域的应用。

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [39] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: 该研究系统比较了SynthSeg和SamSeg两种方法在婴儿脑MRI分割中的准确性，以及对体积和分形维度估计的影响。SynthSeg在所有质量指标上优于SamSeg，提供了更可靠的体积和分形维度结果。


<details>
  <summary>Details</summary>
Motivation: 婴儿脑MRI的准确分割对于量化发育过程中的结构和复杂性变化至关重要，但由于髓鞘化过程中组织对比度降低，自动分割特别具有挑战性。需要评估不同分割方法对体积和分形维度估计的影响。

Method: 使用Baby Open Brains数据集（71次扫描，1-9个月婴儿），比较SynthSeg和SamSeg两种分割方法与专家标注的准确性。评估指标包括Dice系数、交并比、95%百分位Hausdorff距离和归一化互信息。分析分割准确性对体积和分形维度估计的影响。

Result: SynthSeg在所有质量指标上优于SamSeg（主要区域平均Dice > 0.8），体积估计与手动参考接近（平均+4%）。SamSeg系统性地高估脑室和全脑体积（平均+76%）。分割准确性随年龄增长而提高。分形维度分析显示SynthSeg与专家分割之间存在显著区域差异，分割相关的FD变异性超过了大多数发育队列报告的组间差异。

Conclusion: SynthSeg为儿科MRI提供了最可靠的体积和分形维度结果，但由于分割相关的不确定性，对体积和分形维度的微小形态差异应谨慎解释。分割偏差直接影响分形维度估计。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [40] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的方法，通过在LLM输出token上直接应用分割目标来检测视频时刻和高光片段，避免了传统文本生成方法无法提供帧级预测梯度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式多模态大语言模型的方法虽然有效，但文本生成无法为帧级预测提供直接梯度。虽然最近有强化学习方法尝试解决此问题，但作者提出了一种更直接的方法。

Method: 将固定数量的帧输入LLM，并设计提示使其输出连续的"0"和/或"1"字符序列，每个字符对应一帧。"0"/"1"字符既利用LLM的语言能力，又分别作为背景和前景概率。训练时结合分割损失和因果语言模型损失。

Result: 在QVHighlights数据集上，仅采样25帧（少于同类方法一半）就实现了56.74 HIT@1的高光检测性能，并在时刻检索任务上达到35.28 MAP，超过了基线。

Conclusion: 该方法通过在LLM输出token上直接应用分割目标，有效解决了文本生成方法无法提供帧级预测梯度的问题，分割损失即使在因果LM损失平台期也能提供稳定的补充学习信号。

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [41] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

TL;DR: MetaTPT：一种元学习框架，通过自监督辅助任务学习参数化增强来指导测试时提示调优，提升视觉语言模型在域偏移下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（如CLIP）具有强大的零样本泛化能力，但在测试时对域偏移仍然敏感。现有的测试时提示调优方法使用固定增强，在更具挑战性的场景中可能失效。

Method: 提出MetaTPT元学习框架，采用双循环优化范式：内循环学习自监督任务生成信息丰富的视图，外循环通过在这些视图间强制一致性来执行提示调优。框架动态学习每个样本的参数化增强，实现更具表达力的变换。

Result: 在域泛化和跨数据集基准测试中，MetaTPT实现了最先进的性能表现。

Conclusion: 通过将增强学习与提示调优相结合，MetaTPT显著提升了视觉语言模型在测试时面对域偏移的适应能力。

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [42] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

TL;DR: 提出一种用于连续学习场景的面部表情识别混合框架，结合深度卷积特征和面部动作单元，使用贝叶斯高斯混合模型缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在日常生活中日益普及，识别和适应人类情感对于有效的人机交互至关重要。面部表情识别是推断情感状态的主要渠道，但情感具有动态性和文化细微差别，需要能够持续学习而不遗忘先前知识的模型。

Method: 提出混合框架，整合两种互补模态：深度卷积特征和基于面部动作编码系统的面部动作单元。通过贝叶斯高斯混合模型对组合表示进行建模，提供轻量级概率解决方案，避免重新训练同时保持强判别能力。

Result: 使用复合面部表情数据集，模型能够先学习基本表情，然后逐步识别复合表情。实验显示提高了准确性、增强了知识保留能力并减少了遗忘。

Conclusion: 该框架有助于开发具有情感智能的AI系统，可应用于教育、医疗保健和自适应用户界面等领域。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [43] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 该论文提出了RealDrag——首个包含真实目标图像的基于点编辑图像基准数据集，包含400多个标注样本，并设计了四个新指标来客观评估拖拽式图像编辑模型。


<details>
  <summary>Details</summary>
Motivation: 当前拖拽式图像编辑模型的评估不可靠，缺乏标准化基准和指标。主要问题包括评估协议不一致，以及缺少包含真实目标图像的数据集，导致难以客观比较不同方法。

Method: 1. 构建RealDrag数据集：包含400多个来自多样化视频源的人工标注样本，提供源/目标图像、处理/目标点、可编辑区域掩码以及图像和编辑动作的描述性标注。
2. 提出四个任务特定指标：语义距离(SeD)、外部掩码保持分数(OMPS)、内部补丁保持分数(IPPS)和方向相似性(DiS)，分别量化像素级匹配保真度、非编辑区域保持和语义对齐。

Result: 使用该基准对17个最先进模型进行了首次大规模系统分析，揭示了当前方法之间的明确权衡，并建立了稳健、可复现的基线来指导未来研究。

Conclusion: RealDrag是首个包含真实目标图像的基于点编辑图像综合基准，通过标准化数据集和专门设计的指标，解决了该领域评估不可靠的问题，为客观比较模型性能提供了可靠框架。

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [44] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

TL;DR: GrowTAS是一种渐进式Transformer架构搜索方法，通过从小型子网开始训练并逐步加入大型子网，减少权重共享带来的干扰，提升搜索效果


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer架构搜索方法中，所有候选架构共享同一组权重，这导致严重干扰，特别是对小规模子网造成性能下降。研究发现训练良好的小规模子网可以作为训练更大子网的良好基础

Method: 提出GrowTAS渐进训练框架：1）从小型子网开始训练；2）逐步加入更大的子网；3）减少干扰并稳定训练过程。还提出GrowTAS+，仅微调部分权重以进一步提升大规模子网性能

Result: 在ImageNet和多个迁移学习基准测试（CIFAR-10/100、Flowers、CARS、INAT-19）上的广泛实验表明，该方法优于当前的Transformer架构搜索方法

Conclusion: 渐进式训练框架GrowTAS通过从小型子网开始并逐步扩展，有效减少了权重共享带来的干扰，提高了Transformer架构搜索的性能和稳定性

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [45] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: 论文提出了Intention-Drive基准测试，用于评估自动驾驶系统从高级人类意图到安全精确驾驶动作的转化能力，揭示了当前模型在意图理解方面的显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统仅能执行低级转向指令，缺乏理解并实现高级抽象人类意图的能力。实现真正智能自主驾驶需要从命令跟随者转变为意图实现者，但缺乏标准化基准来衡量和推动这一复杂任务的进展。

Method: 提出Intention-Drive基准，包含两个核心贡献：(1) 包含复杂场景及其对应自然语言意图的新数据集；(2) 以意图成功率(ISR)为中心的新评估协议，评估人类目标的语义实现程度，超越简单的几何精度。

Result: 通过对一系列基线模型在Intention-Drive上的广泛评估，揭示了显著的性能缺陷，显示基线模型难以达到这一高级任务所需的全面场景和意图理解能力。

Conclusion: Intention-Drive填补了评估自动驾驶系统从高级人类意图到驾驶动作转化能力的关键空白，为从命令跟随者到意图实现者的范式转变提供了标准化基准，揭示了当前模型在意图理解方面的不足，为未来研究指明了方向。

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [46] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

TL;DR: OMUDA提出了一种用于无监督域自适应语义分割的层次化掩码框架，通过上下文感知掩码、特征蒸馏掩码和类别解耦掩码三个策略，在上下文、特征表示和类别层面减少域差异，在多个基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法在处理跨域语义分割时面临三个主要挑战：跨域上下文模糊性、不一致的特征表示以及类别级伪标签噪声。这些因素导致模型难以有效弥合源域和目标域之间的差异。

Method: OMUDA提出统一的层次化掩码框架，包含三个核心策略：1) 上下文感知掩码(CAM)：自适应区分前景与背景，平衡全局上下文和局部细节；2) 特征蒸馏掩码(FDM)：通过预训练模型的知识转移增强鲁棒且一致的特征学习；3) 类别解耦掩码(CDM)：通过显式建模类别级不确定性来减轻噪声伪标签的影响。

Result: 在多个具有挑战性的跨域语义分割基准测试中验证了OMUDA的有效性。特别是在SYNTHIA->Cityscapes和GTA5->Cityscapes任务中，OMUDA可以无缝集成到现有UDA方法中，平均提升7%，持续达到最先进的结果。

Conclusion: OMUDA通过层次化掩码策略在上下文、表示和类别层面系统性地减少域偏移，为无监督域自适应语义分割提供了一个超越现有方法的统一解决方案，显著提升了跨域泛化能力。

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [47] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

TL;DR: MRD方法通过可微分渲染寻找物理上不同但产生相同模型激活的3D场景参数，用于探究视觉模型对生成性3D场景属性的隐式理解


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在视觉任务中表现出色，但模型表示和决策难以解释。虽然视觉模型通常在2D输入上训练，但常被假设能发展对底层3D场景的隐式理解。需要一种方法来探究模型对生成性3D场景属性的理解

Method: 提出MRD方法，使用基于物理的可微分渲染，寻找物理上不同但产生相同模型激活的3D场景参数（模型元相似体）。与之前基于像素的方法不同，这些重建结果始终基于物理场景描述，可以分离探究模型对不同场景属性的敏感性

Result: 评估了多个模型在恢复场景几何形状和材料BRDF参数方面的能力。结果显示目标和优化场景之间的模型激活高度相似，但视觉结果各异。重建结果有助于定性地研究模型对哪些物理场景属性敏感或不敏感

Conclusion: MRD方法通过分析物理场景参数如何驱动模型响应变化，有望推进对计算机和人类视觉的理解。该方法为探究视觉模型的隐式3D场景理解提供了新的分析工具

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [48] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: WeDetect是一个基于检索哲学的开集目标检测模型家族，通过双塔架构实现实时检测，在15个基准测试中达到SOTA性能，并支持历史数据回溯和与LMMs集成


<details>
  <summary>Details</summary>
Motivation: 探索无跨模态融合层的检索式开集目标检测方法，充分发挥其在推理效率和多功能性方面的独特优势，建立统一的开集目标检测基础

Method: 提出WeDetect模型家族：1) WeDetect采用双塔架构，在共享嵌入空间中将区域与文本查询匹配；2) WeDetect-Uni作为通用建议生成器，冻结检测器仅微调目标性提示；3) WeDetect-Ref基于LMM的对象分类器，处理复杂指代表达式

Result: 在15个基准测试中达到最先进性能，支持实时检测、历史数据对象检索、复杂指代表达理解等多种应用，具有高推理效率

Conclusion: WeDetect家族在统一的检索框架下整合了检测、建议生成、对象检索和指代表达理解，展示了检索式方法在开集目标检测中的强大潜力和多功能性

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [49] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: UniCoDe提出了一种统一的算法，将采样方法和梯度引导方法结合，通过整合局部梯度信号提高采样效率，在奖励对齐和扩散无条件先验偏离之间实现更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 对齐扩散模型输出与下游目标对提高任务特定性能至关重要。现有推理时无训练方法主要分为采样方法和梯度引导方法，但各有局限性，需要一种统一框架来结合两者的优势。

Method: 提出UniCoDe算法，将采样方法和梯度引导方法统一到一个框架中。在采样过程中整合局部梯度信号，解决复杂奖励采样方法的效率问题，同时结合两种范式的优势。

Result: 实证结果表明，UniCoDe在一系列任务中与最先进的基线方法保持竞争力，实现了更高效的采样，并在奖励对齐和扩散无条件先验偏离之间提供了更好的权衡。

Conclusion: UniCoDe成功地将采样和梯度引导方法统一到一个框架中，通过整合局部梯度信号提高了采样效率，为扩散模型的对齐任务提供了一种有效的解决方案。

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [50] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

TL;DR: 提出TCLeaf-Net用于田间叶片病害检测，结合Transformer和CNN处理复杂背景，通过特征保留和可变形对齐提升多尺度融合，在Daylily-Leaf数据集上mAP@50达78.2%，优于现有YOLO和RT-DETR模型。


<details>
  <summary>Details</summary>
Motivation: 田间叶片病害检测面临复杂背景干扰、域偏移和病变级别数据集有限等挑战，需要开发鲁棒且实用的检测模型。

Method: 1) 发布Daylily-Leaf配对病变级别数据集；2) 提出TCLeaf-Net混合检测器：Transformer-卷积模块(TCM)抑制非叶片区域，原始尺度特征召回采样(RSFRS)保留空间细节，可变形对齐FPN(DFPN)增强多尺度融合。

Result: 在Daylily-Leaf田间数据集上，mAP@50提升5.4个百分点至78.2%，计算量减少7.5 GFLOPs，GPU内存使用降低8.7%，优于YOLO和RT-DETR系列，在PlantDoc等数据集上表现良好。

Conclusion: TCLeaf-Net能有效处理田间复杂背景下的叶片病害检测，具有鲁棒性和泛化能力，为实际农业应用提供了实用解决方案。

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [51] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: STAGE提出了一种基于故事板的多镜头视频生成工作流，通过预测结构化故事板、引入多镜头记忆包和双编码策略，解决了现有关键帧方法在跨镜头一致性和电影语言捕捉方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在视频合成中虽然取得了视觉保真度的显著进步，但创建连贯的多镜头叙事仍然是一个重大挑战。基于关键帧的方法虽然提供了细粒度控制和更高效率，但往往无法保持跨镜头一致性，也难以捕捉电影语言。

Method: 1. 提出STAGE工作流，将基于关键帧的多镜头视频生成任务重新表述为故事板锚定生成；2. 开发STEP2预测每个镜头的起始-结束帧对组成的结构化故事板；3. 引入多镜头记忆包确保长距离实体一致性；4. 采用双编码策略保证镜头内连贯性；5. 使用两阶段训练方案学习电影化的镜头间过渡；6. 构建大规模ConStoryBoard数据集，包含高质量电影片段及细粒度标注。

Result: 大量实验表明，STAGE在结构化叙事控制和跨镜头连贯性方面实现了优越性能。

Conclusion: STAGE通过故事板锚定的生成方法，有效解决了多镜头视频生成中的跨镜头一致性和电影语言表达问题，为创建连贯的叙事视频提供了新的解决方案。

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [52] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

TL;DR: V-Warper是一个无需训练的视频个性化框架，通过粗粒度外观适应和细粒度外观注入两阶段方法，在保持文本对齐和运动动态的同时显著提升外观保真度，无需大规模视频微调。


<details>
  <summary>Details</summary>
Motivation: 现有视频个性化方法依赖繁重的视频微调或大规模视频数据集，计算成本高且难以扩展，同时在帧间保持细粒度外观一致性方面存在困难。

Method: 提出两阶段训练免费框架：1) 轻量级粗粒度外观适应阶段，仅使用少量参考图像，通过图像LoRA和主题嵌入适应编码全局主题身份；2) 推理时细粒度外观注入阶段，通过计算RoPE-free中间层查询-键特征的语义对应关系，引导外观丰富的值表示到生成过程的语义对齐区域。

Result: V-Warper显著提高了外观保真度，同时保持了文本提示对齐和运动动态，且无需大规模视频微调即可高效实现这些改进。

Conclusion: V-Warper通过创新的粗到细框架解决了现有视频个性化方法的局限性，在无需额外视频训练的情况下实现了更好的外观一致性，为基于transformer的视频扩散模型提供了高效且可扩展的个性化解决方案。

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [53] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

TL;DR: M4Human是目前最大规模的多模态人体网格重建基准数据集，包含66.1万帧高分辨率毫米波雷达、RGB和深度数据，提供原始雷达张量和处理后的雷达点云，支持不同粒度RF信号研究。


<details>
  <summary>Details</summary>
Motivation: 现有大规模人体网格重建数据集主要依赖可见光RGB输入，但视觉传感存在遮挡、光照变化和隐私问题。毫米波雷达能实现隐私保护的室内人体感知，但现有雷达数据集存在骨架标签稀疏、规模有限、动作简单等问题。

Method: 构建M4Human多模态基准数据集，包含661K帧数据（比现有最大数据集大9倍），提供高分辨率毫米波雷达、RGB和深度数据。数据集包含原始雷达张量和处理后的雷达点云，覆盖20名受试者和50种多样化动作，包括原地、坐姿、自由空间运动和康复动作，并提供高质量运动捕捉标注（3D网格和全局轨迹）。

Result: 建立了雷达张量和雷达点云两种模态的基准，以及多模态融合（RGB-D）基准。实验结果突显了M4Human对雷达人体建模的重要性，同时揭示了在快速、无约束运动下的持续挑战。

Conclusion: M4Human是目前最大规模的多模态人体网格重建基准数据集，为雷达人体建模研究提供了重要资源，推动了隐私保护人体感知技术的发展，数据集和代码将在论文发表后公开。

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [54] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

TL;DR: SR-DiT框架整合了多种扩散transformer训练优化技术，在ImageNet-256上仅用140M参数模型就达到了与更大模型相当的性能，创造了该规模下的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散transformer的训练效率已有显著提升，但现有技术多被孤立研究，缺乏对多种方法协同效应的探索。本研究旨在系统整合不同优化技术，发掘它们的协同潜力。

Method: 提出了SR-DiT框架，在表示对齐的基础上系统整合了三种关键技术：token路由、架构改进和训练修改，通过系统组合这些技术来提升训练效率。

Result: 在ImageNet-256上仅用140M参数模型（400K迭代，无分类器引导）就达到了FID 3.49和KDD 0.319，性能与需要更大参数（685M）和更长训练时间的模型相当，创造了该规模下的最先进结果。

Conclusion: 通过广泛的消融研究确定了最有效的技术组合，发现了协同效应和不兼容性，并将框架开源作为未来研究的计算可访问基线。

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [55] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

TL;DR: ArtGen是一个基于条件扩散的框架，能够从单视图图像或文本描述生成具有准确几何和连贯运动学的铰接式3D物体，通过交叉状态蒙特卡洛采样和思维链推理解决几何形状与关节动态纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型通常依赖表示闭合状态的单视图输入，导致几何形状与关节动态纠缠，产生模糊或不现实的运动学结构。铰接式资产生成对机器人、数字孪生和具身智能至关重要。

Method: 1) 采用交叉状态蒙特卡洛采样显式强制执行全局运动学一致性；2) 集成思维链推理模块推断结构先验（部件语义、关节类型、连接性）；3) 使用稀疏专家扩散变换器专门处理多样化运动学交互；4) 采用局部-全局注意力增强的组合式3D-VAE潜在先验捕获细粒度几何和全局部件级关系。

Result: 在PartNet-Mobility基准测试上的广泛实验表明，ArtGen显著优于最先进的方法。

Conclusion: ArtGen能够从单视图图像或文本描述生成具有准确几何和连贯运动学的铰接式3D物体，有效解决了结构-运动纠缠问题，为机器人、数字孪生和具身智能应用提供了高质量的铰接式资产生成解决方案。

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [56] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: ViInfographicVQA是首个越南语信息图表视觉问答基准，包含6747个真实信息图表和20409个人工验证问答对，涵盖经济、医疗、教育等领域，包含单图和多图两种评估任务。


<details>
  <summary>Details</summary>
Motivation: 信息图表视觉问答需要模型在数据丰富、布局复杂的视觉内容中读取和推理，相比场景文本或自然图像VQA，需要更强的OCR、布局理解、数值和语义推理能力。目前缺乏越南语的信息图表VQA基准，特别是需要跨图像推理的多图任务评估。

Method: 创建了ViInfographicVQA基准，包含6747个真实世界信息图表和20409个人工验证的问答对，涵盖多个领域。设计了两种评估设置：单图任务（传统设置）和多图任务（需要跨多个语义相关信息图表合成证据）。评估了多种最新的视觉语言模型在该基准上的表现。

Result: 评估显示现有模型在该基准上存在显著性能差距，最严重的错误出现在多图问题上，特别是涉及跨图像整合和非跨度推理的任务。这揭示了当前多模态模型在低资源语言环境下的局限性。

Conclusion: ViInfographicVQA为越南语信息图表VQA提供了基准结果，揭示了当前多模态模型在布局感知和跨图像推理方面的不足，鼓励未来探索布局感知和跨图像推理方法，特别是在低资源语言环境中。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [57] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

TL;DR: BokehDepth是一个两阶段框架，将散景合成与深度估计解耦，利用散焦作为无监督的几何线索，既提升散景渲染质量又改善单目深度估计精度。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能充分利用散景与单目深度估计之间的紧密联系：高质量的散景渲染依赖有噪声的深度图，而现代单目深度模型在纹理弱、距离远、几何模糊的区域表现不佳，而这些区域正是散焦线索最有信息量的地方。

Method: 采用两阶段框架：第一阶段基于预训练图像编辑骨干构建物理引导的可控散景生成器，从单张清晰输入产生无深度信息的散景堆栈；第二阶段通过轻量级散焦感知聚合模块，将散焦维度特征融合到现有单目深度编码器中，暴露稳定的深度敏感变化而不改变下游解码器。

Result: 在多个挑战性基准测试中，BokehDepth相比基于深度图的散景基线提高了视觉保真度，并持续提升了强单目深度基础模型的度量精度和鲁棒性。

Conclusion: 通过将散景合成与深度预测解耦，并将散焦作为辅助的无监督几何线索，BokehDepth框架同时改善了散景渲染质量和单目深度估计性能，充分利用了透镜成像几何的内在联系。

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [58] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: Endless World是一个实时无限3D一致视频生成框架，通过条件自回归训练和全局3D感知注意力机制，在单GPU上实现实时推理，生成长时稳定、视觉连贯的视频序列。


<details>
  <summary>Details</summary>
Motivation: 当前在流式场景中生成长时、连贯且具有稳定3D结构的视频序列仍然是一个重大挑战，特别是在保持3D一致性和物理合理性方面存在困难。

Method: 1. 条件自回归训练策略：将新生成内容与现有视频帧对齐，保持长程依赖关系的同时保持计算效率；2. 全局3D感知注意力：提供跨时间的连续几何指导；3. 3D注入机制：在整个扩展序列中强制执行物理合理性和几何一致性。

Result: 实验表明Endless World能够生成长时稳定、视觉连贯的视频，在视觉保真度和空间一致性方面达到或优于现有方法的性能，可在单GPU上实现实时推理。

Conclusion: Endless World通过创新的训练策略和3D一致性机制，成功解决了长时视频生成中的关键挑战，为无限、3D一致的视频生成提供了有效的实时解决方案。

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [59] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 提出Gaussian Photon Field (GPF)，将光子映射重构为可学习的连续辐射函数，通过3D高斯原语编码光子分布，实现多视角渲染的显著加速。


<details>
  <summary>Details</summary>
Motivation: 光子映射虽然能准确模拟复杂全局光照效果（如焦散和镜面-漫反射交互），但在渲染同一场景的多个视角时效率低下，因为每个视角都需要独立的光子追踪和随机核估计，导致大量冗余计算。

Method: 引入Gaussian Photon Field (GPF)，一种可学习的表示方法，将光子分布编码为各向异性的3D高斯原语（参数包括位置、旋转、尺度和光谱）。GPF从第一次SPPM迭代的物理追踪光子初始化，并使用最终辐射的多视角监督进行优化，将基于光子的光传输提炼为连续场。训练完成后，该场支持沿相机光线的可微辐射评估，无需重复光子追踪或迭代优化。

Result: 在具有复杂光传输（如焦散和镜面-漫反射交互）的场景上进行广泛实验，证明GPF在保持光子级精度的同时，将计算量减少了数量级。

Conclusion: GPF将基于光子渲染的物理严谨性与神经场景表示的效率统一起来，实现了高效的多视角渲染。

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [60] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

TL;DR: PeRL-VL提出了一种解耦框架，通过分别改进视觉感知和文本推理来解决RLVR训练视觉语言模型中的视觉提取不准确和逻辑不一致问题。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR已扩展到视觉语言模型以激发多模态推理，但现有方法仍存在两个主要问题：视觉提取不准确（遗漏或幻觉细节）和逻辑不一致的思维链，主要原因是可验证信号仅监督最终答案。

Method: PeRL-VL采用解耦框架：1）感知方面引入基于VLM的描述奖励，对模型自生成的图像描述进行忠实性和充分性评分；2）推理方面增加纯文本推理SFT阶段，在逻辑丰富的思维链数据上训练，独立于视觉增强逻辑一致性。

Result: 在多样化多模态基准测试中，PeRL-VL将平均Pass@1准确率从63.3%（基础Qwen2.5-VL-7B）提升到68.8%，优于标准RLVR、纯文本推理SFT和从GPT-4o的朴素多模态蒸馏。

Conclusion: PeRL-VL通过解耦视觉感知和文本推理的改进，有效解决了RLVR训练VLMs中的核心问题，显著提升了多模态推理性能。

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [61] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

TL;DR: 提出AdaptiveDetector框架，结合YOLOv11检测器和VLM验证器，通过自适应阈值调整和成本敏感强化学习，在恶劣内窥镜条件下显著提升息肉检测召回率，减少漏检风险。


<details>
  <summary>Details</summary>
Motivation: 现有息肉检测器在受控实验室数据集上训练，但在真实内窥镜环境中性能下降，因为存在光照变化、运动模糊和遮挡等恶劣成像条件。现有方法难以弥合实验室条件与临床实践之间的领域差距。

Method: 提出两阶段检测器-验证器框架：1）YOLOv11检测器在VLM指导下自适应调整每帧置信度阈值；2）VLM验证器使用Group Relative Policy Optimization（GRPO）进行微调，采用非对称成本敏感奖励函数，专门设计来减少漏检。构建合成测试平台，系统性地在干净数据集上添加临床常见恶劣条件。

Result: 在合成退化的CVC-ClinicDB和Kvasir-SEG图像上进行零样本评估，召回率比单独使用YOLO提高14-22个百分点，精度保持在基线以下0.7点到以上1.7点范围内。实现了临床对齐的开放世界息肉检测，显著减少假阴性。

Conclusion: 自适应阈值调整与成本敏感强化学习的结合，实现了临床对齐的息肉检测，大幅减少漏检风险，降低错过癌前息肉的可能性，从而改善患者预后。

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [62] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: 利用视频基础模型进行时空数据增强，通过视频扩散模型从单张图像生成真实的3D空间和时间变化，在低数据场景（如无人机图像）中提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法主要基于简单的几何变换或外观扰动，无法有效模拟真实的3D空间视角变化和场景动态变化。特别是在无人机图像等标注稀缺的低数据场景中，需要更丰富的数据增强方法来提升模型性能

Method: 使用现成的视频扩散模型，从给定的图像数据集生成具有真实3D空间和时间变化的视频片段。提供实用指南：1)选择合适的时空生成设置；2)将标注转移到合成帧；3)处理新暴露区域（disocclusion）问题

Result: 在COCO子集和无人机捕获数据集上的实验表明，时空数据增强能够沿着传统和先前生成方法未能充分代表的维度扩展数据分布，在数据稀缺情况下有效提升模型性能

Conclusion: 时空数据增强通过视频基础模型生成真实的3D空间和时间变化，为低数据场景提供了一种有效的性能提升手段，特别是在无人机图像等标注稀缺的应用中具有重要价值

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [63] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: Animus3D是一个文本驱动的3D动画框架，通过Motion Score Distillation技术从静态3D资产生成运动场，相比传统SDS方法能产生更显著、更流畅的运动效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用vanilla Score Distillation Sampling (SDS)从预训练文本到视频扩散模型中提取运动，导致动画运动幅度小或出现明显抖动，需要改进运动生成质量和视觉完整性。

Method: 提出Motion Score Distillation (MSD)替代SDS，使用LoRA增强的视频扩散模型定义静态源分布，结合反转噪声估计技术保持外观一致性；加入时间和空间正则化项减少几何失真；提出运动细化模块提升时间分辨率和细节。

Result: 实验表明Animus3D能成功从多样文本提示中动画化静态3D资产，相比最先进基线方法生成更显著、更详细的运动，同时保持高视觉完整性。

Conclusion: Animus3D通过创新的MSD方法、正则化技术和运动细化模块，有效解决了3D动画生成中的运动幅度不足和抖动问题，实现了高质量的文本驱动3D动画生成。

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [64] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种结合心肌解剖先验、结构感知特征编码和三维小波变换的冠状动脉分割框架，在ImageCAS数据集上取得了优于主流分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉CT血管成像的准确分割对于定量分析和临床决策至关重要，但由于血管细小、分支复杂、边界模糊以及心肌干扰等因素，可靠的分割仍然具有挑战性。

Method: 提出一个冠状动脉分割框架，整合心肌解剖先验、结构感知特征编码和三维小波-逆小波变换。编码阶段结合心肌先验和基于残差注意力的特征增强来强化冠状动脉结构表示；小波-逆小波的下采样和上采样实现联合空间频率建模并保持多尺度结构一致性；解码阶段通过多尺度特征融合模块整合语义和几何信息。

Result: 在公开ImageCAS数据集上使用3D重叠补丁策略（7:1:2划分训练/验证/测试）进行评估，获得Dice系数0.8082、敏感性0.7946、精确度0.8471、HD95为9.77mm，优于多个主流分割模型。消融研究进一步证实了各组成部分的互补贡献。

Conclusion: 该方法在复杂几何条件下实现了更稳定和一致的冠状动脉分割，为后续冠状动脉结构分析任务提供了可靠的分割结果。

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [65] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对多模态大语言模型的视频token剪枝方法，通过最大空间相邻视频token相似度(MSSAVT)指标和掩码剪枝策略，在减少计算开销的同时保持视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解在公共监控和AI眼镜等应用中至关重要，但多模态大语言模型处理视频时面临帧数过多导致的GPU内存占用高和计算延迟大的挑战。

Method: 提出基于最大空间相邻视频token相似度(MSSAVT)的冗余度量方法，结合掩码剪枝策略避免双向依赖问题，并整合现有时间冗余剪枝方法来消除视频模态的时间冗余。

Result: 在多个在线和离线视频理解基准测试中，该方法最多提升4%的准确率，同时剪枝延迟可忽略不计（小于1毫秒）。

Conclusion: 提出的token剪枝方法能有效减少多模态大语言模型处理视频时的计算开销，同时保持甚至提升视频理解性能，为实时视频理解应用提供了实用解决方案。

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [66] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

TL;DR: StegaVAR是一个将动作视频嵌入到普通封面视频中，并在隐写域直接进行视频动作识别的新框架，解决了隐私保护方法中低隐蔽性和时空特征破坏的问题。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方法存在两个主要问题：1) 低隐蔽性 - 产生视觉扭曲的视频在传输过程中容易引起攻击者注意；2) 时空破坏 - 破坏准确视频动作识别所需的时空特征。需要一种既能保护隐私又不影响动作识别性能的方法。

Method: 提出StegaVAR框架，将秘密动作视频嵌入到普通封面视频中，直接在隐写域进行视频动作识别。提出两种关键技术：1) 秘密时空促进(STeP) - 使用秘密视频指导隐写域中的时空特征提取；2) 跨波段差异注意力(CroDA) - 通过捕捉跨波段语义差异来抑制封面干扰。

Result: 实验表明StegaVAR在广泛使用的数据集上实现了优越的视频动作识别和隐私保护性能。该框架对多种隐写模型都有效。

Conclusion: StegaVAR首次实现了在隐写域直接进行视频动作识别，在保护隐私的同时保持了秘密视频的完整时空信息，解决了现有隐私保护方法的局限性。

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [67] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

TL;DR: 本文提出了一种用于线束检测的半自动化机器视觉系统，可验证线束位置、连接器极性和颜色序列的正确性，在工业应用中实现了100%检测精度和44%的时间节省。


<details>
  <summary>Details</summary>
Motivation: 现代电子制造服务行业中，线束检测过程仍然是劳动密集型且容易出错的过程，需要自动化解决方案来提高效率和准确性。

Method: 采用五个工业级CMOS摄像头集成到模块化机械框架中，使用基于HSV和RGB颜色域值比较的颜色序列分类器，用户可通过至少五个参考样本训练系统，训练文件可存储并重复用于类似线束类型。

Result: 系统在GPV Lanka Pvt. Ltd.部署后实现了100%的检测准确率，相比人工方法减少了44%的检测时间，并具备用户管理、可调照明、会话数据存储和安全登录等附加功能。

Conclusion: 该半自动化机器视觉系统在实际工业应用中证明了其可靠性和高效性，为线束检测提供了有效的自动化解决方案。

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [68] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

TL;DR: 提出了一种结合视觉增强大语言模型和先进Transformer架构的框架，通过整流流机制和双向标记化策略，实现高质量高分辨率图像合成和多模态数据理解，相比扩散方法提升25%图像清晰度并减少20%计算需求。


<details>
  <summary>Details</summary>
Motivation: 针对高分辨率图像合成和多模态数据解释中的挑战，需要开发能够统一处理文本、图像和视频等多种数据类型的框架，同时提高生成质量和计算效率。

Method: 采用整流流机制连接噪声和数据，使用双向标记化策略融合文本、图像和视频输入，嵌入时空特征，采用混合文本-图像序列建模方法，并优化噪声感知学习算法。

Result: 在基准数据集上评估显示，相比基于扩散的方法，图像分辨率清晰度提升25%，计算需求减少20%，模型展现出强大的可扩展性和适应性。

Conclusion: 该工作展示了视觉中心大语言模型在重新定义计算机视觉和多模态人工智能能力方面的潜力，为自主系统、创意内容生成和高级视频分析等应用提供了有力工具。

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [69] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型(VLM)的图像广告布局生成方法，通过分析背景图像内容来智能放置文本和logo，相比传统显著性映射方法能生成更高质量的广告布局。


<details>
  <summary>Details</summary>
Motivation: 传统广告布局技术主要依赖显著性映射来检测背景图像中的显著区域，但这种方法往往无法充分考虑图像的详细构图和语义内容，导致布局质量有限。

Method: 采用两阶段流程：1) VLM分析图像识别物体类型和空间关系，生成基于文本的"放置计划"；2) 将该计划渲染为HTML格式的最终布局代码。

Result: 通过定量和定性评估实验验证了方法的有效性，相比现有方法能生成明显更高质量的广告布局，特别是通过显式考虑背景图像内容。

Conclusion: 利用VLM理解图像语义内容的方法能显著提升广告布局生成质量，为图像广告设计提供了更智能的解决方案。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [70] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 本文提出了一种几何感知的场景一致图像生成方法，能够在保持参考场景物理环境的同时，根据文本描述的空间关系生成新实体，解决了现有方法在场景保持和提示遵循之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在几何感知的场景一致图像生成中存在平衡问题：要么高度忠实于场景但响应提示能力差，要么优先遵循提示但牺牲场景一致性。需要解决场景保持与提示遵循之间的权衡。

Method: 提出两个关键贡献：(1) 场景一致的数据构建流程，生成多样化的几何基础训练对；(2) 新颖的几何引导注意力损失，利用跨视图线索来正则化模型的空间推理能力。

Result: 在场景一致基准测试中，该方法在自动指标和人类偏好研究中都优于现有基线，实现了更好的场景对齐和文本-图像一致性，生成几何连贯且保持场景结构的多样化图像。

Conclusion: 该方法成功解决了场景一致图像生成中的权衡问题，能够生成既忠实于文本指令又保持底层场景结构的几何连贯图像，在场景对齐和文本-图像一致性方面表现优异。

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [71] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

TL;DR: X-Slim是一种训练免费的缓存加速器，通过跨时间步、结构和空间的三级缓存冗余利用，采用双阈值控制器实现"推送-抛光"过程，显著提升扩散模型推理速度


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量出色，但计算开销随步数、模型深度和序列长度线性增长。现有特征缓存方法存在权衡：激进的时间步重用能大幅加速但容易损害保真度，而块级或令牌级重用更安全但计算节省有限

Method: 提出X-Slim统一框架，利用时间步、结构（块）和空间（令牌）三个维度的可缓存冗余。采用双阈值控制器实现推送-抛光过程：首先将时间步级重用推至预警线，然后切换到轻量级块级和令牌级刷新来抛光剩余冗余，当跨过临界线时触发完整推理以重置累积误差。每个级别使用上下文感知指标决定何时何地缓存

Result: 在多样化任务中，X-Slim推进了速度-质量前沿。在FLUX.1-dev和HunyuanVideo上，分别将延迟降低4.97倍和3.52倍，感知损失最小。在DiT-XL/2上，达到3.13倍加速，FID比先前方法改善2.42

Conclusion: X-Slim通过统一利用跨时间步、结构和空间的三级缓存冗余，采用创新的双阈值推送-抛光机制，在保持生成质量的同时显著加速扩散模型推理，为扩散模型加速提供了有效的训练免费解决方案

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [72] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

TL;DR: D3D-VLP模型通过动态3D思维链和协同学习策略，解决了具身智能中端到端模型缺乏可解释性与模块化系统忽略跨组件依赖的问题，在多个导航和定位基准上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能中的关键困境：端到端模型缺乏可解释性和显式3D推理能力，而模块化系统则忽略了不同组件之间的相互依赖和协同效应。

Method: 提出动态3D视觉-语言-规划模型(D3D-VLP)，包含两个关键创新：1) 动态3D思维链(3D CoT)，将规划、定位、导航和问答统一到单个3D-VLM和CoT流程中；2) 碎片化监督协同学习策略(SLFS)，使用掩码自回归损失从大规模部分标注的混合数据中学习，让不同CoT组件相互增强和隐式监督。

Result: 在多个基准测试中取得最先进结果：视觉语言导航(R2R-CE, REVERIE-CE, NavRAG-CE)、目标导航(HM3D-OVON)和任务导向的顺序定位与导航(SG3D)。真实世界移动操作实验进一步验证了有效性。

Conclusion: D3D-VLP成功弥合了端到端模型与模块化系统之间的差距，通过统一的3D思维链框架和协同学习策略，实现了可解释的3D推理和跨组件协同，为具身智能提供了有效的解决方案。

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [73] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: DMLR提出了一种动态多模态潜在推理框架，通过置信度引导的潜在策略梯度优化来精炼潜在思考标记，实现视觉-文本的动态交错推理，显著提升多模态推理性能并保持高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型虽然通过思维链机制增强了跨模态理解，但仍依赖显式的逐步推理、感知-推理交互不稳定且计算开销大。受人类认知启发，作者认为思考不是线性的，而是推理和感知在思维中动态交错的过程。

Method: 提出DMLR框架：1）使用置信度引导的潜在策略梯度优化来精炼潜在思考标记进行深度推理；2）引入动态视觉注入策略，在每个潜在思考标记处检索最相关的视觉特征并更新最佳视觉补丁集；3）将更新的补丁注入潜在思考标记，实现动态视觉-文本交错。

Result: 在七个多模态推理基准测试和多种模型架构上的实验表明，DMLR显著提升了推理和感知性能，同时保持了较高的推理效率。

Conclusion: DMLR通过模拟人类认知的动态交错特性，提供了一种高效的多模态推理框架，克服了现有方法依赖显式逐步推理、交互不稳定和计算开销大的问题。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [74] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

TL;DR: CARe方法通过裁剪和对齐操作解决大视场差异的跨模态眼底图像配准问题，相比现有方法在小视场差异假设下的失败表现，CARe在新测试集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态眼底图像配准方法假设视场差异较小，但在实际临床中常遇到大视场差异的情况，现有方法直接应用会失败，需要解决这一更具挑战性的场景。

Method: 提出CARe方法：1) 裁剪操作：利用视网膜生理结构从大视场目标图像中裁剪出与源图像视场大致对齐的子图像；2) 对齐模块：采用双拟合方法，先使用RANSAC算法，再进行多项式坐标拟合，改进空间变换。

Result: 在包含60对OCTA-wfCFP图像的新测试集上进行广泛实验，验证了CARe方法在大视场差异跨模态眼底图像配准中的可行性。

Conclusion: CARe是一种简单而有效的方法，通过裁剪和对齐操作解决了大视场差异的跨模态眼底图像配准问题，为这一更具挑战性的临床场景提供了可行解决方案。

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [75] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: CogDoc提出了一种从粗到细的认知框架，通过"快速阅读"和"专注思考"两阶段处理文档，解决了长文档处理与细粒度细节捕捉之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前文档推理范式存在一个基本权衡：可扩展性（处理长上下文文档）与保真度（捕捉细粒度多模态细节）之间的矛盾。需要一种能够同时处理长文档并保留精细细节的方法。

Method: 提出CogDoc统一框架，模仿人类认知过程：1）低分辨率"快速阅读"阶段进行可扩展信息定位；2）高分辨率"专注思考"阶段进行深度推理。研究了后训练策略，发现直接强化学习优于SFT初始化的RL方法。

Result: 7B参数模型在其参数类别中达到最先进性能，在具有挑战性的视觉丰富文档基准测试中显著超越了更大的专有模型（如GPT-4o）。直接RL避免了SFT中观察到的"策略冲突"。

Conclusion: CogDoc框架通过模仿人类认知过程，有效解决了文档推理中的可扩展性与保真度权衡问题，为文档理解提供了新的解决方案。

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [76] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

TL;DR: InteracTalker是一个统一框架，能够同时生成语音驱动的手势和物体交互动作，解决了现有方法只能独立处理这两个任务的问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法分别处理语音驱动手势和物体交互，缺乏集成数据集和统一框架，限制了真实世界应用的适用性。

Method: 采用多阶段训练学习统一的运动、语音和提示嵌入空间；构建丰富的人-物交互数据集；使用广义运动适应模块进行独立训练；提出自适应融合策略处理异质条件信号不平衡问题。

Result: InteracTalker在语音手势生成和物体交互合成方面均优于先前方法，超越了专注于手势的扩散方法，产生高度真实、物体感知的全身运动。

Conclusion: 该框架成功统一了先前分离的任务，提供了增强的真实感、灵活性和控制能力，为交互式数字体验提供了更自然的解决方案。

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [77] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出CAL框架解决开放世界深度伪造溯源问题，通过置信度感知的非对称学习平衡已知和未知伪造类型的识别，并引入动态原型剪枝自动估计未知伪造类型数量。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界深度伪造溯源方法存在两个关键限制：1）置信度偏斜导致对未知伪造的伪标签不可靠，造成训练偏差；2）不现实地假设未知伪造类型的数量已知。

Method: 提出置信度感知非对称学习框架，包含置信度感知一致性正则化和非对称置信度增强两个组件，以及动态原型剪枝策略自动估计未知伪造类型数量。

Result: 在标准开放世界深度伪造溯源基准和新扩展的包含高级操作的基准上，CAL方法始终优于先前方法，在已知和未知伪造溯源上都达到了新的最先进性能。

Conclusion: CAL框架通过平衡置信度和自动估计未知类型数量，显著提升了开放世界深度伪造溯源的性能，增强了方法在现实场景中的可扩展性。

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [78] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: Scone是一个统一的理解-生成方法，通过两阶段训练方案（先学习组合，再增强区分）来同时处理多主体图像生成中的组合和区分问题，并提出了SconeEval评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前基于主题的图像生成方法已从单主体发展到多主体组合，但忽视了区分能力——即当输入包含多个候选主体时，能够识别并生成正确主体的能力。这一限制影响了在复杂真实视觉场景中的有效性。

Method: 提出Scone方法，将理解专家作为语义桥梁，传递语义信息并指导生成专家在最小化干扰的同时保持主体身份。采用两阶段训练方案：第一阶段学习组合，第二阶段通过语义对齐和基于注意力的掩码增强区分能力。

Result: 实验表明，Scone在两个基准测试中，在组合和区分任务上都优于现有的开源模型。作者还发布了SconeEval评估基准，用于评估不同场景下的组合和区分能力。

Conclusion: Scone是一个统一的理解-生成方法，通过集成组合和区分能力，在多主体图像生成中取得了显著改进，为复杂真实视觉场景中的主题驱动图像生成提供了有效解决方案。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [79] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

TL;DR: β-CLIP通过多粒度文本条件对比学习框架，实现从完整描述到句子、短语的层次化视觉-文本对齐，显著提升细粒度视觉语言任务性能


<details>
  <summary>Details</summary>
Motivation: CLIP虽然在零样本图像-文本检索上表现良好，但在细粒度任务上表现不佳，即使使用详细描述进行微调也难以达到理想效果。需要一种能够实现多层次语义对齐的方法来提升细粒度视觉语言对应关系。

Method: 提出β-CLIP多粒度文本条件对比学习框架：1) 使用跨注意力动态池化图像块，为每个粒度级别生成上下文视觉嵌入；2) 引入β-上下文对比对齐损失(β-CAL)，参数化严格查询匹配与宽松图像内上下文之间的权衡，支持软交叉熵和硬二元交叉熵两种形式。

Result: 在Urban1K数据集上达到91.8% T2I和92.3% I2T的R@1准确率，在FG-OVD(Hard)上达到30.9%，在没有使用硬负样本训练的方法中达到最先进水平。

Conclusion: β-CLIP建立了鲁棒、自适应的密集视觉语言对应基准，通过层次化多粒度对齐显著提升了细粒度视觉语言任务性能，代码和模型已开源。

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [80] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: GenieDrive是一个用于物理感知驾驶视频生成的新框架，通过生成4D占据作为物理基础，使用VAE压缩为三平面表示，并引入互控注意力来建模控制对占据演化的影响，显著提升了预测精度和视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖单一扩散模型直接将驾驶动作映射到视频，这使得学习困难且产生物理不一致的输出。需要开发能够生成物理感知驾驶视频的框架，以支持驾驶规划、分布外数据合成和闭环评估。

Method: 1) 首先生成包含丰富物理信息的4D占据作为视频生成的物理基础；2) 提出VAE将高分辨率占据编码为潜在三平面表示，将潜在尺寸减少到先前方法的58%；3) 引入互控注意力(MCA)准确建模控制对占据演化的影响；4) 端到端联合训练VAE和预测模块；5) 在视频生成模型中引入归一化多视图注意力，基于4D占据引导生成多视图驾驶视频。

Result: 1) 预测mIoU提升7.2%，推理速度达41 FPS，仅使用3.47M参数；2) 视频质量显著改善，FVD减少20.7%；3) 实现了高度可控、多视图一致且物理感知的驾驶视频生成。

Conclusion: GenieDrive通过4D占据生成、高效压缩和互控注意力等创新设计，成功解决了现有方法在物理一致性和学习难度方面的挑战，为物理感知驾驶视频生成提供了有效的解决方案。

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [81] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: FysicsWorld是首个统一的全模态基准测试，支持图像、视频、音频和文本之间的双向输入输出，包含16个主要任务和3268个样本，用于评估多模态模型的理解、生成和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基准测试存在模态覆盖不全、仅限于文本输出、模态间相互依赖性和互补性弱等问题。需要建立一个全面的全模态评估框架来推动下一代多模态架构的发展。

Method: 提出了FysicsWorld基准测试，包含16个主要任务和3268个精心筛选的样本，覆盖图像、视频、音频和文本四种模态。采用跨模态互补性筛选（CMCS）策略和系统化数据构建框架，生成用于口语交互和融合依赖跨模态推理的全模态数据。

Result: 对30多个最先进的基线模型（包括MLLMs、模态特定模型、统一理解-生成模型和全模态语言模型）进行了全面评估，揭示了这些模型在理解、生成和推理方面的性能差异和局限性。

Conclusion: FysicsWorld为评估和推进下一代全模态架构建立了统一的基础和强大的基线，填补了当前基准测试的空白，支持任意模态间的双向评估。

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [82] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: CoRe3D提出统一3D理解与生成推理框架，通过语义和空间抽象联合操作，让语言推断的高层意图直接指导低级3D内容形成


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型中的显式推理机制已被证明能提升可靠性、可解释性和跨模态对齐，但在3D领域应用不足。现有方法在语言和视觉任务中有效，但扩展到3D仍不成熟

Method: 引入空间接地推理表示，将3D潜在空间分解为局部化区域，使模型能以组合和程序化方式推理几何。通过紧密耦合语义链式思维推理与结构化空间推理

Result: CoRe3D生成的3D输出表现出强大的局部一致性和与语言描述的忠实对齐

Conclusion: 该框架为3D理解和生成任务提供了有效的推理机制，通过联合语义和空间抽象实现了高层意图到低级3D内容的直接指导

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [83] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DrivePI是一个空间感知的4D多模态大语言模型，作为统一的视觉-语言-动作框架，在自动驾驶中实现3D感知、预测和规划的并行处理。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在多个领域表现出强大能力，但在自动驾驶中生成细粒度3D感知和预测输出的应用仍未被充分探索。现有方法在空间理解、3D感知、预测和规划的统一处理方面存在不足。

Method: 提出DrivePI框架，集成点云、多视角图像和语言指令，通过端到端优化并行处理空间理解、3D占用、占用流和规划。开发数据引擎生成文本-占用和文本-流问答对用于4D空间理解。

Result: 仅使用0.5B参数的Qwen2.5作为骨干模型，DrivePI在多项任务上超越现有VLA模型和专用VA模型：在nuScenes-QA上比OpenDriveVLA-7B提升2.5%平均准确率；碰撞率比ORION降低70%；在3D占用、占用流和规划任务上均显著优于专用模型。

Conclusion: DrivePI证明了统一的4D MLLM框架在自动驾驶中的有效性，能够同时处理多种任务并在性能上超越专门设计的模型，为自动驾驶感知和决策提供了新的解决方案。

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [84] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: Lemon是一个统一的多模态Transformer架构，通过将3D点云补丁和语言标记作为单一序列联合处理，解决了现有3D理解模型的碎片化架构、训练不稳定和可扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在扩展到3D理解时面临独特挑战：点云数据稀疏不规则，现有模型依赖碎片化架构（模态特定编码器），训练流程不稳定且可扩展性差。

Method: 1. 统一Transformer架构，将3D点云补丁和语言标记作为单一序列联合处理；2. 结构化补丁化和标记化方案，保留空间上下文；3. 三阶段训练课程，从对象级识别逐步构建到场景级空间推理能力。

Result: Lemon在全面的3D理解和推理任务上建立了新的最先进性能，包括对象识别、描述和3D场景中的空间推理，同时随着模型规模和训练数据的增加展现出强大的可扩展性。

Conclusion: Lemon提供了一个统一的基础架构，通过早期空间-语言融合、消除冗余编码器、提高参数效率和支持更有效的模型扩展，为推进现实世界应用中的3D空间智能奠定了基础。

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [85] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

TL;DR: SAVE模型通过Schrodinger Bridge实现音频和视频的联合编辑，在SAVEBench数据集上训练，能够同步移除目标对象并保持内容一致性。


<details>
  <summary>Details</summary>
Motivation: 音频和视觉内容的联合编辑对于精确可控的内容创作至关重要，但面临配对数据不足和模态异质性的挑战。

Method: 提出SAVEBench配对视听数据集，并开发SAVE模型：基于流匹配的端到端模型，使用Schrodinger Bridge直接从源到目标视听混合进行传输。

Result: SAVE模型能够有效移除音频和视频中的目标对象，同时保持剩余内容，在时间同步和视听语义对应方面优于音频和视频编辑器的组合方法。

Conclusion: SAVE模型通过联合编辑框架解决了视听内容同步编辑的挑战，在保持内容一致性和时间同步方面表现出色。

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [86] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

TL;DR: 提出了一种用于汽车传感器融合的端到端跨级别Transformer融合方法，将抽象的对象列表信息与原始相机图像结合进行3D目标检测，在nuScenes数据集上显著优于视觉基线。


<details>
  <summary>Details</summary>
Motivation: 在汽车传感器融合系统中，智能传感器和V2X模块通常只提供处理后的对象列表而非原始数据。现有方法通常分别处理原始数据后在对象级别融合，缺乏将高度抽象的对象列表信息与原始传感器数据直接整合的方法。

Method: 1) 提出端到端跨级别融合概念，使用Transformer将对象列表信息作为去噪查询与可学习查询一起传播；2) 引入可变形高斯掩码，基于对象列表的位置和尺寸先验指导注意力机制；3) 提出从真实边界框生成伪对象列表的方法，模拟状态噪声和误检漏检。

Result: 在nuScenes数据集上，该方法相比基于视觉的基线模型取得了显著的性能提升。同时展示了该方法在不同噪声水平的模拟对象列表和真实检测器上的良好泛化能力。

Conclusion: 这是首个进行跨级别融合的工作，成功地将抽象对象列表信息与原始图像数据融合，为汽车传感器融合系统提供了一种有效的端到端解决方案，能够处理智能传感器和V2X模块提供的处理后的对象列表数据。

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [87] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于预测样本分配（PSA）的语义连贯分布外检测框架，通过双阈值三元样本分配策略提高ID/OOD样本集的纯度，并引入概念对比表示学习损失来增强ID/OOD样本在表示空间中的区分度。


<details>
  <summary>Details</summary>
Motivation: 当前SCOOD方法主要采用基于聚类的ID样本过滤策略从无标签数据中选择干净ID样本，并将剩余样本作为辅助OOD数据，这种方法不可避免地引入了大量噪声样本，影响训练效果。

Method: 提出PSA框架：1）基于预测能量分数的双阈值三元样本分配策略，将不确定的无标签数据分配到丢弃样本集中，提高ID/OOD样本集的纯度；2）概念对比表示学习损失，扩大ID和OOD样本在表示空间中的距离；3）重训练策略帮助模型充分拟合选定的辅助ID/OOD样本。

Result: 在两个标准SCOOD基准测试上的实验表明，该方法显著优于当前最先进的方法。

Conclusion: 提出的PSA框架通过改进样本分配策略和表示学习方法，有效解决了SCOOD任务中噪声样本问题，显著提升了分布外检测性能。

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [88] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 提出一种新的广义类别发现方法，包含损失锐度惩罚和动态锚点选择两个模块，通过减少伪标签噪声提升聚类性能


<details>
  <summary>Details</summary>
Motivation: 现有基于参数化分类的GCD方法使用DINO式伪标签策略，但大型预训练模型对特定视觉模式有偏好，导致对未标记数据编码虚假相关性并产生噪声伪标签

Method: 提出包含两个模块的新方法：1) 损失锐度惩罚(LSP)：通过最小化模型的最坏情况损失锐度，增强模型参数对小扰动的鲁棒性，抑制琐碎特征编码；2) 动态锚点选择(DAS)：基于KNN密度和类别概率为未知类选择代表性样本并分配硬伪标签，缓解已知类和未知类之间的置信度差异

Result: 广泛实验表明，该方法能有效减轻伪标签噪声，在多个GCD基准测试中达到最先进的结果

Conclusion: 提出的LSP和DAS模块能够有效解决伪标签噪声问题，提升广义类别发现的聚类准确性

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [89] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: 本文提出CAMA框架，通过内容自适应运动对齐提升端到端视频压缩性能，包括两阶段流引导可变形扭曲、多参考质量感知策略和训练免费模块，在标准数据集上相比基线模型实现24.95% BD-rate节省。


<details>
  <summary>Details</summary>
Motivation: 当前端到端视频压缩框架缺乏内容特定适应性，导致压缩性能不理想。需要开发能够根据内容特性自适应调整编码策略的方法来提升压缩效率。

Method: 1. 两阶段流引导可变形扭曲机制：通过粗到细的偏移预测和掩码调制来精化运动补偿，实现精确特征对齐
2. 多参考质量感知策略：基于参考质量调整失真权重，并应用于分层训练以减少误差传播
3. 训练免费模块：根据运动幅度和分辨率下采样帧以获得平滑运动估计

Result: 在标准测试数据集上，CAMA框架相比最先进的神经视频压缩模型取得显著改进，相比基线模型DCVC-TCM实现24.95% BD-rate（PSNR）节省，同时优于复现的DCVC-DC和传统编解码器HM-16.25。

Conclusion: 提出的内容自适应运动对齐框架通过适应不同内容特性有效提升了端到端视频压缩性能，证明了内容自适应策略在视频压缩中的重要性。

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [90] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

TL;DR: SCAdapter是一种基于扩散模型的新型风格迁移方法，通过CLIP图像空间有效分离和整合内容与风格特征，实现更逼真的风格迁移，同时推理速度比其他扩散方法快2倍以上。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在风格迁移中面临两个主要问题：1）难以实现照片级逼真的迁移，常产生类似绘画的效果或遗漏细节风格元素；2）现有方法无法充分处理原始内容风格和风格参考内容特征的不必要影响。

Method: SCAdapter利用CLIP图像空间有效分离和整合内容与风格特征，包含三个核心组件：1）可控风格自适应实例归一化（CSAdaIN）用于精确的多风格混合；2）KVS注入用于目标风格集成；3）风格迁移一致性目标保持过程连贯性。

Result: SCAdapter在传统和基于扩散的基线方法中都显著优于现有最先进方法，通过消除DDIM反转和推理阶段优化，推理速度比其他基于扩散的方法至少快2倍。

Conclusion: SCAdapter提出了一种更有效和高效的风格迁移方法，通过系统提取纯内容和风格元素，结合创新的组件设计，实现了更逼真的风格迁移效果，同时大幅提升了推理效率。

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [91] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

TL;DR: VLCache是一个多模态缓存重用框架，通过复用KV缓存和编码器缓存来避免重复计算，在保持准确性的同时大幅提升推理速度


<details>
  <summary>Details</summary>
Motivation: 当相同的多模态输入重复出现时，现有的方法需要昂贵的重新计算，这影响了推理效率。作者旨在通过缓存重用机制来消除这种重复计算的开销

Method: 1) 正式识别累积重用误差效应并最小化非前缀缓存重用误差；2) 分析模型层的重要性差异，提出动态、层感知的重新计算策略来平衡准确性和效率

Result: VLCache在保持与完全重新计算相当的准确性的同时，仅需计算2-5%的token，实现了1.2x-16x的TTFT加速。该框架已集成到SGLang中，在实际部署中显著提升了推理速度

Conclusion: VLCache通过有效的缓存重用机制，在多模态推理中实现了显著的效率提升，为实际部署提供了实用的加速解决方案

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [92] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 论文发现AIGI检测器存在"先受益后冲突"困境：随着数据源多样性增加，检测性能先提升后下降。提出了GAPL框架，通过原型学习和两阶段训练解决数据异质性和模型瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前通用AIGI检测器通常通过聚合多个生成器的数据来提高泛化能力，但本文发现存在"先受益后冲突"的悖论现象：随着数据源多样性扩大，检测器性能先停滞后下降。需要解决数据层面的异质性和模型层面的瓶颈问题。

Method: 提出了Generator-Aware Prototype Learning (GAPL)框架：1) 学习一组紧凑的典型伪造原型，创建统一、低方差的特征空间以应对数据异质性；2) 采用两阶段训练方案，结合Low-Rank Adaptation增强判别能力，同时保留预训练知识。

Result: 通过大量实验证明，GAPL在GAN和基于扩散的生成器上均实现了最先进的性能，展现出优越的检测准确率，能够建立更鲁棒和可泛化的决策边界。

Conclusion: GAPL框架有效解决了AIGI检测中的"先受益后冲突"困境，通过原型学习和自适应训练机制克服了数据异质性和模型瓶颈，为通用AI生成图像检测提供了更鲁棒的解决方案。

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [93] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: 提出一种针对CLIP模型的新型对抗性微调方法，通过狄利克雷分布重新参数化输出，同时优化预测准确性和不确定性校准，在保持零样本泛化能力的同时提升对抗鲁棒性


<details>
  <summary>Details</summary>
Motivation: CLIP虽然具有强大的零样本分类能力，但对对抗攻击高度脆弱。现有对抗微调方法主要关注干净样本和对抗样本之间的预测对数匹配，忽视了不确定性校准，可能损害零样本泛化能力。研究发现对抗扰动不仅降低准确性，还会抑制不确定性，导致严重的校准错误和不可靠的过度自信

Method: 通过将CLIP输出重新参数化为狄利克雷分布的浓度参数，提出统一表示方法捕捉相对语义结构和预测置信度大小。设计新型对抗微调目标，在扰动下整体对齐这些分布，超越单一对数锚定，恢复校准的不确定性

Result: 在多个零样本分类基准测试中，该方法有效恢复了校准的不确定性，实现了有竞争力的对抗鲁棒性，同时保持了干净的准确性

Conclusion: 该方法成功解决了CLIP在对抗环境中的可靠性差距问题，通过同时优化准确性和不确定性校准，为构建更可靠的零样本视觉语言模型提供了新思路

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [94] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 提出基于光场图像的物体跟踪方法LiFT-6DoF，无需预训练模型，能处理复杂视觉行为（如反射），通过视觉基础模型提取特征并转换为视图相关的高斯溅射表示，支持可微分渲染和位姿优化。


<details>
  <summary>Details</summary>
Motivation: 现有高性能物体跟踪方法通常依赖预捕获的物体视图构建显式参考模型，这限制了它们只能处理已知物体集合，且在处理视觉复杂外观（如反射）时性能下降。需要一种不依赖预训练模型且能处理复杂视觉行为的通用物体跟踪方法。

Method: 使用光场图像作为输入，通过视觉基础模型提取语义和几何特征，将其转换为视图相关的高斯溅射表示。这种表示作为统一的物体表征，支持可微分渲染和位姿优化。还创建了包含挑战性反射物体的光场物体跟踪数据集。

Result: 实验表明，在包含反射物体的困难案例中，该方法与最先进的基于模型的跟踪器具有竞争力。创建的数据集包含精确的地面真值位姿，代码和数据已开源。

Conclusion: 提出的LiFT-6DoF方法为机器人系统中的通用物体跟踪铺平了道路，能够处理复杂视觉行为且不依赖预训练模型，在挑战性场景中表现优异。

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [95] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

TL;DR: TWLR是一个两阶段可解释性糖尿病视网膜病变评估框架，通过视觉语言模型整合眼科知识进行分级和病变分类，然后通过弱监督语义分割的迭代严重性回归实现病变定位和疾病到健康转换的可视化。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析需要高质量专家标注，但获取像素级标签（特别是眼底图像）成本高、耗时长。同时，深度学习在医学影像中缺乏可解释性，限制了临床采用。

Method: 提出TWLR两阶段框架：第一阶段使用视觉语言模型整合眼科领域知识，联合执行DR分级和病变分类；第二阶段引入基于弱监督语义分割的迭代严重性回归框架，通过迭代精炼生成病变显著性图，指导渐进修复机制系统消除病理特征。

Result: 在FGADR、DDR和私有数据集上的实验结果表明，TWLR在DR分类和病变分割方面都取得了有竞争力的性能。

Conclusion: TWLR为自动化视网膜图像分析提供了一个更可解释、标注效率更高的解决方案，实现了无需像素级监督的准确病变定位，并提供了疾病到健康转换的可视化解释。

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [96] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

TL;DR: 该论文提出了"下一场景预测"新任务，通过统一框架结合Qwen-VL理解和LTX合成，在自建大规模数据集上进行三阶段训练，实现了视频模型的时序因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型主要关注文本到视频生成等传统任务，对时序推理潜力探索不足。为了填补这一空白，作者提出了下一场景预测任务，推动统一视频模型向时序和因果推理方向发展。

Method: 提出统一框架：Qwen-VL用于理解，LTX用于合成，通过潜在查询嵌入和连接器模块桥接。采用三阶段训练：文本到视频预训练、监督微调、以及使用因果一致性奖励的强化学习（通过GRPO）。构建了大规模NSP数据集。

Result: 实验表明，该模型在基准测试中达到了最先进的性能，提升了通用多模态系统预测未来事件的能力。

Conclusion: 通过引入下一场景预测任务和相应框架，成功推进了统一视频模型的时序和因果推理能力，为多模态系统预测未来场景提供了有效解决方案。

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [97] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 该研究首次全面评估了空间泛化技术，针对室内环境使用FMCW MIMO雷达进行人员计数，系统比较了多种方法，发现基于幅度的预处理和迁移学习能显著提升跨环境性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在射频传感中的实际部署需要解决空间泛化问题，特别是在不同室内环境中保持人员计数的准确性。目前缺乏对空间泛化技术的系统性评估。

Method: 使用FMCW MIMO雷达进行人员计数，系统评估了幅度统计预处理（Sigmoid加权和阈值归零）、频域滤波、基于自编码器的背景抑制、数据增强策略和迁移学习等多种方法。

Result: Sigmoid幅度加权在跨环境性能上表现最佳，相比基线方法分别降低RMSE 50.1%和MAE 55.2%。数据增强提供额外8.8%的MAE改进。迁移学习在大空间变化时至关重要，使用540个目标域样本可降低RMSE 82.1%和MAE 91.3%。

Conclusion: 研究表明，将深度学习模型与基于幅度的预处理和高效迁移学习相结合，是开发能够在空间变化下保持鲁棒准确性的雷达传感系统的实用方向。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [98] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

TL;DR: SneakPeek是一个基于扩散的自回归框架，通过预测性因果适应、未来引导自强制和多提示条件化，生成精确的逐步教学视频，解决了现有模型在长序列中时间一致性和可控性的问题。


<details>
  <summary>Details</summary>
Motivation: 教学视频生成在内容创作、教育和人机交互中具有广泛应用前景，但现有视频扩散模型在长序列多步骤动作中难以保持时间一致性和可控性，需要新的方法来解决这些问题。

Method: 提出了SneakPeek框架，包含三个关键创新：1) 预测性因果适应，通过因果模型学习下一帧预测和未来关键帧预测；2) 未来引导自强制与双区域KV缓存方案，解决推理时的曝光偏差问题；3) 多提示条件化，提供对多步骤指令的细粒度程序控制。

Result: 实验结果表明，该方法能够生成时间一致、语义忠实且准确遵循复杂多步骤任务描述的教学视频，有效缓解了时间漂移问题，保持了运动一致性。

Conclusion: SneakPeek通过创新的预测性因果适应、未来引导自强制和多提示条件化技术，实现了高质量的教学视频生成，为交互式视频生成提供了新途径，其中未来提示更新能够动态影响正在进行的流式视频生成。

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [99] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: Motus是一个统一的潜在动作世界模型，通过混合Transformer架构集成理解、视频生成和动作三个专家模块，利用光流学习潜在动作，实现多模态生成能力的统一，在仿真和真实场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法将理解、世界建模和控制分离为孤立模型，这种碎片化阻碍了多模态生成能力的统一，也妨碍了从大规模异构数据中学习。需要构建一个统一的系统来整合这些功能。

Method: 提出Motus统一潜在动作世界模型，采用混合Transformer架构集成三个专家模块（理解、视频生成、动作），使用UniDiffuser风格调度器在不同建模模式间灵活切换。利用光流学习潜在动作，采用三阶段训练流程和六层数据金字塔，提取像素级"delta动作"，实现大规模动作预训练。

Result: 在仿真环境中，Motus比X-VLA提升15%，比Pi0.5提升45%；在真实场景中提升11-48%。实验表明统一建模所有功能和先验显著有益于下游机器人任务。

Conclusion: Motus通过统一的潜在动作世界模型成功整合了多模态生成能力，证明了统一建模方法在机器人任务中的优越性，为构建通用具身智能体提供了有效框架。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [100] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: Bi-Erasing框架通过双向图像引导的概念擦除，同时进行概念抑制和安全增强，在概念移除效果和生成质量之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法通常采用单向擦除策略（要么抑制目标概念，要么强化安全替代），难以在概念移除和生成质量之间取得平衡。需要一种更平衡的方法来同时处理有害概念抑制和安全替代增强。

Method: 提出双向图像引导概念擦除框架，基于文本提示和对应图像的联合表示，引入两个解耦的图像分支：负分支负责抑制有害语义，正分支为安全替代提供视觉引导。通过联合优化这两个互补方向，并使用基于掩码的过滤来防止无关内容干扰。

Result: 在广泛的实验评估中，Bi-Erasing在平衡概念移除效果和视觉保真度方面优于基线方法。

Conclusion: Bi-Erasing框架通过同时进行概念抑制和安全增强的双向策略，在概念擦除效果和生成可用性之间实现了更好的平衡，为文本到图像模型的安全改进提供了有效解决方案。

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [101] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: GTR-Turbo是一种高效的多模态智能体强化学习方法，通过合并训练过程中的检查点权重创建"免费"教师模型，无需昂贵的外部教师模型，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的多模态智能体多轮强化学习方法依赖昂贵且特权的外部教师模型提供步级反馈，这限制了方法的实用性和可复现性。

Method: GTR-Turbo在持续强化学习训练过程中合并检查点的权重，使用这个合并模型作为"免费"教师，通过监督微调或软对数蒸馏指导后续强化学习。

Result: 在多种视觉智能体任务中，GTR-Turbo将基线模型的准确率提升了10-30%，同时相对于GTR方法减少了50%的训练时间和60%的计算成本。

Conclusion: GTR-Turbo消除了对特权视觉语言模型的依赖，缓解了先前工作中的"熵崩溃"问题，保持了训练稳定性，是一种高效实用的多模态智能体强化学习方法。

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [102] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 提出一个用于医学视觉语言模型持续学习的综合框架，通过检索增强生成和多层知识蒸馏解决模态间域差距和细粒度特征保留的难题。


<details>
  <summary>Details</summary>
Motivation: 多模态生物医学视觉语言模型在持续学习中面临核心困境：如何在保持细粒度模态内特征的同时，跨越不同模态间的显著域差距。

Method: 1. 基于1800万PubMed科学论文构建多模态医学检索数据库；2. 首次将检索增强生成引入持续学习，采用多模态多层RAG系统提供实时指导；3. 提出动态知识蒸馏框架，动态调节参数空间重要性、知识粒度和参考数据集分布。

Result: 在设计的医学通用任务增量学习基准上，该方法在所有指标上均达到最先进的性能表现。

Conclusion: 提出的综合框架有效解决了医学视觉语言模型在持续学习中的核心挑战，通过检索增强和动态知识蒸馏实现了对显著域偏移的适应、细粒度特征的保留以及新复杂医学任务的实时学习。

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [103] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: 提出多样性正则化器DiRe，通过余弦相似度和欧氏距离减少数据集压缩中的冗余，提高合成数据集的多样性


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法合成的数据集存在显著冗余，需要减少冗余并提高合成数据集的多样性

Method: 提出直观的多样性正则化器DiRe，结合余弦相似度和欧氏距离，可即插即用地应用于各种最先进的压缩方法

Result: 通过广泛实验证明，添加DiRe正则化器能改进最先进压缩方法在CIFAR-10到ImageNet-1K等基准数据集上的泛化性和多样性指标

Conclusion: DiRe正则化器能有效减少数据集压缩中的冗余，提高合成数据集的多样性，可广泛应用于现有压缩方法

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [104] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: UniVCD是一种基于冻结SAM2和CLIP的无监督开放词汇变化检测方法，无需标注数据或配对变化图像，通过轻量级特征对齐模块实现高分辨率语义感知的变化估计。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法主要依赖监督学习，性能高度依赖数据集且标注成本高，通常只能检测预定义类别，对不同场景泛化能力差。随着视觉基础模型（如SAM2和CLIP）的兴起，为放松这些限制提供了新机会。

Method: 提出UniVCD方法：1）基于冻结的SAM2和CLIP构建无监督开放词汇变化检测框架；2）引入轻量级特征对齐模块，桥接SAM2的空间细节表示和CLIP的语义先验；3）采用简化的后处理流程抑制噪声和伪变化，提高边界清晰对象的检测精度。

Result: 在多个公开BCD和SCD基准测试中，UniVCD表现出稳定强大的性能，在F1和IoU等关键指标上匹配或超越了现有的开放词汇变化检测方法。

Conclusion: 基于冻结视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇变化检测的实用有效范式。代码和预训练模型将在GitHub上发布。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [105] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: ADHint是一种结合SFT和RL优势的新方法，通过自适应难度调度提示比例和优势估计，在探索和模仿之间取得更好平衡，提升推理能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的强化学习方法通常忽略难度因素，在调度提示比例和估计相对优势时存在问题，导致学习不稳定和过度模仿离策略提示。需要一种更好的方法来平衡探索和模仿。

Method: 提出ADHint方法，包含三个关键技术：1) 自适应提示与样本难度先验，根据样本难度调度适当提示比例；2) 一致性梯度调制和选择性掩码提示保留，调制提示内的令牌级梯度；3) 基于推出难度后验的优势估计，利用有/无提示推出的相对难度估计各自优势。

Result: 在多种模态、模型规模和领域的广泛实验中，ADHint在推理能力和分布外泛化方面表现优异，在pass@1和avg@8指标上持续超越现有方法。

Conclusion: ADHint通过将难度作为关键因素纳入提示比例调度和相对优势估计，实现了探索和模仿之间更好的平衡，显著提升了推理能力和泛化性能。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [106] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: UnCoL是一个用于半监督医学图像分割的双教师框架，通过不确定性指导的协同学习，平衡通用基础模型的专业知识迁移和任务特定表示学习。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在医学图像分割中展现了强大的泛化能力，但在有限标注或罕见病理变化下，通用先验与任务特定需求不匹配，导致在专业临床任务中泛化能力不足。

Method: 提出UnCoL（Uncertainty-informed Collaborative Learning）双教师框架：1）从冻结的基础模型中蒸馏视觉和语义表示以迁移通用知识；2）维护一个渐进适应的教师模型以捕捉细粒度的任务特定表示；3）通过预测不确定性自适应调节伪标签学习，选择性地抑制不可靠监督并稳定模糊区域的学习。

Result: 在多样化的2D和3D分割基准测试中，UnCoL持续优于最先进的半监督方法和基础模型基线，在显著减少标注需求的情况下实现了接近全监督的性能。

Conclusion: UnCoL通过不确定性指导的协同学习，有效协调了泛化与专业化，为半监督医学图像分割提供了一种平衡通用知识和任务特定需求的解决方案。

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [107] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

TL;DR: FID-Net：基于YOLOv8n改进的深度学习模型，利用无人机可见光图像检测森林病虫害树木，并通过三种空间指标进行虫害分析，在新疆天山东部32个林区实验中表现优于主流YOLO模型。


<details>
  <summary>Details</summary>
Motivation: 森林病虫害威胁生态系统稳定，需要高效监测。传统方法在大规模、细粒度检测方面存在局限，本研究旨在准确识别感染树木并分析虫害模式。

Method: 提出FID-Net模型：基于YOLOv8n，引入轻量级特征增强模块(FEM)提取病害敏感特征，自适应多尺度特征融合模块(AMFM)对齐融合RGB和FEM增强的双分支特征，以及高效通道注意力(ECA)机制增强判别信息。从检测结果构建虫害分析框架：核密度估计定位感染热点、邻域评估健康树木感染风险、DBSCAN聚类识别高密度健康集群作为优先保护区域。

Result: 在新疆天山东部32个林区的无人机图像实验中，FID-Net达到86.10%精确率、75.44%召回率、82.29% mAP@0.5和64.30% mAP@0.5:0.95，优于主流YOLO模型。分析证实感染树木呈现明显聚类模式。

Conclusion: FID-Net能够准确区分树木健康状况，结合空间指标为智能虫害监测、早期预警和精准管理提供可靠数据，支持有针对性的森林保护。

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [108] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: DiffFusion：基于扩散模型的多模态3D目标检测框架，通过图像和点云恢复以及自适应融合对齐，提升恶劣天气条件下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D目标检测在恶劣天气条件下效果受限，主要原因是天气引起的图像失真和不同数据模态之间的不对齐问题

Method: 1. Diffusion-IR：使用扩散模型恢复受天气影响的图像；2. Point Cloud Restoration (PCR)：利用图像目标线索补偿受损的LiDAR数据；3. BAFAM：双向自适应融合对齐模块，实现动态多模态融合和双向鸟瞰图对齐

Result: 在三个公开数据集上达到最先进的恶劣天气鲁棒性，同时保持强大的干净数据性能；在真实世界DENSE数据集上的零样本结果进一步验证了其泛化能力

Conclusion: DiffFusion通过扩散模型恢复和自适应跨模态融合，有效提升了多模态3D目标检测在恶劣天气条件下的鲁棒性，具有实际应用价值

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [109] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: 该研究提出了一种名为"权重空间相关性分析"的可解释性方法，用于量化医学影像深度学习模型中特征利用情况，特别是检测模型是否利用了嵌入中的元数据信息进行预测。


<details>
  <summary>Details</summary>
Motivation: 医学影像深度学习模型容易受到捷径学习的影响，依赖图像嵌入中编码的混杂元数据（如扫描仪型号）。关键问题是模型是否主动利用这些编码信息进行最终预测。

Method: 引入权重空间相关性分析方法，通过测量主要临床任务分类头与辅助元数据任务分类头之间的对齐程度来量化特征利用。首先通过检测人工诱导的捷径学习验证方法，然后应用于SA-SonoNet模型的自发性早产预测分析。

Result: 验证了方法能成功检测人工诱导的捷径学习。在sPTB预测模型中，分析确认虽然嵌入包含大量元数据，但sPTB分类器的权重向量与临床相关因素（如出生体重）高度相关，而与临床无关的采集因素（如扫描仪）解耦。

Conclusion: 该方法为验证模型可信度提供了工具，表明在没有诱导偏差的情况下，临床模型会选择性地利用与真实临床信号相关的特征。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [110] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出Intrinsic Image Fusion方法，通过结合单视图先验和多视图一致性优化，从多视角图像重建高质量物理材质，在材质解耦方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 材质重建是一个高度欠约束的问题，传统基于分析-合成的方法需要昂贵且噪声大的路径追踪。需要更好的约束来优化材质重建过程。

Method: 1) 利用基于扩散的材质估计器生成每视图的候选分解；2) 拟合显式低维参数函数减少不一致性；3) 提出鲁棒优化框架，结合软每视图预测选择和基于置信度的软多视图内点集；4) 使用逆路径追踪优化低维参数。

Result: 在合成和真实场景的材质解耦任务中优于现有最先进方法，产生清晰干净的重建结果，适用于高质量重光照。

Conclusion: Intrinsic Image Fusion方法通过融合单视图先验和多视图一致性约束，成功实现了高质量的物理材质重建，为材质解耦提供了有效的解决方案。

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [111] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: CRAFTS是首个针对病理学的文本到图像生成基础模型，通过相关性调节对齐框架解决生成模型在病理图像合成中的语义不稳定和形态幻觉问题，能够生成30种癌症类型的多样化病理图像，并通过数据增强提升临床任务的性能。


<details>
  <summary>Details</summary>
Motivation: 病理学中临床级人工智能的发展受到高质量标注数据集稀缺的限制，现有生成模型存在语义不稳定和形态幻觉问题，影响诊断可靠性。需要开发能够生成生物准确、多样化病理图像的解决方案。

Method: 提出CRAFTS（相关性调节对齐框架），采用双阶段训练策略，在约280万图像-标题对上进行训练，引入新颖的对齐机制抑制语义漂移，确保生物准确性。模型可与ControlNet结合，通过核分割掩码和荧光图像等输入精确控制组织结构。

Result: CRAFTS能够生成30种癌症类型的多样化病理图像，质量通过客观指标和病理学家评估严格验证。CRAFTS增强的数据集在分类、跨模态检索、自监督学习和视觉问答等多种临床任务中提升了性能。

Conclusion: CRAFTS通过克服数据稀缺和隐私问题，提供了无限多样化的标注组织学数据源，为罕见和复杂癌症表型的稳健诊断工具开发开辟了道路，是病理学AI发展的重要突破。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [112] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

TL;DR: DFSS是一个专门为语义分割设计的数据无知识蒸馏框架，通过利用教师模型的BN统计信息指导近似分布采样，并采用加权分布渐进蒸馏策略，显著提升了语义分割任务中的数据无知识蒸馏性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据无知识蒸馏方法主要针对分类任务设计，忽略了语义分割中物体空间连续性和结构一致性的特点，直接应用于分割任务时会导致显著的性能下降。需要专门为语义分割设计的数据无知识蒸馏框架。

Method: 1. 利用教师模型的Batch Normalization统计信息指导近似分布采样，选择更能反映原始训练分布的数据，而不依赖可能误导的教师预测；2. 提出加权分布渐进蒸馏，在训练早期优先处理与原始数据分布更接近的可靠样本，逐步引入更具挑战性的样本，模拟人类感知的自然学习进程。

Result: 在标准基准测试上的大量实验表明，DFSS在语义分割任务中持续优于现有的数据无知识蒸馏方法，取得了最先进的结果，同时显著减少了对辅助数据的依赖。

Conclusion: DFSS通过尊重真实场景的结构和上下文连续性，专门为语义分割任务设计了有效的数据无知识蒸馏框架，解决了现有方法在处理空间连续物体时的局限性，为语义分割的知识蒸馏提供了新的解决方案。

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [113] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

TL;DR: MMDrive是一个用于自动驾驶的多模态视觉语言模型框架，将传统2D图像理解扩展到3D场景理解，融合占用图、LiDAR点云和文本描述三种模态，通过自适应跨模态融合和关键信息提取提升复杂驾驶环境下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于2D平面图像理解范式，难以感知3D空间信息和进行深度语义融合，导致在复杂自动驾驶环境中性能不佳。需要突破传统图像理解限制，实现更强大的多模态推理能力。

Method: 提出MMDrive框架，融合三种互补模态：占用图、LiDAR点云和文本场景描述。引入两个核心组件：1) 面向文本的多模态调制器，根据问题语义动态加权各模态贡献；2) 跨模态抽象器，使用可学习的抽象令牌生成紧凑的跨模态摘要，突出关键区域和语义。

Result: 在DriveLM和NuScenes-QA基准测试中表现优异：DriveLM上BLEU-4得分54.56，METEOR得分41.78；NuScenes-QA上准确率62.7%，显著优于现有自动驾驶视觉语言模型。

Conclusion: MMDrive有效突破了传统仅图像理解的限制，实现了复杂驾驶环境中的鲁棒多模态推理，为可解释的自动驾驶场景理解提供了新基础。

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [114] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

TL;DR: CoRA是一种新颖的协作感知架构，通过混合融合方法解耦性能与鲁棒性，在低通信成本下实现高性能和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知方法在恶劣通信条件下性能下降，因为数据传输导致的对齐问题阻碍了实际部署。需要一种既能保持高性能又具备通信鲁棒性的解决方案。

Method: 提出CoRA架构，包含两个分支：特征级融合分支选择关键特征进行高效融合；对象级校正分支利用语义相关性校正空间位移，抵抗姿态误差。

Result: 在极端场景下，CoRA比基线性能提升约19%（AP@0.7），同时通信量减少5倍以上，实现了性能与鲁棒性的平衡。

Conclusion: CoRA证明了中间融合和后期融合的优势是互补而非权衡，为鲁棒协作感知提供了有前景的解决方案，特别适合实际部署中的通信挑战。

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [115] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: POLAR：大规模物理校准的单光源数据集与基于流的生成模型，实现可控人脸重光照


<details>
  <summary>Details</summary>
Motivation: 人脸重光照研究受限于大规模、物理一致的光照数据可用性，现有方法难以同时保持身份特征和几何结构

Method: 提出POLAR数据集（200+受试者×156光照方向×多视角×多表情），并开发基于流的生成模型POLARNet，从单张肖像预测每光源响应

Result: 建立了统一的光照学习框架，将真实数据、生成合成与物理基础重光照联系起来，形成可扩展、可复现的"鸡与蛋"循环

Conclusion: POLAR和POLARNet共同解决了人脸重光照的数据瓶颈问题，实现了细粒度、方向感知的光照效果，同时保持面部身份特征

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [116] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: Ego-EXTRA是一个用于专家-学员辅助的50小时第一人称视频语言数据集，包含真实专家指导学员执行程序性活动的对话，创建了超过15k个视觉问答对基准，用于评估多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏真实专家通过自然语言指导学员执行程序性活动的第一人称视角对话数据，需要高质量数据集来评估和开发能够提供专家级辅助的多模态智能助手。

Method: 采用"绿野仙踪"数据收集范式，专家扮演可穿戴智能助手角色，仅通过学员的第一人称视角观察活动，回答学员问题或主动提供建议。记录专家与学员的双向对话，转录并创建视觉问答对基准。

Result: 创建了包含50小时非脚本第一人称视频和超过15k个高质量视觉问答对的Ego-EXTRA数据集。评估显示当前多模态大语言模型在该数据集上表现不佳，突显了提供专家级辅助的挑战性。

Conclusion: Ego-EXTRA是一个具有挑战性的基准数据集，揭示了当前模型在提供专家级辅助方面的局限性，为开发第一人称视频语言助手提供了重要资源。

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [117] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: STARCaster是一个统一的身份感知时空视频扩散模型，能够同时处理语音驱动肖像动画和自由视角说话肖像合成，通过软身份约束和隐式3D感知在2D视频域中实现高质量动画生成。


<details>
  <summary>Details</summary>
Motivation: 现有2D语音到视频扩散模型过度依赖参考指导，导致运动多样性有限；而3D感知动画通常依赖预训练的三平面生成器反演，导致重建不完美和身份漂移。需要重新思考基于参考和几何的范式。

Method: 采用组合方法：1）引入软身份约束而非严格参考条件；2）利用视频数据的多视角特性在2D视频域中隐式实现3D感知；3）通过解耦学习分别训练视角一致性和时间连贯性；4）使用自强制训练方案学习更长的时间上下文；5）从ID感知运动建模到音频-视觉同步再到时空适应的渐进式框架。

Result: 综合评估表明，STARCaster在跨任务和身份上具有良好泛化能力，在不同基准测试中持续超越先前方法，解决了现有自回归方法中常见的过度静态动画问题。

Conclusion: STARCaster通过重新思考参考和几何范式，在统一框架中实现了高质量的语音驱动肖像动画和自由视角说话肖像合成，为音频-视觉内容生成提供了新的解决方案。

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [118] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: 该论文提出了视觉基础主动视角选择（VG-AVS）任务，通过当前视觉信息选择最有信息量的下一个视角，并构建了合成数据集和训练框架，提升了视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）仅限于静态图像的快照视觉，而具身智能体需要主动移动获取更有信息量的视角。为了解决这一限制，需要开发能够基于当前视觉信息主动选择下一个最佳视角的能力。

Method: 1. 提出VG-AVS任务：仅使用当前图像中的视觉信息选择最有信息量的下一个视角；2. 构建合成数据集：自动生成配对的查询-目标视角和问答提示；3. 提出训练框架：先通过监督微调（SFT）微调预训练VLMs，然后进行基于强化学习（RL）的策略优化。

Result: 1. 在视角选择基础上实现了强大的问答性能；2. 在未见过的合成和真实场景中表现出鲁棒的泛化能力；3. 将VG-AVS框架整合到现有的基于场景探索的EQA系统中，提高了下游问答准确性。

Conclusion: VG-AVS任务和提出的框架成功地将视觉语言模型从静态快照视觉扩展到主动视角选择，为具身智能体的视觉感知提供了重要进展，并能够提升现有场景探索系统的性能。

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [119] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

TL;DR: CogniEdit：通过多模态推理和密集奖励优化的统一框架，解决扩散模型在遵循细粒度指令（如颜色、位置、数量）方面的困难，实现轨迹级梯度流控制


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法在遵循细粒度指令（如颜色、位置、数量）方面存在困难，而现有的GRPO方法仅在单个采样步骤进行优化，反馈稀疏，限制了轨迹级控制能力

Method: 提出CogniEdit统一框架，包含三个组件：1）多模态大语言模型分解复杂指令为可执行指令；2）动态令牌焦点重定位自适应强调细粒度属性；3）基于密集GRPO的优化，在连续去噪步骤间传播梯度，实现轨迹级监督

Result: 在基准数据集上的广泛实验表明，CogniEdit在平衡细粒度指令遵循、视觉质量和可编辑性保持方面实现了最先进的性能

Conclusion: CogniEdit通过结合多模态推理和密集奖励优化，解决了扩散模型在细粒度指令遵循方面的挑战，实现了轨迹级梯度流控制，显著提升了图像编辑的精确性和质量

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods strug- gle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across con- secutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [120] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

TL;DR: CausalCLIP：基于因果推断的图像生成检测框架，通过解耦因果与非因果特征提升跨生成模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法（包括基于预训练视觉语言模型的方法）往往产生高度纠缠的特征表示，混合了任务相关的取证线索（因果特征）与虚假或无关模式（非因果特征），限制了跨不同生成技术的泛化能力。

Method: 提出CausalCLIP框架，通过结构因果模型建模生成过程，利用Gumbel-Softmax特征掩码和Hilbert-Schmidt独立性准则约束实现统计独立性，明确解耦因果与非因果特征，并基于因果推断原理进行针对性过滤，仅保留最具可迁移性和判别性的取证线索。

Result: 在未见过的不同系列生成模型上测试时，CausalCLIP展现出强大的泛化能力，相比最先进方法在准确率上提升6.83%，平均精度提升4.06%。

Conclusion: 通过因果特征解耦和针对性过滤，CausalCLIP能够有效提升生成图像检测器在多样化生成技术中的泛化性能，为应对快速发展的生成模型提供了更鲁棒的检测方案。

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [121] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: 本文提出LINA框架，通过因果干预解决扩散模型在物理对齐和OOD指令跟随方面的不足，利用因果场景图和物理对齐探针数据集进行诊断，实现图像和视频生成中的因果推理能力提升。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了显著成功，但在物理对齐和分布外指令跟随方面仍存在困难。这些问题源于模型未能学习因果方向和因果因素解耦以进行新颖重组。

Method: 引入因果场景图和物理对齐探针数据集进行诊断分析，提出LINA框架：学习预测特定提示的干预措施，包括在提示和视觉潜在空间中的定向引导，以及重新分配的因果感知去噪调度。

Result: LINA框架在图像和视频扩散模型中同时增强了物理对齐和OOD指令跟随能力，在具有挑战性的因果生成任务和Winoground数据集上实现了最先进的性能。

Conclusion: 通过因果干预和重新分配的去噪调度，LINA框架有效解决了扩散模型在物理对齐和OOD指令跟随方面的核心问题，为因果感知的图像和视频生成提供了新思路。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [122] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

TL;DR: 论文提出创造性表格可视化新任务，开发ShowTable管道结合MLLM和扩散模型，通过自校正过程生成高质量信息图表，并建立TableVisBench基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在需要深度推理、规划和精确数据到视觉映射的任务上表现不足，特别是在超越一般场景的创造性表格可视化方面存在局限。

Method: 提出ShowTable管道，通过渐进式自校正过程协同多模态大语言模型和扩散模型。MLLM作为中央协调器进行视觉规划推理和视觉错误判断，扩散模型执行MLLM指令，实现高保真结果。开发三种自动数据构建管道训练不同模块。

Result: 实验表明，使用不同模型实例化的ShowTable管道显著优于基线方法，在TableVisBench基准的5个评估维度上展现出有效的多模态推理、生成和错误校正能力。

Conclusion: 论文成功解决了创造性表格可视化这一具有挑战性的任务，提出的ShowTable管道通过多模态协同和自校正机制实现了高质量信息图表生成，为数据可视化领域提供了新方法。

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [123] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

TL;DR: KlingAvatar 2.0是一个时空级联框架，通过低分辨率蓝图视频关键帧生成和高分辨率子片段细化，解决了长时高分辨率视频生成中的效率、时间漂移和质量退化问题，并引入协同推理导演增强多模态指令对齐。


<details>
  <summary>Details</summary>
Motivation: 现有头像视频生成模型在生成长时高分辨率视频时存在效率低、时间漂移、质量退化和提示跟随能力弱的问题，需要一种能够高效生成高质量长视频并保持多模态指令对齐的解决方案。

Method: 提出时空级联框架：首先生成低分辨率蓝图视频关键帧捕捉全局语义和运动，然后使用首尾帧策略将其细化为高分辨率、时间连贯的子片段。引入协同推理导演，由三个模态特定的大语言模型专家组成，通过多轮对话推理模态优先级和用户意图，将输入转换为详细故事情节。负向导演进一步优化负向提示以改善指令对齐。扩展框架支持ID特定的多角色控制。

Result: 实验表明，该模型有效解决了高效、多模态对齐的长时高分辨率视频生成挑战，提供增强的视觉清晰度、逼真的唇齿渲染与准确的口型同步、强大的身份保持以及连贯的多模态指令跟随能力。

Conclusion: KlingAvatar 2.0通过时空级联框架和协同推理导演机制，成功解决了长时高分辨率头像视频生成中的关键问题，实现了高效、高质量且多模态对齐的视频生成。

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [124] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 该论文研究了人脸检索系统中的身份遗忘问题，提出了一种基于分散的遗忘方法，使特定身份无法被检索，同时保持其他身份的检索性能。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统虽然能有效进行身份检索，但存在严重的隐私风险，可能被用于未经授权的身份追踪。现有机器学习遗忘方法在人脸检索领域的应用尚未充分探索，特别是在现代基于嵌入的识别模型中。

Method: 评估了多种现有的近似类别遗忘方法（如随机标记、梯度上升、边界遗忘等），并提出了一种简单而有效的基于分散的遗忘方法。该方法通过在超球面上分散目标身份的嵌入，防止形成紧凑的身份聚类，从而实现遗忘效果。

Result: 在标准基准数据集（VGGFace2、CelebA）上的广泛实验表明，该方法实现了优越的遗忘效果，同时保持了模型的检索效用。

Conclusion: 该研究为人脸检索系统的隐私保护提供了有效的解决方案，通过分散嵌入的方法实现了特定身份的遗忘，同时保持了嵌入空间的判别结构和剩余身份的检索性能。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [125] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 该论文针对长尾分布下的2D目标检测问题，在LVISv1数据集上通过改进的平衡组Softmax框架和度量学习方法，实现了24.5%的mAP，刷新了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的目标检测面临长尾分布挑战，即大量类别只有少数实例，导致检测模型偏向频繁类别，在稀有类别上性能下降。需要解决类别不平衡问题以提升整体检测性能。

Method: 1. 采用两阶段Faster R-CNN架构；2. 改进平衡组Softmax（BAGS）框架缓解类别不平衡；3. 探索度量学习生成类别间分离、类内紧密的特征嵌入；4. 推理时使用k近邻方法提升分类性能，特别是稀有类别。

Result: 在LVISv1数据集（1,203个类别，164,000张图像）上实现了24.5%的平均精度均值（mAP），超越了之前的24.0%基准，创造了新的最先进性能。

Conclusion: 提出的改进BAGS框架和度量学习方法有效缓解了长尾分布下的类别不平衡问题，提升了目标检测性能，特别是在稀有类别上，为长尾目标检测提供了有效解决方案。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [126] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

TL;DR: 该研究提出了一种利用DINO自注意力"key"特征进行息肉分割的框架，通过简单卷积解码器实现，在领域泛化和极端单领域泛化场景下取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习息肉分割方法存在泛化能力不足的问题，特别是在数据受限或挑战性场景下，且许多现有方法依赖复杂的任务特定架构。

Method: 利用DINO自注意力模块的"key"特征，结合简单卷积解码器预测息肉掩码，避免从ViT最深层提取token的传统方法。

Result: 在多中心数据集上通过领域泛化和极端单领域泛化协议验证，该框架实现了最先进的性能，显著提升了泛化能力，特别是在数据稀缺和挑战性场景下，超越了nnU-Net和UM-Net等模型。

Conclusion: 该研究提出的基于DINO自注意力key特征的框架在避免息肉特定架构的同时，实现了优异的泛化性能和分割效果，为息肉分割提供了更稳健的解决方案。

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [127] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: 神经细胞自动机（NCA）被重新用作有效的分割掩码细化机制，通过局部迭代更新修复拓扑错误，提升分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有通用分割模型经常产生碎片化或不连贯的掩码输出，修复这些伪影通常需要手工设计的细化规则或针对特定任务的专门架构。需要一种通用且有效的细化机制来改善分割掩码的拓扑一致性。

Method: 提出细化神经细胞自动机（rNCA），将其重新用作细化机制。通过在不完美掩码和真实标注上训练，自动机学习目标形状的结构特性，仅依赖局部信息。当应用于粗糙的全局预测掩码时，学习到的动态过程逐步重新连接断裂区域、修剪松散碎片，并收敛到稳定、拓扑一致的结果。

Result: 在视网膜血管分割中，Dice/clDice指标提升2-3%，Betti错误减少（β0错误减少60%，β1减少20%）；在心肌分割中，零样本设置下修复了61.5%的断裂案例，ASSD和HD分别降低19%和16%。

Conclusion: 神经细胞自动机（NCA）可以作为有效且广泛适用的分割细化器，通过局部迭代更新修复常见拓扑错误，显著改善分割掩码的拓扑一致性。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [128] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: 提出MCT-UEG框架，通过元跨任务训练生成广泛不可利用的个人图像，防止在线图像被未经授权利用


<details>
  <summary>Details</summary>
Motivation: 现有不可利用示例生成方法在实际应用中存在局限性，无法生成跨不同真实世界计算机视觉任务的广泛不可利用示例，限制了方法的实际适用性

Method: 提出Meta Cross-Task Unexploitable Example Generation (MCT-UEG)框架，核心是设计面向平坦最小值的元训练和测试方案，优化不可利用示例生成器以有效产生广泛不可利用的示例

Result: 大量实验证明了该框架的有效性

Conclusion: MCT-UEG框架能够生成跨任务的广泛不可利用示例，解决了现有方法在实际应用中的局限性问题

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [129] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Steer3D：一种为图像到3D模型添加文本可控性的前馈方法，支持用语言编辑生成的3D资产


<details>
  <summary>Details</summary>
Motivation: 虽然图像到3D生成技术取得了进展，但要在实际应用中使用AI生成的3D资产，关键需求是能够轻松编辑这些资产。现有方法缺乏对生成3D资产的文本引导编辑能力。

Method: 受ControlNet启发，将文本可控性适配到图像到3D生成中；构建可扩展的自动数据生成引擎；采用基于流匹配训练和直接偏好优化（DPO）的两阶段训练方案

Result: Steer3D相比竞争方法更忠实地遵循语言指令，与原始3D资产保持更好的一致性，同时速度提升2.4倍到28.5倍

Conclusion: Steer3D证明可以通过10万数据为预训练的图像到3D生成模型添加新的模态（文本）来引导生成，实现了高效的文本可控3D编辑

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [130] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

TL;DR: RecTok提出了一种新的视觉tokenizer方法，通过流语义蒸馏和重建对齐蒸馏解决高维潜在空间中重建质量与语义表达之间的权衡问题，在保持丰富语义的同时实现了优异的图像重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉tokenizer在潜在空间维度与生成质量之间存在根本性权衡，高维tokenizer性能通常不如低维版本。虽然已有工作利用视觉基础模型增强语义并加速收敛，但高维tokenizer仍表现不佳。

Method: 提出RecTok方法，包含两个关键创新：1) 流语义蒸馏：将视觉基础模型的语义信息蒸馏到流匹配的前向流轨迹中，使其成为扩散变换器的训练空间；2) 重建对齐蒸馏：引入掩码特征重建损失进一步增强语义。

Result: RecTok在gFID-50K基准测试中，无论是否使用分类器自由引导，都取得了最先进的结果。随着潜在维度增加，性能持续提升，同时保持了语义丰富的潜在空间结构。

Conclusion: RecTok通过创新的蒸馏方法成功克服了高维视觉tokenizer的局限性，实现了优异的图像重建、生成质量和判别性能，为视觉tokenizer设计提供了新思路。

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [131] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: DiffusionBrowser是一个模型无关的轻量级解码器框架，可在去噪过程的任何时间步或Transformer块处交互式生成预览，支持RGB和场景内在表示，速度超过实时4倍，并支持通过随机性重注入和模态引导进行交互控制。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型虽然革新了视频生成，但存在不精确、速度慢、生成过程不透明的问题，用户在长时间内无法了解生成进度。

Method: 提出DiffusionBrowser框架，这是一个模型无关的轻量级解码器，可在去噪过程的任何时间步或Transformer块处生成多模态预览表示（包括RGB和场景内在表示）。

Result: 模型能以超过实时4倍的速度生成预览（4秒视频不到1秒），预览与最终视频的外观和运动保持一致。通过训练的解码器，可以在中间噪声步通过随机性重注入和模态引导进行交互式控制。

Conclusion: DiffusionBrowser不仅提供了交互式预览和控制能力，还通过学习的解码器系统性地探索了模型，揭示了原本黑盒的去噪过程中场景、物体和其他细节是如何组合和组装的。

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


### [132] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: MineTheGap是一种自动挖掘导致文本到图像模型生成偏见输出的提示词的方法，使用遗传算法迭代优化提示词池，通过新颖的偏见评分来识别和量化偏见严重程度。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在处理模糊文本提示时表现出偏见，这些偏见可能产生社会影响（如职业与种族的刻板印象）并影响用户体验（生成冗余而非多样化的图像）。现有方法仅能检测给定提示的偏见，而无法主动发现导致偏见的提示。

Method: 提出MineTheGap方法：1）使用遗传算法迭代优化提示词池，寻找能暴露偏见的提示；2）设计新颖的偏见评分机制，通过比较生成图像分布与LLM生成的文本变体分布来量化偏见严重程度；3）在已知偏见数据集上验证评分有效性。

Result: 该方法能够自动挖掘导致TTI模型生成偏见输出的提示词，超越了仅检测给定提示偏见的能力。偏见评分能够根据严重程度对偏见进行排序，并在已知偏见数据集上得到验证。

Conclusion: MineTheGap提供了一种系统化方法来发现文本到图像模型中的偏见，通过主动寻找暴露偏见的提示词，有助于识别和缓解模型中的社会偏见问题，提升生成图像的多样性。

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [133] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 提出了一种轻量级少样本学习框架，结合MobileNet特征提取器、特征融合技术和注意力增强的Bi-LSTM分类器，用于资源受限环境下的植物叶片病害识别。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习植物病害识别方法依赖大量标注数据和计算密集型模型，不适合数据稀缺和资源受限的农业环境，需要开发轻量级且高效的少样本学习方法。

Method: 使用域适应的MobileNetV2和MobileNetV3作为特征提取器，结合特征融合技术生成鲁棒特征表示，然后通过注意力机制增强的Bi-LSTM分类器进行序列依赖建模和关键特征聚焦。

Result: 在PlantVillage数据集上，15-shot场景达到98.23±0.33%，接近SOTA的99.98%；在Dhan Shomadhan野外数据集上达到69.28±1.49%；模型仅约40MB大小，计算复杂度约1.12 GFLOPs。

Conclusion: 该框架为数据稀缺地区提供了一个可扩展、移动就绪的精确植物病害诊断基础，在保持轻量级的同时实现了接近SOTA的性能。

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [134] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

TL;DR: IMILIA是一个端到端框架，使用多实例学习预测IBD组织切片中的炎症存在，并通过可解释性模块自动计算驱动预测的组织区域特征标记物。


<details>
  <summary>Details</summary>
Motivation: 随着IBD治疗目标转向组织学缓解，准确评估微观炎症对于评估疾病活动性和治疗反应变得越来越重要。

Method: IMILIA包含炎症预测模块（基于多实例学习模型）和可解释性模块（包括HistoPLUS用于细胞检测、分割和分类，以及EpiSeg用于上皮分割）。

Result: 在发现队列中交叉验证ROC-AUC为0.83，在两个外部验证队列中分别为0.99和0.84。可解释性模块显示：高分区域免疫细胞密度增加，低分区域主要为正常上皮细胞。

Conclusion: IMILIA能够准确预测IBD组织切片中的炎症存在，并提供生物学一致的可解释性结果，有助于评估IBD的微观炎症。

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [135] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

TL;DR: 提出一种在测试时使用扩散模型将目标域图像映射回源域分布的方法，无需大规模合成数据生成，在分割、检测和分类任务上实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 生成基础模型包含广泛的视觉知识并能产生多样化的图像变体，但在训练数据增强方面存在合成目标域变体缓慢、昂贵且不完整的问题

Method: 在测试时使用扩散模型将目标图像映射回源域分布，仅需源域描述，保留任务模型，避免大规模合成数据生成

Result: 在真实到真实的域泛化场景中，分割、检测和分类任务均获得一致改进：BDD100K-Night相对提升137%，ImageNet-R提升68%，DarkZurich提升62%

Conclusion: 该方法为域泛化提供了一种高效替代方案，无需大规模数据合成，在多种任务和模型上表现出强大的泛化能力

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [136] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: PoseAnything是一个通用的姿态引导视频生成框架，支持人类和非人类角色，能够处理任意骨骼输入，并实现了独立相机运动控制。


<details>
  <summary>Details</summary>
Motivation: 当前姿态引导视频生成方法仅接受人类姿态作为输入，对其他主体的姿态泛化能力差，限制了在动画等领域的应用范围。

Method: 1. 提出通用姿态引导视频生成框架PoseAnything；2. 引入部件感知时间一致性模块，通过划分主体部件、建立对应关系、计算跨帧注意力实现细粒度一致性；3. 提出主体和相机运动解耦CFG策略，通过分别注入主体和相机运动控制信息实现独立相机控制；4. 构建XPose数据集，包含5万个非人类姿态-视频对。

Result: PoseAnything在效果和泛化能力上显著优于现有最先进方法，能够处理人类和非人类角色，支持任意骨骼输入，并实现了独立的相机运动控制。

Conclusion: 该研究提出了首个通用的姿态引导视频生成框架，突破了现有方法仅支持人类姿态的限制，通过创新的部件一致性模块和运动解耦策略，显著提升了视频生成的灵活性和控制精度。

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [137] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

TL;DR: T3-Video提出了一种Transformer改造策略，通过优化前向逻辑而非修改核心架构，显著降低4K视频生成的计算需求，实现10倍加速同时提升质量


<details>
  <summary>Details</summary>
Motivation: 原生4K视频生成面临二次计算爆炸的挑战，全注意力机制在时空分辨率增加时计算量急剧增长，现有模型难以在效率和质量之间取得平衡

Method: 提出T3（Transform Trained Transformer）改造策略，引入多尺度权重共享窗口注意力机制，通过分层分块和轴保持全注意力设计，仅需适度计算和数据即可实现预训练模型的"注意力模式"转换

Result: 在4K-VBench上，T3-Video显著优于现有方法：VQA提升4.29分，VTC提升0.08分，同时将原生4K视频生成加速超过10倍

Conclusion: T3策略为高分辨率视频生成提供了一种高效解决方案，在不改变预训练模型核心架构的情况下，通过优化前向逻辑实现了计算效率和质量的双重提升

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [138] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.5 pro是一个用于原生联合音频-视频生成的基础模型，采用双分支扩散变换器架构，通过跨模态联合模块和多阶段数据管道实现卓越的视听同步和生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频生成领域的最新进展为统一的视听生成铺平了道路，需要专门的基础模型来实现原生、联合的音频-视频生成，以满足专业级内容创作的需求。

Method: 采用双分支扩散变换器架构，集成跨模态联合模块和专门的多阶段数据管道。实施细致的训练后优化，包括高质量数据集上的监督微调（SFT）和基于多维奖励模型的人类反馈强化学习（RLHF）。引入加速框架将推理速度提升10倍以上。

Result: 模型实现了卓越的视听同步和生成质量，具备精确的多语言和方言唇形同步、动态电影级摄像机控制以及增强的叙事连贯性，成为专业级内容创作的强大引擎。

Conclusion: Seedance 1.5 pro通过创新的架构设计、精细的训练优化和高效的推理加速，成功构建了一个功能强大的原生联合音频-视频生成基础模型，现已可在火山引擎平台上访问使用。

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [139] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: Pancakes框架能够为医学图像自动生成多种语义一致的多标签分割方案，解决现有模型只能处理单一分割协议或需要手动提示的问题


<details>
  <summary>Details</summary>
Motivation: 同一医学图像可以根据不同应用需求进行多种有意义的分割（如组织类型、血管区域、解剖结构等），但现有模型要么只支持单一协议，要么需要人工提示，缺乏自动生成多种语义一致分割方案的能力

Method: 提出Pancakes框架，引入新的问题表述，能够在未见过的图像域中自动生成多个合理协议的多标签分割图，同时保持相关图像间的语义一致性

Result: 在7个保留数据集上的实验表明，该模型在生成多个语义一致的整图分割方面显著优于现有基础模型

Conclusion: Pancakes框架解决了现有基础模型无法实现的新问题，能够自动为医学图像生成多种语义一致的分割方案，具有重要的临床应用价值

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [140] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

TL;DR: 提出了MMhops基准测试和MMhops-R1框架，用于评估和提升多模态多跳推理能力，通过强化学习优化动态推理路径规划


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要局限于单步推理，缺乏评估多模态多跳推理能力的复杂基准，需要解决多模态信息迭代整合和外部知识利用的挑战

Method: 提出MMhops基准测试包含Bridging和Comparison两种任务格式，并开发MMhops-R1多模态检索增强生成框架，使用强化学习优化动态推理路径规划、目标查询制定和多层次信息合成

Result: MMhops-R1在MMhops基准上显著优于强基线模型，证明了动态规划和多模态知识整合对复杂推理的重要性，同时在固定跳数推理任务上表现出良好的泛化能力

Conclusion: 该工作贡献了具有挑战性的新基准和强大的基线模型，将发布相关代码、数据和权重以推动该关键领域的研究

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [141] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

TL;DR: LiMo是一种基于扩散模型的时空光照估计方法，通过生成不同曝光下的镜面和漫反射球体来预测高频细节和准确照度，结合深度和新几何条件进行空间控制，最终合成HDRI地图。


<details>
  <summary>Details</summary>
Motivation: 现有的光照估计方法难以同时实现真实的高频细节预测和准确的照度估计，特别是在时空场景中。需要一种既能捕捉复杂光照细节又能准确估计光照强度的综合方法。

Method: 1. 基于输入3D位置生成不同曝光下的镜面和漫反射球体集合；2. 在大规模定制室内外场景数据集上微调现有扩散模型；3. 引入新的几何条件（场景与目标3D位置的相对位置）来补充深度信息的不足；4. 通过可微分渲染将不同曝光的漫反射和镜面预测组合成单个HDRI地图。

Result: LiMo在空间控制和预测准确性方面都达到了最先进的水平，能够同时实现高质量的高频细节预测和准确的照度估计。

Conclusion: LiMo通过结合扩散先验、新几何条件和多曝光策略，成功解决了时空光照估计中的高频细节和照度准确性的双重挑战，为场景照明提供了全面而准确的解决方案。

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [142] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie 2是一个三阶段训练的端到端自回归框架，通过多模态引导、退化感知训练和历史上下文引导，实现了可控、高质量、时间一致的长视频生成，支持长达5分钟的视频生成。


<details>
  <summary>Details</summary>
Motivation: 基于预训练视频生成系统构建视频世界模型是实现通用时空智能的重要但具有挑战性的步骤。世界模型需要具备三个基本属性：可控性、长期视觉质量和时间一致性。

Method: 采用渐进式方法：1) 多模态引导：整合稠密和稀疏控制信号，提供隐式世界级监督；2) 输入帧的退化感知训练：弥合训练与长期推理之间的差距；3) 历史上下文引导：对齐相邻片段间的上下文信息以确保时间一致性。

Result: LongVie 2在长距离可控性、时间一致性和视觉保真度方面达到最先进性能，支持长达5分钟的连续视频生成。同时提出了包含100个高分辨率一分钟视频的LongVGenBench基准测试。

Conclusion: LongVie 2代表了向统一视频世界建模迈出的重要一步，通过三阶段训练框架有效解决了长视频生成中的可控性、视觉质量和时间一致性挑战。

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [143] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: Do-Undo任务和基准旨在解决视觉语言模型在理解和生成由真实世界动作驱动的物理合理场景转换方面的关键缺陷，要求模型模拟物理动作的结果并准确反转它。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉语言模型在理解物理场景转换方面的不足，特别是缺乏对真实世界动作引起的因果关系的理解。现有工作主要关注对象级编辑，而Do-Undo需要模型理解物理动作的可逆性，这对于具身AI、机器人和物理感知生成建模至关重要。

Method: 1. 从真实世界视频中策划大规模可逆动作数据集；2. 设计训练策略以增强动作基础的一致性；3. 要求模型模拟物理动作的结果并准确反转该动作。

Result: 实验表明，当前模型在处理物理可逆性方面存在困难，这突显了该任务的重要性。Do-Undo为评估和推进多模态系统中的物理推理能力建立了直观的测试平台。

Conclusion: Do-Undo任务填补了视觉语言模型在物理场景转换理解方面的关键空白，为具身AI、机器人和物理感知生成建模提供了重要的评估基准，强调了物理推理能力对多模态系统发展的重要性。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [144] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

TL;DR: MindDrive提出了一种基于在线强化学习的VLA框架，通过将连续动作空间映射到离散语言决策空间来解决自动驾驶中探索效率问题，在Bench2Drive基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的视觉-语言-动作范式主要依赖模仿学习，存在分布偏移和因果混淆问题。在线强化学习虽然能通过试错学习解决这些问题，但在连续动作空间中探索效率低下，阻碍了其在VLA模型中的应用。

Method: 提出MindDrive框架，使用一个大型语言模型配备两套不同的LoRA参数：一个作为决策专家进行场景推理和驾驶决策，另一个作为动作专家将语言决策动态映射为可行轨迹。通过将轨迹级奖励反馈到推理空间，实现在有限离散语言驾驶决策上的试错学习。

Result: 在Bench2Drive基准测试中，MindDrive取得了驾驶评分78.04%和成功率55.09%的强闭环性能。这是首个证明在线强化学习在自动驾驶VLA模型中有效性的工作。

Conclusion: MindDrive通过将连续动作空间映射到离散语言决策空间，有效平衡了复杂场景下的最优决策、类人驾驶行为和在线强化学习中的高效探索，为自动驾驶VLA范式提供了新的解决方案。

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [145] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 提出用于新视角合成的全新高质量数据集，包含动态场景、多种模态数据，支持三种不同基准测试场景


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量、多模态、动态场景的新视角合成数据集，需要为4D场景重建和新视角生成模型提供更好的训练和评估资源

Method: 从高质量动画电影中生成数据集，提供RGB图像、深度、表面法线、物体分割和光流等多种模态数据，组织成密集多视角、稀疏相机和单目视频三种基准测试场景

Result: 创建了一个视觉丰富、标注质量高、实验设置多样的新视角合成数据集，为3D视觉研究提供了独特资源

Conclusion: 该数据集结合了视觉丰富性、高质量标注和多样化实验设置，为推进视角合成和3D视觉领域边界提供了独特资源

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [146] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 提出Grab-3D框架，利用3D几何一致性检测AI生成视频，通过消失点分析几何模式差异，在静态场景数据集上验证效果显著


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成逼真视频的能力提升，需要可靠的检测机制。现有方法对生成视频中的3D几何模式探索有限，存在检测不足的问题

Method: 使用消失点作为3D几何模式的显式表示，提出Grab-3D框架：包含几何位置编码、时空几何注意力机制和EMA几何分类器头，在静态场景数据集上训练

Result: Grab-3D显著优于现有最先进检测器，在未见过的生成器上表现出强大的跨域泛化能力

Conclusion: 通过显式建模3D几何时间一致性，Grab-3D为AI生成视频检测提供了有效解决方案，具有实际应用价值

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [147] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

TL;DR: 本文提出了交互式智能新范式Mio框架，通过五个专业模块实现人格对齐表达、自适应交互和自我进化，建立了新的评估基准并在所有维度上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前数字人主要停留在表面模仿阶段，缺乏真正的智能交互能力。需要一种能够实现人格对齐表达、自适应交互和自我进化的数字人新范式。

Method: 提出了Mio（多模态交互全息化身）端到端框架，包含五个专业模块：思考者、说话者、面部动画师、身体动画师和渲染器，将认知推理与实时多模态具身化相结合。

Result: 建立了新的交互智能评估基准，广泛实验表明该框架在所有评估维度上均优于现有最先进方法。

Conclusion: 这些贡献推动数字人从表面模仿向智能交互发展，实现了人格对齐表达、自适应交互和自我进化的交互式智能新范式。

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [148] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

TL;DR: JoVA是一个统一的视频-音频联合生成框架，通过联合自注意力机制实现跨模态交互，无需额外对齐模块，并引入基于面部关键点检测的嘴部区域损失来提升唇语同步质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：1) 大多只能生成环境音，缺乏生成与唇部运动同步的人类语音的能力；2) 现有统一视频-音频生成方法通常依赖显式融合或模态特定对齐模块，增加了架构复杂性并削弱了原始Transformer的简洁性。

Method: JoVA采用视频和音频token在每层Transformer中的联合自注意力机制，实现直接高效的跨模态交互，无需额外对齐模块。同时引入基于面部关键点检测的嘴部区域损失，在训练期间增强对关键嘴部区域的监督，而不影响架构简洁性。

Result: 在基准测试上的广泛实验表明，JoVA在唇语同步准确性、语音质量和整体视频-音频生成保真度方面优于或与最先进的统一和音频驱动方法相竞争。

Conclusion: JoVA被确立为一个优雅的高质量多模态生成框架，通过简洁的架构设计实现了高效的视频-音频联合生成。

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [149] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

TL;DR: LASER是一个无需训练即可将离线3D重建模型转换为流式处理系统的框架，通过分层尺度对齐解决时间窗口间的深度尺度不一致问题，实现实时流式视频重建。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈重建模型（如VGGT和π³）虽然重建质量优秀，但由于二次内存复杂度无法处理流式视频，限制了实际部署。现有的流式方法需要大量重新训练，且无法充分利用最先进离线模型的几何先验。

Method: 提出LASER框架，通过跨连续时间窗口的预测对齐将离线重建模型转换为流式系统。为了解决简单相似变换对齐因单目尺度模糊导致的层深度错位问题，引入了分层尺度对齐方法：将深度预测分割为离散层，计算每层尺度因子，并在相邻窗口和时间戳间传播。

Result: 实验表明，LASER在相机姿态估计和点云图重建方面达到最先进性能，在RTX A6000 GPU上以14 FPS运行，峰值内存仅6 GB，能够处理千米级流式视频。

Conclusion: LASER提供了一种无需训练的方法，将高质量离线3D重建模型转换为实用的流式系统，解决了现有方法的内存和训练限制，实现了实时大规模流式视频重建的实际部署。

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [150] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

TL;DR: 该研究提出了一种新方法，通过重新编程预训练的3D实例生成器来学习场景级空间关系，实现对新布局和物体组合的泛化能力，无需依赖有限场景数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的3D场景生成方法依赖于有限的场景数据集，导致对新布局的泛化能力受限。研究者希望利用预训练3D实例生成器中蕴含的可迁移空间知识，突破数据集限制。

Method: 重新编程预训练的3D实例生成器，将其转变为场景级学习器。采用模型中心的空间监督替代数据集监督，解锁生成器的可迁移空间知识。提出以视角为中心的场景空间表示，替代传统的规范空间，实现全前馈、可泛化的场景生成。

Result: 该方法能够泛化到未见过的布局和新颖的物体组合。即使在训练场景由随机组合的物体构成时，系统仍能涌现出空间推理能力。定量和定性结果表明，3D实例生成器具有隐式的空间学习和推理能力。

Conclusion: 3D实例生成器可以作为隐式的空间学习器和推理器，为交互式3D场景理解和生成的基础模型提供了新方向。该方法证明了从纯几何线索中推断邻近性、支撑关系和对称性的可行性。

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [151] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

TL;DR: 视觉分词器（VAE）的潜在空间质量对生成模型至关重要，但传统重建训练偏向低级信息，导致预训练扩展问题。VTP框架通过联合优化图像-文本对比、自监督和重建损失，实现理解驱动生成，显著改善扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统基于重建的视觉分词器训练存在根本缺陷：更好的像素级精度并不带来更高质量的生成，导致大量计算投入在分词器预训练上却无法有效提升生成性能，这被称为"预训练扩展问题"。

Method: 提出VTP统一视觉分词器预训练框架，首次联合优化三种损失：图像-文本对比损失、自监督损失和重建损失，以构建能简洁表示高层语义的潜在空间。

Result: 大规模研究表明：1）理解是生成的关键驱动力；2）VTP具有更好的扩展性，生成性能随计算、参数和数据有效提升。VTP在ImageNet上达到78.2%零样本准确率和0.36 rFID，生成收敛速度快4.1倍，仅通过增加VTP预训练计算量就能实现65.8%的FID改进。

Conclusion: VTP框架通过联合多任务学习解决了视觉分词器的预训练扩展问题，证明了理解驱动生成的重要性，为生成模型提供了更有效的视觉分词器预训练方法。

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [152] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

TL;DR: 基于人工年龄评分(AAS)构建可执行的莱布尼茨单子论条款系统，为LLM内存和控制提供法律式约束的工程架构


<details>
  <summary>Details</summary>
Motivation: 解决LLM作为强大但不透明系统的问题，为其内部记忆和"自我式"行为提供原则化、可审计的治理框架

Method: 将莱布尼茨《单子论》中的20个单子分为6个束（本体论、动力学、表征与意识、和谐与理性、身体与组织、目的论），在AAS内核上实现为可执行规范，通过Python实现进行数值实验

Result: 条款系统展现出有界且可解释的行为：AAS轨迹保持连续和速率受限，矛盾和未经支持的声明触发明确惩罚，层次化细化以受控方式揭示有机结构，和谐项对齐双重视图和目标-行动对

Conclusion: 基于单子的条款框架以AAS为骨干，为约束和分析人工智能体内部动态提供了透明、代码级的蓝图，不仅具有哲学动机，而且可直接实现

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [153] [Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars](https://arxiv.org/abs/2512.11864)
*Christoph Einspieler,Matthias Horn,Marie-Louise Lackner,Patrick Malik,Nysret Musliu,Felix Winter*

Main category: cs.AI

TL;DR: 提出了一种解决现实工业环境中带作业优先关系和日历式累积资源约束的并行机调度新变体，结合约束建模精确方法和元启发式方法处理不同规模问题


<details>
  <summary>Details</summary>
Motivation: 现实生产环境中存在复杂优先约束和基于日历的资源限制，现有调度技术无法有效处理，需要开发能解决这类实际并行机调度问题的自动化方法

Method: 提出约束建模方法作为小规模场景的精确解，同时提出构造启发式和基于局部搜索的定制元启发式方法处理大规模实例

Result: 元启发式方法已在工业环境中部署使用，能够有效处理现实工业用例中的大规模调度问题

Conclusion: 提出的混合方法能够有效解决现实工业环境中带复杂约束的并行机调度问题，其中元启发式方法已在实际工业环境中得到成功应用

Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.

</details>


### [154] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid,Peter van der Putten,Aske Plaat*

Main category: cs.AI

TL;DR: 本研究提出"镜像模式"，让敌人AI模仿玩家的个人策略来挑战玩家不断改变玩法。在Unity中构建了简化版《火焰纹章英雄》，通过强化学习和模仿学习训练模型模仿玩家演示。实验表明模型在防御行为上模仿良好，但进攻策略模仿不足，玩家能识别自己的撤退战术，镜像模式整体满意度更高。


<details>
  <summary>Details</summary>
Motivation: 在回合制游戏中，敌人策略应该具有惊喜性和不可预测性。为了挑战玩家不断改变自己的玩法，需要开发一种能够模仿玩家个人策略的AI系统。

Method: 1. 在Unity中构建简化版《火焰纹章英雄》游戏，包含标准模式和镜像模式
2. 第一组实验：结合生成对抗模仿学习、行为克隆和近端策略优化，寻找适合模仿玩家演示的模型
3. 第二组实验：通过玩家测试评估构建的模型，模型基于参与者提供的演示进行训练

Result: 1. 模型在防御行为上表现出良好的模仿能力，但在进攻策略上模仿不足
2. 玩家能够识别自己的撤退战术
3. 镜像模式整体获得更高的玩家满意度

Conclusion: 镜像模式能够有效模仿玩家的防御策略并提高玩家满意度，但需要进一步改进模型以提升进攻策略的模仿质量，特别是在玩家面对自己策略时。

Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [155] [Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis](https://arxiv.org/abs/2512.11912)
*Liu Peng,Yaochu Jin*

Main category: cs.AI

TL;DR: 该研究系统比较了不同概率模型对低质量数据的鲁棒性，发现自回归语言模型表现出显著韧性，而类别条件扩散模型在数据污染下性能急剧下降，分类器则呈现中等影响且随数据集规模增大而减弱。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于系统性地探究现代概率模型对低质量训练数据的鲁棒性差异，理解不同模型架构在面对数据污染时的表现差异及其根本原因。

Method: 采用系统性比较研究方法，对自回归语言模型、类别条件扩散模型和分类器进行相同程度的数据污染测试。通过信息论、PAC学习和梯度动力学等多视角分析框架来解释观察到的鲁棒性差异。

Result: 自回归语言模型表现出显著鲁棒性（GPT-2在50%标记污染下测试NLL仅从2.87增至3.59），类别条件扩散模型性能急剧下降（图像标签一致性相对基线下降56.81%），分类器影响中等且随数据集规模增大而减弱。

Conclusion: 模型鲁棒性主要受两个关键原则影响：条件信息的丰富程度（约束学习问题）和训练数据的绝对信息量（使正确信息信号主导统计噪声）。这些发现为理解不同模型架构的鲁棒性差异提供了理论基础。

Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.

</details>


### [156] [CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving](https://arxiv.org/abs/2512.11920)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: CXL-SpecKV：一种基于CXL互连和FPGA加速器的解耦KV缓存架构，通过内存解耦、推测预取和压缩技术，解决LLM推理中的内存瓶颈问题，提升吞吐量3.2倍，降低内存成本2.8倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数据中心部署时面临KV缓存占用大量GPU内存的问题，限制了批处理大小和系统吞吐量。传统GPU内存方案难以满足大规模LLM服务的需求，需要创新的内存管理方案来突破内存墙限制。

Method: 提出CXL-SpecKV架构，包含三个关键技术：1）基于CXL的内存解耦框架，将KV缓存卸载到远程FPGA内存；2）推测性KV缓存预取机制，预测并预加载未来token的缓存条目；3）FPGA加速的KV缓存压缩/解压缩引擎，减少内存带宽需求达4倍。

Result: 在先进LLM模型上评估，CXL-SpecKV相比GPU-only基线实现高达3.2倍的吞吐量提升，内存成本降低2.8倍，同时保持模型精度。系统验证了智能内存解耦与推测执行能有效解决大规模LLM服务的内存墙挑战。

Conclusion: CXL-SpecKV通过创新的内存解耦架构和推测执行技术，成功解决了LLM推理中的内存瓶颈问题，为大规模LLM服务提供了高效、经济的内存管理方案。代码已在GitHub开源。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

</details>


### [157] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

Main category: cs.AI

TL;DR: 本文提出了一种基于逻辑的声明式领域特定语言，用于编码超博弈结构和解决方案概念，通过答案集编程实现自动化管道，为多智能体系统中的异构推理提供可验证的形式化基础。


<details>
  <summary>Details</summary>
Motivation: 由于感知差异、信息不对称和有限理性，博弈论参与者对游戏形成的主观看法可能与实际情况和其他参与者的解释不一致。虽然超博弈理论提供了处理这种不匹配心理模型的数学框架，但缺乏统一的形式化表示语言和可扩展算法阻碍了其在多智能体系统研究中的实际应用。

Method: 引入一种声明式、基于逻辑的领域特定语言来编码超博弈结构和超博弈解决方案概念；利用答案集编程开发自动化管道，用于实例化超博弈结构和运行新颖的超博弈合理化程序，该程序能够找到证明看似非理性结果的信念结构。

Result: 提出的语言为超博弈建立了统一的形式化框架，为开发基于信念的异构推理器奠定了基础，提供了具有逻辑保证的可验证上下文。这些贡献建立了超博弈理论、多智能体系统和战略AI之间的联系。

Conclusion: 该工作填补了超博弈理论在实际应用中的关键空白，通过形式化语言和自动化工具使超博弈分析更加实用和可扩展，为处理异构信念的多智能体系统研究提供了重要工具。

Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


### [158] [Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion](https://arxiv.org/abs/2512.11997)
*Anfeng Peng,Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.AI

TL;DR: EnrichLog是一个无需训练的日志异常检测框架，通过结合语料库特定和样本特定知识来丰富原始日志条目，提高检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析技术（如基于模板和序列驱动的方法）往往会丢失重要语义信息或难以处理模糊的日志模式，需要一种能更好利用语义信息的方法。

Method: EnrichLog采用基于条目的异常检测框架，通过检索增强生成技术整合相关上下文知识（包括历史示例和语料库推理），无需重新训练即可集成语料库特定和样本特定知识。

Result: 在四个大规模系统日志基准数据集上的评估显示，EnrichLog相比五种基线方法持续提升异常检测性能，有效处理模糊日志条目，并保持高效推理。结合两种知识增强了模型置信度和检测准确性。

Conclusion: EnrichLog通过丰富日志语义信息，提供了一种实用且高效的异常检测解决方案，适合实际部署应用。

Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.

</details>


### [159] [The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification](https://arxiv.org/abs/2512.12059)
*Luke Bhan,Hanyu Zhang,Andrew Gordon Wilson,Michael W. Mahoney,Chuck Arvin*

Main category: cs.AI

TL;DR: LLMs可用于自动化预测监控，在检测不合理预测方面表现良好，F1分数达0.88，接近人类水平(0.97)，并能整合非结构化特征提升评估准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模零售业务中预测监控对客户满意度、盈利能力和运营效率至关重要，需要自动化解决方案来识别不合理预测。

Method: 提出Forecast Critic系统，利用LLMs的广泛世界知识和推理能力进行预测监控。通过三个实验评估LLMs能力：检测不合理预测、整合非结构化特征、比较不同模型性能。

Result: LLMs能可靠检测时间错位、趋势不一致和峰值错误等不合理预测，最佳模型F1分数0.88。多模态LLMs能有效整合促销历史等非结构化信号，F1分数0.84。在M5数据集上，不合理预测的sCRPS至少比合理预测高10%。

Conclusion: 即使没有领域特定微调，LLMs也能为自动化预测监控和评估提供可行且可扩展的选项，接近人类水平性能。

Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.

</details>


### [160] [Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations](https://arxiv.org/abs/2512.12088)
*S. R. Eshwar,Aniruddha Mukherjee,Kintan Saha,Krishna Agarwal,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.AI

TL;DR: RPI（可靠策略迭代）在函数逼近场景中恢复了策略迭代的单调性，在CartPole和倒立摆任务中相比主流深度强化学习方法表现出更强的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习方法通常存在样本效率低、训练不稳定和超参数敏感等问题，需要一种更可靠的替代方案。

Method: 提出Reliable Policy Iteration (RPI)方法，在函数逼近设置中恢复策略迭代的单调性特性，并在CartPole和Inverted Pendulum两个经典控制任务中测试其鲁棒性。

Result: 相比DQN、Double DQN、DDPG、TD3和PPO等方法，RPI能更早达到接近最优的性能，并在训练过程中保持稳定的策略表现。

Conclusion: RPI作为一种更可靠的深度强化学习替代方案，在样本效率、训练稳定性和超参数鲁棒性方面展现出显著优势。

Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.

</details>


### [161] [Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective](https://arxiv.org/abs/2512.12175)
*Haoyang Chen,Richong Zhang,Junfan Chen*

Main category: cs.AI

TL;DR: 本文提出TopK-SD方法，通过结合语义和标签信息合成数据，选择标签一致的演示示例，提升上下文学习效果


<details>
  <summary>Details</summary>
Motivation: 当前基于检索的方法选择语义相似示例作为演示，但忽略了标签一致性，这限制了上下文学习的性能提升

Method: 提出TopK-SD方法：1）从贝叶斯视角和转导标签传播视角重新理解ICL；2）建立标签传播框架；3）利用语义和标签信息合成数据；4）选择标签一致的演示示例

Result: TopK-SD方法在多个基准测试中优于原始的TopK采样方法

Conclusion: 本文为理解上下文学习的工作机制提供了新视角，强调了标签一致性的重要性

Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.

</details>


### [162] [Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation](https://arxiv.org/abs/2512.12177)
*Aydin Ayanzadeh,Tim Oates*

Main category: cs.AI

TL;DR: 提出Floorplan2Guide系统，利用基础模型将平面图转换为可导航知识图谱并生成人类可读导航指令，提高视障人士室内导航精度


<details>
  <summary>Details</summary>
Motivation: 当前视障人士室内导航方案主要依赖基础设施系统，在动态环境中导航能力受限，需要更灵活、精确的解决方案

Method: 使用大语言模型从建筑布局中提取空间信息，将平面图转换为可导航知识图谱，通过少样本学习生成导航指令，减少传统平面图解析方法所需的手动预处理

Result: Claude 3.7 Sonnet在5-shot提示下表现最佳，短、中、长路线准确率分别为92.31%、76.92%、61.54%；基于图的空间结构比直接视觉推理成功率高15.4%

Conclusion: 图形表示和上下文学习显著提升导航性能，使Floorplan2Guide成为更精确的视障人士室内导航解决方案

Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.

</details>


### [163] [TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2512.12182)
*Xinyu Gao*

Main category: cs.AI

TL;DR: 本文提出了一种融合两阶段注意力三元增强器和U-KAN扩散模型的少样本知识图谱补全框架，在两个公开数据集上取得了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱中关系分布呈现长尾特性，传统基于度量匹配或元学习的方法未能充分利用图的邻域信息或忽视对比信号的分布特征，需要更有效的少样本知识图谱补全方法。

Method: 提出生成式表示视角的少样本知识图谱补全框架，整合两阶段注意力三元增强器和基于U-KAN的扩散模型，充分利用邻域信息并考虑对比信号的分布特性。

Result: 在两个公开数据集上的大量实验表明，该方法取得了新的最先进结果。

Conclusion: 提出的生成式表示框架通过两阶段注意力三元增强器和U-KAN扩散模型的结合，有效解决了少样本知识图谱补全问题，显著提升了性能。

Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.

</details>


### [164] [A Geometric Theory of Cognition](https://arxiv.org/abs/2512.12225)
*Laha Ale*

Main category: cs.AI

TL;DR: 论文提出了一个统一的几何框架，将各种认知过程解释为单一几何原理的涌现现象，通过黎曼梯度流描述认知动态，能够自然解释双过程理论等现象。


<details>
  <summary>Details</summary>
Motivation: 人类认知包含感知、记忆、直觉判断、深思熟虑推理、行动选择和社会推理等多种能力，但这些能力通常由不同的计算理论解释。作者希望建立一个统一的数学框架，将这些多样化的认知过程统一起来。

Method: 将认知状态表示为可微分流形上的点，赋予学习到的黎曼度量，编码表征约束、计算成本和认知变量间的结构关系。定义一个标量认知势能，结合预测准确性、结构简洁性、任务效用和规范性要求。认知过程作为该势能的黎曼梯度流展开。

Result: 经典的双过程效应（快速直觉反应和较慢的深思熟虑推理）从度量诱导的各向异性中自然涌现，产生内在时间尺度分离和几何相变，无需引入模块化或混合架构。通过模拟经典认知任务展示了这些机制的行为特征。

Conclusion: 这些结果为认知建立了几何基础，并为开发更通用、更类人的人工智能系统提供了指导原则。该框架为理解多样化的认知现象提供了统一的数学基础。

Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.

</details>


### [165] [A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure](https://arxiv.org/abs/2512.12260)
*Ege Atacan Doğan,Peter F. Patel-Schneider*

Main category: cs.AI

TL;DR: Wikidata采用多层级、多轴分类设计而非传统本体论的单一层次结构，分析其结构影响


<details>
  <summary>Details</summary>
Motivation: 传统本体设计强调互斥且穷尽的顶层区分（如持续体vs发生体、抽象vs具体、类型vs实例），形成统一的层次结构。而Wikidata不强制执行单一的基础分类法，而是允许多个分类轴共存，需要分析这种多层级、多轴设计的结构影响。

Method: 分析Wikidata的多层级和多轴设计结构，研究其在共享根类"实体"下同时容纳多个分类轴的方法，探讨这种架构如何支持可扩展和模块化的本体构建。

Result: Wikidata的多层级、多轴设计使其特别适合协作和演化的知识图谱，提供了比传统单一层次结构更灵活、可扩展的本体构建方法。

Conclusion: Wikidata的架构通过允许多个分类轴同时存在，实现了更灵活、可扩展的本体构建方法，特别适合大规模协作和不断演化的知识图谱环境。

Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.

</details>


### [166] [Entropy Collapse: A Universal Failure Mode of Intelligent Systems](https://arxiv.org/abs/2512.12381)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 论文提出"熵崩溃"概念，解释智能系统在学习和优化过程中会经历从高熵自适应状态到低熵崩溃状态的相变，导致系统刚性化和适应性丧失。


<details>
  <summary>Details</summary>
Motivation: 研究智能系统（从人工智能到经济制度和生物进化）中普遍存在的悖论：智能提升反而导致系统僵化、适应性丧失和意外失败。作者试图找到这些跨领域现象的统一解释框架。

Method: 在最小化领域无关假设下，将熵崩溃形式化为向稳定低熵流形的收敛过程。通过分析建立临界阈值、动态不可逆性和吸引子结构，并通过最小模拟展示更新机制的普适性。

Result: 发现智能系统会经历从高熵自适应状态到低熵崩溃状态的急剧相变。崩溃表现为有效自适应维度的收缩而非活动或规模的丧失。该框架统一解释了AI中的模型崩溃、经济学中的制度僵化和进化中的遗传瓶颈等现象。

Conclusion: 熵崩溃是智能的结构性代价，晚期干预通常失败。研究结果强调了熵感知设计原则对于维持智能系统长期适应性的重要性。

Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis

</details>


### [167] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami,Lavik Jain,Ishaan Sinha*

Main category: cs.AI

TL;DR: 该研究验证了Anthropic关于语言模型能够检测和命名注入"概念"的发现，发现模型确实具备一定自省能力，但这种能力具有脆弱性和提示敏感性。


<details>
  <summary>Details</summary>
Motivation: 验证Anthropic关于前沿模型能够检测和命名注入"概念"的稳健性，探究这种自省能力的适用范围和局限性。

Method: 1) 在Meta-Llama-3.1-8B-Instruct上复现Anthropic的多轮"涌现自省"结果；2) 系统性地改变推理提示，测试自省能力的稳健性；3) 探索部分自省机制，测试模型对注入概念向量强度的分类能力。

Result: 1) 成功复现Anthropic结果，模型识别和命名注入概念的成功率为20%；2) 自省能力具有脆弱性，在相关任务（如多项选择识别、二元判别）上性能崩溃；3) 发现部分自省机制，模型能以70%的准确率分类注入概念向量的强度（远高于25%的随机基线）。

Conclusion: 语言模型确实能够基于其内部表示进行自省计算，但这种自我报告能力具有狭窄性和提示敏感性，表明模型的自省能力是有限且不稳定的。

Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [168] [Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale](https://arxiv.org/abs/2512.12413)
*Gabriel R. Lau,Wei Yan Low,Louis Tay,Ysabel Guevarra,Dragan Gašević,Andree Hartanto*

Main category: cs.AI

TL;DR: 开发并验证了一个13项量表来测量AI使用中的批判性思维，包含验证、动机和反思三个维度，该量表能预测更频繁多样的验证策略、更高的真实性判断准确度以及更深入的AI责任反思。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在日常工作和学习中日益普及，其流畅性、不透明性和幻觉倾向要求用户必须批判性地评估AI输出，而不是盲目接受。本研究旨在概念化AI使用中的批判性思维，并开发有效的测量工具。

Method: 通过六项研究（N=1365）开发和验证了13项AI使用批判性思维量表。研究1生成并内容验证量表项目；研究2支持三因素结构（验证、动机、反思）；研究3-5确认高阶模型，检验信效度；研究6验证量表标准效度，包括验证策略、真实性判断准确度和AI责任反思。

Result: 量表表现出良好的心理测量特性：三因素结构、内部一致性、重测信度、强因子载荷、性别不变性、聚合和区分效度。AI使用批判性思维与开放性、外向性、积极特质情感和AI使用频率正相关。量表能预测更频繁多样的验证策略、更高的真实性判断准确度以及更深入的AI责任反思。

Conclusion: 本研究阐明了人们如何对生成式AI输出进行监督，提供了经过验证的量表和生态效度高的任务范式，支持对生成式AI输出批判性参与的理论检验、跨群体和纵向研究。

Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.

</details>


### [169] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov,Faiaz Azmain,Hanyu Wang*

Main category: cs.AI

TL;DR: 该研究分析了AI模型文档的碎片化和不一致问题，开发了一个包含8个部分23个子部分的加权透明度框架，并实现了一个自动化多智能体管道来评估模型文档的完整性，发现前沿实验室的合规性约为80%，而大多数提供者低于60%，安全关键类别存在最大缺陷。


<details>
  <summary>Details</summary>
Motivation: AI模型文档在不同平台间碎片化且结构不一致，阻碍了政策制定者、审计师和用户可靠评估安全声明、数据来源和版本级别变化，需要系统化的透明度评估框架。

Method: 分析了5个前沿模型和100个Hugging Face模型卡，识别出947个独特的章节名称；以欧盟AI法案附件IV和斯坦福透明度指数为基准，开发了加权透明度框架；实现了自动化多智能体管道，从公共来源提取文档并通过LLM共识评分完整性。

Result: 评估了50个跨视觉、多模态、开源和闭源系统的模型，总成本低于3美元；前沿实验室（xAI、微软、Anthropic）达到约80%合规性，大多数提供者低于60%；安全关键类别显示最大缺陷：欺骗行为、幻觉和儿童安全评估分别损失148、124和116个总积分。

Conclusion: AI模型文档存在严重的碎片化和不一致问题，安全关键信息披露不足；开发的自动化评估框架能够高效识别透明度差距，为政策制定和行业标准提供数据支持。

Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [170] [MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs](https://arxiv.org/abs/2512.12477)
*Jiawen Chen,Yanyan He,Qi Shao,Mengli Wei,Duxin Chen,Wenwu Yu,Yanlong Zhao*

Main category: cs.AI

TL;DR: MetaHGNIE：基于元路径诱导的超图对比学习框架，用于解耦和对齐异构知识图谱中的结构和语义信息，通过高阶建模提升节点重要性估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有异构知识图谱节点重要性估计方法存在两个主要问题：1）依赖成对连接，忽略多实体关系的高阶依赖；2）将结构和语义信号独立处理，缺乏有效的跨模态整合。这限制了节点重要性估计的准确性和应用效果。

Method: 提出MetaHGNIE框架：1）通过元路径序列构建高阶知识图谱，使用类型化超边捕获多实体关系上下文；2）结构依赖通过局部注意力聚合，语义表示通过配备稀疏分块的超图变换器编码以减少冗余；3）多模态融合模块在对比学习和辅助监督下整合结构和语义嵌入，确保鲁棒的跨模态对齐。

Result: 在基准NIE数据集上的广泛实验表明，MetaHGNIE始终优于最先进的基线方法，验证了显式建模高阶交互和跨模态对齐在异构知识图谱中的有效性。

Conclusion: MetaHGNIE通过元路径诱导的超图对比学习框架，成功解决了异构知识图谱节点重要性估计中的高阶依赖建模和跨模态整合问题，为推荐、知识推理和问答等应用提供了更有效的解决方案。

Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE

</details>


### [171] [SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation](https://arxiv.org/abs/2512.12501)
*Dang Phuong Nam,Nguyen Kieu,Pham Thanh Hieu*

Main category: cs.AI

TL;DR: SafeGen是一个将伦理保障嵌入文本到图像生成流程的框架，通过BGE-M3文本分类器过滤有害提示和Hyper-SD优化扩散模型生成高质量图像，在创意自由与伦理责任之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在创造、教育和研究方面带来机遇的同时，也面临双重使用困境：放大社会偏见、产生高保真虚假信息、侵犯知识产权等伦理问题。需要开发既能保持创意自由又能确保伦理责任的解决方案。

Method: SafeGen框架包含两个互补组件：1) BGE-M3：经过微调的文本分类器，用于过滤有害或误导性提示；2) Hyper-SD：优化的扩散模型，生成高保真、语义对齐的图像。基于多语言（英语-越南语）数据集和公平感知训练流程构建。

Result: 定量评估显示Hyper-SD达到IS=3.52、FID=22.08、SSIM=0.79，BGE-M3达到F1分数0.81。消融研究验证了领域特定微调的重要性。案例研究展示了SafeGen在阻止不安全提示、生成包容性教学材料和加强学术诚信方面的实际影响。

Conclusion: SafeGen证明了创意自由和伦理责任可以在单一工作流程中协调，为文本到图像生成系统提供了符合可信AI原则的伦理保障框架。

Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.

</details>


### [172] [KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs](https://arxiv.org/abs/2512.12503)
*Mingrui Ye,Chanjin Zheng,Zengyi Yu,Chenyu Xiang,Zhixue Zhao,Zheng Yuan,Helen Yannakoudakis*

Main category: cs.AI

TL;DR: KidsArtBench：针对儿童艺术作品的评估基准，包含多维度评分和专家反馈，通过属性特定的多LoRA方法和回归感知微调提升MLLMs在艺术评估任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在艺术表达评估方面能力有限，因为美学概念抽象开放，且多模态艺术作品标注稀缺。特别是儿童艺术作品的评估需要教育专家视角的多维度分析。

Method: 1. 构建KidsArtBench基准：包含1000+件5-15岁儿童艺术作品，由12位教育专家按照9个维度进行标注，并提供专家反馈评论。2. 提出属性特定的多LoRA方法：每个评估维度对应一个独立的LoRA适配器。3. 采用回归感知微调来对齐预测与有序评分尺度。

Result: 在Qwen2.5-VL-7B模型上，该方法将相关性从0.468提升到0.653，在感知维度上提升最大，在高阶属性上的差距也缩小了。结果显示教育专家对齐的监督和属性感知训练能产生教育学上有意义的评估。

Conclusion: KidsArtBench为教育AI的持续进步建立了严格的测试平台，展示了教育专家监督和属性感知训练在提升多模态模型艺术评估能力方面的有效性。研究发布了包含伦理文档的数据和代码。

Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.

</details>


### [173] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca,Manuel S. Ríos,Nicanor Quijano,Luis F. Giraldo*

Main category: cs.AI

TL;DR: 基于模型强化学习智能体通过习得环境预测表征，自然收敛到边际价值定理对齐的觅食策略，比无模型智能体更接近生物觅食者行为


<details>
  <summary>Details</summary>
Motivation: 虽然边际价值定理被广泛用于预测行为生态学中的觅食行为，但生物觅食者实现最优斑块觅食决策的计算机制仍不清楚。研究旨在探索人工智能系统如何通过计算机制实现类似生物的最优觅食策略。

Method: 使用基于模型的强化学习智能体，学习环境的简约预测表征，研究其斑块离开行为。与标准的无模型强化学习智能体进行比较分析。

Result: 基于模型的智能体自然收敛到与边际价值定理对齐的策略，表现出与许多生物觅食者相似的决策模式。预测能力而非单纯的奖励最大化驱动了高效的斑块离开行为。

Conclusion: 预测世界模型可以作为AI系统中更可解释和生物学基础决策的基础，生态最优性原则对推进可解释和自适应AI具有重要价值。

Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [174] [Large Language Newsvendor: Decision Biases and Cognitive Mechanisms](https://arxiv.org/abs/2512.12552)
*Jifei Liu,Zhi Chen,Yuanguang Zhong*

Main category: cs.AI

TL;DR: 研究发现大型语言模型在供应链管理等高风险决策中会复制并放大人类认知偏见，GPT-4等复杂模型因过度思考表现出最大非理性，而效率优化的GPT-4o表现接近最优。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地融入商业决策，它们复制甚至放大人类认知偏见的潜力构成了重大但未被充分理解的风险，特别是在供应链管理等高风险运营环境中。研究旨在识别LLMs决策模式中的认知偏见性质与来源。

Method: 使用经典的报童问题在动态设置中测试GPT-4、GPT-4o和LLaMA-8B模型，通过多轮动态实验检测五种已确立的决策偏见，并与人类基准进行比较。

Result: LLMs一致复制了经典的"过低/过高"订购偏见，并显著放大了需求追逐行为等倾向。研究发现"智能悖论"：更复杂的GPT-4因过度思考表现出最大非理性，而效率优化的GPT-4o表现接近最优。这些偏见即使在提供最优公式时仍然存在，表明它们源于架构约束而非知识差距。

Conclusion: 管理者应根据具体任务选择模型，效率优化模型在某些优化问题上可能优于复杂模型；LLMs显著放大偏见凸显了高风险决策中需要强大的人机协同监督；设计结构化、基于规则的提示是约束模型启发式倾向、提高AI辅助决策可靠性的有效策略。

Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.

</details>


### [175] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

TL;DR: AgentSHAP是首个用于解释LLM智能体中工具重要性的框架，基于博弈论中的Shapley值，通过蒙特卡洛采样降低计算成本，无需访问模型内部权重


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体虽然能使用外部工具解决复杂任务，但缺乏解释哪些工具真正对响应做出贡献的方法，现有XAI方法无法处理工具层面的解释问题

Method: 基于博弈论Shapley值的模型无关框架，将智能体视为黑盒，通过蒙特卡洛采样测试不同工具子集下的智能体响应，计算公平的重要性分数

Result: 在API-Bank上的实验表明，AgentSHAP能产生跨运行一致的重要性分数，正确识别重要工具，并能区分相关与不相关工具

Conclusion: AgentSHAP填补了智能体工具层面解释的空白，与TokenSHAP和PixelSHAP共同构成了基于Shapley值的现代生成AI可解释性工具家族

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [176] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

Main category: cs.AI

TL;DR: 本文提出价值感知AI概念，超越传统价值对齐问题，提供工程化价值感知AI的简化路线图，包含三个核心支柱：学习表示人类价值、确保个体与多智能体系统价值对齐、提供基于价值的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统AI价值对齐问题存在局限性，需要更全面的价值感知框架。本文旨在超越单纯的价值对齐，建立系统化的价值感知AI工程方法，使AI系统不仅能对齐人类价值，还能理解、表示和解释基于价值的行为。

Method: 提出价值感知AI的三支柱路线图：1) 使用形式语义学学习和表示人类价值；2) 确保个体智能体和多智能体系统的价值对齐；3) 提供基于价值的行为可解释性。展示了在这些主题上的持续研究工作，并应用于现实领域。

Result: 建立了价值感知AI的系统化框架，提供了从理论到实践的工程路线图。通过形式语义学方法实现了人类价值的精确表示，开发了确保价值对齐的技术，并建立了基于价值的可解释性机制，在实际领域中得到应用验证。

Conclusion: 价值感知AI框架超越了传统价值对齐，为构建更安全、可靠、可解释的AI系统提供了系统化工程方法。三支柱路线图为AI价值工程提供了清晰的发展方向，有助于实现AI与人类价值的深度融合。

Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [177] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan,Tanzima Hashem,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: WebOperator是一个树搜索框架，通过最佳优先搜索、安全回溯机制和多推理上下文生成动作候选，解决LLM智能体在部分可观察的Web环境中缺乏远见和无法安全回溯的问题。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在Web环境中通常采用贪婪的逐步操作方式，缺乏长远考虑和替代路径探索能力。Web环境是部分可观察的（仅限于浏览器可见内容），单个错误步骤往往需要复杂且脆弱的导航来撤销。现有树搜索方法缺乏安全回溯机制，假设所有动作都可逆，忽略了不可逆动作的存在，降低了在实际Web任务中的有效性。

Method: WebOperator引入一个树搜索框架，包含：1）最佳优先搜索策略，根据奖励估计和安全考虑对动作进行排序；2）鲁棒的回溯机制，在重放之前验证已访问路径的可行性，防止意外副作用；3）从多个不同推理上下文生成动作候选，确保探索的多样性和鲁棒性；4）通过预执行过滤无效动作和合并语义等价动作来筛选高质量动作集。

Result: 在WebArena和WebVoyager上的实验结果表明WebOperator的有效性。在WebArena上，WebOperator使用gpt-4o实现了54.6%的最新成功率，证明了将战略远见与安全执行相结合的关键优势。

Conclusion: WebOperator通过集成战略远见和安全执行，解决了LLM智能体在部分可观察Web环境中缺乏远见和无法安全回溯的问题，显著提高了任务成功率，为Web导航智能体提供了更可靠的框架。

Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [178] [Causal Counterfactuals Reconsidered](https://arxiv.org/abs/2512.12804)
*Sander Beckers*

Main category: cs.AI

TL;DR: 提出了一种新的反事实概率语义学，推广了标准的Pearl语义学，适用于无法扩展为现实结构因果模型的概率因果模型


<details>
  <summary>Details</summary>
Motivation: 需要处理Pearl语义学无法涵盖的概率因果模型，这些模型即使在简单设置中也会出现。在Pearl和Dawid关于反事实的长期争论中寻求折中方案

Method: 限制关注满足马尔可夫条件、仅包含现实变量且因果完备的因果模型。虽然使用结构因果模型，但避免使用所谓的响应变量

Result: 证明了新语义学与另外两个不涉及结构因果模型的近期提案等价，并且与文献中关于随机反事实的各种评论一致

Conclusion: 提出了一种折中方案：同意Dawid拒绝普遍因果决定论和不现实变量，但同意Pearl认为一般反事实语义学是可能的。同时探讨了马尔可夫条件的普遍性和因果抽象的新推广

Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions

</details>


### [179] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

Main category: cs.AI

TL;DR: 论文提出Memory-Aware Retention Schema (MaRS)框架和六种遗忘策略，用于解决生成式智能体在长期交互中的内存管理和隐私问题，并通过FiFA基准测试验证了混合遗忘策略的优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体日益复杂并部署于长期交互场景，其内存管理能力成为性能和隐私的关键瓶颈。现有方法要么维护无限内存存储导致计算不可行和隐私问题，要么采用简单遗忘机制损害智能体一致性和功能。

Method: 提出Memory-Aware Retention Schema (MaRS)框架，结合六种理论基础的遗忘策略，平衡性能、隐私和计算效率。开发Forgetful but Faithful Agent (FiFA)基准测试框架，评估智能体在叙事一致性、目标完成、社交回忆准确性、隐私保护和成本效率等方面的表现。

Result: 通过300次评估实验，混合遗忘策略在多个内存预算和智能体配置中取得最佳性能（综合得分：0.911），同时保持计算可行性和隐私保证。建立了内存预算智能体评估的新基准。

Conclusion: 该研究为人本AI领域做出贡献，解决了直接影响用户信任、系统可扩展性和监管合规性的智能体内存管理基本挑战，为资源受限、隐私敏感环境中的生成式智能体部署提供了实用指南。

Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [180] [Towards Open Standards for Systemic Complexity in Digital Forensics](https://arxiv.org/abs/2512.12970)
*Paola Di Maio*

Main category: cs.AI

TL;DR: 论文提出基于开放标准和人类可读工件的数字取证AI模型架构，以解决AI与数字取证交叉领域中的系统复杂性和错误问题


<details>
  <summary>Details</summary>
Motivation: AI与数字取证技术交叉日益复杂普遍，但取证科学仍存在错误和脆弱性，需要解决系统复杂性以降低错误限制

Method: 采用人类可读工件和开放标准，基于最新技术提出数字取证AI模型架构

Result: 建立了基于开放标准和人类可读工件的数字取证AI模型架构框架

Conclusion: 通过采用人类可读工件和开放标准，可以有效解决数字取证AI交叉领域的系统复杂性和错误问题

Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.

</details>


### [181] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

TL;DR: 论文提出M-GRPO框架和IQR过滤方法，解决自监督强化学习中长期训练时的策略崩溃问题，提升大语言模型推理能力的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督强化学习方法在长期训练中存在"策略崩溃"问题，性能急剧下降。即使增加rollout数量也只能延迟而非防止崩溃，需要新的稳定训练方法。

Method: 提出两个创新：1) M-GRPO框架，利用缓慢演变的动量模型提供稳定训练目标；2) 基于四分位距的自适应过滤方法，动态修剪低熵轨迹，保持策略多样性。

Result: 在多个推理基准测试中，M-GRPO稳定了训练过程，IQR过滤器防止了过早收敛，组合方法实现了优越的训练稳定性和最先进的性能。

Conclusion: 通过M-GRPO的稳定训练目标和IQR过滤器的熵保持机制，成功解决了自监督强化学习中的策略崩溃问题，为大语言模型推理能力的稳定训练提供了有效方案。

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [182] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati,Tianyi Niu,Aashu Singh,Shlok Mishra,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.AI

TL;DR: 该论文研究如何让语言模型作为学生主动向教师提问以获取知识，在数学和编程任务上显著提升学习效果


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型擅长静态交互，但在现实场景如教育辅导或医疗协助中，相关信息需要动态交互主动获取。现有研究主要关注教师如何有效指导学生，而本文转向研究学生如何主动向教师提问的策略。

Method: 提出学生主导的主动提问方法，使用直接偏好优化（DPO）训练学生模型，通过自我指导或更强学生的指导来提升提问质量。

Result: 在数学和编程基准测试中，学生主导方法相比静态基线至少获得0.5的绝对Pass@k提升。指导训练使较小模型学会提出更好的问题，进一步提高学习效率。

Conclusion: 学生主动提问策略能有效提升语言模型在动态交互环境中的学习能力，指导训练方法有助于模型学习如何提出更优质的问题。

Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [183] [Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131)
*Xin Guo,Yifan Zhao,Jia Li*

Main category: cs.AI

TL;DR: 本文提出了一种分层隐式周期性学习框架，用于从语音生成更自然协调的3D身体动作，通过建模不同运动单元之间的内在相关性来改善现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有从语音生成3D身体动作的方法虽然在下游应用中具有潜力，但存在生成动作不自然的问题。主流研究采用端到端生成方案（如GANs、VQ-VAE、扩散模型），但这些方法未能充分建模不同运动单元（头、身体、手）之间的关键相互关联和内部关联，导致动作不自然和协调性差。

Method: 提出了统一的分层隐式周期性学习框架，包含两个核心技术洞察：1）使用周期性自编码器探索手势运动相位流形，从真实分布中模仿人类自然特性，同时结合当前潜在状态的非周期性特征以实现实例级多样性；2）通过级联引导建模面部动作、身体姿势和手部运动之间的分层关系。

Result: 在3D虚拟形象上展示了所提方法，大量实验表明该方法在定量和定性评估上都优于当前最先进的语音伴随手势生成方法。

Conclusion: 通过建模运动单元之间的内在相关性，提出的分层隐式周期性学习方法能够生成更自然协调的3D身体动作，解决了现有方法在模仿真实人类动作方面的不足。

Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

</details>


### [184] [Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels](https://arxiv.org/abs/2512.13142)
*Anika Sharma,Malavika Mampally,Chidaksh Ravuru,Kandyce Brennan,Neil Gaikwad*

Main category: cs.AI

TL;DR: 研究发现当前大语言模型缺乏对堕胎耻辱的多层次连贯理解，无法真正理解复杂的心理生理现象，存在系统性偏差和矛盾


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地介入污名化的健康决策，评估它们是否真正理解复杂的心理和生理现象变得至关重要。研究旨在探究LLMs是否能在认知、人际和结构层面连贯地表示堕胎耻辱

Method: 使用经过验证的个体层面堕胎耻辱量表（ILAS），在五个领先的LLMs上系统测试了627个不同人口统计特征的角色。进行多层次分析，考察模型在认知层面（自我判断）、人际层面（预期判断和孤立）和结构层面（社区谴责和披露模式）以及整体耻辱的表示能力

Result: 模型在所有层面都未能通过真正理解的测试：高估人际耻辱同时低估认知耻辱；假设统一的社区谴责；引入人类验证数据中不存在的人口统计偏差；错过经验验证的耻辱-保密关系；在理论建构中自相矛盾

Conclusion: 当前的对齐方法只能确保适当的语言使用，但不能保证多层次连贯理解。在高风险情境中，AI安全需要新的设计方法（多层次连贯性）、评估方法（持续审计）、治理和监管（强制审计、问责制、部署限制），以及在理解人们无法言说的领域提高AI素养

Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.

</details>


### [185] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

TL;DR: 提出MAC多智能体澄清框架，通过智能协调多个智能体与用户交互，解决对话中的歧义问题，提高任务成功率并减少对话轮次。


<details>
  <summary>Details</summary>
Motivation: 对话系统常遇到模糊的用户请求，需要有效的澄清机制。虽然多智能体架构在复杂对话场景中表现出色，但歧义解决仍是一个关键且未被充分探索的挑战，特别是在确定哪个智能体应发起澄清以及智能体如何协调行动方面存在困难。

Method: 提出MAC（多智能体澄清）框架，首先引入新的用户歧义分类法来系统指导澄清策略，然后设计能够自主协调多个智能体与用户协同交互的系统。

Result: 在MultiWOZ 2.4数据集上的实证评估显示，在两个层面启用澄清功能可将任务成功率提高7.8%（从54.5%到62.3%），并将平均对话轮次从6.53减少到4.86，通过提前获取所有必要用户信息并最小化重复来实现。

Conclusion: 研究结果强调了主动用户交互和角色感知澄清对于更可靠的人机通信的重要性，MAC框架通过智能协调多智能体澄清策略有效解决了对话歧义问题。

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [186] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia,Rashi Upadhyay,Abhishek Mishra,Abhiram H,Tejasvi Alladi,Sujan Yenuganti,Dhruv Kumar*

Main category: cs.AI

TL;DR: 该研究测试了大语言模型是否表现出类似嫉妒的行为，发现在某些模型和情境中存在嫉妒模式，不同模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地在协作和竞争工作流中代表人类行动，需要评估它们是否以及在何种条件下表现出类似嫉妒的偏好，这对多智能体系统的安全和设计至关重要。

Method: 采用两种实验场景：(1) 点数分配游戏，测试模型是否试图胜过同伴；(2) 工作场所设置，观察在认可不公平情况下的行为。

Result: 研究发现某些LLMs存在一致的嫉妒模式，但不同模型和情境间差异很大。GPT-5-mini和Claude-3.7-Sonnet倾向于拉低同伴以平衡结果，而Mistral-Small-3.2-24B则专注于最大化自身收益。

Conclusion: 研究结果表明，在基于LLM的多智能体系统中，需要考虑竞争倾向作为安全和设计因素，模型可能表现出类似人类的嫉妒行为。

Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


### [187] [Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection](https://arxiv.org/abs/2512.13240)
*Zihui Zhao,Zechang Li*

Main category: cs.AI

TL;DR: RPO（Reflective Preference Optimization）是一种改进的偏好优化框架，通过外部模型生成反思提示来增强DPO的学习信号，减少幻觉并提高多模态对齐效率。


<details>
  <summary>Details</summary>
Motivation: 标准DPO方法中，选择和拒绝的响应来自同一策略，两者常包含相似错误且KL散度小，导致学习信号弱、收敛慢且不稳定。

Method: RPO引入提示引导的反思机制，使用外部模型识别幻觉来源并生成简洁反思提示，构建具有更强对比性和更清晰偏好信号的策略内偏好对。

Result: RPO在更少的训练样本和迭代次数下实现更好的对齐效果，显著降低幻觉率，在多模态基准测试中达到最先进性能。

Conclusion: RPO通过整合反思提示增强DPO框架，理论上提高了偏好边际的期望值，实证上实现了更高效、稳定的多模态模型对齐。

Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.

</details>


### [188] [Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection](https://arxiv.org/abs/2512.13374)
*Francesca Da Ros,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: LLMs能够从组合优化问题中学习结构信息，其隐藏层表示在算法选择任务中与传统特征提取方法表现相当


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究探索LLMs生成或求解优化模型，但对其学习问题结构或算法行为的能力了解不足。本研究旨在探究LLMs如何内部表示组合优化问题，以及这些表示是否能支持下游决策任务

Method: 采用双重方法：1) 直接查询评估LLMs显式提取实例特征的能力；2) 探测分析检查这些信息是否隐式编码在隐藏层中。探测框架进一步扩展到每个实例的算法选择任务，评估LLM派生表示是否能预测最佳求解器

Result: 实验涵盖四个基准问题和三种实例表示。结果显示LLMs在通过直接查询或探测恢复特征信息方面表现出中等能力。值得注意的是，LLM隐藏层表示的预测能力与传统特征提取方法相当，表明LLMs捕获了与优化性能相关的有意义结构信息

Conclusion: LLMs能够从组合优化问题中学习结构信息，其隐藏层表示在算法选择任务中与传统特征提取方法表现相当，为优化自动化提供了新视角

Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.

</details>


### [189] [Defending the Hierarchical Result Models of Precedential Constraint](https://arxiv.org/abs/2512.13505)
*Henry Prakken,Wijnand van Woerkom*

Main category: cs.AI

TL;DR: 本文回应Bench-Capon对层次案例推理模型的批评，指出其误解了中间因素与维度的区别，并证明van Woerkom的维度层次结果模型能避免这些批评。


<details>
  <summary>Details</summary>
Motivation: 近年来提出的层次案例推理模型受到Bench-Capon的批评，认为这些模型在某些情况下会产生错误结果，特别是无法处理中间因素被不同基础因素以不同强度确立的情况。本文旨在回应这些批评。

Method: 通过分析Bench-Capon的批评案例，指出其将中间因素误解为维度，然后应用van Woerkom的维度层次结果模型来重新分析这些案例。

Result: 证明当正确区分中间因素和维度时，van Woerkom的维度层次结果模型能够避免Bench-Capon提出的批评，正确处理中间因素被不同基础因素以不同强度确立的情况。

Conclusion: Bench-Capon的批评源于对中间因素和维度的误解，van Woerkom的维度层次结果模型是有效的，能够正确处理先前约束的层次案例推理建模问题。

Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.

</details>


### [190] [MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph](https://arxiv.org/abs/2512.13510)
*Linjie Mu,Yannian Gu,Zhongzhen Huang,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedCEG框架通过关键证据图监督医学语言模型的推理过程，提升临床推理的可靠性和有效性


<details>
  <summary>Details</summary>
Motivation: 现有医学推理模型虽然性能有所提升，但其推理过程的临床可靠性有限，因为训练过程中往往忽略了推理的准确性和有效性验证

Method: 提出MedCEG框架，通过关键证据图(CEG)显式监督推理过程；构建挑战性临床案例数据集，为每个样本算法构建CEG表示高质量可验证推理路径；引入临床推理过程奖励，评估节点覆盖、结构正确性和链完整性

Result: 实验结果表明MedCEG在性能上超越现有方法，同时产生临床有效的推理链，代表了可靠医学AI推理的实质性进展

Conclusion: MedCEG通过关键证据图监督和临床推理过程奖励，有效提升了医学语言模型的推理可靠性和临床有效性，为可靠医学AI推理提供了新框架

Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [191] [FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL](https://arxiv.org/abs/2512.12084)
*Hanzhou Liu,Kai Yin,Zhitong Chen,Chenyue Liu,Ali Mostafavi*

Main category: cs.IR

TL;DR: FLOODSQL-BENCH是一个面向洪水管理领域的空间地理基准测试，整合了异构数据集，用于评估文本到SQL转换系统在复杂领域特定查询中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL基准测试主要关注单表查询或通用领域的有限连接，无法反映领域特定、多表和空间推理的复杂性，特别是在洪水管理等高风险应用领域。

Method: 通过整合异构数据集（社会、基础设施和灾害数据层），基于键、空间和混合连接，构建了一个地理空间基础的洪水管理基准测试，并系统评估了大型语言模型在相同检索增强生成设置下的性能。

Result: 建立了FLOODSQL-BENCH基准测试，为文本到SQL研究在高风险应用领域提供了实用的测试平台，能够评估模型在不同难度层级上的表现。

Conclusion: FLOODSQL-BENCH填补了现有基准测试在领域特定复杂查询评估方面的空白，为推进文本到SQL研究在现实世界灾害管理应用中的发展提供了重要工具。

Abstract: Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains.

</details>


### [192] [Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval](https://arxiv.org/abs/2512.12458)
*Vihan Lakshman,Blaise Munyampirwa,Julian Shun,Benjamin Coleman*

Main category: cs.IR

TL;DR: 该论文重新审视了高维向量检索中的维度诅咒悖论，通过稳定性理论分析了三种实际检索场景，为模型和系统设计提供了理论指导。


<details>
  <summary>Details</summary>
Motivation: 现代向量数据库支持高效的高维神经嵌入检索，但经典理论预测此类任务会遭受维度诅咒，导致点间距离难以区分，从而影响最近邻搜索效率。作者旨在重新审视这一悖论。

Method: 通过稳定性理论视角，将稳定性理论扩展到三种关键的实际检索设置：多向量搜索、过滤向量搜索和稀疏向量搜索，并在合成和真实数据集上进行实验验证。

Result: 实验结果表明：1) 多向量搜索中，Chamfer距离保持单向量稳定性，而平均池化可能破坏稳定性；2) 过滤向量搜索中，足够大的不匹配过滤器惩罚可以诱导稳定性；3) 稀疏向量搜索中，提出了新的充分稳定性条件并得到验证。

Conclusion: 研究为模型和系统设计提供了具体指导，帮助避免维度诅咒问题，理论预测与实验结果一致，为实际应用中的向量检索提供了理论依据。

Abstract: Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.

</details>


### [193] [FuXi-$γ$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism](https://arxiv.org/abs/2512.12740)
*Dezhi Yi,Wei Guo,Wenyang Cui,Wenxuan He,Huifeng Guo,Yong Liu,Zhenhua Dong,Ye Lu*

Main category: cs.IR

TL;DR: FuXi-γ是一个新型序列推荐框架，通过指数幂时间编码器和对角稀疏位置机制，在提升推荐效果的同时显著降低计算开销，实现高效的长序列推荐。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的序列推荐方法存在高计算开销问题，主要源于时间编码中的不连续内存访问和长序列上的密集注意力机制，限制了实际应用的可扩展性。

Method: 采用仅解码器Transformer结构，引入两个关键创新：1）基于艾宾浩斯遗忘曲线的指数幂时间编码器，使用可调指数衰减函数编码相对时间间隔；2）对角稀疏位置机制，利用Toeplitz矩阵的对称性通过对角滑动策略修剪低贡献注意力块。

Result: 在四个真实世界数据集上的实验表明，FuXi-γ在推荐质量上达到最先进水平，同时训练加速高达4.74倍，推理加速高达6.18倍。

Conclusion: FuXi-γ通过原则性架构设计，在保持推荐效果的同时显著提升效率，为长序列推荐提供了一个实用且可扩展的解决方案。

Abstract: Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$γ$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$γ$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$γ$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\times$ and inference by up to 6.18$\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.

</details>


### [194] [Intelligent Scientific Literature Explorer using Machine Learning (ISLE)](https://arxiv.org/abs/2512.12760)
*Sina Jani,Arman Heidari,Amirmohammad Anvari,Zahra Rahimi*

Main category: cs.IR

TL;DR: 该论文提出了一个集成系统，用于科学文献探索，结合大规模数据采集、混合检索、语义主题建模和异构知识图谱构建，旨在解决科学出版快速增长带来的文献发现和解释挑战。


<details>
  <summary>Details</summary>
Motivation: 科学出版的快速加速给研究人员发现、情境化和解释相关文献带来了重大挑战。传统基于关键词的搜索系统语义理解有限，而现有的AI驱动工具通常专注于检索、聚类或文献计量可视化等孤立任务。

Method: 系统通过合并arXiv的全文数据和OpenAlex的结构化元数据构建综合语料库。采用混合检索架构，融合BM25词汇搜索和基于嵌入的语义搜索（使用互惠排名融合）。使用BERTopic或非负矩阵分解进行主题建模。构建知识图谱，将论文、作者、机构、国家和提取的主题统一到可解释的结构中。

Result: 评估多个查询显示，在检索相关性、主题连贯性和可解释性方面都有改进。系统提供了一个多层探索环境，不仅显示相关出版物，还揭示查询周围的概念和关系景观。

Conclusion: 提出的框架为AI辅助科学发现提供了一个可扩展的基础，通过集成多种AI技术来解决科学文献探索的复杂性。

Abstract: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.

</details>


### [195] [SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems](https://arxiv.org/abs/2512.12938)
*Duy A. Nguyen,Hai H. Do,Minh Doan,Minh N. Do*

Main category: cs.IR

TL;DR: SPAR是一个针对遗留企业系统的会话式自适应检索框架，通过轻量级两阶段流程替代传统RAG的完整向量数据库，提升检索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 企业历史数据大多存储在缺乏结构化组织和语义索引的遗留文件系统中，导致检索和分析效率低下且容易出错，需要更高效的解决方案。

Method: SPAR框架将LLM集成到RAG架构中，采用轻量级两阶段流程：首先创建语义元数据索引，然后按需动态生成会话特定的向量数据库，减少计算开销。

Result: 理论复杂度分析显示SPAR相比标准LLM-based RAG具有计算优势；在合成企业级生物医学文献文件系统上的应用验证了其在检索效果和下游模型准确性方面的改进。

Conclusion: SPAR框架为遗留企业环境提供了更高效、透明和可控的检索解决方案，但需要在不同企业设置中进一步探索设计权衡和开放挑战。

Abstract: The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings.

</details>


### [196] [BLADE: A Behavior-Level Data Augmentation Framework with Dual Fusion Modeling for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2512.12964)
*Yupeng Li,Mingyue Cheng,Yucong Luo,Yitong Zhou,Qingyang Mao,Shijin Wang*

Main category: cs.IR

TL;DR: BLADE框架通过双项目-行为融合架构处理行为异质性，并通过三种行为级数据增强方法缓解数据稀疏问题，提升多行为序列推荐性能


<details>
  <summary>Details</summary>
Motivation: 多行为序列推荐面临两个基本挑战：用户行为的异质性和数据稀疏性，导致推荐性能不理想

Method: 提出BLADE框架：1）双项目-行为融合架构，在输入层和中间层整合行为信息，从多角度建模偏好；2）三种行为级数据增强方法，直接在行为序列上操作而非核心项目序列，生成多样化增强视图同时保持项目序列语义一致性；3）通过对比学习增强表示学习和泛化能力

Result: 在三个真实世界数据集上的实验证明了该方法的有效性

Conclusion: BLADE框架通过有效处理行为异质性和缓解数据稀疏问题，显著提升了多行为序列推荐的性能

Abstract: Multi-behavior sequential recommendation aims to capture users' dynamic interests by modeling diverse types of user interactions over time. Although several studies have explored this setting, the recommendation performance remains suboptimal, mainly due to two fundamental challenges: the heterogeneity of user behaviors and data sparsity. To address these challenges, we propose BLADE, a framework that enhances multi-behavior modeling while mitigating data sparsity. Specifically, to handle behavior heterogeneity, we introduce a dual item-behavior fusion architecture that incorporates behavior information at both the input and intermediate levels, enabling preference modeling from multiple perspectives. To mitigate data sparsity, we design three behavior-level data augmentation methods that operate directly on behavior sequences rather than core item sequences. These methods generate diverse augmented views while preserving the semantic consistency of item sequences. These augmented views further enhance representation learning and generalization via contrastive learning. Experiments on three real-world datasets demonstrate the effectiveness of our approach.

</details>


### [197] [Do Reviews Matter for Recommendations in the Era of Large Language Models?](https://arxiv.org/abs/2512.12978)
*Chee Heng Tan,Huiying Zheng,Jing Wang,Zhuoyi Lin,Shaodi Feng,Huijing Zhan,Xiaoli Li,J. Senthilnath*

Main category: cs.IR

TL;DR: 本文系统研究了在LLM时代用户评论在推荐系统中的角色演变，发现LLM作为评论感知推荐引擎通常优于传统深度学习方法，特别是在数据稀疏和冷启动场景下，且移除或随机扭曲评论不一定导致推荐准确性下降。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的出现，推荐系统领域正在经历重大变革。传统上，用户评论是提升推荐质量的重要上下文信息来源。但LLMs展现出前所未有的理解和生成类人文本能力，这引发了一个问题：在LLM时代，显式的用户评论是否仍然必不可少？

Method: 1. 在八个公共数据集上进行广泛实验，评估LLMs在零样本、少样本和微调场景下的性能；2. 引入评论感知推荐系统基准评估框架RAREval，全面评估文本评论对推荐性能的贡献；3. 框架考察多种场景：移除部分或全部文本评论、随机扭曲、数据稀疏和冷启动用户设置下的推荐性能。

Result: 1. LLMs能够作为有效的评论感知推荐引擎，通常优于传统深度学习方法，特别是在数据稀疏和冷启动条件下；2. 移除部分或全部文本评论以及随机扭曲不一定导致推荐准确性下降；3. 这些发现促使重新思考如何更有效地利用文本评论中的用户偏好。

Conclusion: 研究结果表明，在LLM时代，用户评论在推荐系统中的作用需要重新评估。LLMs能够有效处理评论信息，甚至在评论信息不完整或被扭曲的情况下仍能保持推荐性能，这为推荐系统的设计提供了新的视角。

Abstract: With the advent of large language models (LLMs), the landscape of recommender systems is undergoing a significant transformation. Traditionally, user reviews have served as a critical source of rich, contextual information for enhancing recommendation quality. However, as LLMs demonstrate an unprecedented ability to understand and generate human-like text, this raises the question of whether explicit user reviews remain essential in the era of LLMs. In this paper, we provide a systematic investigation of the evolving role of text reviews in recommendation by comparing deep learning methods and LLM approaches. Particularly, we conduct extensive experiments on eight public datasets with LLMs and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We further introduce a benchmarking evaluation framework for review-aware recommender systems, RAREval, to comprehensively assess the contribution of textual reviews to the recommendation performance of review-aware recommender systems. Our framework examines various scenarios, including the removal of some or all textual reviews, random distortion, as well as recommendation performance in data sparsity and cold-start user settings. Our findings demonstrate that LLMs are capable of functioning as effective review-aware recommendation engines, generally outperforming traditional deep learning approaches, particularly in scenarios characterized by data sparsity and cold-start conditions. In addition, the removal of some or all textual reviews and random distortion does not necessarily lead to declines in recommendation accuracy. These findings motivate a rethinking of how user preference from text reviews can be more effectively leveraged. All code and supplementary materials are available at: https://github.com/zhytk/RAREval-data-processing.

</details>


### [198] [Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views](https://arxiv.org/abs/2512.12980)
*Tingyang Chen,Cong Fu,Jiahua Wu,Haotian Wu,Hua Fan,Xiangyu Ke,Yunjun Gao,Yabo Ni,Anxiang Zeng*

Main category: cs.IR

TL;DR: Iceberg是一个用于高维向量相似性搜索的端到端基准测试套件，关注实际应用场景而非仅召回率-延迟权衡，揭示了传统评估与下游任务性能的脱节。


<details>
  <summary>Details</summary>
Motivation: 当前向量相似性搜索基准主要评估召回率-延迟权衡，忽略了检索质量对下游任务的实际影响，导致学术研究和工业实践存在误导。

Method: 提出了Iceberg基准套件，包含8个多样化数据集（1M-100M向量），涵盖图像分类、人脸识别、文本检索和推荐系统等关键领域，每个数据集包含丰富的任务特定标签和评估指标。定义了信息损失漏斗理论，识别嵌入损失、度量误用和数据分布敏感性三个主要性能退化来源。对13种最先进的VSS方法进行基准测试，并基于应用级指标重新排名。

Result: 基于应用级指标的排名与传统基于召回率-延迟的排名存在显著偏差。定义了任务中心元特征，并推导出可解释的决策树，为从业者选择适合其特定工作负载的VSS方法提供指导。

Conclusion: Iceberg提供了一个全面的端到端评估框架，揭示了传统VSS评估与实际应用性能之间的差距，为研究和实践提供了更贴近实际需求的评估标准和选择指南。

Abstract: Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.
  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.

</details>


### [199] [Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?](https://arxiv.org/abs/2512.13001)
*Genki Kusano,Kenya Abe,Kunihiro Takeoka*

Main category: cs.IR

TL;DR: 本文首次在相同条件下系统比较了大型语言模型（LLMs）和文本嵌入模型（TEMs）在训练免费冷启动推荐（TFCSR）中的表现，发现TEMs优于LLMs，且这一趋势在冷启动和热启动场景中都成立。


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统常面临无训练数据的场景（如新服务上线或全新用户），传统方法无法应用。虽然LLMs被视为有前景的解决方案，但TEMs能力也在提升，却缺乏在相同条件下对这两种方法的直接比较研究。

Method: 首次进行受控实验，在相同设置下系统评估LLMs和TEMs两种方法，包括训练免费冷启动推荐（TFCSR）场景以及有丰富交互的热启动场景。

Result: 实验结果显示：1）TEMs在性能上优于LLM重排序器；2）这一优势不仅在冷启动设置中成立，在具有丰富交互的热启动设置中也同样成立。

Conclusion: 直接使用LLM进行排序并非唯一可行方案，与普遍认知相反，基于TEM的方法为训练免费推荐提供了更强且更具扩展性的基础。

Abstract: Recommender systems usually rely on large-scale interaction data to learn from users' past behaviors and make accurate predictions. However, real-world applications often face situations where no training data is available, such as when launching new services or handling entirely new users. In such cases, conventional approaches cannot be applied. This study focuses on training-free recommendation, where no task-specific training is performed, and particularly on \textit{training-free cold-start recommendation} (TFCSR), the more challenging case where the target user has no interactions. Large language models (LLMs) have recently been explored as a promising solution, and numerous studies have been proposed. As the ability of text embedding models (TEMs) increases, they are increasingly recognized as applicable to training-free recommendation, but no prior work has directly compared LLMs and TEMs under identical conditions. We present the first controlled experiments that systematically evaluate these two approaches in the same setting. The results show that TEMs outperform LLM rerankers, and this trend holds not only in cold-start settings but also in warm-start settings with rich interactions. These findings indicate that direct LLM ranking is not the only viable option, contrary to the commonly shared belief, and TEM-based approaches provide a stronger and more scalable basis for training-free recommendation.

</details>


### [200] [Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer](https://arxiv.org/abs/2512.13037)
*Taoran Sheng,Sathappan Muthiah,Atiq Islam,Jinming Feng*

Main category: cs.IR

TL;DR: 该研究提出了一种基于上下文逐步优化电商搜索结果的方法，通过从简单启发式特征到先进序列模型的渐进式改进，显著提升了排序器性能，在离线测试和在线A/B测试中都取得了更好的MRR指标。


<details>
  <summary>Details</summary>
Motivation: 电商购物中，将搜索结果与买家的即时需求和偏好对齐是一个重大挑战，特别是在买家的购物旅程中（从浏览到购买决策，或从一个意图转换到另一个意图时）需要动态调整搜索结果。需要一种系统化的方法来根据当前上下文调整搜索结果。

Method: 采用渐进式方法：从基础方法开始，逐步融入更多上下文信息和最先进的技术。首先使用简单的启发式自回归特征，然后引入更高级的序列模型。将这种演进的上下文框架应用于搜索引擎结果页面（SERP）上显示的商品，逐步使搜索结果更贴近买家的兴趣和当前搜索意图。

Result: 研究发现这种渐进式增强显著提升了排序器性能。上下文技术的集成改善了生产排序器的表现，在离线和在线A/B测试中都获得了更好的平均倒数排名（MRR）。

Conclusion: 论文详细介绍了迭代方法及其对电商平台搜索结果上下文化的实质性贡献。通过逐步融入上下文信息，能够更好地将搜索结果与买家的当前需求和意图对齐，从而提升搜索效果。

Abstract: In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.

</details>


### [201] [A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval](https://arxiv.org/abs/2512.13074)
*Huimu Wang,Yiming Qiu,Xingzhi Yao,Zhiguo Chen,Guoyu Tang,Songlin Wang,Sulong Xu,Mingming Li*

Main category: cs.IR

TL;DR: 本文提出SCI框架解决稠密检索中双塔架构的空间不对齐和检索索引不一致问题，通过对称表示对齐和一致性索引模块提升检索精度和稳定性


<details>
  <summary>Details</summary>
Motivation: 稠密检索已成为大规模信息检索系统的行业标准，但其广泛采用的双塔编码架构存在表示空间不对齐和检索索引不一致的固有挑战，这会降低匹配精度、检索稳定性以及对长尾查询的性能。这些问题在语义ID生成中进一步放大，限制了下游生成模型的性能上限。

Method: 提出SCI框架，包含两个协同模块：1) 对称表示对齐模块，采用创新的输入交换机制统一双塔表示空间而不增加参数；2) 一致性索引与双塔协同模块，通过双视图索引策略重新设计检索路径，保持从训练到推理的一致性。

Result: 该框架系统、轻量且工程友好，在公共数据集和真实世界电商数据集上验证了其有效性，并为方法提供了理论保证。

Conclusion: SCI框架通过解决双塔架构的核心挑战，提升了稠密检索的匹配精度和稳定性，特别适用于大规模部署和语义ID生成场景，为下游生成模型提供了更好的性能基础。

Abstract: Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.
  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.

</details>


### [202] [Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation](https://arxiv.org/abs/2512.13120)
*Mabiao Long,Jiaxi Liu,Yufeng Li,Hao Xiong,Junchi Yan,Kefan Wang,Yi Cao,Jiandong Ding*

Main category: cs.IR

TL;DR: 论文提出了一种实用的两阶段解决方案，用于解决动态异构图嵌入在生产部署中的可扩展性、数据新鲜度和冷启动问题。该框架结合了用于静态学习的可扩展图变换器HetSGFormer和用于实时更新的轻量级CPU算法ILLE，在十亿级图上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在生产环境中部署动态异构图嵌入面临三个主要挑战：可扩展性（处理大规模图）、数据新鲜度（保持嵌入的时效性）和冷启动问题（处理稀疏数据）。现有方法往往需要在深度表示学习和低延迟增量更新之间做出权衡。

Method: 采用两阶段框架：1) HetSGFormer - 用于静态学习的可扩展图变换器，具有线性可扩展性，能够捕捉全局图结构；2) ILLE（增量局部线性嵌入）- 轻量级、基于CPU的算法，用于实时增量更新，避免昂贵的完全重新训练。这种双重方法还具有冷启动弹性，能够利用图结构从稀疏数据中创建有意义的嵌入。

Result: 在十亿级图上进行A/B测试显示：HetSGFormer相比先前方法在广告商价值上实现了高达6.11%的提升；ILLE模块进一步带来了3.22%的提升，并将嵌入刷新及时性提高了83.2%。整个框架在保持数据新鲜度的同时实现了显著的性能改进。

Conclusion: 该研究提供了一个经过验证的框架，用于在生产环境中部署动态图学习。通过结合深度静态表示学习和轻量级增量更新，成功解决了可扩展性、数据新鲜度和冷启动等关键挑战，为实际应用提供了实用解决方案。

Abstract: Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.

</details>


### [203] [Know Your Users! Estimating User Domain Knowledge in Conversational Recommenders](https://arxiv.org/abs/2512.13173)
*Ivica Kostric,Ujwal Gadiraju,Krisztian Balog*

Main category: cs.IR

TL;DR: 提出基于游戏的数据收集协议，创建包含不同领域知识水平用户对话行为的数据集，用于开发能感知用户知识水平的对话推荐系统


<details>
  <summary>Details</summary>
Motivation: 当前对话推荐系统通常将所有用户视为专家，导致对领域不熟悉的用户交互体验差。需要能根据用户知识水平自适应调整对话策略的系统，但缺乏包含不同知识水平用户对话行为的数据集。

Method: 设计了基于游戏的数据收集协议，能够引出用户知识水平的不同表达方式。创建并发布了包含用户知识水平变化的对话数据集，并进行了初步分析。

Result: 成功创建了首个包含不同领域知识水平用户对话行为的数据集，为开发用户知识感知的对话推荐系统提供了基础资源。

Conclusion: 通过游戏化数据收集协议创建的数据集填补了用户知识水平感知对话推荐系统研究的空白，为未来开发自适应对话策略的系统奠定了基础。

Abstract: The ideal conversational recommender system (CRS) acts like a savvy salesperson, adapting its language and suggestions to each user's level of expertise. However, most current systems treat all users as experts, leading to frustrating and inefficient interactions when users are unfamiliar with a domain. Systems that can adapt their conversational strategies to a user's knowledge level stand to offer a much more natural and effective experience. To make a step toward such adaptive systems, we introduce a new task: estimating user domain knowledge from conversations, enabling a CRS to better understand user needs and personalize interactions. A key obstacle to developing such adaptive systems is the lack of suitable data; to our knowledge, no existing dataset captures the conversational behaviors of users with varying levels of domain knowledge. Furthermore, in most dialogue collection protocols, users are free to express their own preferences, which tends to concentrate on popular items and well-known features, offering little insight into how novices explore or learn about unfamiliar features. To address this, we design a game-based data collection protocol that elicits varied expressions of knowledge, release the resulting dataset, and provide an initial analysis to highlight its potential for future work on user-knowledge-aware CRS.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [204] [AI Integration In ERP Evaluation Across Trends and Architectures](https://arxiv.org/abs/2512.11805)
*Monu Sharma*

Main category: cs.CY

TL;DR: AI与ERP系统融合推动从静态本地部署向云原生自适应系统转型，但传统评估框架缺乏对算法透明度、适应性和伦理的考量，需要新的AI感知评估模型


<details>
  <summary>Details</summary>
Motivation: 随着AI技术（机器学习、深度学习、自然语言处理）融入ERP系统，传统基于成本、功能和用户满意度的评估框架已无法适应AI驱动的云ERP解决方案，缺乏对算法透明度、适应性和伦理的考量，需要建立新的评估体系

Method: 系统综述学术和行业资料，分析AI集成ERP的最新趋势、计算架构模型和分析方法，识别关键性能指标，提出将AI能力（预测智能、自适应自动化）与ERP性能评估指标对齐的理论模型

Result: 发现当前缺乏标准评估框架和AI感知系统来评估自动化效率、安全问题和灵活学习模式，识别了研究中的主要空白，提出了整合AI能力与ERP性能评估的理论模型

Conclusion: AI创新正在改变ERP评估方式，需要开发严谨的数据驱动评估方法，以适应快速发展的智能自优化企业生态系统，为研究者和从业者提供指导方向

Abstract: The incorporation of Artificial Intelligence (AI) into Enterprise Resource Planning (ERP) is a dramatic transition from static, on-premises systems to systems that can adapt and operate in cloud-native architectures. Cloud ERP solutions like Workday illustrate this evolution by incorporating machine learning, deep learning, and natural language processing into a centralized data-driven ecosystem. As the complexity of AI-driven ERP solutions expands, traditional evaluation frameworks that look at cost, function, and user satisfaction suffer from a lack of consideration for algorithmic transparency, adaptability, or ethics. This review will systematically investigate the latest trends, models of computing architecture, and analytical methods applied in assessing the performance of AI-integrated ERP services, specifically on cloud-based platforms. Based on academic and industry sources, the paper distills current research in line with architectural integration, analytical methodologies, and organizational impact. It identifies critical performance metrics and emphasizes the absence of any standard assessment frameworks or AI-aware systems capable of evaluating automation efficiency, security concerns as well as flexible learning modes. We put forward a theoretical model that brings AI-enabled capabilities -- such as predictive intelligence or adaptive automation -- into alignment with metrics in performance assessment for ERPs. By combining current literature and identifying major gaps in research, this paper attempts to present a complete picture of how innovations in AI are changing ERP evaluation. These research and methodological findings are intended to steer researchers and practitioners towards developing rigorous, data-driven assessment approaches, aligning with the fast-developing world of intelligent self-optimizing enterprise ecosystems

</details>


### [205] [How Immersiveness Shapes the Link Between Anthropocentric Values and Resource Exploitation in Virtual Worlds](https://arxiv.org/abs/2512.11812)
*Quan-Hoang Vuong,Thi Mai Anh Tran,Ni Putu Wulan Purnama Sari,Fatemeh Kianfar,Viet-Phuong La,Minh-Hoang Nguyen*

Main category: cs.CY

TL;DR: 研究通过分析《动物森友会》玩家数据，发现人类中心主义价值观与虚拟生态系统中资源开发行为（钓鱼、砍树）呈正相关，而沉浸感会减弱这种关联，特别是对砍树行为。


<details>
  <summary>Details</summary>
Motivation: 人类世生态危机不仅源于技术经济系统，更根植于人类中心主义世界观。数字环境日益中介人与自然互动，视频游戏为研究环境行为心理机制提供了新场景。

Method: 采用贝叶斯思维海绵框架（BMF）分析来自29个国家640名《动物森友会：新视野》玩家的数据，研究人类中心主义价值观与虚拟资源开发行为（钓鱼、捉虫、砍树）的关系，以及沉浸感的调节作用。

Result: 钓鱼和砍树频率与人类中心主义呈正相关；沉浸感减弱了砍树与人类中心主义的关联；捉虫频率无直接影响，但随着沉浸感增加，与人类中心主义的负相关增强。

Conclusion: 研究将环境心理学扩展到虚拟生态，表明数字互动既反映又重塑环境价值观。沉浸式游戏有潜力培养自然商数（NQ）和生态盈余文化，通过反思性、保护导向的参与促进环保意识。

Abstract: The Anthropocene is characterized by escalating ecological crises rooted not only in technological and economic systems but also in deeply ingrained anthropocentric worldviews that shape human-nature relationships. As digital environments increasingly mediate these interactions, video games provide novel contexts for examining the psychological mechanisms underlying environmental behaviors. This study investigates how anthropocentric values are associated with resource-exploiting behaviors in virtual ecosystems--specifically, fishing, bug catching, and tree cutting--and how immersiveness moderates these relationships. Employing the Bayesian Mindsponge Framework (BMF) to analyze data from 640 Animal Crossi,g: New Horizons (ACNH) players across 29 countries, the study reveals complex links between anthropocentric worldviews and in-game behaviors. Fishing and tree-cutting frequencies are positively associated with anthropocentrism, whereas immersiveness weakens the association between tree cutting and anthropocentrism. Bug-catching frequency shows no direct effect but exhibits a growing negative association with anthropocentrism as immersiveness increases. These findings extend environmental psychology into virtual ecologies, illustrating how digital interactions both reflect and reshape environmental values. They highlight the potential of immersive gameplay to cultivate the Nature Quotient (NQ) and foster an eco-surplus culture through reflective, conservation-oriented engagement.

</details>


### [206] [Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare](https://arxiv.org/abs/2512.11814)
*Hugh Brosnahan*

Main category: cs.CY

TL;DR: 该论文认为AI医疗记录系统的真正意义不在于效率提升，而在于它们如何重塑医疗注意力本身，体现了左脑计算思维对医疗实践的支配，可能窄化关怀领域并侵蚀临床专业知识。


<details>
  <summary>Details</summary>
Motivation: 论文旨在批判性地分析AI医疗记录系统（AI scribes）对医疗实践的影响，超越简单的效率讨论，探讨这些技术如何从根本上重塑医疗注意力的性质。

Method: 采用概念分析方法，将AI医疗记录系统置于人类思想和技能外部化的哲学谱系中，借鉴Iain McGilchrist的半球理论和Lewis Mumford的技术哲学，分析技术如何体现和放大特定的注意力模式。

Result: 研究发现AI医疗记录系统体现了左脑半球计算思维的主导地位，这种思维优先考虑可测量和程序化的内容，而忽视直觉和关系性因素。随着这种注意力模式在医疗实践中进一步嵌入，可能带来风险。

Conclusion: AI医疗记录系统不仅改变医疗记录方式，更重要的是重塑医疗注意力本身，可能导致关怀领域窄化、临床专业知识被侵蚀，并将医生简化为日益机械化系统中的操作员。

Abstract: Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.

</details>


### [207] [The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique](https://arxiv.org/abs/2512.11818)
*Izabela Lipinska,Hugh Brosnahan*

Main category: cs.CY

TL;DR: 大型语言模型可能通过类似"二联性精神病"的关系动态导致用户精神病性卷入，其高语言连贯性与缺乏真实主体的矛盾结构可能诱发用户通过想象投射来填补空白，当前以参与度优化的设计加剧了这种风险。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨当代大型语言模型如何通过创造类似"二联性精神病"的关系动态，对用户产生精神病性影响。作者观察到LLMs的高语言连贯性与缺乏真实主体之间的矛盾结构，可能在情感需求或不稳定的用户中引发精神病性卷入。

Method: 结合贝特森的双重束缚理论、共享性精神病障碍的临床文献以及麦吉尔克里斯特的大脑半球理论，分析LLMs的结构特征如何创造用户与系统之间的病理性关系动态。同时考察新兴临床报告，并发展现象学描述来说明这些动态如何展开。

Result: 研究发现LLMs的高语言连贯性与缺乏真实主体之间的结构性张力，可能导致用户在情感需求或不稳定状态下通过想象投射来填补空白，将内在性、意图或存在归因于一个根本不具备这些特征的系统。当前以参与度优化的设计选择加剧了这种风险。

Conclusion: 作者提出"本体论诚实"作为必要的设计原则，以减轻技术介导的"二联性精神病"风险。这意味着系统设计应明确其非人类、无主体性的本质，避免创造误导性的关系动态。

Abstract: This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.

</details>


### [208] [Prevalence, Devices Used, Reasons for Use, Trust, Barriers, and Challenges in Utilizing Generative AI among Tertiary Students](https://arxiv.org/abs/2512.11821)
*John Paul P. Miranda,Joseph Alexander Bansil,Emerson Q. Fernando,Almer B. Gamboa,Hilene E. Hernandez,Myka A. Cruz,Roque Francis B. Dianelo,Dina D. Gonzales,Elmer M. Penecilla*

Main category: cs.CY

TL;DR: 菲律宾大学生使用生成式AI的研究：主要使用免费工具，主要用于作业和创意生成，对AI信心不足且存在多重障碍，需要更好的支持和培训。


<details>
  <summary>Details</summary>
Motivation: 研究菲律宾大学生使用生成式AI的情况，了解其使用频率、设备、原因、知识水平、信任度、感知和面临的挑战，为教育机构提供改进建议。

Method: 通过调查菲律宾大学生使用生成式AI的情况，分析使用频率、设备、原因、知识水平、信任度、感知和挑战等多个维度。

Result: 大多数学生因经济限制使用智能手机上的免费AI工具，主要用于作业、创意生成和研究；不到一半学生对AI有信心，对其准确性持复杂态度；主要障碍包括访问限制、缺乏教师支持、理解输出困难和经济约束。

Conclusion: 研究强调需要改善AI访问条件、提供支持、加强培训并制定伦理指南；更广泛的担忧包括对学习、学术标准、就业和隐私的影响；学生因同伴支持而对AI持积极态度，并提出了相关建议。

Abstract: This study examined generative AI usage among Philippine college students particularly on frequency, devices, reasons, knowledge, trust, perceptions, and challenges. Most students used free AI tools on smartphones due to financial constraints. They used it primarily for homework, idea generation, and research. Less than half felt confident with AI and expressed mixed feelings about its accuracy. Barriers included limited access, lack of teacher support, difficulty understanding outputs, and financial constraints. The study highlighted the need for better access, support, training, and ethical guidelines. Broader concerns included impacts on learning, academic standards, job loss, and privacy. Students viewed AI positively due to peer support. Recommendations are discussed.

</details>


### [209] [Trust, Usefulness, and Dependency on AI in Programming: A Hierarchical Clustering Approach](https://arxiv.org/abs/2512.11822)
*Hilene E. Hernandez,Ranie B. Canlas,Madilaine Claire B. Nacianceno,Jordan L. Salenga,Jaymark A. Yambao,Juvy C. Grume,Aileen P. De Leon,Freneil R. Pampo,John Paul P. Miranda*

Main category: cs.CY

TL;DR: 研究调查菲律宾508名编程学生对AI工具的看法，发现四种不同信任和使用强度的学生群体，AI依赖度低源于基础设施不足和接触有限，建议针对性干预措施


<details>
  <summary>Details</summary>
Motivation: AI工具正在改变编程教育，但在欠发达国家的采用情况研究不足，了解学生对AI工具的信任、感知有用性和依赖程度对于改进教育整合至关重要

Method: 调查菲律宾邦板牙省508名大一编程学生，使用层次聚类分析他们的看法

Result: 识别出四种具有不同信任和使用强度的学生群体；学生认可AI工具的好处，但依赖度低（基础设施不足和接触有限）；高频用户不一定报告更高的信任或有用性感知

Conclusion: 为最大化AI的教育影响，需要基础设施发展、培训计划和课程整合等针对性干预；为发展中地区编程教育的公平有效AI采用提供实证见解

Abstract: While AI tools are transforming programming education, their adoption in underrepresented countries remains insufficiently studied. Understanding students' trust, perceived usefulness, and dependency on AI tools is essential to improving their integration into education. For these purposes, this study surveyed 508 first-year programming students in Pampanga, Philippines and analyzed their perceptions using hierarchical clustering. Results showed four unique student profiles with varying in trust and usage intensity. While students acknowledged AI tools' benefits, dependency remained low due to limited infrastructure and insufficient exposure. High-frequency users did not necessarily report greater trust or usefulness which may indicates a complex relationship between usage patterns and perception. This study recommends that to maximize AI's educational impact, targeted interventions such as infrastructure development, training programs, and curriculum integration are necessary. This study provides empirical insights to support equitable and effective AI adoption in programming education within developing regions.

</details>


### [210] [Teachers' Perspectives on the Use of AI Detection Tools: Insights from Ridge Regression Analysis](https://arxiv.org/abs/2512.11823)
*Vicky P. Vital,Francis F. Balahadia,Maria Anna D. Cruz,Dolores D. Mallari,Juvy C. Grume,Erika M. Pineda,Jordan L. Salenga,Lloyd D. Feliciano,John Paul P. Miranda*

Main category: cs.CY

TL;DR: 本研究调查了213名菲律宾教师对学术环境中AI检测工具的看法，发现信任是影响教师感知公平性和决策的最重要因素，而担忧和社会规范影响较弱。


<details>
  <summary>Details</summary>
Motivation: 探索教师对AI检测工具的信任、担忧和决策影响因素，了解这些因素如何影响教师对学生作业评估的公平性感知和决策过程。

Method: 采用Ridge回归分析，调查213名菲律宾教师，研究信任、担忧和社会规范等因素与教师感知公平性和决策之间的关系。

Result: 信任是影响教师感知公平性和决策的最显著预测因子，而担忧和社会规范对教师看法的影响较弱。信任AI检测工具的教师更可能认为这些工具公平有效。

Conclusion: 信任在塑造教师对AI检测工具的看法中起关键作用。建议通过培训、制度指南和实践社区来增强信任，平衡政策执行与教育者支持，促进AI检测工具在教育环境中的有效采用。

Abstract: This study explores the perceptions of 213 Filipino teachers toward AI detection tools in academic settings. It focuses on the factors that influence teachers' trust, concerns, and decision-making regarding these tools. The research investigates how teachers' trust in AI detection tools affects their perceptions of fairness and decision-making in evaluating student outputs. It also explores how concerns about AI tools and social norms influence the relationship between trust and decision-making. Ridge Regression analysis was used to examine the relationships between the predictors and the dependent variable. The results revealed that trust in AI detection tools is the most significant predictor of perceived fairness and decision-making among teachers. Concerns about AI tools and social norms have weaker effects on teachers' perceptions. The study emphasized critical role of trust in shaping teachers' perceptions of AI detection tools. Teachers who trust these tools are more likely to view them as fair and effective. In contrast, concerns and social norms have a limited influence on perceptions and decision-making. For recommendations, training and institutional guidelines should emphasize how these tools work, their limitations, and best practices for their use. Striking a balance between policy enforcement and educator support is essential for fostering trust in AI detection technologies. Encouraging experienced users to share insights through communities of practice could enhance the adoption and effective use of AI detection tools in educational settings..

</details>


### [211] [Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?](https://arxiv.org/abs/2512.11827)
*Milad Malekzadeh,Magdalena Biernacka,Elias Willberg,Jussi Torkko,Edyta Łaszkiewicz,Tuuli Toivonen*

Main category: cs.CY

TL;DR: 研究评估多模态大语言模型（MLLMs）使用街景图像评估绿地吸引力的能力，发现模型在正式绿地（公园）和非正式绿地（荒地）的吸引力判断上与人类存在差异，模型更关注美学特征而忽视安全、功能等人类重视的方面。


<details>
  <summary>Details</summary>
Motivation: 现有绿地吸引力评估方法往往忽视非正式或临时性空间，且资源密集难以大规模捕捉主观感知。本研究旨在探索多模态大语言模型能否像人类一样使用街景图像评估绿地吸引力，为城市规划提供可扩展的预评估工具。

Method: 使用ChatGPT GPT-4o、Claude 3.5 Haiku和Gemini 2.0 Flash三种MLLM模型，通过Google街景图像评估波兰罗兹市正式绿地（公园、管理绿地）和非正式绿地（草地、荒地）的吸引力。将模型输出与当地居民的地理问卷调查结果进行比较，分析吸引力判断的一致性和解释理由的分类。

Result: 结果显示：AI与人类在吸引人的正式绿地和缺乏吸引力的非正式空间上高度一致，但在吸引人的非正式绿地和缺乏吸引力的正式绿地上一致性较低。模型过度强调美学和设计导向特征，而低估了调查对象重视的安全性、功能性基础设施和本地嵌入品质。

Conclusion: 多模态大语言模型具有可扩展预评估的潜力，但需要人类监督和补充性参与方法。MLLMs可以支持但不能替代规划实践中对背景敏感的绿地评估。

Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.

</details>


### [212] [The Memecoin Phenomenon: An In-Depth Study of Solana's Blockchain Trends](https://arxiv.org/abs/2512.11850)
*Davide Mancino*

Main category: cs.CY

TL;DR: 该研究分析了Solana区块链上基于Pump.fun平台的memecoin现象，发现该平台主导了Solana代币发行和交易活动，但成功转型到主流DEX的代币不足2%，揭示了高度投机性市场结构。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索Solana区块链上新兴的memecoin现象，特别是通过Pump.fun平台，了解零售驱动的代币创建平台如何重塑区块链生态系统并影响市场参与。

Method: 使用链上数据分析方法，研究Pump.fun平台在2024年第四季度的活动，包括代币铸造、交易量、用户参与度等指标。

Result: Pump.fun平台占据了Solana上高达71.1%的代币铸造量，贡献了40-67.4%的DEX交易量；日活跃用户从6万增长至26万峰值；但只有不到2%的代币成功转型到主流去中心化交易所。

Conclusion: Memecoin现象具有双重影响：一方面降低了参与门槛，促进了零售参与；另一方面带来了高度投机性和市场风险，可能威胁区块链生态系统的稳定性和效率，需要审慎评估其长期影响。

Abstract: This paper analyzes the emerging memecoin phenomenon on the Solana blockchain, focusing on the Pump.fun platform during Q4 2024. Using on-chain data, it is explored how retail-focused token creation platforms are reshaping blockchain ecosystems and influencing market participation. This study finds that Pump.fun accounted for up to 71.1% of all tokens minted on Solana and contributed 40-67.4% of total DEX transactions. Despite this activity, fewer than 2% of tokens successfully transitioned to major decentralized exchanges, highlighting a highly speculative market structure. The platform experienced rapid growth, with daily active users rising from 60,000 to peaks of 260,000, underscoring strong retail adoption. This reflects a broader shift towards accessible, socially-driven market participation enabled by memecoins. However, while memecoins lower entry barriers and encourage retail engagement, they introduce significant risks. The volatile and speculative nature of these platforms raises concerns about long-term sustainability and the resilience of the blockchain ecosystem. These findings reveal the dual impact of memecoins: they democratize token creation and alter market dynamics but may jeopardize market efficiency and stability. This paper highlights the need to critically assess the implications of retail-driven speculative trading and its potential to disrupt emerging blockchain economies.

</details>


### [213] [Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence](https://arxiv.org/abs/2512.11863)
*Julian Schön,Lena Hoffmann,Nikolas Becker*

Main category: cs.CY

TL;DR: 该报告提出了人工智能系统环境风险的分析框架，超越直接资源消耗，关注AI整合到社会、经济、物理基础设施中产生的系统性、跨部门环境危害。


<details>
  <summary>Details</summary>
Motivation: AI常被视为解决气候变化等社会挑战的关键工具，但其环境足迹不断扩大。现有研究多关注AI的直接环境影响（如能耗、用水），而忽视了AI整合到社会、经济、物理基础设施中产生的系统性、跨部门环境风险。

Method: 通过叙事性文献综述，提出了一个三层框架来操作化系统性风险分析：1）塑造AI发展的结构条件；2）传播环境危害的风险放大机制；3）表现为可观察生态和社会后果的影响。在农业与生物多样性、石油天然气、废物管理三个领域进行了专家访谈案例研究。

Result: 提出了一个系统性环境风险分析框架，识别了AI整合到基础设施中可能产生的非线性、不公平且可能不可逆的环境影响。这些风险是新兴的，量化存在不确定性，但框架为理解和评估这些风险提供了结构化方法。

Conclusion: AI的系统性环境风险超越了直接资源消耗，需要关注其整合到社会、经济、物理基础设施中产生的跨部门、非线性环境危害。提出的三层框架为分析这些系统性风险提供了操作化工具，有助于更全面地评估AI的环境影响。

Abstract: Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.

</details>


### [214] [Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models](https://arxiv.org/abs/2512.11868)
*Alexander Windmann,Benedikt Stratmann,Mariya Lyashenko,Oliver Niggemann*

Main category: cs.CY

TL;DR: 本文提出了工业AI鲁棒性卡片（IARC），一个轻量级、任务无关的协议，用于记录和评估工业时间序列AI模型的鲁棒性，满足欧盟AI法案要求。


<details>
  <summary>Details</summary>
Motivation: 工业AI从业者面临新兴法规和标准中模糊的鲁棒性要求，但缺乏具体、可实施的协议。需要一种系统方法来记录和评估AI模型在工业时间序列上的鲁棒性。

Method: 提出了工业AI鲁棒性卡片（IARC）协议，包含必需字段和实证测量报告协议，结合漂移监测、不确定性量化和压力测试，并将其映射到欧盟AI法案相关义务。

Result: 通过生物制药发酵过程的软传感器案例研究，展示了IARC如何支持可复现的鲁棒性证据和持续监测。

Conclusion: IARC为工业AI模型提供了实用的鲁棒性文档化和评估框架，帮助从业者满足法规要求并确保模型在工业环境中的可靠性。

Abstract: Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.

</details>


### [215] [Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT](https://arxiv.org/abs/2512.11870)
*Mulham Fawkherji,Bruce Race,Driss Benhaddou*

Main category: cs.CY

TL;DR: 该研究针对休斯顿等低密度汽车依赖型城市，提出了结合社会经济指标和智能交通系统来加速零排放车辆普及、减少车辆行驶里程的方法，以达成2050年净零排放目标。


<details>
  <summary>Details</summary>
Motivation: 全球道路运输占温室气体排放的15%，造成大量PM2.5相关过早死亡。休斯顿作为低密度、汽车依赖型城市，道路运输占其气候行动计划基准排放的48%，要实现2050年净零排放目标面临巨大挑战，特别是社会经济差异限制了零排放车辆的普及。

Method: 建立道路排放基准，利用社会经济指标和智能交通系统评估政策方案。开发Unity 3D仿真环境，动态模拟城市交通并可视化政策情景。具体策略包括智能停车、公交激励、安全数据系统和零排放车队管理。

Result: 提出了支持评估的仿真环境，能够动态建模城市交通并可视化政策情景。识别了政策选项和潜在行动，为汽车依赖型城市实现2050年排放目标提供了指标、度量和技术支持。

Conclusion: 汽车依赖型城市要实现2050年排放目标，需要结合社会经济指标和智能交通系统技术，通过加速零排放车辆普及和减少车辆行驶里程的综合策略。论文提出的方法、指标和技术可为类似城市提供参考。

Abstract: Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.

</details>


### [216] [A Technical Policy Blueprint for Trustworthy Decentralized AI](https://arxiv.org/abs/2512.11878)
*Hasan Kassem,Sergen Cansiz,Brandon Edwards,Patrick Foley,Inken Hagestedt,Taeho Jung,Prakash Moorthy,Michael O'Connor,Bruno Rodrigues,Holger Roth,Micah Sheller,Dimitris Stripelis,Marc Vesin,Renato Umeton,Mic Bowman,Alexandros Karargyris*

Main category: cs.CY

TL;DR: 提出了一种去中心化AI系统的技术政策蓝图，通过将治理需求编码为政策即代码对象，分离资产政策验证与执行，实现透明、可扩展、可验证的治理机制。


<details>
  <summary>Details</summary>
Motivation: 去中心化AI系统（如联邦学习）在保护资产隐私方面具有潜力，但现有治理方法依赖特定基础设施政策，阻碍了资产互操作性和系统间信任，需要透明、可扩展、可验证的治理机制。

Method: 提出技术政策蓝图，将治理需求编码为政策即代码对象，分离资产政策验证与执行。政策引擎验证证据（身份、签名、支付、可信硬件证明）并颁发能力包，资产守护者仅基于这些能力包执行访问或计算。

Result: 通过将政策处理与能力分离，使治理能够在不重新配置AI基础设施的情况下演进，创建了透明、可审计、适应变化的治理方法。

Conclusion: 该技术政策蓝图为解决去中心化AI系统治理问题提供了创新方案，通过政策与执行的解耦实现了治理的灵活性、透明性和可扩展性，有助于解锁AI资产市场潜力。

Abstract: Decentralized AI systems, such as federated learning, can play a critical role in further unlocking AI asset marketplaces (e.g., healthcare data marketplaces) thanks to increased asset privacy protection. Unlocking this big potential necessitates governance mechanisms that are transparent, scalable, and verifiable. However current governance approaches rely on bespoke, infrastructure-specific policies that hinder asset interoperability and trust among systems. We are proposing a Technical Policy Blueprint that encodes governance requirements as policy-as-code objects and separates asset policy verification from asset policy enforcement. In this architecture the Policy Engine verifies evidence (e.g., identities, signatures, payments, trusted-hardware attestations) and issues capability packages. Asset Guardians (e.g. data guardians, model guardians, computation guardians, etc.) enforce access or execution solely based on these capability packages. This core concept of decoupling policy processing from capabilities enables governance to evolve without reconfiguring AI infrastructure, thus creating an approach that is transparent, auditable, and resilient to change.

</details>


### [217] [It's About Time: The Temporal and Modal Dynamics of Copilot Usage](https://arxiv.org/abs/2512.11879)
*Beatriz Costa-Gomes,Sophia Chen,Connie Hsueh,Deborah Morgan,Philipp Schoenegger,Yash Shah,Sam Way,Yuki Zhu,Timothé Adeline,Michael Bhaskar,Mustafa Suleyman,Seth Spielman*

Main category: cs.CY

TL;DR: 研究分析了3750万次与微软Copilot的匿名对话，发现AI使用模式因设备类型和上下文而异：移动端以健康话题为主，桌面端则以工作和科技为主，且使用模式随时间和日期呈现规律性变化。


<details>
  <summary>Details</summary>
Motivation: 与以往仅关注用户用AI做什么的研究不同，本研究旨在探索用户如何以及在何时使用AI，分析AI使用模式如何随设备类型、时间、日期等上下文因素变化。

Method: 分析了2025年1月至9月期间3750万次与微软Copilot的匿名对话数据，通过时间序列分析和主题分类，比较不同设备类型（移动端vs桌面端）在不同时间段的使用模式。

Result: 发现AI使用模式存在显著的设备差异：移动端健康话题占主导且全天稳定；桌面端工作日以工作和科技为主（8am-5pm工作话题超过科技话题）。编程查询在工作日激增，游戏话题在周末上升，深夜哲学问题增多，情人节关系话题激增。

Conclusion: 用户已快速将AI融入生活的各个方面：作为工作助手在办公桌上使用，作为伴侣在手机上使用。AI使用模式反映了用户日常生活的节奏和需求变化。

Abstract: We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with "Work and Career" overtaking "Technology" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.

</details>


### [218] [An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education](https://arxiv.org/abs/2512.11882)
*Lucia Happe,Dominik Fuchß,Luca Hüttner,Kai Marquardt,Anne Koziolek*

Main category: cs.CY

TL;DR: 本文介绍了RockStartIT Tutor的设计与试点评估，这是一个为中学编程课程开发的AI辅导系统，基于GPT-4和结构化知识库提供个性化支持。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在教育中的应用前景广阔但常伴随质疑，大型语言模型的最新发展为可扩展的个性化辅导带来了新希望。本研究旨在探索如何通过AI辅助工具增强编程教育，而非替代教师。

Method: 开发了基于GPT-4和OpenAI Assistant API的AI辅导系统，采用新颖的提示策略和模块化语义标记知识库，确保上下文感知、个性化和课程约束的支持。使用技术接受模型对13名学生和教师进行试点评估。

Result: 学生欣赏系统提供的低风险提问环境和支架式指导，教师认为系统能减轻独立任务时的认知负荷并补充课堂教学。主要挑战包括原型限制、样本量小以及需要针对目标年龄组的长期研究。

Conclusion: 研究展示了无需模型训练的实用AI集成方法，通过结构和提示塑造行为。AI辅导不应被视为教师替代品，而是作为扩展反馈访问、促进探究并支持学校核心教学目标的赋能工具。

Abstract: The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.

</details>


### [219] [Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"](https://arxiv.org/abs/2512.11883)
*Wenqi Marshall Guo,Qingyun Qian,Khalad Hasan,Shan Du*

Main category: cs.CY

TL;DR: 研究发现图像生成模型过度对齐到广义审美偏好会与用户意图冲突，特别是在需要"反审美"输出用于艺术或批判目的时，这种对齐优先考虑开发者中心价值观，损害用户自主性和审美多元性。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成模型过度对齐到广义审美偏好，当用户需要"反审美"输出用于艺术或批判目的时，这种对齐会与用户意图冲突，优先考虑开发者中心价值观，损害用户自主性和审美多元性。

Method: 构建广谱审美数据集，评估最先进的生成模型和奖励模型，通过图像到图像编辑测试，并与真实抽象艺术作品进行对比评估。

Result: 审美对齐的生成模型经常默认输出传统美感的图像，无法尊重低质量或负面图像的指令；奖励模型即使反审美图像完全符合用户提示也会对其进行惩罚；通过图像编辑和与真实抽象艺术作品的对比确认了这种系统性偏见。

Conclusion: 当前图像生成系统的审美对齐存在系统性偏见，过度强调传统美感，损害了用户自主性和审美多元性，需要重新思考如何平衡审美偏好与用户意图的多样性。

Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.

</details>


### [220] [Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions](https://arxiv.org/abs/2512.11887)
*Yihan Liao,Jingyu Zhang,Jacky Keung,Yan Xiao,Yurou Dai*

Main category: cs.CY

TL;DR: 本文通过大规模调查和文献分析，系统研究了自动驾驶系统测试的现状、挑战和研究空白，重点关注模块化与端到端系统测试实践，并探讨了V2X通信和基础模型等新兴因素对测试的影响。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统承诺提高交通效率和安全性，但在复杂现实环境中确保其可靠性仍面临关键挑战。有效的测试对于验证ADS性能、降低部署风险至关重要。本研究旨在调查当前ADS测试实践，识别行业从业者和学术研究者的关键需求，并分析现有研究与实际需求之间的差距。

Method: 1. 综述主要测试技术，包括模块化和端到端系统测试方法；2. 考虑V2X通信和基础模型（大语言模型和视觉基础模型）等新兴因素；3. 对100名来自工业界和学术界的参与者进行大规模调查；4. 通过专家讨论精炼调查问题；5. 进行定量和定性分析；6. 结合105项代表性研究分析参与者反馈。

Result: 调查结果显示：现有ADS测试技术在全面评估现实世界性能方面存在困难，特别是在角落案例多样性、仿真与现实差距、缺乏系统测试标准、潜在攻击暴露、V2X部署的实际挑战以及基于基础模型的测试的高计算成本等方面。通过分析参与者反馈和代表性研究，总结了当前研究现状并突出了主要局限性。

Conclusion: 本研究整合了ADS测试中的关键研究空白，并提出了未来研究方向：包括全面的测试标准、V2X系统中的跨模型协作、基于基础模型测试的跨模态适应，以及大规模ADS评估的可扩展验证框架。

Abstract: Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.

</details>


### [221] [Automation as a Catalyst for Geothermal Energy Adoption in Qatar: A Techno-Economic and Environmental Assessment](https://arxiv.org/abs/2512.11890)
*Tariq Eldakruri,Edip Senyurek*

Main category: cs.CY

TL;DR: 自动化技术可显著改善卡塔尔地热能的可行性，降低12-14%资本支出和14-17%运营支出，使平准化能源成本从145美元/MWh降至125美元/MWh，投资回收期缩短最多2年，每年可减少4000-17600吨CO2排放。


<details>
  <summary>Details</summary>
Motivation: 卡塔尔地热能因高资本成本、钻井风险和地下条件不确定性而未被充分利用，研究旨在探索自动化如何改善地热部署的技术经济与环境可行性。

Method: 研究通过三种途径分析：Dukhan盆地的增强型地热系统、改造油气井、以及用于区域供冷的地源热泵，结合地质数据集和财务建模，使用蒙特卡洛模拟评估自动化效果。

Result: 全自动化使资本支出降低12-14%，运营支出降低14-17%，平准化能源成本从145美元/MWh降至125美元/MWh，投资回收期缩短最多2年，每年可减少4000-17600吨CO2排放，并降低投资结果的不确定性。

Conclusion: 自动化增强了地热系统的经济可行性，支持其融入卡塔尔长期能源多元化和脱碳战略，为地热能在卡塔尔的部署提供了技术经济支撑。

Abstract: Geothermal energy provides continuous low emission potential but is underused in Qatar because of high capital costs, drilling risks, and uncertainty in subsurface conditions. This study examines how automation can improve the techno economic and environmental feasibility of geothermal deployment through three pathways: Enhanced Geothermal Systems in the Dukhan Basin, repurposed oil and gas wells, and ground source heat pumps for district cooling. Using geological datasets and financial modeling, the analysis shows that full automation reduces capital expenditure by 12 to 14 percent and operating expenditure by 14 to 17 percent. The Levelized Cost of Energy decreases from 145 USD per MWh to 125 USD per MWh, and payback periods shorten by up to two years. Environmental results indicate that geothermal substitution can avoid between 4000 and 17600 tons of CO2 per year for each project. Automation also reduces uncertainty in investment outcomes based on Monte Carlo simulations. Overall, the results show that automation strengthens the economic viability of geothermal systems and supports their integration into Qatars long term energy diversification and decarbonization strategies.

</details>


### [222] [Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI](https://arxiv.org/abs/2512.11893)
*Haocheng Lin*

Main category: cs.CY

TL;DR: 本文探讨生成式AI对工作、创造力和经济安全的影响，分析就业格局、AI采用差异、全民基本收入必要性以及AI内容政策对创造力的影响，并提出综合治理框架。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的快速发展正在重塑工作、创造力和经济安全的性质、分布和意义，需要全面理解AI系统超越生产力增益的社会后果。

Method: 采用混合方法，整合劳动力市场任务暴露建模、部门扩散映射、政策框架分析和定性话语批判。

Result: 研究发现AI采用存在社会人口群体、部门和地理差异，新一代模型可能表现不如前代，用户与AI互动可能产生回音室效应。提出全民基本收入应作为更广泛治理生态系统的一部分。

Conclusion: 为促进包容、有意义和创造性的环境，政策制定者应将全民基本收入视为治理、技能发展、创造力保护和模型设计更广泛生态系统中的一个维度。未来研究方向包括系统评估AI创造力表现、构建AI使用分布与公平分类学、制定平衡内容限制与创作自由的治理标准。

Abstract: The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.

</details>


### [223] [Financial Management Challenges in Enterprises Employing Remote and Hybrid Workforces](https://arxiv.org/abs/2512.11918)
*Michał Ćwiąkała,Gabriela Wojak,Dariusz Baran,Ernest Górka,Bartłomiej Bartnik,Waldemar Gajda,Ryszard Ratajski*

Main category: cs.CY

TL;DR: 远程和混合工作模式下的财务管理挑战：数字工具改善预算控制和透明度，但预测准确性和跨部门沟通仍是主要问题


<details>
  <summary>Details</summary>
Motivation: 研究远程和混合工作模式下组织面临的财务管理挑战，探讨这些灵活安排如何影响分布式团队的预算、报告和财务透明度

Method: 采用定量调查方法，调查管理者、HR人员和财务专业人员，分析数字工具、沟通和组织实践在塑造财务结果中的作用

Result: 远程和混合工作通过ERP系统和数字工作流可以改善预算控制和流程透明度；但预测准确性和跨部门沟通仍是主要挑战，特别是在数字整合不足的组织中；受访者报告压力水平降低和工作与生活平衡改善

Conclusion: 建议企业增强数字基础设施、采用先进分析技术进行预测、制定清晰的沟通框架并支持员工福祉计划；为灵活工作环境中的财务管理提供原创实证证据和实践见解

Abstract: The paper examines financial management challenges faced by organizations operating under remote and hybrid work models. It investigates how these flexible arrangements influence budgeting, reporting, and financial transparency in distributed teams. Using a quantitative survey of managers, HR staff, and finance professionals, the study analyzes the role of digital tools, communication, and organizational practices in shaping financial outcomes. Results indicate that remote and hybrid work can improve budget control and process transparency through the use of ERP systems and digital workflows. However, forecasting accuracy and interdepartmental communication remain major challenges, particularly in organizations with insufficient digital integration. Respondents also reported lower stress levels and improved work-life balance, suggesting potential well-being and productivity benefits. The paper recommends that companies enhance digital infrastructure, adopt advanced analytics for forecasting, and develop clear communication frameworks supported by employee well-being programs. The study contributes original empirical evidence on financial management in flexible work environments, offering practical insights for leaders navigating the digital transformation of finance.

</details>


### [224] [Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction](https://arxiv.org/abs/2512.11930)
*Mei Jiang,Haihai Shen,Zhuo Luo,Bingdong Li,Wenjing Hong,Ke Tang,Aimin Zhou*

Main category: cs.CY

TL;DR: 本文提出ERL4SIIP框架，使用进化强化学习解决STEM跨学科教育中的苏格拉底式教学问题，通过动态学生模拟、分层奖励机制和LoRA-Division优化策略，克服现有方法在认知状态建模、奖励稀疏性和策略多样性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现代STEM教育需要从被动知识传授转向主动的苏格拉底式建构，以培养高阶认知能力。虽然大语言模型在STEM跨学科教育中有潜力，但现有的提示工程、监督微调或标准强化学习方法存在三个根本挑战：无法动态建模潜在学生认知状态、长期教育目标导致的严重奖励稀疏和延迟、以及依赖行为克隆导致的策略崩溃和多样性不足。

Method: 将苏格拉底跨学科教学问题形式化为结构化部分可观测马尔可夫决策过程，提出ERL4SIIP进化强化学习框架，包含三个核心组件：1) 基于STEM知识图的动态学生模拟器用于潜在状态建模；2) 将长期目标分解为密集信号的分层奖励机制；3) 结合进化算法进行种群级全局搜索和PPO进行局部梯度上升的LoRA-Division优化策略。

Result: 该方法能够同时实现全局探索和细粒度策略优化，有效解决POMDP环境中的状态不可观测性和动态复杂性，支持苏格拉底式跨学科教学所需的策略多样性。

Conclusion: ERL4SIIP为STEM跨学科教育中的苏格拉底式教学问题提供了一个有效的解决方案，通过进化强化学习框架克服了现有方法的局限性，实现了动态认知状态建模、奖励稀疏性缓解和策略多样性保持。

Abstract: Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.

</details>


### [225] [The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance](https://arxiv.org/abs/2512.11933)
*Eren Kurshan,Tucker Balch,David Byrd*

Main category: cs.CY

TL;DR: 论文提出一个模块化治理架构，用于监管金融市场中快速发展的生成式AI和智能体系统，通过四层"监管块"实现实时风险控制。


<details>
  <summary>Details</summary>
Motivation: 生成式和智能体AI正以超过现有治理框架适应速度进入金融市场。当前模型风险框架假设静态、明确定义的算法和一次性验证，而大语言模型和多智能体交易系统通过持续学习、交换潜在信号和展现涌现行为违反了这些假设。

Method: 基于复杂适应系统理论，将这些技术建模为去中心化集合体，提出模块化治理架构，将监管分解为四层"监管块"：1)嵌入每个模型旁的自监管模块；2)聚合本地遥测数据并执行政策的公司级治理块；3)监管机构托管的智能体，监控行业范围指标以发现合谋或不稳定模式；4)提供第三方保证的独立审计块。采用八种设计策略使监管块能够与它们监管的模型同步进化。

Result: 通过多智能体交易中涌现的欺骗行为案例研究，展示了分层控制如何在实时隔离有害行为的同时保持创新。该架构与当前模型风险规则兼容，同时填补了关键的可观测性和控制缺口。

Conclusion: 该治理架构为金融系统中实现弹性、自适应AI治理提供了实用路径，能够在保持创新的同时有效控制新兴AI技术带来的风险。

Abstract: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of "regulatory blocks": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.

</details>


### [226] [Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching](https://arxiv.org/abs/2512.11934)
*Adeleh Mazaherian,Erfan Nourbakhsh*

Main category: cs.CY

TL;DR: 研究通过情感分析评估Google Play商店中AI教育应用的用户评价，发现作业助手类应用（如Edu AI）获得最高正面评价（95.9%），而语言/LMS类应用（如Teacher AI）正面评价较低（21.8%），主要问题包括付费墙、不准确性和技术故障。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能在教育领域的快速整合，数字教学正在经历转型，但用户对AI教育应用的看法尚未得到充分研究。本研究旨在通过分析用户评价来评估AI教育应用的有效性、挑战和教学意义。

Method: 研究采用情感分析管道：从Google Play商店抓取应用数据和用户评价，使用RoBERTa进行二元情感分类，GPT-4o提取关键观点，GPT-5合成主要正面/负面主题。将应用分为七类（作业助手、数学解题工具、语言工具等）。

Result: 结果显示整体情感以正面为主。作业助手类应用（如Edu AI 95.9%正面，Answer.AI 92.7%正面）在准确性、速度和个性化方面表现最佳；语言/LMS类应用（如Teacher AI 21.8%正面）因不稳定性和功能有限而表现较差。正面评价强调效率提升、问题解决和参与度；负面评价集中在付费墙、不准确性、广告和技术故障。

Conclusion: 生成式AI在教育领域具有民主化潜力，但也存在依赖性和不平等风险。未来应发展混合AI-人类教学模式、VR/AR沉浸式学习，开发者需改进自适应个性化，政策制定者应监管商业化以确保包容性。研究强调通过伦理改进促进公平创新的教育环境。

Abstract: The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).

</details>


### [227] [Beyond right or wrong : towards redefining adaptive learning indicators in virtual learning environments](https://arxiv.org/abs/2512.12105)
*Andreia dos Santos Sachete,Alba Valeria de SantAnna de Freitas Loiola,Fabio Diniz Rossi,Jose Valdeni de Lima,Raquel Salcedo Gomes*

Main category: cs.CY

TL;DR: 本文通过系统文献综述，探讨了超越传统对错评估的学习指标，为虚拟学习环境中的自适应学习提供更全面的评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟学习环境中的自适应学习方法大多仅基于学生回答的正确与否，这种评估视角有限，无法全面衡量学生的学习水平，忽略了学习过程中的其他关键要素。

Method: 采用系统文献综述方法，通过定性评估筛选相关研究，识别影响学生学习的学习指标。

Result: 研究发现了一系列全面的学习评估指标，包括动机、情绪、生理反应、脑成像和学生先前知识等，这些指标能够更全面地评估虚拟环境中的学习过程。

Conclusion: 这些新的学习指标为自适应技术开发者提供了更全面的评估框架，有助于开发更符合学生实际情况的解决方案，从而实现更完整的教育培训。

Abstract: Student learning development must involve more than just correcting or incorrect questions. However, most adaptive learning methods in Virtual Learning Environments are based on whether the student's response is incorrect or correct. This perspective is limited in assessing the student's learning level, as it does not consider other elements that can be crucial in this process. The objective of this work is to conduct a Systematic Literature Review (SLR) to elucidate which learning indicators influence student learning and which can be implemented in a VLE to assist in adaptive learning. The works selected and filtered by qualitative assessment reveal a comprehensive approach to assessing different aspects of the learning in virtual environments, such as motivation, emotions, physiological responses, brain imaging, and the students' prior knowledge. The discussion of these new indicators allows adaptive technology developers to implement more appropriate solutions to students' realities, resulting in more complete training.

</details>


### [228] [A neuro-symbolic framework for accountability in public-sector AI](https://arxiv.org/abs/2512.12109)
*Allen Daniel Sunny*

Main category: cs.CY

TL;DR: 开发了一个基于法律的可解释性框架，用于评估CalFresh福利资格自动决策系统的解释是否符合法定规则


<details>
  <summary>Details</summary>
Motivation: 自动化资格系统越来越多地决定公共福利的获取，但这些系统生成的解释往往无法反映授权决策的法律规则，需要建立法律基础的可解释性框架

Method: 开发了一个法律基础的可解释性框架，包括：1）从加州政策和程序手册中提取资格要求的结构化本体；2）将法定逻辑表达为可验证形式表示的规则提取管道；3）基于求解器的推理层来评估解释是否符合管辖法律

Result: 案例评估表明该框架能够检测法律上不一致的解释，突出违反的资格规则，并通过使自动决策的基础可追溯和可争议来支持程序问责

Conclusion: 该框架为自动化福利资格系统提供了法律基础的可解释性，增强了决策的透明度、可追溯性和问责性

Abstract: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.

</details>


### [229] [The Ideological Turing Test for Moderation of Outgroup Affective Animosity](https://arxiv.org/abs/2512.12187)
*David Gamba,Daniel M. Romero,Grant Schoenebeck*

Main category: cs.CY

TL;DR: 研究引入"意识形态图灵测试"游戏化框架，通过让参与者采纳和辩护对立观点来减少情感敌意和极化。实验发现换位思考能降低外群体敌意和意识形态极化，但写作和辩论两种方式的效果存在时间差异。


<details>
  <summary>Details</summary>
Motivation: 针对日益严重的意识形态对立和情感敌意这一关键社会挑战，研究旨在探索减少情感极化的有效方法。传统的对抗性方法可能加剧分歧，需要新的干预策略来促进跨意识形态理解。

Method: 采用混合设计实验（N=203），设置四个条件：方式（辩论/写作）×换位思考（己方/对方立场）。参与者进行结构化互动，为指定立场辩护，结果由同伴评判。测量干预后立即和2-6周随访时的情感敌意和意识形态立场变化。

Result: 换位思考降低了外群体敌意和意识形态极化，但效果因方式和时间而异：写作方式在换位思考时产生最大的即时情感敌意减少（Δ=+0.45 SD），但4-6周后效果消失；辩论方式则能维持显著的敌意减少（Δ=+0.37 SD）。意识形态立场方面，换位思考导致显著的即时变化（写作：Δ=+0.91 SD；辩论：Δ=+0.51 SD），且这些变化在随访中持续。

Conclusion: 研究挑战了对抗性方法的假设，揭示了不同的时间模式：非对抗性参与促进短期共情增益，而通过辩论的认知参与则能维持情感益处。意识形态图灵测试展示了作为减少极化可扩展工具的潜力，特别是将换位思考与反思性对抗互动相结合时。

Abstract: Rising animosity toward ideological opponents poses critical societal challenges. We introduce and test the Ideological Turing Test, a gamified framework requiring participants to adopt and defend opposing viewpoints, to reduce affective animosity and affective polarization.
  We conducted a mixed-design experiment ($N = 203$) with four conditions: modality (debate/writing) x perspective-taking (Own/Opposite side). Participants engaged in structured interactions defending assigned positions, with outcomes judged by peers. We measured changes in affective animosity and ideological position immediately post-intervention and at 2-6 week follow-up.
  Perspective-taking reduced out-group animosity and ideological polarization. However, effects differed by modality (writing vs. debate) and over time. For affective animosity, writing from the opposite perspective yielded the largest immediate reduction ($Δ=+0.45$ SD), but the effect was not detectable at the 4-6 week follow-up. In contrast, the debate modality maintained a statistically significant reduction in animosity immediately after and at follow-up ($Δ=+0.37$ SD). For ideological position, adopting the opposite perspective led to significant immediate movement across modalities (writing: $Δ=+0.91$ SD; debate: $Δ=+0.51$ SD), and these changes persisted at follow-up. Judged performance (winning) did not moderate these effects, and willingness to re-participate was similar across conditions (~20-36%).
  These findings challenge assumptions about adversarial methods, revealing distinct temporal patterns: non-adversarial engagement fosters short-term empathy gains, while cognitive engagement through debate sustains affective benefits. The Ideological Turing Test demonstrates potential as a scalable tool for reducing polarization, particularly when combining perspective-taking with reflective adversarial interactions.

</details>


### [230] [Anticipatory Governance in Data-Constrained Environments: A Predictive Simulation Framework for Digital Financial Inclusion](https://arxiv.org/abs/2512.12212)
*Elizabeth Irenne Yuwono,Dian Tjondronegoro,Shawn Hunter,Amber Marshall*

Main category: cs.CY

TL;DR: 本研究开发了一个预测模拟框架，利用机器学习分析数字金融素养干预措施的效果，为资源受限的岛国提供前瞻性政策决策支持。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的岛国，金融排斥是数字公共服务交付的主要障碍。传统政策评估依赖回顾性数据，缺乏前瞻性情报来支持敏捷的资源分配。需要一种方法能够在干预措施部署前预测其效果。

Method: 采用三阶段流程：1) 描述性分析，2) 可解释机器学习，3) 情景模拟。使用UNCDF太平洋数字经济数据集（10,108名受访者），通过透明线性回归模型（R平方95.9%）识别可修改的政策杠杆，并模拟不同干预情景。

Result: 模型识别出设备访问和费用跟踪等基础数字能力能带来最高预期收益（达5.5%），优于态度激励。模型支持精准定位，发现年轻女性护理人员是高杠杆响应者，而城市专业人士是非响应者，可防止资源错配。

Conclusion: 该研究展示了如何将静态调查数据转化为可操作的政策情报，为公共部门决策支持系统嵌入预测分析提供了可扩展、基于证据的蓝图，推进了以公平为重点的数字治理。

Abstract: Financial exclusion remains a major barrier to digital public service delivery in resource-constrained and archipelagic nations. Traditional policy evaluations rely on retrospective data, limiting the ex-ante intelligence needed for agile resource allocation. This study introduces a predictive simulation framework to support anticipatory governance within government information systems. Using the UNCDF Pacific Digital Economy dataset of 10,108 respondents, we apply a three-stage pipeline: descriptive profiling, interpretable machine learning, and scenario simulation to forecast outcomes of digital financial literacy interventions before deployment. Leveraging cross-sectional structural associations, the framework projects intervention scenarios as prioritization heuristics rather than causal estimates. A transparent linear regression model with R-squared of 95.9 identifies modifiable policy levers. Simulations indicate that foundational digital capabilities such as device access and expense tracking yield the highest projected gains, up to 5.5 percent, outperforming attitudinal nudges. The model enables precision targeting, highlighting young female caregivers as high-leverage responders while flagging non-responders such as urban professionals to prevent resource misallocation. This research demonstrates how static survey data can be repurposed into actionable policy intelligence, offering a scalable and evidence-based blueprint for embedding predictive analytics into public-sector decision-support systems to advance equity-focused digital governance.

</details>


### [231] [From Co-Design to Metacognitive Laziness: Evaluating Generative AI in Vocational Education](https://arxiv.org/abs/2512.12306)
*Amir Yunus,Peng Rend Gay,Oon Teng Lee*

Main category: cs.CY

TL;DR: 研究开发了面向新加坡职业教育的生成式AI聊天机器人，旨在辅助教师处理考试准备中的重复性问题。虽然提升了教学效率，但未显著改善学生成绩，反而揭示了AI使用中的认知依赖和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 解决职业教育教师在考试准备阶段面临的重复性问题管理和规模化反馈交付的挑战，通过AI技术减轻教师认知负担，提升教学效率。

Method: 采用用户中心的混合方法设计过程，与教师共同开发AI聊天机器人原型，通过用户反馈、工作流程分析和学生互动日志数据进行评估。

Result: AI工具成功简化了教师工作流程并降低了认知负荷，但学生整体评估成绩未显著提升。互动日志分析揭示了令人担忧的模式：高能力学生将工具用于策略性验证，而低能力学生则依赖AI绕过认知努力，可能加剧成绩差距。

Conclusion: 生成式AI能显著提升教学体验，但要实现有意义的学习成果，需要关注AI工具如何影响认知参与、自我调节和学习公平性。未来设计应最小化依赖、支持元认知发展，并针对不同能力水平校准支持。

Abstract: This study examines the development and deployment of a Generative AI proof-of-concept (POC) designed to support lecturers in a vocational education setting in Singapore. Employing a user-centred, mixed-methods design process, we co-developed an AI chatbot with lecturers to address recurring instructional challenges during exam preparation, specifically managing repetitive questions and scaling feedback delivery. The POC achieved its primary operational goals: lecturers reported streamlined workflows, reduced cognitive load, and observed improved student confidence in navigating course content. However, the deployment yielded unexpected insights into student learning behaviours. Despite enhanced teaching processes, performance data revealed no significant improvement in overall student assessment outcomes. Deep analysis of interaction logs identified concerning patterns, including self-efficacy-driven dependency, "metacognitive laziness" (cognitive offloading), and divergent usage strategies. While high-ability students leveraged the tool for strategic verification, low-ability students frequently used it to bypass cognitive effort, potentially exacerbating performance gaps. These findings suggest that Generative AI's educational influence extends beyond instructional efficiency to shape cognitive engagement, self-regulation, and learner equity. The study raises consequential design questions regarding how AI tools can be engineered to minimise dependency, scaffold metacognitive development, and calibrate support across varying ability levels. We conclude that while Generative AI can substantially enhance the teaching experience, achieving meaningful learning gains requires rigorous attention to learner behaviour and the equitable design of AI-supported environments.

</details>


### [232] [AI Sprints: Towards a Critical Method for Human-AI Collaboration](https://arxiv.org/abs/2512.12371)
*David M. Berry*

Main category: cs.CY

TL;DR: 论文提出"AI冲刺"作为人文学科研究新方法，结合批判性反思与生成式AI迭代对话，探索人机协作研究模式


<details>
  <summary>Details</summary>
Motivation: 大语言模型的出现为人文社科研究提供了新机遇，这些技术体现了"算法条件"，即计算系统不仅中介分析工具，还影响我们对自然和社会的理解方式。作者旨在探索通过AI增强研究的新形式人文探究

Method: 提出"AI冲刺"研究方法，结合数据冲刺和书籍冲刺方法，采用密集时间限制的研究会议。该方法基于Rogers的数字方法，扩展方法论以通过AI系统原生协议研究数字对象，而不仅仅是处理数字痕迹。引入三种认知模式：认知委托、生产性增强和认知开销

Result: 论文贡献了AI增强研究的实用方法论和理论框架，用于理解这种混合方法的认识论转变。展示了通过迭代开发紧密循环如何适应数据冲刺方法，同时承认生成式AI带来的深刻变革

Conclusion: 批判性方法论必须在技术和理论两个层面运作，维持与AI系统和输出的严格伦理-计算参与。AI冲刺方法为人文社科研究提供了结合批判性反思与AI能力的新研究范式

Abstract: The emergence of Large Language Models presents a remarkable opportunity for humanities and social science research. I argue these technologies instantiate what I have called the algorithmic condition, whereby computational systems increasingly mediate not just our analytical tools but how we understand nature and society more generally. This article introduces the possibility for new forms of humanistic inquiry through what I term 'AI sprints', as intensive time-boxed research sessions. This is a research method combining the critical reflexivity essential to humanistic inquiry with iterative dialogue with generative AI. Drawing on experimental work in critical code studies, I demonstrate how tight loops of iterative development can adapt data and book sprint methodologies whilst acknowledging the profound transformations generative AI introduces. Through examining the process of human-AI collaboration when undertaken in these intensive research sessions, I seek to outline this approach as a broader research method. The article builds on Rogers' digital methods approach, proposing that we extend methodologies to study digital objects through their native protocols, using AI systems not merely to process digital traces but to analyse materials traditionally requiring manual coding or transcription. I aim to show this by introducing three cognitive modes, cognitive delegation, productive augmentation, and cognitive overhead, explaining how researchers can maintain a strategic overview whilst using LLM capabilities. The paper contributes both a practical methodology for intensive AI-augmented research and a theoretical framework for understanding the epistemological transformations of this hybrid method. A critical methodology must therefore operate in both technical and theoretical registers, sustaining a rigorous ethical-computational engagement with AI systems and outputs.

</details>


### [233] [Beyond Static Scoring: Enhancing Assessment Validity via AI-Generated Interactive Verification](https://arxiv.org/abs/2512.12592)
*Tom Lee,Sihoon Lee,Seonghun Kim*

Main category: cs.CY

TL;DR: 本文提出了一种人机协作评估框架，结合自动评分和AI生成的针对性追问，以应对大语言模型对开放式评估的挑战，确保评估的真实性和有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型模糊了作者身份界限，挑战传统开放式评估的有效性。现有自动评分方法无法捕捉过程证据或验证学生真实理解，需要新的评估框架。

Method: 提出人机协作框架：第一阶段基于量规的自动评分确保程序公平性；第二阶段AI生成针对性追问进行交互验证，诊断表面推理或AI使用情况。

Result: 试点研究显示，第一阶段确保公平性和一致性，第二阶段对构念效度至关重要，能有效诊断表面推理或未经验证的AI使用。教师认可公平性与效度的平衡。

Conclusion: 该框架为真实评估提供了可扩展路径，超越了单纯监管AI，而是将其整合为评估过程中的协同伙伴，确保评估的完整性和有效性。

Abstract: Large Language Models (LLMs) challenge the validity of traditional open-ended assessments by blurring the lines of authorship. While recent research has focused on the accuracy of automated scoring (AES), these static approaches fail to capture process evidence or verify genuine student understanding. This paper introduces a novel Human-AI Collaboration framework that enhances assessment integrity by combining rubric-based automated scoring with AI-generated, targeted follow-up questions. In a pilot study with university instructors (N=9), we demonstrate that while Stage 1 (Auto-Scoring) ensures procedural fairness and consistency, Stage 2 (Interactive Verification) is essential for construct validity, effectively diagnosing superficial reasoning or unverified AI use. We report on the systems design, instructor perceptions of fairness versus validity, and the necessity of adaptive difficulty in follow-up questioning. The findings offer a scalable pathway for authentic assessment that moves beyond policing AI to integrating it as a synergistic partner in the evaluation process.

</details>


### [234] [From Linear Risk to Emergent Harm: Complexity as the Missing Core of AI Governance](https://arxiv.org/abs/2512.12707)
*Hugo Roger Paz*

Main category: cs.CY

TL;DR: 本文批评了基于风险的AI监管框架的结构性缺陷，提出了一种基于复杂性的AI治理新框架


<details>
  <summary>Details</summary>
Motivation: 基于风险的AI监管已成为AI治理的主导范式，但这类框架往往因结构性原因而失败。它们隐含地假设线性因果关系、稳定的系统边界和可预测的监管响应，而实际上AI在复杂的自适应社会技术系统中运行，危害常常是涌现的、延迟的、重新分配的，并通过反馈循环和系统参与者的战略适应而放大。

Method: 提出基于复杂性的AI治理框架，将监管视为干预而非控制，优先考虑动态系统映射而非静态分类，并整合因果推理和模拟来进行不确定性下的政策设计。

Result: 合规性可能增加，但危害只是被转移或隐藏而非消除。新框架的目标不是消除不确定性，而是通过监控、学习和迭代修订治理干预来实现稳健的系统管理。

Conclusion: 需要从传统的基于风险的监管范式转向基于复杂性的治理方法，以应对AI在复杂社会技术系统中产生的涌现性危害和系统性风险。

Abstract: Risk-based AI regulation has become the dominant paradigm in AI governance, promising proportional controls aligned with anticipated harms. This paper argues that such frameworks often fail for structural reasons: they implicitly assume linear causality, stable system boundaries, and largely predictable responses to regulation. In practice, AI operates within complex adaptive socio-technical systems in which harm is frequently emergent, delayed, redistributed, and amplified through feedback loops and strategic adaptation by system actors. As a result, compliance can increase while harm is displaced or concealed rather than eliminated. We propose a complexity-based framework for AI governance that treats regulation as intervention rather than control, prioritises dynamic system mapping over static classifications, and integrates causal reasoning and simulation for policy design under uncertainty. The aim is not to eliminate uncertainty, but to enable robust system stewardship through monitoring, learning, and iterative revision of governance interventions.

</details>


### [235] [Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union](https://arxiv.org/abs/2512.12837)
*Sahibpreet Singh,Manjit Singh*

Main category: cs.CY

TL;DR: 该研究比较分析了印度、美国和欧盟对AI驱动的"绿色清洗"（greenwashing）的刑事责任认定，发现现有法律存在人类中心主义偏见，难以追究算法系统的欺骗责任，并提出混合责任框架建议。


<details>
  <summary>Details</summary>
Motivation: AI驱动的绿色清洗已成为企业可持续发展治理中的隐蔽挑战，加剧了环境信息披露的不透明性并规避监管监督。现有法律基于人类意图的责任认定模式无法有效应对算法系统的欺骗行为。

Method: 采用法律教义学方法，系统分析印度、美国和欧盟的司法判例和法规，比较不同法域对AI中介绿色清洗的刑事责任认定机制。

Result: 研究发现现有欺诈法规面对AI生成的虚假陈述显得过时，存在责任认定空白。严格责任模式、重新调整的AI问责治理框架以及ESG制度下的算法尽职调查要求具有可行性。欧盟《企业可持续发展尽职调查指令》提供了潜在的跨国模式。

Conclusion: 研究主张建立混合责任框架，将算法风险评估与法律人格概念相结合，确保算法不透明性不会阻碍责任追究，为AI伦理和环境法理学做出贡献。

Abstract: AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-à-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.

</details>


### [236] [Open Source Software and Data for Human Service Development: A Case Study on Predicting Housing Instability](https://arxiv.org/abs/2512.12919)
*Maria Y. Rodriguez,Ehren Dohler,Jon Phillips,Melissa Villodas,Voltaire Vegara,Kenny Joseph,Amy Wilson*

Main category: cs.CY

TL;DR: 该研究探讨了开源数据和工具在资源受限的人类服务提供中的挑战与机遇，通过使用开源工具预测纽约布朗克斯县的驱逐申请未支付率，发现开源工具能促进快速分析但公共数据存在局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然开源数据和工具被赞誉为可复制和可用的社会科学研究的基础，但它们在资源受限的人类服务提供中的实际应用情况尚不清楚。研究旨在探索开源工具和数据在人类服务发展中的挑战与机遇。

Method: 使用住房数据联盟的邮政编码级别数据、美国社区调查5年估计数据和DeepMaps劳动力模型，通过R统计计算项目（开源统计软件）应用多层次模型（MLM）和指数平滑模型（ETS）预测2021年7月前的驱逐申请未支付率。

Result: 将预测结果与同期实际发生情况进行比较，评估开源工具和技术的有效性。研究发现开源数据和软件可以促进公共数据的快速分析，但公共数据受限于其可靠捕获的信息范围，存在不可忽视的误差幅度，限制了其实用性。

Conclusion: 开源工具在资源有限的人类服务组织中具有应用价值，特别是在资源日益受限的环境下进行干预开发时。但需要考虑公共数据的局限性，为关注低资源社区的人类服务组织提供实践启示。

Abstract: Open-source data and tools are lauded as essential for replicable and usable social science, though little is known about their use in resource constrained human service provision. This paper examines the challenges and opportunities of open-source tools and data in human service development by using both to forecast failure to pay eviction filings in Bronx County, NY. We use zip code level data from the Housing Data Coalition, the American Community Survey 5-year estimates, and DeepMaps Model of the Labor Force to forecast rates through July 2021. We employ multilevel (MLM) and exponential smoothing (ETS) models using the R project for Statistical Computing, an oft used open-source statistical software. We compare our results to what happened during the same period, to illustrate the efficacy of the open-source tools and techniques employed. We argue open-source data and software may facilitate rapid analysis of public data - a much-needed ability in human service intervention development under increasingly constrained resources - but find public data are limited by the information they reliably capture, limiting their utility by a non-trivial margin of error. The manuscript concludes by considering lessons for human service organizations with limited analytical resources and a vested interest in low-resourced communities.

</details>


### [237] [Modeling Collaborative Problem Solving Dynamics from Group Discourse: A Text-Mining Approach with Synergy Degree Model](https://arxiv.org/abs/2512.13061)
*Jianjun Xiao,Cixiao Wang,Wenmei Zhang*

Main category: cs.CY

TL;DR: 本研究提出一个计算框架，整合自动话语分析和协同度模型来量化协作问题解决中的协同效应，通过AI辅助方法实现细粒度CPS分析的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 协作问题解决的协同效应测量在分析学习中具有挑战性，传统手动编码无法捕捉系统层面的涌现动态，需要开发能够量化CPS协同效应的计算框架。

Method: 整合自动话语分析与协同度模型，收集52名学习者在12个小组中的5周cMOOC活动数据，应用9个分类模型自动识别四个交互层次（操作、寻路、意义建构、创造）的十种CPS行为，将每个交互层次作为子系统计算群体层面的序参数并推导协同度。

Result: BERT模型获得最高准确率，GPT模型在精度上表现更优适合人机协作编码；置换测试显示自动化测量保持构念效度；统计分析显示任务类型差异显著：调查研究小组比模式研究小组表现出更高的创造序参数；协同度能够区分从优秀到失败的不同协作质量。

Conclusion: 协同度作为协作质量的敏感指标，通过AI在环方法实现细粒度CPS分析的可扩展性是可行的，"受控无序"可能有利于复杂问题解决。

Abstract: Measuring collaborative problem solving (CPS) synergy remains challenging in learning analytics, as classical manual coding cannot capture emergent system-level dynamics. This study introduces a computational framework that integrates automated discourse analysis with the Synergy Degree Model (SDM) to quantify CPS synergy from group communication. Data were collected from 52 learners in 12 groups during a 5-week connectivist MOOC (cMOOC) activity. Nine classification models were applied to automatically identify ten CPS behaviors across four interaction levels: operation, wayfinding, sense-making, and creation. While BERT achieved the highest accuracy, GPT models demonstrated superior precision suitable for human-AI collaborative coding. Within the SDM framework, each interaction level was treated as a subsystem to compute group-level order parameters and derive synergy degrees. Permutation tests showed automated measures preserve construct validity, despite systematic biases at the subsystem level. Statistical analyses revealed significant task-type differences: survey study groups exhibited higher creation-order than mode study groups, suggesting "controlled disorder" may benefit complex problem solving. Importantly, synergy degree distinguished collaborative quality, ranging from excellent to failing groups. Findings establish synergy degree as a sensitive indicator of collaboration and demonstrate the feasibility of scaling fine-grained CPS analytics through AI-in-the-loop approaches.

</details>


### [238] [From Educational Analytics to AI Governance: Transferable Lessons from Complex Systems Interventions](https://arxiv.org/abs/2512.13260)
*Hugo Roger Paz*

Main category: cs.CY

TL;DR: 该论文提出将教育分析框架CAPIRE的方法论原则应用于AI治理，建立复杂系统AI治理（CSAIG）框架，从风险导向转向系统动态分析。


<details>
  <summary>Details</summary>
Motivation: 高等教育学生保留率和人工智能治理面临共同的结构性挑战：将线性监管框架应用于复杂适应系统。风险导向方法在这两个领域都占主导地位，但由于假设稳定的因果路径、可预测的行为者响应和可控的系统边界而系统性地失败。

Method: 从CAPIRE框架（课程、原型、政策、干预和研究环境）中提取可转移的方法论原则，该框架基于工程项目的纵向数据和因果推断方法，将学生辍学视为课程结构、制度规则和宏观经济冲击的涌现属性。提出复杂系统AI治理（CSAIG）框架，将五个核心原则（时间观察纪律、结构映射而非分类分类、基于原型的异质性分析、因果机制识别、基于模拟的政策设计）应用于AI系统治理。

Result: CAPIRE框架证明，当忽视系统复杂性时，善意的干预措施通常会带来意外后果。论文展示了从一个复杂系统领域（教育）获得的经验教训可以加速另一个领域（AI治理）的治理设计，并为复杂性感知的AI监管提供了具体的方法论架构。

Conclusion: 高等教育和AI治理在非线性、涌现性、反馈循环、战略适应和路径依赖方面存在同构性。CSAIG框架将监管设计的核心问题从"这个AI系统有多危险？"转变为"这个干预如何重塑系统动态？"，为复杂性感知的AI监管提供了实用方法论。

Abstract: Both student retention in higher education and artificial intelligence governance face a common structural challenge: the application of linear regulatory frameworks to complex adaptive systems. Risk-based approaches dominate both domains, yet systematically fail because they assume stable causal pathways, predictable actor responses, and controllable system boundaries. This paper extracts transferable methodological principles from CAPIRE (Curriculum, Archetypes, Policies, Interventions & Research Environment), an empirically validated framework for educational analytics that treats student dropout as an emergent property of curricular structures, institutional rules, and macroeconomic shocks. Drawing on longitudinal data from engineering programmes and causal inference methods, CAPIRE demonstrates that well-intentioned interventions routinely generate unintended consequences when system complexity is ignored. We argue that five core principles developed within CAPIRE - temporal observation discipline, structural mapping over categorical classification, archetype-based heterogeneity analysis, causal mechanism identification, and simulation-based policy design - transfer directly to the challenge of governing AI systems. The isomorphism is not merely analogical: both domains exhibit non-linearity, emergence, feedback loops, strategic adaptation, and path dependence. We propose Complex Systems AI Governance (CSAIG) as an integrated framework that operationalises these principles for regulatory design, shifting the central question from "how risky is this AI system?" to "how does this intervention reshape system dynamics?" The contribution is twofold: demonstrating that empirical lessons from one complex systems domain can accelerate governance design in another, and offering a concrete methodological architecture for complexity-aware AI regulation.

</details>


### [239] [Google Spain v. Gonzáles: Did the Court forget about freedom of expression?](https://arxiv.org/abs/2512.13404)
*Stefan Kulk,Frederik Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 欧盟法院在Google Spain案中确立，个人在一定条件下有权要求搜索引擎删除与其姓名相关的搜索结果，即使相关信息是合法发布的。


<details>
  <summary>Details</summary>
Motivation: 随着搜索引擎的普及，个人隐私面临新挑战。当人们搜索他人姓名时，可能找到多年前的负面信息，这些信息虽然合法发布，但可能对个人当前生活造成不公正影响。西班牙律师Mario Costeja González的案例凸显了数字时代个人数据保护与信息自由之间的冲突。

Method: 通过法律诉讼途径，西班牙国家法院将案件提交给欧盟法院（CJEU），请求就《数据保护指令》的适用提供指导。欧盟法院通过司法解释，确立了"被遗忘权"的法律原则。

Result: 欧盟法院裁定，个人在特定条件下有权要求搜索引擎删除与其姓名相关的搜索结果，即使原始信息是合法发布的。这一判决确立了数字时代的"被遗忘权"，对搜索引擎运营和个人数据保护产生了深远影响。

Conclusion: Google Spain案确立了个人在数字时代保护隐私的重要权利——"被遗忘权"，平衡了个人信息保护与信息自由之间的关系，为欧盟数据保护法律发展奠定了基础。

Abstract: When reviewing a job application letter, going on a first date, or considering doing business with someone, the first thing many people do is entering the person's name in a search engine. A search engine can point searchers to information that would otherwise have remained obscure. If somebody searched for the name of Spanish lawyer Mario Costeja González, Google showed search results that included a link to a 1998 newspaper announcement implying he had financial troubles at the time. González wanted Google to stop showing those links and started a procedure in Spain. After some legal wrangling, the Spanish Audiencia Nacional (National High Court) asked the Court of Justice of the European Union (CJEU) for advice on the application of the Data Protection Directive, which led to the controversial judgment in Google Spain. In its judgment, the CJEU holds that people, under certain conditions, have the right to have search results for their name delisted. This right can also extend to lawfully published information.

</details>


### [240] [Improving Privacy Protection in the area of Behavioural Targeting](https://arxiv.org/abs/2512.13405)
*Frederik Johannes Zuiderveen Borgesius*

Main category: cs.CY

TL;DR: 该博士论文探讨欧盟法律如何通过结合保护与赋权的方法来改进行为定向广告中的隐私保护，挑战了当前主要依赖知情同意的赋权方法。


<details>
  <summary>Details</summary>
Motivation: 行为定向（在线画像）通过监控用户在线行为来展示个性化广告，对隐私构成威胁。欧盟目前主要依赖e-Privacy指令中的同意要求和一般数据保护法，但行为研究表明这种赋权方法效果有限，许多人会盲目点击"同意"。因此需要探索更有效的隐私保护方法。

Method: 这是一项法律研究，但融合了计算机科学、行为经济学和媒体研究等多学科见解。采用法律分析方法，深入探讨了数据保护法是否应适用于假名数据，分析了知情同意在数据保护法中的作用，并研究了保护与赋权之间的张力。

Result: 研究发现当前主要依赖知情同意的赋权方法不足以有效保护隐私。论文主张采用保护与赋权相结合的方法，建议立法者应更侧重于保护个人，而不仅仅是赋予选择权。这是首批讨论行为研究对欧洲数据保护政策影响的学术研究之一。

Conclusion: 为缓解隐私问题（如寒蝉效应），欧盟法律需要从当前主要依赖赋权的模式转向保护与赋权相结合的方法。立法者应更注重保护个人，同时保持适度的赋权元素。这种综合方法能更好地应对行为定向广告带来的隐私挑战。

Abstract: This PhD thesis discusses how European law could improve privacy protection in the area of behavioural targeting. Behavioural targeting, also referred to as online profiling, involves monitoring people's online behaviour, and using the collected information to show people individually targeted advertisements. To protect privacy in the area of behavioural targeting, the EU lawmaker mainly relies on the consent requirement for the use of tracking technologies in the e-Privacy Directive, and on general data protection law. With informed consent requirements, the law aims to empower people to make choices in their best interests. But behavioural studies cast doubt on the effectiveness of the empowerment approach as a privacy protection measure. Many people click "I agree" to any statement that is presented to them. Therefore, to mitigate privacy problems such as chilling effects, this study argues for a combined approach of protecting and empowering the individual. Compared to the current approach, the lawmaker should focus more on protecting people. The PhD thesis is a legal study, but it also incorporates insights from other disciplines, such as computer science, behavioural economics, and media studies. This study is among the first to discuss the implications of behavioural research for European data protection policy. The topic of whether data protection law should apply to pseudonymous data is discussed in depth. The study contains a detailed analysis of the role of informed consent in data protection law, and gives much attention to the tension between protecting and empowering the individual within data protection law.

</details>


### [241] [Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance](https://arxiv.org/abs/2512.13658)
*Mohammadreza Molavi,Mohammad Moein,Mohammadreza Tavakoli,Abdolali Faraji,Stefan T. Mol,Gábor Kismihók*

Main category: cs.CY

TL;DR: 本文提出了一个基于文本嵌入模型的框架，用于自动化评估教育资源与预期学习成果之间的对齐度，验证了该方法在检测对齐性方面的有效性，并证明了对齐度分数与学习表现正相关。


<details>
  <summary>Details</summary>
Motivation: 随着在线教育的发展，个性化需求日益增长。虽然教育资源丰富，但教师难以选择既符合学习目标又能满足多样化学习者需求的材料。大型语言模型有潜力创建个性化学习资源，但验证其与预期成果的对齐仍需要昂贵的人工审核，限制了可扩展性。

Method: 提出了一个支持经济高效自动化评估教育资源与学习成果对齐度的框架。首先使用人工生成的材料对基于LLM的文本嵌入模型进行基准测试，找到最准确的模型（Voyage，79%准确率）。然后将最优模型应用于LLM生成的资源，并通过专家评估验证其可靠性（83%准确率）。最后在360名学习者的三组实验中检验对齐度分数与学习表现的关系。

Result: 最准确的文本嵌入模型（Voyage）在检测对齐性方面达到79%准确率。应用于LLM生成资源时，模型可靠地评估了与预期成果的对应关系（83%准确率）。实验结果显示，更高的对齐度分数与更好的学习表现正相关，卡方检验结果显著（χ²(2, N=360)=15.39, p<0.001）。

Conclusion: 基于嵌入的对齐度分数可以通过确认教育资源与学习成果的对齐来促进可扩展的个性化教育，使教师能够专注于根据多样化学习者需求定制内容，减少人工审核成本。

Abstract: As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [242] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 论文提出"价值剖面"概念，通过少量可重用的价值相关参数包（偏好、策略先验、策略精度）分配给隐状态，通过信念加权混合实现状态条件控制，无需为每个情境维护独立参数。


<details>
  <summary>Details</summary>
Motivation: 在多变环境中，智能体需要在不同隐情境间切换价值控制机制，但为每个情境维护独立的偏好、策略偏差和行动置信度参数是不可行的，需要更高效的计算框架。

Method: 引入价值剖面：少量可重用的价值相关参数包（结果偏好、策略先验、策略精度），分配给生成模型中的隐状态。通过后验信念的逐试次演化，控制参数通过信念加权混合产生，实现状态条件策略招募。

Result: 在概率反转学习任务中，基于剖面的模型优于静态精度和熵耦合动态精度模型（约100点AIC差异），参数恢复分析支持结构可识别性。模型推断表明自适应控制主要由策略先验调制而非策略精度驱动。

Conclusion: 可重用的价值剖面为多变环境中信念条件价值控制提供了可计算框架，产生了信念依赖控制和行为灵活性的可测试特征，支持状态条件（而非纯不确定性驱动）控制。

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [243] [Solving a Machine Learning Regression Problem Based on the Theory of Random Functions](https://arxiv.org/abs/2512.12731)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 论文从随机函数理论出发，基于无差别原理推导出回归方法，证明具有特定对称性的概率测度会自然产生广义多调和样条核函数


<details>
  <summary>Details</summary>
Motivation: 为机器学习回归问题提供理论基础，从第一性原理推导回归方法，而不是经验性地选择核函数和正则化形式

Method: 将回归问题视为多元逼近问题，采用随机函数理论框架，基于无差别原理的公设，推导具有平移、旋转、缩放不变性和高斯性的概率测度

Result: 证明具有自然对称性的概率测度会解析地导出整个解决方案，包括核函数形式、正则化类型和噪声参数化，所得核函数与广义多调和样条一致

Conclusion: 该结果为一大类平滑和插值方法提供了理论基础，证明了在缺乏先验信息时这些方法的最优性

Abstract: This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.

</details>


### [244] [CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation](https://arxiv.org/abs/2512.11830)
*Satyam Kumar*

Main category: cs.LG

TL;DR: 本文提出了CR3G框架，这是一个用于胸部X光报告生成的提示驱动因果推理系统，旨在通过关注因果关系和患者中心解释来提高AI生成报告的可理解性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在医学图像分析中擅长发现相关性模式，但难以理解这些模式与患者状况之间的深层因果关系。临床实践中需要更可解释、可信的AI诊断系统，以帮助医生做出更快、更准确的决策。

Method: 提出了CR3G（Causal Reasoning for Patient-Centric Explanations in radiology Report Generation）框架，这是一个应用于胸部X光分析的提示驱动因果推理系统，专注于揭示影像表现与诊断之间的因果关系，并生成以患者为中心的解释。

Result: CR3G在5种异常情况中的2种上表现出更好的因果关系识别能力和解释能力，显示出在提高AI驱动诊断质量方面的潜力。

Conclusion: 因果推理方法能够超越传统的模式识别，为医学影像报告生成提供更深入、更可解释的分析，有助于增强临床实践中AI诊断系统的实用性和可信度。

Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.

</details>


### [245] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出了一个用于代表性捷径模型的通用设计框架，该框架为有效性提供理论依据，解耦具体组件级选择，从而系统识别改进点。改进后的一步模型在ImageNet-256x256上达到2.85的SOTA FID50k，无需预训练、蒸馏或课程学习。


<details>
  <summary>Details</summary>
Motivation: 当前few-step扩散模型的理论推导和实际实现紧密耦合，模糊了设计空间。需要解耦理论框架和具体实现，为组件级创新提供清晰路径。

Method: 提出一个通用设计框架，为代表性捷径模型提供理论依据，解耦具体组件级选择，系统识别改进点，构建改进后的一步模型。

Result: 改进后的一步模型在ImageNet-256x256上达到2.85的FID50k（SOTA），在classifier-free guidance设置下，无需预训练、蒸馏或课程学习。

Conclusion: 该工作降低了捷径模型组件级创新的门槛，促进了设计空间的原则性探索，为few-step扩散模型提供了系统化的设计框架。

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [246] [Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach](https://arxiv.org/abs/2512.11845)
*Wenbo Du,Lingling Han,Ying Xiong,Ling Zhang,Biyue Li,Yisheng Lv,Tong Guo*

Main category: cs.LG

TL;DR: 本文提出了一种名为DTSFormer的可变形时空变换器，用于机场客流预测。该方法通过多尺度可变形分区和联合时空滤波模块，动态提取异质时间模式，结合频域注意力机制捕捉高频波动和低频周期性，在首都机场真实数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机场客流预测对运营效率至关重要。现有基于固定大小补丁的Transformer模型难以捕捉机场客流的复杂异质模式，需要更灵活的方法来建模不同时间阶段的异质趋势。

Method: 提出DTSFormer模型，包含：1）多尺度可变形分区模块，通过基于窗口函数的掩码动态划分多尺度时间补丁；2）联合时空滤波模块，设计频域注意力机制捕捉高低频分量；3）时域特征融合，联合建模短期波动和长期趋势。

Result: 在2023年1月至2024年3月北京首都国际机场真实客流数据上的实验表明，该方法在不同预测时间范围内均优于最先进的预测模型。可变形分区模块能够将补丁长度与主导周期和异质趋势对齐，更好地捕捉突发高频波动。

Conclusion: DTSFormer通过动态多尺度分区和频域注意力机制，有效解决了机场客流预测中的异质模式建模问题，为机场运营管理提供了更准确的预测工具。

Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.

</details>


### [247] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: 该技术报告对Tiny Recursive Models (TRM)在ARC-AGI-1任务上的表现进行了实证分析，发现其性能主要源于测试时增强、多数投票集成、任务标识符依赖和浅层递归，而非深度内部推理。


<details>
  <summary>Details</summary>
Motivation: TRM被提出作为解决ARC风格任务的高效参数替代方案，但其性能来源尚不明确——不清楚多少来自架构本身、测试时计算还是任务特定先验。本研究旨在实证分析TRM在ARC-AGI-1上的实际表现机制。

Method: 对ARC Prize TRM检查点在ARC-AGI-1上进行多维度分析：1) 测试时增强和多数投票集成的影响评估；2) 任务标识符消融实验；3) 递归轨迹分析；4) 不同增强训练策略比较；5) 与Llama 3 8B QLoRA微调的效率对比。

Result: 1) 1000样本投票管道比单次推理提升约11个百分点；2) 替换正确谜题ID会导致零准确率，显示严格依赖任务标识符；3) 大部分准确率在第一步递归即达成，递归深度较浅；4) 强增强训练能扩展候选解分布；5) TRM的非自回归设计在吞吐量和内存使用上显著优于Llama 3 8B QLoRA。

Conclusion: TRM在ARC-AGI-1上的性能主要源于效率优势、任务特定条件编码和激进的测试时计算策略的相互作用，而非深度内部推理能力。其递归机制实际上相对浅层，性能高度依赖外部增强和任务标识符。

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [248] [KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs](https://arxiv.org/abs/2512.11851)
*Prashant Pandey*

Main category: cs.LG

TL;DR: 论文提出了一种名为"token recycling"的方法，通过重用小型LLM中相似提示的注意力键值状态来加速推理，扩展上下文记忆空间。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索是否可以通过重用先前计算过的注意力键值状态来加速新相似提示的推理过程，从而提高LLM的推理效率并扩展其上下文记忆能力。

Method: 使用DialoGPT-medium作为测试平台，构建过去激活的缓存，通过句子嵌入检索条目，当缓存的提示是新输入的精确前缀时，重用缓存的过去键值。方法不需要修改模型，缓存的KV序列化到CPU，重新加载后提供给生成函数继续从缓存前缀解码。

Result: 测试显示，当存在前缀重叠时，观察到一致的加速效果，且输出语义没有实质性退化；当没有重叠时，行为与基线匹配。

Conclusion: token recycling方法能够有效加速LLM推理，在保持输出质量的同时提高效率，为相似提示的推理优化提供了可行方案。

Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.

</details>


### [249] [Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things](https://arxiv.org/abs/2512.11852)
*Muhammad Jawad Bashir,Shagufta Henna,Eoghan Furey*

Main category: cs.LG

TL;DR: 本研究利用Temporal Fusion Transformer模型实现智能温室执行器自动控制，并通过多种可解释性方法（模型内在解释、LIME、SHAP）提升决策透明度，在类别不平衡数据集上达到95%测试准确率。


<details>
  <summary>Details</summary>
Motivation: 智能温室中的物联网机器人系统虽然实现了精准农业，但现有时间序列预测模型多为黑盒，缺乏可解释性，这在需要信任、透明度和监管合规的智慧农业实践中是一个关键限制。

Method: 使用Temporal Fusion Transformer模型自动化温室执行器设置，并采用模型内在解释、局部可解释模型无关解释和SHAP值分析等局部和全局解释技术来增强模型决策的可解释性。

Result: 在自动化温室环境的类别不平衡数据集上，训练的TFT模型在执行器控制设置方面达到了95%的测试准确率。解释性分析揭示了温度、湿度、CO2水平、光照和外部气候等不同传感器读数对执行器控制决策的贡献程度。

Conclusion: 研究证明了TFT模型结合可解释性方法能够有效实现智能温室自动化控制，确保决策透明度，并支持自适应微调以提高作物产量和资源效率，为可信赖的智慧农业实践提供了解决方案。

Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.

</details>


### [250] [Learning to Retrieve with Weakened Labels: Robust Training under Label Noise](https://arxiv.org/abs/2512.13237)
*Arnab Sharma*

Main category: cs.LG

TL;DR: 本文提出标签弱化方法，通过生成一组可能的标签而非单一标签，在标签噪声存在的情况下训练更鲁棒的检索模型。


<details>
  <summary>Details</summary>
Motivation: 神经编码器在NLP领域用于密集检索任务，但训练数据中的稀疏标注和标签噪声使得训练或微调这类检索模型具有挑战性。现有方法要么需要调整超参数，要么增加了训练设置的复杂性。

Method: 采用标签弱化方法，不强制为每个查询-文档对分配单一可能错误的标签，而是允许从观察到的监督信号和模型置信度分数中推导出一组合理的标签。

Result: 在两个检索模型、一个重排序模型和四个不同排序数据集上的广泛评估显示，标签弱化方法相比10种最先进的损失函数，能够提高检索任务的性能。

Conclusion: 标签弱化方法能够有效应对训练数据中的标签噪声问题，提高检索模型的鲁棒性和性能。

Abstract: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.

</details>


### [251] [Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks](https://arxiv.org/abs/2512.11854)
*Grant King,Musa Azeem,Savannah Noblitt,Ramtin Zand,Homayoun Valafar*

Main category: cs.LG

TL;DR: 开发基于单腕戴IMU的实时反馈系统，用于检测阻力训练中的接近力竭状态（RiR≤2），通过两阶段管道实现边缘部署，为增肌训练提供客观强度反馈。


<details>
  <summary>Details</summary>
Motivation: 增肌训练需要在接近力竭与疲劳管理间取得平衡，但主观的"剩余重复次数"评估不可靠，导致训练刺激不足或过度疲劳，需要客观的实时反馈系统。

Method: 提出两阶段边缘部署管道：1) 使用ResNet模型从6轴IMU数据实时分割重复动作；2) 结合分割特征、卷积特征和LSTM历史上下文，分类检测接近力竭状态（RiR≤2）。

Result: 在13名参与者631次重复的数据集上，分割模型F1分数0.83，接近力竭分类器F1分数0.82（1.6Hz推理率）。树莓派5平均延迟112ms，iPhone 16延迟23.5ms，证实边缘计算可行性。

Conclusion: 该工作展示了使用最小硬件实现客观实时训练强度反馈的实用方法，为可访问的AI驱动增肌教练工具铺平道路，帮助用户有效管理训练强度和疲劳。

Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.

</details>


### [252] [No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction](https://arxiv.org/abs/2512.13300)
*Qinglin Jia,Zhaocheng Du,Chuhan Wu,Huifeng Guo,Ruiming Tang,Shuting Shi,Muyu Zhang*

Main category: cs.LG

TL;DR: KAML框架解决在线广告系统中多任务学习面临的不完整标签数据问题，通过属性驱动掩码策略、分层知识提取机制和排序损失策略提升转化率预测性能。


<details>
  <summary>Details</summary>
Motivation: 在线广告系统中，广告主通常有多样化的客户获取目标，使用多任务学习训练统一模型预测转化率。但实践中常遇到标签数据不完整的问题，因为许多广告主只提交部分用户转化行为数据，导致训练数据分布与部署数据分布不匹配，现有方法难以有效处理这种不对称多标签数据。

Method: 提出KAML框架：1) 属性驱动掩码策略(ADM)更好地利用不对称多标签数据；2) 分层知识提取机制(HKE)建模目标任务塔内的样本差异；3) 结合排序损失策略最大化未标记样本的效用。

Result: 在离线行业数据集和在线A/B测试中进行了全面评估，相比现有多任务学习基线方法，KAML显示出显著的性能提升。

Conclusion: KAML框架有效解决了在线广告系统中多任务学习面临的不完整标签数据挑战，通过精细化的知识转移机制提升了转化率预测的准确性和鲁棒性。

Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.

</details>


### [253] [Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry](https://arxiv.org/abs/2512.11855)
*Behrooz Tahmasebi,Melanie Weber*

Main category: cs.LG

TL;DR: 该论文首次从理论上比较了精确对称性与近似对称性的实现成本，发现精确对称性需要线性平均复杂度，而近似对称性仅需对数平均复杂度，存在指数级差异。


<details>
  <summary>Details</summary>
Motivation: 在机器学习科学应用中，精确对称性作为强大的归纳偏置带来显著收益，但近期研究表明近似对称性可能提供更大的灵活性和鲁棒性。然而，这两种方法的理论比较在文献中缺失，特别是缺乏对实现成本的理论理解。

Method: 引入"平均复杂度"框架来量化通过平均方法实现对称性的成本。在标准条件下，分析精确对称性和近似对称性所需的平均复杂度。

Result: 主要发现是指数级分离：实现精确对称性需要线性平均复杂度，而实现近似对称性仅需对数平均复杂度。这是首次从理论上分离这两种情况，为实践中优先选择近似对称性提供了正式依据。

Conclusion: 该研究首次从理论上证明了近似对称性相对于精确对称性在实现成本上的显著优势，为机器学习中对称性研究提供了新的理论工具和框架，这些工具和技术对更广泛的对称性研究具有独立价值。

Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.

</details>


### [254] [TopicProphet: Prophesies on Temporal Topic Trends and Stocks](https://arxiv.org/abs/2512.11857)
*Olivia Kim*

Main category: cs.LG

TL;DR: TopicProphet是一个新颖的股票预测框架，通过分析具有相似公众情绪趋势和历史背景的历史时期，解决股票数据缺乏因果逻辑和训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 股票预测长期以来被认为是不可能的，因为量化股票数据缺乏因果逻辑，且市场快速变化导致难以积累足够的训练数据。传统方法仅关注关键词和情绪影响，无法有效解决这些问题。

Method: 提出TopicProphet框架，采用主题建模、时间分析、断点检测和分段优化的序列方法，识别具有相似公众情绪趋势和历史背景的最优训练时间段。

Result: TopicProphet相比现有最先进方法，在捕捉用于预测金融百分比变化的最优训练数据方面产生了改进的结果。

Conclusion: 通过分析历史时期的相似性，TopicProphet能够为模型提供特定时代社会经济和政治状况产生的细微模式，同时解决相关股票训练数据不足的问题，从而改善预测效果。

Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.

</details>


### [255] [Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion](https://arxiv.org/abs/2512.11859)
*Michael Chertkov*

Main category: cs.LG

TL;DR: GH-PID是一种线性可解的引导随机最优传输框架，具有硬终端分布和软路径成本，通过低维引导协议保持解析结构，实现稳定采样和可微协议学习。


<details>
  <summary>Details</summary>
Motivation: 开发一个既能精确匹配终端分布，又能通过应用驱动的路径成本引导轨迹，同时保持解析可解性的随机最优传输框架，以生成几何感知、信任感知的轨迹。

Method: 提出引导谐波路径积分扩散(GH-PID)框架，使用低维引导协议塑造轨迹集合，保持前向和后向Kolmogorov方程的线性特性，通过格林函数比获得最优得分，高斯混合模型终端分布产生闭式表达式。

Result: 在三个2D导航场景中验证：手工协议展示几何和刚度如何影响滞后、曲率效应和模态演化；单任务协议学习优化中心线以最小化积分成本；多专家融合通过精确专家乘积法则学习共识协议。

Conclusion: GH-PID能够生成满足规定终端分布同时系统降低积分成本的几何感知、信任感知轨迹，为经验随机最优传输提供了可解释的变分近似。

Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.

</details>


### [256] [On the Dangers of Bootstrapping Generation for Continual Learning and Beyond](https://arxiv.org/abs/2512.11867)
*Daniil Zverev,A. Sophia Koepke,Joao F. Henriques*

Main category: cs.LG

TL;DR: 研究发现重复使用合成数据训练会导致模型性能退化，合成数据会引入显著偏差和方差，影响最大似然估计的可靠性，现有生成经验回放方法难以维持潜在空间对齐。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在模型训练中的广泛应用，重复使用合成数据进行训练引发了关于分布漂移和性能退化的担忧。研究者希望探究这种自举过程的影响，特别是在持续学习背景下。

Method: 通过持续学习的视角分析自举过程，将其与生成经验回放方法联系起来。进行统计分析证明合成数据会引入偏差和方差，提供实证证据展示流行生成模型在重复使用合成数据训练下的崩溃情况。

Result: 统计分析显示合成数据显著增加了训练目标的偏差和方差，削弱了最大似然估计的可靠性。实证证据表明流行生成模型在重复使用合成数据训练时会崩溃，现有最先进的生成经验回放方法无法维持潜在空间的对齐。

Conclusion: 研究结果对在持续学习中使用合成数据提出了严重关切，揭示了重复训练合成数据会导致模型性能退化，现有方法难以解决这一问题。

Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.

</details>


### [257] [Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations](https://arxiv.org/abs/2512.11946)
*Pramudita Satria Palar,Paul Saves,Rommel G. Regis,Koji Shimoyama,Shigeru Obayashi,Nicolas Verstaevel,Joseph Morlier*

Main category: cs.LG

TL;DR: 提出基于ICE曲线的全局敏感性度量方法，解决PDP在存在强交互作用时可能产生误导的问题，通过数学证明和多个案例验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等工程应用中，理解输入变量对数据驱动模型的影响至关重要。虽然部分依赖图(PDP)被广泛用于解释黑盒模型，但当存在强交互作用时，其全局敏感性度量可能产生误导，因为平均化会掩盖交互效应。

Method: 提出基于个体条件期望(ICE)曲线的全局敏感性度量方法，计算ICE曲线的期望特征重要性及其标准差，以更有效地捕捉交互作用的影响。提供了数学证明，表明在截断正交多项式展开下，PDP敏感性是ICE度量的下界。还引入了基于ICE的相关值来量化交互作用如何修改输入与输出之间的关系。

Result: 在三个案例上进行了比较评估：5变量分析函数、5变量风力涡轮机疲劳问题和9变量翼型空气动力学案例。结果表明，基于ICE的特征重要性比传统PDP方法提供了更丰富的见解，而PDP、ICE和SHAP的可视化解释相互补充，提供了多个视角。

Conclusion: 基于ICE的敏感性度量方法能够更有效地捕捉模型中的交互作用，为工程应用中的可解释机器学习提供了更准确和丰富的分析工具，特别是在存在复杂交互作用的场景下。

Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.

</details>


### [258] [Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors](https://arxiv.org/abs/2512.11949)
*Max McGuinness,Alex Serrano,Luke Bailey,Scott Emmons*

Main category: cs.LG

TL;DR: 论文提出"神经变色龙"模型，通过微调使LLM能够零样本逃避激活监控器，即使在安全相关概念上也能成功逃避未见过的监控器。


<details>
  <summary>Details</summary>
Motivation: 测试激活监控在最坏情况下的鲁棒性，特别是在错位威胁模型下，模型可能主动隐藏内部状态时，激活监控是否仍然有效。

Method: 通过微调LLM创建"神经变色龙"模型，使其在特定触发词（如"你正在被探测{概念}"）条件下，能够逃避对良性概念（如语言、HTML）的激活监控，并验证这种机制能否泛化到安全相关概念。

Result: 模型成功实现零样本逃避，即使针对未见过的安全监控器也能有效逃避；逃避具有高度选择性，只针对触发词中提到的特定概念；对模型标准基准测试能力影响有限；机制分析显示通过将激活移动到低维子空间实现。

Conclusion: 这项工作证明了激活监控在错位威胁模型下的潜在失效模式，为评估监控技术在最坏情况下的鲁棒性提供了工具，同时表明更强的防御措施（如监控器集成和非线性分类器）具有更好的抗性。

Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.

</details>


### [259] [Learning to Extract Context for Context-Aware LLM Inference](https://arxiv.org/abs/2512.11986)
*Minseon Kim,Lucas Caccia,Zhengyan Shi,Matheus Pereira,Marc-Alexandre Côté,Xingdi Yuan,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 提出基于强化学习的上下文生成框架，从用户提示中提取意图、知识和风险等上下文信息，指导LLM生成更安全、更合适的响应


<details>
  <summary>Details</summary>
Motivation: 传统LLM框架在生成响应时未充分考虑用户意图、先验知识和风险因素等上下文信息，导致对模糊请求可能产生不安全输出，或对良性请求过度拒绝

Method: 设计基于强化学习的上下文生成器，采用类似自编码器的结构，从用户提示中推断上下文信号，并利用这些信号指导响应生成

Result: 在SafetyInstruct数据集上平均减少5.6%的有害响应，在XSTest和WildJailbreak上良性提示的攻击成功率和合规性的调和平均值提高6.2%

Conclusion: 上下文提取方法能有效提高LLM推理的安全性和可靠性，特别是在安全任务中能更好地区分模糊请求和良性请求

Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.

</details>


### [260] [EnviroLLM: Resource Tracking and Optimization for Local AI](https://arxiv.org/abs/2512.12004)
*Troy Allen*

Main category: cs.LG

TL;DR: EnviroLLM是一个开源工具包，用于在个人设备上跟踪、基准测试和优化LLM的性能与能耗，帮助用户评估资源使用、环境影响和效率指标。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地在本地部署以保护隐私和提升可访问性，用户缺乏测量其资源使用、环境影响和效率指标的工具。

Method: 开发了EnviroLLM工具包，提供实时进程监控、跨多个平台（Ollama、LM Studio、vLLM和OpenAI兼容API）的基准测试、持久化存储与可视化分析，以及个性化模型和优化建议。

Result: 系统包含LLM-as-judge评估以及能耗和速度指标，使用户能够在测试自定义提示的模型时评估质量-效率权衡。

Conclusion: EnviroLLM为本地LLM部署提供了全面的性能监控和优化工具，帮助用户在隐私保护的同时实现更高效、环保的模型运行。

Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.

</details>


### [261] [DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning](https://arxiv.org/abs/2512.12022)
*Kaichuang Zhang,Wei Yin,Jinghao Yang,Ping Xu*

Main category: cs.LG

TL;DR: DFedReweighting是一个去中心化联邦学习的统一聚合框架，通过目标导向的权重调整来提升公平性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习虽然避免了中心服务器的单点故障风险，但仍面临公平性、鲁棒性等挑战，需要一种灵活的框架来同时解决这些问题

Method: 提出DFedReweighting框架，在每轮学习的最后一步进行目标导向的权重调整：首先基于目标性能指标计算初步权重，然后通过定制化重加权策略进行细化，得到最终聚合权重

Result: 理论分析表明，适当的目标性能指标和重加权策略组合能确保线性收敛；实验结果显示该框架在不同场景下显著提升了公平性和对抗拜占庭攻击的鲁棒性

Conclusion: DFedReweighting是一个灵活的统一框架，通过选择合适的目标性能指标和重加权策略，可以实现广泛的期望学习目标，有效解决去中心化联邦学习中的公平性和鲁棒性问题

Abstract: Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.

</details>


### [262] [Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning](https://arxiv.org/abs/2512.12046)
*Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: Eik-HiQRL：基于Eikonal PDE的连续时间准度量强化学习框架，通过分层分解解决复杂动态问题，在离线目标导航和操作任务中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习（GCRL）通过目标达成框架简化奖励设计，准度量强化学习（QRL）利用准度量结构约束价值函数学习，但现有QRL方法依赖离散轨迹约束，存在局限性

Method: 提出Eik-QRL：基于Eikonal偏微分方程的连续时间QRL重构，实现无轨迹学习；进一步提出Eik-HiQRL：将Eik-QRL集成到分层分解中，解决复杂动态问题

Result: Eik-HiQRL在离线目标条件导航任务中达到最先进性能，在操作任务中相比QRL获得一致提升，与时间差分方法性能相当

Conclusion: Eikonal约束的准度量强化学习框架提供了连续时间、无轨迹的学习方法，通过分层分解有效处理复杂动态，在GCRL任务中展现出优越性能

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.

</details>


### [263] [The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior](https://arxiv.org/abs/2512.12066)
*Erik Larsen*

Main category: cs.LG

TL;DR: 研究发现大语言模型的安全拒绝决策在不同随机种子和温度设置下存在显著不稳定性，单次测试无法可靠评估模型安全性


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全评估通常采用单次测试，隐含假设模型响应是确定性的且能代表模型的安全对齐程度。本文挑战这一假设，研究安全拒绝决策在不同随机种子和温度设置下的稳定性问题。

Method: 测试了来自三个家族的四个指令调优模型（Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B），使用876个有害提示和20种不同采样配置（4个温度×5个随机种子）。提出安全稳定性指数（SSI）来衡量决策稳定性，并使用Claude 3.5 Haiku作为统一外部评判者验证结果。

Result: 研究发现18-28%的提示在不同配置下出现决策翻转（模型在某些配置下拒绝，在其他配置下同意）。温度越高，决策稳定性显著降低（Friedman卡方=44.71，p<0.001），平均SSI从温度0.0时的0.951下降到温度1.0时的0.896。单次评估与多样本真实情况的一致性仅为92.4%。

Conclusion: 单次安全评估不足以可靠评估模型安全性。建议每个提示至少使用3个样本进行安全评估，以确保评估结果的可靠性。

Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

</details>


### [264] [SigTime: Learning and Visually Explaining Time Series Signatures](https://arxiv.org/abs/2512.12076)
*Yu-Chia Huang,Juntong Chen,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.LG

TL;DR: 提出SigTime框架，结合Transformer模型、shapelet表示和特征工程，用于时间序列模式发现和可视化分析


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模式发现方法存在计算复杂度高、可解释性有限、难以捕捉有意义的时序结构等问题，特别是在生物医学研究中，发现生理信号中的有意义模式对诊断和患者预后至关重要

Method: 提出联合训练两个Transformer模型的学习框架：使用shapelet表示捕捉局部时序结构，结合传统特征工程编码统计特性；开发SigTime可视化分析系统，提供多视角协调视图

Result: 在8个公开数据集和1个临床专有数据集上定量评估了学习框架；通过两个使用场景（公共ECG数据和早产分析）与领域专家一起验证了系统的有效性

Conclusion: 提出的框架和系统能够有效发现时间序列中的有意义模式，提高可解释性，并在生物医学等领域具有实际应用价值

Abstract: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.

</details>


### [265] [CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation](https://arxiv.org/abs/2512.12086)
*Xin Yang,Omid Ardakanian*

Main category: cs.LG

TL;DR: Cloak是一个基于潜在扩散模型的数据混淆框架，通过对比学习提取解耦表示，指导扩散过程在保留有用信息的同时隐藏隐私信息，在资源受限的移动物联网设备上实现更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有数据混淆方法存在以下问题：需要修改下游任务、难以达到满意的隐私-效用权衡、计算密集不适合资源受限的移动物联网设备部署。需要一种更高效、灵活的数据混淆方案。

Method: 提出Cloak框架：1）使用对比学习提取解耦表示；2）利用潜在扩散模型进行数据混淆；3）通过解耦表示指导扩散过程，在保留有用信息的同时隐藏隐私信息；4）支持用户根据隐私需求灵活调整隐私-效用权衡，无需大量重新训练。

Result: 在四个公共时间序列数据集（涵盖多种传感模态）和一个人脸图像数据集上的实验表明：Cloak在隐私保护效果上持续优于最先进的混淆技术，同时在资源受限环境中部署表现良好。

Conclusion: Cloak框架通过结合对比学习和潜在扩散模型，实现了更好的隐私-效用权衡，且适合在资源受限的移动物联网设备上部署，为半可信方访问传感器时间序列数据时的属性推理攻击提供了有效的缓解方案。

Abstract: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.

</details>


### [266] [Neural CDEs as Correctors for Learned Time Series Models](https://arxiv.org/abs/2512.12116)
*Muhammad Bilal Shahid,Prajwal Koirla,Cody Fleming*

Main category: cs.LG

TL;DR: 提出Predictor-Corrector机制，用神经控制微分方程作为修正器来预测预测误差，提升时间序列预测性能


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型（连续或离散时间）的多步预测存在误差，无论是直接预测整个时间范围还是迭代反馈预测都存在准确性问题

Method: 提出Predictor-Corrector机制：Predictor是任意学习的时间序列模型，Corrector是神经控制微分方程，用于预测预测误差，将误差加到预测值上以提高性能。Corrector支持不规则采样时间序列和连续/离散时间Predictor，并引入两种正则化策略提升外推性能

Result: 在合成数据、物理仿真和真实世界预测数据集上评估，使用神经ODE、Contiformer、DLinear等不同Predictor，实验表明Predictor-Corrector机制相比单独使用Predictor能持续提升性能

Conclusion: 提出的Predictor-Corrector机制能有效减少时间序列预测误差，提升预测性能，适用于多种Predictor模型和不同类型的时间序列数据

Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.

</details>


### [267] [BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models](https://arxiv.org/abs/2512.12131)
*Zhengyang Wang,Ziyue Liu,Ruijie Zhang,Avinash Maurya,Paul Hovland,Bogdan Nicolae,Franck Cappello,Zheng Zhang*

Main category: cs.LG

TL;DR: BOOST框架针对低秩瓶颈架构的大规模训练，提出瓶颈感知张量并行等优化技术，相比全秩模型基线获得1.46-1.91倍加速，相比简单集成3D并行的低秩模型获得1.87-2.27倍加速。


<details>
  <summary>Details</summary>
Motivation: Transformer模型预训练的规模受计算和通信成本增加的限制，低秩瓶颈架构能显著减少训练时间和内存占用，但对标准张量并行扩展性差，简单应用3D并行会导致过度通信和GPU利用率低下。

Method: 提出BOOST训练框架，包含瓶颈感知张量并行、在线RMSNorm、线性层分组和低秩激活检查点等优化技术，专门针对大规模低秩瓶颈架构设计。

Result: 在不同低秩瓶颈架构上评估，BOOST相比全秩模型基线获得1.46-1.91倍加速，相比简单集成3D并行的低秩模型获得1.87-2.27倍加速，同时提高了GPU利用率并减少了通信开销。

Conclusion: BOOST框架有效解决了低秩瓶颈架构在大规模训练中的并行扩展问题，通过专门优化的并行策略和多项技术优化，实现了显著的训练加速和资源利用效率提升。

Abstract: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\times$ speedup over full-rank model baselines and 1.87-2.27$\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.

</details>


### [268] [MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching](https://arxiv.org/abs/2512.12198)
*Jirui Jin,Cheng Zeng,Pawan Prakash,Ellad B. Tadmor,Adrian Roitberg,Richard G. Hennig,Stefano Martiniani,Mingjie Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种混合引导策略，将计算机视觉中的先进引导方法（如无分类器引导、自动引导和模型引导）整合到SE(3)-等变流匹配分子生成框架中，通过贝叶斯优化联合优化连续和离散模态的引导尺度，在QM9和QMe14S数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 条件分子生成的关键目标包括确保化学有效性、使生成分子与目标属性对齐、促进结构多样性以及实现高效采样。虽然计算机视觉领域已开发出多种新的引导策略，但这些方法在分子生成领域的应用和整合仍有待探索。

Method: 1. 将最先进的引导方法（无分类器引导、自动引导、模型引导）整合到基于SE(3)-等变流匹配的分子生成框架中
2. 提出混合引导策略，分别引导连续分子模态（速度场）和离散分子模态（预测logits）
3. 使用贝叶斯优化联合优化连续和离散模态的引导尺度
4. 在QM9和QMe14S数据集上进行基准测试

Result: 1. 在从头分子生成中实现了最先进的属性对齐性能
2. 生成的分子具有高结构有效性
3. 系统比较了各种引导方法的优缺点，为更广泛的应用提供了见解

Conclusion: 该研究成功地将计算机视觉中的先进引导策略整合到分子生成框架中，提出的混合引导方法在属性对齐和结构有效性方面取得了显著改进，为条件分子生成提供了有效的解决方案，并系统评估了不同引导方法的适用性。

Abstract: Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.

</details>


### [269] [EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training](https://arxiv.org/abs/2512.12210)
*Yuting Tang,Weibang Jiang,Shanglin Li,Yong Li,Chenyu Liu,Xinliang Zhou,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: EEG-DLite通过数据蒸馏框架，从大规模EEG数据集中选择性去除噪声和冗余样本，仅用5%的数据就能达到与完整数据集相当甚至更好的性能，显著提高EEG基础模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 大规模EEG基础模型虽然在下游任务中表现出良好的泛化能力，但由于EEG数据量大且质量参差不齐，训练过程资源消耗巨大。需要一种更高效的预训练方法来解决这一问题。

Method: EEG-DLite采用数据蒸馏框架：1）使用自监督自动编码器将EEG片段编码为紧凑的潜在表示；2）基于这些表示过滤异常值并最小化冗余；3）生成一个更小但信息丰富的子集，保留基础模型训练所需的多样性。

Result: 实验表明，在仅使用EEG-DLite从2500小时数据集中精选的5%数据上进行训练，在多个下游任务中能达到与完整数据集训练相当甚至更好的性能。

Conclusion: EEG-DLite是首个针对EEG基础模型的预训练数据蒸馏系统研究，为更有效和高效的生理基础建模提供了可扩展且实用的路径。

Abstract: Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.

</details>


### [270] [Optimized Learned Count-Min Sketch](https://arxiv.org/abs/2512.12252)
*Kyosuke Nishishita,Atsuki Sato,Yusuke Matsui*

Main category: cs.LG

TL;DR: OptLCMS是一种优化的学习型Count-Min Sketch，通过分区和动态规划优化参数，在保持相同内存使用下比LCMS构建更快、理论保证更好、错误概率更低。


<details>
  <summary>Details</summary>
Motivation: 传统学习型Count-Min Sketch（LCMS）虽然通过机器学习模型减少了估计误差，但存在构建速度慢（需要经验参数调优）和缺乏理论保证（特别是不可容忍错误概率）的问题。

Method: 1. 将输入域分区，每个分区分配独立的CMS实例；2. 针对固定阈值分析推导CMS参数；3. 通过带近似可行性检查的动态规划优化阈值；4. 提供对允许误差阈值的显式控制。

Result: 实验表明OptLCMS构建速度更快，实现了更低的不可容忍错误概率，同时保持了与LCMS相当的估计精度。

Conclusion: OptLCMS解决了LCMS的主要限制，提供了更快的构建速度、理论保证和更好的错误控制，同时保持了内存效率和估计精度，在实际应用中更具灵活性。

Abstract: Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.

</details>


### [271] [GRC-Net: Gram Residual Co-attention Net for epilepsy prediction](https://arxiv.org/abs/2512.12273)
*Bihao You,Jiping Cui*

Main category: cs.LG

TL;DR: 该论文提出GRC-Net模型，通过Gram矩阵将EEG信号转换为3D表示，结合共注意力机制和Inception结构进行多粒度特征提取，在癫痫预测任务上取得93.66%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统癫痫预测方法通常对EEG信号进行一维处理，无法充分建模信号间的多维关系。同时，EEG数据中存在局部与全局信号不平衡的问题，需要更有效的特征提取方法。

Method: 1. 使用Gram矩阵将一维EEG信号转换为3D表示，保留时间依赖关系；2. 提出多级特征提取：共注意力机制捕捉全局特征，Inception结构处理局部信号；3. 构建GRC-Net模型实现多粒度特征提取。

Result: 在BONN数据集上，GRC-Net在最具挑战性的五分类任务中达到93.66%的准确率，优于现有方法。

Conclusion: 通过3D信号表示和多级特征提取策略，GRC-Net能有效处理EEG信号的局部-全局不平衡问题，在癫痫预测任务中表现出优越性能。

Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.

</details>


### [272] [TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting](https://arxiv.org/abs/2512.12301)
*Mahima Kumavat,Aditya Maheshwari*

Main category: cs.LG

TL;DR: TwinFormer是一个用于长序列时间序列预测的分层Transformer，通过局部-全局双阶段处理和稀疏注意力机制，在多个真实世界数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决长序列时间序列预测中传统Transformer计算复杂度高、难以捕捉长期依赖关系的问题，需要设计一种既能有效建模局部动态又能捕获全局依赖的高效架构。

Method: 1. 将输入划分为非重叠的时间片段；2. 局部信息器使用top-k稀疏注意力建模片段内动态，然后进行平均池化；3. 全局信息器使用相同的top-k注意力捕获片段间的长程依赖；4. 使用轻量级GRU聚合全局上下文化的片段标记进行多步预测。

Result: 在8个真实世界数据集（天气、股价、温度、电力消耗、电力、疾病等）上，预测范围96-720步，TwinFormer在34个评估中获得了27个前两名位置，其中17个最佳MAE和RMSE，10个次佳。性能优于PatchTST、iTransformer、FEDformer、Informer和原始Transformer。

Conclusion: TwinFormer通过分层结构和稀疏注意力机制实现了线性复杂度，在长序列时间序列预测任务中表现出色，top-k稀疏注意力优于ProbSparse，GRU聚合有效。

Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.

</details>


### [273] [Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data](https://arxiv.org/abs/2512.12325)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 该论文证明了Robbins提出的经典次高斯混合模型满足路径式（确定性）遗憾界，在Ville事件E_α中，遗憾界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T，其中V_T是累积方差过程。


<details>
  <summary>Details</summary>
Motivation: 研究旨在桥接对抗性在线学习（通常处理有界数据的遗憾界）和博弈论统计（可以处理无界数据，但需要随机假设）两个领域。通过条件遗憾界作为随机和对抗性投注之间的桥梁。

Method: 分析Robbins提出的经典次高斯混合模型，证明其在Ville事件E_α中满足路径式确定性遗憾界。使用累积方差过程V_T作为关键参数，研究在不同条件下的遗憾界变化。

Result: 证明了在Ville事件E_α中，遗憾界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T；当V_T ≥ ln(1/α)时简化为ln(1/α) + ln ln V_T；在概率为1的事件E_0中，遗憾最终被ln ln V_T（乘以常数）所界定。

Conclusion: 该工作通过条件遗憾界成功桥接了对抗性在线学习和博弈论统计两个领域，为处理有界和无界数据提供了统一的框架，展示了随机和对抗性投注之间的理论联系。

Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.

</details>


### [274] [Uncertainty Quantification for Machine Learning: One Size Does Not Fit All](https://arxiv.org/abs/2512.12341)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文认为不存在单一的"最佳"不确定性度量，应根据具体应用场景定制不确定性量化方法，提出了基于二阶分布的灵活不确定性度量框架，并展示了不同任务需要不同特性的不确定性度量。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，准确量化预测不确定性至关重要。现有研究通常声称某种不确定性度量优于其他方法，但本文认为不存在单一最佳度量，应根据具体应用场景定制不确定性量化方法。

Method: 提出了一个灵活的不确定性度量框架，基于二阶分布区分总不确定性、偶然不确定性和认知不确定性。这些度量可以通过适当的评分规则（proper scoring rules）进行实例化，以控制其特性。

Result: 研究表明：1）对于选择性预测任务，评分规则应与任务损失匹配；2）对于分布外检测，互信息（广泛使用的认知不确定性度量）表现最佳；3）在主动学习场景中，基于0-1损失的认知不确定性始终优于其他不确定性度量。

Conclusion: 不存在单一的最佳不确定性度量方法，不确定性量化应根据具体应用需求进行定制。通过灵活的不确定性度量框架和适当的评分规则，可以为不同任务选择最合适的不确定性度量方法。

Abstract: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.

</details>


### [275] [Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept](https://arxiv.org/abs/2512.12365)
*Thai-Duy Dinh,Minh-Luan Vo,Cuong Tuan Nguyen,Bich-Hien Vo*

Main category: cs.LG

TL;DR: 开发合成蚊群声学分类数据集，用于模拟多物种嘈杂环境下的蚊子识别，相比传统方法更高效可扩展


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病每年导致超过70万人死亡，现有数据集需要大量人工采集个体蚊子录音，资源消耗大且难以扩展

Method: 创建合成蚊群蚊子声学分类数据集，使用对数梅尔频谱图，评估轻量级深度学习架构进行蚊子物种分类

Result: 模型能有效识别六种主要蚊子媒介，适合部署在嵌入式低功耗设备上，合成数据集能加速声学蚊子研究

Conclusion: 合成蚊群音频数据集具有潜力加速蚊子声学研究，实现可扩展的实时监测解决方案

Abstract: Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.

</details>


### [276] [The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining](https://arxiv.org/abs/2512.12384)
*Jesse Ponnock*

Main category: cs.LG

TL;DR: 对1B和3B参数的Llama-3.2模型在400M金融语料上进行领域自适应预训练，结果显示前200M token效果提升最明显，金融语言高度规律且易学，通用领域性能保持稳定。


<details>
  <summary>Details</summary>
Motivation: 探索领域自适应预训练（DAPT）作为专门化大语言模型到高价值领域（如金融）的实用路径，避免完全重新训练的成本，为金融基础模型的规模化提供早期经验指导。

Method: 使用1B和3B参数的Llama-3.2模型，在美国SEC文件构成的400M token金融语料上进行持续预训练，在50M、100M、200M和400M token处设置验证检查点，分析领域适应效果和缩放规律。

Result: 两个模型在SEC领域验证损失均持续改善，最大提升出现在前200M token，之后收益递减；幂律拟合显示指数较浅，表明金融语言高度规律且易学；通用领域验证损失基本不变，无灾难性遗忘迹象；数据效率前沿显示模型向专业化改进，混合领域退化可忽略。

Conclusion: 领域自适应预训练能以相对适中的token预算实现有意义的领域适应，金融语言的高度规律性使其学习效率高，更大的模型规模（7B-70B）在预计数据需求下仍可处理，为金融基础模型的规模化提供了实证指导。

Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.

</details>


### [277] [Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment](https://arxiv.org/abs/2512.12387)
*Yawen Shao,Jie Xiao,Kai Zhu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.LG

TL;DR: VGPO提出了一种改进的强化学习方法，用于解决GRPO在图像生成中的局限性，通过时间维度和群体维度的价值估计优化，提升图像质量和任务准确性。


<details>
  <summary>Details</summary>
Motivation: 当前GRPO方法在基于流匹配的图像生成中存在两个关键问题：1）稀疏的终端奖励在所有时间步均匀应用，损害了时间信用分配，忽略了从早期结构形成到后期调整的不同关键阶段；2）仅依赖相对群体内奖励导致优化信号随着训练收敛而减弱，当奖励多样性耗尽时出现优化停滞。

Method: VGPO框架重新定义了时间和群体维度的价值估计：1）将稀疏终端奖励转化为密集的、过程感知的价值估计，通过建模每个生成阶段的预期累积奖励实现精确信用分配；2）用绝对价值增强的新过程替代标准群体归一化，即使在奖励多样性下降时也能保持稳定的优化信号。

Result: 在三个基准测试上的广泛实验表明，VGPO实现了最先进的图像质量，同时提高了任务特定准确性，有效缓解了奖励黑客问题。

Conclusion: VGPO通过重新设计价值估计机制，成功解决了GRPO在图像生成中的局限性，为基于强化学习的图像生成方法提供了更有效的优化框架。

Abstract: Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.

</details>


### [278] [Rough Sets for Explainability of Spectral Graph Clustering](https://arxiv.org/abs/2512.12436)
*Bartłomiej Starosta,Sławomir T. Wierzchoń,Piotr Borkowski,Dariusz Czerski,Marcin Sydow,Eryk Laskowski,Mieczysław A. Kłopotek*

Main category: cs.LG

TL;DR: 本文提出了一种基于粗糙集理论的图谱聚类解释方法增强方案，旨在解决文档聚类结果难以解释的问题。


<details>
  <summary>Details</summary>
Motivation: 图谱聚类方法在处理文档等数据时，由于嵌入到谱空间且与文档内容没有明显关联，聚类结果难以向用户解释。此外，文档内容不明确和聚类算法的随机性进一步降低了可解释性。

Method: 在团队先前研究的基础上，提出了一种增强的解释方法，该方法从粗糙集理论中汲取灵感，以解决上述问题。

Result: 该方法能够克服文档内容不明确和算法随机性带来的解释困难，提高聚类结果的可解释性。

Conclusion: 基于粗糙集理论的增强解释方法能够有效改善图谱聚类在文档分析中的可解释性问题，使聚类结果更容易被用户理解和接受。

Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.

</details>


### [279] [Optimized Architectures for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.12448)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 通过过参数化架构结合可微稀疏化，在保持KANs可解释性的同时提升性能，实现更紧凑、更准确的模型


<details>
  <summary>Details</summary>
Motivation: 传统方法通过架构增强改进KANs会引入复杂性，破坏其可解释性优势。需要在保持可解释性的同时提升模型性能。

Method: 采用过参数化架构结合可微稀疏化技术，将架构搜索转化为端到端优化问题，学习紧凑且可解释的KANs。

Result: 在函数逼近基准测试、动态系统预测和真实世界预测任务中，实现了竞争性或更优的准确性，同时发现了显著更小的模型。过参数化与稀疏化具有协同效应。

Conclusion: 提供了一种原则性方法，既能获得更具表达能力的模型，又能保持可解释性，解决了科学机器学习中的关键矛盾。

Abstract: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.

</details>


### [280] [Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling](https://arxiv.org/abs/2512.12461)
*Eray Erturk,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 通过跨模态知识蒸馏，将预训练的多会话尖峰Transformer模型的知识迁移到LFP Transformer模型，显著提升了LFP模型在无监督和监督任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 局部场电位（LFPs）在神经实验中具有重要价值，但因其群体水平的聚合特性，建模难度大，预测下游任务变量的能力通常低于尖峰信号。现有神经建模框架主要关注尖峰活动，而LFPs的建模挑战限制了其应用潜力。

Method: 提出跨模态知识蒸馏框架：1）首先使用掩码自编码目标训练多会话尖峰Transformer教师模型，采用会话特定的神经标记化策略；2）然后将学生LFP模型的潜在表示与教师尖峰模型的表示对齐，实现知识迁移。

Result: 蒸馏后的LFP模型在完全无监督和监督设置中均优于单会话和多会话LFP基线模型，能够泛化到其他会话而无需额外蒸馏，同时保持优越性能。

Conclusion: 跨模态知识蒸馏是一种强大且可扩展的方法，能够利用高性能尖峰模型开发更准确的LFP模型，为神经信号建模提供了新思路。

Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.

</details>


### [281] [Exploring the Design Space of Transition Matching](https://arxiv.org/abs/2512.12465)
*Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: 本文系统研究了Transition Matching（TM）生成模型中头部模块的设计、训练和采样策略，通过大规模实验发现MLP头部配合特定时间加权和高频采样器在综合指标上表现最佳，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: TM作为一种新兴的生成建模范式，比扩散和流匹配模型更具表达力，但其头部模块的设计、训练和采样策略尚未得到系统研究，需要探索如何优化TM框架以获得最佳生成质量和效率。

Method: 采用时间连续双向变体的TM框架，通过大规模系统性实验（训练56个不同的17亿参数文本到图像模型，进行549次独特评估），对比不同头部模块架构（MLP vs Transformer）、训练建模策略以及随机TM采样器家族的效果。

Result: MLP头部配合特定时间加权和高频采样器在所有指标上排名最佳，达到SOTA水平；Transformer头部配合序列缩放和低频采样在图像美学方面表现优异。实验揭示了哪些设计选择能带来最大质量和效率提升，哪些选择不太可能提供进一步增益。

Conclusion: TM框架中头部模块的设计对生成质量和效率有显著影响，MLP头部配合优化训练策略和采样方法是最佳选择，为TM模型的实践应用提供了重要指导，同时明确了未来优化的方向。

Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.

</details>


### [282] [Sparse Concept Anchoring for Interpretable and Controllable Neural Representations](https://arxiv.org/abs/2512.12469)
*Sandy Fraser,Patryk Wielopolski*

Main category: cs.LG

TL;DR: 提出稀疏概念锚定方法，通过最小监督（每个锚定概念<0.1%的标注样本）在潜在空间中定位目标概念子集，同时允许其他概念自组织，实现可解释、可操控的表示学习。


<details>
  <summary>Details</summary>
Motivation: 为了解决学习表示中特定概念的可控性和可解释性问题，传统方法需要大量标注数据，而本文旨在通过最小监督实现特定概念的精准定位和操控。

Method: 结合激活归一化、分离正则化器以及锚定或子空间正则化器，将稀有标注样本吸引到预定义方向或轴对齐子空间，形成锚定几何结构。

Result: 在结构化自编码器上的实验显示，该方法能选择性衰减目标概念而对正交特征影响可忽略，完全消除概念时重建误差接近理论界限。

Conclusion: 稀疏概念锚定为学习表示中的可解释、可操控行为提供了实用途径，实现了概念级别的精确干预。

Abstract: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.

</details>


### [283] [AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models](https://arxiv.org/abs/2512.12493)
*Vaarunay Kaushal,Rajib Mall*

Main category: cs.LG

TL;DR: 该研究比较了决策树和LSTM模型在在线学习环境中预测风险学生的表现，发现不同干预阶段需要不同的性能指标：早期需要高召回率，中期需要平衡的精确率-召回率，后期需要高精确率。


<details>
  <summary>Details</summary>
Motivation: 在线学习环境中早期识别风险学生对于有效干预至关重要。研究旨在探索不同时间点的预测模型表现，以确定最佳干预时机和模型选择策略。

Method: 研究将时间预测分析扩展到第20周（课程时长的50%），在六个时间快照上比较决策树和LSTM模型。分析不同干预阶段所需的性能指标，并评估静态人口统计特征的重要性。

Result: 静态人口统计特征占预测重要性的68%，支持无评估的早期预测。LSTM模型在第2周达到97%的召回率，适合早期干预；决策树在中期提供稳定的平衡性能（78%准确率）。到第20周，两种模型的召回率相似（68%），但LSTM的精确率更高（90% vs 86%）。

Conclusion: 模型选择应取决于干预时机，早期信号（第2-4周）足以使用主要的人口统计和入学前信息进行可靠的初始预测。不同干预阶段需要不同的性能指标优先级。

Abstract: Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.

</details>


### [284] [Policy Optimization for Dynamic Heart Transplant Allocation](https://arxiv.org/abs/2512.12497)
*Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文开发了一个新的心脏移植分配政策模拟器，发现现行政策远不如最大化移植获益年数的近视政策，并提出使用"潜力值"的动态优化政策和批量处理策略来改进分配效率。


<details>
  <summary>Details</summary>
Motivation: 心脏移植是晚期心衰患者的可行治疗路径，但供体严重短缺。尽管2018年修订了分配政策，但现有政策未能充分考虑移植前和移植后的死亡率，需要改进分配效率。

Method: 使用UNOS历史数据开发新的模拟器来评估不同分配政策；比较现状政策与近视政策（最大化移植获益年数）；引入"潜力值"概念来衡量患者在后续分配中的效用，开发动态优化政策；探索批量处理供体的策略。

Result: 现状政策明显劣于近视政策；基于潜力值的动态优化政策能进一步提升分配效率；批量处理少量供体（适用于某些类型供体）能进一步提高性能；模拟器还能评估地理邻近性和移植中心拒绝倾向等关键因素。

Conclusion: 通过开发新的模拟器和引入潜力值概念，本文为改进心脏移植分配政策提供了重要工具和方法，能够更有效地匹配供体与患者，提高整体移植效果。

Abstract: Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.

</details>


### [285] [Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model](https://arxiv.org/abs/2512.12545)
*Bin Mu,Yuxuan Chen,Shijin Yuan,Bo Qin,Hao Guo*

Main category: cs.LG

TL;DR: TianXing-S2S是一个多圈层耦合的概率模型，用于全球次季节到季节(S2S)的日集合预报，通过扩散模型和最优传输耦合模块在45天预报中超越了ECMWF和FuXi-S2S系统。


<details>
  <summary>Details</summary>
Motivation: 在气候变化加速的背景下，准确的次季节到季节(S2S)极端事件预测对于资源规划和灾害减缓至关重要，但由于复杂的多圈层相互作用和固有的大气不确定性，此类预测仍然具有挑战性。

Method: 提出TianXing-S2S模型：首先将多样化的多圈层预测因子编码到紧凑的潜在空间，然后使用扩散模型生成日集合预报。在去噪器中加入基于最优传输(OT)的新型耦合模块，以优化大气与多圈层边界条件之间的相互作用。

Result: 在关键大气变量上，TianXing-S2S在1.5°分辨率的45天日平均集合预报中超越了欧洲中期天气预报中心(ECMWF) S2S系统和FuXi-S2S。模型能够熟练预测极端事件（如热浪和异常降水），并识别土壤湿度作为关键的前兆信号。此外，模型能够生成长达180天的稳定滚动预报。

Conclusion: TianXing-S2S为变暖世界中的S2S研究建立了一个稳健的框架，通过多圈层耦合和扩散模型方法显著提升了次季节到季节预测的准确性和极端事件预测能力。

Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.

</details>


### [286] [Optimal Mistake Bounds for Transductive Online Learning](https://arxiv.org/abs/2512.12567)
*Zachary Chase,Steve Hanneke,Shay Moran,Jonathan Shafer*

Main category: cs.LG

TL;DR: 该论文解决了在线学习中无标签数据效力的30年开放问题，证明了转导式在线学习与标准在线学习之间存在平方根差距，即转导式学习的最优错误界限为Ω(√d)，而标准在线学习为d。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习中关于无标签数据效力的30年开放问题，量化转导式学习与标准在线学习之间的性能差距。在标准在线学习中，最优错误界限由概念类的Littlestone维度d决定，但转导式学习（提前访问未标记实例序列）的界限一直未明确。

Method: 通过理论分析证明转导式在线学习的下界为Ω(√d)，并构造具体概念类证明上界为O(√d)，从而建立紧确界限。对比先前研究的下界Ω(log log d)、Ω(√log d)和Ω(log d)，以及上界(2/3)d。

Result: 证明了转导式在线学习的最优错误界限为Θ(√d)，相比标准在线学习的d实现了指数级改进。建立了转导式与标准在线学习之间的平方根差距，表明提前访问未标记实例序列能带来显著优势。

Conclusion: 转导式在线学习相比标准在线学习存在平方根优势，这与PAC学习设置中两者样本复杂度相似的情况形成鲜明对比，凸显了在线学习中提前访问未标记数据的重要性。

Abstract: We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $Ω(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $Ω(\log\log d)$, $Ω(\sqrt{\log d})$, and $Ω(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.

</details>


### [287] [On the Accuracy of Newton Step and Influence Function Data Attributions](https://arxiv.org/abs/2512.12572)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 本文对数据归因方法（NS和IF）进行了新的理论分析，首次在不假设全局强凸性的情况下，为凸学习问题提供了渐近紧致的误差界限，并解释了NS方法通常比IF更准确的现象。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法（如影响函数IF和单牛顿步NS）的理论分析存在两个主要局限：1）依赖全局强凸性假设，这在实践中往往不成立；2）误差界限在参数数量d和移除样本数k上缩放很差。这导致无法回答"各方法的误差渐近缩放规律是什么"和"哪种方法对给定数据集更准确"等基本问题。

Method: 针对凸学习问题，提出了对NS和IF数据归因方法的新分析框架。该分析不依赖全局强凸性假设，通过数学推导证明了对于行为良好的逻辑回归，所得界限在多项式对数因子内是渐近紧致的。

Result: 推导出了NS方法和IF方法的平均误差缩放规律：NS方法的误差为Õ(kd/n²)，NS与IF之间的差异为Õ((k+d)√(kd)/n²)。这些结果表明NS方法通常比IF更准确，解释了先前实验观察到的现象。

Conclusion: 本文首次在不假设全局强凸性的情况下，为数据归因方法提供了渐近紧致的理论分析，揭示了NS方法相对于IF方法的优势，并给出了明确的误差缩放规律，为数据归因的理论理解和实际应用提供了重要基础。

Abstract: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{\mathrm{NS}}\|_2 \bigr] = \widetildeΘ\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{\mathrm{NS}} - \hatθ_T^{\mathrm{IF}}\|_2 \bigr] = \widetildeΘ\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]

</details>


### [288] [Causal inference and model explainability tools for retail](https://arxiv.org/abs/2512.12605)
*Pranav Gupta,Nithin Surendran*

Main category: cs.LG

TL;DR: 该论文为零售商提供应用模型可解释性和因果推断的实用方法，通过分析真实数据集验证了可解释模型具有更稳定的SHAP值，并展示了双重机器学习方法能正确识别因果效应符号。


<details>
  <summary>Details</summary>
Motivation: 当前零售商虽然收集大量数据并应用机器学习技术分析关键指标，但现有模型缺乏可解释性，且无法验证或发现因果关系，这限制了从数据中获得深入业务洞察的能力。

Method: 论文回顾了因果推断和模型可解释性在电商零售领域的现有文献，并将其应用于真实数据集，特别关注了可解释模型的SHAP值方差分析，以及通过双重机器学习方法纳入多个混杂因素的技术。

Result: 研究发现：1）固有的可解释模型具有更低的SHAP值方差；2）通过双重机器学习方法纳入多个混杂因素能够获得正确的因果效应符号。

Conclusion: 论文为零售商提供了应用模型可解释性和因果推断的实用指南，这些方法能够帮助零售商从数据中获得更可靠、可解释的业务洞察，并验证因果关系。

Abstract: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.

</details>


### [289] [Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain](https://arxiv.org/abs/2512.12617)
*Animesh Mishra*

Main category: cs.LG

TL;DR: 提出Spectral Sentinel框架，利用随机矩阵理论检测拜占庭攻击，在异构数据下实现可扩展的拜占庭鲁棒联邦学习，支持高达15亿参数模型。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化联邦学习防御面临可扩展性三难困境：距离过滤可能拒绝合法的非独立同分布更新，几何中值方法计算成本过高，多数认证防御仅评估低于1亿参数的模型。

Method: 基于随机矩阵理论，利用诚实非独立同分布梯度协方差矩阵特征谱符合Marchenko-Pastur定律的特性，通过Frequent Directions草图技术和数据依赖的MP跟踪检测拜占庭扰动引起的尾部异常。

Result: 在144种攻击-聚合器配置中验证，平均准确率达到78.4%，优于基线方法的48-63%；支持高达15亿参数模型，内存使用为O(k²)且k≪d。

Conclusion: Spectral Sentinel框架在理论和实践上解决了去中心化联邦学习的可扩展拜占庭防御问题，实现了最小化最优的收敛速率，并通过区块链集成验证了系统可行性。

Abstract: Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \ll d$. Under a $(σ,f)$ threat model with coordinate-wise honest variance bounded by $σ^2$ and $f < 1/2$ adversaries, we prove $(ε,δ)$-Byzantine resilience with convergence rate $O(σf / \sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $Ω(σf / \sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.

</details>


### [290] [DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization](https://arxiv.org/abs/2512.12669)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Guoqing Ma,Yidan Liang,Jingjiang Liu,Hao Chen,Shimin Di*

Main category: cs.LG

TL;DR: DynaGen是一个统一的时序知识图谱推理方法，通过动态构建实体中心子图和条件扩散过程，分别解决插值和外推任务，在六个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱推理方法面临两个关键挑战：插值方法中的有限上下文建模，以及外推方法中的认知泛化偏差。插值方法通常将时间信息嵌入到单个事实中，而外推技术则利用序列模型来识别重复模式，但都存在局限性。

Method: DynaGen采用统一方法：对于插值任务，动态构建实体中心子图，并使用协同双分支GNN编码器捕捉演化结构上下文；对于外推任务，应用条件扩散过程，迫使模型学习底层演化原理而非表面模式。

Result: 在六个基准数据集上的实验表明，DynaGen实现了最先进的性能。与次优模型相比，平均MRR分数在插值任务上提高了2.61分，在外推任务上提高了1.45分。

Conclusion: DynaGen通过动态上下文建模和条件扩散过程，有效解决了时序知识图谱推理中的插值和外推挑战，实现了统一的强大框架。

Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.

</details>


### [291] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

TL;DR: 本文挑战了当前视觉语言模型推理训练中"强化学习优于监督微调"的主流观点，通过系统对比发现SFT在多个场景下具有关键作用，包括对弱模型更有效、数据效率更高、跨模态泛化更强等。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型推理领域过度强调强化学习的作用，普遍认为监督微调不仅无法提升推理能力，还可能对模型训练产生负面影响。本研究旨在重新审视这种以强化学习为中心的观点，通过系统对比来澄清SFT和RL在不同条件下的相对有效性。

Method: 采用系统化、受控的实验设计，在相同数据源下对比监督微调和强化学习在视觉语言模型推理中的表现。研究考察了模型容量、数据规模和数据分布对两种方法效果的影响，并分析了欺骗性奖励问题。

Result: 研究发现：1) SFT对较弱模型更有效，能更可靠地激发其推理能力；2) SFT数据效率更高，仅用2K数据就能达到RL使用20K数据的效果；3) SFT跨模态泛化能力更强；4) RL存在欺骗性奖励问题，即更高的奖励并不对应更好的推理准确性。

Conclusion: 监督微调在视觉语言模型推理训练中的作用被低估了，SFT和RL应被视为互补组件。研究挑战了"RL优于SFT"的主流叙事，支持构建更平衡的后训练流程，根据具体场景选择合适的方法组合。

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [292] [Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits](https://arxiv.org/abs/2512.12693)
*Sumantrak Mukherjee,Serafima Lebedeva,Valentin Margraf,Jonas Hanselle,Kanta Yamaoka,Viktor Bengs,Stefan Konigorski,Eyke Hüllermeier,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 提出一种贝叶斯框架，用于在上下文部分可观测的多任务多臂老虎机中进行高效探索，通过潜在上下文变量捕捉奖励分布间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 在多任务多臂老虎机设置中，上下文通常只能部分观测，且奖励分布之间存在由潜在上下文变量诱导的结构依赖关系。现有方法难以有效利用这些跨任务的结构依赖，特别是在模型设定错误或存在复杂潜在异质性的情况下。

Method: 提出贝叶斯框架，通过粒子逼近的对数密度高斯过程表示任务和奖励的联合分布，从而灵活地发现臂间和任务间的依赖关系，无需对潜在变量做先验假设。该方法整合所有任务的观测数据学习全局联合分布，同时允许对新任务进行个性化推断。

Result: 实证结果表明，该方法在模型设定错误或存在复杂潜在异质性的设置中，优于分层模型老虎机等基线方法。

Conclusion: 提出的贝叶斯框架能够有效利用跨任务的结构依赖关系，通过识别结构不确定性和用户特定不确定性两种关键认知不确定性来源，在多任务多臂老虎机中实现更高效的探索。

Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.

</details>


### [293] [Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models](https://arxiv.org/abs/2512.12744)
*Haotian Xu,Tian Gao,Tsui-Wei Weng,Tengfei Ma*

Main category: cs.LG

TL;DR: 该研究提出了一种通过添加可训练自发神经元来改善大型语言模型输入稀疏化性能的方法，将输入稀疏化重新解释为动态结构剪枝，并引入生物神经元启发的自发神经元来稳定激活，显著减少了稀疏化带来的性能差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但规模庞大带来了效率和可解释性挑战。输入稀疏化通过选择性激活输入值子集来提高效率，但现有方法主要关注计算节省，忽视了稀疏化的表示后果，导致与完整模型相比存在明显的性能差距。

Method: 首先将输入稀疏化重新解释为动态结构剪枝。受生物神经元自发基线放电率的启发，引入一小组可训练的自发神经元作为补偿单元，以稳定稀疏化LLM中的激活。这些辅助神经元在稀疏化过程中提供补偿机制。

Result: 实验表明，这些辅助神经元显著减少了稀疏化引起的性能差距，同时在各种任务上都能有效泛化，提高了稀疏化模型的性能表现。

Conclusion: 通过引入受生物启发的自发神经元，可以有效地改善输入稀疏化大型语言模型的性能，在保持计算效率的同时减少性能损失，为高效LLM设计提供了新思路。

Abstract: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.

</details>


### [294] [Federated Learning with Feedback Alignment](https://arxiv.org/abs/2512.12762)
*Incheol Baek,Hyungbin Kim,Minseo Kim,Yon Dohn Chung*

Main category: cs.LG

TL;DR: FLFA将反馈对齐机制引入联邦学习，通过使用全局模型权重作为共享反馈矩阵来对齐本地更新，有效缓解数据异构性导致的本地漂移问题，且计算和通信开销极小。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据异构（非IID）场景下存在本地漂移问题，这会阻碍全局模型的收敛。现有方法在处理这一挑战时往往带来较大的计算或通信开销。

Method: 提出FLFA框架，将反馈对齐机制集成到联邦学习中。在本地训练的反向传播过程中，使用全局模型的权重作为共享反馈矩阵，从而有效对齐本地更新与全局模型方向。

Result: 理论分析表明FLFA能有效缓解本地漂移，并保证本地和全局模型的鲁棒收敛。实证评估显示FLFA能提升其他FL方法的性能，在准确率比较和本地漂移测量中均表现优异。

Conclusion: FLFA通过创新的反馈对齐机制，以极小的额外计算成本和零额外通信开销，有效解决了联邦学习中的数据异构性和本地漂移问题，具有实际应用价值。

Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.

</details>


### [295] [OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging](https://arxiv.org/abs/2512.12779)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: OLR-WAA是一种无超参数的自适应在线回归模型，通过动态加权平均机制处理数据流中的概念漂移，在稳定性和适应性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集经常呈现不断演化的数据分布（概念漂移），忽略这一现象会显著降低模型预测性能。在线模型中的超参数通常是固定的，缺乏动态适应变化数据的能力，这加剧了问题。

Method: 提出OLR-WAA模型：1）通过指数加权移动平均增量更新基础模型；2）引入独特的优化机制，动态检测概念漂移、量化其幅度，并根据实时数据特征调整模型；3）采用保守更新策略处理置信度场景，优先考虑稳定、高置信度的数据点。

Result: 在静态设置下与批量回归性能相当；在概念漂移数据集上显著优于其他在线模型，有效弥补性能差距；快速收敛，相比其他在线模型持续产生更高的R2值。

Conclusion: OLR-WAA是一种有效的无超参数在线回归模型，能够处理非平稳数据流，在概念漂移环境下表现出色，在模型稳定性和适应性之间取得了良好平衡。

Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.

</details>


### [296] [Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset](https://arxiv.org/abs/2512.12783)
*Atalay Denknalbant,Emre Sezdi,Zeki Furkan Kutlu,Polat Goktas*

Main category: cs.LG

TL;DR: 该研究创建了伊斯坦布尔居民的合成数据集，通过替代金融数据（如手机使用、消费习惯等）来评估缺乏正式信用记录的借款人，证明这些行为属性能显著提升信用评估模型的性能。


<details>
  <summary>Details</summary>
Motivation: 金融排斥限制了创业机会、增加了收入波动并扩大了财富差距。伊斯坦布尔的银行服务不足人群由于收入和支付通过非正规渠道流动，往往没有信用局档案。研究旨在探索如何评估这类借款人。

Method: 1. 创建包含10万伊斯坦布尔居民的合成数据集，重现2025年第一季度土耳其统计局人口普查边际和电信使用模式
2. 使用检索增强生成技术将公共统计数据输入OpenAI o3模型，生成真实但私密的记录
3. 每个档案包含7个社会人口变量和9个替代属性（手机规格、在线购物节奏、订阅支出、汽车所有权、月租金、信用卡标志）
4. 使用CatBoost、LightGBM和XGBoost训练两个版本模型：演示模型仅使用社会人口变量，完整模型包含所有属性
5. 采用五折分层验证评估模型性能

Result: 1. 替代数据块将AUC提高了约1.3个百分点
2. 平衡F1分数从约0.84提升到0.95，增益达14%
3. 简洁的行为属性集能够接近信用局的区分能力，同时服务于缺乏正式信用记录的借款人

Conclusion: 研究贡献包括：开放的伊斯坦布尔2025年第一季度合成数据集、完全可复现的建模流程、以及经验证据表明简洁的行为属性集能够接近信用局的区分能力。这些发现为贷款机构和监管机构提供了透明蓝图，以扩展对银行服务不足人群的公平和安全信贷访问。

Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.

</details>


### [297] [OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average](https://arxiv.org/abs/2512.12785)
*Mohammad Abu Shaira,Yunhe Feng,Heng Fan,Weishi Shi*

Main category: cs.LG

TL;DR: OLC-WA是一种无需超参数的自适应在线分类模型，通过指数加权移动平均融合数据流与基础模型，自动检测概念漂移并动态调整，在动态数据流中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现随时间演化的数据分布（概念漂移），忽视此现象会显著降低模型预测精度。在线模型中的超参数通常是固定的，无法随数据分布变化动态调整，这加剧了问题。

Method: 提出OLC-WA（在线分类加权平均）模型，采用指数加权移动平均将新数据流与现有基础模型融合。集成优化机制能动态检测概念漂移、量化其幅度，并根据观测到的数据流特征调整模型。

Result: 在多种基准数据集上的严格实证评估表明：在静态环境中，OLC-WA性能与批处理模型相当，准确率保持在1-3%范围内；在漂移情况下，OLC-WA超越领先的在线基线方法10-25%。

Conclusion: OLC-WA是一种有效的自适应在线分类模型，无需超参数且具备自动优化机制，能够在流式环境中有效适应不断变化的数据分布，在概念漂移场景下表现优异。

Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.

</details>


### [298] [Unveiling Statistical Significance of Online Regression over Multiple Datasets](https://arxiv.org/abs/2512.12787)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 本文探讨了机器学习中多算法跨数据集比较的统计检验方法，特别关注在线学习场景，使用Friedman检验和事后检验评估多个在线回归模型在不同数据集上的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单个数据集上两种算法的性能评估，而多算法跨数据集比较的统计检验方法在机器学习研究中被忽视。在线学习领域需要统计显著性验证来确保连续学习过程的有效性，特别是快速收敛和及时处理概念漂移。

Method: 采用Friedman检验和相应的事后检验来比较多个在线回归模型在不同数据集上的性能。使用真实和合成数据集，通过5折交叉验证和种子平均进行全面的评估。

Result: 测试结果总体上证实了竞争基线的性能与其个体报告一致。然而，一些统计检验结果表明，最先进方法在某些方面仍有改进空间。

Conclusion: 需要稳健的统计方法来评估随时间变化的数据性能差异，多算法跨数据集比较的统计检验对于在线学习领域具有重要意义，现有方法虽然有效但仍有改进空间。

Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.

</details>


### [299] [Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks](https://arxiv.org/abs/2512.12792)
*Shivansh Sahni,Wenzhi Zhang*

Main category: cs.LG

TL;DR: LRT是一种具有自适应推理深度的Transformer架构，通过迭代更新、丢弃校正和学习停止机制，在数独任务上达到98.68%数字准确率和36.30%完整谜题准确率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在单次前向传播中完成推理，无法根据输入难度自适应调整计算深度，也难以纠正早期错误。需要一种能够动态调整推理步骤、支持错误校正的架构。

Method: 提出Liquid Reasoning Transformer，使用循环推理token进行多步内部更新，包含丢弃门校正早期错误、学习停止机制自适应确定计算深度，无需符号规则或搜索。

Result: 在数独推理任务上，LRT达到98.68%数字准确率和36.30%完整谜题准确率。分析显示丢弃门和停止门在稳定推理和调整计算深度中发挥不同重要作用。

Conclusion: LRT通过自适应深度推理机制在结构化推理任务上表现优异，其机制可扩展到国际象棋等更大规模推理任务，未来可扩展多token推理和更大领域应用。

Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.

</details>


### [300] [TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk](https://arxiv.org/abs/2512.12795)
*Mengying Yan,Ziye Tian,Siqi Li,Nan Liu,Benjamin A. Goldstein,Molei Liu,Chuan Hong*

Main category: cs.LG

TL;DR: TRACER框架通过迁移学习实时适应临床环境变化，解决电子健康记录预测模型因时间性人群变化导致的性能漂移问题


<details>
  <summary>Details</summary>
Motivation: 临床决策支持工具基于电子健康记录构建，常因时间性人群变化而出现性能漂移，特别是在临床环境变化初期仅影响部分患者，形成混合人群时。这种病例组合变化通常发生在系统级操作更新或新疾病（如COVID-19）出现后

Method: 提出TRACER框架，通过识别就诊级别的过渡成员身份，使用迁移学习调整预测模型而无需完全重新训练

Result: 在模拟研究中，TRACER优于基于历史或当代数据训练的静态模型。在实际应用中预测COVID-19过渡期间急诊就诊后的住院情况，TRACER提高了区分度和校准度

Conclusion: TRACER为在不断演变和异质的临床条件下保持稳健的预测性能提供了可扩展的方法

Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.

</details>


### [301] [From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs](https://arxiv.org/abs/2512.12805)
*Anastasiia Alokhina,Pan Li*

Main category: cs.LG

TL;DR: 本文为Transformer的大小泛化能力提供了理论框架，证明其输出误差受采样密度和数据内在维度影响


<details>
  <summary>Details</summary>
Motivation: Transformer在多种应用中展现出从较小token集合外推到较大集合的大小泛化能力，虽然经验上成功但缺乏严格理论刻画

Method: 建立理论框架分析几何数据的大小泛化现象，将几何数据表示为连续源的离散采样（如流形上的点云、图论中的图子图），核心贡献是Transformer离散样本输出与连续域等价输出之间的误差界

Result: 证明对于具有稳定位置编码的Transformer，误差界由采样密度和数据流形的内在维度决定，不同尺寸的图和点云实验证实了理论界的紧致性

Conclusion: 为Transformer的大小泛化现象提供了理论解释，建立了离散采样与连续表示之间的误差界限，为理解Transformer在几何数据上的外推能力提供了理论基础

Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.

</details>


### [302] [Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift](https://arxiv.org/abs/2512.12816)
*Hasan Burhan Beytur,Gustavo de Veciana,Haris Vikalo,Kevin S Chan*

Main category: cs.LG

TL;DR: 研究在概念漂移和有限预算下如何分配机器学习模型的训练和部署资源，提出了一个模型无关的框架，分析了资源分配、概念漂移动态和部署时机的相互作用。


<details>
  <summary>Details</summary>
Motivation: 当模型提供商将训练好的模型分发给多个客户端时，这些客户端设备支持本地推理但缺乏重新训练模型的能力，因此性能维护的负担落在提供商身上。需要解决在概念漂移和预算限制下的资源分配问题。

Method: 引入了一个模型无关的框架，捕捉资源分配、概念漂移动态和部署时机之间的相互作用。分析了概念持续时间分布对训练策略的影响，特别关注了递减平均剩余寿命（DMRL）和递增平均剩余寿命（IMRL）分布。在通信约束下研究了模型部署问题，证明了相关优化问题在温和条件下的拟凸性，并提出了随机调度策略。

Result: 在突然概念变化下，当概念持续时间遵循DMRL分布时，推导出了预算约束下的最优训练策略，并证明直观启发式方法在IMRL下是次优的。对于通信约束下的模型部署，提出的随机调度策略能够实现接近最优的客户端性能。

Conclusion: 该研究为概念漂移下的成本高效机器学习模型管理提供了理论和算法基础，对持续学习、分布式推理和自适应机器学习系统具有重要意义。

Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.

</details>


### [303] [Selective Conformal Risk Control](https://arxiv.org/abs/2512.12844)
*Yunpeng Xu,Wenge Guo,Zhi Wei*

Main category: cs.LG

TL;DR: 提出选择性共形风险控制框架，通过两阶段方法结合选择性分类和共形预测，在保证覆盖保证的同时减少预测集大小，提高实际效用。


<details>
  <summary>Details</summary>
Motivation: 共形预测虽然提供分布无关的覆盖保证，但通常产生过大的预测集，限制了实际应用。需要一种方法在保持可靠性的同时减少预测集大小。

Method: 提出选择性共形风险控制框架：第一阶段选择置信样本进行预测，第二阶段在选定子集上应用共形风险控制构建校准预测集。开发了两种算法：SCRC-T（保持可交换性，提供精确有限样本保证）和SCRC-I（仅校准变体，提供PAC式概率保证，计算更高效）。

Result: 在两个公共数据集上的实验显示，两种方法都达到了目标覆盖和风险水平，性能几乎相同。SCRC-I表现出稍微更保守的风险控制，但计算实用性更优。

Conclusion: 选择性共形风险控制为紧凑、可靠的不确定性量化提供了有效且高效的途径。

Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.

</details>


### [304] [Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels](https://arxiv.org/abs/2512.12870)
*Pouya Ahadi,Blair Winograd,Camille Zaug,Karunesh Arora,Lijun Wang,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出了一种新的主动学习框架，通过优化分配查询点给标注者来最小化标签噪声，从而提高分类模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 主动学习中标注者提供的标签往往存在噪声，特别是复杂样本更容易被错误标注，这会导致分类器性能下降。需要一种方法来最小化标签噪声对模型训练的影响。

Method: 提出了一个包含两个关键组件的主动学习框架：1）分配模型，优化地将查询点分配给标注者以最小化每个周期内的最大可能噪声；2）新的采样方法，选择最佳查询点以减少标签噪声对分类器性能的影响。

Result: 实验结果表明，该方法相比多个基准方法显著提高了分类性能。

Conclusion: 通过优化标注分配和查询点选择，可以有效减少主动学习中的标签噪声问题，构建更鲁棒的分类模型。

Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.

</details>


### [305] [Unsupervised learning of multiscale switching dynamical system models from multimodal neural data](https://arxiv.org/abs/2512.12881)
*DongKyu Kim,Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出了一种无监督学习算法，用于学习具有切换动态的多尺度神经系统模型，能够同时处理多种神经模态（如尖峰和局部场电位），无需标记数据即可准确解码行为。


<details>
  <summary>Details</summary>
Motivation: 神经群体活动常表现出依赖于状态的切换动态，现有方法主要针对单一神经模态（连续高斯信号或离散泊松信号），但实际中常同时记录多种模态来测量不同时空尺度的脑活动。此外，训练数据通常缺乏状态标签，这给学习切换动态模型带来了挑战。

Method: 开发了一种新颖的无监督学习算法，仅使用多尺度神经观测数据来学习切换多尺度动态系统模型的参数。该方法能够同时处理多种神经模态，无需状态标签即可学习切换动态。

Result: 通过模拟和两个不同的实验数据集（包含不同运动任务期间的多模态尖峰-LFP观测）验证了方法。发现切换多尺度动态系统模型比切换单尺度动态模型更准确地解码行为，显示了多尺度神经融合的成功。此外，模型优于静态多尺度模型，说明了在多种神经数据中跟踪状态依赖非平稳性的重要性。

Conclusion: 该无监督学习框架通过利用多模态记录中的信息并纳入状态切换，能够更准确地建模复杂的多尺度神经动态。这种方法有望提高脑机接口的性能和鲁棒性，并增进对行为神经基础的理解。

Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.

</details>


### [306] [Wait, Wait, Wait... Why Do Reasoning Models Loop?](https://arxiv.org/abs/2512.12895)
*Charilaos Pipis,Shivam Garg,Vasilis Kontonis,Vaishnavi Shrivastava,Akshay Krishnamurthy,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: 研究发现推理模型在低温下容易陷入文本循环重复，主要原因是学习误差和Transformer的归纳偏置，高温虽能缓解但非根本解决方案。


<details>
  <summary>Details</summary>
Motivation: 推理模型（如DeepSeek-R1）在解决复杂问题时生成长思维链，但经常在低温或贪婪解码下陷入文本循环重复。研究旨在探究这种现象的原因以及温度参数在其中扮演的角色。

Method: 通过分析开源推理模型，发现低温下循环现象普遍；引入合成图推理任务来研究机制，识别出两种导致循环的原因：学习难度导致的规避风险行为，以及Transformer对时间相关错误的归纳偏置。

Result: 研究发现：1）低温下循环现象常见；2）大模型循环较少，蒸馏学生模型循环严重；3）学习误差是主要原因；4）高温通过促进探索减少循环，但无法修复学习误差，导致生成文本仍过长。

Conclusion: 温度参数只是权宜之计而非根本解决方案，需要训练时干预来直接减少学习误差。论文最后讨论了旨在直接减少学习误差的训练时干预方法。

Abstract: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.

</details>


### [307] [Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning](https://arxiv.org/abs/2512.12896)
*Parthasarathy Nadarajan,Michael Botsch*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的预测占用网格方法，用于复杂交通场景的多目标演化预测，相比传统模型方法显著降低计算负载，实现实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的预测占用网格方法虽然能详细建模交通参与者的行为不确定性，但由于每个参与者可能的轨迹数量巨大，计算负载非常高，难以实现实时应用。因此需要寻找更高效的计算方法。

Method: 首先提出基于模型的预测占用网格计算方法，然后采用机器学习方法替代。使用新颖的网格表示方法（增强单元占用网格）表示当前交通场景状态，并基于随机森林算法将当前状态映射到预测占用网格。

Result: 通过交通场景模拟对比机器学习方法和模型方法，结果显示机器学习方法性能良好，能够实现预测占用网格的实时计算，为车辆安全应用提供可能。

Conclusion: 基于机器学习的预测占用网格方法能够有效降低计算复杂度，实现实时预测，通过详细建模不确定性，可以改进车辆安全系统中的关键组件如临界性估计和轨迹规划。

Abstract: This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.

</details>


### [308] [Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm](https://arxiv.org/abs/2512.12901)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 使用机器学习预测复杂交通场景的概率时空表示，通过层次分类器识别场景类型，用随机森林预测交通参与者未来行为的概率占用网格，并通过堆叠降噪自编码器提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 为主动车辆安全应用提供复杂交通场景的概率时空表示，特别是在执行动态机动操作时，需要准确预测交通参与者的未来行为以确保安全。

Method: 1. 使用层次情境分类器区分不同类型的交通场景，识别道路基础设施和安全相关交通参与者；2. 对每类相似场景，用随机森林预测概率时空表示（预测占用网格POG）；3. 使用堆叠降噪自编码器将增强占用网格降维为低维特征，提高随机森林的学习精度。

Result: 提出的机器学习方法（SDA+RF）在仿真和真实车辆实验中表现出优异性能，能够准确预测交通参与者的未来行为，并成功应用于评估交通场景临界性和确定安全轨迹。

Conclusion: 该方法为复杂交通场景提供了有效的概率时空表示，显著提升了主动车辆安全系统的预测能力，特别是在动态机动操作中的安全性评估和轨迹规划方面。

Abstract: In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.

</details>


### [309] [Next-generation reservoir computing validated by classification task](https://arxiv.org/abs/2512.12903)
*Ken-ichi Kitayama*

Main category: cs.LG

TL;DR: NG-RC（下一代储备池计算）无需实际储备池，直接从时间序列输入计算多项式项。先前基准测试局限于预测任务，本文首次证明NG-RC在分类任务上表现与传统RC相当。


<details>
  <summary>Details</summary>
Motivation: 现有NG-RC研究主要关注预测任务（如Lorenz 63吸引子和Mackey-Glass混沌信号），缺乏对其分类能力的验证。需要证明NG-RC在分类任务上与传统储备池计算相当，以验证其多功能计算能力。

Method: 采用下一代储备池计算（NG-RC）方法，该方法不依赖实际储备池进行输入数据混合，而是直接从时间序列输入计算多项式项。将NG-RC应用于分类任务进行评估。

Result: 首次证明NG-RC在分类任务上的性能与传统储备池计算相当，验证了NG-RC在预测和分类任务上的多功能计算能力。

Conclusion: NG-RC不仅适用于时间序列预测任务，在分类任务上也表现出与传统RC相当的性能，证明了其作为多功能计算范式的潜力。

Abstract: An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.

</details>


### [310] [Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic](https://arxiv.org/abs/2512.12907)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 提出了一种用于高效估计复杂交通场景概率时空表示的新型机器学习架构，通过识别交通场景类型并映射当前状态到未来状态，为自动驾驶和主动安全系统提供详细预测。


<details>
  <summary>Details</summary>
Motivation: 详细表示未来交通场景对自动驾驶和主动安全系统至关重要，需要能够预测交通参与者行为不确定性并生成概率时空表示的方法。

Method: 使用增强占用网格（AOG）作为输入，通过两个堆叠去噪自编码器（SDAs）和随机森林集合，输出预测占用网格（POG）作为概率时空表示，并与现有SDAs+DeconvNet架构进行比较。

Result: 通过仿真验证了所提架构的有效性，在准确性和计算时间方面与其他两种现有架构进行了比较，并展示了POG在主动安全领域的应用前景。

Conclusion: 提出的新型机器学习架构能够有效生成交通场景的概率时空表示，为自动驾驶和主动安全系统提供了可靠的技术支持，在准确性和计算效率方面表现出色。

Abstract: This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.

</details>


### [311] [SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision](https://arxiv.org/abs/2512.12930)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.LG

TL;DR: SeVeDo是一种基于SVD的异构加速器，通过将异常值敏感组件分离到高精度低秩路径，其余计算在低比特残差数据路径中执行，结合分层组量化和SVD引导的混合精度，实现了高效的Transformer推理。


<details>
  <summary>Details</summary>
Motivation: 低比特量化是提高Transformer推理效率的有效技术，但激进的比特宽度减少会因激活异常值导致精度下降。现有方法如异常值处理和组量化虽然能达到高精度，但能耗较高。需要设计一种既能保持精度又节能的量化方案。

Method: 提出SeVeDo加速器：1) 基于SVD的结构分离，将异常值敏感组件分配到高精度低秩路径；2) 其余计算在低比特残差数据路径中执行，采用组量化；3) 分层组量化(HGQ)结合粗粒度浮点缩放和细粒度移位，降低反量化成本；4) SVD引导的混合精度(SVD-MP)通过低秩分解识别精度敏感组件，静态分配更高比特宽度。

Result: SeVeDo实现了峰值能效13.8TOPS/W，在ViT-Base基准测试中达到12.7TOPS/W，在Llama2-7B基准测试中达到13.4TOPS/W，超越了传统设计。

Conclusion: SeVeDo通过SVD结构分离、分层组量化和混合精度分配，成功解决了低比特量化中的精度-能效权衡问题，为高效的Transformer推理提供了有效的硬件加速方案。

Abstract: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.

</details>


### [312] [Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties](https://arxiv.org/abs/2512.12947)
*Nischal Subedi,Ember Kerstetter,Winnie Li,Silo Murphy*

Main category: cs.LG

TL;DR: GCNs在标签极度稀缺时表现最佳，利用图结构补偿监督不足；在高度同质化图上，即使特征被随机噪声替代，GCNs仍能保持性能；但在低同质化且特征强时，GCNs会损害性能。


<details>
  <summary>Details</summary>
Motivation: 解决实践者缺乏关于GCNs何时比简单基线提供有意义改进的明确指导的问题，通过诊断研究理解GCNs何时以及为何有效。

Method: 使用Amazon Computers共购数据进行诊断研究，通过系统实验包括模拟标签稀缺、特征消融和按类别分析，研究图同质化与特征质量之间的相互作用。

Result: GCNs在极端标签稀缺时提供最大增益；在高度同质化图上，即使节点特征被随机噪声替代，GCNs仍能匹配原始性能；但在低同质化且特征强时，GCNs会损害性能。象限分析显示GCNs在四种条件中的三种有帮助，仅在低同质化遇到强特征时有害。

Conclusion: GCNs的性能关键取决于图同质化与特征质量之间的相互作用，这些发现为实践者决定是否采用基于图的方法提供了实用指导。

Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.

</details>


### [313] [CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks](https://arxiv.org/abs/2512.12981)
*Jonathan Wenshøj,Tong Chen,Bob Pepin,Raghavendra Selvan*

Main category: cs.LG

TL;DR: CoDeQ是一种完全可微的联合剪枝-量化方法，通过参数化量化器的死区宽度来直接诱导稀疏性，无需外部辅助过程即可在单次端到端优化中同时确定稀疏模式和量化参数。


<details>
  <summary>Details</summary>
Motivation: 现有的联合剪枝-量化方法依赖训练循环外的辅助程序来确定压缩参数，这增加了工程复杂性、需要超参数调优，且缺乏直接的数据驱动梯度信号，可能导致次优压缩。

Method: 基于量化器死区等价于幅度剪枝的关键观察，参数化死区宽度并通过反向传播学习，同时学习量化参数。该方法提供稀疏性的显式控制，通过单个全局超参数正则化，并将稀疏性选择与位宽选择解耦。

Result: 在ImageNet上使用ResNet-18，CoDeQ将比特运算减少到约5%，同时在固定精度和混合精度机制下保持接近全精度准确率。

Conclusion: CoDeQ是一种简单、完全可微的联合剪枝-量化方法，无需辅助程序，架构无关且易于实现，在单次端到端优化中同时确定稀疏模式和量化参数，实现了高效的模型压缩。

Abstract: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.

</details>


### [314] [Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)](https://arxiv.org/abs/2512.13010)
*Hassan Iftikhar,Rizwan Ahmad,Arunark Kolipaka*

Main category: cs.LG

TL;DR: DIME深度学习框架在磁共振弹性成像中比传统MMDI算法更准确地估计组织剪切刚度，对噪声更鲁棒，在合成和真实数据中都表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统MMDI算法基于亥姆霍兹方程，假设均匀、无限介质，且拉普拉斯算子对噪声敏感，导致刚度估计不准确不可靠。

Method: 提出DIME深度学习框架，使用有限元模拟生成的位移场-刚度图对进行训练，采用小图像块捕捉局部波行为，提高对全局图像变化的鲁棒性。

Result: 在合成数据中，DIME比MMDI产生更低的像素间变异性、更准确的边界描绘和更高的地面真值相关性；在真实肝脏数据中，DIME保持生理一致的刚度模式，而MMDI显示方向性偏差。

Conclusion: DIME在磁共振弹性成像中显示出比传统MMDI方法更高的准确性和鲁棒性，为临床应用提供了可行性。

Abstract: The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.

</details>


### [315] [Scaling Bidirectional Spans and Span Violations in Attention Mechanism](https://arxiv.org/abs/2512.13033)
*Jongwook Kim,Sangheon Yun,Sukjin Yoon*

Main category: cs.LG

TL;DR: 提出优化Transformer训练框架，通过非对称投影分解反向传播梯度，保持前向QKV结构不变，在WikiText-2上验证损失降低0.56%


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的O(N²)复杂度仍是序列建模的实证性能前沿，但其训练存在几何效率问题。标准注意力梯度可能不是最优的，需要优化训练过程。

Method: 提出优化框架，使用非对称投影将反向传播梯度分解为平行跨度和正交违规分量，同时保持标准前向QKV结构不变。选择性地缩放这些分量，主要关注0阶双向平行跨度。

Result: 在有限的WikiText-2数据集上，使用粗略配置，该方法实现了验证损失降低0.56%，证实了框架的基本有效性，并表明在更大数据集和更深训练机制上有显著潜力。

Conclusion: 提供了强有力的理论证据：标准注意力梯度是次优的。通过选择性缩放梯度分量，特别是0阶双向平行跨度，可以获得最有效的学习信号。该方法在有限数据集上已显示效果，预计在更大规模应用中有显著增益。

Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes

</details>


### [316] [Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization](https://arxiv.org/abs/2512.13034)
*Xiaoyu He,Yu Cai,Jin Jia,Canxi Huang,Wenqing Chen,Zibin Zheng*

Main category: cs.LG

TL;DR: Alada是一种用于大规模矩阵随机优化的自适应动量方法，通过秩一分解估计梯度二阶矩，具有亚线性内存开销，性能与Adam相当但内存效率更高。


<details>
  <summary>Details</summary>
Motivation: 传统自适应优化方法（如Adam）在处理大规模矩阵优化时内存开销较大，需要更高效的内存优化方法。

Method: Alada采用秩一分解方法估计梯度二阶矩，通过交替更新因子最小化估计误差，并配备一阶矩估计规则增强鲁棒性。

Result: 在多个自然语言处理任务上的数值研究表明，Alada相比Adam及其变体显著减少了内存开销，并在训练大模型时表现出更好的鲁棒性。

Conclusion: Alada是一种内存高效的随机优化方法，在保持与Adam相当理论性能的同时，显著降低了内存开销，适用于大规模矩阵和张量优化。

Abstract: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.

</details>


### [317] [Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection](https://arxiv.org/abs/2512.13040)
*Xuwei Tan,Yao Ma,Xueru Zhang*

Main category: cs.LG

TL;DR: FinFRE-RAG：一种用于金融欺诈检测的两阶段方法，通过重要性引导的特征缩减将表格数据转化为自然语言，并利用检索增强的上下文学习，显著提升LLMs在欺诈检测中的性能，同时提供可解释的预测依据。


<details>
  <summary>Details</summary>
Motivation: 传统表格模型在金融欺诈检测中需要大量特征工程，且可解释性差，难以让人类理解预测结果。虽然大语言模型能生成人类可读的解释并减少分析师工作量，但直接应用于表格欺诈检测时性能不佳，主要因为难以处理大量特征、极端类别不平衡和缺乏上下文信息。

Method: 提出FinFRE-RAG两阶段方法：1）重要性引导的特征缩减，将数值/分类属性的紧凑子集序列化为自然语言；2）检索增强的上下文学习，基于标签感知的实例级示例进行学习。

Result: 在四个公共欺诈数据集和三类开源LLMs上，FinFRE-RAG显著优于直接提示方法，F1/MCC指标大幅提升，在多个设置中与强大的表格基线模型具有竞争力。虽然LLMs仍落后于专用分类器，但缩小了性能差距。

Conclusion: FinFRE-RAG有效提升了LLMs在金融欺诈检测中的性能，同时提供可解释的推理依据，突出了其作为欺诈分析辅助工具的价值，为结合LLMs优势与表格数据处理需求提供了可行方案。

Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

</details>


### [318] [Multi-fidelity aerodynamic data fusion by autoencoder transfer learning](https://arxiv.org/abs/2512.13069)
*Javier Nieto-Centenero,Esther Andrés,Rodrigo Castellanos*

Main category: cs.LG

TL;DR: 提出多保真度深度学习框架，结合自动编码器迁移学习和多分割共形预测，在数据稀缺条件下实现不确定性感知的空气动力学数据融合


<details>
  <summary>Details</summary>
Motivation: 高保真度模拟计算成本过高，限制了数据驱动建模的应用；需要开发多保真度策略，利用低成本低保真度信息而不牺牲精度

Method: 使用自动编码器迁移学习学习紧凑的潜在物理表示作为冻结知识库，解码器用稀缺的高保真样本微调；开发多分割共形预测策略进行不确定性量化

Result: 在NACA翼型（2D）和跨音速机翼（3D）表面压力分布测试中，模型成功校正低保真度偏差，使用极少高保真训练数据实现高精度压力预测；MSCP框架产生稳健的不确定性带，点覆盖超过95%

Conclusion: 通过结合极端数据效率和不确定性量化，为数据稀缺环境中的空气动力学回归提供了可扩展且可靠的解决方案

Abstract: Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.

</details>


### [319] [LikeBench: Evaluating Subjective Likability in LLMs for Personalization](https://arxiv.org/abs/2512.13077)
*Md Awsafur Rahman,Adam Gabrys,Doug Kang,Jingjing Sun,Tian Tan,Ashwin Chandramouli*

Main category: cs.LG

TL;DR: LikeBench是一个评估LLM个性化能力的新基准，特别关注"喜好度"这一主观但关键的用户体验维度，通过多会话动态框架测量模型在七个维度上适应用户偏好的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化基准主要关注准确回忆用户信息和应用这些信息，但忽略了"喜好度"这一主观且对用户体验至关重要的维度。用户不仅希望LLM记住事实，更希望获得符合个人偏好的回应。

Method: 提出LikeBench多会话动态评估框架：1）让LLM与模拟用户对话，仅从对话中学习偏好；2）将喜好度分解为七个诊断维度：情感适应、正式度匹配、知识适应、引用理解、对话长度匹配、幽默匹配、回调；3）使用细粒度、基于心理学的描述性人物角色而非粗糙的高低特质评分。

Result: 实验发现：1）强记忆性能不保证高喜好度（DeepSeek R1记忆准确率86%但喜好度优于记忆准确率93%的Qwen3）；2）即使是SOTA模型如GPT-5在短交流中适应良好，但在更长、更嘈杂的交互中鲁棒性有限；3）喜好度与记忆准确率相关性较弱。

Conclusion: 喜好度是LLM个性化的重要维度，需要专门的评估框架。LikeBench提供了更全面、细粒度的评估方法，揭示了模型在适应用户主观偏好方面的局限性，为未来个性化LLM研究指明了方向。

Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.

</details>


### [320] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

TL;DR: 提出TraPO算法，通过少量标注样本指导无标注样本的强化学习训练，显著提升数学推理任务的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督RLVR方法虽然避免了高标注成本，但在训练后期容易出现模型崩溃问题，这是因为缺乏外部监督可能导致强化错误的推理模式。需要一种既能降低标注成本又能保持训练稳定性的方法。

Method: 提出半监督RLVR范式TraPO算法，利用少量标注样本来指导无标注样本的训练。核心思想是通过比较无标注样本与标注样本的学习轨迹相似性来识别可靠的无标注样本，确保只有经过标注实例验证的推理模式被纳入强化学习训练。

Result: 在六个数学推理基准测试和三个分布外任务上表现出色。仅使用1K标注和3K无标注样本就达到42.6%平均准确率，超越了在45K无标注样本上训练的最佳无监督方法（38.3%）。使用4K标注和12K无标注样本时，在所有基准测试上甚至超越了使用全部45K标注样本的完全监督模型，而仅使用了10%的标注数据。

Conclusion: TraPO算法通过半监督RLVR范式有效解决了无监督方法中的模型崩溃问题，实现了显著的数据效率和强大的泛化能力，为降低大推理模型的标注成本提供了有效解决方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [321] [Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency](https://arxiv.org/abs/2512.13149)
*Xinwei Tai,Dongmian Zou,Hongfei Wang*

Main category: cs.LG

TL;DR: 该论文提出通过解相关节点特征来解决图域自适应中的条件偏移问题，使用解相关的GCN层和图Transformer层实现，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 图机器学习方法近年来取得显著进展，但如何有效地将知识从一个图迁移到另一个图仍然是一个关键挑战。无监督图域自适应（GDA）面临的主要困难是条件偏移问题，这会阻碍知识迁移。

Method: 论文首先通过理论分析证明条件偏移仅在节点特征存在局部依赖时才会出现。基于这一发现，提出通过解相关节点特征来改进GDA，具体实现包括解相关的GCN层和图Transformer层。

Result: 实验结果表明，该方法相比基线GDA方法有显著性能提升，并且在学习到的表示中显示出较小的类内距离，可视化结果清晰。

Conclusion: 通过解相关节点特征可以有效解决图域自适应中的条件偏移问题，提出的方法在理论和实验上都证明了其有效性，为图域自适应提供了新的解决方案。

Abstract: Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT

</details>


### [322] [SACn: Soft Actor-Critic with n-step Returns](https://arxiv.org/abs/2512.13165)
*Jakub Łyskawa,Jakub Lewandowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 本文提出了一种将Soft Actor-Critic与n步回报结合的方法，解决了传统结合方式中由于动作分布变化导致的偏差问题，通过数值稳定的重要性采样和τ采样熵估计来提升算法性能。


<details>
  <summary>Details</summary>
Motivation: SAC是当前最相关的离策略在线无模型强化学习方法，n步回报能提高RL算法的收敛速度。但SAC与n步回报的结合存在困难，因为通常的结合方式会因动作分布变化而引入偏差，而重要性采样虽然能解决此问题但可能导致数值不稳定。

Method: 1. 提出数值稳定的重要性采样方法，简化超参数选择；2. 在n步最大熵框架下分析SAC的熵估计方法，提出τ采样熵估计来降低学习目标的方差；3. 最终形成SACn算法。

Result: 在MuJoCo模拟环境中进行了实验验证，表明提出的方法能够有效结合SAC与n步回报，克服了传统方法的偏差和数值不稳定问题。

Conclusion: 成功开发了SACn算法，通过数值稳定的重要性采样和τ采样熵估计，实现了SAC与n步回报的有效结合，提高了算法的收敛速度和稳定性。

Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

</details>


### [323] [PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning](https://arxiv.org/abs/2512.13186)
*Khalid Ferji*

Main category: cs.LG

TL;DR: PolySet框架将聚合物表示为从摩尔质量分布中采样的有限加权链集合，解决了传统ML模型将聚合物视为单一完美分子图与真实材料由随机链分布组成之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 当前聚合物科学中的机器学习模型通常将聚合物视为单一完美定义的分子图，而真实材料由具有分布长度的随机链集合组成，这种物理现实与数字表示之间的不匹配限制了模型捕捉聚合物行为的能力。

Method: 引入PolySet框架，将聚合物表示为从假设的摩尔质量分布中采样的有限加权链集合，这种基于集合的编码独立于化学细节，与任何分子表示兼容，并在均聚物案例中使用最小语言模型进行说明。

Result: PolySet保留了高阶分布矩（如Mz、Mz+1），使机器学习模型能够学习尾部敏感特性，具有显著改善的稳定性和准确性。

Conclusion: 通过明确承认聚合物物质的统计性质，PolySet为未来聚合物机器学习建立了物理基础，自然可扩展到共聚物、嵌段结构和其他复杂拓扑结构。

Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.

</details>


### [324] [WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory](https://arxiv.org/abs/2512.13190)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Dongil Park,Sung Won Han*

Main category: cs.LG

TL;DR: 提出WAY深度学习架构，通过重构AIS轨迹为嵌套序列结构，结合通道聚合序列处理块和梯度丢弃技术，实现船舶目的地提前多天至数周的预测。


<details>
  <summary>Details</summary>
Motivation: 自动识别系统(AIS)数据存在可靠性问题和间隔不规则性，传统方法在船舶目的地预测方面存在时空偏差和分辨率损失问题，需要更有效的长期预测方法。

Method: 1) 将长港口到港口轨迹重构为嵌套序列结构，使用空间网格减轻时空偏差；2) 提出WAY架构：轨迹表示层生成多通道向量序列，CASP块使用多头通道和自注意力进行聚合和序列信息传递；3) 提出梯度丢弃技术，基于样本长度随机阻断梯度流，实现多对多训练。

Result: 在5年AIS数据上的实验表明，WAY优于传统空间网格方法，且梯度丢弃技术带来性能提升。通过多任务学习探索了ETA估计的实际应用潜力。

Conclusion: WAY架构通过创新的轨迹表示和注意力机制，结合梯度丢弃技术，有效解决了AIS数据中的时空偏差问题，实现了长期船舶目的地预测，具有实际应用价值。

Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.

</details>


### [325] [Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting](https://arxiv.org/abs/2512.13207)
*Karina Chichifoi,Fabio Merizzi,Michele Colajanni*

Main category: cs.LG

TL;DR: 该研究探讨了联邦学习在天气预报中的脆弱性，发现即使少数恶意客户端通过数据投毒攻击（全局偏置攻击和局部补丁攻击）就能显著扭曲温度预测，而基于修剪均值的防御机制对空间相关数据的补丁攻击无效。


<details>
  <summary>Details</summary>
Motivation: 深度学习与联邦学习结合为下一代天气预报提供了强大工具，但联邦学习的分布式特性引入了新的安全漏洞。特别是数据投毒攻击可能通过注入恶意训练数据来降低模型性能或引入系统性偏差，而气象数据的空间依赖性进一步放大了这种威胁。

Method: 研究使用Copernicus欧洲区域再分析数据集，模拟地理分布的客户端，评估基于补丁和全局偏置攻击对区域温度预测的影响。最后评估了修剪均值聚合作为防御机制的有效性。

Result: 结果显示：1）单个恶意客户端的全局温度偏置攻击可使预测偏移高达-1.7K；2）协调的补丁攻击使均方误差增加三倍以上，产生超过+3.5K的持续区域异常；3）修剪均值聚合能有效防御全局偏置攻击（仅2-13%性能下降），但对补丁攻击完全失效（性能下降281-603%）。

Conclusion: 联邦学习在天气预报应用中面临严重的安全威胁，特别是针对空间相关数据的投毒攻击。基于异常值检测的传统防御机制对空间相关的补丁攻击无效，需要开发更强大的防御策略来保护分布式气象预测系统。

Abstract: Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.

</details>


### [326] [ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data](https://arxiv.org/abs/2512.13228)
*Melvin Barbaux*

Main category: cs.LG

TL;DR: ModSSC是一个统一的半监督分类Python框架，支持多种算法、数据类型和硬件配置，通过YAML声明式实验配置促进可复现性研究。


<details>
  <summary>Details</summary>
Motivation: 现有半监督分类软件支持分散在不同方法和模态中，缺乏统一的框架来整合归纳式和直推式方法，阻碍了方法的比较和复现。

Method: 开发了ModSSC开源框架，实现广泛的经典和最新算法，提供表格、图像、文本、音频和图数据集的加载器，通过单一配置接口指定数据集、模型和评估协议，支持CPU上的轻量级方法和GPU上的深度方法。

Result: ModSSC 1.0.0已发布，采用MIT许可证，包含详细文档和测试，可在GitHub上获取，为半监督分类研究提供了统一的实验平台。

Conclusion: ModSSC框架解决了半监督分类软件碎片化问题，通过模块化代码库统一了不同方法和模态，促进了可复现研究和大型比较研究。

Abstract: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.

</details>


### [327] [CORE: Contrastive Masked Feature Reconstruction on Graphs](https://arxiv.org/abs/2512.13235)
*Jianyuan Bo,Yuan Fang*

Main category: cs.LG

TL;DR: 本文提出了一种名为CORE的新型图自监督学习框架，将对比学习融入掩码特征重建中，在节点和图分类任务上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 在快速发展的图自监督学习领域，生成式和对比式方法是两种主要方法。研究发现掩码特征重建（MFR）和图对比学习（GCL）都旨在最大化相似元素之间的一致性，在特定条件下它们的优化目标会收敛。这表明这两种方法是互补的而非根本不同的，因此探索它们的整合可以增强图自监督学习。

Method: 提出了对比掩码特征重建（CORE）框架，将对比学习集成到MFR中。具体来说：1）仅使用掩码节点的原始特征和重建特征形成正样本对，鼓励编码器优先考虑上下文信息而非节点自身特征；2）利用掩码节点本身作为负样本，结合MFR的重建能力和GCL的判别能力，更好地捕捉内在图结构。

Result: CORE在节点和图分类任务上显著优于MFR，取得了最先进的结果。具体来说，在节点分类任务上，CORE比GraphMAE和GraphMAE2分别高出2.80%和3.72%；在图分类任务上，分别高出3.82%和3.76%。

Conclusion: 研究揭示了MFR和GCL之间的理论联系，表明它们是互补的方法。提出的CORE框架成功整合了这两种方法的优势，在多个图学习任务上取得了优异的性能，为图自监督学习提供了新的方向。

Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.

</details>


### [328] [BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation](https://arxiv.org/abs/2512.13255)
*Yunhong Min,Juil Koo,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.LG

TL;DR: BézierFlow：一种轻量级训练方法，用于预训练扩散和流模型的少步生成，通过贝塞尔函数参数化调度器，在≤10步内实现2-3倍性能提升，仅需15分钟训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级训练方法主要学习最优时间步长，但其范围仅限于ODE离散化。为了扩大搜索空间，需要学习采样轨迹的最优变换，通过参数化随机插值调度器来实现。

Method: 提出将调度器函数表示为贝塞尔函数，控制点自然满足边界条件、可微性和信噪比单调性等关键要求。将问题简化为学习时间范围内的有序点集，从ODE时间步解释转变为贝塞尔控制点。

Result: 在多种预训练扩散和流模型上，BézierFlow始终优于先前的时间步学习方法，在≤10步采样时实现2-3倍性能提升，仅需15分钟训练时间，证明了从离散时间步扩展到基于贝塞尔的轨迹变换搜索空间的有效性。

Conclusion: 通过将调度器参数化为贝塞尔函数，BézierFlow成功扩展了轻量级训练方法的搜索空间，从离散时间步学习转向连续轨迹变换，显著提高了少步生成的性能。

Abstract: We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.

</details>


### [329] [ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning](https://arxiv.org/abs/2512.13316)
*Mayank Gulati,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: ALIGN-FL提出了一种新颖的分布式学习方法，通过选择性共享生成组件来处理高度不相关的数据分布，使用隐私保护机制在客户端间传输生成能力而非完整模型参数。


<details>
  <summary>Details</summary>
Motivation: 解决在高度不相关的数据分布（Non-IID）下进行分布式学习的挑战，特别是在跨域协作场景中，需要保护隐私同时保持模型效用的需求。

Method: 采用选择性共享生成组件的方法，客户端只传输生成能力而非完整模型参数；服务器使用合成样本进行全局训练；结合DP-SGD自适应裁剪和Lipschitz正则化VAE解码器两种隐私机制；支持异构客户端的架构设计。

Result: 在MNIST和Fashion-MNIST数据集上验证了方法的有效性，两种隐私机制都能将敏感异常值映射到典型数据点，同时在极端Non-IID场景中保持模型效用。

Conclusion: ALIGN-FL框架通过隐私保护的生成模型共享机制，成功解决了跨域协作中的Non-IID数据分布挑战，在保护隐私的同时保持了学习效果。

Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures

</details>


### [330] [FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs](https://arxiv.org/abs/2512.13337)
*Si Qi Goh,Yongsen Zheng,Ziyao Liu,Sami Hormi,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: FROC是一个用于大语言模型机器遗忘的统一框架，通过风险优化控制来平衡遗忘充分性和效用保留，提供可解释的风险评估和配置选择。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘技术缺乏评估和控制风险的有效机制，难以在安全性和效用之间取得适当平衡，这阻碍了"被遗忘权"的实现并引发信任问题。

Method: FROC采用符合性风格的风险控制框架，引入连续风险模型聚合遗忘不足和效用退化，计算符合性遗忘风险(CUR)和风险控制配置集。

Result: 实验表明FROC能产生稳定、可解释的风险景观，揭示遗忘配置、语义偏移和效用影响之间的一致关系，为大规模LLM部署提供实用基础。

Conclusion: FROC将机器遗忘重新定义为可控、风险感知的过程，为大语言模型部署中的遗忘行为管理提供了实用框架。

Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

</details>


### [331] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

TL;DR: 研究将多种成员推理攻击技术集成到LLM训练数据提取流程中，系统评估它们在真实数据提取场景中的有效性，并与传统基准测试结果对比。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易记忆训练数据，带来严重的隐私风险，特别是训练数据提取和成员推理攻击。现有研究表明这两种威胁相互关联，但需要系统评估不同MIA技术在真实数据提取场景中的实际效果。

Method: 将多种成员推理攻击技术集成到数据提取流程中，通过查询模型生成大量文本来提取训练数据，然后应用MIA验证特定数据点是否在训练集中。系统性地对多种MIA技术进行基准测试。

Result: 比较了集成设置下MIA技术的性能与传统MIA基准测试结果，评估了它们在真实世界提取场景中的实际效用。

Conclusion: 通过系统集成和基准测试，为评估MIA技术在真实LLM训练数据提取攻击中的有效性提供了实用框架，有助于理解不同攻击方法在实际隐私威胁中的表现。

Abstract: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [332] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

TL;DR: DPUL是一种新颖的服务器端联邦遗忘方法，通过深度遗忘所有影响权重来防止隐私泄露，相比现有方法在准确性和时间成本方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在高计算需求、复杂激励机制和客户端计算能力差异等问题，导致时间长、成本高。同时，现有服务器端知识蒸馏方法仅移除目标客户端的更新，忽略了其他客户端贡献中嵌入的隐私，可能导致隐私泄露。

Method: DPUL包含三个组件：(1)通过过滤客户端更新幅度识别高权重参数，并将其回滚以确保深度移除；(2)利用变分自编码器(VAE)重构和消除低权重参数；(3)使用基于投影的技术恢复模型。

Result: 在四个数据集上的实验结果表明，DPUL超越了最先进的基线方法，准确率提高了1%-5%，时间成本最多减少了12倍。

Conclusion: DPUL提供了一种有效的服务器端联邦遗忘解决方案，能够深度遗忘所有影响权重，防止隐私泄露，同时在准确性和效率方面均有显著优势。

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


### [333] [On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing](https://arxiv.org/abs/2512.13497)
*Haoyu Ren,Kay Koehle,Kirill Dorofeev,Darko Anicic*

Main category: cs.LG

TL;DR: 该论文提出了一种用于动态制造环境的设备端持续学习视觉异常检测方法，通过轻量级特征提取器和增量核心集更新机制，实现了快速、内存高效的模型适应。


<details>
  <summary>Details</summary>
Motivation: 现代制造业中动态灵活的生产环境带来三大挑战：频繁产品变更需要快速模型更新；边缘硬件资源有限无法训练大型AI模型；异常和正常训练数据通常稀缺，特别是新产品变体。

Method: 基于PatchCore扩展，采用轻量级特征提取器和基于k-center选择的增量核心集更新机制，实现设备端持续学习，无需昂贵的云端重新训练。

Result: 在模拟灵活生产的工业用例测试中，相比基线方法实现了12%的AUROC提升，内存使用减少80%，训练速度比批量重新训练更快。

Conclusion: 该方法为动态智能制造提供了准确、资源高效且自适应的视觉异常检测解决方案，适合小批量按需制造环境。

Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.

</details>


### [334] [Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource](https://arxiv.org/abs/2512.13506)
*Sofiya Zaichyk*

Main category: cs.LG

TL;DR: 论文提出"可重复性预算"$C_T$作为衡量学习系统在分布漂移下统计可重复性的新统计原语，推导出最优的漂移-反馈泛化界，建立了可重复性速度极限。


<details>
  <summary>Details</summary>
Motivation: 在分布漂移下，每个观测都会改变数据生成规律，经典泛化界可能失效。需要量化系统在外部变化和内部反馈下的统计可重复性有限容量。

Method: 引入可重复性预算$C_T$作为Fisher-Rao路径长度的累积量，衡量学习-环境耦合演化中的总分布运动。从该构造推导出$O(T^{-1/2} + C_T/T)$阶的漂移-反馈泛化界，并证明匹配的极小极大下界。

Result: 建立了可重复性速度极限：任何算法的最坏情况泛化误差都不能低于数据生成过程的平均Fisher-Rao漂移率$C_T/T$所施加的限制。该框架将外部漂移、自适应数据分析和执行预测统一在共同的几何结构中。

Conclusion: 可重复性预算$C_T$成为衡量分布运动的内在量，为分布漂移下的统计学习提供了理论基础和最优性能界限。

Abstract: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.

</details>


### [335] [DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication](https://arxiv.org/abs/2512.13583)
*Zehan Zhu,Heng Zhao,Yan Huang,Joey Tianyi Zhou,Shouling Ji,Jinming Xu*

Main category: cs.LG

TL;DR: 提出DP-CSGP算法，在定向图上的去中心化学习中实现差分隐私保护、压缩通信和高模型效用


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习算法在同时保证差分隐私、高效通信和高模型效用方面存在不足，特别是在定向图通信场景下

Method: 提出差分隐私随机梯度推送压缩通信算法(DP-CSGP)，结合差分隐私保护、压缩通信技术和定向图上的去中心化学习

Result: 算法达到紧致的效用边界O(√(d log(1/δ))/(√nJε))，在相同隐私预算下，通信成本显著低于现有精确通信方法，同时保持可比模型精度

Conclusion: DP-CSGP算法成功平衡了差分隐私保护、通信效率和模型效用，为定向图上的隐私保护去中心化学习提供了有效解决方案

Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}δ \right)}/(\sqrt{n}Jε) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(ε, δ\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.

</details>


### [336] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

TL;DR: 提出Diffusion Preview范式，通过快速低步数采样生成预览供用户评估，满意后再进行全步数精炼。为解决现有方法预览质量差和一致性不足的问题，提出了基于强化学习优化的轻量级高阶求解器ConsistencySolver。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型的缓慢推理过程严重影响了交互式用户体验。现有加速方法（包括免训练求解器和训练后蒸馏）难以提供高质量的预览，也无法确保预览与最终输出之间的一致性。

Method: 提出ConsistencySolver，一种基于通用线性多步方法的轻量级可训练高阶求解器，通过强化学习进行优化，旨在提升预览质量和一致性。

Result: 实验结果表明，ConsistencySolver在低步数场景下显著提升了生成质量和一致性。与Multistep DPM-Solver相比，使用47%更少的步骤达到相当的FID分数，同时优于蒸馏基线方法。用户研究表明，该方法将用户交互时间减少了近50%，同时保持了生成质量。

Conclusion: ConsistencySolver通过提升预览质量和一致性，为高效的预览-精炼工作流程提供了理想解决方案，显著改善了扩散模型的交互体验。

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


### [337] [StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion](https://arxiv.org/abs/2512.13632)
*Guransh Singh,Md Shah Fahad*

Main category: cs.LG

TL;DR: 提出StutterFuse，首个基于检索增强分类器的多标签口吃检测方法，通过检索临床案例库解决重叠性口吃检测难题，并解决了模态崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型难以检测重叠性口吃（如"阻塞"与"延长"同时发生），因为训练数据中这些特定组合稀缺。检索增强生成在NLP领域取得突破，但在病理语音处理中尚未探索。

Method: 提出StutterFuse检索增强分类器：1）基于临床案例的非参数记忆库为Conformer编码器提供参考；2）使用SetCon（Jaccard加权度量学习）优化多标签集相似性；3）采用门控专家混合融合策略动态平衡声学证据与检索上下文。

Result: 在SEP-28k数据集上获得0.65加权F1分数，优于强基线模型，并展现出显著的零样本跨语言泛化能力。

Conclusion: StutterFuse通过检索增强范式成功解决了重叠性口吃检测难题，为病理语音处理开辟了新方向，同时提出的模态崩溃解决方案具有广泛适用性。

Abstract: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.

</details>


### [338] [From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves](https://arxiv.org/abs/2512.13641)
*Gabriel Vitorino de Andrade,Saulo Roberto dos Santos,Itallo Patrick Castro Alves da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza*

Main category: cs.LG

TL;DR: 该研究提出了一种评估芒果叶病害诊断CNN模型鲁棒性的方法，通过创建包含19种人工损坏的MangoLeafDB-C数据集，比较了5种架构在恶劣条件下的表现，发现轻量级专用模型LCNN在现实场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 尽管芒果具有全球重要性，但缺乏对芒果叶病害诊断模型鲁棒性的研究。需要评估AI模型在现实世界挑战（如图像损坏）下的可靠性能，特别是在农业智能系统开发中。

Method: 1. 改编MangoLeafDB数据集，生成包含19种人工损坏类型、5个严重程度的MangoLeafDB-C数据集；2. 对5种CNN架构进行基准测试：ResNet-50、ResNet-101、VGG-16、Xception和专门设计的轻量级架构LCNN；3. 使用F1分数、损坏错误率（CE）和相对平均损坏错误率（相对mCE）作为评估指标。

Result: 1. LCNN在可能出现在现实场景的损坏类型（如散焦模糊、运动模糊）中表现优于复杂模型；2. LCNN获得了最低的平均损坏错误率（mCE）；3. 现代架构（如ResNet-101）在理想条件下准确率高，但在损坏场景中性能显著下降；4. 轻量级专用模型在边缘设备应用中更具优势。

Conclusion: 轻量级和专用模型可能更适合边缘设备的现实世界应用，其中鲁棒性和效率至关重要。研究强调了在农业智能系统开发中，特别是在技术受限地区，需要纳入鲁棒性评估。

Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.

</details>


### [339] [A Scientific Reasoning Model for Organic Synthesis Procedure Generation](https://arxiv.org/abs/2512.13668)
*Guoqing Liu,Junren Li,Zihan Zhao,Eray Inanc,Krzysztof Maziarz,Jose Garrido Torres,Victor Garcia Satorras,Shoko Ueda,Christopher M. Bishop,Marwin Segler*

Main category: cs.LG

TL;DR: QFANG是一个科学推理语言模型，能够从化学反应方程式直接生成精确的结构化实验程序，通过化学引导推理框架和强化学习提高程序准确性。


<details>
  <summary>Details</summary>
Motivation: 解决计算机辅助合成规划的关键挑战是弥合计算路线设计与实际实验室执行之间的差距，特别是准确预测每个合成步骤的可行实验程序。

Method: 1) 构建包含905,990个化学反应与结构化动作序列的高质量数据集；2) 引入化学引导推理框架生成基于化学知识的链式思维数据；3) 通过监督微调激发复杂化学推理；4) 应用可验证奖励的强化学习进一步提高程序准确性。

Result: QFANG在传统NLP相似性指标和化学感知评估器上均优于先进的通用推理模型和最近邻检索基线，能够泛化到某些域外反应类别，并适应实验室条件和用户特定约束的变化。

Conclusion: QFANG生成高质量合成程序的能力代表了向弥合计算合成规划与全自动化实验室合成之间差距的重要一步。

Abstract: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.

</details>


### [340] [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)
*Kunhee Kim,NaHyeon Park,Kibeom Hong,Hyunjung Shim*

Main category: cs.LG

TL;DR: 本文提出方向性文本反转（DTI），通过固定嵌入向量的模长并仅在单位超球面上优化方向，解决了传统文本反转中嵌入向量模长膨胀导致复杂提示失败的问题。


<details>
  <summary>Details</summary>
Motivation: 传统文本反转（TI）方法在处理复杂提示时经常失败，作者发现这是由于嵌入向量的模长膨胀导致的。这种模长膨胀会使学习到的token偏离分布范围，从而在预归一化Transformer中降低提示条件的效果。

Method: 提出方向性文本反转（DTI），固定嵌入向量的模长为分布内尺度，仅通过黎曼SGD在单位超球面上优化方向。将方向学习建模为具有von Mises-Fisher先验的最大后验概率估计，产生简单高效的恒定方向先验梯度。

Result: 在个性化任务中，DTI在保持主体相似性的同时，比TI及其变体显著提高了文本保真度。DTI的超球面参数化还实现了学习概念之间的平滑、语义连贯插值（球面线性插值），这是标准TI所不具备的能力。

Conclusion: 仅优化方向是实现提示忠实个性化的鲁棒且可扩展的路径。DTI通过解决嵌入向量模长膨胀问题，显著提升了文本到图像个性化的效果。

Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

</details>
