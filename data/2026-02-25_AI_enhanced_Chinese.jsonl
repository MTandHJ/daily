{"id": "2602.20191", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20191", "abs": "https://arxiv.org/abs/2602.20191", "authors": ["Dongwei Wang", "Jinhee Kim", "Seokho Han", "Denis Gudovskiy", "Yohei Nakata", "Tomoyuki Okuno", "KhayTze Peong", "Kang Eun Jeon", "Jong Hwan Ko", "Yiran Chen", "Huanrui Yang"], "title": "MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs", "comment": "17 pages, 12 figures", "summary": "Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \\texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration.", "AI": {"tldr": "MoBiQuant\u662f\u4e00\u4e2a\u6df7\u5408\u6bd4\u7279\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8etoken\u654f\u611f\u6027\u7684\u6743\u91cd\u7cbe\u5ea6\u8c03\u6574\uff0c\u5b9e\u73b0\u5f39\u6027LLM\u63a8\u7406\uff0c\u65e0\u9700\u91cd\u590d\u6821\u51c6\u5373\u53ef\u5728\u4e0d\u540c\u7cbe\u5ea6\u95f4\u5e73\u6ed1\u5207\u6362\u3002", "motivation": "\u4e91\u548c\u8fb9\u7f18\u8bbe\u5907\u8fd0\u884c\u65f6\u590d\u6742\u6027\u53d8\u5316\u9700\u8981\u5f39\u6027LLM\u90e8\u7f72\uff0c\u4f46\u4f20\u7edf\u91cf\u5316\u6821\u51c6\u53c2\u6570\u4e0e\u7279\u5b9a\u7cbe\u5ea6\u7ed1\u5b9a\uff0c\u5bfc\u81f4\u5f39\u6027\u7cbe\u5ea6\u6821\u51c6\u548c\u8fd0\u884c\u65f6\u7cbe\u5ea6\u5207\u6362\u56f0\u96be\u3002", "method": "\u63d0\u51faMoBiQuant\u6846\u67b6\uff1a1) \u591a\u5408\u4e00\u9012\u5f52\u6b8b\u5dee\u91cf\u5316\uff0c\u53ef\u8fed\u4ee3\u91cd\u5efa\u66f4\u9ad8\u7cbe\u5ea6\u6743\u91cd\uff1b2) token\u611f\u77e5\u8def\u7531\u5668\uff0c\u52a8\u6001\u9009\u62e9\u6b8b\u5dee\u6bd4\u7279\u7247\u6570\u91cf\u3002\u901a\u8fc7token\u7ea7\u654f\u611f\u6027\u8c03\u6574\u6743\u91cd\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMoBiQuant\u8868\u73b0\u51fa\u5f3a\u5f39\u6027\uff0c\u5728LLaMA3-8B\u4e0a\u65e0\u9700\u91cd\u590d\u6821\u51c6\u5373\u53ef\u5339\u914d\u6bd4\u7279\u7279\u5b9a\u6821\u51c6PTQ\u7684\u6027\u80fd\u3002", "conclusion": "MoBiQuant\u901a\u8fc7\u57fa\u4e8etoken\u654f\u611f\u6027\u7684\u6df7\u5408\u6bd4\u7279\u91cf\u5316\uff0c\u89e3\u51b3\u4e86\u5f39\u6027LLM\u90e8\u7f72\u4e2d\u7684\u7cbe\u5ea6\u5207\u6362\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5bf9token\u5f02\u5e38\u503c\u5206\u5e03\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.20194", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.20194", "abs": "https://arxiv.org/abs/2602.20194", "authors": ["Takato Yasuno"], "title": "FedAvg-Based CTMC Hazard Model for Federated Bridge Deterioration Assessment", "comment": "10 pages, 4 figures, 2 tables", "summary": "Bridge periodic inspection records contain sensitive information about public infrastructure, making cross-organizational data sharing impractical under existing data governance constraints. We propose a federated framework for estimating a Continuous-Time Markov Chain (CTMC) hazard model of bridge deterioration, enabling municipalities to collaboratively train a shared benchmark model without transferring raw inspection records. Each User holds local inspection data and trains a log-linear hazard model over three deterioration-direction transitions -- Good$\\to$Minor, Good$\\to$Severe, and Minor$\\to$Severe -- with covariates for bridge age, coastline distance, and deck area. Local optimization is performed via mini-batch stochastic gradient descent on the CTMC log-likelihood, and only a 12-dimensional pseudo-gradient vector is uploaded to a central server per communication round. The server aggregates User updates using sample-weighted Federated Averaging (FedAvg) with momentum and gradient clipping. All experiments in this paper are conducted on fully synthetic data generated from a known ground-truth parameter set with region-specific heterogeneity, enabling controlled evaluation of federated convergence behaviour. Simulation results across heterogeneous Users show consistent convergence of the average negative log-likelihood, with the aggregated gradient norm decreasing as User scale increases. Furthermore, the federated update mechanism provides a natural participation incentive: Users who register their local inspection datasets on a shared technical-standard platform receive in return the periodically updated global benchmark parameters -- information that cannot be obtained from local data alone -- thereby enabling evidence-based life-cycle planning without surrendering data sovereignty.", "AI": {"tldr": "\u63d0\u51fa\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6865\u6881\u9000\u5316CTMC\u98ce\u9669\u6a21\u578b\u4f30\u8ba1\uff0c\u5b9e\u73b0\u8de8\u7ec4\u7ec7\u6570\u636e\u5171\u4eab\u800c\u4e0d\u4f20\u8f93\u539f\u59cb\u68c0\u67e5\u8bb0\u5f55", "motivation": "\u6865\u6881\u5b9a\u671f\u68c0\u67e5\u8bb0\u5f55\u5305\u542b\u654f\u611f\u516c\u5171\u57fa\u7840\u8bbe\u65bd\u4fe1\u606f\uff0c\u73b0\u6709\u6570\u636e\u6cbb\u7406\u7ea6\u675f\u4e0b\u8de8\u7ec4\u7ec7\u6570\u636e\u5171\u4eab\u4e0d\u5207\u5b9e\u9645\uff0c\u9700\u8981\u4fdd\u62a4\u6570\u636e\u4e3b\u6743\u7684\u540c\u65f6\u5b9e\u73b0\u534f\u4f5c\u5efa\u6a21", "method": "\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff1a\u5404\u7528\u6237\u672c\u5730\u8bad\u7ec3\u5bf9\u6570\u7ebf\u6027\u98ce\u9669\u6a21\u578b\uff08\u4e09\u4e2a\u9000\u5316\u65b9\u5411\u8f6c\u79fb\uff09\uff0c\u901a\u8fc7\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4f18\u5316CTMC\u5bf9\u6570\u4f3c\u7136\uff0c\u4ec5\u4e0a\u4f2012\u7ef4\u4f2a\u68af\u5ea6\u5411\u91cf\uff1b\u670d\u52a1\u5668\u4f7f\u7528\u5e26\u52a8\u91cf\u548c\u68af\u5ea6\u88c1\u526a\u7684\u6837\u672c\u52a0\u6743FedAvg\u805a\u5408\u66f4\u65b0", "result": "\u5728\u5b8c\u5168\u5408\u6210\u6570\u636e\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u5f02\u6784\u7528\u6237\u95f4\u5e73\u5747\u8d1f\u5bf9\u6570\u4f3c\u7136\u4e00\u81f4\u6536\u655b\uff0c\u805a\u5408\u68af\u5ea6\u8303\u6570\u968f\u7528\u6237\u89c4\u6a21\u589e\u52a0\u800c\u51cf\u5c0f\uff1b\u8054\u90a6\u66f4\u65b0\u673a\u5236\u63d0\u4f9b\u81ea\u7136\u53c2\u4e0e\u6fc0\u52b1", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u5e02\u653f\u90e8\u95e8\u80fd\u591f\u534f\u4f5c\u8bad\u7ec3\u5171\u4eab\u57fa\u51c6\u6a21\u578b\u800c\u4e0d\u4f20\u8f93\u539f\u59cb\u68c0\u67e5\u8bb0\u5f55\uff0c\u7528\u6237\u5728\u5171\u4eab\u6280\u672f\u6807\u51c6\u5e73\u53f0\u6ce8\u518c\u672c\u5730\u6570\u636e\u96c6\u53ef\u83b7\u5f97\u5b9a\u671f\u66f4\u65b0\u7684\u5168\u5c40\u57fa\u51c6\u53c2\u6570\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8bc1\u636e\u7684\u751f\u547d\u5468\u671f\u89c4\u5212\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u4e3b\u6743"}}
{"id": "2602.20197", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20197", "abs": "https://arxiv.org/abs/2602.20197", "authors": ["Zhuoxu Huang", "Mengxi Jia", "Hao Sun", "Xuelong Li", "Jungong Han"], "title": "Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning", "comment": "Published as a conference paper at ICLR 2026", "summary": "Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL.", "AI": {"tldr": "CalibRL\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u7684RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u4f18\u52bf\u52a0\u6743\u548cLeakyReLU\u6fc0\u6d3b\u51fd\u6570\u5b9e\u73b0\u53ef\u63a7\u63a2\u7d22\uff0c\u89e3\u51b3MLLM\u8bad\u7ec3\u4e2d\u7684\u71b5\u5d29\u6e83\u3001\u7b56\u7565\u9000\u5316\u7b49\u95ee\u9898\u3002", "motivation": "\u5728MLLM\u7684RLVR\u8bad\u7ec3\u4e2d\uff0c\u5de8\u5927\u7684\u72b6\u6001\u7a7a\u95f4\u548c\u7a00\u758f\u5956\u52b1\u4f1a\u5bfc\u81f4\u71b5\u5d29\u6e83\u3001\u7b56\u7565\u9000\u5316\u6216\u5bf9\u6b21\u4f18\u884c\u4e3a\u7684\u8fc7\u5ea6\u5229\u7528\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u751f\u4ea7\u6027\u968f\u673a\u6027\u53c8\u907f\u514d\u65e0\u63a7\u5236\u968f\u673a\u91c7\u6837\u7f3a\u70b9\u7684\u63a2\u7d22\u7b56\u7565\u3002", "method": "\u63d0\u51faCalibRL\u6df7\u5408\u7b56\u7565RLVR\u6846\u67b6\uff1a1\uff09\u5206\u5e03\u611f\u77e5\u4f18\u52bf\u52a0\u6743\uff0c\u901a\u8fc7\u7ec4\u7a00\u6709\u6027\u7f29\u653e\u66f4\u65b0\u6765\u6821\u51c6\u5206\u5e03\uff1b2\uff09\u975e\u5bf9\u79f0\u6fc0\u6d3b\u51fd\u6570\uff08LeakyReLU\uff09\uff0c\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u4f5c\u4e3a\u6821\u51c6\u57fa\u7ebf\u6765\u8c03\u8282\u8fc7\u5ea6\u81ea\u4fe1\u7684\u66f4\u65b0\u3002\u901a\u8fc7\u5728\u7ebf\u91c7\u6837\u4f30\u8ba1\u5728\u7b56\u7565\u5206\u5e03\uff0c\u4ee5\u4fe1\u606f\u6027\u884c\u4e3a\u9a71\u52a8\u66f4\u65b0\u3002", "result": "\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u57df\u5185\u548c\u57df\u5916\u8bbe\u7f6e\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u663e\u793a\u4e86\u4e00\u81f4\u7684\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u53ef\u63a7\u6df7\u5408\u7b56\u7565RLVR\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "conclusion": "CalibRL\u901a\u8fc7\u53ef\u63a7\u63a2\u7d22\u7f13\u89e3\u4e86\u6a21\u578b\u7b56\u7565\u4e0e\u4e13\u5bb6\u8f68\u8ff9\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5b9e\u73b0\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u66f4\u7a33\u5b9a\u7684\u5e73\u8861\uff0c\u63d0\u9ad8\u4e86RLVR\u8bad\u7ec3\u7684\u6548\u679c\u3002"}}
{"id": "2602.20507", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.20507", "abs": "https://arxiv.org/abs/2602.20507", "authors": ["William Anthony Mason"], "title": "Indaleko: The Unified Personal Index", "comment": "PhD dissertation, University of British Columbia, August 2025. 287 pages", "summary": "Personal information retrieval fails when systems ignore how human memory works. While existing platforms force keyword searches across isolated silos, humans naturally recall through episodic cues like when, where, and in what context information was encountered. This dissertation presents the Unified Personal Index (UPI), a memory-aligned architecture that bridges this fundamental gap. The Indaleko prototype demonstrates the UPI's feasibility on a 31-million file dataset spanning 160TB across eight storage platforms. By integrating temporal, spatial, and activity metadata into a unified graph database, Indaleko enables natural language queries like \"photos near the conference venue last spring\" that existing systems cannot process. The implementation achieves sub-second query responses through memory anchor indexing, eliminates cross-platform search fragmentation, and maintains perfect precision for well-specified memory patterns. Evaluation against commercial systems (Google Drive, OneDrive, Dropbox, Windows Search) reveals that all fail on memory-based queries, returning overwhelming result sets without contextual filtering. In contrast, Indaleko successfully processes multi-dimensional queries combining time, location, and activity patterns. The extensible architecture supports rapid integration of new data sources (10 minutes to 10 hours per provider) while preserving privacy through UUID-based semantic decoupling. The UPI's architectural synthesis bridges cognitive theory with distributed systems design, as demonstrated through the Indaleko prototype and rigorous evaluation. This work transforms personal information retrieval from keyword matching to memory-aligned finding, providing immediate benefits for existing data while establishing foundations for future context-aware systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7edf\u4e00\u4e2a\u4eba\u7d22\u5f15\uff08UPI\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8bb0\u5fc6\u7684\u65f6\u7a7a\u60c5\u5883\u7ebf\u7d22\u6765\u6539\u8fdb\u4e2a\u4eba\u4fe1\u606f\u68c0\u7d22\uff0c\u5f00\u53d1\u4e86Indaleko\u539f\u578b\u7cfb\u7edf\uff0c\u572831\u767e\u4e07\u6587\u4ef6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u5546\u4e1a\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u5904\u7406\u57fa\u4e8e\u8bb0\u5fc6\u7684\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u4e2a\u4eba\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5ffd\u7565\u4e86\u4eba\u7c7b\u8bb0\u5fc6\u7684\u5de5\u4f5c\u65b9\u5f0f\uff0c\u5f3a\u5236\u4f7f\u7528\u5173\u952e\u8bcd\u641c\u7d22\u5b64\u7acb\u7684\u6570\u636e\u5b64\u5c9b\uff0c\u800c\u4eba\u7c7b\u81ea\u7136\u901a\u8fc7\u60c5\u5883\u7ebf\u7d22\uff08\u65f6\u95f4\u3001\u5730\u70b9\u3001\u4e0a\u4e0b\u6587\uff09\u56de\u5fc6\u4fe1\u606f\uff0c\u5bfc\u81f4\u68c0\u7d22\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u4e2a\u4eba\u7d22\u5f15\uff08UPI\uff09\u67b6\u6784\uff0c\u5c06\u65f6\u95f4\u3001\u7a7a\u95f4\u548c\u6d3b\u52a8\u5143\u6570\u636e\u6574\u5408\u5230\u7edf\u4e00\u56fe\u6570\u636e\u5e93\u4e2d\uff0c\u5f00\u53d1Indaleko\u539f\u578b\u7cfb\u7edf\uff0c\u91c7\u7528\u5185\u5b58\u951a\u7d22\u5f15\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u67e5\u8be2\u54cd\u5e94\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u548c\u8de8\u5e73\u53f0\u641c\u7d22\u3002", "result": "Indaleko\u539f\u578b\u572831\u767e\u4e07\u6587\u4ef6\u3001160TB\u8de88\u4e2a\u5b58\u50a8\u5e73\u53f0\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u53ef\u884c\u6027\uff0c\u76f8\u6bd4Google Drive\u3001OneDrive\u7b49\u5546\u4e1a\u7cfb\u7edf\u80fd\u6210\u529f\u5904\u7406\u57fa\u4e8e\u8bb0\u5fc6\u7684\u591a\u7ef4\u67e5\u8be2\uff0c\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u54cd\u5e94\u548c\u5b8c\u7f8e\u7cbe\u5ea6\uff0c\u65b0\u6570\u636e\u6e90\u96c6\u6210\u4ec5\u970010\u5206\u949f\u523010\u5c0f\u65f6\u3002", "conclusion": "UPI\u67b6\u6784\u5c06\u8ba4\u77e5\u7406\u8bba\u4e0e\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u5c06\u4e2a\u4eba\u4fe1\u606f\u68c0\u7d22\u4ece\u5173\u952e\u8bcd\u5339\u914d\u8f6c\u53d8\u4e3a\u57fa\u4e8e\u8bb0\u5fc6\u7684\u67e5\u627e\uff0c\u4e3a\u73b0\u6709\u6570\u636e\u63d0\u4f9b\u5373\u65f6\u6548\u76ca\uff0c\u5e76\u4e3a\u672a\u6765\u60c5\u5883\u611f\u77e5\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.20303", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20303", "abs": "https://arxiv.org/abs/2602.20303", "authors": ["Joyanta Jyoti Mondal"], "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health", "comment": null, "summary": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7f8e\u56fd\u9752\u5c11\u5e74\u8d85\u91cd/\u80a5\u80d6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u903b\u8f91\u56de\u5f52\u4e0e\u66f4\u590d\u6742\u6a21\u578b\u76f8\u6bd4\u8868\u73b0\u76f8\u5f53\uff0c\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u79cd\u65cf\u548c\u8d2b\u56f0\u7fa4\u4f53\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u8d85\u91cd\u80a5\u80d6\u662f\u7f8e\u56fd\u4e3b\u8981\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u53d7\u884c\u4e3a\u3001\u5bb6\u5ead\u548c\u793e\u533a\u56e0\u7d20\u5171\u540c\u5f71\u54cd\u3002\u76ee\u524d\u5bf9\u8fd9\u4e9b\u591a\u5c42\u6b21\u9884\u6d4b\u56e0\u7d20\u5728\u4eba\u7fa4\u5c42\u9762\u7684\u8054\u5408\u9884\u6d4b\u7ed3\u6784\u4e86\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u75282021\u5e74\u5168\u7f8e\u513f\u7ae5\u5065\u5eb7\u8c03\u67e5\u4e2d18,792\u540d10-17\u5c81\u513f\u7ae5\u7684\u6570\u636e\u3002\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u996e\u98df\u3001\u4f53\u80b2\u6d3b\u52a8\u3001\u7761\u7720\u3001\u7236\u6bcd\u538b\u529b\u3001\u793e\u4f1a\u7ecf\u6d4e\u6761\u4ef6\u3001\u4e0d\u826f\u7ecf\u5386\u548c\u793e\u533a\u7279\u5f81\u3002\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001\u68af\u5ea6\u63d0\u5347\u3001XGBoost\u3001LightGBM\u3001\u591a\u5c42\u611f\u77e5\u673a\u548cTabNet\u7b49\u6a21\u578b\uff0c\u4f7f\u7528AUC\u3001\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cBrier\u5206\u6570\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u533a\u5206\u5ea6\u57280.66-0.79\u4e4b\u95f4\u3002\u903b\u8f91\u56de\u5f52\u3001\u68af\u5ea6\u63d0\u5347\u548c\u591a\u5c42\u611f\u77e5\u673a\u5728\u533a\u5206\u5ea6\u548c\u6821\u51c6\u65b9\u9762\u8868\u73b0\u6700\u7a33\u5b9a\u3002\u63d0\u5347\u7b97\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u6709\u8f7b\u5fae\u6539\u5584\uff0c\u4f46\u6ca1\u6709\u6a21\u578b\u5728\u6240\u6709\u65b9\u9762\u90fd\u8868\u73b0\u6700\u4f18\u3002\u4e0d\u540c\u79cd\u65cf\u548c\u8d2b\u56f0\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u5728\u6240\u6709\u7b97\u6cd5\u4e2d\u90fd\u6301\u7eed\u5b58\u5728\u3002", "conclusion": "\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u63d0\u5347\u6709\u9650\uff0c\u903b\u8f91\u56de\u5f52\u8868\u73b0\u4e0e\u590d\u6742\u6a21\u578b\u76f8\u5f53\u3002\u9884\u6d4b\u56e0\u7d20\u6301\u7eed\u6db5\u76d6\u884c\u4e3a\u3001\u5bb6\u5ead\u548c\u793e\u533a\u9886\u57df\u3002\u6301\u7eed\u7684\u4e9a\u7ec4\u5dee\u5f02\u8868\u660e\u9700\u8981\u6539\u8fdb\u6570\u636e\u8d28\u91cf\u548c\u5173\u6ce8\u516c\u5e73\u6027\u7684\u76d1\u6d4b\uff0c\u800c\u4e0d\u662f\u8ffd\u6c42\u66f4\u9ad8\u7684\u7b97\u6cd5\u590d\u6742\u5ea6\u3002"}}
{"id": "2602.20167", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.20167", "abs": "https://arxiv.org/abs/2602.20167", "authors": ["Elliott Wen", "Paul Denny", "Andrew Luxton-Reilly", "Sean Ma", "Bruce Sham", "Chenye Ni", "Jun Seo", "Yu Yang"], "title": "Playsemble: Learning Low-Level Programming Through Interactive Games", "comment": null, "summary": "Teaching assembly programming is a fundamental component of undergraduate computer science education, yet many students struggle with its abstract and low-level concepts. Existing learning tools, such as simulators and visualisers, support understanding by exposing machine states. However, they often limit students to passive observation and provide few opportunities for meaningful interaction. To address these limitations, we introduce Playsemble, a gamified learning system that transforms assembly instructions into interactive, game-like tasks in which students control Pac-Man to collect items, avoid ghosts, and reach targets. Playsemble integrates a code editor, a CPU emulator, and visual debugging tools within a browser-based environment, allowing students to work offline without installation or configuration. It also provides immediate formative feedback enhanced by large language models. We deployed Playsemble in an undergraduate computer architecture course with 107 students. The course featured a sequence of assignments of increasing complexity, covering core concepts such as register and memory manipulation, control structures including loops and conditionals, and arithmetic operations. Our findings suggest that Playsemble promotes active experimentation, sustained engagement, and deeper conceptual understanding through meaningful game-based learning experiences.", "AI": {"tldr": "Playsemble\u662f\u4e00\u4e2a\u6e38\u620f\u5316\u7684\u6c47\u7f16\u5b66\u4e60\u7cfb\u7edf\uff0c\u5c06\u6c47\u7f16\u6307\u4ee4\u8f6c\u5316\u4e3aPac-Man\u6e38\u620f\u4efb\u52a1\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u73af\u5883\u63d0\u4f9b\u4ee3\u7801\u7f16\u8f91\u3001CPU\u6a21\u62df\u548c\u53ef\u89c6\u5316\u8c03\u8bd5\u5de5\u5177\uff0c\u7ed3\u5408LLM\u5373\u65f6\u53cd\u9988\uff0c\u63d0\u5347\u5b66\u751f\u53c2\u4e0e\u5ea6\u548c\u6982\u5ff5\u7406\u89e3\u3002", "motivation": "\u6c47\u7f16\u7f16\u7a0b\u6559\u5b66\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u672c\u79d1\u6559\u80b2\u7684\u57fa\u7840\uff0c\u4f46\u8bb8\u591a\u5b66\u751f\u96be\u4ee5\u7406\u89e3\u5176\u62bd\u8c61\u548c\u5e95\u5c42\u6982\u5ff5\u3002\u73b0\u6709\u7684\u5b66\u4e60\u5de5\u5177\u5982\u6a21\u62df\u5668\u548c\u53ef\u89c6\u5316\u5de5\u5177\u4e3b\u8981\u652f\u6301\u88ab\u52a8\u89c2\u5bdf\uff0c\u7f3a\u4e4f\u6709\u610f\u4e49\u7684\u4e92\u52a8\u673a\u4f1a\u3002", "method": "\u5f00\u53d1Playsemble\u6e38\u620f\u5316\u5b66\u4e60\u7cfb\u7edf\uff0c\u5c06\u6c47\u7f16\u6307\u4ee4\u8f6c\u5316\u4e3aPac-Man\u6e38\u620f\u4efb\u52a1\uff08\u6536\u96c6\u7269\u54c1\u3001\u8eb2\u907f\u5e7d\u7075\u3001\u5230\u8fbe\u76ee\u6807\uff09\u3002\u7cfb\u7edf\u96c6\u6210\u4ee3\u7801\u7f16\u8f91\u5668\u3001CPU\u6a21\u62df\u5668\u548c\u53ef\u89c6\u5316\u8c03\u8bd5\u5de5\u5177\uff0c\u57fa\u4e8e\u6d4f\u89c8\u5668\u73af\u5883\u65e0\u9700\u5b89\u88c5\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5373\u65f6\u53cd\u9988\u3002", "result": "\u5728107\u540d\u672c\u79d1\u751f\u7684\u8ba1\u7b97\u673a\u67b6\u6784\u8bfe\u7a0b\u4e2d\u90e8\u7f72Playsemble\uff0c\u5305\u542b\u9010\u6b65\u590d\u6742\u7684\u4f5c\u4e1a\u5e8f\u5217\uff0c\u6db5\u76d6\u5bc4\u5b58\u5668\u5185\u5b58\u64cd\u4f5c\u3001\u63a7\u5236\u7ed3\u6784\uff08\u5faa\u73af\u548c\u6761\u4ef6\uff09\u3001\u7b97\u672f\u8fd0\u7b97\u7b49\u6838\u5fc3\u6982\u5ff5\u3002\u7ed3\u679c\u8868\u660e\u7cfb\u7edf\u4fc3\u8fdb\u4e86\u4e3b\u52a8\u5b9e\u9a8c\u3001\u6301\u7eed\u53c2\u4e0e\u548c\u66f4\u6df1\u5c42\u6b21\u7684\u6982\u5ff5\u7406\u89e3\u3002", "conclusion": "Playsemble\u901a\u8fc7\u6709\u610f\u4e49\u7684\u6e38\u620f\u5316\u5b66\u4e60\u4f53\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c47\u7f16\u7f16\u7a0b\u6559\u5b66\u4e2d\u7684\u4e92\u52a8\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b66\u751f\u7684\u5b66\u4e60\u6548\u679c\u548c\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2602.20199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20199", "abs": "https://arxiv.org/abs/2602.20199", "authors": ["Soufiane Bacha", "Laouni Djafri", "Sahraoui Dhelim", "Huansheng Ning"], "title": "IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning", "comment": "28 pages", "summary": "Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits.", "AI": {"tldr": "IMOVNO+\u662f\u4e00\u4e2a\u4e24\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u7ea7\u4f18\u5316\uff08\u6761\u4ef6\u6982\u7387\u91cf\u5316\u6837\u672c\u4fe1\u606f\u3001\u533a\u57df\u5212\u5206\u3001\u91cd\u53e0\u6e05\u7406\u3001\u667a\u80fd\u8fc7\u91c7\u6837\uff09\u548c\u7b97\u6cd5\u7ea7\u4f18\u5316\uff08\u5143\u542f\u53d1\u5f0f\u96c6\u6210\u526a\u679d\uff09\u8054\u5408\u5904\u7406\u591a\u7c7b\u548c\u4e8c\u7c7b\u4efb\u52a1\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u91cd\u53e0\u548c\u566a\u58f0\u95ee\u9898\uff0c\u572835\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u91cd\u53e0\u548c\u566a\u58f0\u4f1a\u964d\u4f4e\u6570\u636e\u8d28\u91cf\u3001\u51cf\u5c11\u6a21\u578b\u53ef\u9760\u6027\u5e76\u9650\u5236\u6cdb\u5316\u80fd\u529b\u3002\u867d\u7136\u5728\u4e8c\u5206\u7c7b\u4e2d\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u591a\u7c7b\u8bbe\u7f6e\u4e2d\u8fd9\u4e9b\u95ee\u9898\u4ecd\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5176\u4e2d\u590d\u6742\u7684\u7c7b\u95f4\u5173\u7cfb\u4f7f\u5f97\u5c11\u6570-\u591a\u6570\u7ed3\u6784\u4e0d\u660e\u786e\uff0c\u4f20\u7edf\u805a\u7c7b\u65e0\u6cd5\u6355\u6349\u5206\u5e03\u5f62\u72b6\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u51e0\u4f55\u8ddd\u79bb\u53ef\u80fd\u79fb\u9664\u4fe1\u606f\u6837\u672c\u5e76\u751f\u6210\u4f4e\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u800c\u4e8c\u503c\u5316\u65b9\u6cd5\u5c40\u90e8\u5904\u7406\u4e0d\u5e73\u8861\u5e76\u5ffd\u7565\u5168\u5c40\u7c7b\u95f4\u4f9d\u8d56\u3002\u5728\u7b97\u6cd5\u5c42\u9762\uff0c\u96c6\u6210\u65b9\u6cd5\u96be\u4ee5\u6574\u5408\u5f31\u5206\u7c7b\u5668\uff0c\u5bfc\u81f4\u9c81\u68d2\u6027\u6709\u9650\u3002", "method": "IMOVNO+\u662f\u4e00\u4e2a\u4e24\u7ea7\u6846\u67b6\uff1a1) \u6570\u636e\u7ea7\uff1a\u4f7f\u7528\u6761\u4ef6\u6982\u7387\u91cf\u5316\u6bcf\u4e2a\u6837\u672c\u7684\u4fe1\u606f\u91cf\uff1b\u5c06\u6570\u636e\u96c6\u5212\u5206\u4e3a\u6838\u5fc3\u3001\u91cd\u53e0\u548c\u566a\u58f0\u533a\u57df\uff1b\u5f15\u5165\u7ed3\u5408Z-score\u5ea6\u91cf\u548cbig-jump gap\u8ddd\u79bb\u7684\u91cd\u53e0\u6e05\u7406\u7b97\u6cd5\uff1b\u57fa\u4e8e\u591a\u6b63\u5219\u5316\u7684\u667a\u80fd\u8fc7\u91c7\u6837\u7b97\u6cd5\u63a7\u5236\u5408\u6210\u6837\u672c\u90bb\u8fd1\u5ea6\uff0c\u9632\u6b62\u65b0\u91cd\u53e0\u30022) \u7b97\u6cd5\u7ea7\uff1a\u4f7f\u7528\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u526a\u679d\u96c6\u6210\u5206\u7c7b\u5668\u4ee5\u51cf\u5c11\u5f31\u5b66\u4e60\u5668\u5f71\u54cd\u3002", "result": "\u572835\u4e2a\u6570\u636e\u96c6\uff0813\u4e2a\u591a\u7c7b\uff0c22\u4e2a\u4e8c\u7c7b\uff09\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aIMOVNO+\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6848\u4f8b\u4e2d\u63a5\u8fd1100%\u6027\u80fd\u3002\u5bf9\u4e8e\u591a\u7c7b\u6570\u636e\uff0cIMOVNO+\u5728G-mean\u4e0a\u83b7\u5f9737-57%\u589e\u76ca\uff0cF1-score 25-44%\uff0c\u7cbe\u786e\u738725-39%\uff0c\u53ec\u56de\u738726-43%\u3002\u5728\u4e8c\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u6027\u80fd\uff0c\u6539\u8fdb14-39%\u3002", "conclusion": "IMOVNO+\u6846\u67b6\u6709\u6548\u5904\u7406\u4e86\u4ece\u6570\u636e\u6536\u96c6\u548c\u9690\u79c1\u9650\u5236\u5bfc\u81f4\u7684\u6570\u636e\u7a00\u7f3a\u548c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u589e\u5f3a\u6570\u636e\u8d28\u91cf\u548c\u7b97\u6cd5\u9c81\u68d2\u6027\uff0c\u5728\u4e8c\u7c7b\u548c\u591a\u7c7b\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.20676", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20676", "abs": "https://arxiv.org/abs/2602.20676", "authors": ["Shuzhi Cao", "Rong Chen", "Ailong He", "Shuguang Han", "Jufeng Chen"], "title": "PRECTR-V2:Unified Relevance-CTR Framework with Cross-User Preference Mining, Exposure Bias Correction, and LLM-Distilled Encoder Optimization", "comment": "arXiv admin note: text overlap with arXiv:2503.18395", "summary": "In search systems, effectively coordinating the two core objectives of search relevance matching and click-through rate (CTR) prediction is crucial for discovering users' interests and enhancing platform revenue. In our prior work PRECTR, we proposed a unified framework to integrate these two subtasks,thereby eliminating their inconsistency and leading to mutual benefit.However, our previous work still faces three main challenges. First, low-active users and new users have limited search behavioral data, making it difficult to achieve effective personalized relevance preference modeling. Second, training data for ranking models predominantly come from high-relevance exposures, creating a distribution mismatch with the broader candidate space in coarse-ranking, leading to generalization bias. Third, due to the latency constraint, the original model employs an Emb+MLP architecture with a frozen BERT encoder, which prevents joint optimization and creates misalignment between representation learning and CTR fine-tuning. To solve these issues, we further reinforce our method and propose PRECTR-V2. Specifically, we mitigate the low-activity users' sparse behavior problem by mining global relevance preferences under the specific query, which facilitates effective personalized relevance modeling for cold-start scenarios. Subsequently, we construct hard negative samples through embedding noise injection and relevance label reconstruction, and optimize their relative ranking against positive samples via pairwise loss, thereby correcting exposure bias. Finally, we pretrain a lightweight transformer-based encoder via knowledge distillation from LLM and SFT on the text relevance classification task. This encoder replaces the frozen BERT module, enabling better adaptation to CTR fine-tuning and advancing beyond the traditional Emb+MLP paradigm.", "AI": {"tldr": "PRECTR-V2\u662f\u4e00\u4e2a\u6539\u8fdb\u7684\u641c\u7d22\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u6316\u6398\u5168\u5c40\u76f8\u5173\u6027\u504f\u597d\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u6784\u5efa\u786c\u8d1f\u6837\u672c\u6765\u7ea0\u6b63\u66dd\u5149\u504f\u5dee\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7Transformer\u7f16\u7801\u5668\u66ff\u4ee3\u51bb\u7ed3\u7684BERT\uff0c\u5b9e\u73b0\u66f4\u597d\u7684CTR\u5fae\u8c03\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3PRECTR\u6846\u67b6\u9762\u4e34\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u4f4e\u6d3b\u8dc3\u7528\u6237\u548c\u65b0\u7528\u6237\u641c\u7d22\u884c\u4e3a\u6570\u636e\u6709\u9650\uff0c\u96be\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u4e2a\u6027\u5316\u76f8\u5173\u6027\u504f\u597d\u5efa\u6a21\uff1b2) \u6392\u5e8f\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e3b\u8981\u6765\u81ea\u9ad8\u76f8\u5173\u6027\u66dd\u5149\uff0c\u4e0e\u7c97\u6392\u5019\u9009\u7a7a\u95f4\u5b58\u5728\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u6cdb\u5316\u504f\u5dee\uff1b3) \u7531\u4e8e\u5ef6\u8fdf\u9650\u5236\uff0c\u539f\u59cb\u6a21\u578b\u4f7f\u7528Emb+MLP\u67b6\u6784\u548c\u51bb\u7ed3\u7684BERT\u7f16\u7801\u5668\uff0c\u963b\u788d\u4e86\u8054\u5408\u4f18\u5316\u5e76\u9020\u6210\u8868\u793a\u5b66\u4e60\u4e0eCTR\u5fae\u8c03\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u3002", "method": "1) \u901a\u8fc7\u6316\u6398\u7279\u5b9a\u67e5\u8be2\u4e0b\u7684\u5168\u5c40\u76f8\u5173\u6027\u504f\u597d\uff0c\u7f13\u89e3\u4f4e\u6d3b\u8dc3\u7528\u6237\u7684\u7a00\u758f\u884c\u4e3a\u95ee\u9898\uff0c\u4fc3\u8fdb\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u7684\u6709\u6548\u4e2a\u6027\u5316\u76f8\u5173\u6027\u5efa\u6a21\uff1b2) \u901a\u8fc7\u5d4c\u5165\u566a\u58f0\u6ce8\u5165\u548c\u76f8\u5173\u6027\u6807\u7b7e\u91cd\u6784\u6784\u5efa\u786c\u8d1f\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u6210\u5bf9\u635f\u5931\u4f18\u5316\u5b83\u4eec\u76f8\u5bf9\u4e8e\u6b63\u6837\u672c\u7684\u76f8\u5bf9\u6392\u5e8f\uff0c\u4ece\u800c\u7ea0\u6b63\u66dd\u5149\u504f\u5dee\uff1b3) \u901a\u8fc7\u4eceLLM\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u548c\u5728\u6587\u672c\u76f8\u5173\u6027\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884cSFT\uff0c\u9884\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\uff0c\u66ff\u4ee3\u51bb\u7ed3\u7684BERT\u6a21\u5757\uff0c\u5b9e\u73b0\u66f4\u597d\u7684CTR\u5fae\u8c03\u9002\u5e94\u3002", "result": "PRECTR-V2\u89e3\u51b3\u4e86\u539f\u59cbPRECTR\u6846\u67b6\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1) \u6709\u6548\u5904\u7406\u51b7\u542f\u52a8\u7528\u6237\u7684\u4e2a\u6027\u5316\u76f8\u5173\u6027\u5efa\u6a21\uff1b2) \u7ea0\u6b63\u4e86\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0e\u5019\u9009\u7a7a\u95f4\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff1b3) \u5b9e\u73b0\u4e86\u8868\u793a\u5b66\u4e60\u4e0eCTR\u5fae\u8c03\u7684\u66f4\u597d\u5bf9\u9f50\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684Emb+MLP\u8303\u5f0f\u3002", "conclusion": "PRECTR-V2\u901a\u8fc7\u6539\u8fdb\u7684\u4e2a\u6027\u5316\u76f8\u5173\u6027\u5efa\u6a21\u3001\u66dd\u5149\u504f\u5dee\u7ea0\u6b63\u548c\u8f7b\u91cf\u7ea7Transformer\u7f16\u7801\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u641c\u7d22\u7cfb\u7edf\u4e2d\u76f8\u5173\u6027\u5339\u914d\u548cCTR\u9884\u6d4b\u7684\u534f\u8c03\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2602.20205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20205", "abs": "https://arxiv.org/abs/2602.20205", "authors": ["Xiwen Chen", "Wenhui Zhu", "Gen Li", "Xuanzhao Dong", "Yujian Xiong", "Hao Wang", "Peijie Qiu", "Qingquan Song", "Zhipeng Wang", "Shao Tang", "Yalin Wang", "Abolfazl Razi"], "title": "OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport", "comment": "Accepted by CVPR2026 (Findings). arXiv admin note: text overlap with arXiv:2503.02175 by other authors", "summary": "Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.", "AI": {"tldr": "OTPrune\uff1a\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u514d\u8bad\u7ec3\u89c6\u89c9token\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u51cf\u5c11MLLM\u63a8\u7406\u6210\u672c", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u89c6\u89c9token\u526a\u679d\u65b9\u6cd5\u5ffd\u89c6\u4e86\u89c6\u89c9\u8868\u793a\u7684\u5206\u5e03\u7ed3\u6784\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u526a\u679d\u65b9\u6cd5", "method": "\u5c06\u526a\u679d\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u5bf9\u9f50\u7684\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u5b8c\u6574token\u5206\u5e03\u4e0e\u526a\u679d\u540e\u5206\u5e03\u4e4b\u95f4\u76842-Wasserstein\u8ddd\u79bb\uff0c\u63a8\u5bfc\u51fa\u53ef\u5904\u7406\u7684\u5b50\u6a21\u76ee\u6807\u51fd\u6570", "result": "\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOTPrune\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90", "conclusion": "OTPrune\u901a\u8fc7\u5206\u5e03\u5bf9\u9f50\u5b9e\u73b0\u4e86\u7a33\u5b9a\u4e14\u8bed\u4e49\u4fdd\u771f\u7684\u89c6\u89c9token\u526a\u679d\uff0c\u4e3aMLLM\u63a8\u7406\u52a0\u901f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6846\u67b6"}}
{"id": "2602.20168", "categories": ["cs.CY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20168", "abs": "https://arxiv.org/abs/2602.20168", "authors": ["KMA Solaiman", "Joshua Sebastian", "Karma Tobden"], "title": "Benchmarking Early Deterioration Prediction Across Hospital-Rich and MCI-Like Emergency Triage Under Constrained Sensing", "comment": "10 pages, 4 figures, 6 tables. Submitted to IEEE ICHI 2026", "summary": "Emergency triage decisions are made under severe information constraints, yet most data-driven deterioration models are evaluated using signals unavailable during initial assessment. We present a leakage-aware benchmarking framework for early deterioration prediction that evaluates model performance under realistic, time-limited sensing conditions. Using a patient-deduplicated cohort derived from MIMIC-IV-ED, we compare hospital-rich triage with a vitals-only, MCI-like setting, restricting inputs to information available within the first hour of presentation. Across multiple modeling approaches, predictive performance declines only modestly when limited to vitals, indicating that early physiological measurements retain substantial clinical signal. Structured ablation and interpretability analyses identify respiratory and oxygenation measures as the most influential contributors to early risk stratification, with models exhibiting stable, graceful degradation as sensing is reduced. This work provides a clinically grounded benchmark to support the evaluation and design of deployable triage decision-support systems in resource-constrained settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6cc4\u6f0f\u611f\u77e5\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u65e9\u671f\u6076\u5316\u9884\u6d4b\u6a21\u578b\u5728\u771f\u5b9e\u65f6\u95f4\u9650\u5236\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u4ec5\u4f7f\u7528\u751f\u547d\u4f53\u5f81\u6570\u636e\u65f6\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\u6709\u9650\uff0c\u547c\u5438\u548c\u6c27\u5408\u6d4b\u91cf\u662f\u6700\u91cd\u8981\u7684\u98ce\u9669\u5206\u5c42\u6307\u6807\u3002", "motivation": "\u6025\u8bca\u5206\u8bca\u51b3\u7b56\u901a\u5e38\u5728\u4e25\u91cd\u4fe1\u606f\u7ea6\u675f\u4e0b\u8fdb\u884c\uff0c\u4f46\u5927\u591a\u6570\u6570\u636e\u9a71\u52a8\u7684\u6076\u5316\u6a21\u578b\u90fd\u4f7f\u7528\u4e86\u521d\u59cb\u8bc4\u4f30\u65f6\u4e0d\u53ef\u7528\u7684\u4fe1\u53f7\u8fdb\u884c\u8bc4\u4f30\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u4e34\u5e8a\u57fa\u7840\u7684\u57fa\u51c6\u6765\u652f\u6301\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u53ef\u90e8\u7f72\u5206\u8bca\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u8bbe\u8ba1\u3002", "method": "\u4f7f\u7528\u4eceMIMIC-IV-ED\u884d\u751f\u7684\u53bb\u91cd\u60a3\u8005\u961f\u5217\uff0c\u6bd4\u8f83\u533b\u9662\u4e30\u5bcc\u5206\u8bca\u4e0e\u4ec5\u4f7f\u7528\u751f\u547d\u4f53\u5f81\u7684MCI\u6837\u8bbe\u7f6e\uff0c\u5c06\u8f93\u5165\u9650\u5236\u5728\u5c31\u8bca\u540e\u7b2c\u4e00\u5c0f\u65f6\u5185\u53ef\u7528\u7684\u4fe1\u606f\u3002\u91c7\u7528\u7ed3\u6784\u5316\u6d88\u878d\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u6765\u8bc6\u522b\u6700\u91cd\u8981\u7684\u98ce\u9669\u5206\u5c42\u6307\u6807\u3002", "result": "\u5728\u591a\u79cd\u5efa\u6a21\u65b9\u6cd5\u4e2d\uff0c\u5f53\u4ec5\u9650\u4e8e\u751f\u547d\u4f53\u5f81\u65f6\uff0c\u9884\u6d4b\u6027\u80fd\u4ec5\u9002\u5ea6\u4e0b\u964d\uff0c\u8868\u660e\u65e9\u671f\u751f\u7406\u6d4b\u91cf\u4fdd\u7559\u4e86\u91cd\u8981\u7684\u4e34\u5e8a\u4fe1\u53f7\u3002\u547c\u5438\u548c\u6c27\u5408\u6d4b\u91cf\u88ab\u786e\u5b9a\u4e3a\u65e9\u671f\u98ce\u9669\u5206\u5c42\u7684\u6700\u6709\u5f71\u54cd\u529b\u7684\u8d21\u732e\u56e0\u7d20\uff0c\u6a21\u578b\u5728\u611f\u77e5\u51cf\u5c11\u65f6\u8868\u73b0\u51fa\u7a33\u5b9a\u3001\u4f18\u96c5\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u652f\u6301\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u53ef\u90e8\u7f72\u5206\u8bca\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e34\u5e8a\u57fa\u7840\u7684\u57fa\u51c6\uff0c\u8868\u660e\u65e9\u671f\u751f\u7406\u6d4b\u91cf\u5728\u4fe1\u606f\u53d7\u9650\u6761\u4ef6\u4e0b\u4ecd\u7136\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u4ef7\u503c\u3002"}}
{"id": "2602.20704", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20704", "abs": "https://arxiv.org/abs/2602.20704", "authors": ["Zesheng Wang", "Longfei Xu", "Weidong Deng", "Huimin Yan", "Kaikui Liu", "Xiangxiang Chu"], "title": "IntRR: A Framework for Integrating SID Redistribution and Length Reduction", "comment": null, "summary": "Generative Recommendation (GR) has emerged as a transformative paradigm that reformulates the traditional cascade ranking system into a sequence-to-item generation task, facilitated by the use of discrete Semantic IDs (SIDs). However, current SIDs are suboptimal as the indexing objectives (Stage 1) are misaligned with the actual recommendation goals (Stage 2). Since these identifiers remain static (Stage 2), the backbone model lacks the flexibility to adapt them to the evolving complexities of user interactions. Furthermore, the prevailing strategy of flattening hierarchical SIDs into token sequences leads to sequence length inflation, resulting in prohibitive computational overhead and inference latency. To address these challenges, we propose IntRR, a novel framework that integrates objective-aligned SID Redistribution and structural Length Reduction. By leveraging item-specific Unique IDs (UIDs) as collaborative anchors, this approach dynamically redistributes semantic weights across hierarchical codebook layers. Concurrently, IntRR handles the SID hierarchy recursively, eliminating the need to flatten sequences. This ensures a fixed cost of one token per item. Extensive experiments on benchmark datasets demonstrate that IntRR yields substantial improvements over representative generative baselines, achieving superior performance in both recommendation accuracy and efficiency.", "AI": {"tldr": "IntRR\u6846\u67b6\u901a\u8fc7\u8bed\u4e49ID\u91cd\u5206\u914d\u548c\u957f\u5ea6\u7f29\u51cf\uff0c\u89e3\u51b3\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u8bed\u4e49ID\u4e0e\u63a8\u8350\u76ee\u6807\u4e0d\u5bf9\u9f50\u3001\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u7684\u8bed\u4e49ID\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u7d22\u5f15\u76ee\u6807\u4e0e\u63a8\u8350\u76ee\u6807\u4e0d\u5bf9\u9f50\uff0c\u5bfc\u81f4\u9759\u6001ID\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u7528\u6237\u4ea4\u4e92\uff1b2\uff09\u6241\u5e73\u5316\u5206\u5c42\u8bed\u4e49ID\u5bfc\u81f4\u5e8f\u5217\u957f\u5ea6\u81a8\u80c0\uff0c\u8ba1\u7b97\u5f00\u9500\u548c\u63a8\u7406\u5ef6\u8fdf\u8fc7\u9ad8\u3002", "method": "\u63d0\u51faIntRR\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u7269\u54c1\u552f\u4e00ID\u4f5c\u4e3a\u534f\u4f5c\u951a\u70b9\uff0c\u52a8\u6001\u91cd\u5206\u914d\u8bed\u4e49\u6743\u91cd\u5230\u5206\u5c42\u7801\u672c\uff1b2\uff09\u9012\u5f52\u5904\u7406\u8bed\u4e49ID\u5c42\u6b21\u7ed3\u6784\uff0c\u907f\u514d\u5e8f\u5217\u6241\u5e73\u5316\uff0c\u5b9e\u73b0\u6bcf\u4e2a\u7269\u54c1\u56fa\u5b9a1\u4e2atoken\u7684\u6210\u672c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIntRR\u76f8\u6bd4\u4ee3\u8868\u6027\u751f\u6210\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "IntRR\u901a\u8fc7\u8bed\u4e49ID\u91cd\u5206\u914d\u548c\u957f\u5ea6\u7f29\u51cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u4e2d\u7684\u8bed\u4e49ID\u4e0d\u5bf9\u9f50\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20333", "abs": "https://arxiv.org/abs/2602.20333", "authors": ["Samarth KaPatel", "Sofia Nikiforova", "Giacinto Paolo Saggese", "Paul Smith"], "title": "DMCD: Semantic-Statistical Framework for Causal Discovery", "comment": null, "summary": "We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions.\n  We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.", "AI": {"tldr": "DMCD\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8eLLM\u7684\u8bed\u4e49\u8349\u56fe\u548c\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u7eaf\u7edf\u8ba1\u65b9\u6cd5\uff08\u53ef\u80fd\u53d7\u6570\u636e\u7a00\u758f\u6027\u5f71\u54cd\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u7eaf\u8bed\u4e49\u65b9\u6cd5\uff08\u7f3a\u4e4f\u7edf\u8ba1\u9a8c\u8bc1\uff09\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u5148\u9a8c\u548c\u7edf\u8ba1\u9a8c\u8bc1\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "DMCD\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u53d8\u91cf\u5143\u6570\u636e\u751f\u6210\u7a00\u758f\u7684\u56e0\u679c\u56fe\u8349\u6848\uff0c\u4f5c\u4e3a\u8bed\u4e49\u5148\u9a8c\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6761\u4ef6\u72ec\u7acb\u6027\u6d4b\u8bd5\u5bf9\u8349\u6848\u8fdb\u884c\u5ba1\u8ba1\u548c\u7cbe\u70bc\uff0c\u68c0\u6d4b\u5230\u7684\u5dee\u5f02\u6307\u5bfc\u6709\u9488\u5bf9\u6027\u7684\u8fb9\u4fee\u6b63\u3002", "result": "\u5728\u5de5\u4e1a\u5de5\u7a0b\u3001\u73af\u5883\u76d1\u6d4b\u548cIT\u7cfb\u7edf\u5206\u6790\u4e09\u4e2a\u5143\u6570\u636e\u4e30\u5bcc\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDMCD\u76f8\u6bd4\u591a\u79cd\u56e0\u679c\u53d1\u73b0\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6216\u9886\u5148\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5c06\u8bed\u4e49\u5148\u9a8c\u4e0e\u539f\u5219\u6027\u7edf\u8ba1\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u6027\u80fd\u4e14\u5b9e\u9645\u6709\u6548\u7684\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bed\u4e49\u63a8\u7406\u800c\u975e\u57fa\u51c6\u56fe\u8bb0\u5fc6\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u3002"}}
{"id": "2602.20291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20291", "abs": "https://arxiv.org/abs/2602.20291", "authors": ["Valentin Bonas", "Martin Sinnona", "Viviana Siless", "Emmanuel Iarussi"], "title": "De-rendering, Reasoning, and Repairing Charts with Vision-Language Models", "comment": null, "summary": "Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u56fe\u8868\u53cd\u6e32\u67d3\u3001\u81ea\u52a8\u5206\u6790\u548c\u8fed\u4ee3\u6539\u8fdb\u7684\u6846\u67b6\uff0c\u4e3a\u53ef\u89c6\u5316\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u64cd\u4f5c\u3001\u53ef\u89e3\u91ca\u7684\u53cd\u9988\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u5347\u56fe\u8868\u8d28\u91cf\u548c\u7528\u6237\u7684\u53ef\u89c6\u5316\u7d20\u517b\u3002", "motivation": "\u6570\u636e\u53ef\u89c6\u5316\u5728\u79d1\u5b66\u4f20\u64ad\u3001\u65b0\u95fb\u548c\u65e5\u5e38\u51b3\u7b56\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e38\u5b58\u5728\u9519\u8bef\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u68c0\u67e5\u5de5\u5177\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4e14\u65e0\u6cd5\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u6539\u8fdb\u5efa\u8bae\uff0c\u800c\u901a\u7528LLM\u5728\u53ef\u89c6\u5316\u8d28\u91cf\u8bc4\u4f30\u4e0a\u4e0d\u53ef\u9760\uff0c\u7f3a\u4e4f\u9075\u5faa\u53ef\u89c6\u5316\u8bbe\u8ba1\u539f\u5219\u7684\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff1a1) \u4ece\u56fe\u50cf\u91cd\u5efa\u56fe\u8868\u7ed3\u6784\uff08\u53cd\u6e32\u67d3\uff09\uff1b2) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\uff1b3) \u57fa\u4e8e\u53ef\u89c6\u5316\u7814\u7a76\u539f\u5219\u63d0\u51fa\u5177\u4f53\u4fee\u6539\u5efa\u8bae\uff1b4) \u7528\u6237\u53ef\u9009\u62e9\u5e94\u7528\u6539\u8fdb\u5e76\u91cd\u65b0\u6e32\u67d3\uff0c\u5f62\u6210\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728Chart2Code\u57fa\u51c6\u6d4b\u8bd5\u76841000\u4e2a\u56fe\u8868\u4e0a\uff0c\u7cfb\u7edf\u751f\u6210\u4e8610,452\u6761\u8bbe\u8ba1\u5efa\u8bae\uff0c\u805a\u7c7b\u4e3a10\u4e2a\u8fde\u8d2f\u7c7b\u522b\uff08\u5982\u8f74\u683c\u5f0f\u5316\u3001\u989c\u8272\u53ef\u8bbf\u95ee\u6027\u3001\u56fe\u4f8b\u4e00\u81f4\u6027\u7b49\uff09\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u63a8\u8350\u7cfb\u7edf\u5728\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u539f\u5219\u7684\u53ef\u89c6\u5316\u8bbe\u8ba1\u53cd\u9988\u65b9\u9762\u5177\u6709\u524d\u666f\uff0c\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u66f4\u6613\u7528\u7684\u521b\u4f5c\u5de5\u5177\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2602.20735", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20735", "abs": "https://arxiv.org/abs/2602.20735", "authors": ["Kun Ran", "Marwah Alaofi", "Danula Hettiachchi", "Chenglong Ma", "Khoi Nguyen Dinh Anh", "Khoi Vo Nguyen", "Sachin Pathiyan Cherumanal", "Lida Rashidi", "Falk Scholer", "Damiano Spina", "Shuoqi Sun", "Oleg Zendel"], "title": "RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition", "comment": "MMU-RAG NeurIPS 2025 winning system", "summary": "This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text\n  track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG\n  (R2RAG), a research-focused retrieval-augmented generation (RAG)\n  architecture composed of lightweight components that dynamically adapt the\n  retrieval strategy based on inferred query complexity and evidence\n  sufficiency. The system uses smaller LLMs, enabling operation on a single\n  consumer-grade GPU while supporting complex research tasks. It builds on the\n  G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it\n  with modules informed by qualitative review of outputs. R2RAG won the Best\n  Dynamic Evaluation award in the Open Source category, demonstrating high\n  effectiveness with careful design and efficient use of resources.", "AI": {"tldr": "R2RAG\u7cfb\u7edf\u5728NeurIPS 2025 MMU-RAG\u7ade\u8d5b\u4e2d\u83b7\u5956\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u67b6\u6784\uff0c\u80fd\u6839\u636e\u67e5\u8be2\u590d\u6742\u6027\u548c\u8bc1\u636e\u5145\u5206\u6027\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u7b56\u7565\uff0c\u53ef\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u8fd0\u884c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfRAG\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u7814\u7a76\u4efb\u52a1\u65f6\u8d44\u6e90\u6d88\u8017\u5927\u3001\u68c0\u7d22\u7b56\u7565\u56fa\u5b9a\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u9ad8\u6548\u8fd0\u884c\u7684\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94RAG\u7cfb\u7edf\u3002", "method": "\u63d0\u51faRouting-to-RAG\u67b6\u6784\uff0c\u57fa\u4e8eG-RAG\u7cfb\u7edf\u6269\u5c55\uff0c\u5305\u542b\u8f7b\u91cf\u7ea7\u7ec4\u4ef6\uff0c\u80fd\u52a8\u6001\u63a8\u65ad\u67e5\u8be2\u590d\u6742\u6027\u548c\u8bc1\u636e\u5145\u5206\u6027\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u68c0\u7d22\u7b56\u7565\uff0c\u4f7f\u7528\u8f83\u5c0f\u7684LLM\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u5728NeurIPS 2025 MMU-RAG\u7ade\u8d5b\u4e2d\u83b7\u5f97\u6700\u4f73\u52a8\u6001\u8bc4\u4f30\u5956\uff0c\u5728\u5f00\u6e90\u7c7b\u522b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u4e5f\u80fd\u9ad8\u6548\u652f\u6301\u590d\u6742\u7814\u7a76\u4efb\u52a1\u3002", "conclusion": "R2RAG\u5c55\u793a\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u548c\u8d44\u6e90\u9ad8\u6548\u5229\u7528\uff0c\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94RAG\u7cfb\u7edf\u80fd\u591f\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u590d\u6742\u7814\u7a76\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20422", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20422", "abs": "https://arxiv.org/abs/2602.20422", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "comment": null, "summary": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "AI": {"tldr": "DMEMM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u73af\u5883\u673a\u5236\uff08\u8f6c\u79fb\u52a8\u6001\u548c\u5956\u52b1\u51fd\u6570\uff09\u6765\u8c03\u5236\u6269\u6563\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u89c4\u5212\u4e2d\u8f68\u8ff9\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u751f\u6210\u8f68\u8ff9\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u8f68\u8ff9\u4e2d\u72b6\u6001\u8f6c\u79fb\u7684\u4e00\u81f4\u6027\u8981\u6c42\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8f68\u8ff9\u4e0e\u771f\u5b9e\u73af\u5883\u673a\u5236\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5f71\u54cd\u89c4\u5212\u6548\u679c\u3002", "method": "\u63d0\u51faDMEMM\u65b9\u6cd5\uff0c\u5728\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u4e2d\u878d\u5165\u5173\u952eRL\u73af\u5883\u673a\u5236\uff0c\u7279\u522b\u662f\u8f6c\u79fb\u52a8\u6001\u548c\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u73af\u5883\u673a\u5236\u5efa\u6a21\u6765\u8c03\u5236\u6269\u6563\u8fc7\u7a0b\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8f68\u8ff9\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5177\u6709\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDMEMM\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5c06\u73af\u5883\u673a\u5236\u5efa\u6a21\u878d\u5165\u6269\u6563\u6a21\u578b\u8bad\u7ec3\uff0cDMEMM\u6709\u6548\u89e3\u51b3\u4e86\u8f68\u8ff9\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20312", "abs": "https://arxiv.org/abs/2602.20312", "authors": ["Guodong Chen", "Huanshuo Dong", "Mallesham Dasari"], "title": "N4MC: Neural 4D Mesh Compression", "comment": null, "summary": "We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.", "AI": {"tldr": "N4MC\u662f\u9996\u4e2a4D\u795e\u7ecf\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u5197\u4f59\u6765\u9ad8\u6548\u538b\u7f29\u65f6\u53d8\u7f51\u683c\u5e8f\u5217\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7387\u5931\u771f\u6027\u80fd\u5e76\u652f\u6301\u5b9e\u65f6\u89e3\u7801\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u683c\u538b\u7f29\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u7f51\u683c\u5e27\uff0c\u5ffd\u7565\u4e86\u65f6\u95f4\u5197\u4f59\u3002\u53d72D\u89c6\u9891\u7f16\u89e3\u7801\u5668\u4e2d\u5e27\u95f4\u538b\u7f29\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5229\u7528\u7f51\u683c\u5e8f\u5217\u65f6\u95f4\u76f8\u5173\u6027\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u5c06\u8fde\u7eed\u7684\u4e0d\u89c4\u5219\u7f51\u683c\u5e27\u8f6c\u6362\u4e3a\u89c4\u5219\u76844D\u5f20\u91cf\u4f5c\u4e3a\u7edf\u4e00\u7d27\u51d1\u8868\u793a\uff1b\u4f7f\u7528\u81ea\u52a8\u89e3\u7801\u5668\u538b\u7f29\u5f20\u91cf\u4ee5\u6355\u83b7\u65f6\u7a7a\u76f8\u5173\u6027\uff1b\u5f15\u5165\u57fa\u4e8etransformer\u7684\u63d2\u503c\u6a21\u578b\uff0c\u901a\u8fc7\u8ddf\u8e2a\u4f53\u79ef\u4e2d\u5fc3\u7684\u6f5c\u5728\u5d4c\u5165\u9884\u6d4b\u4e2d\u95f4\u7f51\u683c\u5e27\uff0c\u6d88\u9664\u8fd0\u52a8\u6a21\u7cca\u3002", "result": "N4MC\u5728\u7387\u5931\u771f\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u65f6\u89e3\u78014D\u7f51\u683c\u5e8f\u5217\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "N4MC\u901a\u8fc7\u5229\u7528\u7f51\u683c\u5e8f\u5217\u7684\u65f6\u95f4\u5197\u4f59\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u76844D\u795e\u7ecf\u538b\u7f29\uff0c\u4e3a\u65f6\u53d8\u7f51\u683c\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20170", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20170", "abs": "https://arxiv.org/abs/2602.20170", "authors": ["Chaeyun Kim", "YongTaek Lim", "Kihyun Kim", "Junghwan Kim", "Minwoo Kim"], "title": "CAGE: A Framework for Culturally Adaptive Red-Teaming Benchmark Generation", "comment": "Accepted at ICLR 2026", "summary": "Existing red-teaming benchmarks, when adapted to new languages via direct translation, fail to capture socio-technical vulnerabilities rooted in local culture and law, creating a critical blind spot in LLM safety evaluation. To address this gap, we introduce CAGE (Culturally Adaptive Generation), a framework that systematically adapts the adversarial intent of proven red-teaming prompts to new cultural contexts. At the core of CAGE is the Semantic Mold, a novel approach that disentangles a prompt's adversarial structure from its cultural content. This approach enables the modeling of realistic, localized threats rather than testing for simple jailbreaks. As a representative example, we demonstrate our framework by creating KoRSET, a Korean benchmark, which proves more effective at revealing vulnerabilities than direct translation baselines. CAGE offers a scalable solution for developing meaningful, context-aware safety benchmarks across diverse cultures. Our dataset and evaluation rubrics are publicly available at https://github.com/selectstar-ai/CAGE-paper. (WARNING: This paper contains model outputs that can be offensive in nature.)", "AI": {"tldr": "CAGE\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u6a21\u5177\u5206\u79bb\u5bf9\u6297\u6027\u7ed3\u6784\u4e0e\u6587\u5316\u5185\u5bb9\uff0c\u5c06\u7ea2\u961f\u6d4b\u8bd5\u57fa\u51c6\u9002\u5e94\u5230\u65b0\u6587\u5316\u8bed\u5883\uff0c\u6bd4\u76f4\u63a5\u7ffb\u8bd1\u66f4\u6709\u6548\u63ed\u793a\u672c\u5730\u5316\u6f0f\u6d1e", "motivation": "\u73b0\u6709\u7ea2\u961f\u6d4b\u8bd5\u57fa\u51c6\u901a\u8fc7\u76f4\u63a5\u7ffb\u8bd1\u9002\u5e94\u65b0\u8bed\u8a00\u65f6\uff0c\u65e0\u6cd5\u6355\u6349\u57fa\u4e8e\u5f53\u5730\u6587\u5316\u548c\u6cd5\u5f8b\u7684\u793e\u4f1a\u6280\u672f\u6f0f\u6d1e\uff0c\u5728LLM\u5b89\u5168\u8bc4\u4f30\u4e2d\u9020\u6210\u5173\u952e\u76f2\u70b9", "method": "\u63d0\u51faCAGE\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u8bed\u4e49\u6a21\u5177\u65b9\u6cd5\uff0c\u5c06\u5df2\u9a8c\u8bc1\u7ea2\u961f\u63d0\u793a\u7684\u5bf9\u6297\u6027\u610f\u56fe\u4ece\u5176\u6587\u5316\u5185\u5bb9\u4e2d\u89e3\u8026\uff0c\u7cfb\u7edf\u6027\u5730\u9002\u5e94\u5230\u65b0\u6587\u5316\u8bed\u5883", "result": "\u4ee5\u97e9\u8bed\u57fa\u51c6KoRSET\u4e3a\u4f8b\uff0c\u8bc1\u660eCAGE\u6bd4\u76f4\u63a5\u7ffb\u8bd1\u57fa\u7ebf\u66f4\u6709\u6548\u63ed\u793a\u6f0f\u6d1e\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8de8\u6587\u5316\u5b89\u5168\u8bc4\u4f30\u65b9\u6848", "conclusion": "CAGE\u4e3a\u5f00\u53d1\u6709\u610f\u4e49\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b89\u5168\u57fa\u51c6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5efa\u6a21\u73b0\u5b9e\u672c\u5730\u5316\u5a01\u80c1\u800c\u975e\u7b80\u5355\u8d8a\u72f1\u6d4b\u8bd5"}}
{"id": "2602.20210", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20210", "abs": "https://arxiv.org/abs/2602.20210", "authors": ["Kiyoung Seong", "Sungsoo Ahn", "Sehui Han", "Changyoung Park"], "title": "Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling", "comment": null, "summary": "Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \\emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \\emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks.", "AI": {"tldr": "MCFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6d41\u6a21\u578b\uff0c\u901a\u8fc7\u72ec\u7acb\u7684\u539f\u5b50\u7c7b\u578b\u548c\u6676\u4f53\u7ed3\u6784\u65f6\u95f4\u53d8\u91cf\uff0c\u5c06\u591a\u79cd\u6676\u4f53\u751f\u6210\u4efb\u52a1\u5b9e\u73b0\u4e3a\u4e0d\u540c\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5927\u591a\u662f\u4efb\u52a1\u7279\u5b9a\u7684\uff0c\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u591f\u8de8\u4e0d\u540c\u751f\u6210\u4efb\u52a1\u5171\u4eab\u6676\u4f53\u8868\u793a\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6676\u4f53\u6d41\u6a21\u578b\uff0c\u901a\u8fc7\u72ec\u7acb\u7684\u539f\u5b50\u7c7b\u578b\u548c\u6676\u4f53\u7ed3\u6784\u65f6\u95f4\u53d8\u91cf\u5b9e\u73b0\u591a\u4efb\u52a1\u7edf\u4e00\uff1b\u5f15\u5165\u57fa\u4e8e\u7ec4\u6210\u548c\u5bf9\u79f0\u6027\u7684\u539f\u5b50\u6392\u5e8f\u4e0e\u5206\u5c42\u6392\u5217\u589e\u5f3a\uff0c\u5728\u6807\u51c6Transformer\u6a21\u578b\u4e2d\u6ce8\u5165\u5f3a\u7ec4\u6210\u548c\u6676\u4f53\u5b66\u5148\u9a8c\u3002", "result": "\u5728MP-20\u548cMPTS-52\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMCFlow\u5728\u591a\u4e2a\u6676\u4f53\u751f\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u4efb\u52a1\u7279\u5b9a\u57fa\u7ebf\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "MCFlow\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u79cd\u6676\u4f53\u751f\u6210\u4efb\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2602.20800", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20800", "abs": "https://arxiv.org/abs/2602.20800", "authors": ["Dalia Nahhas", "Xiaohao Cai", "Imran Razzak", "Shoaib Jameel"], "title": "Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking", "comment": null, "summary": "In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6cc4\u6f0f\u7684\u53cc\u6cd5\u5b98\u6846\u67b6\u6765\u89e3\u51b3GenIR\u4e2d\u6587\u5316\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u5faa\u73af\u6027\u548c\u504f\u597d\u6cc4\u6f0f\u95ee\u9898\uff0c\u901a\u8fc7\u4e25\u683c\u5206\u79bb\u76d1\u7763\u548c\u8bc4\u4f30\u6a21\u578b\uff0c\u8bc1\u660e\u53ef\u4ee5\u5c06\u6587\u5316\u504f\u597d\u84b8\u998f\u5230\u9ad8\u6548\u6392\u5e8f\u5668\u4e2d\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u4fe1\u606f\u68c0\u7d22(GenIR)\u4e2d\u7684\u74f6\u9888\u5df2\u4ece\u751f\u6210\u8f6c\u5411\u5019\u9009\u9009\u62e9\uff0c\u7279\u522b\u662f\u5728\u6587\u5316\u76f8\u5173\u6027\u7b49\u89c4\u8303\u6027\u6807\u51c6\u65b9\u9762\u3002\u73b0\u6709\u7684LLM-as-a-Judge\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5faa\u73af\u6027\u548c\u504f\u597d\u6cc4\u6f0f\u95ee\u9898\uff0c\u5373\u76d1\u7763\u548c\u8bc4\u4f30\u6a21\u578b\u91cd\u53e0\u4f1a\u5938\u5927\u6027\u80fd\u8868\u73b0\u3002", "method": "1) \u5c06\u6587\u5316\u76f8\u5173\u6027\u5f62\u5f0f\u5316\u4e3a\u67e5\u8be2\u5185\u6392\u5e8f\u4efb\u52a1\uff1b2) \u5f15\u5165\u65e0\u6cc4\u6f0f\u7684\u53cc\u6cd5\u5b98\u6846\u67b6\uff0c\u4e25\u683c\u5206\u79bb\u76d1\u7763(Judge B)\u548c\u8bc4\u4f30(Judge A)\uff1b3) \u5728\u5305\u542b33,052\u4e2a\u6587\u5316\u57fa\u7840\u6545\u4e8b\u7684\u65b0\u57fa\u51c6(NGR-33k)\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff1b4) \u4eceJudge B\u76d1\u7763\u7684\u4ea4\u53c9\u7f16\u7801\u5668\u4e2d\u84b8\u998f\u51fa\u5bc6\u96c6\u53cc\u7f16\u7801\u5668\uff1b5) \u5728\u4eba\u5de5\u7b56\u5212\u7684Moral Stories\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "1) \u7ecf\u5178\u57fa\u7ebf\u65b9\u6cd5\u4ec5\u83b7\u5f97\u9002\u5ea6\u589e\u76ca\uff1b2) \u4ece\u4ea4\u53c9\u7f16\u7801\u5668\u84b8\u998f\u51fa\u7684\u5bc6\u96c6\u53cc\u7f16\u7801\u5668(BGE-M3)\u975e\u5e38\u6709\u6548\uff1b3) \u867d\u7136\u4ea4\u53c9\u7f16\u7801\u5668\u4e3a\u84b8\u998f\u63d0\u4f9b\u4e86\u5f3a\u76d1\u7763\u4fe1\u53f7\uff0c\u4f46\u5728\u65e0\u6cc4\u6f0f\u7684Judge A\u8bc4\u4f30\u4e0b\uff0c\u84b8\u998f\u540e\u7684BGE-M3\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4ea4\u53c9\u7f16\u7801\u5668\uff1b4) \u5728Moral Stories\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u4e0e\u4eba\u7c7b\u89c4\u8303\u7684\u5f3a\u5bf9\u9f50\u3002", "conclusion": "\u4e25\u683c\u7684\u8bc4\u4f30\u5668\u5206\u79bb\u662f\u53ef\u4fe1GenIR\u8bc4\u4f30\u7684\u524d\u63d0\uff0c\u8bc1\u660e\u53ef\u4ee5\u5c06\u5fae\u5999\u7684\u6587\u5316\u504f\u597d\u84b8\u998f\u5230\u9ad8\u6548\u6392\u5e8f\u5668\u4e2d\u800c\u65e0\u9700\u6cc4\u6f0f\u3002\u8be5\u6846\u67b6\u4e3a\u6587\u5316\u76f8\u5173\u6027\u548c\u5176\u4ed6\u89c4\u8303\u6027\u6807\u51c6\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u65b9\u6cd5\u3002"}}
{"id": "2602.20328", "categories": ["cs.CV", "eess.IV", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.20328", "abs": "https://arxiv.org/abs/2602.20328", "authors": ["Romario Gualdr\u00f3n-Hurtado", "Roman Jacome", "Rafael S. Suarez", "Henry Arguello"], "title": "GSNR: Graph Smooth Null-Space Representation for Inverse Problems", "comment": "23 pages, 24 figures, Accepted to The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2026", "summary": "Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.", "AI": {"tldr": "\u63d0\u51faGraph-Smooth Null-Space Representation (GSNR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e0d\u53ef\u89c1\u5206\u91cf\uff08\u96f6\u7a7a\u95f4\uff09\u4e2d\u5f15\u5165\u56fe\u5e73\u6ed1\u7ed3\u6784\u6765\u6539\u8fdb\u56fe\u50cf\u9006\u95ee\u9898\u91cd\u5efa\uff0c\u5728\u591a\u79cd\u9006\u95ee\u9898\u573a\u666f\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u8fbe4.3 dB PSNR\u3002", "motivation": "\u56fe\u50cf\u9006\u95ee\u9898\u901a\u5e38\u5b58\u5728\u65e0\u9650\u591a\u89e3\uff0c\u4f20\u7edf\u56fe\u50cf\u5148\u9a8c\uff08\u5982\u7a00\u758f\u6027\u3001\u5e73\u6ed1\u6027\u6216\u5f97\u5206\u51fd\u6570\uff09\u4ec5\u7ea6\u675f\u56fe\u50cf\u6d41\u5f62\u800c\u4e0d\u7ea6\u675f\u96f6\u7a7a\u95f4\u5206\u91cf\uff0c\u53ef\u80fd\u5bfc\u81f4\u91cd\u5efa\u504f\u5dee\u3002\u56e0\u6b64\u9700\u8981\u5c06\u6709\u610f\u4e49\u7684\u96f6\u7a7a\u95f4\u4fe1\u606f\u7eb3\u5165\u91cd\u5efa\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u56fe\u5e73\u6ed1\u96f6\u7a7a\u95f4\u8868\u793a(GSNR)\uff1a\u57fa\u4e8e\u56fe\u62c9\u666e\u62c9\u65af\u6784\u9020\u96f6\u9650\u5236\u62c9\u666e\u62c9\u65af\uff0c\u7f16\u7801\u96f6\u7a7a\u95f4\u4fe1\u53f7\u4e2d\u76f8\u90bb\u50cf\u7d20\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u4ecep\u4e2a\u6700\u5e73\u6ed1\u7684\u8c31\u56fe\u6a21\u5f0f\uff08\u6700\u4f4e\u56fe\u9891\u7387\uff09\u8bbe\u8ba1\u4f4e\u7ef4\u6295\u5f71\u77e9\u9635\u3002\u5c06GSNR\u96c6\u6210\u5230PnP\u3001DIP\u548c\u6269\u6563\u6c42\u89e3\u5668\u7b49\u9006\u95ee\u9898\u6c42\u89e3\u5668\u4e2d\u3002", "result": "\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u3001\u538b\u7f29\u611f\u77e5\u3001\u53bb\u9a6c\u8d5b\u514b\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u56db\u79cd\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5PSNR\u63d0\u5347\u8fbe4.3 dB\uff0c\u76f8\u6bd4\u7aef\u5230\u7aef\u5b66\u4e60\u6a21\u578b\u63d0\u5347\u8fbe1 dB\u3002GSNR\u5e26\u6765\uff1ai) \u901a\u8fc7\u96f6\u7a7a\u95f4\u56fe\u6b63\u5219\u5316\u6539\u5584\u6536\u655b\u6027\uff0cii) \u66f4\u597d\u7684\u8986\u76d6\u8303\u56f4\uff08p\u4e2a\u6a21\u5f0f\u6355\u83b7\u7684\u96f6\u7a7a\u95f4\u65b9\u5dee\uff09\uff0ciii) \u66f4\u9ad8\u7684\u53ef\u9884\u6d4b\u6027\uff08\u8fd9\u4e9b\u6a21\u5f0f\u4ece\u6d4b\u91cf\u4e2d\u63a8\u65ad\u7684\u80fd\u529b\uff09\u3002", "conclusion": "\u901a\u8fc7\u5728\u56fe\u5e73\u6ed1\u96f6\u7a7a\u95f4\u8868\u793a\u4e2d\u5f15\u5165\u7ed3\u6784\u5316\u7684\u96f6\u7a7a\u95f4\u4fe1\u606f\uff0c\u80fd\u591f\u663e\u8457\u6539\u8fdb\u56fe\u50cf\u9006\u95ee\u9898\u91cd\u5efa\u8d28\u91cf\uff0c\u4e3a\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u9006\u95ee\u9898\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.20180", "categories": ["cs.CY", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20180", "abs": "https://arxiv.org/abs/2602.20180", "authors": ["EunJeong Cheon", "Do Yeon Shin"], "title": "Is Robot Labor Labor? Delivery Robots and the Politics of Work in Public Space", "comment": null, "summary": "As sidewalk delivery robots become increasingly integrated into urban life, this paper begins with a critical provocation: Is robot labor labor? More than a rhetorical question, this inquiry invites closer attention to the social and political arrangements that robot labor entails. Drawing on ethnographic fieldwork across two smart-city districts in Seoul, we examine how delivery robot labor is collectively sustained. While robotic actions are often framed as autonomous and efficient, we show that each successful delivery is in fact a distributed sociotechnical achievement--reliant on human labor, regulatory coordination, and social accommodations. We argue that delivery robots do not replace labor but reconfigure it--rendering some forms more visible (robotic performance) while obscuring others (human and institutional support). Unlike industrial robots, delivery robots operate in shared public space, engage everyday passersby, and are embedded in policy and progress narratives. In these spaces, we identify \"robot privilege\"--humans routinely yielding to robots--and distinct perceptions between casual observers (\"cute\") and everyday coexisters (\"admirable\"). We contribute a conceptual reframing of robot labor as a collective assemblage, empirical insights into South Korea's smart-city automation, and a call for HRI to engage more deeply with labor and spatial politics to better theorize public-facing robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u9996\u5c14\u4e24\u4e2a\u667a\u6167\u57ce\u533a\u7684\u6c11\u65cf\u5fd7\u7814\u7a76\uff0c\u63a2\u8ba8\u4e86\u4eba\u884c\u9053\u914d\u9001\u673a\u5668\u4eba\u662f\u5426\u6784\u6210\"\u52b3\u52a8\"\u7684\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u673a\u5668\u4eba\u914d\u9001\u5b9e\u9645\u4e0a\u662f\u4f9d\u8d56\u4eba\u7c7b\u52b3\u52a8\u3001\u76d1\u7ba1\u534f\u8c03\u548c\u793e\u4f1a\u9002\u5e94\u7684\u5206\u5e03\u5f0f\u793e\u4f1a\u6280\u672f\u6210\u5c31\uff0c\u800c\u975e\u7b80\u5355\u7684\u81ea\u52a8\u5316\u66ff\u4ee3\u3002", "motivation": "\u968f\u7740\u4eba\u884c\u9053\u914d\u9001\u673a\u5668\u4eba\u65e5\u76ca\u878d\u5165\u57ce\u5e02\u751f\u6d3b\uff0c\u4f5c\u8005\u63d0\u51fa\u4e00\u4e2a\u6279\u5224\u6027\u95ee\u9898\uff1a\u673a\u5668\u4eba\u52b3\u52a8\u662f\u5426\u7b97\u52b3\u52a8\uff1f\u8fd9\u4e0d\u4ec5\u662f\u4fee\u8f9e\u95ee\u9898\uff0c\u66f4\u662f\u4e3a\u4e86\u5173\u6ce8\u673a\u5668\u4eba\u52b3\u52a8\u6240\u6d89\u53ca\u7684\u793e\u4f1a\u548c\u653f\u6cbb\u5b89\u6392\uff0c\u6311\u6218\u673a\u5668\u4eba\u4f5c\u4e3a\u81ea\u4e3b\u9ad8\u6548\u884c\u52a8\u7684\u5e38\u89c1\u53d9\u4e8b\u3002", "method": "\u91c7\u7528\u6c11\u65cf\u5fd7\u7530\u91ce\u8c03\u67e5\u65b9\u6cd5\uff0c\u5728\u9996\u5c14\u7684\u4e24\u4e2a\u667a\u6167\u57ce\u533a\u8fdb\u884c\u5b9e\u5730\u7814\u7a76\uff0c\u89c2\u5bdf\u548c\u5206\u6790\u914d\u9001\u673a\u5668\u4eba\u5de5\u4f5c\u7684\u5b9e\u9645\u8fd0\u4f5c\u65b9\u5f0f\u53ca\u5176\u4e0e\u793e\u4f1a\u73af\u5883\u7684\u4e92\u52a8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u6210\u529f\u7684\u673a\u5668\u4eba\u914d\u9001\u662f\u5206\u5e03\u5f0f\u793e\u4f1a\u6280\u672f\u6210\u5c31\uff0c\u4f9d\u8d56\u4eba\u7c7b\u52b3\u52a8\u3001\u76d1\u7ba1\u534f\u8c03\u548c\u793e\u4f1a\u9002\u5e94\uff1b2\uff09\u673a\u5668\u4eba\u4e0d\u662f\u66ff\u4ee3\u52b3\u52a8\u800c\u662f\u91cd\u65b0\u914d\u7f6e\u52b3\u52a8\uff0c\u4f7f\u67d0\u4e9b\u5f62\u5f0f\uff08\u673a\u5668\u4eba\u8868\u73b0\uff09\u66f4\u53ef\u89c1\uff0c\u800c\u5176\u4ed6\u5f62\u5f0f\uff08\u4eba\u7c7b\u548c\u673a\u6784\u652f\u6301\uff09\u88ab\u906e\u853d\uff1b3\uff09\u5728\u516c\u5171\u7a7a\u95f4\u4e2d\u89c2\u5bdf\u5230\"\u673a\u5668\u4eba\u7279\u6743\"\u73b0\u8c61\u2014\u2014\u4eba\u7c7b\u7ecf\u5e38\u4e3a\u673a\u5668\u4eba\u8ba9\u8def\uff1b4\uff09\u4e0d\u540c\u4eba\u7fa4\u5bf9\u673a\u5668\u4eba\u6709\u4e0d\u540c\u611f\u77e5\uff1a\u5076\u7136\u89c2\u5bdf\u8005\u8ba4\u4e3a\"\u53ef\u7231\"\uff0c\u65e5\u5e38\u5171\u5b58\u8005\u8ba4\u4e3a\"\u53ef\u656c\"\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u5c06\u673a\u5668\u4eba\u52b3\u52a8\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u96c6\u4f53\u7ec4\u5408\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u97e9\u56fd\u667a\u6167\u57ce\u5e02\u81ea\u52a8\u5316\u7684\u5b9e\u8bc1\u89c1\u89e3\uff0c\u5e76\u547c\u5401\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u66f4\u6df1\u5165\u5730\u53c2\u4e0e\u52b3\u52a8\u548c\u7a7a\u95f4\u653f\u6cbb\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u8bba\u5316\u9762\u5411\u516c\u4f17\u7684\u673a\u5668\u4eba\u3002"}}
{"id": "2602.20217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20217", "abs": "https://arxiv.org/abs/2602.20217", "authors": ["Seongjin Cha", "Gyuwan Kim", "Dongsu Han", "Tao Yang", "Insu Han"], "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem", "comment": null, "summary": "Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework that reformulates draft model selection as a knapsack problem to maximize tokens-per-time throughput. By decoupling Attention and MLP layers and modeling their hardware-specific latencies as functions of context length, KnapSpec adaptively identifies optimal draft configurations on the fly via a parallel dynamic programming algorithm. Furthermore, we provide the first rigorous theoretical analysis establishing cosine similarity between hidden states as a mathematically sound proxy for the token acceptance rate. This foundation allows our method to maintain high drafting faithfulness while navigating the shifting bottlenecks of real-world hardware. Our experiments on Qwen3 and Llama3 demonstrate that KnapSpec consistently outperforms state-of-the-art SSD baselines, achieving up to 1.47x wall-clock speedup across various benchmarks. Our plug-and-play approach ensures high-speed inference for long sequences without requiring additional training or compromising the target model's output distribution.", "AI": {"tldr": "KnapSpec\u901a\u8fc7\u5c06\u8349\u7a3f\u6a21\u578b\u9009\u62e9\u91cd\u6784\u4e3a\u80cc\u5305\u95ee\u9898\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u52a8\u6001\u4f18\u5316\u63a8\u7406\u901f\u5ea6\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u6700\u9ad81.47\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u8ba1\u7b97\u5f00\u9500\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u8349\u7a3f\u6a21\u578b\u9009\u62e9\u673a\u5236\u6765\u6700\u5927\u5316\u63a8\u7406\u541e\u5410\u91cf\u3002", "method": "\u5c06\u8349\u7a3f\u6a21\u578b\u9009\u62e9\u5efa\u6a21\u4e3a\u80cc\u5305\u95ee\u9898\uff0c\u89e3\u8026\u6ce8\u610f\u529b\u5c42\u548cMLP\u5c42\u5e76\u5efa\u6a21\u5176\u786c\u4ef6\u7279\u5b9a\u7684\u5ef6\u8fdf\u51fd\u6570\uff0c\u4f7f\u7528\u5e76\u884c\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\u5728\u7ebf\u8bc6\u522b\u6700\u4f18\u8349\u7a3f\u914d\u7f6e\uff0c\u5e76\u9996\u6b21\u5efa\u7acb\u9690\u85cf\u72b6\u6001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u4ee4\u724c\u63a5\u53d7\u7387\u7684\u7406\u8bba\u5173\u8054\u3002", "result": "\u5728Qwen3\u548cLlama3\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKnapSpec\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u81ea\u63a8\u6d4b\u89e3\u7801\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad81.47\u500d\u7684\u5b9e\u65f6\u52a0\u901f\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6539\u53d8\u76ee\u6807\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u3002", "conclusion": "KnapSpec\u63d0\u4f9b\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u80fd\u591f\u5728\u957f\u5e8f\u5217\u63a8\u7406\u4e2d\u5b9e\u73b0\u9ad8\u901f\u6027\u80fd\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u8349\u7a3f\u6a21\u578b\u914d\u7f6e\u6765\u5e94\u5bf9\u5b9e\u9645\u786c\u4ef6\u4e2d\u7684\u74f6\u9888\u53d8\u5316\u3002"}}
{"id": "2602.20330", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20330", "abs": "https://arxiv.org/abs/2602.20330", "authors": ["Jingcheng Yang", "Tianhu Xiong", "Shengyi Qian", "Klara Nahrstedt", "Mingyuan Wu"], "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking", "comment": "To appear in the Findings of CVPR 2026", "summary": "Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u900f\u660e\u7535\u8def\u8ffd\u8e2a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u5206\u6790\u65b9\u6cd5\u63ed\u793a\u591a\u6a21\u6001\u63a8\u7406\u7684\u5185\u90e8\u673a\u5236", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5f3a\u5927\u4f46\u4ecd\u7136\u662f\u9ed1\u76d2\uff0c\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5176\u591a\u6a21\u6001\u63a8\u7406\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236", "method": "\u4f7f\u7528\u8f6c\u7801\u5668\u3001\u5f52\u56e0\u56fe\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u5f15\u5bfc\u548c\u7535\u8def\u4fee\u8865\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u8ffd\u8e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u7535\u8def", "result": "\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5c42\u6b21\u5316\u65b9\u5f0f\u6574\u5408\u89c6\u89c9\u548c\u8bed\u4e49\u6982\u5ff5\uff0c\u4e0d\u540c\u7684\u89c6\u89c9\u7279\u5f81\u7535\u8def\u53ef\u4ee5\u5904\u7406\u6570\u5b66\u63a8\u7406\u5e76\u652f\u6301\u8de8\u6a21\u6001\u5173\u8054\uff0c\u8fd9\u4e9b\u7535\u8def\u5177\u6709\u56e0\u679c\u6027\u548c\u53ef\u63a7\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u9760\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u5185\u90e8\u7535\u8def\u7684\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u63a7\u5236\u6027"}}
{"id": "2602.20342", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20342", "abs": "https://arxiv.org/abs/2602.20342", "authors": ["Christos Maikos", "Georgios Angelidis", "Georgios Th. Papadopoulos"], "title": "Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques", "comment": "7 pages, 2 figures", "summary": "In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u65e0\u4eba\u673a\u89c6\u9891\u6d41\u5b9e\u65f63D\u91cd\u5efa\u7cfb\u7edf\uff0c\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684AR/VR\u53ef\u89c6\u5316\u5e94\u7528", "motivation": "\u65e0\u4eba\u673a\u5728\u5b9e\u65f6\u611f\u77e5\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5c55\u73b0\u51fa\u5b9e\u65f6\u795e\u7ecf\u6e32\u67d3\u6f5c\u529b\uff0c\u4f46\u5c06\u5176\u6574\u5408\u5230\u7aef\u5230\u7aef\u65e0\u4eba\u673a\u91cd\u5efa\u7cfb\u7edf\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u91c7\u7528\u9ad8\u6548\u67b6\u6784\u6574\u5408RTMP\u76f4\u64ad\u89c6\u9891\u91c7\u96c6\u3001\u4f20\u611f\u5668\u878d\u5408\u540c\u6b65\u3001\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c3DGS\u4f18\u5316\uff0c\u5b9e\u73b0\u8fde\u7eed\u6a21\u578b\u66f4\u65b0\u548c\u4f4e\u5ef6\u8fdf\u90e8\u7f72", "result": "\u76f8\u6bd4NeRF\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u663e\u8457\u66f4\u9ad8\u7684\u6e32\u67d3\u6027\u80fd\u548c\u5927\u5e45\u964d\u4f4e\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u91cd\u5efa\u8d28\u91cf\u4fdd\u6301\u5728\u79bb\u7ebf\u53c2\u8003\u76844-7%\u8303\u56f4\u5185", "conclusion": "\u8be5\u7cfb\u7edf\u9002\u7528\u4e8e\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u7684\u7a7a\u4e2d\u5e73\u53f0\u589e\u5f3a\u611f\u77e5\u5e94\u7528\uff0c\u9a8c\u8bc1\u4e863DGS\u5728\u65e0\u4eba\u673a\u5b9e\u65f6\u91cd\u5efa\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2602.20232", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.20232", "abs": "https://arxiv.org/abs/2602.20232", "authors": ["Luca Thiede", "Abdulrahman Aldossary", "Andreas Burger", "Jorge Arturo Campos-Gonzalez-Angulo", "Ning Wang", "Alexander Zook", "Melisa Alkan", "Kouhei Nakaji", "Taylor Lee Patti", "J\u00e9r\u00f4me Florian Gonthier", "Mohammad Ghazi Vakili", "Al\u00e1n Aspuru-Guzik"], "title": "Coupled Cluster con M\u014dLe: Molecular Orbital Learning for Neural Wavefunctions", "comment": null, "summary": "Density functional theory (DFT) is the most widely used method for calculating molecular properties; however, its accuracy is often insufficient for quantitative predictions. Coupled-cluster (CC) theory is the most successful method for achieving accuracy beyond DFT and for predicting properties that closely align with experiment. It is known as the ''gold standard'' of quantum chemistry. Unfortunately, the high computational cost of CC limits its widespread applicability. In this work, we present the Molecular Orbital Learning (M\u014dLe) architecture, an equivariant machine learning model that directly predicts CC's core mathematical objects, the excitation amplitudes, from the mean-field Hartree-Fock molecular orbitals as inputs. We test various aspects of our model and demonstrate its remarkable data efficiency and out-of-distribution generalization to larger molecules and off-equilibrium geometries, despite being trained only on small equilibrium geometries. Finally, we also examine its ability to reduce the number of cycles required to converge CC calculations. M\u014dLe can set the foundations for high-accuracy wavefunction-based ML architectures to accelerate molecular design and complement force-field approaches.", "AI": {"tldr": "M\u014dLe\u67b6\u6784\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u76f4\u63a5\u4eceHF\u5206\u5b50\u8f68\u9053\u9884\u6d4bCC\u6fc0\u53d1\u632f\u5e45\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u5c0f\u5206\u5b50\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u5927\u5206\u5b50\u548c\u975e\u5e73\u8861\u6784\u578b\u6cdb\u5316", "motivation": "DFT\u8ba1\u7b97\u7cbe\u5ea6\u4e0d\u8db3\uff0cCC\u7406\u8bba\u867d\u4e3a\"\u91d1\u6807\u51c6\"\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u65b9\u6cd5\u964d\u4f4eCC\u8ba1\u7b97\u6210\u672c", "method": "\u63d0\u51faM\u014dLe\u67b6\u6784\uff0c\u4e00\u79cd\u7b49\u53d8\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u5e73\u5747\u573aHartree-Fock\u5206\u5b50\u8f68\u9053\u4f5c\u4e3a\u8f93\u5165\u9884\u6d4bCC\u7684\u6838\u5fc3\u6570\u5b66\u5bf9\u8c61\u2014\u2014\u6fc0\u53d1\u632f\u5e45", "result": "\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ec5\u5728\u5c0f\u5206\u5b50\u5e73\u8861\u6784\u578b\u4e0a\u8bad\u7ec3\u5c31\u80fd\u6cdb\u5316\u5230\u66f4\u5927\u5206\u5b50\u548c\u975e\u5e73\u8861\u6784\u578b\uff0c\u5e76\u80fd\u51cf\u5c11CC\u8ba1\u7b97\u6536\u655b\u6240\u9700\u7684\u5faa\u73af\u6b21\u6570", "conclusion": "M\u014dLe\u4e3a\u57fa\u4e8e\u6ce2\u51fd\u6570\u7684\u9ad8\u7cbe\u5ea6\u673a\u5668\u5b66\u4e60\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u53ef\u52a0\u901f\u5206\u5b50\u8bbe\u8ba1\u5e76\u8865\u5145\u529b\u573a\u65b9\u6cd5"}}
{"id": "2602.20354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20354", "abs": "https://arxiv.org/abs/2602.20354", "authors": ["Bhavik Chandna", "Kelsey R. Allen"], "title": "3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism", "comment": null, "summary": "AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.", "AI": {"tldr": "3DSPA\u662f\u4e00\u4e2a\u81ea\u52a8\u8bc4\u4f30\u89c6\u9891\u771f\u5b9e\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc73D\u65f6\u7a7a\u70b9\u81ea\u7f16\u7801\u5668\u6574\u54083D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u65e0\u9700\u53c2\u8003\u89c6\u9891\u5373\u53ef\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5408\u7406\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524dAI\u89c6\u9891\u751f\u6210\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u771f\u5b9e\u6027\u4ecd\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u6709\u9650\u8303\u56f4\u7684\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u3001\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e863DSPA\uff083D\u65f6\u7a7a\u70b9\u81ea\u7f16\u7801\u5668\uff09\uff0c\u6574\u54083D\u70b9\u8f68\u8ff9\u3001\u6df1\u5ea6\u7ebf\u7d22\u548cDINO\u8bed\u4e49\u7279\u5f81\u5230\u7edf\u4e00\u8868\u793a\u4e2d\uff0c\u5efa\u6a21\u7269\u4f53\u8fd0\u52a8\u548c\u573a\u666f\u5185\u5bb9\uff0c\u8bc4\u4f30\u89c6\u9891\u7684\u771f\u5b9e\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "result": "3DSPA\u80fd\u53ef\u9760\u8bc6\u522b\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\u7684\u89c6\u9891\uff0c\u5bf9\u8fd0\u52a8\u4f2a\u5f71\u66f4\u654f\u611f\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u5bf9\u89c6\u9891\u8d28\u91cf\u548c\u771f\u5b9e\u6027\u7684\u5224\u65ad\u66f4\u4e00\u81f4\u3002", "conclusion": "\u5c063D\u8bed\u4e49\u4fe1\u606f\u878d\u5165\u8f68\u8ff9\u8868\u793a\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840\uff0c\u80fd\u591f\u9690\u5f0f\u6355\u6349\u7269\u7406\u89c4\u5219\u8fdd\u53cd\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2602.21052", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21052", "abs": "https://arxiv.org/abs/2602.21052", "authors": ["Timur Nabiev", "Evgeny Frolov"], "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations", "comment": null, "summary": "Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4d\u7f6e\u6838\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u89e3\u8026\u4f4d\u7f6e\u4fe1\u606f\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u6539\u8fdb\u5e8f\u5217\u5efa\u6a21\u80fd\u529b", "motivation": "\u4f20\u7edf\u7684\u52a0\u6027\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u9879\u76ee\u8bed\u4e49\u7ea0\u7f20\u3001\u5728\u6df1\u5c42\u67b6\u6784\u4e2d\u4f20\u64ad\u5f31\u3001\u96be\u4ee5\u6355\u6349\u4e30\u5bcc\u7684\u5e8f\u5217\u6a21\u5f0f\u3002\u4f5c\u8005\u8ba4\u4e3a\u52a0\u6027\u4f4d\u7f6e\u5d4c\u5165\u53ea\u662f\u8868\u9762\u4e0a\u5bf9\u5e8f\u5217\u987a\u5e8f\u654f\u611f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u673a\u5236\u6765\u5efa\u6a21\u65f6\u95f4\u987a\u5e8f\u3002", "method": "\u5f15\u5165\u6838\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5176\u4e2d\u53ef\u5b66\u4e60\u7684\u4f4d\u7f6e\u6838\u5728\u7eaf\u4f4d\u7f6e\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u4e0e\u8bed\u4e49\u76f8\u4f3c\u6027\u89e3\u8026\uff0c\u76f4\u63a5\u8c03\u8282\u6ce8\u610f\u529b\u6743\u91cd\u3002\u5728\u6bcf\u4e2a\u6ce8\u610f\u529b\u5757\u4e2d\u5e94\u7528\u8be5\u4f4d\u7f6e\u6838\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u5e8f\u5217\u5efa\u6a21\u3002", "result": "\u5728\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u9879\u76ee\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f4d\u7f6e\u6838\u6ce8\u610f\u529b\u673a\u5236\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u4f4d\u7f6e\u4fe1\u606f\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u6838\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u66f4\u6709\u6548\u5730\u5efa\u6a21\u5e8f\u5217\u987a\u5e8f\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u52a0\u6027\u4f4d\u7f6e\u5d4c\u5165\u7684\u5c40\u9650\u6027\uff0c\u5728\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.20273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20273", "abs": "https://arxiv.org/abs/2602.20273", "authors": ["Zhuofan Josh Ying", "Shauli Ravfogel", "Nikolaus Kriegeskorte", "Peter Hase"], "title": "The Truthfulness Spectrum Hypothesis", "comment": "28 pages, 26 figures", "summary": "Large language models (LLMs) have been reported to linearly encode truthfulness, yet recent work questions this finding's generality. We reconcile these views with the truthfulness spectrum hypothesis: the representational space contains directions ranging from broadly domain-general to narrowly domain-specific. To test this hypothesis, we systematically evaluate probe generalization across five truth types (definitional, empirical, logical, fictional, and ethical), sycophantic and expectation-inverted lying, and existing honesty benchmarks. Linear probes generalize well across most domains but fail on sycophantic and expectation-inverted lying. Yet training on all domains jointly recovers strong performance, confirming that domain-general directions exist despite poor pairwise transfer. The geometry of probe directions explains these patterns: Mahalanobis cosine similarity between probes near-perfectly predicts cross-domain generalization (R^2=0.98). Concept-erasure methods further isolate truth directions that are (1) domain-general, (2) domain-specific, or (3) shared only across particular domain subsets. Causal interventions reveal that domain-specific directions steer more effectively than domain-general ones. Finally, post-training reshapes truth geometry, pushing sycophantic lying further from other truth types, suggesting a representational basis for chat models' sycophantic tendencies. Together, our results support the truthfulness spectrum hypothesis: truth directions of varying generality coexist in representational space, with post-training reshaping their geometry. Code for all experiments is provided in https://github.com/zfying/truth_spec.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u771f\u5b9e\u6027\u8868\u5f81\u5b58\u5728\u4ece\u5e7f\u6cdb\u9886\u57df\u901a\u7528\u5230\u72ed\u7a84\u9886\u57df\u7279\u5b9a\u7684\u9891\u8c31\uff0c\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u771f\u5b9e\u6027\u7c7b\u578b\u7684\u8868\u5f81\u7ed3\u6784\u53ca\u5176\u6cdb\u5316\u6a21\u5f0f\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5bf9LLMs\u662f\u5426\u7ebf\u6027\u7f16\u7801\u771f\u5b9e\u6027\u5b58\u5728\u5206\u6b67\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8c03\u548c\u8fd9\u4e9b\u89c2\u70b9\uff0c\u9a8c\u8bc1\u771f\u5b9e\u6027\u9891\u8c31\u5047\u8bf4\uff1a\u8868\u5f81\u7a7a\u95f4\u4e2d\u5b58\u5728\u4ece\u5e7f\u6cdb\u9886\u57df\u901a\u7528\u5230\u72ed\u7a84\u9886\u57df\u7279\u5b9a\u7684\u771f\u5b9e\u6027\u65b9\u5411\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e94\u79cd\u771f\u5b9e\u6027\u7c7b\u578b\uff08\u5b9a\u4e49\u6027\u3001\u7ecf\u9a8c\u6027\u3001\u903b\u8f91\u6027\u3001\u865a\u6784\u6027\u3001\u4f26\u7406\u6027\uff09\u3001\u5949\u627f\u6027\u548c\u671f\u671b\u53cd\u8f6c\u6027\u8bf4\u8c0e\uff0c\u4ee5\u53ca\u73b0\u6709\u8bda\u5b9e\u6027\u57fa\u51c6\uff1b\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u3001\u51e0\u4f55\u5206\u6790\uff08\u9a6c\u6c0f\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff09\u3001\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u548c\u56e0\u679c\u5e72\u9884\u3002", "result": "\u7ebf\u6027\u63a2\u9488\u5728\u5927\u591a\u6570\u9886\u57df\u6cdb\u5316\u826f\u597d\uff0c\u4f46\u5728\u5949\u627f\u6027\u548c\u671f\u671b\u53cd\u8f6c\u6027\u8bf4\u8c0e\u4e0a\u5931\u8d25\uff1b\u8054\u5408\u8bad\u7ec3\u6062\u590d\u5f3a\u6027\u80fd\uff1b\u63a2\u9488\u65b9\u5411\u7684\u51e0\u4f55\u7ed3\u6784\u5b8c\u7f8e\u9884\u6d4b\u8de8\u9886\u57df\u6cdb\u5316\uff1b\u6982\u5ff5\u64e6\u9664\u5206\u79bb\u51fa\u4e09\u79cd\u771f\u5b9e\u6027\u65b9\u5411\uff1b\u56e0\u679c\u5e72\u9884\u663e\u793a\u9886\u57df\u7279\u5b9a\u65b9\u5411\u6bd4\u9886\u57df\u901a\u7528\u65b9\u5411\u66f4\u6709\u6548\uff1b\u540e\u8bad\u7ec3\u91cd\u5851\u771f\u5b9e\u6027\u51e0\u4f55\u7ed3\u6784\u3002", "conclusion": "\u652f\u6301\u771f\u5b9e\u6027\u9891\u8c31\u5047\u8bf4\uff1a\u4e0d\u540c\u901a\u7528\u7a0b\u5ea6\u7684\u771f\u5b9e\u6027\u65b9\u5411\u5728\u8868\u5f81\u7a7a\u95f4\u4e2d\u5e76\u5b58\uff0c\u540e\u8bad\u7ec3\u91cd\u5851\u5176\u51e0\u4f55\u7ed3\u6784\uff0c\u4e3a\u804a\u5929\u6a21\u578b\u7684\u5949\u627f\u503e\u5411\u63d0\u4f9b\u4e86\u8868\u5f81\u57fa\u7840\u3002"}}
{"id": "2602.20558", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.20558", "abs": "https://arxiv.org/abs/2602.20558", "authors": ["Yucheng Shi", "Ying Li", "Yu Wang", "Yesu Feng", "Arjun Rao", "Rein Houthooft", "Shradha Sehgal", "Jin Wang", "Hao Zhen", "Ninghao Liu", "Linas Baltrunas"], "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production", "comment": "Work in progress", "summary": "Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3verbalization agent\uff0c\u5c06\u7ed3\u6784\u5316\u7528\u6237\u4ea4\u4e92\u65e5\u5fd7\u8f6c\u6362\u4e3a\u4f18\u5316\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u5165\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u662fverbalization\u95ee\u9898\u2014\u2014\u5982\u4f55\u5c06\u7ed3\u6784\u5316\u7528\u6237\u4ea4\u4e92\u65e5\u5fd7\u6709\u6548\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u3002\u5f53\u524d\u57fa\u4e8e\u6a21\u677f\u7684\u65b9\u6cd5\u7b80\u5355\u62fc\u63a5\u5b57\u6bb5\uff0c\u65e0\u6cd5\u751f\u6210\u6700\u4f18\u7684\u6587\u672c\u8868\u793a\uff0c\u9650\u5236\u4e86\u63a8\u8350\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684verbalization\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e00\u4e2averbalization agent\u3002\u8be5agent\u5c06\u539f\u59cb\u4ea4\u4e92\u5386\u53f2\u8f6c\u6362\u4e3a\u4f18\u5316\u7684\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u4ee5\u63a8\u8350\u51c6\u786e\u6027\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\u3002agent\u5b66\u4e60\u8fc7\u6ee4\u566a\u58f0\u3001\u6574\u5408\u76f8\u5173\u5143\u6570\u636e\u3001\u91cd\u7ec4\u4fe1\u606f\u4ee5\u63d0\u5347\u4e0b\u6e38\u9884\u6d4b\u3002", "result": "\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u6d41\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5b66\u4e60\u5230\u7684verbalization\u76f8\u6bd4\u57fa\u4e8e\u6a21\u677f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u53d1\u73b0\u9879\u76ee\u63a8\u8350\u51c6\u786e\u6027\u4e0a\u83b7\u5f97\u9ad8\u8fbe93%\u7684\u76f8\u5bf9\u63d0\u5347\u3002\u5206\u6790\u63ed\u793a\u4e86\u6d8c\u73b0\u7684\u7b56\u7565\uff1a\u7528\u6237\u5174\u8da3\u603b\u7ed3\u3001\u566a\u58f0\u53bb\u9664\u3001\u8bed\u6cd5\u89c4\u8303\u5316\u7b49\u3002", "conclusion": "\u5b66\u4e60verbalization\u662f\u63d0\u5347LLM\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u53ef\u4ee5\u81ea\u52a8\u53d1\u73b0\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u6784\u5efa\u7b56\u7565\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.20722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20722", "abs": "https://arxiv.org/abs/2602.20722", "authors": ["Xu Wan", "Yansheng Wang", "Wenqi Huang", "Mingyang Sun"], "title": "Buffer Matters: Unleashing the Power of Off-Policy Reinforcement Learning in Large Language Model Reasoning", "comment": null, "summary": "Traditional on-policy Reinforcement Learning with Verifiable Rewards (RLVR) frameworks suffer from experience waste and reward homogeneity, which directly hinders learning efficiency on difficult samples during large language models post-training. In this paper, we introduce Batch Adaptation Policy Optimization (BAPO), an off-policy RLVR framework to improve the data efficiency in large language models post-training. It dynamically selects training batches by re-evaluating historically difficult samples and reusing high-quality ones, while holding a lower bound guarantee for policy improvement. Extensive experiments further demonstrate that BAPO achieves an average 12.5% improvement over GRPO across mathematics, planning, and visual reasoning tasks. Crucially, BAPO successfully resolves 40.7% of problems that base models consistently fail to solve.", "AI": {"tldr": "BAPO\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u79bb\u7b56\u7565RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6279\u6b21\u3001\u91cd\u65b0\u8bc4\u4f30\u5386\u53f2\u56f0\u96be\u6837\u672c\u548c\u91cd\u7528\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u5728\u6570\u5b66\u3001\u89c4\u5212\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\u5e73\u5747\u6bd4GRPO\u63d0\u534712.5%\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7b56\u7565\u7684RLVR\u6846\u67b6\u5b58\u5728\u7ecf\u9a8c\u6d6a\u8d39\u548c\u5956\u52b1\u540c\u8d28\u5316\u95ee\u9898\uff0c\u8fd9\u76f4\u63a5\u963b\u788d\u4e86\u5728\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u5bf9\u56f0\u96be\u6837\u672c\u7684\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51faBatch Adaptation Policy Optimization (BAPO)\uff0c\u8fd9\u662f\u4e00\u4e2a\u79bb\u7b56\u7565RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6279\u6b21\uff0c\u91cd\u65b0\u8bc4\u4f30\u5386\u53f2\u56f0\u96be\u6837\u672c\u5e76\u91cd\u7528\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u7b56\u7565\u6539\u8fdb\u7684\u4e0b\u754c\u4fdd\u8bc1\u3002", "result": "\u5728\u6570\u5b66\u3001\u89c4\u5212\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cBAPO\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u534712.5%\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u6301\u7eed\u5931\u8d25\u768440.7%\u7684\u95ee\u9898\u3002", "conclusion": "BAPO\u901a\u8fc7\u63d0\u9ad8\u6570\u636e\u6548\u7387\u548c\u6709\u6548\u5904\u7406\u56f0\u96be\u6837\u672c\uff0c\u663e\u8457\u6539\u5584\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2602.20732", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20732", "abs": "https://arxiv.org/abs/2602.20732", "authors": ["Chao Fei", "Guozhong Li", "Chenxi Liu", "Panos Kalnis"], "title": "CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference", "comment": null, "summary": "Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local semantics, which undermines quality. Moreover, their irregular accesses and selection overheads yield only limited wall-clock speedups. To address this, we propose \\textbf{CHESS}, an \\textit{algorithm-system co-design} KV-cache management system. Algorithmically, CHESS introduces a context-aware, hierarchical selection policy that dynamically reconstructs a coherent context for the current decoding. System-wise, coarse granularity selection eliminates expensive data movement, fully realizing practical acceleration from theoretical sparsity. Extensive evaluations demonstrate that CHESS surpasses Full-KV quality using only \\textbf{1\\%} of the KV cache, delivers low-latency stable inference with up to \\textbf{4.56$\\times$} higher throughput, and consistently outperforms other strong baselines. Code is available at \\href{https://anonymous.4open.science/r/CHESS-9958/}{https://anonymous.4open.science/r/CHESS/}.", "AI": {"tldr": "CHESS\u662f\u4e00\u79cd\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u7684KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u4ec5\u4f7f\u75281%\u7684KV\u7f13\u5b58\u5c31\u80fd\u8d85\u8d8aFull-KV\u8d28\u91cf\uff0c\u5b9e\u73b0\u9ad8\u8fbe4.56\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587LLM\u9700\u8981\u4f4e\u5ef6\u8fdf\u7684\u51c6\u786e\u63a8\u7406\uff0c\u4f46\u968f\u7740\u4e0a\u4e0b\u6587\u589e\u957f\uff0c\u89e3\u7801\u4e3b\u8981\u53d7KV\u7f13\u5b58\u7ea6\u675f\u3002\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5927\u591a\u662f\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\uff1a\u5b83\u4eec\u7684token\u9009\u62e9\u5ffd\u7565\u4e86\u9010\u6b65\u76f8\u5173\u6027\u548c\u5c40\u90e8\u8bed\u4e49\uff0c\u8fd9\u635f\u5bb3\u4e86\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u7684\u4e0d\u89c4\u5219\u8bbf\u95ee\u548c\u9009\u62e9\u5f00\u9500\u4ec5\u5e26\u6765\u6709\u9650\u7684\u65f6\u949f\u52a0\u901f\u3002", "method": "CHESS\u91c7\u7528\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u3002\u7b97\u6cd5\u4e0a\uff0c\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5206\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u52a8\u6001\u91cd\u5efa\u5f53\u524d\u89e3\u7801\u7684\u8fde\u8d2f\u4e0a\u4e0b\u6587\u3002\u7cfb\u7edf\u4e0a\uff0c\u7c97\u7c92\u5ea6\u9009\u62e9\u6d88\u9664\u4e86\u6602\u8d35\u7684\u6570\u636e\u79fb\u52a8\uff0c\u5145\u5206\u5b9e\u73b0\u7406\u8bba\u7a00\u758f\u6027\u7684\u5b9e\u9645\u52a0\u901f\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cCHESS\u4ec5\u4f7f\u75281%\u7684KV\u7f13\u5b58\u5c31\u80fd\u8d85\u8d8aFull-KV\u8d28\u91cf\uff0c\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7a33\u5b9a\u63a8\u7406\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe4.56\u500d\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u5f3a\u57fa\u7ebf\u3002", "conclusion": "CHESS\u901a\u8fc7\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u4e2dKV\u7f13\u5b58\u7ba1\u7406\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u63a8\u7406\u52a0\u901f\u3002"}}
{"id": "2602.20739", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20739", "abs": "https://arxiv.org/abs/2602.20739", "authors": ["Shitian Zhao", "Shaoheng Lin", "Ming Li", "Haoquan Zhang", "Wenshuo Peng", "Kaipeng Zhang", "Chen Wei"], "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "comment": "preprint", "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86PyVision-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u4ea4\u4e92\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u8fc7\u91c7\u6837-\u8fc7\u6ee4-\u6392\u5e8f\u7b56\u7565\u548c\u7d2f\u79ef\u5de5\u5177\u5956\u52b1\u6765\u7ef4\u6301\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\uff0c\u5f00\u53d1\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u6a21\u578b\uff0c\u89c6\u9891\u63a8\u7406\u91c7\u7528\u6309\u9700\u4e0a\u4e0b\u6587\u6784\u5efa\u51cf\u5c11\u89c6\u89c9token\u4f7f\u7528\u3002", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7ecf\u5e38\u51fa\u73b0\u4ea4\u4e92\u5d29\u6e83\u95ee\u9898\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u51cf\u5c11\u5de5\u5177\u4f7f\u7528\u548c\u591a\u8f6e\u63a8\u7406\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u4f18\u52bf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7a33\u5b9a\u8bad\u7ec3\u5e76\u7ef4\u6301\u4ea4\u4e92\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faPyVision-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u8fc7\u91c7\u6837-\u8fc7\u6ee4-\u6392\u5e8f\u7684rollout\u7b56\u7565\u548c\u7d2f\u79ef\u5de5\u5177\u5956\u52b1\u6765\u9632\u6b62\u4ea4\u4e92\u5d29\u6e83\uff1b\u5f00\u53d1\u7edf\u4e00\u8bad\u7ec3\u6d41\u7a0b\uff0c\u521b\u5efaPyVision-Image\u548cPyVision-Video\u6a21\u578b\uff1b\u89c6\u9891\u63a8\u7406\u91c7\u7528\u6309\u9700\u4e0a\u4e0b\u6587\u6784\u5efa\uff0c\u9009\u62e9\u6027\u91c7\u6837\u4efb\u52a1\u76f8\u5173\u5e27\u4ee5\u51cf\u5c11\u89c6\u89c9token\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPyVision\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u548c\u6539\u8fdb\u7684\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u6301\u7eed\u4ea4\u4e92\u548c\u6309\u9700\u89c6\u89c9\u5904\u7406\u5bf9\u4e8e\u53ef\u6269\u5c55\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6027\u3002", "conclusion": "PyVision-RL\u6846\u67b6\u901a\u8fc7\u9632\u6b62\u4ea4\u4e92\u5d29\u6e83\u548c\u9f13\u52b1\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\uff0c\u7ed3\u5408\u6309\u9700\u89c6\u89c9\u5904\u7406\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20556", "abs": "https://arxiv.org/abs/2602.20556", "authors": ["Hanhui Li", "Xuan Huang", "Wanquan Liu", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Xiaodan Liang", "Chenqiang Gao"], "title": "WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos", "comment": null, "summary": "Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\\%$ relative gain in PSNR and a $23.1\\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.", "AI": {"tldr": "WildGHand\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6270\u52a8\u89e3\u8026\u6a21\u5757\u548c\u6270\u52a8\u611f\u77e5\u4f18\u5316\u7b56\u7565\uff0c\u5728\u91ce\u5916\u89c6\u9891\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e943D\u9ad8\u65af\u6e85\u5c04\uff0c\u91cd\u5efa\u9ad8\u4fdd\u771f\u624b\u90e8\u5316\u8eab\u3002", "motivation": "\u73b0\u67093D\u624b\u90e8\u91cd\u5efa\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u53d7\u63a7\u73af\u5883\u6570\u636e\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5b58\u5728\u624b\u7269\u4ea4\u4e92\u3001\u6781\u7aef\u59ff\u6001\u3001\u5149\u7167\u53d8\u5316\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u6270\u52a8\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u91ce\u5916\u89c6\u9891\u4e2d\u7684\u9c81\u68d2\u91cd\u5efa\u95ee\u9898\u3002", "method": "1) \u52a8\u6001\u6270\u52a8\u89e3\u8026\u6a21\u5757\uff1a\u5c06\u6270\u52a8\u8868\u793a\u4e3a3D\u9ad8\u65af\u5c5e\u6027\u4e0a\u7684\u65f6\u53d8\u504f\u5dee\uff1b2) \u6270\u52a8\u611f\u77e5\u4f18\u5316\u7b56\u7565\uff1a\u751f\u6210\u6bcf\u5e27\u5404\u5411\u5f02\u6027\u52a0\u6743\u63a9\u7801\u6307\u5bfc\u4f18\u5316\uff1b\u4e24\u8005\u7ed3\u5408\u5728\u65f6\u7a7a\u7ef4\u5ea6\u8bc6\u522b\u548c\u6291\u5236\u6270\u52a8\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWildGHand\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u5728PSNR\u4e0a\u76f8\u5bf9\u63d0\u534715.8%\uff0cLPIPS\u76f8\u5bf9\u51cf\u5c1123.1%\u3002", "conclusion": "WildGHand\u901a\u8fc7\u6270\u52a8\u89e3\u8026\u548c\u611f\u77e5\u4f18\u5316\uff0c\u6709\u6548\u5904\u7406\u91ce\u5916\u89c6\u9891\u4e2d\u7684\u5404\u79cd\u6270\u52a8\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u9ad8\u8d28\u91cf3D\u624b\u90e8\u5316\u8eab\u91cd\u5efa\u3002"}}
{"id": "2602.20577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20577", "abs": "https://arxiv.org/abs/2602.20577", "authors": ["Jiaru Zhang", "Manav Gagvani", "Can Cui", "Juntong Peng", "Ruqi Zhang", "Ziran Wang"], "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion", "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.", "AI": {"tldr": "MVLAD-AD\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6269\u6563\u6a21\u578b\uff0c\u5728\u9ad8\u6548\u89c4\u5212\u548c\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM/VLM\u6a21\u578b\u5728\u63a8\u7406\u5ef6\u8fdf\u3001\u52a8\u4f5c\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709LLM\u548cVLM\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u9762\u4e34\u63a8\u7406\u5ef6\u8fdf\u3001\u52a8\u4f5c\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u4e09\u5927\u6311\u6218\u3002\u81ea\u56de\u5f52\u65b9\u6cd5\u5b58\u5728\u9010token\u751f\u6210\u7684\u6162\u901f\u95ee\u9898\uff0c\u800c\u5148\u524d\u7684\u6269\u6563\u89c4\u5212\u5668\u4f9d\u8d56\u5197\u957f\u3001\u901a\u7528\u7684\u8bed\u8a00token\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u63d0\u51faMVLAD-AD\u6846\u67b6\uff1a1\uff09\u5f15\u5165\u79bb\u6563\u52a8\u4f5c\u6807\u8bb0\u5316\u7b56\u7565\uff0c\u4ece\u771f\u5b9e\u9a7e\u9a76\u5206\u5e03\u6784\u5efa\u7d27\u51d1\u7684\u8fd0\u52a8\u5b66\u53ef\u884c\u8def\u5f84\u70b9\u7801\u672c\uff1b2\uff09\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u5d4c\u5165\u5b66\u4e60\uff0c\u786e\u4fdd\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165\u8fd1\u4f3c\u7269\u7406\u51e0\u4f55\u5ea6\u91cf\uff1b3\uff09\u5f15\u5165\u52a8\u4f5c\u4f18\u5148\u89e3\u7801\u7b56\u7565\uff0c\u4f18\u5148\u751f\u6210\u8f68\u8ff9\u3002", "result": "\u5728nuScenes\u53ca\u5176\u884d\u751f\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMVLAD-AD\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6548\u7387\uff0c\u5728\u89c4\u5212\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u81ea\u56de\u5f52\u548c\u6269\u6563\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u9ad8\u4fdd\u771f\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002", "conclusion": "MVLAD-AD\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709LLM/VLM\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u6548\u7387\u3001\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u52a8\u4f5c\u6807\u8bb0\u5316\u3001\u51e0\u4f55\u611f\u77e5\u5d4c\u5165\u548c\u52a8\u4f5c\u4f18\u5148\u89e3\u7801\u7b56\u7565\uff0c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20583", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20583", "abs": "https://arxiv.org/abs/2602.20583", "authors": ["Wonyong Seo", "Jaeho Moon", "Jaehyup Lee", "Soo Ye Kim", "Munchurl Kim"], "title": "PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models", "comment": "The first two authors contributed equally to this work (equal contribution)", "summary": "Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.", "AI": {"tldr": "PropFly\uff1a\u65e0\u9700\u914d\u5bf9\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5728\u7ebf\u76d1\u7763\u7684\u89c6\u9891\u4f20\u64ad\u7f16\u8f91\u8bad\u7ec3\u6846\u67b6", "motivation": "\u57fa\u4e8e\u4f20\u64ad\u7684\u89c6\u9891\u7f16\u8f91\u9700\u8981\u5927\u89c4\u6a21\u914d\u5bf9\u7684\u6e90\u89c6\u9891\u548c\u7f16\u8f91\u540e\u89c6\u9891\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u96c6\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u9884\u8ba1\u7b97\u914d\u5bf9\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPropFly\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5728\u7ebf\u76d1\u7763\u3002\u901a\u8fc7\u4e0d\u540cCFG\u5c3a\u5ea6\u7684\u5355\u6b65\u5e72\u51c0\u6f5c\u5728\u4f30\u8ba1\uff0c\u5728\u7ebf\u5408\u6210\"\u6e90\"\uff08\u4f4eCFG\uff09\u548c\"\u7f16\u8f91\"\uff08\u9ad8CFG\uff09\u6f5c\u5728\u5bf9\u3002\u6e90\u6f5c\u5728\u63d0\u4f9b\u89c6\u9891\u7ed3\u6784\u4fe1\u606f\uff0c\u7f16\u8f91\u6f5c\u5728\u63d0\u4f9b\u76ee\u6807\u53d8\u6362\u3002\u901a\u8fc7\u9644\u52a0\u9002\u914d\u5668\u5e76\u4f7f\u7528\u5f15\u5bfc\u8c03\u5236\u6d41\u5339\u914d\u635f\u5931\u5b66\u4e60\u4f20\u64ad\u7f16\u8f91\u3002", "result": "PropFly\u5728\u591a\u79cd\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7f16\u8f91\u7ed3\u679c\uff0c\u5b66\u4e60\u5230\u65f6\u95f4\u4e00\u81f4\u4e14\u52a8\u6001\u7684\u53d8\u6362\u3002", "conclusion": "PropFly\u901a\u8fc7\u5728\u7ebf\u76d1\u7763\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u4f20\u64ad\u7684\u89c6\u9891\u7f16\u8f91\u8bad\u7ec3\u4e2d\u914d\u5bf9\u6570\u636e\u96c6\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.20608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20608", "abs": "https://arxiv.org/abs/2602.20608", "authors": ["Aihua Mao", "Kaihang Huang", "Yong-Jin Liu", "Chee Seng Chan", "Ying He"], "title": "VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos", "comment": null, "summary": "3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u9891\u5f15\u5bfc\u76843D\u7269\u4f53\u53ef\u4f9b\u6027\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u5e8f\u5217\u800c\u975e\u9759\u6001\u89c6\u89c9/\u6587\u672c\u7ebf\u7d22\u6765\u8bc6\u522b\u652f\u6301\u4eba-\u7269\u4ea4\u4e92\u7684\u533a\u57df\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9a\u4f4d\u771f\u5b9e\u63a5\u89e6\u533a\u57df\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u7269\u4f53\u53ef\u4f9b\u6027\u5b9a\u4f4d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u6216\u6587\u672c\u7ebf\u7d22\uff0c\u4f46\u53ef\u4f9b\u6027\u672c\u8d28\u4e0a\u662f\u52a8\u6001\u52a8\u4f5c\u5b9a\u4e49\u7684\uff0c\u5bfc\u81f4\u96be\u4ee5\u51c6\u786e\u5b9a\u4f4d\u771f\u5b9e\u4ea4\u4e92\u4e2d\u7684\u63a5\u89e6\u533a\u57df\u3002\u4eba\u7c7b\u901a\u8fc7\u89c2\u5bdf\u548c\u6a21\u4eff\u52a8\u4f5c\u5b66\u4e60\u4f7f\u7528\u7269\u4f53\uff0c\u800c\u975e\u4ec5\u901a\u8fc7\u5f62\u72b6\u68c0\u67e5\uff0c\u8fd9\u542f\u53d1\u4e86\u4f7f\u7528\u52a8\u6001\u4ea4\u4e92\u5e8f\u5217\u63d0\u4f9b\u529f\u80fd\u76d1\u7763\u7684\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51faVAGNet\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u884d\u751f\u7684\u4ea4\u4e92\u7ebf\u7d22\u4e0e3D\u7ed3\u6784\u5bf9\u9f50\uff0c\u89e3\u51b3\u9759\u6001\u7ebf\u7d22\u65e0\u6cd5\u5904\u7406\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002\u540c\u65f6\u6784\u5efa\u4e86\u9996\u4e2aHOI\u89c6\u9891-3D\u914d\u5bf9\u53ef\u4f9b\u6027\u6570\u636e\u96c6PVAD\uff0c\u63d0\u4f9b\u5148\u524d\u5de5\u4f5c\u4e2d\u7f3a\u4e4f\u7684\u529f\u80fd\u76d1\u7763\u3002", "result": "\u5728PVAD\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVAGNet\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u9759\u6001\u7ebf\u7d22\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u89c6\u9891\u5f15\u5bfc\u76843D\u53ef\u4f9b\u6027\u5b9a\u4f4d\u65b0\u8303\u5f0f\uff0c\u5229\u7528\u52a8\u6001\u4ea4\u4e92\u5e8f\u5217\u63d0\u4f9b\u529f\u80fd\u76d1\u7763\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u771f\u5b9e\u4ea4\u4e92\u4e2d\u7684\u63a5\u89e6\u533a\u57df\uff0c\u4e3a\u5177\u8eab\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.21143", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21143", "abs": "https://arxiv.org/abs/2602.21143", "authors": ["Debjit Paul", "Daniel Murphy", "Milan Gritta", "Ronald Cardenas", "Victor Prokhorov", "Lena Sophia Bolliger", "Aysim Toker", "Roy Miles", "Andreea-Maria Oncescu", "Jasivan Alex Sivakumar", "Philipp Borchert", "Ismail Elezi", "Meiru Zhang", "Ka Yiu Lee", "Guchun Zhang", "Jun Wang", "Gerasimos Lampouras"], "title": "A Benchmark for Deep Information Synthesis", "comment": "Accepted at ICLR 2026", "summary": "Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.", "AI": {"tldr": "DEEPSYNTH\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b120\u4e2a\u8de87\u4e2a\u9886\u57df\u300167\u4e2a\u56fd\u5bb6\u7684\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4fe1\u606f\u6536\u96c6\u3001\u5408\u6210\u548c\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u4ece\u591a\u4e2a\u6765\u6e90\u5408\u6210\u4fe1\u606f\u5e76\u8fdb\u884c\u8d85\u8d8a\u7b80\u5355\u4e8b\u5b9e\u68c0\u7d22\u7684\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u6784\u5efaDEEPSYNTH\u57fa\u51c6\uff1a\u6536\u96c6\u5b98\u65b9\u6570\u636e\u6e90\u3001\u521b\u5efa\u5047\u8bbe\u3001\u8fdb\u884c\u624b\u52a8\u5206\u6790\u3001\u8bbe\u8ba1\u5177\u6709\u53ef\u9a8c\u8bc1\u7b54\u6848\u7684\u4efb\u52a1\u3002\u5305\u542b120\u4e2a\u4efb\u52a1\uff0c\u8986\u76d67\u4e2a\u9886\u57df\u548c67\u4e2a\u56fd\u5bb6\u7684\u6570\u636e\u6e90\u3002", "result": "11\u4e2a\u6700\u5148\u8fdb\u7684LLM\u548c\u6df1\u5ea6\u7814\u7a76\u667a\u80fd\u4f53\u5728DEEPSYNTH\u4e0a\u7684\u6700\u5927F1\u5206\u6570\u4ec5\u4e3a8.97\u548c17.5\uff08\u57fa\u4e8eLLM-judge\u6307\u6807\uff09\uff0c\u8868\u660e\u57fa\u51c6\u96be\u5ea6\u5f88\u9ad8\u3002\u5206\u6790\u663e\u793a\u5f53\u524d\u667a\u80fd\u4f53\u5728\u5904\u7406\u5e7b\u89c9\u548c\u5927\u578b\u4fe1\u606f\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "DEEPSYNTH\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u57fa\u51c6\uff0c\u7a81\u663e\u4e86\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u8bc4\u4f30\u65b9\u5411\u3002"}}
{"id": "2602.21172", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21172", "abs": "https://arxiv.org/abs/2602.21172", "authors": ["Ishaan Rawal", "Shubh Gupta", "Yihan Hu", "Wei Zhan"], "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning", "comment": "Accepted to CVPR 2026", "summary": "Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \\modelname (\\textbf{No} \\textbf{R}easoning for \\textbf{D}riving). Compared to existing VLAs, \\modelname achieves competitive performance while being fine-tuned on $<$60\\% of the data and no reasoning annotations, resulting in 3$\\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \\modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \\modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.", "AI": {"tldr": "NoRD\u6a21\u578b\u901a\u8fc7\u6d88\u9664\u63a8\u7406\u6807\u6ce8\u9700\u6c42\uff0c\u5728\u5c11\u4e8e60%\u7684\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709VLA\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e863\u500dtoken\u4f7f\u7528\uff0c\u5e76\u91c7\u7528Dr. GRPO\u7b97\u6cd5\u514b\u670d\u96be\u5ea6\u504f\u5dee\u95ee\u9898", "motivation": "\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u9762\u4e34\u4e24\u5927\u6602\u8d35\u9700\u6c42\uff1a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6536\u96c6\u548c\u5bc6\u96c6\u63a8\u7406\u6807\u6ce8\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027", "method": "\u63d0\u51faNoRD\u6a21\u578b\uff0c\u4f7f\u7528\u5c11\u4e8e60%\u7684\u8bad\u7ec3\u6570\u636e\u4e14\u65e0\u9700\u63a8\u7406\u6807\u6ce8\uff0c\u91c7\u7528Dr. GRPO\u7b97\u6cd5\u6765\u7f13\u89e3\u6807\u51c6GRPO\u5728\u5c0f\u578b\u65e0\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u9047\u5230\u7684\u96be\u5ea6\u504f\u5dee\u95ee\u9898", "result": "\u5728Waymo\u548cNAVSIM\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709VLA\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u5f00\u9500", "conclusion": "NoRD\u6a21\u578b\u901a\u8fc7\u6d88\u9664\u63a8\u7406\u6807\u6ce8\u9700\u6c42\u5e76\u6709\u6548\u5904\u7406\u96be\u5ea6\u504f\u5dee\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.20658", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20658", "abs": "https://arxiv.org/abs/2602.20658", "authors": ["Mohammad Sadra Rajabi", "Aanuoluwapo Ojelade", "Sunwook Kim", "Maury A. Nussbaum"], "title": "Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video", "comment": null, "summary": "Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4eceRGB\u89c6\u9891\u6d41\u4e2d\u975e\u4fb5\u5165\u5f0f\u4f30\u8ba1NIOSH\u4e3e\u91cd\u65b9\u7a0b\u4e2d\u6c34\u5e73\u548c\u5782\u76f4\u624b\u90e8\u8ddd\u79bb\u7684\u53ef\u884c\u6027\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u591a\u9636\u6bb5VLM\u7ba1\u9053\uff0c\u5176\u4e2d\u57fa\u4e8e\u5206\u5272\u7684\u591a\u89c6\u56fe\u7ba1\u9053\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u624b\u52a8\u4e3e\u91cd\u4efb\u52a1\u662f\u804c\u4e1a\u6027\u808c\u8089\u9aa8\u9abc\u75be\u75c5\u7684\u4e3b\u8981\u8bf1\u56e0\uff0c\u800c\u4fee\u8ba2\u7248NIOSH\u4e3e\u91cd\u65b9\u7a0b\uff08RNLE\uff09\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u5de5\u6548\u5b66\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u5176\u6240\u9700\u7684\u6c34\u5e73\u548c\u5782\u76f4\u624b\u90e8\u8ddd\u79bb\u53c2\u6570\u901a\u5e38\u9700\u8981\u901a\u8fc7\u624b\u52a8\u6d4b\u91cf\u6216\u4e13\u7528\u4f20\u611f\u7cfb\u7edf\u83b7\u53d6\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u5de5\u4f5c\u73af\u5883\u4e2d\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u591a\u9636\u6bb5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7ba1\u9053\uff1a1\uff09\u6587\u672c\u5f15\u5bfc\u7684\u4ec5\u68c0\u6d4b\u7ba1\u9053\uff1b2\uff09\u68c0\u6d4b\u52a0\u5206\u5272\u7ba1\u9053\u3002\u4e24\u79cd\u7ba1\u9053\u90fd\u4f7f\u7528\u6587\u672c\u5f15\u5bfc\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u611f\u5174\u8da3\u533a\u57df\uff0c\u4ece\u8fd9\u4e9b\u533a\u57df\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u57fa\u4e8etransformer\u7684\u65f6\u95f4\u56de\u5f52\u6765\u4f30\u8ba1\u4e3e\u91cd\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u7684\u6c34\u5e73\u548c\u5782\u76f4\u8ddd\u79bb\u3002", "result": "\u57fa\u4e8e\u5206\u5272\u7684\u591a\u89c6\u56fe\u7ba1\u9053\u8868\u73b0\u6700\u4f73\uff0c\u4f30\u8ba1\u6c34\u5e73\u8ddd\u79bb\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u7ea6\u4e3a6-8\u5398\u7c73\uff0c\u5782\u76f4\u8ddd\u79bb\u7ea6\u4e3a5-8\u5398\u7c73\u3002\u5728\u6240\u6709\u7ba1\u9053\u548c\u76f8\u673a\u89c6\u56fe\u914d\u7f6e\u4e2d\uff0c\u50cf\u7d20\u7ea7\u5206\u5272\u5c06\u6c34\u5e73\u8ddd\u79bb\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e86\u7ea620-30%\uff0c\u5782\u76f4\u8ddd\u79bb\u964d\u4f4e\u4e86\u7ea635-40%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ba1\u9053\u7528\u4e8e\u89c6\u9891\u4f30\u8ba1RNLE\u8ddd\u79bb\u53c2\u6570\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u9645\u5de5\u4f5c\u73af\u5883\u4e2d\u7684\u975e\u4fb5\u5165\u5f0f\u5de5\u6548\u5b66\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.20664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20664", "abs": "https://arxiv.org/abs/2602.20664", "authors": ["Hailong Yan", "Shice Liu", "Tao Wang", "Xiangtao Zhang", "Yijie Zhong", "Jinwei Chen", "Le Zhang", "Bo Li"], "title": "AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?", "comment": "Tech Report", "summary": "Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to \"copy-paste\" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's \"Combination of Straight Ahead and Pose to Pose\" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.", "AI": {"tldr": "AnimeAgent\uff1a\u9996\u4e2a\u57fa\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u5236\u6545\u4e8b\u677f\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u9759\u6001\u6269\u6563\u6a21\u578b\u5728\u52a8\u6001\u8868\u73b0\u529b\u3001\u8fed\u4ee3\u4fee\u6b63\u548c\u8bc4\u4f30\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9759\u6001\u6269\u6563\u6a21\u578b\u7684\u6545\u4e8b\u677f\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u9759\u6001\u6a21\u578b\u7f3a\u4e4f\u52a8\u6001\u8868\u73b0\u529b\uff0c\u5e38\u91c7\u7528\"\u590d\u5236\u7c98\u8d34\"\u6a21\u5f0f\uff1b2) \u4e00\u6b21\u6027\u63a8\u7406\u65e0\u6cd5\u8fed\u4ee3\u4fee\u6b63\u7f3a\u5931\u5c5e\u6027\u6216\u63d0\u793a\u8bcd\u9075\u5faa\u4e0d\u4f73\u7684\u95ee\u9898\uff1b3) \u591a\u667a\u80fd\u4f53\u6846\u67b6\u4f9d\u8d56\u4e0d\u7a33\u5065\u7684\u8bc4\u4f30\u5668\uff0c\u4e0d\u9002\u5408\u8bc4\u4f30\u98ce\u683c\u5316\u3001\u975e\u5199\u5b9e\u7684\u52a8\u753b\u3002", "method": "\u63d0\u51faAnimeAgent\u6846\u67b6\uff0c\u53d7\u8fea\u58eb\u5c3c\"\u76f4\u8fdb\u4e0e\u59ff\u52bf\u7ed3\u5408\"\u5de5\u4f5c\u6d41\u7a0b\u542f\u53d1\uff0c\u5229\u7528\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u7684\u9690\u5f0f\u8fd0\u52a8\u5148\u9a8c\u6765\u589e\u5f3a\u4e00\u81f4\u6027\u548c\u8868\u73b0\u529b\uff0c\u540c\u65f6\u91c7\u7528\u6df7\u5408\u4e3b\u89c2-\u5ba2\u89c2\u7684\u8bc4\u5ba1\u673a\u5236\u5b9e\u73b0\u53ef\u9760\u7684\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAnimeAgent\u5728\u4e00\u81f4\u6027\u3001\u63d0\u793a\u8bcd\u5fe0\u5b9e\u5ea6\u548c\u98ce\u683c\u5316\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u6536\u96c6\u4e86\u5e26\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9e\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "AnimeAgent\u662f\u9996\u4e2a\u57fa\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u52a8\u6001\u8fd0\u52a8\u5148\u9a8c\u548c\u6df7\u5408\u8bc4\u5ba1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b9a\u5236\u6545\u4e8b\u677f\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2602.20574", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20574", "abs": "https://arxiv.org/abs/2602.20574", "authors": ["Alex Stein", "Furong Huang", "Tom Goldstein"], "title": "GATES: Self-Distillation under Privileged Context with Consensus Gating", "comment": "10 Pages of main text with an additional 7 pages of supplementary material", "summary": "We study self-distillation in settings where supervision is unreliable: there are no ground truth labels, verifiable rewards, or external graders to evaluate answers. We focus on document-grounded question answering with asymmetric context, where a single model serves as both tutor (with access to a relevant source document during training) and student (answering from the question alone at test time). Rather than assuming tutor correctness, we derive supervision online from tutor consensus by sampling multiple document-grounded reasoning traces and using agreement to gate learning. Conditioned on this reliability signal, we distill knowledge through full tutor reasoning trajectories (not just final answers), providing a dense and stable learning signal. Empirically, this consensus-gated trajectory distillation substantially improves transfer to the document-free student. Held-out in-domain accuracy under asymmetric evaluation improves from 46.0\\% to 62.0\\%, and average (maj@8) accuracy on public document-free math benchmarks improves from 20.2\\% to 35.4\\%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u65e0\u76d1\u7763\u73af\u5883\u4e0b\u901a\u8fc7\u5bfc\u5e08\u5171\u8bc6\u95e8\u63a7\u8f68\u8ff9\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u6863\u95ee\u7b54\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u76d1\u7763\u4e0d\u53ef\u9760\uff08\u65e0\u771f\u5b9e\u6807\u7b7e\u3001\u53ef\u9a8c\u8bc1\u5956\u52b1\u6216\u5916\u90e8\u8bc4\u5206\u8005\uff09\u60c5\u51b5\u4e0b\u7684\u81ea\u84b8\u998f\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u5bfc\u5e08\u6a21\u578b\uff08\u8bad\u7ec3\u65f6\u53ef\u8bbf\u95ee\u6587\u6863\uff09\u548c\u5b66\u751f\u6a21\u578b\uff08\u6d4b\u8bd5\u65f6\u4ec5\u57fa\u4e8e\u95ee\u9898\u56de\u7b54\uff09\u4e4b\u95f4\u5b58\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u3002", "method": "\u63d0\u51fa\u5171\u8bc6\u95e8\u63a7\u8f68\u8ff9\u84b8\u998f\u65b9\u6cd5\uff1a\u901a\u8fc7\u91c7\u6837\u591a\u4e2a\u57fa\u4e8e\u6587\u6863\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u5229\u7528\u5bfc\u5e08\u95f4\u7684\u4e00\u81f4\u6027\u4f5c\u4e3a\u53ef\u9760\u6027\u4fe1\u53f7\u6765\u95e8\u63a7\u5b66\u4e60\uff1b\u57fa\u4e8e\u6b64\u4fe1\u53f7\uff0c\u901a\u8fc7\u5b8c\u6574\u7684\u5bfc\u5e08\u63a8\u7406\u8f68\u8ff9\uff08\u800c\u975e\u4ec5\u6700\u7ec8\u7b54\u6848\uff09\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u63d0\u4f9b\u5bc6\u96c6\u4e14\u7a33\u5b9a\u7684\u5b66\u4e60\u4fe1\u53f7\u3002", "result": "\u5728\u975e\u5bf9\u79f0\u8bc4\u4f30\u4e0b\uff0c\u9886\u57df\u5185\u51c6\u786e\u7387\u4ece46.0%\u63d0\u5347\u81f362.0%\uff1b\u5728\u516c\u5f00\u7684\u65e0\u6587\u6863\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u4ece20.2%\u63d0\u5347\u81f335.4%\u3002", "conclusion": "\u5171\u8bc6\u95e8\u63a7\u8f68\u8ff9\u84b8\u998f\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u4e0d\u53ef\u9760\u76d1\u7763\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4ece\u5bfc\u5e08\u5230\u5b66\u751f\u7684\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u4fe1\u606f\u4e0d\u5bf9\u79f0\u7684\u6587\u6863\u95ee\u7b54\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2602.20593", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.20593", "abs": "https://arxiv.org/abs/2602.20593", "authors": ["Yige Liu", "Yiwei Lou", "Che Wang", "Yongzhi Cao", "Hanpin Wang"], "title": "Is the Trigger Essential? A Feature-Based Triggerless Backdoor Attack in Vertical Federated Learning", "comment": null, "summary": "As a distributed collaborative machine learning paradigm, vertical federated learning (VFL) allows multiple passive parties with distinct features and one active party with labels to collaboratively train a model. Although it is known for the privacy-preserving capabilities, VFL still faces significant privacy and security threats from backdoor attacks. Existing backdoor attacks typically involve an attacker implanting a trigger into the model during the training phase and executing the attack by adding the trigger to the samples during the inference phase. However, in this paper, we find that triggers are not essential for backdoor attacks in VFL. In light of this, we disclose a new backdoor attack pathway in VFL by introducing a feature-based triggerless backdoor attack. This attack operates under a more stringent security assumption, where the attacker is honest-but-curious rather than malicious during the training phase. It comprises three modules: label inference for the targeted backdoor attack, poison generation with amplification and perturbation mechanisms, and backdoor execution to implement the attack. Extensive experiments on five benchmark datasets demonstrate that our attack outperforms three baseline backdoor attacks by 2 to 50 times while minimally impacting the main task. Even in VFL scenarios with 32 passive parties and only one set of auxiliary data, our attack maintains high performance. Moreover, when confronted with distinct defense strategies, our attack remains largely unaffected and exhibits strong robustness. We hope that the disclosure of this triggerless backdoor attack pathway will encourage the community to revisit security threats in VFL scenarios and inspire researchers to develop more robust and practical defense strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5782\u76f4\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u65e0\u89e6\u53d1\u5668\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u653b\u51fb\u5728\u66f4\u4e25\u683c\u7684\u5b89\u5168\u5047\u8bbe\u4e0b\uff08\u653b\u51fb\u8005\u5728\u8bad\u7ec3\u9636\u6bb5\u662f\u8bda\u5b9e\u4f46\u597d\u5947\u7684\u800c\u975e\u6076\u610f\u7684\uff09\uff0c\u901a\u8fc7\u7279\u5f81\u800c\u975e\u89e6\u53d1\u5668\u5b9e\u73b0\u540e\u95e8\u653b\u51fb\uff0c\u6027\u80fd\u8fdc\u8d85\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5782\u76f4\u8054\u90a6\u5b66\u4e60\uff08VFL\uff09\u4ee5\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u8457\u79f0\uff0c\u4f46\u4ecd\u9762\u4e34\u540e\u95e8\u653b\u51fb\u7684\u9690\u79c1\u548c\u5b89\u5168\u5a01\u80c1\u3002\u73b0\u6709\u540e\u95e8\u653b\u51fb\u901a\u5e38\u9700\u8981\u5728\u8bad\u7ec3\u9636\u6bb5\u690d\u5165\u89e6\u53d1\u5668\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u6dfb\u52a0\u89e6\u53d1\u5668\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u89e6\u53d1\u5668\u5728VFL\u540e\u95e8\u653b\u51fb\u4e2d\u5e76\u975e\u5fc5\u9700\uff0c\u56e0\u6b64\u63a2\u7d22\u66f4\u9690\u853d\u7684\u653b\u51fb\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u7684\u65e0\u89e6\u53d1\u5668\u540e\u95e8\u653b\u51fb\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1\uff09\u9488\u5bf9\u76ee\u6807\u540e\u95e8\u653b\u51fb\u7684\u6807\u7b7e\u63a8\u65ad\uff1b2\uff09\u5177\u6709\u653e\u5927\u548c\u6270\u52a8\u673a\u5236\u7684\u6bd2\u836f\u751f\u6210\uff1b3\uff09\u5b9e\u65bd\u653b\u51fb\u7684\u540e\u95e8\u6267\u884c\u3002\u653b\u51fb\u5728\u66f4\u4e25\u683c\u7684\u5b89\u5168\u5047\u8bbe\u4e0b\u8fdb\u884c\uff0c\u653b\u51fb\u8005\u5728\u8bad\u7ec3\u9636\u6bb5\u662f\u8bda\u5b9e\u4f46\u597d\u5947\u7684\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u653b\u51fb\u6027\u80fd\u6bd4\u4e09\u79cd\u57fa\u7ebf\u540e\u95e8\u653b\u51fb\u9ad8\u51fa2\u523050\u500d\uff0c\u540c\u65f6\u5bf9\u4e3b\u4efb\u52a1\u5f71\u54cd\u6700\u5c0f\u3002\u5373\u4f7f\u5728\u670932\u4e2a\u88ab\u52a8\u65b9\u4e14\u53ea\u6709\u4e00\u7ec4\u8f85\u52a9\u6570\u636e\u7684VFL\u573a\u666f\u4e2d\uff0c\u653b\u51fb\u4ecd\u4fdd\u6301\u9ad8\u6027\u80fd\u3002\u9762\u5bf9\u4e0d\u540c\u9632\u5fa1\u7b56\u7565\u65f6\uff0c\u653b\u51fb\u57fa\u672c\u4e0d\u53d7\u5f71\u54cd\u4e14\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86VFL\u4e2d\u65e0\u89e6\u53d1\u5668\u540e\u95e8\u653b\u51fb\u7684\u65b0\u9014\u5f84\uff0c\u5e0c\u671b\u8fd9\u4e00\u53d1\u73b0\u80fd\u4fc3\u4f7f\u793e\u533a\u91cd\u65b0\u5ba1\u89c6VFL\u573a\u666f\u4e2d\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5e76\u6fc0\u53d1\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u5b9e\u7528\u7684\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2602.20181", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20181", "abs": "https://arxiv.org/abs/2602.20181", "authors": ["Lei Shu", "Armin Yeganeh", "Sinem Mollaoglu", "Jiayu Zhou", "Dong Zhao"], "title": "Closing the Expertise Gap in Residential Building Energy Retrofits: A Domain-Specific LLM for Informed Decision-Making", "comment": "A preprint version is available via SSRN (Elsevier Preprint Service)", "summary": "Residential energy retrofit decision-making is constrained by an expertise gap, as homeowners lack the technical literacy required for energy assessments. To address this challenge, this study develops a domain-specific large language model (LLM) that provides optimal retrofit recommendations using homeowner-accessible descriptions of basic dwelling characteristics. The model is fine-tuned on physics-based energy simulations and techno-economic calculations derived from 536,416 U.S. residential building prototypes across nine major retrofit categories. Using Low-Rank Adaptation (LoRA), the LLM maps dwelling characteristics to optimal retrofit selections and associated performance outcomes. Evaluation against physics-grounded baselines shows that the model identifies the optimal retrofit for CO2 reduction within its top three recommendations in 98.9% of cases and the shortest discounted payback period in 93.3% of cases. Fine-tuning yields an order-of-magnitude reduction in CO2 prediction error and multi-fold reductions for energy use and retrofit cost. The model maintains performance under incomplete input conditions, supporting informed residential decarbonization decisions.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u4f4f\u5b85\u80fd\u6e90\u6539\u9020\u51b3\u7b56\u7684\u9886\u57df\u7279\u5b9a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u623f\u4e3b\u53ef\u8bbf\u95ee\u7684\u57fa\u672c\u4f4f\u5b85\u7279\u5f81\u63cf\u8ff0\u63d0\u4f9b\u6700\u4f18\u6539\u9020\u5efa\u8bae\uff0c\u5728CO2\u51cf\u6392\u548c\u6295\u8d44\u56de\u6536\u671f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f4f\u5b85\u80fd\u6e90\u6539\u9020\u51b3\u7b56\u53d7\u5230\u4e13\u4e1a\u77e5\u8bc6\u5dee\u8ddd\u7684\u9650\u5236\uff0c\u623f\u4e3b\u7f3a\u4e4f\u8fdb\u884c\u80fd\u6e90\u8bc4\u4f30\u6240\u9700\u7684\u6280\u672f\u7d20\u517b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u57fa\u4e8e\u623f\u4e3b\u53ef\u7406\u89e3\u7684\u57fa\u672c\u4f4f\u5b85\u7279\u5f81\u63d0\u4f9b\u6539\u9020\u5efa\u8bae\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e536,416\u4e2a\u7f8e\u56fd\u4f4f\u5b85\u5efa\u7b51\u539f\u578b\u8fdb\u884c\u7269\u7406\u80fd\u6e90\u6a21\u62df\u548c\u6280\u672f\u7ecf\u6d4e\u8ba1\u7b97\uff0c\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u65b9\u6cd5\u8fdb\u884c\u5fae\u8c03\uff0c\u5c06\u4f4f\u5b85\u7279\u5f81\u6620\u5c04\u5230\u6700\u4f18\u6539\u9020\u9009\u62e9\u548c\u6027\u80fd\u7ed3\u679c\u3002", "result": "\u6a21\u578b\u5728CO2\u51cf\u6392\u65b9\u9762\uff0c\u524d\u4e09\u4e2a\u63a8\u8350\u4e2d\u5305\u542b\u6700\u4f18\u6539\u9020\u65b9\u6848\u7684\u6bd4\u4f8b\u8fbe\u523098.9%\uff1b\u5728\u6700\u77ed\u8d34\u73b0\u6295\u8d44\u56de\u6536\u671f\u65b9\u9762\u8fbe\u523093.3%\u3002\u5fae\u8c03\u4f7fCO2\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u80fd\u6e90\u4f7f\u7528\u548c\u6539\u9020\u6210\u672c\u9884\u6d4b\u8bef\u5dee\u4e5f\u5927\u5e45\u964d\u4f4e\u3002\u6a21\u578b\u5728\u4e0d\u5b8c\u6574\u8f93\u5165\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u8be5\u9886\u57df\u7279\u5b9aLLM\u80fd\u591f\u6709\u6548\u652f\u6301\u4f4f\u5b85\u8131\u78b3\u51b3\u7b56\uff0c\u901a\u8fc7\u623f\u4e3b\u53ef\u8bbf\u95ee\u7684\u4f4f\u5b85\u7279\u5f81\u63cf\u8ff0\u63d0\u4f9b\u53ef\u9760\u7684\u6539\u9020\u5efa\u8bae\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u4e13\u4e1a\u77e5\u8bc6\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u4f4f\u5b85\u80fd\u6e90\u6539\u9020\u7684\u666e\u53ca\u3002"}}
{"id": "2602.20629", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20629", "abs": "https://arxiv.org/abs/2602.20629", "authors": ["Santiago Gonzalez", "Alireza Amiri Bavandpour", "Peter Ye", "Edward Zhang", "Ruslans Aleksejevs", "Todor Anti\u0107", "Polina Baron", "Sujeet Bhalerao", "Shubhrajit Bhattacharya", "Zachary Burton", "John Byrne", "Hyungjun Choi", "Nujhat Ahmed Disha", "Koppany Istv\u00e1n Encz", "Yuchen Fang", "Robert Joseph George", "Ebrahim Ghorbani", "Alan Goldfarb", "Jing Guo", "Meghal Gupta", "Stefano Huber", "Annika Kanckos", "Minjung Kang", "Hyun Jong Kim", "Dino Lorenzini", "Levi Lorenzo", "Tianyi Mao", "Giovanni Marzenta", "Ariane M. Masuda", "Lukas Mauth", "Ana Mickovic", "Andres Miniguano-Trujillo", "Antoine Moulin", "Wenqi Ni", "Tomos Parry", "Kevin Ren", "Hossein Roodbarani", "Mathieu Rundstr\u00f6m", "Manjil Saikia", "Detchat Samart", "Rebecca Steiner", "Connor Stewart", "Dhara Thakkar", "Jeffrey Tse", "Vasiliki Velona", "Yunhai Xiang", "Sibel Yal\u00e7\u0131n", "Jun Yan", "Ji Zeng", "Arman Cohan", "Quanquan C. Liu"], "title": "QEDBENCH: Quantifying the Alignment Gap in Automated Evaluation of University-Level Mathematical Proofs", "comment": null, "summary": "As Large Language Models (LLMs) saturate elementary benchmarks, the research frontier has shifted from generation to the reliability of automated evaluation. We demonstrate that standard \"LLM-as-a-Judge\" protocols suffer from a systematic Alignment Gap when applied to upper-undergraduate to early graduate level mathematics. To quantify this, we introduce QEDBench, the first large-scale dual-rubric alignment benchmark to systematically measure alignment with human experts on university-level math proofs by contrasting course-specific rubrics against expert common knowledge criteria. By deploying a dual-evaluation matrix (7 judges x 5 solvers) against 1,000+ hours of human evaluation, we reveal that certain frontier evaluators like Claude Opus 4.5, DeepSeek-V3, Qwen 2.5 Max, and Llama 4 Maverick exhibit significant positive bias (up to +0.18, +0.20, +0.30, +0.36 mean score inflation, respectively). Furthermore, we uncover a critical reasoning gap in the discrete domain: while Gemini 3.0 Pro achieves state-of-the-art performance (0.91 average human evaluation score), other reasoning models like GPT-5 Pro and Claude Sonnet 4.5 see their performance significantly degrade in discrete domains. Specifically, their average human evaluation scores drop to 0.72 and 0.63 in Discrete Math, and to 0.74 and 0.50 in Graph Theory. In addition to these research results, we also release QEDBench as a public benchmark for evaluating and improving AI judges. Our benchmark is publicly published at https://github.com/qqliu/Yale-QEDBench.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u8bc4\u4f30\u534f\u8bae\u5728\u9ad8\u7b49\u6570\u5b66\u8bc1\u660e\u8bc4\u4f30\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u5bf9\u9f50\u5dee\u8ddd\uff0c\u67d0\u4e9b\u524d\u6cbf\u8bc4\u4f30\u6a21\u578b\u5b58\u5728\u663e\u8457\u6b63\u5411\u504f\u5dee\uff0c\u79bb\u6563\u6570\u5b66\u9886\u57df\u5b58\u5728\u5173\u952e\u63a8\u7406\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u7840\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8d8b\u4e8e\u9971\u548c\uff0c\u7814\u7a76\u524d\u6cbf\u4ece\u751f\u6210\u8f6c\u5411\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002\u6807\u51c6\"LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\"\u534f\u8bae\u5728\u9ad8\u7b49\u6570\u5b66\u8bc1\u660e\u8bc4\u4f30\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u5bf9\u9f50\u5dee\u8ddd\u3002", "method": "\u5f15\u5165QEDBench\u57fa\u51c6\uff0c\u91c7\u7528\u53cc\u8bc4\u5206\u6807\u51c6\u5bf9\u6bd4\u8bfe\u7a0b\u7279\u5b9a\u8bc4\u5206\u6807\u51c6\u548c\u4e13\u5bb6\u5e38\u8bc6\u6807\u51c6\uff0c\u90e8\u7f727\u4f4d\u8bc4\u5224\u8005\u00d75\u4e2a\u6c42\u89e3\u5668\u7684\u53cc\u8bc4\u4f30\u77e9\u9635\uff0c\u57fa\u4e8e1000+\u5c0f\u65f6\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0Claude Opus 4.5\u3001DeepSeek-V3\u3001Qwen 2.5 Max\u548cLlama 4 Maverick\u7b49\u524d\u6cbf\u8bc4\u4f30\u6a21\u578b\u5b58\u5728\u663e\u8457\u6b63\u5411\u504f\u5dee\uff1bGemini 3.0 Pro\u8868\u73b0\u6700\u4f73\uff0c\u800cGPT-5 Pro\u548cClaude Sonnet 4.5\u5728\u79bb\u6563\u6570\u5b66\u9886\u57df\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "LLM\u8bc4\u4f30\u534f\u8bae\u5728\u9ad8\u7b49\u6570\u5b66\u9886\u57df\u5b58\u5728\u7cfb\u7edf\u6027\u5bf9\u9f50\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u53d1\u5e03QEDBench\u4f5c\u4e3a\u516c\u5f00\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbAI\u8bc4\u5224\u8005\u3002"}}
{"id": "2602.20643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20643", "abs": "https://arxiv.org/abs/2602.20643", "authors": ["Jiawei Wang", "Chuang Yang", "Jiawei Yong", "Xiaohang Xu", "Hongjun Wang", "Noboru Koshizuka", "Shintaro Fukushima", "Ryosuke Shibasaki", "Renhe Jiang"], "title": "TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer", "comment": "TrajGPT-R is a Reinforcement Learning-Enhanced Generative Pre-trained Transformer for Mobility Trajectory Generation", "summary": "Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6846\u67b6TrajGPT\uff0c\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u57ce\u5e02\u79fb\u52a8\u8f68\u8ff9\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\u89e3\u51b3\u9690\u79c1\u6570\u636e\u83b7\u53d6\u95ee\u9898\uff0c\u5728\u53ef\u9760\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u57ce\u5e02\u79fb\u52a8\u8f68\u8ff9\u6570\u636e\u5bf9\u7406\u89e3\u57ce\u5e02\u52a8\u6001\u548c\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9690\u79c1\u95ee\u9898\u9650\u5236\u4e86\u6570\u636e\u7684\u83b7\u53d6\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u8f68\u8ff9\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u62a4\u4e2a\u4eba\u9690\u79c1\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5Transformer\u6a21\u578b\uff1a1) \u5c06\u8f68\u8ff9\u751f\u6210\u5efa\u6a21\u4e3a\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7tokenization\u51cf\u5c11\u8bcd\u6c47\u7a7a\u95f4\uff1b2) \u96c6\u6210\u9006\u5f3a\u5316\u5b66\u4e60\u6355\u6349\u8f68\u8ff9\u7ea7\u5956\u52b1\u4fe1\u53f7\uff0c\u63a8\u65ad\u4e2a\u4f53\u79fb\u52a8\u504f\u597d\uff1b3) \u4f7f\u7528\u6784\u5efa\u7684\u5956\u52b1\u6a21\u578b\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u89e3\u51b3\u4f20\u7edfRL\u65b9\u6cd5\u7684\u957f\u671f\u4fe1\u7528\u5206\u914d\u548c\u7a00\u758f\u5956\u52b1\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u53ef\u9760\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4e3a\u57ce\u5e02\u6570\u636e\u6a21\u62df\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63a8\u52a8\u4e86\u57ce\u5e02\u79fb\u52a8\u5efa\u6a21\u9886\u57df\u7684\u53d1\u5c55\uff0c\u8fd8\u4e3a\u4ea4\u901a\u7ba1\u7406\u548c\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u65b9\u6cd5\u8bba\u652f\u6301\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2602.20671", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20671", "abs": "https://arxiv.org/abs/2602.20671", "authors": ["Antonios Tziorvas", "Andreas Tritsarolis", "Yannis Theodoridis"], "title": "Bikelution: Federated Gradient-Boosting for Scalable Shared Micro-Mobility Demand Forecasting", "comment": null, "summary": "The rapid growth of dockless bike-sharing systems has generated massive spatio-temporal datasets useful for fleet allocation, congestion reduction, and sustainable mobility. Bike demand, however, depends on several external factors, making traditional time-series models insufficient. Centralized Machine Learning (CML) yields high-accuracy forecasts but raises privacy and bandwidth issues when data are distributed across edge devices. To overcome these limitations, we propose Bikelution, an efficient Federated Learning (FL) solution based on gradient-boosted trees that preserves privacy while delivering accurate mid-term demand forecasts up to six hours ahead. Experiments on three real-world BSS datasets show that Bikelution is comparable to its CML-based variant and outperforms the current state-of-the-art. The results highlight the feasibility of privacy-aware demand forecasting and outline the trade-offs between FL and CML approaches.", "AI": {"tldr": "Bikelution\uff1a\u57fa\u4e8e\u68af\u5ea6\u63d0\u5347\u6811\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6848\uff0c\u7528\u4e8e\u65e0\u6869\u5171\u4eab\u5355\u8f66\u9700\u6c42\u9884\u6d4b\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u51c6\u786e\u7684\u4e2d\u671f\u9884\u6d4b", "motivation": "\u65e0\u6869\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u4ea7\u751f\u5927\u91cf\u65f6\u7a7a\u6570\u636e\uff0c\u4f46\u5355\u8f66\u9700\u6c42\u53d7\u591a\u79cd\u5916\u90e8\u56e0\u7d20\u5f71\u54cd\uff0c\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u4e0d\u8db3\u3002\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u867d\u7136\u9884\u6d4b\u51c6\u786e\uff0c\u4f46\u5728\u6570\u636e\u5206\u5e03\u5728\u8fb9\u7f18\u8bbe\u5907\u65f6\u5b58\u5728\u9690\u79c1\u548c\u5e26\u5bbd\u95ee\u9898\u3002", "method": "\u63d0\u51faBikelution\u65b9\u6848\uff0c\u57fa\u4e8e\u68af\u5ea6\u63d0\u5347\u6811\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u8fdb\u884c\u51c6\u786e\u7684\u4e2d\u671f\u9700\u6c42\u9884\u6d4b\uff08\u6700\u591a\u63d0\u524d6\u5c0f\u65f6\uff09", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754cBSS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBikelution\u6027\u80fd\u4e0e\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u53d8\u4f53\u76f8\u5f53\uff0c\u5e76\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u9690\u79c1\u611f\u77e5\u9700\u6c42\u9884\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u8054\u90a6\u5b66\u4e60\u4e0e\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb"}}
{"id": "2602.20677", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20677", "abs": "https://arxiv.org/abs/2602.20677", "authors": ["Wei Chen", "Yuqian Wu", "Junle Chen", "Xiaofang Zhou", "Yuxuan Liang"], "title": "UrbanFM: Scaling Urban Spatio-Temporal Foundation Models", "comment": null, "summary": "Urban systems, as dynamic complex systems, continuously generate spatio-temporal data streams that encode the fundamental laws of human mobility and city evolution. While AI for Science has witnessed the transformative power of foundation models in disciplines like genomics and meteorology, urban computing remains fragmented due to \"scenario-specific\" models, which are overfitted to specific regions or tasks, hindering their generalizability. To bridge this gap and advance spatio-temporal foundation models for urban systems, we adopt scaling as the central perspective and systematically investigate two key questions: what to scale and how to scale. Grounded in first-principles analysis, we identify three critical dimensions: heterogeneity, correlation, and dynamics, aligning these principles with the fundamental scientific properties of urban spatio-temporal data. Specifically, to address heterogeneity through data scaling, we construct WorldST. This billion-scale corpus standardizes diverse physical signals, such as traffic flow and speed, from over 100 global cities into a unified data format. To enable computation scaling for modeling correlations, we introduce the MiniST unit, a novel split mechanism that discretizes continuous spatio-temporal fields into learnable computational units to unify representations of grid-based and sensor-based observations. Finally, addressing dynamics via architecture scaling, we propose UrbanFM, a minimalist self-attention architecture designed with limited inductive biases to autonomously learn dynamic spatio-temporal dependencies from massive data. Furthermore, we establish EvalST, the largest-scale urban spatio-temporal benchmark to date. Extensive experiments demonstrate that UrbanFM achieves remarkable zero-shot generalization across unseen cities and tasks, marking a pivotal first step toward large-scale urban spatio-temporal foundation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u6784\u5efa\u57ce\u5e02\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u3001\u8ba1\u7b97\u548c\u67b6\u6784\u4e09\u4e2a\u7ef4\u5ea6\u7684\u6269\u5c55\u6765\u89e3\u51b3\u57ce\u5e02\u7cfb\u7edf\u4e2d\u65f6\u7a7a\u6570\u636e\u7684\u5f02\u8d28\u6027\u3001\u76f8\u5173\u6027\u548c\u52a8\u6001\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8de8\u57ce\u5e02\u548c\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u57ce\u5e02\u7cfb\u7edf\u4f5c\u4e3a\u52a8\u6001\u590d\u6742\u7cfb\u7edf\uff0c\u6301\u7eed\u4ea7\u751f\u65f6\u7a7a\u6570\u636e\u6d41\uff0c\u4f46\u5f53\u524d\u57ce\u5e02\u8ba1\u7b97\u9886\u57df\u5b58\u5728\"\u573a\u666f\u7279\u5b9a\"\u6a21\u578b\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u6a21\u578b\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u533a\u57df\u6216\u4efb\u52a1\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u6784\u5efa\u57ce\u5e02\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u9700\u8981\u89e3\u51b3\u5f02\u8d28\u6027\u3001\u76f8\u5173\u6027\u548c\u52a8\u6001\u6027\u8fd9\u4e09\u4e2a\u6838\u5fc3\u79d1\u5b66\u95ee\u9898\u3002", "method": "1. \u6570\u636e\u6269\u5c55\uff1a\u6784\u5efaWorldST\u4ebf\u7ea7\u8bed\u6599\u5e93\uff0c\u6807\u51c6\u5316100\u591a\u4e2a\u5168\u7403\u57ce\u5e02\u7684\u4ea4\u901a\u6d41\u91cf\u548c\u901f\u5ea6\u7b49\u7269\u7406\u4fe1\u53f7\uff1b2. \u8ba1\u7b97\u6269\u5c55\uff1a\u5f15\u5165MiniST\u5355\u5143\uff0c\u5c06\u8fde\u7eed\u65f6\u7a7a\u573a\u79bb\u6563\u5316\u4e3a\u53ef\u5b66\u4e60\u7684\u8ba1\u7b97\u5355\u5143\uff1b3. \u67b6\u6784\u6269\u5c55\uff1a\u63d0\u51faUrbanFM\uff0c\u4e00\u79cd\u6700\u5c0f\u5316\u5f52\u7eb3\u504f\u7f6e\u7684\u81ea\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60\u52a8\u6001\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff1b4. \u5efa\u7acbEvalST\u5927\u89c4\u6a21\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "UrbanFM\u5728\u672a\u89c1\u8fc7\u7684\u57ce\u5e02\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u6807\u5fd7\u7740\u5927\u89c4\u6a21\u57ce\u5e02\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u7684\u91cd\u8981\u8fdb\u5c55\u3002WorldST\u8bed\u6599\u5e93\u5305\u542b\u4ebf\u7ea7\u6807\u51c6\u5316\u6570\u636e\uff0cEvalST\u662f\u76ee\u524d\u6700\u5927\u89c4\u6a21\u7684\u65f6\u7a7a\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u5f02\u8d28\u6027\u3001\u76f8\u5173\u6027\u548c\u52a8\u6001\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u57ce\u5e02\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u6269\u5c55\u65b9\u6cd5\u5728\u7edf\u4e00\u57ce\u5e02\u65f6\u7a7a\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u57ce\u5e02\u79d1\u5b66\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.20698", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20698", "abs": "https://arxiv.org/abs/2602.20698", "authors": ["Maryam Aliakbarpour", "Vladimir Braverman", "Yuhan Liu", "Junze Yin"], "title": "High-Dimensional Robust Mean Estimation with Untrusted Batches", "comment": null, "summary": "We study high-dimensional mean estimation in a collaborative setting where data is contributed by $N$ users in batches of size $n$. In this environment, a learner seeks to recover the mean $\u03bc$ of a true distribution $P$ from a collection of sources that are both statistically heterogeneous and potentially malicious. We formalize this challenge through a double corruption landscape: an $\\varepsilon$-fraction of users are entirely adversarial, while the remaining ``good'' users provide data from distributions that are related to $P$, but deviate by a proximity parameter $\u03b1$.\n  Unlike existing work on the untrusted batch model, which typically measures this deviation via total variation distance in discrete settings, we address the continuous, high-dimensional regime under two natural variants for deviation: (1) good batches are drawn from distributions with a mean-shift of $\\sqrt\u03b1$, or (2) an $\u03b1$-fraction of samples within each good batch are adversarially corrupted. In particular, the second model presents significant new challenges: in high dimensions, unlike discrete settings, even a small fraction of sample-level corruption can shift empirical means and covariances arbitrarily.\n  We provide two Sum-of-Squares (SoS) based algorithms to navigate this tiered corruption. Our algorithms achieve the minimax-optimal error rate $O(\\sqrt{\\varepsilon/n} + \\sqrt{d/nN} + \\sqrt\u03b1)$, demonstrating that while heterogeneity $\u03b1$ represents an inherent statistical difficulty, the influence of adversarial users is suppressed by a factor of $1/\\sqrt{n}$ due to the internal averaging afforded by the batch structure.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u9ad8\u7ef4\u5747\u503c\u4f30\u8ba1\u7684\u534f\u4f5c\u5b66\u4e60\u95ee\u9898\uff0c\u7528\u6237\u4ee5\u5927\u5c0f\u4e3an\u7684\u6279\u6b21\u8d21\u732e\u6570\u636e\uff0c\u9762\u4e34\u53cc\u91cd\u8150\u8d25\uff1a\u03b5\u6bd4\u4f8b\u7528\u6237\u5b8c\u5168\u5bf9\u6297\uff0c\u5176\u4f59\"\u597d\"\u7528\u6237\u6570\u636e\u5206\u5e03\u4e0e\u771f\u5b9e\u5206\u5e03P\u5b58\u5728\u03b1\u504f\u5dee\u3002\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u5e73\u65b9\u548c\u7b97\u6cd5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fbe\u5230\u6700\u4f18\u8bef\u5dee\u7387\u3002", "motivation": "\u7814\u7a76\u5728\u534f\u4f5c\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u5f53\u6570\u636e\u6765\u81ea\u7edf\u8ba1\u5f02\u8d28\u4e14\u53ef\u80fd\u6076\u610f\u7684\u7528\u6237\u65f6\uff0c\u5982\u4f55\u4ece\u771f\u5b9e\u5206\u5e03\u4e2d\u6062\u590d\u5747\u503c\u3002\u73b0\u6709\u5de5\u4f5c\u5728\u79bb\u6563\u8bbe\u7f6e\u4e2d\u4f7f\u7528\u603b\u53d8\u5dee\u8ddd\u79bb\uff0c\u800c\u672c\u6587\u5173\u6ce8\u8fde\u7eed\u9ad8\u7ef4\u573a\u666f\uff0c\u8003\u8651\u4e24\u79cd\u504f\u5dee\u53d8\u4f53\uff1a\u5747\u503c\u504f\u79fb\u548c\u6837\u672c\u7ea7\u8150\u8d25\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u5e73\u65b9\u548c(SoS)\u7684\u7b97\u6cd5\u6765\u5904\u7406\u5206\u5c42\u8150\u8d25\u3002\u7b2c\u4e00\u79cd\u5904\u7406\u597d\u7528\u6237\u5206\u5e03\u5b58\u5728\u5747\u503c\u504f\u79fb\u7684\u60c5\u51b5\uff0c\u7b2c\u4e8c\u79cd\u5904\u7406\u597d\u7528\u6237\u6279\u6b21\u5185\u90e8\u5206\u6837\u672c\u88ab\u5bf9\u6297\u6027\u6c61\u67d3\u7684\u60c5\u51b5\u3002\u7b97\u6cd5\u5229\u7528\u6279\u6b21\u7ed3\u6784\u63d0\u4f9b\u7684\u5185\u90e8\u5e73\u5747\u6765\u6291\u5236\u5bf9\u6297\u7528\u6237\u7684\u5f71\u54cd\u3002", "result": "\u7b97\u6cd5\u8fbe\u5230\u6781\u5c0f\u6781\u5927\u6700\u4f18\u8bef\u5dee\u7387O(\u221a(\u03b5/n) + \u221a(d/nN) + \u221a\u03b1)\uff0c\u8868\u660e\u5f02\u8d28\u6027\u03b1\u4ee3\u8868\u56fa\u6709\u7edf\u8ba1\u96be\u5ea6\uff0c\u800c\u5bf9\u6297\u7528\u6237\u7684\u5f71\u54cd\u88ab\u6279\u6b21\u7ed3\u6784\u7684\u5185\u90e8\u5e73\u5747\u6291\u5236\u4e861/\u221an\u56e0\u5b50\u3002", "conclusion": "\u5728\u9ad8\u7ef4\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\u4e2d\uff0c\u6279\u6b21\u7ed3\u6784\u80fd\u6709\u6548\u51cf\u8f7b\u5bf9\u6297\u7528\u6237\u7684\u5f71\u54cd\uff0c\u800c\u7edf\u8ba1\u5f02\u8d28\u6027\u6784\u6210\u56fa\u6709\u6311\u6218\u3002\u63d0\u51fa\u7684SoS\u7b97\u6cd5\u5728\u53cc\u91cd\u8150\u8d25\u73af\u5883\u4e0b\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e3a\u9ad8\u7ef4\u8fde\u7eed\u8bbe\u7f6e\u4e2d\u7684\u9c81\u68d2\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.20721", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20721", "abs": "https://arxiv.org/abs/2602.20721", "authors": ["Xiaoman Feng", "Mingkun Lei", "Yang Wang", "Dingwen Fu", "Chi Zhang"], "title": "CleanStyle: Plug-and-Play Style Conditioning Purification for Text-to-Image Stylization", "comment": "26 pages", "summary": "Style transfer in diffusion models enables controllable visual generation by injecting the style of a reference image. However, recent encoder-based methods, while efficient and tuning-free, often suffer from content leakage, where semantic elements from the style image undesirably appear in the output, impairing prompt fidelity and stylistic consistency. In this work, we introduce CleanStyle, a plug-and-play framework that filters out content-related noise from the style embedding without retraining. Motivated by empirical analysis, we observe that such leakage predominantly stems from the tail components of the style embedding, which are isolated via Singular Value Decomposition (SVD). To address this, we propose CleanStyleSVD (CS-SVD), which dynamically suppresses tail components using a time-aware exponential schedule, providing clean, style-preserving conditional embeddings throughout the denoising process. Furthermore, we present Style-Specific Classifier-Free Guidance (SS-CFG), which reuses the suppressed tail components to construct style-aware unconditional inputs. Unlike conventional methods that use generic negative embeddings (e.g., zero vectors), SS-CFG introduces targeted negative signals that reflect style-specific but prompt-irrelevant visual elements. This enables the model to effectively suppress these distracting patterns during generation, thereby improving prompt fidelity and enhancing the overall visual quality of stylized outputs. Our approach is lightweight, interpretable, and can be seamlessly integrated into existing encoder-based diffusion models without retraining. Extensive experiments demonstrate that CleanStyle substantially reduces content leakage, improves stylization quality and improves prompt alignment across a wide range of style references and prompts.", "AI": {"tldr": "CleanStyle\uff1a\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7SVD\u5206\u89e3\u548c\u52a8\u6001\u6291\u5236\u5c3e\u90e8\u6210\u5206\u6765\u8fc7\u6ee4\u98ce\u683c\u5d4c\u5165\u4e2d\u7684\u5185\u5bb9\u6cc4\u6f0f\uff0c\u540c\u65f6\u63d0\u51fa\u98ce\u683c\u7279\u5b9a\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u673a\u5236\u6765\u63d0\u5347\u63d0\u793a\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u6269\u6563\u6a21\u578b\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u867d\u7136\u9ad8\u6548\u4e14\u65e0\u9700\u8c03\u4f18\uff0c\u4f46\u5b58\u5728\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\u2014\u2014\u98ce\u683c\u56fe\u50cf\u4e2d\u7684\u8bed\u4e49\u5143\u7d20\u4f1a\u4e0d\u671f\u671b\u5730\u51fa\u73b0\u5728\u8f93\u51fa\u4e2d\uff0c\u635f\u5bb3\u4e86\u63d0\u793a\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faCleanStyle\u6846\u67b6\uff1a1) CleanStyleSVD (CS-SVD)\uff1a\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3(SVD)\u9694\u79bb\u98ce\u683c\u5d4c\u5165\u7684\u5c3e\u90e8\u6210\u5206\uff0c\u5e76\u4f7f\u7528\u65f6\u95f4\u611f\u77e5\u6307\u6570\u8c03\u5ea6\u52a8\u6001\u6291\u5236\u8fd9\u4e9b\u6210\u5206\uff1b2) Style-Specific Classifier-Free Guidance (SS-CFG)\uff1a\u91cd\u65b0\u5229\u7528\u88ab\u6291\u5236\u7684\u5c3e\u90e8\u6210\u5206\u6784\u5efa\u98ce\u683c\u611f\u77e5\u7684\u65e0\u6761\u4ef6\u8f93\u5165\uff0c\u63d0\u4f9b\u9488\u5bf9\u6027\u7684\u8d1f\u4fe1\u53f7\u6765\u6291\u5236\u4e0e\u63d0\u793a\u65e0\u5173\u7684\u89c6\u89c9\u5143\u7d20\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCleanStyle\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5bb9\u6cc4\u6f0f\uff0c\u63d0\u9ad8\u4e86\u98ce\u683c\u5316\u8d28\u91cf\u548c\u63d0\u793a\u5bf9\u9f50\u5ea6\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u98ce\u683c\u53c2\u8003\u548c\u63d0\u793a\u3002", "conclusion": "CleanStyle\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.20725", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20725", "abs": "https://arxiv.org/abs/2602.20725", "authors": ["Junwei Shu", "Wenjie Liu", "Changgu Chen", "Hantang Liu", "Yang Li", "Changbo Wang"], "title": "Bridging Physically Based Rendering and Diffusion Models with Stochastic Differential Equation", "comment": "preprint", "summary": "Diffusion-based image generators excel at producing realistic content from text or image conditions, but they offer only limited explicit control over low-level, physically grounded shading and material properties. In contrast, physically based rendering (PBR) offers fine-grained physical control but lacks prompt-driven flexibility. Although these two paradigms originate from distinct communities, both share a common evolution -- from noisy observations to clean images. In this paper, we propose a unified stochastic formulation that bridges Monte Carlo rendering and diffusion-based generative modeling. First, a general stochastic differential equation (SDE) formulation for Monte Carlo integration under the Central Limit Theorem is modeled. Through instantiation via physically based path tracing, we convert it into a physically grounded SDE representation. Moreover, we provide a systematic analysis of how the physical characteristics of path tracing can be extended to existing diffusion models from the perspective of noise variance. Extensive experiments across multiple tasks show that our method can exert physically grounded control over diffusion-generated results, covering tasks such as rendering and material editing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u968f\u673a\u516c\u5f0f\uff0c\u5c06\u8499\u7279\u5361\u6d1b\u6e32\u67d3\u4e0e\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u5bf9\u6269\u6563\u751f\u6210\u7ed3\u679c\u7684\u7269\u7406\u57fa\u7840\u63a7\u5236\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u6587\u672c/\u56fe\u50cf\u6761\u4ef6\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u4f46\u7f3a\u4e4f\u5bf9\u4f4e\u5c42\u7ea7\u7269\u7406\u5c5e\u6027\uff08\u5982\u7740\u8272\u548c\u6750\u8d28\uff09\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u800c\u7269\u7406\u6e32\u67d3\uff08PBR\uff09\u63d0\u4f9b\u7cbe\u7ec6\u7269\u7406\u63a7\u5236\u4f46\u7f3a\u4e4f\u63d0\u793a\u9a71\u52a8\u7684\u7075\u6d3b\u6027\u3002\u4e24\u8005\u90fd\u9075\u5faa\u4ece\u566a\u58f0\u89c2\u6d4b\u5230\u5e72\u51c0\u56fe\u50cf\u7684\u5171\u540c\u6f14\u5316\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u968f\u673a\u516c\u5f0f\uff1a1\uff09\u57fa\u4e8e\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u5efa\u7acb\u8499\u7279\u5361\u6d1b\u79ef\u5206\u7684\u901a\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u516c\u5f0f\uff1b2\uff09\u901a\u8fc7\u57fa\u4e8e\u7269\u7406\u7684\u8def\u5f84\u8ffd\u8e2a\u5b9e\u4f8b\u5316\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u7269\u7406\u57fa\u7840\u7684SDE\u8868\u793a\uff1b3\uff09\u4ece\u566a\u58f0\u65b9\u5dee\u89d2\u5ea6\u5206\u6790\u8def\u5f84\u8ffd\u8e2a\u7684\u7269\u7406\u7279\u6027\u5982\u4f55\u6269\u5c55\u5230\u73b0\u6709\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5bf9\u6269\u6563\u751f\u6210\u7ed3\u679c\u65bd\u52a0\u7269\u7406\u57fa\u7840\u7684\u63a7\u5236\uff0c\u6db5\u76d6\u6e32\u67d3\u548c\u6750\u8d28\u7f16\u8f91\u7b49\u4efb\u52a1\u3002", "conclusion": "\u6210\u529f\u5efa\u7acb\u4e86\u8499\u7279\u5361\u6d1b\u6e32\u67d3\u4e0e\u6269\u6563\u751f\u6210\u5efa\u6a21\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6269\u6563\u751f\u6210\u5185\u5bb9\u7684\u7269\u7406\u57fa\u7840\u63a7\u5236\uff0c\u4e3a\u7ed3\u5408\u4e24\u79cd\u8303\u5f0f\u7684\u4f18\u52bf\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2602.20729", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20729", "abs": "https://arxiv.org/abs/2602.20729", "authors": ["Xu Wan", "Chao Yang", "Cheng Yang", "Jie Song", "Mingyang Sun"], "title": "Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty", "comment": null, "summary": "Safe Reinforcement Learning (RL) is crucial for achieving high performance while ensuring safety in real-world applications. However, the complex interplay of multiple uncertainty sources in real environments poses significant challenges for interpretable risk assessment and robust decision-making. To address these challenges, we propose Fuz-RL, a fuzzy measure-guided robust framework for safe RL. Specifically, our framework develops a novel fuzzy Bellman operator for estimating robust value functions using Choquet integrals. Theoretically, we prove that solving the Fuz-RL problem (in Constrained Markov Decision Process (CMDP) form) is equivalent to solving distributionally robust safe RL problems (in robust CMDP form), effectively avoiding min-max optimization. Empirical analyses on safe-control-gym and safety-gymnasium scenarios demonstrate that Fuz-RL effectively integrates with existing safe RL baselines in a model-free manner, significantly improving both safety and control performance under various types of uncertainties in observation, action, and dynamics.", "AI": {"tldr": "Fuz-RL\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u7cca\u6d4b\u5ea6\u7684\u9c81\u68d2\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528Choquet\u79ef\u5206\u4f30\u8ba1\u9c81\u68d2\u503c\u51fd\u6570\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u6e90\uff0c\u907f\u514dmin-max\u4f18\u5316\uff0c\u5728\u5b89\u5168\u6027\u548c\u63a7\u5236\u6027\u80fd\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u6e90\u7684\u590d\u6742\u4ea4\u4e92\u7ed9\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u8bc4\u4f30\u548c\u9c81\u68d2\u51b3\u7b56\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFuz-RL\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eChoquet\u79ef\u5206\u7684\u65b0\u578b\u6a21\u7ccaBellman\u7b97\u5b50\u6765\u4f30\u8ba1\u9c81\u68d2\u503c\u51fd\u6570\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5f62\u5f0f\uff0c\u907f\u514dmin-max\u4f18\u5316\u3002", "result": "\u5728safe-control-gym\u548csafety-gymnasium\u573a\u666f\u4e0a\u7684\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0cFuz-RL\u80fd\u591f\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u4e0e\u73b0\u6709\u5b89\u5168RL\u57fa\u7ebf\u6709\u6548\u96c6\u6210\uff0c\u5728\u89c2\u6d4b\u3001\u52a8\u4f5c\u548c\u52a8\u529b\u5b66\u7b49\u591a\u79cd\u4e0d\u786e\u5b9a\u6027\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u63a7\u5236\u6027\u80fd\u3002", "conclusion": "Fuz-RL\u901a\u8fc7\u6a21\u7cca\u6d4b\u5ea6\u5f15\u5bfc\u7684\u9c81\u68d2\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u4e2d\u591a\u6e90\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u7b49\u4ef7\u4e8e\u5206\u5e03\u9c81\u68d2\u5b89\u5168RL\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2602.20730", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20730", "abs": "https://arxiv.org/abs/2602.20730", "authors": ["Zhenxing Xu", "Zeyuan Ma", "Weidong Bao", "Hui Yan", "Yan Zheng", "Ji Wang"], "title": "Rethink Efficiency Side of Neural Combinatorial Solver: An Offline and Self-Play Paradigm", "comment": null, "summary": "We propose ECO, a versatile learning paradigm that enables efficient offline self-play for Neural Combinatorial Optimization (NCO). ECO addresses key limitations in the field through: 1) Paradigm Shift: Moving beyond inefficient online paradigms, we introduce a two-phase offline paradigm consisting of supervised warm-up and iterative Direct Preference Optimization (DPO); 2) Architecture Shift: We deliberately design a Mamba-based architecture to further enhance the efficiency in the offline paradigm; and 3) Progressive Bootstrapping: To stabilize training, we employ a heuristic-based bootstrapping mechanism that ensures continuous policy improvement during training. Comparison results on TSP and CVRP highlight that ECO performs competitively with up-to-date baselines, with significant advantage on the efficiency side in terms of memory utilization and training throughput. We provide further in-depth analysis on the efficiency, throughput and memory usage of ECO. Ablation studies show rationale behind our designs.", "AI": {"tldr": "ECO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u79bb\u7ebf\u81ea\u5b66\u4e60\u8303\u5f0f\uff0c\u7528\u4e8e\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u79bb\u7ebf\u8bad\u7ec3\u3001Mamba\u67b6\u6784\u548c\u6e10\u8fdb\u5f15\u5bfc\u673a\u5236\uff0c\u5728TSP\u548cCVRP\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u6700\u65b0\u57fa\u7ebf\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u5185\u5b58\u5229\u7528\u7387\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\uff08NCO\uff09\u9886\u57df\u4e2d\u5728\u7ebf\u5b66\u4e60\u8303\u5f0f\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u79bb\u7ebf\u81ea\u5b66\u4e60\u65b9\u6848\uff0c\u4ee5\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf\u3002", "method": "1\uff09\u8303\u5f0f\u8f6c\u53d8\uff1a\u91c7\u7528\u4e24\u9636\u6bb5\u79bb\u7ebf\u8303\u5f0f\uff0c\u5305\u62ec\u76d1\u7763\u9884\u70ed\u548c\u8fed\u4ee3\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\uff1b2\uff09\u67b6\u6784\u8f6c\u53d8\uff1a\u8bbe\u8ba1\u57fa\u4e8eMamba\u7684\u67b6\u6784\u4ee5\u63d0\u5347\u79bb\u7ebf\u8303\u5f0f\u6548\u7387\uff1b3\uff09\u6e10\u8fdb\u5f15\u5bfc\uff1a\u4f7f\u7528\u542f\u53d1\u5f0f\u5f15\u5bfc\u673a\u5236\u7a33\u5b9a\u8bad\u7ec3\uff0c\u786e\u4fdd\u7b56\u7565\u6301\u7eed\u6539\u8fdb\u3002", "result": "\u5728TSP\u548cCVRP\u95ee\u9898\u4e0a\uff0cECO\u4e0e\u6700\u65b0\u57fa\u7ebf\u8868\u73b0\u76f8\u5f53\uff0c\u5728\u5185\u5b58\u5229\u7528\u7387\u548c\u8bad\u7ec3\u541e\u5410\u91cf\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u5408\u7406\u6027\u3002", "conclusion": "ECO\u4e3a\u795e\u7ecf\u7ec4\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5185\u5b58\u53cb\u597d\u7684\u79bb\u7ebf\u81ea\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u8303\u5f0f\u8f6c\u53d8\u3001\u67b6\u6784\u521b\u65b0\u548c\u8bad\u7ec3\u7a33\u5b9a\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2602.20904", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20904", "abs": "https://arxiv.org/abs/2602.20904", "authors": ["Nathan Hu", "Jake Ward", "Thomas Icard", "Christopher Potts"], "title": "Transcoder Adapters for Reasoning-Model Diffing", "comment": "9 pages main, 27 pages total, 10 figures. Code and visualizations at https://transcoder-adapters.github.io/", "summary": "While reasoning models are increasingly ubiquitous, the effects of reasoning training on a model's internal mechanisms remain poorly understood. In this work, we introduce transcoder adapters, a technique for learning an interpretable approximation of the difference in MLP computation before and after fine-tuning. We apply transcoder adapters to characterize the differences between Qwen2.5-Math-7B and its reasoning-distilled variant, DeepSeek-R1-Distill-Qwen-7B. Learned adapters are faithful to the target model's internal computation and next-token predictions. When evaluated on reasoning benchmarks, adapters match the reasoning model's response lengths and typically recover 50-90% of the accuracy gains from reasoning fine-tuning. Adapter features are sparsely activating and interpretable. When examining adapter features, we find that only ~8% have activating examples directly related to reasoning behaviors. We deeply study one such behavior -- the production of hesitation tokens (e.g., \"wait\"). Using attribution graphs, we trace hesitation to only ~2.4% of adapter features (5.6k total) performing one of two functions. These features are necessary and sufficient for producing hesitation tokens; removing them reduces response length, often without affecting accuracy. Overall, our results provide insight into reasoning training and suggest transcoder adapters may be useful for studying fine-tuning more broadly.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\"transcoder adapters\"\u7684\u6280\u672f\uff0c\u7528\u4e8e\u5b66\u4e60\u6a21\u578b\u5728\u5fae\u8c03\u524d\u540eMLP\u8ba1\u7b97\u5dee\u5f02\u7684\u53ef\u89e3\u91ca\u8fd1\u4f3c\uff0c\u5e76\u5e94\u7528\u4e8e\u5206\u6790\u63a8\u7406\u8bad\u7ec3\u5bf9\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u63a8\u7406\u6a21\u578b\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u4f46\u63a8\u7406\u8bad\u7ec3\u5bf9\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u5f71\u54cd\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u91ca\u63a8\u7406\u5fae\u8c03\u5982\u4f55\u6539\u53d8\u6a21\u578b\u7684\u5185\u90e8\u8ba1\u7b97\u3002", "method": "\u63d0\u51fatranscoder adapters\u6280\u672f\uff0c\u5b66\u4e60\u6a21\u578b\u5fae\u8c03\u524d\u540eMLP\u8ba1\u7b97\u5dee\u5f02\u7684\u53ef\u89e3\u91ca\u8fd1\u4f3c\u3002\u5e94\u7528\u4e8eQwen2.5-Math-7B\u53ca\u5176\u63a8\u7406\u84b8\u998f\u53d8\u4f53DeepSeek-R1-Distill-Qwen-7B\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u901a\u8fc7\u5f52\u56e0\u56fe\u7b49\u65b9\u6cd5\u7814\u7a76\u7279\u5b9a\u884c\u4e3a\u3002", "result": "\u5b66\u4e60\u5230\u7684adapters\u80fd\u591f\u5fe0\u5b9e\u53cd\u6620\u76ee\u6807\u6a21\u578b\u7684\u5185\u90e8\u8ba1\u7b97\u548c\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u3002\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cadapters\u80fd\u5339\u914d\u63a8\u7406\u6a21\u578b\u7684\u54cd\u5e94\u957f\u5ea6\uff0c\u901a\u5e38\u6062\u590d50-90%\u7684\u63a8\u7406\u5fae\u8c03\u5e26\u6765\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002\u53d1\u73b0\u53ea\u6709\u7ea68%\u7684adapter\u7279\u5f81\u4e0e\u63a8\u7406\u884c\u4e3a\u76f4\u63a5\u76f8\u5173\uff0c\u5e76\u6df1\u5165\u7814\u7a76\u4e86\"\u72b9\u8c6btoken\"\u884c\u4e3a\u3002", "conclusion": "transcoder adapters\u4e3a\u7406\u89e3\u63a8\u7406\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u53ef\u80fd\u6210\u4e3a\u7814\u7a76\u66f4\u5e7f\u6cdb\u5fae\u8c03\u8fc7\u7a0b\u7684\u6709\u7528\u5de5\u5177\u3002\u7814\u7a76\u63ed\u793a\u4e86\u63a8\u7406\u5fae\u8c03\u53ea\u6539\u53d8\u4e86\u6a21\u578b\u7684\u4e00\u5c0f\u90e8\u5206\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5bf9\u7279\u5b9a\u884c\u4e3a\uff08\u5982\u4ea7\u751f\u72b9\u8c6btoken\uff09\u662f\u5fc5\u8981\u4e14\u5145\u5206\u7684\u3002"}}
{"id": "2602.20839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20839", "abs": "https://arxiv.org/abs/2602.20839", "authors": ["Niki Foteinopoulou", "Ignas Budvytis", "Stephan Liwicki"], "title": "Training-Free Multi-Concept Image Editing", "comment": "17 pages, 13 figures", "summary": "Editing images with diffusion models without training remains challenging. While recent optimisation-based methods achieve strong zero-shot edits from text, they struggle to preserve identity or capture details that language alone cannot express. Many visual concepts such as facial structure, material texture, or object geometry are impossible to express purely through text prompts alone. To address this gap, we introduce a training-free framework for concept-based image editing, which unifies Optimised DDS with LoRA-driven concept composition, where the training data of the LoRA represent the concept. Our approach enables combining and controlling multiple visual concepts directly within the diffusion process, integrating semantic guidance from text with low-level cues from pretrained concept adapters. We further refine DDS for stability and controllability through ordered timesteps, regularisation, and negative-prompt guidance. Quantitative and qualitative results demonstrate consistent improvements over existing training-free diffusion editing methods on InstructPix2Pix and ComposLoRA benchmarks. Code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e\u6982\u5ff5\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u7ed3\u5408\u4f18\u5316DDS\u4e0eLoRA\u6982\u5ff5\u7ec4\u5408\uff0c\u5b9e\u73b0\u591a\u89c6\u89c9\u6982\u5ff5\u7684\u7ec4\u5408\u63a7\u5236", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff1a\u4f18\u5316\u65b9\u6cd5\u867d\u80fd\u5b9e\u73b0\u96f6\u6837\u672c\u7f16\u8f91\uff0c\u4f46\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u6216\u6355\u6349\u8bed\u8a00\u65e0\u6cd5\u8868\u8fbe\u7684\u89c6\u89c9\u6982\u5ff5\uff08\u5982\u9762\u90e8\u7ed3\u6784\u3001\u6750\u8d28\u7eb9\u7406\u3001\u51e0\u4f55\u5f62\u72b6\u7b49\uff09", "method": "\u8bad\u7ec3\u514d\u8d39\u7684\u6982\u5ff5\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u7edf\u4e00\u4f18\u5316DDS\u4e0eLoRA\u9a71\u52a8\u7684\u6982\u5ff5\u7ec4\u5408\uff0c\u5176\u4e2dLoRA\u7684\u8bad\u7ec3\u6570\u636e\u4ee3\u8868\u6982\u5ff5\uff1b\u901a\u8fc7\u6709\u5e8f\u65f6\u95f4\u6b65\u3001\u6b63\u5219\u5316\u548c\u8d1f\u63d0\u793a\u6307\u5bfc\u6539\u8fdbDDS\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u63a7\u6027", "result": "\u5728InstructPix2Pix\u548cComposLoRA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u8bad\u7ec3\u514d\u8d39\u6269\u6563\u7f16\u8f91\u65b9\u6cd5\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u5747\u663e\u793a\u4e00\u81f4\u6539\u8fdb", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8bed\u8a00\u65e0\u6cd5\u8868\u8fbe\u7684\u89c6\u89c9\u6982\u5ff5\u7f16\u8f91\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8bed\u4e49\u6587\u672c\u6307\u5bfc\u4e0e\u9884\u8bad\u7ec3\u6982\u5ff5\u9002\u914d\u5668\u4f4e\u7ea7\u7ebf\u7d22\u7684\u96c6\u6210\uff0c\u4ee3\u7801\u5c06\u516c\u5f00"}}
{"id": "2602.20911", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20911", "abs": "https://arxiv.org/abs/2602.20911", "authors": ["Ruiqi Liu", "Boyu Diao", "Hangda Liu", "Zhulin An", "Fei Wang", "Yongjun Xu"], "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning", "comment": null, "summary": "Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.", "AI": {"tldr": "SAEF\u662f\u4e00\u79cd\u65b0\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9002\u914d\u5668\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u5c42\u6b21\u6765\u6539\u5584\u77e5\u8bc6\u5171\u4eab\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u65b0\u7c7b\u5b66\u4e60\u540c\u65f6\u907f\u514d\u65e7\u7c7b\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u8fd9\u79cd\u65b9\u6cd5\u867d\u7136\u9632\u6b62\u4e86\u9057\u5fd8\uff0c\u4f46\u5c06\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u89c6\u4e3a\u7b80\u5355\u7684\u3001\u975e\u7ed3\u6784\u5316\u7684\u96c6\u5408\uff0c\u672a\u80fd\u5229\u7528\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u4e13\u5bb6\u68ee\u6797\uff08SAEF\uff09\u65b9\u6cd5\uff1a\u9996\u5148\u57fa\u4e8e\u8bed\u4e49\u5173\u7cfb\u5c06\u4efb\u52a1\u5206\u7ec4\u4e3a\u6982\u5ff5\u7c07\uff1b\u7136\u540e\u5728\u6bcf\u4e2a\u7c07\u5185\uff0c\u901a\u8fc7\u5408\u5e76\u76f8\u4f3c\u4efb\u52a1\u7684\u9002\u914d\u5668\u6784\u5efa\u5e73\u8861\u7684\u4e13\u5bb6\u6811\uff1b\u63a8\u7406\u65f6\uff0c\u4e3a\u7ed9\u5b9a\u8f93\u5165\u627e\u5230\u5e76\u6fc0\u6d3b\u68ee\u6797\u4e2d\u7684\u76f8\u5173\u4e13\u5bb6\u96c6\uff1b\u6700\u7ec8\u9884\u6d4b\u901a\u8fc7\u7ec4\u5408\u8fd9\u4e9b\u6fc0\u6d3b\u4e13\u5bb6\u7684\u8f93\u51fa\uff0c\u5e76\u6839\u636e\u6bcf\u4e2a\u4e13\u5bb6\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5f97\u5230\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAEF\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SAEF\u901a\u8fc7\u7ed3\u6784\u5316\u7ec4\u7ec7\u9002\u914d\u5668\u5c42\u6b21\uff0c\u6709\u6548\u5229\u7528\u4e86\u4efb\u52a1\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u77e5\u8bc6\u5171\u4eab\u548c\u7c7b\u589e\u91cf\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2602.20932", "categories": ["cs.LG", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.20932", "abs": "https://arxiv.org/abs/2602.20932", "authors": ["Anupam Sharma", "Harish Katti", "Prajwal Singh", "Shanmuganathan Raman", "Krishna Miyapuram"], "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels", "comment": null, "summary": "An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations may exist. In this work, we investigate whether EEG captures object representations across multiple hierarchical levels, and propose episodic analysis, in which a Machine Learning (ML) model is evaluated across various, yet related, classification tasks (episodes). Unlike prior episodic EEG studies that rely on fixed or randomly sampled classes of equal cardinality, we adopt hierarchy-aware episode sampling using WordNet to generate episodes with variable classes of diverse hierarchy. We also present the largest episodic framework in the EEG domain for detecting observed text from EEG signals in the PEERS dataset, comprising $931538$ EEG samples under $1610$ object labels, acquired from $264$ human participants (subjects) performing controlled cognitive tasks, enabling the study of neural dynamics underlying perception, decision-making, and performance monitoring.\n  We examine how the semantic abstraction level affects classification performance across multiple learning techniques and architectures, providing a comprehensive analysis. The models tend to improve performance when the classification categories are drawn from higher levels of the hierarchy, suggesting sensitivity to abstraction. Our work highlights abstraction depth as an underexplored dimension of EEG decoding and motivates future research in this direction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u8111\u7535\u56fe\uff08EEG\uff09\u662f\u5426\u80fd\u5728\u591a\u4e2a\u5c42\u6b21\u7ea7\u522b\u4e0a\u6355\u83b7\u7269\u4f53\u8868\u5f81\uff0c\u63d0\u51fa\u4f7f\u7528WordNet\u8fdb\u884c\u5c42\u6b21\u611f\u77e5\u7684\u7247\u6bb5\u91c7\u6837\u65b9\u6cd5\uff0c\u5728PEERS\u6570\u636e\u96c6\u4e0a\u6784\u5efa\u4e86\u6700\u5927\u7684EEG\u7247\u6bb5\u5206\u6790\u6846\u67b6\uff0c\u53d1\u73b0\u5206\u7c7b\u7c7b\u522b\u6765\u81ea\u66f4\u9ad8\u5c42\u6b21\u65f6\u6a21\u578b\u6027\u80fd\u66f4\u597d\u3002", "motivation": "EEG\u4fe1\u53f7\u4fe1\u566a\u6bd4\u4f4e\uff0c\u8bc6\u522b\u5927\u91cf\u7c7b\u522b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8868\u5f81\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u53ef\u80fd\u5b58\u5728\u62bd\u8c61\u7ea7\u522b\u7684\u7269\u4f53\u8868\u5f81\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22EEG\u662f\u5426\u80fd\u5728\u591a\u4e2a\u5c42\u6b21\u7ea7\u522b\u4e0a\u6355\u83b7\u7269\u4f53\u8868\u5f81\uff0c\u5e76\u7814\u7a76\u8bed\u4e49\u62bd\u8c61\u7ea7\u522b\u5982\u4f55\u5f71\u54cd\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7247\u6bb5\u5206\u6790\u6846\u67b6\uff0c\u4f7f\u7528WordNet\u8fdb\u884c\u5c42\u6b21\u611f\u77e5\u7684\u7247\u6bb5\u91c7\u6837\uff0c\u751f\u6210\u5177\u6709\u53ef\u53d8\u7c7b\u522b\u548c\u4e0d\u540c\u5c42\u6b21\u7684\u7247\u6bb5\u3002\u5728PEERS\u6570\u636e\u96c6\uff08\u5305\u542b264\u540d\u53c2\u4e0e\u8005\u7684931,538\u4e2aEEG\u6837\u672c\uff0c1610\u4e2a\u7269\u4f53\u6807\u7b7e\uff09\u4e0a\u6784\u5efa\u6700\u5927\u7684EEG\u7247\u6bb5\u5206\u6790\u6846\u67b6\uff0c\u7814\u7a76\u795e\u7ecf\u52a8\u6001\u3002", "result": "\u6a21\u578b\u5728\u5206\u7c7b\u7c7b\u522b\u6765\u81ea\u66f4\u9ad8\u5c42\u6b21\u65f6\u6027\u80fd\u66f4\u597d\uff0c\u8868\u660e\u5bf9\u62bd\u8c61\u7ea7\u522b\u7684\u654f\u611f\u6027\u3002\u7814\u7a76\u63ed\u793a\u4e86\u62bd\u8c61\u6df1\u5ea6\u4f5c\u4e3aEEG\u89e3\u7801\u4e2d\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7ef4\u5ea6\u3002", "conclusion": "EEG\u80fd\u591f\u6355\u83b7\u591a\u4e2a\u5c42\u6b21\u7ea7\u522b\u7684\u7269\u4f53\u8868\u5f81\uff0c\u62bd\u8c61\u6df1\u5ea6\u662f\u5f71\u54cdEEG\u89e3\u7801\u6027\u80fd\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.20937", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20937", "abs": "https://arxiv.org/abs/2602.20937", "authors": ["Akshita Gupta", "Marieme Ngom", "Sam Foreman", "Venkatram Vishwanath"], "title": "Extending $\u03bc$P: Spectral Conditions for Feature Learning Across Optimizers", "comment": "10 main pages, 16 appendix pages and 17 figures; Amended version of the publication in 17th International OPT Workshop on Optimization for Machine Learning", "summary": "Several variations of adaptive first-order and second-order optimization methods have been proposed to accelerate and scale the training of large language models. The performance of these optimization routines is highly sensitive to the choice of hyperparameters (HPs), which are computationally expensive to tune for large-scale models. Maximal update parameterization $(\u03bc$P$)$ is a set of scaling rules which aims to make the optimal HPs independent of the model size, thereby allowing the HPs tuned on a smaller (computationally cheaper) model to be transferred to train a larger, target model. Despite promising results for SGD and Adam, deriving $\u03bc$P for other optimizers is challenging because the underlying tensor programming approach is difficult to grasp. Building on recent work that introduced spectral conditions as an alternative to tensor programs, we propose a novel framework to derive $\u03bc$P for a broader class of optimizers, including AdamW, ADOPT, LAMB, Sophia, Shampoo and Muon. We implement our $\u03bc$P derivations on multiple benchmark models and demonstrate zero-shot learning rate transfer across increasing model width for the above optimizers. Further, we provide empirical insights into depth-scaling parameterization for these optimizers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u6761\u4ef6\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u5bfc\u66f4\u5e7f\u6cdb\u4f18\u5316\u5668\u7684\u6700\u5927\u66f4\u65b0\u53c2\u6570\u5316(\u03bcP)\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u578b\u5bbd\u5ea6\u7684\u96f6\u5b66\u4e60\u7387\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u03bcP\u65b9\u6cd5\u4e3b\u8981\u9002\u7528\u4e8eSGD\u548cAdam\uff0c\u4f46\u96be\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u4f18\u5316\u5668\uff0c\u56e0\u4e3a\u5f20\u91cf\u7f16\u7a0b\u65b9\u6cd5\u96be\u4ee5\u7406\u89e3\u3002\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6846\u67b6\u6765\u4e3a\u5404\u79cd\u4f18\u5316\u5668\u63a8\u5bfc\u03bcP\u89c4\u5219\uff0c\u4ece\u800c\u51cf\u5c11\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u6210\u672c\u3002", "method": "\u57fa\u4e8e\u8c31\u6761\u4ef6\u66ff\u4ee3\u5f20\u91cf\u7f16\u7a0b\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u65b0\u6846\u67b6\u6765\u63a8\u5bfc\u03bcP\u89c4\u5219\u3002\u8be5\u6846\u67b6\u9002\u7528\u4e8eAdamW\u3001ADOPT\u3001LAMB\u3001Sophia\u3001Shampoo\u548cMuon\u7b49\u591a\u79cd\u4f18\u5316\u5668\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6a21\u578b\u4e0a\u5b9e\u73b0\u03bcP\u63a8\u5bfc\uff0c\u5e76\u8fdb\u884c\u96f6\u5b66\u4e60\u7387\u8fc1\u79fb\u5b9e\u9a8c\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e0a\u8ff0\u4f18\u5316\u5668\u7684\u03bcP\u63a8\u5bfc\uff0c\u5e76\u5728\u8de8\u6a21\u578b\u5bbd\u5ea6\u4e0a\u5c55\u793a\u4e86\u96f6\u5b66\u4e60\u7387\u8fc1\u79fb\u80fd\u529b\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u8fd9\u4e9b\u4f18\u5316\u5668\u6df1\u5ea6\u7f29\u653e\u53c2\u6570\u5316\u7684\u7ecf\u9a8c\u89c1\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u8c31\u6761\u4ef6\u7684\u65b0\u6846\u67b6\u80fd\u591f\u4e3a\u66f4\u5e7f\u6cdb\u7684\u4f18\u5316\u5668\u63a8\u5bfc\u03bcP\u89c4\u5219\uff0c\u6709\u6548\u51cf\u5c11\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u6210\u672c\uff0c\u5b9e\u73b0\u4ece\u5c0f\u6a21\u578b\u5230\u5927\u6a21\u578b\u7684\u8d85\u53c2\u6570\u8fc1\u79fb\u3002"}}
{"id": "2602.20947", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20947", "abs": "https://arxiv.org/abs/2602.20947", "authors": ["Thorbj\u00f8rn Mosekj\u00e6r Iversen", "Zebin Duan", "Frederik Hagelskj\u00e6r"], "title": "Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation", "comment": null, "summary": "The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.", "AI": {"tldr": "\u63d0\u51faWilson Score\u6838\u5bc6\u5ea6\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e8c\u5143\u5206\u7c7b\u4e2d\u7684\u7f6e\u4fe1\u533a\u95f4\u4f30\u8ba1\uff0c\u5728\u9009\u62e9\u6027\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u5206\u7c7b\u76f8\u5f53\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e8c\u5143\u5206\u7c7b\u5668\u6027\u80fd\u63d0\u5347\u4f7f\u5176\u53ef\u7528\u4e8e\u5173\u952e\u68c0\u6d4b\u4efb\u52a1\uff0c\u4f46\u8fd9\u4e9b\u5173\u952e\u5e94\u7528\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u533a\u95f4\u4f30\u8ba1\u4ee5\u786e\u4fdd\u7cfb\u7edf\u6027\u80fd\u8fbe\u5230\u7279\u5b9a\u7edf\u8ba1\u663e\u8457\u6027\u6c34\u5e73\u3002", "method": "\u63d0\u51faWilson Score\u6838\u5bc6\u5ea6\u5206\u7c7b\u65b9\u6cd5\uff0c\u6838\u5fc3\u662fWilson Score\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u4f30\u8ba1\u6761\u4ef6\u53d8\u5316\u6210\u529f\u6982\u7387\u7684\u4e8c\u9879\u5b9e\u9a8c\u4e2d\u7684\u7f6e\u4fe1\u533a\u95f4\u3002\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u4efb\u4f55\u7279\u5f81\u63d0\u53d6\u5668\uff08\u5305\u62ec\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff09\u7684\u5206\u7c7b\u5934\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u7684\u9009\u62e9\u6027\u5206\u7c7b\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u5206\u7c7b\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "Wilson Score\u6838\u5bc6\u5ea6\u5206\u7c7b\u4e3a\u4e8c\u5143\u5206\u7c7b\u4e2d\u7684\u7f6e\u4fe1\u533a\u95f4\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u5173\u952e\u68c0\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2602.20971", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20971", "abs": "https://arxiv.org/abs/2602.20971", "authors": ["Himadri Mandal", "Vishnu Varadarajan", "Jaee Ponde", "Aritra Das", "Mihir More", "Debayan Gupta"], "title": "Does Order Matter : Connecting The Law of Robustness to Robust Generalization", "comment": null, "summary": "Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function to be Lipschitz. Robust generalization asks whether small robust training loss implies small robust test loss. We resolve this problem by explicitly connecting the two for arbitrary data distributions. Specifically, we introduce a nontrivial notion of robust generalization error and convert it into a lower bound on the expected Rademacher complexity of the induced robust loss class. Our bounds recover the $\u03a9(n^{1/d})$ regime of Wu et al.\\ (2023) and show that, up to constants, robust generalization does not change the order of the Lipschitz constant required for smooth interpolation. We conduct experiments to probe the predicted scaling with dataset size and model capacity, testing whether empirical behavior aligns more closely with the predictions of Bubeck and Sellke (2021) or Wu et al.\\ (2023). For MNIST, we find that the lower-bound Lipschitz constant scales on the order predicted by Wu et al.\\ (2023). Informally, to obtain low robust generalization error, the Lipschitz constant must lie in a range that we bound, and the allowable perturbation radius is linked to the Lipschitz scale.", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86Bubeck\u548cSellke\u63d0\u51fa\u7684\u9c81\u68d2\u6027\u5b9a\u5f8b\u4e0e\u9c81\u68d2\u6cdb\u5316\u4e4b\u95f4\u7684\u5f00\u653e\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u4e24\u8005\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3Bubeck\u548cSellke\u63d0\u51fa\u7684\u5f00\u653e\u95ee\u9898\uff1a\u5efa\u7acb\u9c81\u68d2\u6027\u5b9a\u5f8b\u4e0e\u9c81\u68d2\u6cdb\u5316\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002\u9c81\u68d2\u6027\u5b9a\u5f8b\u6307\u51fa\u8fc7\u53c2\u6570\u5316\u662f\u5b9e\u73b0\u9c81\u68d2\u63d2\u503c\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u800c\u9c81\u68d2\u6cdb\u5316\u5173\u6ce8\u5c0f\u9c81\u68d2\u8bad\u7ec3\u635f\u5931\u662f\u5426\u80fd\u4fdd\u8bc1\u5c0f\u9c81\u68d2\u6d4b\u8bd5\u635f\u5931\u3002", "method": "\u5f15\u5165\u975e\u5e73\u51e1\u7684\u9c81\u68d2\u6cdb\u5316\u8bef\u5dee\u6982\u5ff5\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u8bf1\u5bfc\u9c81\u68d2\u635f\u5931\u7c7b\u7684\u671f\u671bRademacher\u590d\u6742\u5ea6\u7684\u4e0b\u754c\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6062\u590dWu\u7b49\u4eba\u63d0\u51fa\u7684\u03a9(n^{1/d})\u673a\u5236\uff0c\u5e76\u8bc1\u660e\u9c81\u68d2\u6cdb\u5316\u4e0d\u4f1a\u6539\u53d8\u5e73\u6ed1\u63d2\u503c\u6240\u9700\u7684Lipschitz\u5e38\u6570\u7684\u9636\u6570\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u4e3a\u83b7\u5f97\u4f4e\u9c81\u68d2\u6cdb\u5316\u8bef\u5dee\uff0cLipschitz\u5e38\u6570\u5fc5\u987b\u4f4d\u4e8e\u4e00\u4e2a\u6709\u754c\u7684\u8303\u56f4\u5185\uff0c\u4e14\u5141\u8bb8\u7684\u6270\u52a8\u534a\u5f84\u4e0eLipschitz\u5c3a\u5ea6\u76f8\u5173\u3002\u5728MNIST\u4e0a\u7684\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4e0b\u754cLipschitz\u5e38\u6570\u7684\u7f29\u653e\u987a\u5e8f\u4e0eWu\u7b49\u4eba\u7684\u9884\u6d4b\u4e00\u81f4\u3002", "conclusion": "\u6210\u529f\u89e3\u51b3\u4e86\u9c81\u68d2\u6027\u5b9a\u5f8b\u4e0e\u9c81\u68d2\u6cdb\u5316\u4e4b\u95f4\u7684\u5f00\u653e\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u7406\u8bba\u8054\u7cfb\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u7814\u7a76\u663e\u793a\u9c81\u68d2\u6cdb\u5316\u4e0d\u4f1a\u6539\u53d8\u5e73\u6ed1\u63d2\u503c\u6240\u9700\u7684Lipschitz\u5e38\u6570\u7684\u9636\u6570\uff0c\u4e3a\u7406\u89e3\u9c81\u68d2\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2602.20880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20880", "abs": "https://arxiv.org/abs/2602.20880", "authors": ["Yongli Xiang", "Ziming Hong", "Zhaoqing Wang", "Xiangyu Zhao", "Bo Han", "Tongliang Liu"], "title": "When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance", "comment": "CVPR 2026; Code is released at https://github.com/tmllab/2026_CVPR_CASG", "summary": "Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCASG\u6846\u67b6\u89e3\u51b3T2I\u6269\u6563\u6a21\u578b\u4e2d\u591a\u7c7b\u522b\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u7684\u5b89\u5168\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u6700\u76f8\u5173\u7684\u6709\u5bb3\u7c7b\u522b\u5e76\u9488\u5bf9\u6027\u5e94\u7528\u5b89\u5168\u5f15\u5bfc\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c06\u6709\u5bb3\u7387\u964d\u4f4e15.4%", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b89\u5168\u5f15\u5bfc\u7684T2I\u6269\u6563\u6a21\u578b\u5b89\u5168\u65b9\u6cd5\u5728\u5904\u7406\u591a\u7c7b\u522b\u6709\u5bb3\u5185\u5bb9\u65f6\u5b58\u5728\"\u6709\u5bb3\u51b2\u7a81\"\u95ee\u9898\u2014\u2014\u6291\u5236\u4e00\u79cd\u6709\u5bb3\u7c7b\u522b\u53ef\u80fd\u65e0\u610f\u4e2d\u653e\u5927\u53e6\u4e00\u79cd\u6709\u5bb3\u7c7b\u522b\uff0c\u5bfc\u81f4\u603b\u4f53\u6709\u5bb3\u7387\u53cd\u800c\u589e\u52a0", "method": "\u63d0\u51fa\u51b2\u7a81\u611f\u77e5\u81ea\u9002\u5e94\u5b89\u5168\u5f15\u5bfc(CASG)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1)\u51b2\u7a81\u611f\u77e5\u7c7b\u522b\u8bc6\u522b(CaCI)\uff0c\u6839\u636e\u6a21\u578b\u751f\u6210\u72b6\u6001\u52a8\u6001\u8bc6\u522b\u6700\u76f8\u5173\u7684\u6709\u5bb3\u7c7b\u522b\uff1b2)\u51b2\u7a81\u89e3\u51b3\u5f15\u5bfc\u5e94\u7528(CrGA)\uff0c\u4ec5\u6cbf\u8bc6\u522b\u51fa\u7684\u7c7b\u522b\u65b9\u5411\u5e94\u7528\u5b89\u5168\u5f15\u5bfc\uff0c\u907f\u514d\u591a\u7c7b\u522b\u5e72\u6270", "result": "\u5728T2I\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCASG\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c06\u6709\u5bb3\u7387\u964d\u4f4e\u9ad8\u8fbe15.4%\uff0c\u53ef\u5e94\u7528\u4e8e\u6f5c\u5728\u7a7a\u95f4\u548c\u6587\u672c\u7a7a\u95f4\u7684\u5b89\u5168\u4fdd\u969c", "conclusion": "CASG\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86T2I\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6709\u5bb3\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u548c\u9488\u5bf9\u6027\u5f15\u5bfc\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u7684\u5185\u5bb9\u751f\u6210\uff0c\u4e3a\u591a\u7c7b\u522b\u6709\u5bb3\u5185\u5bb9\u7684\u5b89\u5168\u9632\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2602.21020", "categories": ["cs.LG", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21020", "abs": "https://arxiv.org/abs/2602.21020", "authors": ["Antoine Bergerault", "Volkan Cevher", "Negar Mehr"], "title": "Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning", "comment": null, "summary": "Multi-agent imitation learning (MA-IL) aims to learn optimal policies from expert demonstrations of interactions in multi-agent interactive domains. Despite existing guarantees on the performance of the resulting learned policies, characterizations of how far the learned polices are from a Nash equilibrium are missing for offline MA-IL. In this paper, we demonstrate impossibility and hardness results of learning low-exploitable policies in general $n$-player Markov Games. We do so by providing examples where even exact measure matching fails, and demonstrating a new hardness result on characterizing the Nash gap given a fixed measure matching error. We then show how these challenges can be overcome using strategic dominance assumptions on the expert equilibrium. Specifically, for the case of dominant strategy expert equilibria, assuming Behavioral Cloning error $\u03b5_{\\text{BC}}$, this provides a Nash imitation gap of $\\mathcal{O}\\left(n\u03b5_{\\text{BC}}/(1-\u03b3)^2\\right)$ for a discount factor $\u03b3$. We generalize this result with a new notion of best-response continuity, and argue that this is implicitly encouraged by standard regularization techniques.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u5728\u79bb\u7ebf\u8bbe\u7f6e\u4e0b\u96be\u4ee5\u4fdd\u8bc1\u5b66\u4e60\u5230\u7684\u7b56\u7565\u63a5\u8fd1\u7eb3\u4ec0\u5747\u8861\uff0c\u672c\u6587\u5c55\u793a\u4e86\u4e0d\u53ef\u80fd\u6027\u548c\u56f0\u96be\u6027\u7ed3\u679c\uff0c\u4f46\u5728\u4e13\u5bb6\u5747\u8861\u5177\u6709\u6218\u7565\u4f18\u52bf\u5047\u8bbe\u4e0b\uff0c\u53ef\u4ee5\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u5e76\u83b7\u5f97\u7eb3\u4ec0\u6a21\u4eff\u5dee\u8ddd\u7684\u754c\u9650\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u867d\u7136\u80fd\u4fdd\u8bc1\u5b66\u4e60\u7b56\u7565\u7684\u6027\u80fd\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5b66\u4e60\u7b56\u7565\u4e0e\u7eb3\u4ec0\u5747\u8861\u4e4b\u95f4\u8ddd\u79bb\u7684\u523b\u753b\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u9996\u5148\u5c55\u793a\u4e00\u822cn\u4eba\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u5b66\u4e60\u4f4e\u53ef\u5265\u524a\u7b56\u7565\u7684\u4e0d\u53ef\u80fd\u6027\u548c\u56f0\u96be\u6027\u7ed3\u679c\uff0c\u5305\u62ec\u7cbe\u786e\u5ea6\u91cf\u5339\u914d\u5931\u8d25\u7684\u4f8b\u5b50\u548c\u56fa\u5b9a\u5ea6\u91cf\u5339\u914d\u8bef\u5dee\u4e0b\u523b\u753b\u7eb3\u4ec0\u5dee\u8ddd\u7684\u56f0\u96be\u6027\u8bc1\u660e\u3002\u7136\u540e\u5f15\u5165\u6218\u7565\u4f18\u52bf\u5047\u8bbe\uff08\u4e3b\u5bfc\u7b56\u7565\u4e13\u5bb6\u5747\u8861\uff09\uff0c\u4f7f\u7528\u884c\u4e3a\u514b\u9686\u8bef\u5dee\u03b5_BC\uff0c\u63a8\u5bfc\u51fa\u7eb3\u4ec0\u6a21\u4eff\u5dee\u8ddd\u754c\u9650\u3002\u6700\u540e\u63a8\u5e7f\u5230\u65b0\u7684\u6700\u4f73\u54cd\u5e94\u8fde\u7eed\u6027\u6982\u5ff5\u3002", "result": "\u5bf9\u4e8e\u4e3b\u5bfc\u7b56\u7565\u4e13\u5bb6\u5747\u8861\u60c5\u51b5\uff0c\u5047\u8bbe\u884c\u4e3a\u514b\u9686\u8bef\u5dee\u03b5_BC\uff0c\u83b7\u5f97\u4e86\u7eb3\u4ec0\u6a21\u4eff\u5dee\u8ddd\u4e3aO(n\u03b5_BC/(1-\u03b3)^2)\u7684\u754c\u9650\uff0c\u5176\u4e2d\u03b3\u4e3a\u6298\u6263\u56e0\u5b50\u3002\u901a\u8fc7\u6700\u4f73\u54cd\u5e94\u8fde\u7eed\u6027\u6982\u5ff5\u63a8\u5e7f\u4e86\u8fd9\u4e00\u7ed3\u679c\uff0c\u5e76\u8bba\u8bc1\u6807\u51c6\u6b63\u5219\u5316\u6280\u672f\u9690\u5f0f\u9f13\u52b1\u8fd9\u79cd\u8fde\u7eed\u6027\u3002", "conclusion": "\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u6a21\u4eff\u5b66\u4e60\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u96be\u4ee5\u4fdd\u8bc1\u5b66\u4e60\u7b56\u7565\u63a5\u8fd1\u7eb3\u4ec0\u5747\u8861\uff0c\u4f46\u5728\u4e13\u5bb6\u5747\u8861\u5177\u6709\u6218\u7565\u4f18\u52bf\u7684\u5408\u7406\u5047\u8bbe\u4e0b\uff0c\u53ef\u4ee5\u83b7\u5f97\u7406\u8bba\u4fdd\u8bc1\u3002\u6700\u4f73\u54cd\u5e94\u8fde\u7eed\u6027\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u4e14\u4e0e\u73b0\u6709\u6b63\u5219\u5316\u6280\u672f\u517c\u5bb9\u3002"}}
{"id": "2602.21158", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21158", "abs": "https://arxiv.org/abs/2602.21158", "authors": ["Dengjia Zhang", "Xiaoou Liu", "Lu Cheng", "Yaqing Wang", "Kenton Murray", "Hua Wei"], "title": "SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.", "AI": {"tldr": "SELAUR\u662f\u4e00\u4e2a\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5956\u52b1\u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u7684LLM\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06LLM\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u878d\u5165\u5956\u52b1\u8bbe\u8ba1\uff0c\u5728ALFWorld\u548cWebShop\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u591a\u6b65\u51b3\u7b56\u667a\u80fd\u4f53\u90e8\u7f72\u65f6\uff0c\u5956\u52b1\u8bbe\u8ba1\u5bf9\u6307\u5bfc\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22\u4e86\u5404\u79cd\u5956\u52b1\u5851\u9020\u548c\u6b65\u7ea7\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\uff0c\u4f46LLM\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u8fd9\u4e00\u5173\u952e\u4fe1\u53f7\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u88ab\u5ffd\u89c6\u4e86\u3002\u4e0d\u786e\u5b9a\u6027\u53cd\u6620\u4e86\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u63ed\u793a\u4e86\u9700\u8981\u63a2\u7d22\u7684\u533a\u57df\uff0c\u5373\u4f7f\u5728\u5931\u8d25\u8f68\u8ff9\u4e2d\u4e5f\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b66\u4e60\u7ebf\u7d22\u3002", "method": "SELAUR\u6846\u67b6\u5c06\u4e0d\u786e\u5b9a\u6027\u76f4\u63a5\u878d\u5165\u5956\u52b1\u8bbe\u8ba1\uff1a1\uff09\u6574\u5408\u57fa\u4e8e\u71b5\u3001\u6700\u5c0f\u7f6e\u4fe1\u5ea6\u548c\u8fb9\u754c\u7684\u6307\u6807\uff0c\u5f62\u6210\u7ec4\u5408\u7684\u4ee4\u724c\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63d0\u4f9b\u5bc6\u96c6\u7684\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u76d1\u7763\uff1b2\uff09\u91c7\u7528\u5931\u8d25\u611f\u77e5\u7684\u5956\u52b1\u91cd\u5851\u673a\u5236\uff0c\u5c06\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u6ce8\u5165\u6b65\u7ea7\u548c\u8f68\u8ff9\u7ea7\u5956\u52b1\uff0c\u4ee5\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u5b66\u4e60\u7a33\u5b9a\u6027\u3002", "result": "\u5728ALFWorld\u548cWebShop\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u6301\u7eed\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u5982\u4f55\u589e\u5f3a\u63a2\u7d22\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SELAUR\u901a\u8fc7\u5c06LLM\u7684\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u878d\u5165\u5956\u52b1\u8bbe\u8ba1\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u3002\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u63a2\u7d22\u6548\u7387\u548c\u5b66\u4e60\u7a33\u5b9a\u6027\uff0c\u8fd8\u5728\u5931\u8d25\u8f68\u8ff9\u4e2d\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b66\u4e60\u7ebf\u7d22\uff0c\u6700\u7ec8\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.21168", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21168", "abs": "https://arxiv.org/abs/2602.21168", "authors": ["Jingya Cheng", "Alaleh Azhir", "Jiazi Tian", "Hossein Estiri"], "title": "Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma", "comment": null, "summary": "Counterfactual inference enables clinicians to ask \"what if\" questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. Applied to 2,723 COVID-19 patients (383 Long COVID heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. We identify a cardiorenal cascade (CKD -> AKI -> HF) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. Our framework transforms counterfactual explanation from \"what if this feature were different?\" to \"what if we had intervened earlier, and how would that propagate forward?\" --  yielding clinically actionable insights grounded in biological plausibility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5e8f\u5217\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u533a\u5206\u4e0d\u53ef\u53d8\u7279\u5f81\u548c\u53ef\u63a7\u7279\u5f81\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u4e2d\u8fdd\u53cd\u751f\u7269\u5b66\u53ef\u80fd\u6027\u7684\u53cd\u4e8b\u5b9e\u63a8\u65ad\u3002", "motivation": "\u4f20\u7edf\u53cd\u4e8b\u5b9e\u63a8\u65ad\u65b9\u6cd5\u5047\u8bbe\u7279\u5f81\u72ec\u7acb\u548c\u540c\u65f6\u53ef\u4fee\u6539\u6027\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u5728\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u4e2d\u4e0d\u6210\u7acb\uff0c\u5bfc\u81f4\u4ea7\u751f\u751f\u7269\u5b66\u4e0a\u4e0d\u53ef\u80fd\u7684\u53cd\u4e8b\u5b9e\u7ed3\u679c\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u5e8f\u5217\u53cd\u4e8b\u5b9e\u6846\u67b6\uff0c\u533a\u5206\u4e0d\u53ef\u53d8\u7279\u5f81\uff08\u5982\u6162\u6027\u8bca\u65ad\uff09\u548c\u53ef\u63a7\u7279\u5f81\uff08\u5982\u5b9e\u9a8c\u5ba4\u6570\u503c\uff09\uff0c\u5efa\u6a21\u5e72\u9884\u63aa\u65bd\u968f\u65f6\u95f4\u4f20\u64ad\u7684\u673a\u5236\uff0c\u5c0a\u91cd\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5e94\u7528\u4e8e2,723\u540dCOVID-19\u60a3\u8005\uff08383\u4f8b\u957f\u65b0\u51a0\u5fc3\u529b\u8870\u7aed\u75c5\u4f8b\uff0c2,340\u540d\u5339\u914d\u5bf9\u7167\uff09\uff0c\u53d1\u73b038-67%\u6162\u6027\u75c5\u60a3\u8005\u5728\u4f20\u7edf\u65b9\u6cd5\u4e0b\u9700\u8981\u751f\u7269\u5b66\u4e0a\u4e0d\u53ef\u80fd\u7684\u53cd\u4e8b\u5b9e\u3002\u8bc6\u522b\u51fa\u5fc3\u80be\u7ea7\u8054\uff08CKD -> AKI -> HF\uff09\uff0c\u76f8\u5bf9\u98ce\u9669\u5206\u522b\u4e3a2.27\u548c1.19\uff0c\u5c55\u793a\u4e86\u5e8f\u5217\u53cd\u4e8b\u5b9e\u80fd\u6355\u6349\u7684\u65f6\u95f4\u4f20\u64ad\u6548\u5e94\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4ece\"\u5982\u679c\u8fd9\u4e2a\u7279\u5f81\u4e0d\u540c\u4f1a\u600e\u6837\uff1f\"\u8f6c\u53d8\u4e3a\"\u5982\u679c\u6211\u4eec\u66f4\u65e9\u5e72\u9884\u4f1a\u600e\u6837\uff0c\u4ee5\u53ca\u8fd9\u4f1a\u5982\u4f55\u5411\u524d\u4f20\u64ad\uff1f\"\uff0c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u4e34\u5e8a\u53ef\u884c\u89c1\u89e3\u3002"}}
{"id": "2602.21185", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21185", "abs": "https://arxiv.org/abs/2602.21185", "authors": ["Justin Deschenaux", "Caglar Gulcehre", "Subham Sekhar Sahoo"], "title": "The Diffusion Duality, Chapter II: $\u03a8$-Samplers and Efficient Curriculum", "comment": null, "summary": "Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u9884\u6d4b\u5668-\u6821\u6b63\u5668\u91c7\u6837\u5668\u5bb6\u65cf\uff0c\u8be5\u91c7\u6837\u5668\u80fd\u6301\u7eed\u6539\u8fdb\u91c7\u6837\u8d28\u91cf\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5728\u8bed\u8a00\u548c\u56fe\u50cf\u5efa\u6a21\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u3002", "motivation": "\u5747\u5300\u72b6\u6001\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u5c11\u6b65\u751f\u6210\u548c\u5f15\u5bfc\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f7f\u7528\u7956\u5148\u91c7\u6837\u5668\u65f6\uff0c\u968f\u7740\u6b65\u6570\u589e\u52a0\u91c7\u6837\u8d28\u91cf\u4f1a\u8fbe\u5230\u5e73\u53f0\u671f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u6301\u7eed\u6539\u8fdb\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u8d28\u7591\u63a9\u7801\u6269\u6563\u662f\u5426\u662f\u6269\u6563\u5f0f\u8bed\u8a00\u5efa\u6a21\u7684\u5fc5\u7136\u672a\u6765\u3002", "method": "1. \u63d0\u51fa\u9884\u6d4b\u5668-\u6821\u6b63\u5668\u91c7\u6837\u5668\u5bb6\u65cf\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u566a\u58f0\u8fc7\u7a0b\uff0c\u80fd\u63a8\u5e7f\u5148\u524d\u65b9\u6cd5\uff1b2. \u4e0e\u5747\u5300\u72b6\u6001\u6269\u6563\u7ed3\u5408\u4f7f\u7528\uff1b3. \u5f00\u53d1\u5185\u5b58\u9ad8\u6548\u7684\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u51cf\u5c11\u9ad8\u65af\u677e\u5f1b\u8bad\u7ec3\u9636\u6bb5\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002", "result": "1. \u5728OpenWebText\u4e0a\u5b9e\u73b0\u66f4\u4f4e\u7684\u751f\u6210\u56f0\u60d1\u5ea6\uff08\u5728\u5339\u914d\u7684\u5355\u5b57\u71b5\u4e0b\uff09\uff1b2. \u5728CIFAR10\u4e0a\u83b7\u5f97\u66f4\u597d\u7684FID/IS\u5206\u6570\uff1b3. \u9884\u6d4b\u5668-\u6821\u6b63\u5668\u65b9\u6cd5\u80fd\u968f\u91c7\u6837\u6b65\u6570\u589e\u52a0\u6301\u7eed\u6539\u8fdb\uff1b4. \u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1125%\uff0c\u5185\u5b58\u51cf\u5c1133%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u56f0\u60d1\u5ea6\u548c\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "\u9884\u6d4b\u5668-\u6821\u6b63\u5668\u91c7\u6837\u5668\u5728\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e2d\u663e\u8457\u4f18\u4e8e\u7956\u5148\u91c7\u6837\uff0c\u5176\u6301\u7eed\u6539\u8fdb\u80fd\u529b\u6311\u6218\u4e86\u63a9\u7801\u6269\u6563\u4f5c\u4e3a\u6269\u6563\u5f0f\u8bed\u8a00\u5efa\u6a21\u5fc5\u7136\u672a\u6765\u7684\u5047\u8bbe\u3002\u540c\u65f6\uff0c\u9ad8\u6548\u8bad\u7ec3\u8bfe\u7a0b\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6539\u8fdb\u3002"}}
{"id": "2602.20999", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.20999", "abs": "https://arxiv.org/abs/2602.20999", "authors": ["Bowen Zheng", "Yongli Xiang", "Ziming Hong", "Zerong Lin", "Chaojian Yu", "Tongliang Liu", "Xinge You"], "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models", "comment": "Project page: https://Zbwwwwwwww.github.io/VII", "summary": "Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction Injection (VII), a training-free and transferable jailbreaking framework that intentionally disguises the malicious intent of unsafe text prompts as benign visual instructions in the safe reference image. Specifically, VII coordinates a Malicious Intent Reprogramming module to distill malicious intent from unsafe text prompts while minimizing their static harmfulness, and a Visual Instruction Grounding module to ground the distilled intent onto a safe input image by rendering visual instructions that preserve semantic consistency with the original unsafe text prompt, thereby inducing harmful content during I2V generation. Empirically, our extensive experiments on four state-of-the-art commercial I2V models (Kling-v2.5-turbo, Gemini Veo-3.1, Seedance-1.5-pro, and PixVerse-V5) demonstrate that VII achieves Attack Success Rates of up to 83.5% while reducing Refusal Rates to near zero, significantly outperforming existing baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVisual Instruction Injection (VII)\u7684\u8bad\u7ec3\u514d\u8d39\u3001\u53ef\u8fc1\u79fb\u7684\u8d8a\u72f1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6076\u610f\u6587\u672c\u610f\u56fe\u4f2a\u88c5\u6210\u53c2\u8003\u56fe\u50cf\u4e2d\u7684\u826f\u6027\u89c6\u89c9\u6307\u4ee4\uff0c\u6765\u653b\u51fb\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5177\u6709\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u4f46\u8fd9\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff1a\u653b\u51fb\u8005\u53ef\u80fd\u901a\u8fc7\u56fe\u50cf\u6a21\u6001\u6ce8\u5165\u6076\u610f\u610f\u56fe\u3002\u76ee\u524d\u8fd9\u4e00\u98ce\u9669\u5c1a\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u548c\u7814\u7a76\u3002", "method": "VII\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1) \u6076\u610f\u610f\u56fe\u91cd\u7f16\u7a0b\u6a21\u5757\uff0c\u4ece\u6076\u610f\u6587\u672c\u63d0\u793a\u4e2d\u63d0\u53d6\u610f\u56fe\u540c\u65f6\u6700\u5c0f\u5316\u5176\u9759\u6001\u5371\u5bb3\u6027\uff1b2) \u89c6\u89c9\u6307\u4ee4\u63a5\u5730\u6a21\u5757\uff0c\u5c06\u63d0\u53d6\u7684\u610f\u56fe\u901a\u8fc7\u89c6\u89c9\u6307\u4ee4\u5f62\u5f0f\u5d4c\u5165\u5230\u5b89\u5168\u8f93\u5165\u56fe\u50cf\u4e2d\uff0c\u4fdd\u6301\u4e0e\u539f\u59cb\u6076\u610f\u6587\u672c\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u5546\u4e1aI2V\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0cVII\u653b\u51fb\u6210\u529f\u7387\u6700\u9ad8\u8fbe83.5%\uff0c\u540c\u65f6\u5c06\u62d2\u7edd\u7387\u964d\u81f3\u63a5\u8fd1\u96f6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u89c6\u89c9\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u7684VII\u6846\u67b6\u80fd\u6709\u6548\u7ed5\u8fc7\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2602.21189", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21189", "abs": "https://arxiv.org/abs/2602.21189", "authors": ["Anas Barakat", "Souradip Chakraborty", "Khushbu Pahwa", "Amrit Singh Bedi"], "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training", "comment": null, "summary": "Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u53ef\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e2d\uff0cpass@k\u4f18\u5316\u4e0epass@1\u6027\u80fd\u4e0b\u964d\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u8fd9\u79cd\u51b2\u7a81\u7684\u6839\u6e90\u3002", "motivation": "pass@k\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684\u6027\u80fd\u6307\u6807\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u53d1\u73b0\u4e00\u4e2a\u53cd\u590d\u51fa\u73b0\u7684\u6743\u8861\uff1apass@k\u4f18\u5316\u65f6pass@1\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5f88\u91cd\u8981\uff0c\u56e0\u4e3apass@1\u901a\u5e38\u7531\u4e8e\u5ef6\u8fdf\u3001\u6210\u672c\u3001\u9a8c\u8bc1\u5668\u8986\u76d6\u8303\u56f4\u6709\u9650\u4ee5\u53ca\u9700\u8981\u53ef\u9760\u7684\u5355\u6b21\u56de\u9000\u800c\u6210\u4e3a\u786c\u6027\u7ea6\u675f\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790pass@k\u7b56\u7565\u68af\u5ea6\u4e0epass@1\u68af\u5ea6\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u7814\u7a76\u63d0\u793a\u5e72\u6270\u5982\u4f55\u5bfc\u81f4\u8fd9\u79cd\u6743\u8861\u3002\u5177\u4f53\u5206\u6790\u4e86pass@k\u4f18\u5316\u5982\u4f55\u9690\u5f0f\u5730\u5bf9\u4f4e\u6210\u529f\u7387\u63d0\u793a\u8fdb\u884c\u91cd\u52a0\u6743\uff0c\u4ee5\u53ca\u5f53\u8fd9\u4e9b\u63d0\u793a\u5177\u6709\u8d1f\u5e72\u6270\u65f6\uff0c\u5982\u4f55\u4f7fpass@k\u66f4\u65b0\u65b9\u5411\u504f\u79bbpass@1\u65b9\u5411\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cpass@k\u7b56\u7565\u68af\u5ea6\u53ef\u80fd\u4e0epass@1\u68af\u5ea6\u51b2\u7a81\uff0c\u56e0\u4e3apass@k\u4f18\u5316\u4f1a\u9690\u5f0f\u5730\u5c06\u63d0\u793a\u91cd\u52a0\u6743\u5411\u4f4e\u6210\u529f\u7387\u63d0\u793a\uff1b\u5f53\u8fd9\u4e9b\u63d0\u793a\u5177\u6709\u8d1f\u5e72\u6270\u7279\u6027\u65f6\uff0c\u5b83\u4eec\u7684\u6743\u91cd\u589e\u52a0\u4f1a\u4f7fpass@k\u66f4\u65b0\u65b9\u5411\u504f\u79bbpass@1\u65b9\u5411\u3002\u5728\u53ef\u9a8c\u8bc1\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86pass@k\u4f18\u5316\u4e0epass@1\u6027\u80fd\u4e0b\u964d\u4e4b\u95f4\u6743\u8861\u7684\u7406\u8bba\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u591a\u6837\u672c\u63a8\u7406\u6307\u6807\u4f18\u5316\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u5e73\u8861\u4e0d\u540c\u6027\u80fd\u6307\u6807\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2602.21191", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.21191", "abs": "https://arxiv.org/abs/2602.21191", "authors": ["Ilias Diakonikolas", "Daniel M. Kane"], "title": "Statistical Query Lower Bounds for Smoothed Agnostic Learning", "comment": null, "summary": "We study the complexity of smoothed agnostic learning, recently introduced by~\\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian perturbations of the inputs. Specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. The best known upper bound for this problem relies on $L_1$-polynomial regression and has complexity $d^{\\tilde{O}(1/\u03c3^2) \\log(1/\u03b5)}$, where $\u03c3$ is the smoothing parameter and $\u03b5$ is the excess error. Our main result is a Statistical Query (SQ) lower bound providing formal evidence that this upper bound is close to best possible. In more detail, we show that (even for Gaussian marginals) any SQ algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{\u03a9(1/\u03c3^{2}+\\log(1/\u03b5))}$. This is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. Roughly speaking, we show that applying $L_1$-polynomial regression to a smoothed version of the function is essentially best possible. Our techniques involve finding a moment-matching hard distribution by way of linear programming duality. This dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $L_1$-polynomial regression to work). Our explicit SQ lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5e73\u6ed1\u4e0d\u53ef\u77e5\u5b66\u4e60\u534a\u7a7a\u95f4\u7684\u590d\u6742\u5ea6\uff0c\u8bc1\u660e\u4e86\u73b0\u6709\u57fa\u4e8eL1\u591a\u9879\u5f0f\u56de\u5f52\u7684\u4e0a\u754c\u63a5\u8fd1\u6700\u4f18\uff0c\u5e76\u7ed9\u51fa\u4e86\u9996\u4e2a\u975e\u5e73\u51e1\u7684\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u5e73\u6ed1\u4e0d\u53ef\u77e5\u5b66\u4e60\uff08\u7279\u522b\u662f\u534a\u7a7a\u95f4\u5b66\u4e60\uff09\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u73b0\u6709\u4e0a\u754c\u4e3ad^{\u00d5(1/\u03c3\u00b2)log(1/\u03b5)}\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e2a\u4e0a\u754c\u662f\u5426\u63a5\u8fd1\u6700\u4f18\u3002", "method": "\u4f7f\u7528\u7edf\u8ba1\u67e5\u8be2(SQ)\u4e0b\u754c\u6280\u672f\uff0c\u901a\u8fc7\u7ebf\u6027\u89c4\u5212\u5bf9\u5076\u627e\u5230\u77e9\u5339\u914d\u7684\u56f0\u96be\u5206\u5e03\uff0c\u8be5\u5bf9\u5076\u95ee\u9898\u7b49\u4ef7\u4e8e\u5bfb\u627e\u76ee\u6807\u51fd\u6570\u5e73\u6ed1\u7248\u672c\u7684\u4f4e\u5ea6\u8fd1\u4f3c\u591a\u9879\u5f0f\u3002", "result": "\u8bc1\u660e\u4e86\u4efb\u4f55SQ\u7b97\u6cd5\u90fd\u9700\u8981\u590d\u6742\u5ea6d^{\u03a9(1/\u03c3\u00b2+log(1/\u03b5))}\uff0c\u8fd9\u63a5\u8fd1\u5df2\u77e5\u4e0a\u754c\uff0c\u8868\u660eL1\u591a\u9879\u5f0f\u56de\u5f52\u65b9\u6cd5\u57fa\u672c\u662f\u6700\u4f18\u7684\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5173\u4e8e\u5e73\u6ed1\u4e0d\u53ef\u77e5\u5b66\u4e60\u534a\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u975e\u5e73\u51e1\u4e0b\u754c\uff0c\u8bc1\u660e\u4e86\u73b0\u6709\u4e0a\u754c\u63a5\u8fd1\u6700\u4f18\uff0c\u4e3a\u7406\u89e3\u8be5\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2602.20951", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20951", "abs": "https://arxiv.org/abs/2602.20951", "authors": ["Jaehyun Park", "Minyoung Ahn", "Minkyu Kim", "Jonghyun Lee", "Jae-Gil Lee", "Dongmin Park"], "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis", "comment": null, "summary": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.", "AI": {"tldr": "ArtiAgent\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u5e26\u6709\u4e30\u5bcc\u4f2a\u5f71\u6807\u6ce8\u7684\u56fe\u50cf\u5bf9\uff08\u771f\u5b9e\u56fe\u50cf\u4e0e\u6ce8\u5165\u4f2a\u5f71\u7684\u56fe\u50cf\uff09\uff0c\u7528\u4e8e\u8bad\u7ec3\u4f2a\u5f71\u68c0\u6d4b\u548c\u4fee\u590d\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u53d6\u5f97\u8fdb\u5c55\uff0cAI\u751f\u6210\u56fe\u50cf\u4ecd\u5e38\u5305\u542b\u5f71\u54cd\u771f\u5b9e\u611f\u7684\u89c6\u89c9\u4f2a\u5f71\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u4f2a\u5f71\u6570\u636e\u96c6\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u83b7\u53d6\u53ef\u9760\u7684\u4f2a\u5f71\u6807\u6ce8\u6570\u636e\u3002", "method": "ArtiAgent\u5305\u542b\u4e09\u4e2a\u667a\u80fd\u4f53\uff1a\u611f\u77e5\u667a\u80fd\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\u771f\u5b9e\u56fe\u50cf\u4e2d\u7684\u5b9e\u4f53\u548c\u5b50\u5b9e\u4f53\uff1b\u5408\u6210\u667a\u80fd\u4f53\u901a\u8fc7\u6269\u6563\u53d8\u6362\u5668\u4e2d\u7684\u65b0\u9896patch-wise\u5d4c\u5165\u64cd\u4f5c\uff0c\u4f7f\u7528\u4f2a\u5f71\u6ce8\u5165\u5de5\u5177\u5f15\u5165\u4f2a\u5f71\uff1b\u7b56\u5c55\u667a\u80fd\u4f53\u7b5b\u9009\u5408\u6210\u4f2a\u5f71\u5e76\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u751f\u6210\u5c40\u90e8\u548c\u5168\u5c40\u89e3\u91ca\u3002", "result": "\u4f7f\u7528ArtiAgent\u5408\u6210\u4e8610\u4e07\u5f20\u5e26\u6709\u4e30\u5bcc\u4f2a\u5f71\u6807\u6ce8\u7684\u56fe\u50cf\uff0c\u5e76\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "conclusion": "ArtiAgent\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u751f\u6210\u4f2a\u5f71\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e3a\u4f2a\u5f71\u68c0\u6d4b\u548c\u4fee\u590d\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2602.21196", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21196", "abs": "https://arxiv.org/abs/2602.21196", "authors": ["Ravi Ghadia", "Maksim Abraham", "Sergei Vorobyov", "Max Ryabinin"], "title": "Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking", "comment": "14 pages, 6 figures", "summary": "Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\\times$H100 node, improving upon prior methods by over 25$\\%$.", "AI": {"tldr": "UPipe\u662f\u4e00\u79cd\u65b0\u578b\u4e0a\u4e0b\u6587\u5e76\u884c\u6280\u672f\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5206\u5757\uff0c\u663e\u8457\u964d\u4f4e\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u6fc0\u6d3b\u5185\u5b58\u4f7f\u7528\uff0c\u652f\u6301\u66f4\u957f\u7684\u5e8f\u5217\u957f\u5ea6\u8bad\u7ec3", "motivation": "\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u5e76\u884c\u65b9\u6cd5\uff08\u5982Ring Attention\u3001DeepSpeed Ulysses\uff09\u867d\u7136\u80fd\u5728\u4e0a\u4e0b\u6587\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u6269\u5c55\uff0c\u4f46\u5185\u5b58\u6548\u7387\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u652f\u6301\u7684\u5e8f\u5217\u957f\u5ea6\u3002\u66f4\u5148\u8fdb\u7684\u6280\u672f\uff08\u5982\u5b8c\u5168\u6d41\u6c34\u7ebf\u5206\u5e03\u5f0fTransformer\u6216\u6fc0\u6d3b\u5378\u8f7d\uff09\u867d\u7136\u80fd\u8fdb\u4e00\u6b65\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u4f46\u4f1a\u727a\u7272\u8bad\u7ec3\u541e\u5410\u91cf", "method": "UPipe\u91c7\u7528\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5206\u5757\u6280\u672f\uff0c\u5728\u81ea\u6ce8\u610f\u529b\u5c42\u8fdb\u884c\u7cbe\u7ec6\u7684\u5185\u5b58\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e2d\u95f4\u5f20\u91cf\u7684\u5185\u5b58\u4f7f\u7528", "result": "\u5bf9\u4e8e32B Transformer\u6a21\u578b\uff0cUPipe\u5c06\u6ce8\u610f\u529b\u5c42\u4e2d\u95f4\u5f20\u91cf\u7684\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u4e8687.5%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5148\u524d\u4e0a\u4e0b\u6587\u5e76\u884c\u6280\u672f\u76f8\u5f53\u7684\u8bad\u7ec3\u901f\u5ea6\u3002\u5728\u5355\u4e2a8\u00d7H100\u8282\u70b9\u4e0a\u8bad\u7ec3Llama3-8B\u65f6\uff0c\u652f\u6301500\u4e07token\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u5347\u8d85\u8fc725%", "conclusion": "UPipe\u901a\u8fc7\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u5206\u5757\uff0c\u6709\u6548\u7a81\u7834\u4e86\u6fc0\u6d3b\u5185\u5b58\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u901f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u6269\u5c55\u4e86\u53ef\u652f\u6301\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u4e3a\u5904\u7406\u8d85\u957f\u5e8f\u5217\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u4f18\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.21198", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21198", "abs": "https://arxiv.org/abs/2602.21198", "authors": ["Yining Hong", "Huang Huang", "Manling Li", "Li Fei-Fei", "Jiajun Wu", "Yejin Choi"], "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs", "comment": null, "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53cd\u5c04\u6d4b\u8bd5\u65f6\u95f4\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u884c\u52a8\u4e2d\u53cd\u601d\u548c\u884c\u52a8\u540e\u53cd\u601d\u4e24\u79cd\u6a21\u5f0f\uff0c\u8ba9\u5177\u8eabLLM\u673a\u5668\u4eba\u80fd\u591f\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60\u79ef\u7d2f\u7ecf\u9a8c\uff0c\u800c\u4e0d\u662f\u91cd\u590d\u72af\u9519\u3002", "motivation": "\u5f53\u524d\u5177\u8eabLLM\u867d\u7136\u8d4b\u4e88\u673a\u5668\u4eba\u9ad8\u7ea7\u4efb\u52a1\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u53cd\u601d\u673a\u5236\uff0c\u5bfc\u81f4\u90e8\u7f72\u8fc7\u7a0b\u53d8\u6210\u4e00\u7cfb\u5217\u72ec\u7acb\u8bd5\u9a8c\uff0c\u9519\u8bef\u91cd\u590d\u53d1\u751f\u800c\u65e0\u6cd5\u79ef\u7d2f\u6210\u7ecf\u9a8c\u3002\u53d7\u4eba\u7c7b\u53cd\u601d\u5b9e\u8df5\u8005\u542f\u53d1\uff0c\u9700\u8981\u8ba9\u673a\u5668\u4eba\u5177\u5907\u53cd\u601d\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53cd\u5c04\u6d4b\u8bd5\u65f6\u95f4\u89c4\u5212\uff0c\u5305\u542b\u4e24\u79cd\u53cd\u601d\u6a21\u5f0f\uff1a1\uff09\u884c\u52a8\u4e2d\u53cd\u601d\uff1a\u4f7f\u7528\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u751f\u6210\u548c\u8bc4\u5206\u591a\u4e2a\u5019\u9009\u52a8\u4f5c\uff0c\u5728\u6267\u884c\u524d\u8fdb\u884c\u5185\u90e8\u53cd\u601d\uff1b2\uff09\u884c\u52a8\u540e\u53cd\u601d\uff1a\u4f7f\u7528\u6d4b\u8bd5\u65f6\u95f4\u8bad\u7ec3\uff0c\u57fa\u4e8e\u6267\u884c\u540e\u7684\u5916\u90e8\u53cd\u601d\u66f4\u65b0\u5185\u90e8\u53cd\u601d\u6a21\u578b\u548c\u52a8\u4f5c\u7b56\u7565\u3002\u8fd8\u5305\u62ec\u56de\u987e\u6027\u53cd\u601d\uff0c\u5141\u8bb8\u91cd\u65b0\u8bc4\u4f30\u65e9\u671f\u51b3\u7b56\u5e76\u8fdb\u884c\u540e\u89c1\u4e4b\u660e\u7684\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5728\u65b0\u8bbe\u8ba1\u7684\u957f\u671f\u5bb6\u5ead\u57fa\u51c6\u548cMuJoCo\u6a71\u67dc\u88c5\u914d\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u884c\u52a8\u4e2d\u53cd\u601d\u548c\u884c\u52a8\u540e\u53cd\u601d\u7684\u4e92\u8865\u4f5c\u7528\u3002\u5b9a\u6027\u5206\u6790\uff08\u5305\u62ec\u771f\u5b9e\u673a\u5668\u4eba\u8bd5\u9a8c\uff09\u5c55\u793a\u4e86\u901a\u8fc7\u53cd\u601d\u5b9e\u73b0\u7684\u884c\u4e3a\u4fee\u6b63\u3002", "conclusion": "\u53cd\u5c04\u6d4b\u8bd5\u65f6\u95f4\u89c4\u5212\u4f7f\u5177\u8eabLLM\u80fd\u591f\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u5c06\u9519\u8bef\u8f6c\u5316\u4e3a\u5b66\u4e60\u673a\u4f1a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u4efb\u52a1\u89c4\u5212\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u53cd\u601d\u6846\u67b6\u3002"}}
{"id": "2602.21204", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21204", "abs": "https://arxiv.org/abs/2602.21204", "authors": ["Junchen Liu", "Sven Elflein", "Or Litany", "Zan Gojcic", "Ruilong Li"], "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "comment": "Webpage: https://research.nvidia.com/labs/sil/projects/tttla/", "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "AI": {"tldr": "\u8bba\u6587\u6311\u6218\u4e86\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TTT\uff09\u4f5c\u4e3a\u8bb0\u5fc6\u5316\u5728\u7ebf\u5143\u5b66\u4e60\u7684\u4f20\u7edf\u89e3\u91ca\uff0c\u63d0\u51faTTT\u5b9e\u9645\u4e0a\u662f\u4e00\u79cd\u5b66\u4e60\u5230\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\uff0c\u8fd9\u4e00\u65b0\u89c6\u89d2\u80fd\u89e3\u91ca\u5f02\u5e38\u73b0\u8c61\u5e76\u5e26\u6765\u67b6\u6784\u7b80\u5316\u3001\u5e76\u884c\u5316\u548c\u6548\u7387\u63d0\u5347\u7b49\u5b9e\u9645\u597d\u5904\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u57fa\u4e8eKV\u7ed1\u5b9a\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u88ab\u89e3\u91ca\u4e3a\u5728\u7ebf\u5143\u5b66\u4e60\uff0c\u901a\u8fc7\u8bb0\u5fc6\u952e\u503c\u6620\u5c04\u6765\u9002\u5e94\u6d4b\u8bd5\u6570\u636e\u3002\u4f46\u4f5c\u8005\u89c2\u5bdf\u5230\u591a\u4e2a\u4e0e\u8fd9\u79cd\u8bb0\u5fc6\u5316\u89e3\u91ca\u76f8\u77db\u76fe\u7684\u73b0\u8c61\uff0c\u4fc3\u4f7f\u4ed6\u4eec\u91cd\u65b0\u5ba1\u89c6TTT\u7684\u6570\u5b66\u672c\u8d28\u3002", "method": "\u4f5c\u8005\u91cd\u65b0\u5f62\u5f0f\u5316TTT\uff0c\u8bc1\u660e\u4e00\u5927\u7c7bTTT\u67b6\u6784\u53ef\u4ee5\u8868\u8fbe\u4e3a\u5b66\u4e60\u5230\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u7b97\u5b50\u3002\u901a\u8fc7\u8fd9\u79cd\u89c6\u89d2\uff0c\u4ed6\u4eec\u5b9e\u73b0\u4e86\u67b6\u6784\u7b80\u5316\u3001\u5f00\u53d1\u4e86\u5b8c\u5168\u5e76\u884c\u7684\u5b9e\u73b0\u65b9\u6848\uff0c\u5e76\u5c06\u591a\u79cdTTT\u53d8\u4f53\u7cfb\u7edf\u6027\u5730\u5f52\u7ea6\u4e3a\u6807\u51c6\u7ebf\u6027\u6ce8\u610f\u529b\u5f62\u5f0f\u3002", "result": "\u65b0\u6846\u67b6\u6210\u529f\u89e3\u91ca\u4e86\u4e4b\u524d\u4ee4\u4eba\u56f0\u60d1\u7684\u6a21\u578b\u884c\u4e3a\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4fdd\u6301\u4e0b\u7684\u6548\u7387\u63d0\u5347\uff0c\u5e76\u4e3a\u4e0d\u540cTTT\u53d8\u4f53\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u57fa\u7840\u3002\u5b9e\u9a8c\u8868\u660e\u5e76\u884c\u5316\u65b9\u6848\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "TTT\u4e0d\u5e94\u88ab\u7406\u89e3\u4e3a\u6d4b\u8bd5\u65f6\u8bb0\u5fc6\u5316\uff0c\u800c\u5e94\u89c6\u4e3a\u5177\u6709\u589e\u5f3a\u8868\u793a\u80fd\u529b\u7684\u5b66\u4e60\u7ebf\u6027\u6ce8\u610f\u529b\u3002\u8fd9\u4e00\u65b0\u89c6\u89d2\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u6570\u5b66\u89e3\u91ca\uff0c\u8fd8\u5e26\u6765\u4e86\u5b9e\u9645\u7684\u67b6\u6784\u6539\u8fdb\u548c\u6548\u7387\u4f18\u5316\u3002"}}
{"id": "2602.21054", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21054", "abs": "https://arxiv.org/abs/2602.21054", "authors": ["Seongheon Park", "Changdae Oh", "Hyeong Kyu Choi", "Xuefeng Du", "Sharon Li"], "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation", "comment": null, "summary": "Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.", "AI": {"tldr": "VAUQ\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u4fe1\u606f\u5206\u6570\u548c\u6838\u5fc3\u533a\u57df\u63a9\u7801\u7b56\u7565\u6765\u8bc4\u4f30LVLM\u8f93\u51fa\u5bf9\u89c6\u89c9\u8bc1\u636e\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u3002\u73b0\u6709\u7684LLM\u81ea\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\uff0c\u4e0d\u9002\u5408\u8bc4\u4f30\u89c6\u89c9\u6761\u4ef6\u9884\u6d4b\u3002", "method": "\u63d0\u51faVAUQ\u6846\u67b6\uff0c\u5f15\u5165\u56fe\u50cf\u4fe1\u606f\u5206\u6570\uff08IS\uff09\u6765\u8861\u91cf\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e2d\u53ef\u5f52\u56e0\u4e8e\u89c6\u89c9\u8f93\u5165\u7684\u90e8\u5206\uff0c\u5e76\u7ed3\u5408\u65e0\u76d1\u7763\u6838\u5fc3\u533a\u57df\u63a9\u7801\u7b56\u7565\u6765\u589e\u5f3a\u663e\u8457\u533a\u57df\u7684\u5f71\u54cd\u3002\u5c06\u9884\u6d4b\u71b5\u4e0e\u6838\u5fc3\u63a9\u7801IS\u7ed3\u5408\uff0c\u5f62\u6210\u65e0\u9700\u8bad\u7ec3\u7684\u6253\u5206\u51fd\u6570\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cVAUQ\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "VAUQ\u901a\u8fc7\u663e\u5f0f\u6d4b\u91cf\u6a21\u578b\u8f93\u51fa\u5bf9\u89c6\u89c9\u8bc1\u636e\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u4e3aLVLM\u81ea\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u90e8\u7f72\u53ef\u9760\u6027\u3002"}}
{"id": "2602.21137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21137", "abs": "https://arxiv.org/abs/2602.21137", "authors": ["Joseph Raj Vishal", "Nagasiri Poluri", "Katha Naik", "Rutuja Patil", "Kashyap Hegde Kota", "Krishna Vinod", "Prithvi Jai Ramesh", "Mohammad Farhadi", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics", "comment": null, "summary": "Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation without compromising scene fidelity. Using a unified annotation pipeline, the dataset contains 28K question-answer pairs generated across 8 hours of densely annotated video, averaging one question per second. Its taxonomy follows a hierarchical reasoning level, spanning basic understanding and attribution to event reasoning, reverse reasoning, and counterfactual inference, enabling systematic evaluation of both visual grounding and causal reasoning. Comprehensive experiments benchmark 10 SOTA VideoLMs on UDVideoQA and 8 models on a complementary video question generation benchmark. Results reveal a persistent perception-reasoning gap, showing models that excel in abstract inference often fail with fundamental visual grounding. While models like Gemini Pro achieve the highest zero-shot accuracy, fine-tuning the smaller Qwen2.5-VL 7B model on UDVideoQA bridges this gap, achieving performance comparable to proprietary systems. In VideoQGen, Gemini 2.5 Pro, and Qwen3 Max generate the most relevant and complex questions, though all models exhibit limited linguistic diversity, underscoring the need for human-centric evaluation. The UDVideoQA suite, including the dataset, annotation tools, and benchmarks for both VideoQA and VideoQGen, provides a foundation for advancing robust, privacy-aware, and real-world multimodal reasoning. UDVideoQA is available at https://ud-videoqa.github.io/UD-VideoQA/UD-VideoQA/.", "AI": {"tldr": "UDVideoQA\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u57ce\u5e02\u4ea4\u901a\u52a8\u6001\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b16\u5c0f\u65f6\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\u548c28K\u95ee\u7b54\u5bf9\uff0c\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u52a8\u6001\u6a21\u7cca\u6280\u672f\u4fdd\u62a4\u9690\u79c1\uff0c\u7cfb\u7edf\u8bc4\u4f30\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u5b9a\u4f4d\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u7406\u89e3\u590d\u6742\u7684\u57ce\u5e02\u4ea4\u901a\u591a\u667a\u80fd\u4f53\u52a8\u6001\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u5b9a\u4f4d\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u4ece\u591a\u4e2a\u57ce\u5e02\u4ea4\u53c9\u53e3\u91c7\u96c616\u5c0f\u65f6\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\uff0c\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u52a8\u6001\u6a21\u7cca\u6280\u672f\u4fdd\u62a4\u9690\u79c1\uff0c\u901a\u8fc7\u7edf\u4e00\u6807\u6ce8\u6d41\u7a0b\u751f\u621028K\u95ee\u7b54\u5bf9\uff0c\u6784\u5efa\u5206\u5c42\u63a8\u7406\u5206\u7c7b\u4f53\u7cfb\uff08\u57fa\u7840\u7406\u89e3\u3001\u4e8b\u4ef6\u63a8\u7406\u3001\u53cd\u5411\u63a8\u7406\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\uff09\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5b58\u5728\u6301\u7eed\u7684\u611f\u77e5-\u63a8\u7406\u9e3f\u6c9f\uff1a\u64c5\u957f\u62bd\u8c61\u63a8\u7406\u7684\u6a21\u578b\u5728\u57fa\u7840\u89c6\u89c9\u5b9a\u4f4d\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002Gemini Pro\u5728\u96f6\u6837\u672c\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u4f46\u5fae\u8c03\u540e\u7684Qwen2.5-VL 7B\u6a21\u578b\u80fd\u5f25\u5408\u8fd9\u4e00\u9e3f\u6c9f\uff0c\u8fbe\u5230\u4e0e\u4e13\u6709\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\u3002\u5728\u89c6\u9891\u95ee\u9898\u751f\u6210\u4efb\u52a1\u4e2d\uff0cGemini 2.5 Pro\u548cQwen3 Max\u751f\u6210\u7684\u95ee\u9898\u6700\u76f8\u5173\u548c\u590d\u6742\uff0c\u4f46\u6240\u6709\u6a21\u578b\u7684\u8bed\u8a00\u591a\u6837\u6027\u6709\u9650\u3002", "conclusion": "UDVideoQA\u4e3a\u63a8\u8fdb\u9c81\u68d2\u3001\u9690\u79c1\u4fdd\u62a4\u3001\u771f\u5b9e\u4e16\u754c\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5b9a\u4f4d\u548c\u56e0\u679c\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u9700\u6c42\u3002"}}
{"id": "2602.21175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21175", "abs": "https://arxiv.org/abs/2602.21175", "authors": ["Jianglin Lu", "Simon Jenni", "Kushal Kafle", "Jing Shi", "Handong Zhao", "Yun Fu"], "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models", "comment": null, "summary": "Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d28\u91cf\u53ef\u63a7\u7684\u68c0\u7d22\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u8a00\u6a21\u578b\u5c06\u7b80\u77ed\u67e5\u8be2\u6269\u5c55\u4e3a\u5305\u542b\u89c6\u89c9\u5c5e\u6027\u548c\u8d28\u91cf\u63a7\u5236\u7684\u63cf\u8ff0\u6027\u67e5\u8be2\uff0c\u4ece\u800c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u7ecf\u5e38\u9762\u4e34\u7528\u6237\u67e5\u8be2\u8fc7\u4e8e\u7b80\u77ed\u548c\u672a\u5145\u5206\u6307\u5b9a\u7684\u6311\u6218\u3002\u8fd9\u4e9b\u77ed\u67e5\u8be2\u901a\u5e38\u53ea\u6709\u4e00\u4e24\u4e2a\u8bcd\uff0c\u5bfc\u81f4\u8bed\u4e49\u6a21\u7cca\u3001\u5bb9\u6613\u4ea7\u751f\u591a\u79cd\u89c6\u89c9\u89e3\u91ca\u78b0\u649e\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bf9\u68c0\u7d22\u56fe\u50cf\u8d28\u91cf\u7684\u660e\u786e\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u8d28\u91cf\u53ef\u63a7\u68c0\u7d22\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u67e5\u8be2\u8865\u5168\u51fd\u6570\uff0c\u5c06\u672a\u5145\u5206\u6307\u5b9a\u7684\u67e5\u8be2\u6269\u5c55\u4e3a\u6355\u83b7\u59ff\u6001\u3001\u573a\u666f\u3001\u7f8e\u5b66\u7b49\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5c5e\u6027\u7684\u63cf\u8ff0\u6027\u5f62\u5f0f\u3002\u901a\u8fc7\u57fa\u4e8e\u76f8\u5173\u6027\u548c\u7f8e\u5b66\u8bc4\u5206\u6a21\u578b\u5bfc\u51fa\u7684\u79bb\u6563\u8d28\u91cf\u7ea7\u522b\u6765\u8c03\u8282\u67e5\u8be2\u8865\u5168\uff0c\u4f7f\u67e5\u8be2\u4e30\u5bcc\u4e0d\u4ec5\u8bed\u4e49\u6709\u610f\u4e49\u800c\u4e14\u8d28\u91cf\u611f\u77e5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u68c0\u7d22\u7ed3\u679c\u5e76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d28\u91cf\u63a7\u5236\uff0c\u5f25\u5408\u4e86\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u7b80\u77ed\u7528\u6237\u67e5\u8be2\u672a\u5145\u5206\u6307\u5b9a\u7279\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d28\u91cf\u53ef\u63a7\u68c0\u7d22\u8303\u5f0f\u5177\u6709\u4e09\u4e2a\u5173\u952e\u4f18\u52bf\uff1a1) \u7075\u6d3b\u6027\uff0c\u517c\u5bb9\u4efb\u4f55\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u800c\u65e0\u9700\u4fee\u6539\uff1b2) \u900f\u660e\u5ea6\uff0c\u4e30\u5bcc\u7684\u67e5\u8be2\u5bf9\u7528\u6237\u660e\u786e\u53ef\u89e3\u91ca\uff1b3) \u53ef\u63a7\u6027\uff0c\u80fd\u591f\u5c06\u68c0\u7d22\u7ed3\u679c\u5f15\u5bfc\u81f3\u7528\u6237\u504f\u597d\u7684\u8d28\u91cf\u7ea7\u522b\u3002"}}
{"id": "2602.21179", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21179", "abs": "https://arxiv.org/abs/2602.21179", "authors": ["Nicol\u00e1s Gaggion", "Maria J. Ledesma-Carbayo", "Stergios Christodoulidis", "Maria Vakalopoulou", "Enzo Ferrante"], "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision", "comment": null, "summary": "Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.", "AI": {"tldr": "Mask-HybridGNet\uff1a\u4e00\u79cd\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u5730\u6807\u5373\u53ef\u8bad\u7ec3\u57fa\u4e8e\u56fe\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u5229\u7528\u6807\u51c6\u50cf\u7d20\u7ea7\u63a9\u7801\u5b9e\u73b0\u89e3\u5256\u7ed3\u6784\u8fb9\u754c\u56fe\u8868\u793a\uff0c\u5177\u6709\u9690\u5f0f\u5bf9\u5e94\u5173\u7cfb\u5b66\u4e60\u80fd\u529b", "motivation": "\u57fa\u4e8e\u56fe\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u8981\u5177\u6709\u70b9\u5bf9\u70b9\u5bf9\u5e94\u5173\u7cfb\u7684\u624b\u52a8\u6807\u6ce8\u5730\u6807\u6570\u636e\u96c6\uff0c\u4f46\u8fd9\u7c7b\u6570\u636e\u5728\u5b9e\u8df5\u4e2d\u5f88\u5c11\u5b58\u5728\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u91c7\u7528", "method": "\u7ed3\u5408Chamfer\u8ddd\u79bb\u76d1\u7763\u548c\u57fa\u4e8e\u8fb9\u7684\u6b63\u5219\u5316\uff0c\u5c06\u53ef\u53d8\u957f\u5ea6\u7684\u771f\u5b9e\u8fb9\u754c\u4e0e\u56fa\u5b9a\u957f\u5ea6\u7684\u5730\u6807\u9884\u6d4b\u5bf9\u9f50\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5149\u6805\u5316\u8fdb\u884c\u7ec6\u5316\uff0c\u5229\u7528\u56fa\u5b9a\u56fe\u90bb\u63a5\u77e9\u9635\u786e\u4fdd\u8fb9\u754c\u8fde\u901a\u6027", "result": "\u5728\u80f8\u90e8X\u5149\u3001\u5fc3\u810f\u8d85\u58f0\u3001\u5fc3\u810fMRI\u548c\u80ce\u513f\u6210\u50cf\u5b9e\u9a8c\u4e2d\uff0c\u6a21\u578b\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u50cf\u7d20\u7ea7\u65b9\u6cd5\u7ade\u4e89\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u786e\u4fdd\u89e3\u5256\u5408\u7406\u6027\uff0c\u9884\u6d4b\u7684\u5730\u6807\u4f4d\u7f6e\u5728\u60a3\u8005\u95f4\u4fdd\u6301\u4e00\u81f4\u7684\u89e3\u5256\u4f4d\u7f6e\u5bf9\u5e94\u5173\u7cfb", "conclusion": "\u8be5\u6846\u67b6\u5229\u7528\u5927\u91cf\u53ef\u7528\u7684\u6807\u51c6\u5206\u5272\u63a9\u7801\u6784\u5efa\u7ed3\u6784\u5316\u6a21\u578b\uff0c\u4fdd\u6301\u62d3\u6251\u5b8c\u6574\u6027\u5e76\u63d0\u4f9b\u9690\u5f0f\u5bf9\u5e94\u5173\u7cfb\uff0c\u652f\u6301\u65f6\u95f4\u8ddf\u8e2a\u3001\u8de8\u5207\u7247\u91cd\u5efa\u548c\u5f62\u6001\u5b66\u7fa4\u4f53\u5206\u6790\u7b49\u5e94\u7528"}}
{"id": "2602.21186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21186", "abs": "https://arxiv.org/abs/2602.21186", "authors": ["Haoyi Jiang", "Liu Liu", "Xinjie Wang", "Yonghao He", "Wei Sui", "Zhizhong Su", "Wenyu Liu", "Xinggang Wang"], "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning", "comment": null, "summary": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.", "AI": {"tldr": "Spa3R\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u7a7a\u95f4\u573a\u5efa\u6a21\u4ece\u65e0\u59ff\u6001\u591a\u89c6\u89d2\u56fe\u50cf\u5b66\u4e60\u7edf\u4e00\u3001\u89c6\u89d2\u4e0d\u53d8\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u65e0\u97003D\u6a21\u6001\u6216\u7a7a\u95f4\u6307\u4ee4\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u7a7a\u95f4\u667a\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u8868\u73b0\u80a4\u6d45\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u663e\u5f0f3D\u6a21\u6001\uff0c\u8981\u4e48\u901a\u8fc7\u90e8\u5206\u89c6\u89d2\u6761\u4ef6\u51e0\u4f55\u5148\u9a8c\u589e\u5f3aVLMs\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u4e14\u8ba9\u8bed\u8a00\u6a21\u578b\u627f\u62c5\u4ece\u7a00\u758f\u7ebf\u7d22\u9690\u5f0f\u91cd\u5efa\u6574\u4f533D\u51e0\u4f55\u7684\u4e0d\u9002\u5b9a\u4efb\u52a1\u3002", "method": "\u63d0\u51faSpa3R\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u57fa\u4e8e\u9884\u6d4b\u6027\u7a7a\u95f4\u573a\u5efa\u6a21\u8303\u5f0f\uff0c\u4ece\u65e0\u59ff\u6001\u591a\u89c6\u89d2\u56fe\u50cf\u5b66\u4e60\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u5408\u6210\u4efb\u610f\u672a\u89c1\u89c6\u89d2\u7684\u7279\u5f81\u573a\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5c06\u9884\u8bad\u7ec3\u7684Spa3R\u7f16\u7801\u5668\u96c6\u6210\u5230\u73b0\u6709VLMs\u4e2d\uff0c\u5f62\u6210Spa3-VLM\u3002", "result": "\u5728VSI-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpa3-VLM\u57283D\u89c6\u89c9\u95ee\u7b54\u4e0a\u8fbe\u523058.6%\u7684\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u9884\u6d4b\u6027\u7a7a\u95f4\u573a\u5efa\u6a21\u4e3a\u63a8\u8fdb\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u8bc1\u660e\u7a7a\u95f4\u667a\u80fd\u53ef\u4ee5\u4ece2D\u89c6\u89c9\u4e2d\u56fa\u6709\u5730\u6d8c\u73b0\uff0c\u800c\u65e0\u9700\u901a\u8fc7\u663e\u5f0f\u7a7a\u95f4\u6307\u4ee4\u5fae\u8c03\u6765\u5f3a\u52a0\u3002"}}
{"id": "2602.21188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21188", "abs": "https://arxiv.org/abs/2602.21188", "authors": ["Tiantian Wang", "Chun-Han Yao", "Tao Hu", "Mallikarjun Byrasandra Ramalinga Reddy", "Ming-Hsuan Yang", "Varun Jampani"], "title": "Human Video Generation from a Single Image with 3D Pose and View Control", "comment": null, "summary": "Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.", "AI": {"tldr": "HVG\u662f\u4e00\u4e2a\u4ece\u5355\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf4D\u4eba\u4f53\u89c6\u9891\u7684\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u7ef4\u59ff\u6001\u548c\u89c6\u89d2\u63a7\u5236\u5b9e\u73b0\u591a\u89c6\u89d2\u65f6\u7a7a\u4e00\u81f4\u7684\u4eba\u4f53\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u5728\u4ece\u5355\u56fe\u50cf\u751f\u6210\u89c6\u9891\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u4eba\u4f53\u89c6\u9891\u751f\u6210\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u4ece\u5355\u56fe\u50cf\u63a8\u65ad\u89c6\u89d2\u4e00\u81f4\u3001\u8fd0\u52a8\u76f8\u5173\u7684\u670d\u88c5\u8936\u76b1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1) \u5173\u8282\u59ff\u6001\u8c03\u5236\uff0c\u901a\u8fc7\u53cc\u7ef4\u5ea6\u9aa8\u9abc\u56fe\u6355\u6349\u4e09\u7ef4\u5173\u8282\u89e3\u5256\u5173\u7cfb\u5e76\u5f15\u5165\u4e09\u7ef4\u4fe1\u606f\u89e3\u51b3\u81ea\u906e\u6321\uff1b2) \u89c6\u89d2\u548c\u65f6\u95f4\u5bf9\u9f50\uff0c\u786e\u4fdd\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u53c2\u8003\u56fe\u50cf\u4e0e\u59ff\u6001\u5e8f\u5217\u7684\u5bf9\u9f50\uff1b3) \u6e10\u8fdb\u65f6\u7a7a\u91c7\u6837\u4e0e\u65f6\u95f4\u5bf9\u9f50\uff0c\u4fdd\u6301\u957f\u591a\u89c6\u89d2\u52a8\u753b\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u5728\u56fe\u50cf\u5230\u89c6\u9891\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHVG\u5728\u4ece\u591a\u6837\u5316\u4eba\u4f53\u56fe\u50cf\u548c\u59ff\u6001\u8f93\u5165\u751f\u6210\u9ad8\u8d28\u91cf4D\u4eba\u4f53\u89c6\u9891\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HVG\u80fd\u591f\u4ece\u5355\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u89c6\u89d2\u3001\u65f6\u7a7a\u4e00\u81f4\u76844D\u4eba\u4f53\u89c6\u9891\uff0c\u901a\u8fc7\u4e09\u7ef4\u59ff\u6001\u548c\u89c6\u89d2\u63a7\u5236\u89e3\u51b3\u4e86\u4eba\u4f53\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2602.21195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21195", "abs": "https://arxiv.org/abs/2602.21195", "authors": ["Xingyi Cheng", "Julien Maufront", "Aur\u00e9lie Di Cicco", "Dani\u00ebl M. Pelt", "Manuela Dezi", "Daniel L\u00e9vy"], "title": "Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography", "comment": null, "summary": "Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.", "AI": {"tldr": "\u5f00\u53d1\u4e86TomoROIS-SurfORA\u6846\u67b6\uff0c\u7528\u4e8e\u51b7\u51bb\u7535\u955c\u65ad\u5c42\u626b\u63cf\u6570\u636e\u7684\u76f4\u63a5\u533a\u57df\u5206\u5272\u548c\u8868\u9762\u5f62\u6001\u5206\u6790\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u819c\u7ed3\u6784", "motivation": "\u5f53\u524d\u51b7\u51bb\u7535\u955c\u65ad\u5c42\u626b\u63cf\u4e2d\uff0c\u611f\u5174\u8da3\u533a\u57df\u901a\u5e38\u901a\u8fc7\u5168\u7ed3\u6784\u5206\u5272\u95f4\u63a5\u83b7\u5f97\uff0c\u5bf9\u4e8e\u8fde\u7eed\u590d\u6742\u7684\u819c\u7ed3\u6784\u5c24\u5176\u4e0d\u4fbf\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u7684ROI\u5206\u5272\u548c\u5f62\u6001\u5206\u6790\u65b9\u6cd5", "method": "\u63d0\u51fa\u4e24\u6b65\u6846\u67b6\uff1a1) TomoROIS\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u76f4\u63a5\u4ece\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u8fdb\u884cROI\u5206\u5272\uff1b2) SurfORA\u5c06\u5206\u5272\u7ed3\u6784\u5904\u7406\u4e3a\u70b9\u4e91\u548c\u8868\u9762\u7f51\u683c\uff0c\u63d0\u53d6\u5f62\u6001\u7279\u5f81\u5982\u819c\u95f4\u8ddd\u79bb\u3001\u66f2\u7387\u3001\u8868\u9762\u7c97\u7cd9\u5ea6", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u4f53\u5916\u91cd\u6784\u7684\u819c\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u52a8\u5b9a\u91cf\u5206\u6790\u819c\u63a5\u89e6\u4f4d\u70b9\u548c\u91cd\u6784\u4e8b\u4ef6\uff08\u5982\u5185\u9677\uff09\uff0c\u652f\u6301\u5f00\u653e\u548c\u5c01\u95ed\u8868\u9762\uff0c\u7279\u522b\u8003\u8651\u4e86\u51b7\u51bb\u7535\u955c\u4e2d\u5e38\u89c1\u7684\u7f3a\u5931\u6954\u6548\u5e94", "conclusion": "TomoROIS-SurfORA\u4e3a\u51b7\u51bb\u7535\u955c\u819c\u6570\u636e\u63d0\u4f9b\u4e86\u76f4\u63a5\u7684ROI\u5206\u5272\u548c\u8868\u9762\u5206\u6790\u5de5\u5177\uff0c\u867d\u7136\u5728\u6b64\u5c55\u793a\u819c\u6570\u636e\u5e94\u7528\uff0c\u4f46\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u79d1\u5b66\u6210\u50cf\u9886\u57df"}}
{"id": "2602.21202", "categories": ["cs.IR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.21202", "abs": "https://arxiv.org/abs/2602.21202", "authors": ["Hanxiang Qin", "Alexander Martin", "Rohan Jha", "Chunsheng Zuo", "Reno Kriz", "Benjamin Van Durme"], "title": "Multi-Vector Index Compression in Any Modality", "comment": "12 pages, 4 figures", "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u6a21\u6001\u591a\u5411\u91cf\u68c0\u7d22\u4e2d\u7684\u7d22\u5f15\u538b\u7f29\u65b9\u6cd5\uff0c\u63d0\u51fa\u6ce8\u610f\u529b\u5f15\u5bfc\u805a\u7c7b\u6280\u672f\uff0c\u5728\u56fa\u5b9a\u5411\u91cf\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\uff0c\u5728\u6587\u672c\u3001\u89c6\u89c9\u6587\u6863\u548c\u89c6\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u540e\u671f\u4ea4\u4e92\u5df2\u6210\u4e3a\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u89c9\u6587\u6863\u548c\u89c6\u9891\u4fe1\u606f\u68c0\u7d22\u7684\u4e3b\u8981\u8303\u5f0f\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u968f\u6587\u6863\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u5bf9\u4e8e\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u4e30\u5bcc\u7684\u8bed\u6599\u5e93\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u89e3\u51b3\u591a\u5411\u91cf\u6587\u6863\u8868\u793a\u5728\u56fa\u5b9a\u5411\u91cf\u9884\u7b97\u4e0b\u7684\u538b\u7f29\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u7d22\u5f15\u538b\u7f29\u65b9\u6cd5\uff1a\u5e8f\u5217\u8c03\u6574\u3001\u8bb0\u5fc6\u4ee4\u724c\u3001\u5206\u5c42\u6c60\u5316\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u805a\u7c7b\uff08AGC\uff09\u3002AGC\u4f7f\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u673a\u5236\u8bc6\u522b\u6587\u6863\u4e2d\u6700\u5177\u8bed\u4e49\u663e\u8457\u6027\u7684\u533a\u57df\u4f5c\u4e3a\u805a\u7c7b\u4e2d\u5fc3\uff0c\u5e76\u5bf9\u4ee4\u724c\u805a\u5408\u8fdb\u884c\u52a0\u6743\u3002", "result": "\u5728\u6587\u672c\uff08BEIR\uff09\u3001\u89c6\u89c9\u6587\u6863\uff08ViDoRe\uff09\u548c\u89c6\u9891\uff08MSR-VTT\u3001MultiVENT 2.0\uff09\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6ce8\u610f\u529b\u5f15\u5bfc\u805a\u7c7b\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u53c2\u6570\u5316\u538b\u7f29\u65b9\u6cd5\uff0c\u6bd4\u975e\u53c2\u6570\u5316\u5206\u5c42\u805a\u7c7b\u63d0\u4f9b\u66f4\u5927\u7684\u7d22\u5f15\u5927\u5c0f\u7075\u6d3b\u6027\uff0c\u5e76\u4e14\u76f8\u6bd4\u5b8c\u6574\u672a\u538b\u7f29\u7d22\u5f15\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6ce8\u610f\u529b\u5f15\u5bfc\u805a\u7c7b\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u5411\u91cf\u6587\u6863\u8868\u793a\u538b\u7f29\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u56fa\u5b9a\u5411\u91cf\u9884\u7b97\u4e0b\u4fdd\u6301\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u8de8\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
