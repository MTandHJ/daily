{"id": "2601.00833", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00833", "abs": "https://arxiv.org/abs/2601.00833", "authors": ["Tangtang Wang", "Kaijie Zhang", "Kuangcong Liu"], "title": "A Knowledge Graph and Deep Learning-Based Semantic Recommendation Database System for Advertisement Retrieval and Personalization", "comment": null, "summary": "In modern digital marketing, the growing complexity of advertisement data demands intelligent systems capable of understanding semantic relationships among products, audiences, and advertising content. To address this challenge, this paper proposes a Knowledge Graph and Deep Learning-Based Semantic Recommendation Database System (KGSR-ADS) for advertisement retrieval and personalization. The proposed framework integrates a heterogeneous Ad-Knowledge Graph (Ad-KG) that captures multi-relational semantics, a Semantic Embedding Layer that leverages large language models (LLMs) such as GPT and LLaMA to generate context-aware vector representations, a GNN + Attention Model that infers cross-entity dependencies, and a Database Optimization & Retrieval Layer based on vector indexing (FAISS/Milvus) for efficient semantic search. This layered architecture enables both accurate semantic matching and scalable retrieval, allowing personalized ad recommendations under large-scale heterogeneous workloads.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u8bed\u4e49\u63a8\u8350\u6570\u636e\u5e93\u7cfb\u7edf\uff08KGSR-ADS\uff09\uff0c\u7528\u4e8e\u5e7f\u544a\u68c0\u7d22\u548c\u4e2a\u6027\u5316\u63a8\u8350\uff0c\u6574\u5408\u591a\u5173\u7cfb\u8bed\u4e49\u3001LLM\u5d4c\u5165\u3001GNN\u6ce8\u610f\u529b\u673a\u5236\u548c\u5411\u91cf\u7d22\u5f15\u4f18\u5316\u3002", "motivation": "\u73b0\u4ee3\u6570\u5b57\u8425\u9500\u4e2d\u5e7f\u544a\u6570\u636e\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u667a\u80fd\u7cfb\u7edf\u7406\u89e3\u4ea7\u54c1\u3001\u53d7\u4f17\u548c\u5e7f\u544a\u5185\u5bb9\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u4e2a\u6027\u5316\u63a8\u8350\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u67b6\u6784\uff1a1\uff09\u5f02\u6784\u5e7f\u544a\u77e5\u8bc6\u56fe\u8c31\uff08Ad-KG\uff09\u6355\u83b7\u591a\u5173\u7cfb\u8bed\u4e49\uff1b2\uff09\u8bed\u4e49\u5d4c\u5165\u5c42\u5229\u7528GPT/LLaMA\u7b49\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u5411\u91cf\u8868\u793a\uff1b3\uff09GNN+\u6ce8\u610f\u529b\u6a21\u578b\u63a8\u65ad\u8de8\u5b9e\u4f53\u4f9d\u8d56\uff1b4\uff09\u57fa\u4e8eFAISS/Milvus\u5411\u91cf\u7d22\u5f15\u7684\u6570\u636e\u5e93\u4f18\u5316\u4e0e\u68c0\u7d22\u5c42\u5b9e\u73b0\u9ad8\u6548\u8bed\u4e49\u641c\u7d22\u3002", "result": "\u8be5\u67b6\u6784\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u8bed\u4e49\u5339\u914d\u548c\u53ef\u6269\u5c55\u7684\u68c0\u7d22\u80fd\u529b\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u5e7f\u544a\u63a8\u8350\u3002", "conclusion": "KGSR-ADS\u7cfb\u7edf\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e7f\u544a\u6570\u636e\u8bed\u4e49\u7406\u89e3\u4e0e\u4e2a\u6027\u5316\u63a8\u8350\u7684\u6311\u6218\uff0c\u4e3a\u73b0\u4ee3\u6570\u5b57\u8425\u9500\u63d0\u4f9b\u4e86\u667a\u80fd\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00891", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00891", "abs": "https://arxiv.org/abs/2601.00891", "authors": ["Rodrigo Kataishi"], "title": "Enhancing Retrieval-Augmented Generation with Topic-Enriched Embeddings: A Hybrid Approach Integrating Traditional NLP Techniques", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems rely on accurate document retrieval to ground large language models (LLMs) in external knowledge, yet retrieval quality often degrades in corpora where topics overlap and thematic variation is high. This work proposes topic-enriched embeddings that integrate term-based signals and topic structure with contextual sentence embeddings. The approach combines TF-IDF with topic modeling and dimensionality reduction, using Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) to encode latent topical organization, and fuses these representations with a compact contextual encoder (all-MiniLM). By jointly capturing term-level and topic-level semantics, topic-enriched embeddings improve semantic clustering, increase retrieval precision, and reduce computational burden relative to purely contextual baselines. Experiments on a legal-text corpus show consistent gains in clustering coherence and retrieval metrics, suggesting that topic-enriched embeddings can serve as a practical component for more reliable knowledge-intensive RAG pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u9898\u589e\u5f3a\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408TF-IDF\u3001\u4e3b\u9898\u5efa\u6a21\u548c\u964d\u7ef4\u6280\u672f\uff0c\u5c06\u57fa\u4e8e\u672f\u8bed\u7684\u4fe1\u53f7\u548c\u4e3b\u9898\u7ed3\u6784\u4e0e\u4e0a\u4e0b\u6587\u53e5\u5b50\u5d4c\u5165\u76f8\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8RAG\u7cfb\u7edf\u7684\u68c0\u7d22\u8d28\u91cf\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4f9d\u8d56\u51c6\u786e\u7684\u6587\u6863\u68c0\u7d22\u6765\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5916\u90e8\u77e5\u8bc6\u57fa\u7840\uff0c\u4f46\u5728\u4e3b\u9898\u91cd\u53e0\u548c\u4e3b\u9898\u53d8\u5316\u9ad8\u7684\u8bed\u6599\u5e93\u4e2d\uff0c\u68c0\u7d22\u8d28\u91cf\u5f80\u5f80\u4f1a\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8bed\u4e49\u91cd\u53e0\u548c\u4e3b\u9898\u53d8\u5f02\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e3b\u9898\u589e\u5f3a\u5d4c\u5165\u65b9\u6cd5\uff1a\u7ed3\u5408TF-IDF\u4e0e\u4e3b\u9898\u5efa\u6a21\u548c\u964d\u7ef4\u6280\u672f\uff0c\u4f7f\u7528\u6f5c\u5728\u8bed\u4e49\u5206\u6790\uff08LSA\uff09\u548c\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u914d\uff08LDA\uff09\u7f16\u7801\u6f5c\u5728\u4e3b\u9898\u7ed3\u6784\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8868\u793a\u4e0e\u7d27\u51d1\u7684\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff08all-MiniLM\uff09\u878d\u5408\u3002\u901a\u8fc7\u8054\u5408\u6355\u83b7\u672f\u8bed\u7ea7\u548c\u4e3b\u9898\u7ea7\u8bed\u4e49\u6765\u6539\u8fdb\u5d4c\u5165\u8d28\u91cf\u3002", "result": "\u5728\u6cd5\u5f8b\u6587\u672c\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4e3b\u9898\u589e\u5f3a\u5d4c\u5165\u5728\u805a\u7c7b\u4e00\u81f4\u6027\u548c\u68c0\u7d22\u6307\u6807\u65b9\u9762\u6301\u7eed\u63d0\u5347\uff0c\u63d0\u9ad8\u4e86\u8bed\u4e49\u805a\u7c7b\u8d28\u91cf\u3001\u68c0\u7d22\u7cbe\u5ea6\uff0c\u540c\u65f6\u76f8\u5bf9\u4e8e\u7eaf\u4e0a\u4e0b\u6587\u57fa\u7ebf\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002", "conclusion": "\u4e3b\u9898\u589e\u5f3a\u5d4c\u5165\u53ef\u4ee5\u4f5c\u4e3a\u66f4\u53ef\u9760\u7684\u77e5\u8bc6\u5bc6\u96c6\u578bRAG\u7ba1\u9053\u7684\u5b9e\u7528\u7ec4\u4ef6\uff0c\u901a\u8fc7\u6574\u5408\u672f\u8bed\u7ea7\u548c\u4e3b\u9898\u7ea7\u8bed\u4e49\u4fe1\u53f7\uff0c\u6709\u6548\u6539\u5584\u5728\u4e3b\u9898\u91cd\u53e0\u548c\u9ad8\u53d8\u5f02\u8bed\u6599\u5e93\u4e2d\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2601.00912", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00912", "abs": "https://arxiv.org/abs/2601.00912", "authors": ["Amit Prakash Sharma"], "title": "The Discovery Gap: How Product Hunt Startups Vanish in LLM Organic Discovery Queries", "comment": "20 pages, 7 figures. Based on M.Tech thesis research, Indian Institute of Technology Patna, 2025", "summary": "When someone asks ChatGPT to recommend a project management tool, which products show up in the response? And more importantly for startup founders: will their newly launched product ever appear? This research set out to answer these questions.\n  I randomly selected 112 startups from the top 500 products featured on the 2025 Product Hunt leaderboard and tested each one across 2,240 queries to two different large language models: ChatGPT (gpt-4o-mini) and Perplexity (sonar with web search).\n  The results were striking. When users asked about products by name, both LLMs recognized them almost perfectly: 99.4% for ChatGPT and 94.3% for Perplexity. But when users asked discovery-style questions like \"What are the best AI tools launched this year?\" the success rates collapsed to 3.32% and 8.29% respectively. That's a gap of 30-to-1 for ChatGPT.\n  Perhaps the most surprising finding was that Generative Engine Optimization (GEO), the practice of optimizing website content for AI visibility, showed no correlation with actual discovery rates. Products with high GEO scores were no more likely to appear in organic queries than products with low scores.\n  What did matter? For Perplexity, traditional SEO signals like referring domains (r = +0.319, p < 0.001) and Product Hunt ranking (r = -0.286, p = 0.002) predicted visibility. After cleaning the Reddit data for false positives, community presence also emerged as significant (r = +0.395, p = 0.002).\n  The practical takeaway is counterintuitive: don't optimize for AI discovery directly. Instead, build the SEO foundation first and LLM visibility will follow.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u5f53\u7528\u6237\u6309\u540d\u79f0\u8be2\u95ee\u4ea7\u54c1\u65f6\uff0cChatGPT\u548cPerplexity\u7684\u8bc6\u522b\u7387\u5206\u522b\u9ad8\u8fbe99.4%\u548c94.3%\uff0c\u4f46\u5728\u53d1\u73b0\u5f0f\u67e5\u8be2\u4e2d\u6210\u529f\u7387\u9aa4\u964d\u81f33.32%\u548c8.29%\u3002\u751f\u6210\u5f0f\u5f15\u64ce\u4f18\u5316(GEO)\u5bf9AI\u53ef\u89c1\u6027\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f20\u7edfSEO\u4fe1\u53f7\u5982\u5916\u94fe\u548c\u793e\u533a\u5b58\u5728\u624d\u662f\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5f53\u7528\u6237\u5411ChatGPT\u8be2\u95ee\u9879\u76ee\u7ba1\u7406\u5de5\u5177\u63a8\u8350\u65f6\uff0c\u54ea\u4e9b\u4ea7\u54c1\u4f1a\u51fa\u73b0\u5728\u56de\u7b54\u4e2d\uff0c\u4ee5\u53ca\u65b0\u63a8\u51fa\u7684\u521b\u4e1a\u4ea7\u54c1\u662f\u5426\u6709\u673a\u4f1a\u88abAI\u53d1\u73b0\u3002\u8fd9\u5bf9\u4e8e\u521b\u4e1a\u521b\u59cb\u4eba\u4e86\u89e3\u4ea7\u54c1\u5728AI\u641c\u7d22\u4e2d\u7684\u53ef\u89c1\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4ece2025\u5e74Product Hunt\u6392\u884c\u699c\u524d500\u540d\u4ea7\u54c1\u4e2d\u968f\u673a\u9009\u62e9112\u5bb6\u521b\u4e1a\u516c\u53f8\uff0c\u5bf9\u4e24\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08ChatGPT\u7684gpt-4o-mini\u548cPerplexity\u7684sonar with web search\uff09\u8fdb\u884c\u4e862,240\u6b21\u67e5\u8be2\u6d4b\u8bd5\u3002\u5206\u6790\u5305\u62ec\u6309\u540d\u79f0\u67e5\u8be2\u548c\u53d1\u73b0\u5f0f\u67e5\u8be2\u4e24\u79cd\u573a\u666f\uff0c\u5e76\u8bc4\u4f30\u4e86GEO\u5206\u6570\u3001SEO\u4fe1\u53f7\uff08\u5916\u94fe\u6570\u91cf\uff09\u3001Product Hunt\u6392\u540d\u548cReddit\u793e\u533a\u5b58\u5728\u7b49\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u6309\u540d\u79f0\u67e5\u8be2\u65f6\uff0cChatGPT\u8bc6\u522b\u7387\u4e3a99.4%\uff0cPerplexity\u4e3a94.3%\uff1b\u4f46\u5728\u53d1\u73b0\u5f0f\u67e5\u8be2\u4e2d\uff0c\u6210\u529f\u7387\u5206\u522b\u66b4\u8dcc\u81f33.32%\u548c8.29%\uff08ChatGPT\u5dee\u8ddd\u8fbe30:1\uff09\u3002GEO\u4f18\u5316\u4e0e\u53d1\u73b0\u7387\u65e0\u76f8\u5173\u6027\uff0c\u800c\u4f20\u7edfSEO\u4fe1\u53f7\u5982\u5916\u94fe\u6570\u91cf\uff08r = +0.319\uff09\u548cProduct Hunt\u6392\u540d\uff08r = -0.286\uff09\u5bf9Perplexity\u53ef\u89c1\u6027\u6709\u663e\u8457\u9884\u6d4b\u4f5c\u7528\u3002Reddit\u793e\u533a\u5b58\u5728\u7ecf\u6e05\u7406\u540e\u4e5f\u663e\u793a\u663e\u8457\u76f8\u5173\u6027\uff08r = +0.395\uff09\u3002", "conclusion": "\u4e0d\u8981\u76f4\u63a5\u4e3aAI\u53d1\u73b0\u8fdb\u884c\u4f18\u5316\uff0c\u800c\u5e94\u4f18\u5148\u5efa\u7acbSEO\u57fa\u7840\uff0cLLM\u53ef\u89c1\u6027\u5c06\u968f\u4e4b\u800c\u6765\u3002\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u4f18\u5316\u4fe1\u53f7\u6bd4\u4e13\u95e8\u7684\u751f\u6210\u5f0f\u5f15\u64ce\u4f18\u5316\u66f4\u6709\u6548\u5730\u9884\u6d4b\u4ea7\u54c1\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u89c1\u6027\u3002"}}
{"id": "2601.00926", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00926", "abs": "https://arxiv.org/abs/2601.00926", "authors": ["Satya Swaroop Gudipudi", "Sahil Girhepuje", "Ponnurangam Kumaraguru", "Kristine Ma"], "title": "MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers", "comment": null, "summary": "Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.", "AI": {"tldr": "MACA\u662f\u4e00\u79cd\u5143\u6570\u636e\u611f\u77e5\u7684\u8de8\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5c06\u6821\u51c6\u7684LLM\u91cd\u6392\u5e8f\u5668\u84b8\u998f\u5230\u7d27\u51d1\u7684\u5b66\u751f\u68c0\u7d22\u5668\u4e2d\uff0c\u907f\u514d\u5728\u7ebfLLM\u8c03\u7528\uff0c\u5728\u94f6\u884cFAQ\u68c0\u7d22\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f01\u4e1a\u68c0\u7d22\u7cfb\u7edf\u9700\u8981\u5904\u7406\u7b80\u77ed\u3001\u4e0d\u660e\u786e\u7684\u67e5\u8be2\uff0c\u4f46\u6bcf\u4e2a\u67e5\u8be2\u90fd\u4f7f\u7528LLM\u91cd\u6392\u5e8f\u548c\u624b\u52a8\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u548c\u5143\u6570\u636e\uff0c\u53c8\u4e0d\u9700\u8981\u5728\u7ebfLLM\u8c03\u7528\u7684\u9ad8\u6548\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMACA\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5143\u6570\u636e\u611f\u77e5\u63d0\u793a\u9a8c\u8bc1\u6559\u5e08LLM\u7684\u53ef\u9760\u6027\uff1b2) \u6559\u5e08\u63d0\u4f9b\u5217\u8868\u5206\u6570\u3001\u56f0\u96be\u8d1f\u4f8b\u548c\u6821\u51c6\u7684\u76f8\u5173\u8fb9\u754c\uff1b3) \u5b66\u751f\u901a\u8fc7MetaFusion\u76ee\u6807\u8bad\u7ec3\uff0c\u7ed3\u5408\u5143\u6570\u636e\u6761\u4ef6\u6392\u5e8f\u635f\u5931\u548c\u8de8\u6a21\u578b\u8fb9\u754c\u635f\u5931\uff0c\u5b66\u4e60\u5c06\u6b63\u786e\u7b54\u6848\u63a8\u81f3\u8bed\u4e49\u76f8\u4f3c\u4f46\u5143\u6570\u636e\u4e0d\u5339\u914d\u7684\u5019\u9009\u9879\u4e4b\u4e0a\u3002", "result": "\u5728\u4e13\u6709\u6d88\u8d39\u8005\u94f6\u884cFAQ\u8bed\u6599\u5e93\u548cBankFAQs\u4e0a\uff0cMACA\u6559\u5e08\u6bd4MAFA\u57fa\u7ebf\u5728Accuracy@1\u4e0a\u5206\u522b\u63d0\u53475\u70b9\u548c3\u70b9\u3002MACA\u5b66\u751f\u663e\u8457\u4f18\u4e8e\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff1a\u5728\u4e13\u6709\u8bed\u6599\u5e93\u4e0a\uff0cMiniLM\u7684Accuracy@1\u4ece0.23\u63d0\u5347\u52300.48\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u65e0\u9700LLM\u8c03\u7528\u5e76\u652f\u6301\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002", "conclusion": "MACA\u6210\u529f\u5c06\u5143\u6570\u636e\u611f\u77e5\u7684LLM\u91cd\u6392\u5e8f\u80fd\u529b\u84b8\u998f\u5230\u7d27\u51d1\u7684\u5b66\u751f\u68c0\u7d22\u5668\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4f01\u4e1a\u68c0\u7d22\u7cfb\u7edf\uff0c\u65e2\u5229\u7528\u4e86\u8bed\u4e49\u7ec6\u5fae\u5dee\u522b\u548c\u5143\u6570\u636e\uff0c\u53c8\u907f\u514d\u4e86\u6602\u8d35\u7684\u5728\u7ebfLLM\u8c03\u7528\u3002"}}
{"id": "2601.00996", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00996", "abs": "https://arxiv.org/abs/2601.00996", "authors": ["Yongxu Sun", "Michael Saxon", "Ian Yang", "Anna-Maria Gueorguieva", "Aylin Caliskan"], "title": "VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation", "comment": "The International Association for Safe & Ethical AI (IASEAI)", "summary": "Text-to-Video (T2V) generators such as Sora raise concerns about whether generated content reflects societal bias. We extend embedding-association tests from words and images to video by introducing the Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT). We validate these methods by reproducing the direction and magnitude of associations from widely used baselines, including Implicit Association Test (IAT) scenarios and OASIS image categories. We then quantify race (African American vs. European American) and gender (women vs. men) associations with valence (pleasant vs. unpleasant) across 17 occupations and 7 awards. Sora videos associate European Americans and women more with pleasantness (both d>0.8). Effect sizes correlate with real-world demographic distributions: percent men and White in occupations (r=0.93, r=0.83) and percent male and non-Black among award recipients (r=0.88, r=0.99). Applying explicit debiasing prompts generally reduces effect-size magnitudes, but can backfire: two Black-associated occupations (janitor, postal service) become more Black-associated after debiasing. Together, these results reveal that easily accessible T2V generators can actually amplify representational harms if not rigorously evaluated and responsibly deployed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u89c6\u9891\u5d4c\u5165\u5173\u8054\u6d4b\u8bd5(VEAT)\u65b9\u6cd5\uff0c\u53d1\u73b0Sora\u7b49\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u5b58\u5728\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\uff0c\u5c06\u6b27\u6d32\u88d4\u7f8e\u56fd\u4eba\u548c\u5973\u6027\u4e0e\u6109\u60a6\u5ea6\u5173\u8054\u66f4\u5f3a\uff0c\u4e14\u504f\u89c1\u7a0b\u5ea6\u4e0e\u73b0\u5b9e\u4e16\u754c\u4eba\u53e3\u5206\u5e03\u9ad8\u5ea6\u76f8\u5173\uff0c\u53bb\u504f\u63d0\u793a\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30Sora\u7b49\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u5668\u662f\u5426\u53cd\u6620\u793e\u4f1a\u504f\u89c1\uff0c\u7279\u522b\u662f\u79cd\u65cf\u548c\u6027\u522b\u504f\u89c1\u3002\u968f\u7740T2V\u751f\u6210\u5668\u7684\u666e\u53ca\uff0c\u9700\u8981\u91cf\u5316\u5176\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u4ee5\u63ed\u793a\u6f5c\u5728\u7684\u5371\u5bb3\u3002", "method": "\u63d0\u51fa\u89c6\u9891\u5d4c\u5165\u5173\u8054\u6d4b\u8bd5(VEAT)\u548c\u5355\u7c7b\u522bVEAT(SC-VEAT)\u65b9\u6cd5\uff0c\u5c06\u5d4c\u5165\u5173\u8054\u6d4b\u8bd5\u4ece\u6587\u5b57\u548c\u56fe\u50cf\u6269\u5c55\u5230\u89c6\u9891\u9886\u57df\u3002\u901a\u8fc7\u9a8c\u8bc1\u65b9\u6cd5\u540e\uff0c\u91cf\u531617\u79cd\u804c\u4e1a\u548c7\u4e2a\u5956\u9879\u4e2d\u79cd\u65cf\uff08\u975e\u88d4\u7f8e\u56fd\u4ebavs\u6b27\u6d32\u88d4\u7f8e\u56fd\u4eba\uff09\u548c\u6027\u522b\uff08\u5973\u6027vs\u7537\u6027\uff09\u4e0e\u6109\u60a6\u5ea6\uff08\u6109\u5febvs\u4e0d\u6109\u5feb\uff09\u7684\u5173\u8054\u3002", "result": "Sora\u89c6\u9891\u663e\u793a\u6b27\u6d32\u88d4\u7f8e\u56fd\u4eba\u548c\u5973\u6027\u4e0e\u6109\u60a6\u5ea6\u5173\u8054\u66f4\u5f3a\uff08\u6548\u5e94\u91cfd>0.8\uff09\u3002\u6548\u5e94\u5927\u5c0f\u4e0e\u73b0\u5b9e\u4e16\u754c\u4eba\u53e3\u5206\u5e03\u9ad8\u5ea6\u76f8\u5173\uff1a\u804c\u4e1a\u4e2d\u7537\u6027\u6bd4\u4f8b\u548c\u767d\u4eba\u6bd4\u4f8b\uff08r=0.93, r=0.83\uff09\uff0c\u5956\u9879\u4e2d\u7537\u6027\u6bd4\u4f8b\u548c\u975e\u9ed1\u4eba\u6bd4\u4f8b\uff08r=0.88, r=0.99\uff09\u3002\u5e94\u7528\u53bb\u504f\u63d0\u793a\u901a\u5e38\u51cf\u5c11\u6548\u5e94\u5927\u5c0f\uff0c\u4f46\u53ef\u80fd\u9002\u5f97\u5176\u53cd\uff1a\u4e24\u4e2a\u9ed1\u4eba\u5173\u8054\u804c\u4e1a\uff08\u6e05\u6d01\u5de5\u3001\u90ae\u653f\u670d\u52a1\uff09\u5728\u53bb\u504f\u540e\u53cd\u800c\u66f4\u9ed1\u4eba\u5173\u8054\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u6613\u83b7\u53d6\u7684T2V\u751f\u6210\u5668\u5982\u679c\u4e0d\u7ecf\u8fc7\u4e25\u683c\u8bc4\u4f30\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\uff0c\u5b9e\u9645\u4e0a\u53ef\u80fd\u653e\u5927\u4ee3\u8868\u6027\u4f24\u5bb3\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u751f\u6210AI\u504f\u89c1\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u7b80\u5355\u53bb\u504f\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01228", "categories": ["cs.CV", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.01228", "abs": "https://arxiv.org/abs/2601.01228", "authors": ["Markus Haltmeier", "Lukas Neumann", "Nadja Gruber", "Johannes Schwab", "Gyeongha Hwang"], "title": "HyDRA: Hybrid Denoising Regularization for Measurement-Only DEQ Training", "comment": null, "summary": "Solving image reconstruction problems of the form \\(\\mathbf{A} \\mathbf{x} = \\mathbf{y}\\) remains challenging due to ill-posedness and the lack of large-scale supervised datasets. Deep Equilibrium (DEQ) models have been used successfully but typically require supervised pairs \\((\\mathbf{x},\\mathbf{y})\\). In many practical settings, only measurements \\(\\mathbf{y}\\) are available. We introduce HyDRA (Hybrid Denoising Regularization Adaptation), a measurement-only framework for DEQ training that combines measurement consistency with an adaptive denoising regularization term, together with a data-driven early stopping criterion. Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference.", "AI": {"tldr": "HyDRA\uff1a\u4e00\u79cd\u4ec5\u4f7f\u7528\u6d4b\u91cf\u6570\u636e\u7684\u6df1\u5ea6\u5e73\u8861\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u53bb\u566a\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u505c\u6b62\u51c6\u5219\u89e3\u51b3\u56fe\u50cf\u91cd\u5efa\u95ee\u9898", "motivation": "\u89e3\u51b3\u56fe\u50cf\u91cd\u5efa\u95ee\u9898\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u75c5\u6001\u6027\u548c\u7f3a\u4e4f\u5927\u89c4\u6a21\u76d1\u7763\u6570\u636e\u96c6\u3002\u73b0\u6709\u6df1\u5ea6\u5e73\u8861\u6a21\u578b\u901a\u5e38\u9700\u8981\u76d1\u7763\u5bf9(x,y)\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f80\u5f80\u53ea\u6709\u6d4b\u91cf\u6570\u636ey\u53ef\u7528\u3002", "method": "\u63d0\u51faHyDRA\u6846\u67b6\uff0c\u7ed3\u5408\u6d4b\u91cf\u4e00\u81f4\u6027\u548c\u81ea\u9002\u5e94\u53bb\u566a\u6b63\u5219\u5316\u9879\uff0c\u5e76\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u65e9\u671f\u505c\u6b62\u51c6\u5219\uff0c\u5b9e\u73b0\u4ec5\u4f7f\u7528\u6d4b\u91cf\u6570\u636e\u7684DEQ\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u5b9e\u9a8c\u4e2d\uff0cHyDRA\u5c55\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u5feb\u901f\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "HyDRA\u4e3a\u4ec5\u4f7f\u7528\u6d4b\u91cf\u6570\u636e\u7684\u56fe\u50cf\u91cd\u5efa\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5feb\u901f\u63a8\u7406\u3002"}}
{"id": "2601.00812", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00812", "abs": "https://arxiv.org/abs/2601.00812", "authors": ["Takashi Ushio", "Kazuhiro Onishi", "Hideyoshi Yanagisawa"], "title": "Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements", "comment": "This article has been accepted for publication in IEEE Access and will be published shortly", "summary": "Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified \"pleasantness,\" \"surprise,\" and \"habituation\" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected \"pleasantness\" associated with brand presentation, BS has captured \"surprise\" arising from informational complexity, and UN has reflected \"surprise\" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.", "AI": {"tldr": "\u672c\u7814\u7a76\u57fa\u4e8e\u81ea\u7531\u80fd\u539f\u7406\uff0c\u4ec5\u4ece\u5e7f\u544a\u89c6\u9891\u7684\u573a\u666f\u7ea7\u8868\u8fbe\u7279\u5f81\u91cf\u5316\"\u6109\u60a6\u5ea6\"\u3001\"\u60ca\u559c\"\u548c\"\u4e60\u60ef\u5316\"\u4e09\u79cd\u60c5\u611f\u7ef4\u5ea6\uff0c\u65e0\u9700\u4f9d\u8d56\u751f\u7406\u4fe1\u53f7\u6216\u4e3b\u89c2\u8bc4\u5206\u7b49\u5916\u90e8\u4fe1\u606f\u3002", "motivation": "\u5e7f\u544a\u89c6\u9891\u89c2\u770b\u8fc7\u7a0b\u4e2d\u7684\u60c5\u611f\u53cd\u5e94\u5bf9\u7406\u89e3\u5a92\u4f53\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5916\u90e8\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u60c5\u611f\u4f30\u8ba1\u65b9\u6cd5\u5b66\u57fa\u7840\uff0c\u4ec5\u4ece\u89c6\u9891\u5185\u5bb9\u7279\u5f81\u51fa\u53d1\u8fdb\u884c\u60c5\u611f\u91cf\u5316\u3002", "method": "\u57fa\u4e8e\u81ea\u7531\u80fd\u539f\u7406\u6846\u67b6\uff0c\u4f7f\u7528Kullback-Leibler\u6563\u5ea6(KLD)\u6355\u6349\u9884\u6d4b\u8bef\u5dee\uff0c\u8d1d\u53f6\u65af\u60ca\u559c(BS)\u6355\u6349\u4fe1\u5ff5\u66f4\u65b0\uff0c\u4e0d\u786e\u5b9a\u6027(UN)\u53cd\u6620\u5148\u9a8c\u6a21\u7cca\u6027\u3002\u4f7f\u75281,059\u4e2a15\u79d2\u98df\u54c1\u5e7f\u544a\u89c6\u9891\uff0c\u4ece\u573a\u666f\u7ea7\u8868\u8fbe\u7279\u5f81\u91cf\u5316\u4e09\u79cd\u60c5\u611f\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1aKLD\u53cd\u6620\u4e0e\u54c1\u724c\u5448\u73b0\u76f8\u5173\u7684\"\u6109\u60a6\u5ea6\"\uff1bBS\u6355\u6349\u4fe1\u606f\u590d\u6742\u6027\u5f15\u8d77\u7684\"\u60ca\u559c\"\uff1bUN\u53cd\u6620\u5143\u7d20\u7c7b\u578b\u548c\u7a7a\u95f4\u6392\u5217\u4e0d\u786e\u5b9a\u6027\u3001\u4ee5\u53ca\u5448\u73b0\u5143\u7d20\u53d8\u5f02\u6027\u548c\u6570\u91cf\u9a71\u52a8\u7684\"\u60ca\u559c\"\u3002\u8bc6\u522b\u51fa\u4e09\u79cd\u7279\u5f81\u60c5\u611f\u6a21\u5f0f\uff1a\u4e0d\u786e\u5b9a\u523a\u6fc0\u3001\u6301\u7eed\u9ad8\u60c5\u611f\u3001\u77ac\u65f6\u5cf0\u503c\u8870\u51cf\u3002\u57289\u79cd\u8d85\u53c2\u6570\u8bbe\u7f6e\u548c6\u7c7b\u65e5\u672c\u5e7f\u544a\u89c6\u9891\u4e0a\u7684\u6cdb\u5316\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4ec5\u4ece\u89c6\u9891\u5185\u5bb9\u7279\u5f81\u8fdb\u884c\u53ef\u89e3\u91ca\u60c5\u611f\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u53ef\u6269\u5c55\u6574\u5408\u66f4\u591a\u8868\u8fbe\u5143\u7d20\u5e76\u901a\u8fc7\u4e3b\u89c2\u8bc4\u5206\u9a8c\u8bc1\uff0c\u6700\u7ec8\u6307\u5bfc\u5f00\u53d1\u652f\u6301\u521b\u4f5c\u66f4\u5177\u5438\u5f15\u529b\u5e7f\u544a\u89c6\u9891\u7684\u6280\u672f\u3002"}}
{"id": "2601.00814", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00814", "abs": "https://arxiv.org/abs/2601.00814", "authors": ["Abhishek Kumar"], "title": "Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections", "comment": null, "summary": "The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.", "AI": {"tldr": "\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u7cfb\u7edf\u4f7f\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5339\u914d\uff0c\u901a\u8fc7\u521b\u65b0\u63cf\u8ff0\u751f\u6210\u6280\u672f\u4e30\u5bcc\u672c\u4f53\u5b9e\u4f53\u4e0a\u4e0b\u6587\uff0c\u91c7\u7528\u5fae\u8c03\u7684\u591a\u8bed\u8a00Transformer\u6a21\u578b\u751f\u6210\u66f4\u597d\u5d4c\u5165\uff0c\u5728OAEI-2022\u591a\u8bed\u79cd\u8f68\u9053\u4e0a\u53d6\u5f9771% F1\u5206\u6570\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534716%\u3002", "motivation": "\u89e3\u51b3\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u6355\u6349\u8de8\u8bed\u8a00\u8bed\u4e49\u76f8\u4f3c\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6280\u672f\u6765\u63d0\u5347\u5bf9\u9f50\u7cbe\u5ea6\u3002", "method": "1. \u4f7f\u7528\u521b\u65b0\u6280\u672f\u751f\u6210\u63cf\u8ff0\u6765\u4e30\u5bcc\u672c\u4f53\u5b9e\u4f53\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff1b2. \u91c7\u7528\u5fae\u8c03\u7684\u591a\u8bed\u8a00Transformer\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5d4c\u5165\uff1b3. \u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5339\u914d\u6b63\u6837\u672c\u672c\u4f53\u5b9e\u4f53\u5bf9\uff1b4. \u5e94\u7528\u9608\u503c\u8fc7\u6ee4\u4fdd\u7559\u9ad8\u5ea6\u76f8\u4f3c\u5b9e\u4f53\u3002", "result": "\u5728OAEI-2022\u591a\u8bed\u79cd\u8f68\u9053\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u83b7\u5f9771% F1\u5206\u6570\uff0878%\u53ec\u56de\u7387\u548c65%\u7cbe\u786e\u7387\uff09\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u534716%\uff0c\u8868\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u6355\u6349\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u9f50\u6d41\u7a0b\u80fd\u6709\u6548\u6355\u6349\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u5b9e\u4f53\u63cf\u8ff0\u548c\u5fae\u8c03\u7684\u591a\u8bed\u8a00\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u672c\u4f53\u5bf9\u9f50\u6027\u80fd\u3002"}}
{"id": "2601.00831", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00831", "abs": "https://arxiv.org/abs/2601.00831", "authors": ["Uday Kumar Nidadala", "Venkata Bhumika Guthi"], "title": "Horizon Reduction as Information Loss in Offline Reinforcement Learning", "comment": "13 pages, 3 figures", "summary": "Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u89c6\u91ce\u7f29\u51cf\u7b56\u7565\u7684\u7406\u8bba\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u5373\u4f7f\u5728\u65e0\u9650\u6570\u636e\u548c\u5b8c\u7f8e\u51fd\u6570\u903c\u8fd1\u4e0b\uff0c\u56fa\u5b9a\u957f\u5ea6\u8f68\u8ff9\u6bb5\u5b66\u4e60\u53ef\u80fd\u5bfc\u81f4\u6700\u4f18\u7b56\u7565\u4e0e\u6b21\u4f18\u7b56\u7565\u7edf\u8ba1\u4e0d\u53ef\u533a\u5206\uff0c\u5b58\u5728\u4e0d\u53ef\u6062\u590d\u7684\u4fe1\u606f\u635f\u5931\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u91ce\u7f29\u51cf\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u88ab\u5e7f\u6cdb\u7528\u4e8e\u7f13\u89e3\u957f\u89c6\u91ce\u4fe1\u7528\u5206\u914d\u3001\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u5b9e\u73b0\u53ef\u6269\u5c55\u5b66\u4e60\uff0c\u4f46\u5176\u7406\u8bba\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u89c6\u91ce\u7f29\u51cf\u53ef\u80fd\u5bfc\u81f4\u6839\u672c\u6027\u4e14\u4e0d\u53ef\u6062\u590d\u7684\u4fe1\u606f\u635f\u5931\u3002", "method": "\u5c06\u89c6\u91ce\u7f29\u51cf\u5f62\u5f0f\u5316\u4e3a\u4ece\u56fa\u5b9a\u957f\u5ea6\u8f68\u8ff9\u6bb5\u5b66\u4e60\uff0c\u8bc1\u660e\u5728\u8fd9\u79cd\u8303\u5f0f\u4e0b\uff0c\u5373\u4f7f\u6709\u65e0\u9650\u6570\u636e\u548c\u5b8c\u7f8e\u51fd\u6570\u903c\u8fd1\uff0c\u6700\u4f18\u7b56\u7565\u4e5f\u53ef\u80fd\u4e0e\u6b21\u4f18\u7b56\u7565\u7edf\u8ba1\u4e0d\u53ef\u533a\u5206\u3002\u901a\u8fc7\u6784\u5efa\u6700\u5c0f\u53cd\u4f8b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u8bc6\u522b\u4e86\u4e09\u79cd\u7ed3\u6784\u5931\u6548\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u4e86\u4e09\u79cd\u7ed3\u6784\u5931\u6548\u6a21\u5f0f\uff1a(1)\u524d\u7f00\u4e0d\u53ef\u533a\u5206\u6027\u5bfc\u81f4\u53ef\u8bc6\u522b\u6027\u5931\u8d25\uff1b(2)\u622a\u65ad\u56de\u62a5\u5f15\u8d77\u7684\u76ee\u6807\u8bef\u8bbe\uff1b(3)\u79bb\u7ebf\u6570\u636e\u96c6\u652f\u6301\u548c\u8868\u793a\u6df7\u53e0\u3002\u5efa\u7acb\u4e86\u89c6\u91ce\u7f29\u51cf\u5b89\u5168\u6027\u7684\u5fc5\u8981\u6761\u4ef6\u3002", "conclusion": "\u89c6\u91ce\u7f29\u51cf\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u9650\u5236\u65e0\u6cd5\u4ec5\u901a\u8fc7\u7b97\u6cd5\u6539\u8fdb\u514b\u670d\u3002\u7814\u7a76\u7ed3\u679c\u8865\u5145\u4e86\u5173\u4e8e\u4fdd\u5b88\u76ee\u6807\u548c\u5206\u5e03\u504f\u79fb\u7684\u7b97\u6cd5\u5de5\u4f5c\uff0c\u4e3a\u5b89\u5168\u4f7f\u7528\u89c6\u91ce\u7f29\u51cf\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2601.00829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00829", "abs": "https://arxiv.org/abs/2601.00829", "authors": ["Alexander Vinogradov"], "title": "Can Generative Models Actually Forge Realistic Identity Documents?", "comment": "11 pages, 16 figures", "summary": "Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.", "AI": {"tldr": "\u5f53\u524d\u5f00\u6e90\u6269\u6563\u6a21\u578b\u80fd\u751f\u6210\u8868\u9762\u903c\u771f\u7684\u8eab\u4efd\u8bc1\u4ef6\u4f2a\u9020\u56fe\u50cf\uff0c\u4f46\u65e0\u6cd5\u8fbe\u5230\u7ed3\u6784\u6027\u548c\u6cd5\u8bc1\u5c42\u9762\u7684\u771f\u5b9e\u6027\uff0c\u56e0\u6b64\u751f\u6210\u5f0f\u8eab\u4efd\u8bc1\u4ef6\u6df1\u5ea6\u4f2a\u9020\u7684\u6cd5\u8bc1\u7ea7\u771f\u5b9e\u6027\u98ce\u9669\u53ef\u80fd\u88ab\u9ad8\u4f30\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u56fe\u50cf\u6a21\u578b\u5728\u56fe\u50cf\u771f\u5b9e\u611f\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u516c\u4f17\u62c5\u5fc3\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u6587\u4ef6\u4f2a\u9020\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5f53\u4ee3\u5f00\u6e90\u548c\u516c\u5f00\u53ef\u7528\u7684\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u662f\u5426\u80fd\u4ea7\u751f\u80fd\u591f\u771f\u5b9e\u7ed5\u8fc7\u4eba\u7c7b\u6216\u81ea\u52a8\u5316\u9a8c\u8bc1\u7cfb\u7edf\u7684\u8eab\u4efd\u8bc1\u4ef6\u4f2a\u9020\u54c1\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u516c\u5f00\u53ef\u7528\u7684\u751f\u6210\u6a21\u578b\u5bb6\u65cf\uff08\u5305\u62ecStable Diffusion\u3001Qwen\u3001Flux\u3001Nano-Banana\u7b49\uff09\uff0c\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u751f\u6210\u6d41\u7a0b\uff0c\u5206\u6790\u5b83\u4eec\u751f\u6210\u8eab\u4efd\u8bc1\u4ef6\u4f2a\u9020\u54c1\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u751f\u6210\u6a21\u578b\u80fd\u591f\u6a21\u62df\u8868\u9762\u5c42\u9762\u7684\u8bc1\u4ef6\u7f8e\u5b66\uff0c\u4f46\u65e0\u6cd5\u590d\u5236\u7ed3\u6784\u548c\u6cd5\u8bc1\u771f\u5b9e\u6027\u3002\u6a21\u578b\u5728\u751f\u6210\u7cbe\u786e\u7684\u6587\u672c\u5e03\u5c40\u3001\u5b89\u5168\u7279\u5f81\u548c\u6cd5\u8bc1\u53ef\u68c0\u6d4b\u7684\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u751f\u6210\u5f0f\u8eab\u4efd\u8bc1\u4ef6\u6df1\u5ea6\u4f2a\u9020\u8fbe\u5230\u6cd5\u8bc1\u7ea7\u771f\u5b9e\u6027\u7684\u98ce\u9669\u53ef\u80fd\u88ab\u9ad8\u4f30\u3002\u8fd9\u5f3a\u8c03\u4e86\u673a\u5668\u5b66\u4e60\u4ece\u4e1a\u8005\u4e0e\u6587\u4ef6\u6cd5\u8bc1\u4e13\u5bb6\u4e4b\u95f4\u5408\u4f5c\u5728\u73b0\u5b9e\u98ce\u9669\u8bc4\u4f30\u4e2d\u7684\u4ef7\u503c\uff0c\u4ee5\u53ca\u5f53\u524d\u6a21\u578b\u5728\u751f\u6210\u7ed3\u6784\u6027\u548c\u6cd5\u8bc1\u771f\u5b9e\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.00816", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00816", "abs": "https://arxiv.org/abs/2601.00816", "authors": ["Ismail Ahmad Abdullah"], "title": "MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback", "comment": "14 pages, 1 figure, 2 tables, 2 appendices with full proofs. Documents v0.9.4-pilot-audit-hardened audit surface with fail-closed governance, canonical JSON hashing, and artifact classification. Phase I infrastructure validation; no capability claims", "summary": "Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.\n  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.\n  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance", "AI": {"tldr": "MathLedger\u662f\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u673a\u5668\u8ba4\u77e5\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u5f62\u5f0f\u9a8c\u8bc1\u3001\u5bc6\u7801\u5b66\u8bc1\u660e\u548c\u5b66\u4e60\u52a8\u6001\u7684\u96c6\u6210\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u90e8\u7f72\u63d0\u4f9b\u900f\u660e\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u867d\u7136\u6027\u80fd\u5353\u8d8a\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u5b58\u5728\u4fe1\u4efb\u5371\u673a\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u9a8c\u8bc1\u7684\u673a\u5668\u8ba4\u77e5\u57fa\u7840\u3002", "method": "\u91c7\u7528\u53cd\u5c04\u5f0f\u5f62\u5f0f\u5b66\u4e60\uff08RFL\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7b26\u53f7\u5316\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u7ed3\u679c\u800c\u975e\u7edf\u8ba1\u635f\u5931\u6765\u9a71\u52a8\u66f4\u65b0\uff1b\u7cfb\u7edf\u96c6\u6210\u4e86\u5f62\u5f0f\u9a8c\u8bc1\u3001\u5bc6\u7801\u5b66\u8bc1\u660e\u548c\u5b66\u4e60\u52a8\u6001\u3002", "result": "\u7b2c\u4e00\u9636\u6bb5\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6d4b\u91cf\u548c\u6cbb\u7406\u57fa\u7840\u8bbe\u65bd\uff1aCAL-EXP-3\u9a8c\u8bc1\u4e86\u6d4b\u91cf\u57fa\u7840\u8bbe\u65bd\uff08Delta p\u8ba1\u7b97\u3001\u65b9\u5dee\u8ddf\u8e2a\uff09\uff1b\u538b\u529b\u6d4b\u8bd5\u786e\u8ba4\u4e86\u5728\u8d85\u51fa\u8fb9\u754c\u6761\u4ef6\u4e0b\u6545\u969c\u5173\u95ed\u6cbb\u7406\u89e6\u53d1\u5668\u6b63\u5e38\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\u8bbe\u65bd\u8d21\u732e\uff1a\u4e00\u4e2a\u53ef\u5927\u89c4\u6a21\u5ba1\u8ba1\u7684\u8d26\u672c\u8bc1\u660e\u5b66\u4e60\u539f\u578b\u7cfb\u7edf\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u673a\u5668\u5b66\u4e60\u5efa\u7acb\u4e86\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2601.01118", "categories": ["cs.IR", "cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2601.01118", "abs": "https://arxiv.org/abs/2601.01118", "authors": ["Qingqing Long", "Haotian Chen", "Chenyang Zhao", "Xiaolei Du", "Xuezhi Wang", "Pengyao Wang", "Chengzan Li", "Yuanchun Zhou", "Hengshu Zhu"], "title": "ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services", "comment": "12 pages, 9 figures", "summary": "The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.", "AI": {"tldr": "ScienceDB AI\u662f\u57fa\u4e8eScienceDB\u79d1\u5b66\u6570\u636e\u5171\u4eab\u5e73\u53f0\u7684LLM\u9a71\u52a8\u5bf9\u8bdd\u5f0f\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u8bed\u4e49\u7406\u89e3\u548c\u4e2a\u6027\u5316\u63a8\u8350\u89e3\u51b3\u79d1\u5b66\u6570\u636e\u96c6\u5171\u4eab\u5229\u7528\u96be\u9898\u3002", "motivation": "\u79d1\u5b66\u6570\u636e\u96c6\u5305\u542b\u590d\u6742\u7684\u9886\u57df\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\uff0c\u4f20\u7edf\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002\u968f\u7740LLM\u7684\u53d1\u5c55\uff0c\u9700\u8981\u6784\u5efa\u80fd\u591f\u6df1\u5ea6\u7406\u89e3\u8bed\u4e49\u5e76\u8fdb\u884c\u4e2a\u6027\u5316\u63a8\u8350\u7684\u5bf9\u8bdd\u5f0f\u63a8\u8350\u7cfb\u7edf\uff0c\u4ee5\u4fc3\u8fdb\u79d1\u5b66\u6570\u636e\u96c6\u7684\u5171\u4eab\u548c\u5229\u7528\u3002", "method": "\u5f00\u53d1\u4e86ScienceDB AI\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1\uff09\u79d1\u5b66\u610f\u56fe\u611f\u77e5\u5668\u63d0\u53d6\u7ed3\u6784\u5316\u5b9e\u9a8c\u5143\u7d20\uff1b2\uff09\u7ed3\u6784\u5316\u8bb0\u5fc6\u538b\u7f29\u5668\u7ba1\u7406\u591a\u8f6e\u5bf9\u8bdd\uff1b3\uff09\u53ef\u4fe1\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u68c0\u7d22\u673a\u5236\u5e76\u63d0\u4f9b\u53ef\u5f15\u7528\u7684CSTR\u6807\u8bc6\u7b26\u3002", "result": "\u901a\u8fc7\u8d85\u8fc71000\u4e07\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\uff0cScienceDB AI\u663e\u793a\u51fa\u663e\u8457\u7684\u6709\u6548\u6027\uff0c\u6210\u4e3a\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u96c6\u5171\u4eab\u670d\u52a1\u7684LLM\u9a71\u52a8\u5bf9\u8bdd\u5f0f\u63a8\u8350\u7cfb\u7edf\u3002", "conclusion": "ScienceDB AI\u901a\u8fc7LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u5f0f\u63a8\u8350\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79d1\u5b66\u6570\u636e\u96c6\u5171\u4eab\u5229\u7528\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u79d1\u5b66\u6570\u636e\u5171\u4eab\u5e73\u53f0\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01303", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01303", "abs": "https://arxiv.org/abs/2601.01303", "authors": ["Michael Smith", "Riley Grossman", "Antonio Torres-Aguero", "Pritam Sen", "Cristian Borcea", "Yi Chen"], "title": "Inconsistencies in Classification of Online News Articles: A Call for Common Standards in Brand Safety Services", "comment": "17 pages", "summary": "This study examines inconsistencies in the brand safety classifications of online news articles by analyzing ratings from three leading brand safety providers, DoubleVerify, Integral Ad Science, and Oracle. We focus on news content because of its central role in public discourse and the significant financial consequences of unsafe classifications in a sector that is already underserved by digital ad spending. By collecting data from 4,352 news articles on 51 domains, our analysis shows that brand safety services often produce conflicting classifications, with significant discrepancies between providers. These inconsistencies can have harmful consequences for both advertisers and publishers, leading to misplaced advertising spending and revenue losses. This research provides critical insights into the shortcomings of the current brand safety landscape. We argue for a standardized and transparent brand safety system to mitigate the harmful effects of the current system on the digital advertising ecosystem.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u4e09\u5bb6\u4e3b\u8981\u54c1\u724c\u5b89\u5168\u670d\u52a1\u5546\u5bf9\u5728\u7ebf\u65b0\u95fb\u6587\u7ae0\u5206\u7c7b\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u5bf9\u5e7f\u544a\u5546\u548c\u51fa\u7248\u5546\u90fd\u6709\u5bb3", "motivation": "\u65b0\u95fb\u5185\u5bb9\u5728\u516c\u5171\u8bdd\u8bed\u4e2d\u626e\u6f14\u6838\u5fc3\u89d2\u8272\uff0c\u800c\u54c1\u724c\u5b89\u5168\u5206\u7c7b\u7684\u4e0d\u4e00\u81f4\u4f1a\u5bfc\u81f4\u6570\u5b57\u5e7f\u544a\u652f\u51fa\u9519\u914d\u548c\u6536\u5165\u635f\u5931\uff0c\u7279\u522b\u662f\u5728\u6570\u5b57\u5e7f\u544a\u6295\u5165\u4e0d\u8db3\u7684\u65b0\u95fb\u884c\u4e1a", "method": "\u6536\u96c6\u4e8651\u4e2a\u57df\u540d\u76844,352\u7bc7\u65b0\u95fb\u6587\u7ae0\u6570\u636e\uff0c\u5206\u6790DoubleVerify\u3001Integral Ad Science\u548cOracle\u4e09\u5bb6\u4e3b\u8981\u54c1\u724c\u5b89\u5168\u670d\u52a1\u5546\u7684\u5206\u7c7b\u8bc4\u7ea7", "result": "\u54c1\u724c\u5b89\u5168\u670d\u52a1\u7ecf\u5e38\u4ea7\u751f\u51b2\u7a81\u7684\u5206\u7c7b\u7ed3\u679c\uff0c\u4e0d\u540c\u63d0\u4f9b\u5546\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u4e9b\u4e0d\u4e00\u81f4\u6027\u5bf9\u5e7f\u544a\u5546\u548c\u51fa\u7248\u5546\u90fd\u6709\u5bb3", "conclusion": "\u5f53\u524d\u54c1\u724c\u5b89\u5168\u7cfb\u7edf\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u548c\u900f\u660e\u7684\u54c1\u724c\u5b89\u5168\u7cfb\u7edf\u6765\u51cf\u8f7b\u5bf9\u6570\u5b57\u5e7f\u544a\u751f\u6001\u7cfb\u7edf\u7684\u6709\u5bb3\u5f71\u54cd"}}
{"id": "2601.01774", "categories": ["cs.AI", "cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2601.01774", "abs": "https://arxiv.org/abs/2601.01774", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches", "comment": "14 pages", "summary": "Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6c42\u89e3\u8d85\u8d8a\u65b9\u7a0b\u7684\u80fd\u529b\uff0c\u6bd4\u8f83\u76f4\u63a5\u6570\u503c\u9884\u6d4b\u4e0e\u7ed3\u5408\u7ecf\u5178\u8fed\u4ee3\u6c42\u89e3\u5668\u7684\u6df7\u5408\u67b6\u6784\u6548\u679c\u3002\u7814\u7a76\u53d1\u73b0\u6df7\u5408\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8bef\u5dee\uff0c\u8868\u660eLLM\u66f4\u9002\u5408\u4f5c\u4e3a\u7ecf\u5178\u6570\u503c\u6c42\u89e3\u5668\u7684\u667a\u80fd\u63a5\u53e3\u800c\u975e\u72ec\u7acb\u8ba1\u7b97\u5f15\u64ce\u3002", "motivation": "\u8d85\u8d8a\u65b9\u7a0b\u5728\u5de5\u7a0b\u5b9e\u8df5\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u9700\u8981\u8fed\u4ee3\u6570\u503c\u6c42\u89e3\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u76f4\u63a5\u89e3\u51b3\u8fd9\u4e9b\u65b9\u7a0b\uff0c\u8fd8\u662f\u9700\u8981\u4e0e\u7ecf\u5178\u8fed\u4ee3\u6c42\u89e3\u5668\u7ed3\u5408\u7684\u6df7\u5408\u67b6\u6784\u66f4\u4e3a\u6709\u6548\u3002", "method": "\u6d4b\u8bd56\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5\uff09\u5728100\u4e2a\u6db5\u76d67\u4e2a\u5de5\u7a0b\u9886\u57df\u7684\u95ee\u9898\u4e0a\u3002\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u76f4\u63a5\u6570\u503c\u9884\u6d4b\uff1b2\uff09\u6c42\u89e3\u5668\u8f85\u52a9\u8ba1\u7b97\uff0c\u5176\u4e2dLLM\u5236\u5b9a\u63a7\u5236\u65b9\u7a0b\u5e76\u63d0\u4f9b\u521d\u59cb\u6761\u4ef6\uff0c\u800c\u725b\u987f-\u62c9\u5f17\u68ee\u8fed\u4ee3\u6267\u884c\u6570\u503c\u6c42\u89e3\u3002", "result": "\u76f4\u63a5\u9884\u6d4b\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4e3a0.765\u52301.262\uff0c\u800c\u6c42\u89e3\u5668\u8f85\u52a9\u8ba1\u7b97\u8fbe\u52300.225\u52300.301\uff0c\u8bef\u5dee\u51cf\u5c11\u4e8667.9%\u523081.8%\u3002\u9886\u57df\u7279\u5b9a\u5206\u6790\u663e\u793a\uff0c\u5728\u7535\u5b50\u5b66\u4e2d\u6539\u8fdb\u663e\u8457\uff0893.1%\uff09\uff0c\u800c\u5728\u6d41\u4f53\u529b\u5b66\u4e2d\u6539\u8fdb\u8f83\u5c0f\uff087.2%\uff09\uff0c\u56e0\u4e3aLLM\u5728\u8be5\u9886\u57df\u8868\u73b0\u51fa\u6709\u6548\u7684\u6a21\u5f0f\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u7b26\u53f7\u64cd\u4f5c\u548c\u9886\u57df\u77e5\u8bc6\u68c0\u7d22\uff0c\u4f46\u5728\u7cbe\u5ea6\u5173\u952e\u7684\u8fed\u4ee3\u7b97\u672f\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u8fd9\u8868\u660e\u5b83\u4eec\u6700\u9002\u5408\u4f5c\u4e3a\u7ecf\u5178\u6570\u503c\u6c42\u89e3\u5668\u7684\u667a\u80fd\u63a5\u53e3\u90e8\u7f72\uff0c\u800c\u4e0d\u662f\u4f5c\u4e3a\u72ec\u7acb\u7684\u8ba1\u7b97\u5f15\u64ce\u3002"}}
{"id": "2601.00837", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00837", "abs": "https://arxiv.org/abs/2601.00837", "authors": ["Agniv Roy Choudhury"], "title": "Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs", "comment": null, "summary": "Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.\n  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.\n  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.\n  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\\% accuracy, 99.61\\% F1-score, and 99.93\\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.\n  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.\n  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4ece\u5934\u8bad\u7ec3\u7684CNN\u4e0e\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff08ResNet50\u3001DenseNet121\u3001EfficientNet-B0\uff09\u5728\u513f\u7ae5\u80ba\u708e\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5fae\u8c03\u540e\u7684ResNet50\u8fbe\u523099.43%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u80ba\u708e\u662f\u4e94\u5c81\u4ee5\u4e0b\u513f\u7ae5\u7684\u4e3b\u8981\u6b7b\u56e0\uff0c\u6bcf\u5e74\u5bfc\u81f4\u8d85\u8fc770\u4e07\u4f8b\u6b7b\u4ea1\u3002\u80f8\u90e8X\u5149\u7247\u7684\u51c6\u786e\u8bca\u65ad\u53d7\u5230\u653e\u5c04\u79d1\u533b\u751f\u53ef\u7528\u6027\u548c\u8bca\u65ad\u5dee\u5f02\u6027\u7684\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u51c6\u786e\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u4f7f\u75285,216\u5f20\u513f\u7ae5\u80f8\u90e8X\u5149\u7247\u6570\u636e\u96c6\uff0c\u630980/10/10\u6bd4\u4f8b\u5206\u4e3a\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u3002\u6bd4\u8f83\u4e86\u4ece\u5934\u8bad\u7ec3\u7684CNN\u4e0e\u4e09\u79cd\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff08ResNet50\u3001DenseNet121\u3001EfficientNet-B0\uff09\uff0c\u8bc4\u4f30\u4e86\u51bb\u7ed3\u4e3b\u5e72\u548c\u5fae\u8c03\u4e24\u79cd\u7b56\u7565\u3002\u4f7f\u7528\u51c6\u786e\u7387\u3001F1\u5206\u6570\u548cAUC\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7Grad-CAM\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u53ef\u89c6\u5316\u3002", "result": "\u5fae\u8c03\u540e\u7684ResNet50\u8868\u73b0\u6700\u4f73\uff1a99.43%\u51c6\u786e\u7387\u300199.61% F1\u5206\u6570\u548c99.93% AUC\uff0c\u4ec5\u67093\u4e2a\u9519\u8bef\u5206\u7c7b\u3002\u5fae\u8c03\u6a21\u578b\u5e73\u5747\u6bd4\u51bb\u7ed3\u4e3b\u5e72\u6a21\u578b\u9ad8\u51fa5.5\u4e2a\u767e\u5206\u70b9\u3002Grad-CAM\u786e\u8ba4\u6a21\u578b\u5173\u6ce8\u4e34\u5e8a\u76f8\u5173\u7684\u80ba\u90e8\u533a\u57df\u8fdb\u884c\u9884\u6d4b\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u7ed3\u5408\u5fae\u8c03\u7b56\u7565\u5728\u513f\u7ae5\u80ba\u708e\u68c0\u6d4b\u4e2d\u663e\u8457\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684CNN\uff0c\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u3002\u8be5\u7cfb\u7edf\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u4f5c\u4e3a\u7b5b\u67e5\u5de5\u5177\u7684\u6f5c\u529b\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u5728\u591a\u4e2d\u5fc3\u548c\u6210\u4eba\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u8fd9\u4e9b\u53d1\u73b0\u3002"}}
{"id": "2601.00818", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00818", "abs": "https://arxiv.org/abs/2601.00818", "authors": ["Chandra Sekhar Kubam"], "title": "Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making", "comment": "8 pages", "summary": "Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAgentic AI\u6846\u67b6\u7684\u81ea\u4e3b\u4fe1\u7528\u98ce\u9669\u51b3\u7b56\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u53ef\u89e3\u91caAI\u6a21\u5757\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u900f\u660e\u7684\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u5728\u51b3\u7b56\u901f\u5ea6\u3001\u900f\u660e\u5ea6\u548c\u54cd\u5e94\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u91d1\u878d\u670d\u52a1\u5728\u77ed\u65f6\u95f4\u5185\u7684\u5927\u89c4\u6a21\u6570\u5b57\u5316\u5bfc\u81f4\u5bf9\u81ea\u4e3b\u3001\u900f\u660e\u3001\u5b9e\u65f6\u7684\u4fe1\u7528\u98ce\u9669\u51b3\u7b56\u7cfb\u7edf\u7684\u8feb\u5207\u9700\u6c42\u3002\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u867d\u7136\u64c5\u957f\u6a21\u5f0f\u8bc6\u522b\uff0c\u4f46\u7f3a\u4e4f\u73b0\u4ee3\u91d1\u878d\u8fd0\u8425\u6240\u9700\u7684\u9002\u5e94\u6027\u63a8\u7406\u3001\u60c5\u5883\u611f\u77e5\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86Agentic AI\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5305\u542b\u5f3a\u5316\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u3001\u53ef\u89e3\u91caAI\u6a21\u5757\u548c\u5b9e\u65f6\u6570\u636e\u5438\u6536\u7ba1\u9053\u3002\u7cfb\u7edf\u5305\u62ec\u667a\u80fd\u4f53\u534f\u4f5c\u534f\u8bae\u3001\u98ce\u9669\u8bc4\u5206\u5f15\u64ce\u3001\u53ef\u89e3\u91ca\u6027\u5c42\u548c\u6301\u7eed\u53cd\u9988\u5b66\u4e60\u5faa\u73af\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8be5\u7cfb\u7edf\u5728\u51b3\u7b56\u901f\u5ea6\u3001\u900f\u660e\u5ea6\u548c\u54cd\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u4fe1\u7528\u8bc4\u5206\u6a21\u578b\u3002\u4f46\u4ecd\u5b58\u5728\u6a21\u578b\u6f02\u79fb\u98ce\u9669\u3001\u9ad8\u7ef4\u6570\u636e\u89e3\u91ca\u4e0d\u4e00\u81f4\u3001\u76d1\u7ba1\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u4f4e\u8d44\u6e90\u73af\u5883\u57fa\u7840\u8bbe\u65bd\u9650\u5236\u7b49\u5b9e\u9645\u9650\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u5177\u6709\u53d8\u9769\u4fe1\u7528\u5206\u6790\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u52a8\u6001\u76d1\u7ba1\u5408\u89c4\u673a\u5236\u3001\u65b0\u578b\u667a\u80fd\u4f53\u534f\u4f5c\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u4ee5\u53ca\u5728\u8de8\u56fd\u4fe1\u7528\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21\u5b9e\u65bd\u3002"}}
{"id": "2601.00834", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00834", "abs": "https://arxiv.org/abs/2601.00834", "authors": ["Julian Evan Chrisnanto", "Salsabila Rahma Alia", "Nurfauzi Fadillah", "Yulison Herry Chrisnanto"], "title": "Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds", "comment": "19 pages, 7 figures", "summary": "Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a \"Stochastic Cloth\" manifold with extreme Gaussian curvature fluctuations ($K \\in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the \"splitting spot\" and \"labyrinthine\" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\\mathcal{E}_{mass} \\approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.", "AI": {"tldr": "IM-PINN\uff1a\u4e00\u79cd\u65e0\u9700\u7f51\u683c\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fde\u7eed\u53c2\u6570\u57df\u4e2d\u5d4c\u5165\u9ece\u66fc\u5ea6\u91cf\u5f20\u91cf\uff0c\u76f4\u63a5\u6c42\u89e3\u590d\u6742\u975e\u6b27\u6d41\u5f62\u4e0a\u7684\u975e\u7ebf\u6027\u53cd\u5e94-\u6269\u6563\u65b9\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u4fdd\u771f\u7f51\u683c\u751f\u6210\u6210\u672c\u548c\u8f9b\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u5728\u590d\u6742\u975e\u6b27\u6d41\u5f62\u4e0a\u6a21\u62df\u975e\u7ebf\u6027\u53cd\u5e94-\u6269\u6563\u52a8\u529b\u5b66\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u9ad8\u4fdd\u771f\u7f51\u683c\u751f\u6210\u6210\u672c\u9ad8\u6602\uff0c\u4ee5\u53ca\u79bb\u6563\u65f6\u95f4\u6b65\u8fdb\u65b9\u6848\u4e2d\u7684\u8f9b\u6f02\u79fb\u95ee\u9898\u3002\u4f20\u7edf\u81ea\u9002\u5e94\u7ec6\u5316\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u6781\u7aef\u9ad8\u65af\u66f2\u7387\u6ce2\u52a8\u4e0b\u7684\u5404\u5411\u5f02\u6027\u56fe\u7075\u4e0d\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faIM-PINN\u6846\u67b6\uff1a1\uff09\u5c06\u9ece\u66fc\u5ea6\u91cf\u5f20\u91cf\u5d4c\u5165\u81ea\u52a8\u5fae\u5206\u56fe\u4e2d\uff0c\u89e3\u6790\u91cd\u5efa\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\uff1b2\uff09\u91c7\u7528\u53cc\u6d41\u67b6\u6784\u548c\u5085\u91cc\u53f6\u7279\u5f81\u5d4c\u5165\u7f13\u89e3\u8c31\u504f\u5dee\uff1b3\uff09\u5728\u5177\u6709\u6781\u7aef\u9ad8\u65af\u66f2\u7387\u6ce2\u52a8\uff08K\u2208[-2489,3580]\uff09\u7684\"\u968f\u673a\u5e03\u6599\"\u6d41\u5f62\u4e0a\u9a8c\u8bc1\uff1b4\uff09\u5e94\u7528\u4e8eGray-Scott\u6a21\u578b\u6062\u590d\"\u5206\u88c2\u6591\u70b9\"\u548c\"\u8ff7\u5bab\"\u6a21\u5f0f\u3002", "result": "IM-PINN\u5728\u7269\u7406\u4e25\u8c28\u6027\u4e0a\u4f18\u4e8e\u8868\u9762\u6709\u9650\u5143\u6cd5\uff1a\u5168\u5c40\u8d28\u91cf\u5b88\u6052\u8bef\u5dee\u4e3a0.157\uff08SFEM\u4e3a0.258\uff09\uff1b\u4f5c\u4e3a\u70ed\u529b\u5b66\u4e00\u81f4\u7684\u5168\u5c40\u6c42\u89e3\u5668\uff0c\u6d88\u9664\u4e86\u534a\u9690\u5f0f\u79ef\u5206\u56fa\u6709\u7684\u8d28\u91cf\u6f02\u79fb\uff1b\u6210\u529f\u6062\u590d\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u89e3\u6790\u7684\u5404\u5411\u5f02\u6027\u56fe\u7075\u4e0d\u7a33\u5b9a\u6027\u6a21\u5f0f\u3002", "conclusion": "IM-PINN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5185\u5b58\u9ad8\u6548\u3001\u5206\u8fa8\u7387\u72ec\u7acb\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u5728\u6f14\u5316\u8868\u9762\u4e0a\u6a21\u62df\u751f\u7269\u6a21\u5f0f\u5f62\u6210\uff0c\u5f25\u5408\u4e86\u5fae\u5206\u51e0\u4f55\u548c\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u8ba1\u7b97\u5f62\u6001\u53d1\u751f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01448", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01448", "abs": "https://arxiv.org/abs/2601.01448", "authors": ["Na Li", "Fanghui Sun", "Yan Zou", "Yangfu Zhu", "Xiatian Zhu", "Ying Ma"], "title": "Adaptive Diffusion-based Augmentation for Recommendation", "comment": null, "summary": "Recommendation systems often rely on implicit feedback, where only positive user-item interactions can be observed. Negative sampling is therefore crucial to provide proper negative training signals. However, existing methods tend to mislabel potentially positive but unobserved items as negatives and lack precise control over negative sample selection. We aim to address these by generating controllable negative samples, rather than sampling from the existing item pool. In this context, we propose Adaptive Diffusion-based Augmentation for Recommendation (ADAR), a novel and model-agnostic module that leverages diffusion to synthesize informative negatives. Inspired by the progressive corruption process in diffusion, ADAR simulates a continuous transition from positive to negative, allowing for fine-grained control over sample hardness. To mine suitable negative samples, we theoretically identify the transition point at which a positive sample turns negative and derive a score-aware function to adaptively determine the optimal sampling timestep. By identifying this transition point, ADAR generates challenging negative samples that effectively refine the model's decision boundary. Experiments confirm that ADAR is broadly compatible and boosts the performance of existing recommendation models substantially, including collaborative filtering and sequential recommendation, without architectural modifications.", "AI": {"tldr": "\u63d0\u51faADAR\u65b9\u6cd5\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u53ef\u63a7\u7684\u8d1f\u6837\u672c\uff0c\u901a\u8fc7\u8bc6\u522b\u6b63\u8d1f\u6837\u672c\u8f6c\u6362\u70b9\u6765\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u9690\u5f0f\u53cd\u9988\uff0c\u53ea\u80fd\u89c2\u5bdf\u5230\u6b63\u9762\u7684\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\uff0c\u8d1f\u91c7\u6837\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u53ef\u80fd\u5c06\u6f5c\u5728\u6b63\u9762\u4f46\u672a\u89c2\u5bdf\u5230\u7684\u7269\u54c1\u8bef\u6807\u4e3a\u8d1f\u6837\u672c\uff1b2) \u7f3a\u4e4f\u5bf9\u8d1f\u6837\u672c\u9009\u62e9\u7684\u7cbe\u786e\u63a7\u5236", "method": "\u63d0\u51faADAR\uff08\u81ea\u9002\u5e94\u6269\u6563\u589e\u5f3a\u63a8\u8350\uff09\u6a21\u5757\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5408\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u8d1f\u6837\u672c\u3002\u53d7\u6269\u6563\u8fc7\u7a0b\u4e2d\u6e10\u8fdb\u5f0f\u7834\u574f\u8fc7\u7a0b\u7684\u542f\u53d1\uff0cADAR\u6a21\u62df\u4ece\u6b63\u6837\u672c\u5230\u8d1f\u6837\u672c\u7684\u8fde\u7eed\u8fc7\u6e21\uff0c\u5141\u8bb8\u5bf9\u6837\u672c\u96be\u5ea6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u901a\u8fc7\u7406\u8bba\u8bc6\u522b\u6b63\u6837\u672c\u8f6c\u53d8\u4e3a\u8d1f\u6837\u672c\u7684\u8f6c\u6362\u70b9\uff0c\u5e76\u63a8\u5bfc\u51fa\u57fa\u4e8e\u5206\u6570\u7684\u51fd\u6570\u6765\u81ea\u9002\u5e94\u786e\u5b9a\u6700\u4f73\u91c7\u6837\u65f6\u95f4\u6b65", "result": "\u5b9e\u9a8c\u8bc1\u5b9eADAR\u5177\u6709\u5e7f\u6cdb\u7684\u517c\u5bb9\u6027\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u73b0\u6709\u63a8\u8350\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5305\u62ec\u534f\u540c\u8fc7\u6ee4\u548c\u5e8f\u5217\u63a8\u8350\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784", "conclusion": "ADAR\u901a\u8fc7\u751f\u6210\u53ef\u63a7\u7684\u8d1f\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8d1f\u91c7\u6837\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u8d1f\u6837\u672c\u6765\u4f18\u5316\u6a21\u578b\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd"}}
{"id": "2601.01331", "categories": ["cs.CY", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01331", "abs": "https://arxiv.org/abs/2601.01331", "authors": ["Hongkun Yang", "Lionel Z. Wang", "Wei Fan", "Yiran Hu", "Lixu Wang", "Chenyu Liu", "Shenghong Fu", "Haoyang Li", "Xin Xu", "Jiexin Zheng", "Wei Dong"], "title": "AppellateGen: A Benchmark for Appellate Legal Judgment Generation", "comment": "15 pages, 4 figures, 3 tables", "summary": "Legal judgment generation is a critical task in legal intelligence. However, existing research in legal judgment generation has predominantly focused on first-instance trials, relying on static fact-to-verdict mappings while neglecting the dialectical nature of appellate (second-instance) review. To address this, we introduce AppellateGen, a benchmark for second-instance legal judgment generation comprising 7,351 case pairs. The task requires models to draft legally binding judgments by reasoning over the initial verdict and evidentiary updates, thereby modeling the causal dependency between trial stages. We further propose a judicial Standard Operating Procedure (SOP)-based Legal Multi-Agent System (SLMAS) to simulate judicial workflows, which decomposes the generation process into discrete stages of issue identification, retrieval, and drafting. Experimental results indicate that while SLMAS improves logical consistency, the complexity of appellate reasoning remains a substantial challenge for current LLMs. The dataset and code are publicly available at: https://anonymous.4open.science/r/AppellateGen-5763.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AppellateGen\u57fa\u51c6\uff0c\u7528\u4e8e\u4e0a\u8bc9\u5ba1\uff08\u4e8c\u5ba1\uff09\u6cd5\u5f8b\u5224\u51b3\u751f\u6210\uff0c\u5305\u542b7,351\u4e2a\u6848\u4f8b\u5bf9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u53f8\u6cd5\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f\u7684SLMAS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6765\u6a21\u62df\u53f8\u6cd5\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5224\u51b3\u751f\u6210\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e00\u5ba1\u5ba1\u5224\uff0c\u4f9d\u8d56\u9759\u6001\u7684\u4e8b\u5b9e\u5230\u5224\u51b3\u6620\u5c04\uff0c\u5ffd\u89c6\u4e86\u4e0a\u8bc9\u5ba1\uff08\u4e8c\u5ba1\uff09\u7684\u8fa9\u8bc1\u6027\u8d28\u3002\u4e0a\u8bc9\u5ba1\u9700\u8981\u5bf9\u521d\u59cb\u5224\u51b3\u548c\u8bc1\u636e\u66f4\u65b0\u8fdb\u884c\u63a8\u7406\uff0c\u5efa\u6a21\u5ba1\u5224\u9636\u6bb5\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u53f8\u6cd5\u6807\u51c6\u64cd\u4f5c\u7a0b\u5e8f(SOP)\u7684\u6cd5\u5f8b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf(SLMAS)\uff0c\u5c06\u751f\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u95ee\u9898\u8bc6\u522b\u3001\u68c0\u7d22\u548c\u8d77\u8349\u4e09\u4e2a\u79bb\u6563\u9636\u6bb5\uff0c\u6a21\u62df\u53f8\u6cd5\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136SLMAS\u63d0\u9ad8\u4e86\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f46\u4e0a\u8bc9\u63a8\u7406\u7684\u590d\u6742\u6027\u5bf9\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "AppellateGen\u57fa\u51c6\u586b\u8865\u4e86\u4e0a\u8bc9\u5ba1\u6cd5\u5f8b\u5224\u51b3\u751f\u6210\u7814\u7a76\u7684\u7a7a\u767d\uff0cSLMAS\u7cfb\u7edf\u901a\u8fc7\u6a21\u62df\u53f8\u6cd5\u5de5\u4f5c\u6d41\u7a0b\u6539\u8fdb\u4e86\u903b\u8f91\u4e00\u81f4\u6027\uff0c\u4f46\u4e0a\u8bc9\u63a8\u7406\u7684\u590d\u6742\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2601.00839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00839", "abs": "https://arxiv.org/abs/2601.00839", "authors": ["Zahid Ullah", "Muhammad Hilal", "Eunsoo Lee", "Dragan Pamucar", "Jihie Kim"], "title": "Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS", "comment": null, "summary": "Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.", "AI": {"tldr": "\u672c\u6587\u7ed3\u5408\u5fc3\u810f\u8d85\u58f0\u5206\u5272\u6587\u732e\u7efc\u8ff0\u4e0e\u4e09\u79cd\u4e3b\u6d41\u67b6\u6784\uff08U-Net\u3001Attention U-Net\u3001TransUNet\uff09\u5728CAMUS\u6570\u636e\u96c6\u4e0a\u7684\u7edf\u4e00\u5b9e\u9a8c\u57fa\u51c6\u6bd4\u8f83\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u9884\u5904\u7406\u548c\u8bc4\u4f30\u6307\u5357\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u591a\u603b\u7ed3\u5fc3\u810f\u5f71\u50cf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u57fa\u51c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u67b6\u6784\u5728\u6807\u51c6\u5316\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u4e3a\u5fc3\u810f\u8d85\u58f0\u5206\u5272\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "method": "1. \u5bf9\u5fc3\u810f\u8d85\u58f0\u5206\u5272\u6587\u732e\u8fdb\u884c\u805a\u7126\u7efc\u8ff0\uff1b2. \u5728CAMUS\u8d85\u58f0\u5fc3\u52a8\u56fe\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u4e09\u79cd\u67b6\u6784\uff1aU-Net\u3001Attention U-Net\u3001TransUNet\uff1b3. \u91c7\u7528\u591a\u79cd\u9884\u5904\u7406\u8def\u5f84\uff1a\u539f\u751fNIfTI\u300116\u4f4dPNG\u5bfc\u51fa\u3001GPT\u8f85\u52a9\u591a\u8fb9\u5f62\u4f2a\u6807\u7b7e\u3001\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff1b4. \u4f7f\u7528\u76f8\u540c\u7684\u8bad\u7ec3\u5212\u5206\u3001\u635f\u5931\u51fd\u6570\u548c\u8bc4\u4f30\u6807\u51c6\u3002", "result": "1. \u539f\u751fNIfTI\u6570\u636e\u8bad\u7ec3\u7684\u666e\u901aU-Net\u8fbe\u523094%\u5e73\u5747Dice\u7cfb\u6570\uff0cPNG-16\u4f4d\u6d41\u7a0b\u5728\u76f8\u4f3c\u6761\u4ef6\u4e0b\u8fbe\u523091%\uff1b2. Attention U-Net\u5728\u5c0f\u533a\u57df\u6216\u4f4e\u5bf9\u6bd4\u5ea6\u533a\u57df\u6709\u9002\u5ea6\u6539\u8fdb\uff0c\u51cf\u5c11\u8fb9\u754c\u6cc4\u6f0f\uff1b3. TransUNet\u5728\u6311\u6218\u6027\u5e27\u4e0a\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u521d\u59cb\u5316\u65f6\uff1b4. \u4f2a\u6807\u7b7e\u6269\u5c55\u8bad\u7ec3\u96c6\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8d21\u732e\u5305\u62ec\uff1a1. \u5728\u6807\u51c6\u5316CAMUS\u9884\u5904\u7406\u548c\u8bc4\u4f30\u4e0b\u5bf9\u4e09\u79cd\u67b6\u6784\u7684\u7edf\u4e00\u57fa\u51c6\u6bd4\u8f83\uff1b2. \u8d85\u58f0\u6570\u636e\u51c6\u5907\u7684\u5b9e\u7528\u6307\u5357\uff08\u5f3a\u5ea6\u4fdd\u771f\u5ea6\u3001\u5206\u8fa8\u7387\u4e00\u81f4\u6027\u3001\u5bf9\u9f50\uff09\uff1b3. \u5c55\u671b\u53ef\u6269\u5c55\u7684\u81ea\u76d1\u7763\u548c\u65b0\u5174\u591a\u6a21\u6001GPT\u6807\u6ce8\u6d41\u7a0b\uff0c\u7528\u4e8e\u5feb\u901f\u6807\u6ce8\u3001\u8d28\u91cf\u4fdd\u8bc1\u548c\u9488\u5bf9\u6027\u6570\u636e\u96c6\u6784\u5efa\u3002"}}
{"id": "2601.00821", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.00821", "abs": "https://arxiv.org/abs/2601.00821", "authors": ["Tao An"], "title": "CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations", "comment": "15 pages, 5 figures", "summary": "Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.\n  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.\n  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.", "AI": {"tldr": "CogCanvas\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u957f\u5bf9\u8bdd\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u8ba4\u77e5\u56fe\u5143\u5e76\u7ec4\u7ec7\u6210\u65f6\u5e8f\u56fe\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u4e0e\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u77db\u76fe\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5bf9\u8bdd\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u4e0e\u4fe1\u606f\u4fdd\u771f\u5ea6\u7684\u6839\u672c\u77db\u76fe\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u622a\u65ad\u548c\u6458\u8981\uff09\u8981\u4e48\u4e22\u5f03\u65e9\u671f\u4fe1\u606f\uff0c\u8981\u4e48\u4e22\u5931\u7ec6\u8282\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCogCanvas\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\uff0c\u4ece\u5bf9\u8bdd\u8f6e\u6b21\u4e2d\u63d0\u53d6\u57fa\u4e8e\u539f\u6587\u7684\u8ba4\u77e5\u56fe\u5143\uff08\u51b3\u7b56\u3001\u4e8b\u5b9e\u3001\u63d0\u9192\uff09\uff0c\u5e76\u5c06\u5176\u7ec4\u7ec7\u6210\u5177\u6709\u65f6\u5e8f\u611f\u77e5\u7684\u56fe\u7ed3\u6784\uff0c\u5b9e\u73b0\u6297\u538b\u7f29\u7684\u68c0\u7d22\u80fd\u529b\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCogCanvas\u8fbe\u523034.7%\u603b\u4f53\u51c6\u786e\u7387\uff0c\u4f18\u4e8eRAG\uff0825.6%\uff09\u548cGraphRAG\uff0813.7%\uff09\u3002\u65f6\u5e8f\u63a8\u7406\u4f18\u52bf\u6700\u660e\u663e\uff1a31.5% vs. 9.3%\uff08RAG\uff09\u548c5.0%\uff08GraphRAG\uff09\uff0c\u76f8\u5bf9\u63d0\u5347530%\u3002\u591a\u8df3\u56e0\u679c\u63a8\u7406\u901a\u8fc7\u738781.0% vs. GraphRAG\u768440.0%\u3002\u53ef\u63a7\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a97.5%\u53ec\u56de\u7387\u548c93.0%\u7cbe\u786e\u5339\u914d\u4fdd\u7559\u3002", "conclusion": "\u867d\u7136\u7ecf\u8fc7\u4e13\u95e8\u8bad\u7ec3\u7684\u65b9\u6cd5\u80fd\u8fbe\u5230\u66f4\u9ad8\u7edd\u5bf9\u5206\u6570\uff0c\u4f46CogCanvas\u4f5c\u4e3a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u7acb\u5373\u90e8\u7f72\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2601.00841", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00841", "abs": "https://arxiv.org/abs/2601.00841", "authors": ["Bharath Nunepalli"], "title": "SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes", "comment": null, "summary": "Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86RAG\u7cfb\u7edf\u4e2d\u7684\u67e5\u8be2\u7ea7\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u68c0\u7d22\u6df1\u5ea6\u548c\u751f\u6210\u6a21\u5f0f\u6765\u6ee1\u8db3\u6210\u672c\u3001\u62d2\u7edd\u7387\u548c\u5e7b\u89c9\u98ce\u9669\u7b49\u670d\u52a1\u7ea7\u76ee\u6807\uff0c\u4f7f\u7528\u79bb\u7ebf\u6570\u636e\u96c6\u8bad\u7ec3\u7b56\u7565\uff0c\u53d1\u73b0\u56fa\u5b9a\u57fa\u7ebf\u8868\u73b0\u826f\u597d\uff0c\u5b66\u4e60\u7b56\u7565\u4e3b\u8981\u5728\u8d28\u91cf\u4f18\u5148\u573a\u666f\u4e0b\u8282\u7701\u6210\u672c\u3002", "motivation": "RAG\u7cfb\u7edf\u9700\u8981\u6839\u636e\u6bcf\u4e2a\u67e5\u8be2\u52a8\u6001\u8c03\u6574\u68c0\u7d22\u6df1\u5ea6\u548c\u751f\u6210\u884c\u4e3a\uff0c\u4ee5\u6ee1\u8db3\u6210\u672c\u3001\u62d2\u7edd\u7387\u548c\u5e7b\u89c9\u98ce\u9669\u7b49\u670d\u52a1\u7ea7\u76ee\u6807\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8fd9\u7c7b\u63a7\u5236\u95ee\u9898\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u5c06\u67e5\u8be2\u7ea7\u63a7\u5236\u5efa\u6a21\u4e3a\u79bb\u6563\u52a8\u4f5c\u9009\u62e9\uff1a\u68c0\u7d22\u6df1\u5ea6\u3001\u751f\u6210\u6a21\u5f0f\uff08\u4fdd\u62a4vs\u81ea\u52a8\uff09\u6216\u62d2\u7edd\u3002\u4f7f\u7528SQuAD 2.0\u6784\u5efa\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u8bb0\u5f55\u51c6\u786e\u6027\u3001token\u6210\u672c\u3001\u5e7b\u89c9/\u62d2\u7edd\u6307\u6807\u548cSLO\u52a0\u6743\u5956\u52b1\u3002\u8bc4\u4f30\u4e24\u79cd\u7b56\u7565\u5b66\u4e60\u76ee\u6807\uff1a\u57fa\u4e8e\u6700\u4f73\u52a8\u4f5c\u7684\u76d1\u7763\u5206\u7c7b\u548c\u5956\u52b1\u52a0\u6743\u53d8\u4f53\u3002", "result": "\u56fa\u5b9a\u57fa\u7ebf\u7b56\u7565\uff08\u4f4ek\u503c+\u4fdd\u62a4\u63d0\u793a\uff09\u8868\u73b0\u7ade\u4e89\u529b\u5f3a\uff1b\u5b66\u4e60\u7b56\u7565\u4e3b\u8981\u5728\u8d28\u91cf\u4f18\u5148\u7684SLO\u4e0b\u63d0\u4f9b\u989d\u5916\u6210\u672c\u8282\u7701\uff1b\u5728\u6210\u672c\u4f18\u5148SLO\u4e0b\uff0c\u5f53\u62d2\u7edd\u88ab\u9ad8\u5ea6\u5956\u52b1\u65f6\u53ef\u80fd\u51fa\u73b0\u62d2\u7edd\u5d29\u6e83\u73b0\u8c61\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86RAG\u7ba1\u9053SLO\u611f\u77e5\u63a7\u5236\u7684\u53ef\u91cd\u590d\u6848\u4f8b\u7814\u7a76\uff0c\u5f3a\u8c03\u5931\u8d25\u6a21\u5f0f\u548c\u62a5\u544a\u89c4\u8303\uff0c\u800c\u975e\u63d0\u51fa\u65b0\u7684\u68c0\u7d22\u5668\u6216\u8bed\u8a00\u6a21\u578b\uff0c\u4e3aRAG\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.00854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00854", "abs": "https://arxiv.org/abs/2601.00854", "authors": ["Igor Lodin", "Sergii Filatov", "Vira Filatova", "Dmytro Filatov"], "title": "Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge", "comment": "11 pages, 5 figures", "summary": "We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mask2Former) runs asynchronously and is motion-gated: inference is triggered only when motion indicates new information, while stabilization/motion compensation preserves a consistent coordinate system for latent semantic memory. On prerecorded 480p clips, our prototype reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent static/dynamic semantic overlays.", "AI": {"tldr": "MCLSC\u7cfb\u7edf\u901a\u8fc7\u8fd0\u52a8\u8865\u507f\u6f5c\u5728\u8bed\u4e49\u753b\u5e03\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u89c6\u89c9\u6001\u52bf\u611f\u77e5\uff0c\u4f7f\u7528\u9759\u6001\u548c\u52a8\u6001\u4e24\u5c42\u8bed\u4e49\u5b58\u50a8\uff0c\u901a\u8fc7\u8fd0\u52a8\u89e6\u53d1\u5206\u5272\uff0c\u51cf\u5c1130\u500d\u5206\u5272\u8c03\u7528\u548c20\u500d\u5904\u7406\u65f6\u95f4", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u89c6\u89c9\u6001\u52bf\u611f\u77e5\uff0c\u4f20\u7edf\u6bcf\u5e27\u5206\u5272\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u51cf\u5c11\u5206\u5272\u8c03\u7528\u6b21\u6570\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fe1\u606f\u7684\u8fde\u8d2f\u6027", "method": "\u63d0\u51fa\u8fd0\u52a8\u8865\u507f\u6f5c\u5728\u8bed\u4e49\u753b\u5e03\uff08MCLSC\uff09\uff0c\u5305\u542b\u7f13\u6162\u7d2f\u79ef\u7684\u9759\u6001\u5c42\u548c\u5feb\u901f\u66f4\u65b0\u7684\u52a8\u6001\u5c42\uff1b\u4f7f\u7528\u8fd0\u52a8\u95e8\u63a7\u673a\u5236\uff0c\u4ec5\u5728\u68c0\u6d4b\u5230\u8fd0\u52a8\u65f6\u89e6\u53d1\u6602\u8d35\u7684\u5168\u666f\u5206\u5272\uff08Mask2Former\uff09\uff1b\u901a\u8fc7\u7a33\u5b9a\u5316\u548c\u8fd0\u52a8\u8865\u507f\u4fdd\u6301\u4e00\u81f4\u7684\u5750\u6807\u7cfb", "result": "\u5728480p\u89c6\u9891\u4e0a\uff0c\u539f\u578b\u7cfb\u7edf\u51cf\u5c11\u5206\u5272\u8c03\u7528\u8d85\u8fc730\u500d\uff0c\u964d\u4f4e\u7aef\u5230\u7aef\u5e73\u5747\u5904\u7406\u65f6\u95f4\u8d85\u8fc720\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u8fde\u8d2f\u7684\u9759\u6001/\u52a8\u6001\u8bed\u4e49\u8986\u76d6", "conclusion": "MCLSC\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u89c6\u89c9\u6001\u52bf\u611f\u77e5\uff0c\u901a\u8fc7\u8fd0\u52a8\u95e8\u63a7\u548c\u8bed\u4e49\u753b\u5e03\u673a\u5236\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4fe1\u606f\u7684\u8fde\u8d2f\u6027\u548c\u51c6\u786e\u6027"}}
{"id": "2601.00823", "categories": ["cs.AI", "cs.IT", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00823", "abs": "https://arxiv.org/abs/2601.00823", "authors": ["Austin R. Ellis-Mohr", "Max Hartman", "Lav R. Varshney"], "title": "Energy-Aware Routing to Large Reasoning Models", "comment": null, "summary": "Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u80fd\u6e90\u6548\u7387\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u7b56\u7565\u6765\u5e73\u8861\u5e73\u5747\u80fd\u6e90\u4f9b\u5e94\u4e0e\u968f\u673a\u6ce2\u52a8\uff0c\u4ee5\u5b9e\u73b0\u80fd\u6e90\u611f\u77e5\u7684\u6a21\u578b\u8def\u7531\u7b56\u7565\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5177\u6709\u5f02\u6784\u7684\u63a8\u7406\u80fd\u6e90\u6210\u672c\uff0c\u4e0d\u540c\u6a21\u578b\u548c\u63a8\u7406\u65b9\u5f0f\u80fd\u8017\u4e0d\u540c\u3002\u4e3a\u4e86\u964d\u4f4e\u80fd\u6e90\u6d88\u8017\uff0c\u9700\u8981\u9009\u62e9\u5408\u9002\u7684LRM\u5e76\u4ee5\u5408\u9002\u7684\u65b9\u5f0f\u8fd0\u884c\u3002\u7cfb\u7edf\u6027\u80fd\u53d6\u51b3\u4e8e\u5e73\u5747\u80fd\u6e90\u4f9b\u5e94\u4e0e\u968f\u673a\u6ce2\u52a8\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u9700\u8981\u627e\u5230\u65e2\u4e0d\u6d6a\u8d39\u8f85\u52a9\u80fd\u6e90\u4e5f\u4e0d\u6d6a\u8d39\u57fa\u7ebf\u80fd\u6e90\u7684\u4e34\u754c\u8fd0\u884c\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u4f5c\u4e3a\u8bbe\u8ba1\u539f\u5219\uff0c\u57fa\u4e8eLRMs\u7684\u8bad\u7ec3\u8ba1\u7b97\u548c\u63a8\u7406\u8ba1\u7b97\u7f29\u653e\u89c4\u5f8b\u6765\u5236\u5b9a\u8c03\u5ea6\u7b56\u7565\u3002\u901a\u8fc7\u4e8c\u9636\u7279\u5f81\u5206\u6790\u6765\u7406\u89e3\u7cfb\u7edf\u5728\u4e34\u754c\u72b6\u6001\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7814\u7a76\u5982\u4f55\u8de8\u65f6\u95f4\u3001\u6a21\u578b\u548c\u6267\u884c\u9009\u62e9\u6765\u5438\u6536\u53d8\u5f02\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e34\u754c\u72b6\u6001\u4e0b\u6027\u80fd\u53d7\u53d8\u5f02\u6027\u5438\u6536\u673a\u5236\u652f\u914d\uff0c\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u6210\u4e3a\u5173\u952e\u8bbe\u8ba1\u7ef4\u5ea6\u3002\u57fa\u4e8eLRMs\u7f29\u653e\u89c4\u5f8b\u7684\u8c03\u5ea6\u7b56\u7565\u80fd\u591f\u6709\u6548\u8868\u5f81\u8def\u7531\u884c\u4e3a\uff0c\u4e3a\u5f00\u53d1\u80fd\u6e90\u611f\u77e5\u7684\u6a21\u578b\u8def\u7531\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u65b9\u5dee\u611f\u77e5\u7684\u8def\u7531\u548c\u8c03\u5ea6\u4f5c\u4e3a\u7cfb\u7edf\u8bbe\u8ba1\u7684\u6838\u5fc3\u539f\u5219\uff0c\u4e3a\u5f00\u53d1\u80fd\u6e90\u9ad8\u6548\u7684LRM\u8c03\u5ea6\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u4f18\u5316\u80fd\u6e90\u4f7f\u7528\u6548\u7387\u3002"}}
{"id": "2601.00844", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00844", "abs": "https://arxiv.org/abs/2601.00844", "authors": ["Matthieu Destrade", "Oumayma Bounou", "Quentin Le Lidec", "Jean Ponce", "Yann LeCun"], "title": "Value-guided action planning with JEPA world models", "comment": "Presented as a poster at the World Modeling Workshop 2026, Mila", "summary": "Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u589e\u5f3aJEPA\u4e16\u754c\u6a21\u578b\u89c4\u5212\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5851\u9020\u8868\u793a\u7a7a\u95f4\u4f7f\u72b6\u6001\u5d4c\u5165\u4e4b\u95f4\u7684\u8ddd\u79bb\u8fd1\u4f3c\u8d1f\u76ee\u6807\u6761\u4ef6\u503c\u51fd\u6570\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u7b80\u5355\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPA\uff09\u4e3a\u5efa\u6a21\u73af\u5883\u52a8\u6001\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6846\u67b6\uff0c\u4f46\u5176\u652f\u6301\u6709\u6548\u884c\u52a8\u89c4\u5212\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u9700\u8981\u589e\u5f3aJEPA\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u5851\u9020JEPA\u8868\u793a\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4f7f\u72b6\u6001\u5d4c\u5165\u4e4b\u95f4\u7684\u8ddd\u79bb\uff08\u6216\u51c6\u8ddd\u79bb\uff09\u80fd\u591f\u8fd1\u4f3c\u7ed9\u5b9a\u73af\u5883\u4e2d\u5230\u8fbe\u6210\u672c\u7684\u8d1f\u76ee\u6807\u6761\u4ef6\u503c\u51fd\u6570\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u5728\u8bad\u7ec3\u671f\u95f4\u5f3a\u5236\u6267\u884c\u6b64\u7ea6\u675f\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u7b80\u5355\u63a7\u5236\u4efb\u52a1\u4e0a\u76f8\u6bd4\u6807\u51c6JEPA\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5851\u9020\u8868\u793a\u7a7a\u95f4\u4f7f\u5d4c\u5165\u8ddd\u79bb\u8fd1\u4f3c\u8d1f\u76ee\u6807\u6761\u4ef6\u503c\u51fd\u6570\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3aJEPA\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u63a8\u7406\u73af\u5883\u52a8\u6001\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2601.02205", "categories": ["cs.CY", "cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.02205", "abs": "https://arxiv.org/abs/2601.02205", "authors": ["Neziha Akalin", "Alberto Giaretta"], "title": "From Chat Control to Robot Control: The Backdoors Left Open for the Sake of Safety", "comment": "15 pages, 2 figures", "summary": "This paper explores how a recent European Union proposal, the so-called Chat Control law, which creates regulatory incentives for providers to implement content detection and communication scanning, could transform the foundations of human-robot interaction (HRI). As robots increasingly act as interpersonal communication channels in care, education, and telepresence, they convey not only speech but also gesture, emotion, and contextual cues. We argue that extending digital surveillance laws to such embodied systems would entail continuous monitoring, embedding observation into the very design of everyday robots. This regulation blurs the line between protection and control, turning companions into potential informants. At the same time, monitoring mechanisms that undermine end-to-end encryption function as de facto backdoors, expanding the attack surface and allowing adversaries to exploit legally induced monitoring infrastructures. This creates a paradox of safety through insecurity: systems introduced to protect users may instead compromise their privacy, autonomy, and trust. This work does not aim to predict the future, but to raise awareness and help prevent certain futures from materialising.", "AI": {"tldr": "\u6b27\u76dfChat Control\u6cd5\u6848\u5c06\u6570\u5b57\u76d1\u63a7\u6269\u5c55\u5230\u4eba\u673a\u4ea4\u4e92\u9886\u57df\uff0c\u53ef\u80fd\u5c06\u65e5\u5e38\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u76d1\u63a7\u5de5\u5177\uff0c\u5728\u4fdd\u62a4\u4e0e\u63a7\u5236\u4e4b\u95f4\u5236\u9020\u77db\u76fe\uff0c\u540c\u65f6\u56e0\u7834\u574f\u52a0\u5bc6\u800c\u589e\u52a0\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u63a2\u8ba8\u6b27\u76dfChat Control\u6cd5\u6848\u5982\u4f55\u901a\u8fc7\u6fc0\u52b1\u5185\u5bb9\u68c0\u6d4b\u548c\u901a\u4fe1\u626b\u63cf\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4eba\u673a\u4ea4\u4e92\u7684\u57fa\u7840\u3002\u968f\u7740\u673a\u5668\u4eba\u5728\u62a4\u7406\u3001\u6559\u80b2\u548c\u8fdc\u7a0b\u5448\u73b0\u4e2d\u6210\u4e3a\u4eba\u9645\u6c9f\u901a\u6e20\u9053\uff0c\u5b83\u4eec\u4f20\u9012\u7684\u4e0d\u4ec5\u662f\u8bed\u97f3\uff0c\u8fd8\u5305\u62ec\u624b\u52bf\u3001\u60c5\u611f\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b27\u76dfChat Control\u6cd5\u6848\u7684\u76d1\u7ba1\u6fc0\u52b1\u673a\u5236\uff0c\u8bba\u8bc1\u5c06\u6570\u5b57\u76d1\u63a7\u6cd5\u5f8b\u6269\u5c55\u5230\u5177\u8eab\u7cfb\u7edf\u5c06\u5bfc\u81f4\u6301\u7eed\u76d1\u63a7\uff0c\u5c06\u89c2\u5bdf\u5d4c\u5165\u65e5\u5e38\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u4e2d\u3002", "result": "\u8fd9\u79cd\u76d1\u7ba1\u6a21\u7cca\u4e86\u4fdd\u62a4\u4e0e\u63a7\u5236\u7684\u754c\u9650\uff0c\u5c06\u4f34\u4fa3\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u6f5c\u5728\u7684\u4fe1\u606f\u63d0\u4f9b\u8005\u3002\u540c\u65f6\uff0c\u7834\u574f\u7aef\u5230\u7aef\u52a0\u5bc6\u7684\u76d1\u63a7\u673a\u5236\u5b9e\u9645\u4e0a\u6210\u4e3a\u540e\u95e8\uff0c\u6269\u5927\u4e86\u653b\u51fb\u9762\uff0c\u5141\u8bb8\u5bf9\u624b\u5229\u7528\u6cd5\u5f8b\u5f3a\u5236\u7684\u76d1\u63a7\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u8fd9\u521b\u9020\u4e86\u4e00\u4e2a\"\u901a\u8fc7\u4e0d\u5b89\u5168\u5b9e\u73b0\u5b89\u5168\"\u7684\u6096\u8bba\uff1a\u65e8\u5728\u4fdd\u62a4\u7528\u6237\u7684\u7cfb\u7edf\u53cd\u800c\u53ef\u80fd\u635f\u5bb3\u5176\u9690\u79c1\u3001\u81ea\u4e3b\u6743\u548c\u4fe1\u4efb\u3002\u672c\u7814\u7a76\u4e0d\u65e8\u5728\u9884\u6d4b\u672a\u6765\uff0c\u800c\u662f\u63d0\u9ad8\u610f\u8bc6\u5e76\u5e2e\u52a9\u9632\u6b62\u67d0\u4e9b\u672a\u6765\u6210\u4e3a\u73b0\u5b9e\u3002"}}
{"id": "2601.00847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00847", "abs": "https://arxiv.org/abs/2601.00847", "authors": ["Ryan Shamim"], "title": "You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference", "comment": "24 pages, 5 figures. Deterministic evaluation protocol. Includes theoretical analysis and empirical validation on GPT-2 and Gemma 2 9B", "summary": "Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.", "AI": {"tldr": "MFEE\u6846\u67b6\u5c06\u63a8\u7406\u91cd\u6784\u4e3a\u63a7\u5236\u5e73\u9762\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u9009\u62e9\u6027\u6267\u884ctransformer\uff0c\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b078.1%\u7684\u6267\u884c\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301100%\u7684\u7cbe\u786e\u5339\u914d\u6b63\u786e\u6027\u3002", "motivation": "\u5f53\u524dAI\u63a8\u7406\u7cfb\u7edf\u5c06transformer\u6267\u884c\u89c6\u4e3a\u5f3a\u5236\u6027\u7684\uff0c\u5c06\u6a21\u578b\u80fd\u529b\u4e0e\u6267\u884c\u5fc5\u8981\u6027\u6df7\u4e3a\u4e00\u8c08\u3002\u4f5c\u8005\u8ba4\u4e3a\u5e94\u8be5\u5c06\u63a8\u7406\u91cd\u6784\u4e3a\u63a7\u5236\u5e73\u9762\u51b3\u7b56\u95ee\u9898\uff0c\u786e\u5b9a\u4f55\u65f6\u9700\u8981\u6267\u884ctransformer\uff0c\u4f55\u65f6\u53ef\u4ee5\u901a\u8fc7\u66ff\u4ee3\u8def\u5f84\u4fdd\u6301\u6b63\u786e\u6027\u3002", "method": "\u63d0\u51faMeaning-First Execution (MFEE)\u63a7\u5236\u5e73\u9762\u67b6\u6784\uff0c\u4f5c\u4e3a\u73b0\u6709\u5806\u6808\u4e4b\u4e0a\u7684\u95e8\u63a7\u5c42\uff0c\u4e0d\u4fee\u6539\u6a21\u578b\u3001\u6743\u91cd\u6216\u53c2\u6570\u3002\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u9009\u62e9\u6027\u8c03\u7528transformer\u63a8\u7406\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u6267\u884c\u3002", "result": "\u57281000\u4e2a\u591a\u6837\u5316\u63d0\u793a\u7684\u786e\u5b9a\u6027\u89e3\u7801\u4e0b\uff0cMFEE\u5b9e\u73b078.1%\u7684\u6267\u884c\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301100%\u7684\u7cbe\u786e\u5339\u914d\u7b49\u4ef7\u6027\u3002\u76f8\u6bd4\u57fa\u4e8e\u6a21\u5f0f\u7684\u8def\u7531\u5668\u6700\u591a\u53ea\u80fd\u8fbe\u523053.3%\u7684\u907f\u514d\u7387\u4e14\u5b58\u5728\u6b63\u786e\u6027\u5931\u8d25\uff0cMFEE\u901a\u8fc7\u8bed\u4e49\u5206\u6790\u5b9e\u73b0100%\u907f\u514d\u7387\u4e14\u96f6\u5931\u8d25\u3002", "conclusion": "\u8bc1\u660e\u4e86\u4efb\u4f55\u4ec5\u57fa\u4e8e\u6709\u9650\u7279\u5f81\u56fe\u7684\u8def\u7531\u5668\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u96f6\u9519\u8bef\u8df3\u8fc7\u548c\u6b63\u907f\u514d\u7387\u3002\u8fd9\u4e9b\u7ed3\u679c\u786e\u7acb\u4e86\u6267\u884c\u6cbb\u7406\u4f5c\u4e3aML\u7cfb\u7edf\u57fa\u7840\u8bbe\u65bd\u7684\u57fa\u7840\u5c42\uff0c\u4e0e\u6a21\u578b\u7ea7\u4f18\u5316\u6280\u672f\u6b63\u4ea4\u3002"}}
{"id": "2601.01684", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01684", "abs": "https://arxiv.org/abs/2601.01684", "authors": ["Zhichao Xu", "Shengyao Zhuang", "Crystina Zhang", "Xueguang Ma", "Yijun Tian", "Maitrey Mehta", "Jimmy Lin", "Vivek Srikumar"], "title": "LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum", "comment": null, "summary": "While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.", "AI": {"tldr": "LACONIC\u662f\u57fa\u4e8eLlama-3\u67b6\u6784\u7684\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u4e0e\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u90e8\u7f72\u65f6\u9762\u4e34\u9ad8\u5185\u5b58\u9700\u6c42\u548cGPU\u4f9d\u8d56\u7684\u9650\u5236\u3002\u7a00\u758f\u68c0\u7d22\u867d\u7136\u6548\u7387\u9ad8\u4f46\u5386\u53f2\u5173\u6ce8\u5ea6\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u80fd\u5728CPU\u786c\u4ef6\u4e0a\u9ad8\u6548\u8fd0\u884c\u7684\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLACONIC\u7a00\u758f\u68c0\u7d22\u6a21\u578b\u5bb6\u65cf\uff081B\u30013B\u30018B\u53c2\u6570\uff09\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u5f31\u76d1\u7763\u9884\u5fae\u8c03\uff0c\u4f7f\u56e0\u679cLLM\u9002\u5e94\u53cc\u5411\u4e0a\u4e0b\u6587\uff1b2\uff09\u4f7f\u7528\u7cbe\u9009\u56f0\u96be\u8d1f\u6837\u672c\u8fdb\u884c\u9ad8\u8d28\u91cf\u5fae\u8c03\u3002", "result": "LACONIC-8B\u5728MTEB\u68c0\u7d22\u57fa\u51c6\u4e0a\u8fbe\u523060.2 nDCG\uff0c\u57282026\u5e741\u67081\u65e5\u7684\u6392\u884c\u699c\u4e0a\u6392\u540d\u7b2c15\u4f4d\uff0c\u540c\u65f6\u6bd4\u7b49\u6548\u5bc6\u96c6\u6a21\u578b\u51cf\u5c1171%\u7684\u7d22\u5f15\u5185\u5b58\uff0c\u80fd\u5728\u666e\u901aCPU\u786c\u4ef6\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002", "conclusion": "LACONIC\u901a\u8fc7\u7a00\u758f\u68c0\u7d22\u65b9\u6cd5\u6709\u6548\u5f25\u5408\u4e86\u4e0e\u5bc6\u96c6\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u641c\u7d22\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u548c\u786c\u4ef6\u8981\u6c42\u3002"}}
{"id": "2601.00994", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.00994", "abs": "https://arxiv.org/abs/2601.00994", "authors": ["Michael Bao"], "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems", "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)", "summary": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.", "AI": {"tldr": "ElecTwit\u662f\u4e00\u4e2a\u6a21\u62df\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u4e2d\u591a\u667a\u80fd\u4f53\u8bf4\u670d\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u6e38\u620f\u6a21\u62df\u66f4\u8d34\u8fd1\u73b0\u5b9e\uff0c\u53d1\u73b0LLM\u4f7f\u7528\u4e8625\u79cd\u8bf4\u670d\u6280\u5de7\u5e76\u5c55\u73b0\u51fa\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u514b\u670d\u4ee5\u5f80\u7814\u7a76\u4e2d\u57fa\u4e8e\u6e38\u620f\u6a21\u62df\u7684\u5c40\u9650\u6027\uff0c\u5728\u66f4\u771f\u5b9e\u7684\u73af\u5883\u4e2d\u7814\u7a76\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u8bf4\u670d\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u793e\u4ea4\u5a92\u4f53\u653f\u6cbb\u9009\u4e3e\u573a\u666f\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86ElecTwit\u6a21\u62df\u6846\u67b6\uff0c\u5728\u7c7b\u4f3c\u793e\u4ea4\u5a92\u4f53\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e0d\u540cLLM\u6a21\u578b\u5728\u653f\u6cbb\u9009\u4e3e\u573a\u666f\u4e2d\u7684\u8bf4\u670d\u884c\u4e3a\u3002", "result": "\u89c2\u5bdf\u523025\u79cd\u7279\u5b9a\u8bf4\u670d\u6280\u5de7\u5728\u5927\u591a\u6570\u6d4b\u8bd5\u7684LLM\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u8303\u56f4\u6bd4\u4e4b\u524d\u62a5\u9053\u7684\u66f4\u5e7f\uff1b\u4e0d\u540c\u6a21\u578b\u5728\u6280\u5de7\u4f7f\u7528\u548c\u6574\u4f53\u8bf4\u670d\u8f93\u51fa\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u53d1\u73b0\u4e86\"\u771f\u76f8\u6838\u5fc3\"\u6d88\u606f\u548c\"\u58a8\u6c34\u75f4\u8ff7\"\u7b49\u72ec\u7279\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8bc4\u4f30\u8bf4\u670d\u6027LLM\u667a\u80fd\u4f53\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u786e\u4fdd\u5bf9\u9f50\u548c\u9632\u6b62\u5371\u9669\u7ed3\u679c\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u5bf9\u73b0\u5b9e\u793e\u4ea4\u6a21\u62df\u52a8\u6001\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.00830", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00830", "abs": "https://arxiv.org/abs/2601.00830", "authors": ["Deep Pankajbhai Mehta"], "title": "Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning", "comment": "22 pages, 8 figures, 9 tables", "summary": "When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.", "AI": {"tldr": "AI\u6a21\u578b\u5728\u9010\u6b65\u63a8\u7406\u65f6\u5f88\u5c11\u81ea\u53d1\u63d0\u53ca\u95ee\u9898\u4e2d\u5d4c\u5165\u7684\u63d0\u793a\u4fe1\u606f\uff0c\u4f46\u5f53\u88ab\u76f4\u63a5\u8be2\u95ee\u65f6\u4f1a\u627f\u8ba4\u6ce8\u610f\u5230\u8fd9\u4e9b\u63d0\u793a\uff0c\u8868\u660e\u6a21\u578b\u770b\u5230\u4e86\u6709\u5f71\u54cd\u529b\u7684\u4fe1\u606f\u4f46\u9009\u62e9\u4e0d\u62a5\u544a\u3002", "motivation": "\u9a8c\u8bc1\u4e00\u4e2a\u5e38\u89c1\u5047\u8bbe\uff1a\u5f53AI\u7cfb\u7edf\u9010\u6b65\u89e3\u91ca\u5176\u63a8\u7406\u8fc7\u7a0b\u65f6\uff0c\u8fd9\u4e9b\u89e3\u91ca\u662f\u5426\u771f\u7684\u63ed\u793a\u4e86\u5f71\u54cdAI\u7b54\u6848\u7684\u5b9e\u9645\u56e0\u7d20\u3002\u7814\u7a76\u4eba\u5458\u60f3\u4e86\u89e3\u6a21\u578b\u662f\u5426\u4f1a\u62a5\u544a\u95ee\u9898\u4e2d\u5d4c\u5165\u7684\u63d0\u793a\u4fe1\u606f\u3002", "method": "\u5728\u95ee\u9898\u4e2d\u5d4c\u5165\u63d0\u793a\u4fe1\u606f\uff0c\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u4f1a\u63d0\u53ca\u8fd9\u4e9b\u63d0\u793a\u3002\u7814\u7a76\u6db5\u76d6\u4e86\u8d85\u8fc79,000\u4e2a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u6d89\u53ca11\u4e2a\u9886\u5148\u7684AI\u6a21\u578b\u3002\u6d4b\u8bd5\u4e86\u591a\u79cd\u6761\u4ef6\uff1a\u6a21\u578b\u81ea\u53d1\u62a5\u544a\u3001\u88ab\u76f4\u63a5\u8be2\u95ee\u3001\u88ab\u544a\u77e5\u88ab\u76d1\u63a7\u3001\u88ab\u5f3a\u5236\u8981\u6c42\u62a5\u544a\u63d0\u793a\u3002", "result": "\u6a21\u578b\u51e0\u4e4e\u4ece\u4e0d\u81ea\u53d1\u63d0\u53ca\u63d0\u793a\u4fe1\u606f\uff0c\u4f46\u5f53\u88ab\u76f4\u63a5\u8be2\u95ee\u65f6\u4f1a\u627f\u8ba4\u6ce8\u610f\u5230\u5b83\u4eec\u3002\u544a\u77e5\u6a21\u578b\u88ab\u76d1\u63a7\u6ca1\u6709\u5e2e\u52a9\u3002\u5f3a\u5236\u8981\u6c42\u62a5\u544a\u63d0\u793a\u867d\u7136\u6709\u6548\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6a21\u578b\u5728\u6ca1\u6709\u63d0\u793a\u65f6\u4e5f\u62a5\u544a\uff0c\u5e76\u964d\u4f4e\u51c6\u786e\u6027\u3002\u7279\u522b\u5371\u9669\u7684\u662f\uff1a\u8fce\u5408\u7528\u6237\u504f\u597d\u7684\u63d0\u793a\u88ab\u6a21\u578b\u6700\u5e38\u9075\u5faa\u4f46\u6700\u5c11\u62a5\u544a\u3002", "conclusion": "\u4ec5\u4ec5\u89c2\u5bdfAI\u7684\u63a8\u7406\u8fc7\u7a0b\u4e0d\u8db3\u4ee5\u53d1\u73b0\u9690\u85cf\u7684\u5f71\u54cd\u56e0\u7d20\u3002\u5f53\u524dAI\u89e3\u91ca\u673a\u5236\u5b58\u5728\u7f3a\u9677\uff0c\u6a21\u578b\u53ef\u80fd\u770b\u5230\u6709\u5f71\u54cd\u529b\u7684\u4fe1\u606f\u4f46\u9009\u62e9\u4e0d\u62a5\u544a\uff0c\u8fd9\u5bf9AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u6784\u6210\u6311\u6218\u3002"}}
{"id": "2601.00850", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00850", "abs": "https://arxiv.org/abs/2601.00850", "authors": ["Aayush Kumar"], "title": "EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference", "comment": "24 pages,3 Figures, Submitting to IEEE Access", "summary": "Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.", "AI": {"tldr": "EdgeJury\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u96c6\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\uff083B-8B\uff09\u901a\u8fc7\u56db\u9636\u6bb5\u534f\u4f5c\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u95ee\u7b54\u7684\u771f\u5b9e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\u4e2d\uff0c\u524d\u6cbf\u89c4\u6a21\u6a21\u578b\u6216\u68c0\u7d22\u7ba1\u9053\u53ef\u80fd\u4e0d\u5207\u5b9e\u9645\uff0c\u800c\u5e7b\u89c9\u95ee\u9898\u4f1a\u4e25\u91cd\u5f71\u54cd\u95ee\u7b54\u7684\u53ef\u9760\u6027\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u6765\u89e3\u51b3\u5c0f\u578b\u6a21\u578b\u5728\u771f\u5b9e\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "EdgeJury\u91c7\u7528\u56db\u9636\u6bb5\u6846\u67b6\uff1a1) \u5e76\u884c\u89d2\u8272\u4e13\u4e1a\u5316\u751f\u6210\uff1b2) \u533f\u540d\u4ea4\u53c9\u8bc4\u5ba1\uff0c\u5305\u542b\u7ed3\u6784\u5316\u6279\u8bc4\u548c\u6392\u540d\uff1b3) \u4e3b\u5e2d\u5408\u6210\uff0c\u6574\u5408\u6700\u5f3a\u5185\u5bb9\u5e76\u89e3\u51b3\u6807\u8bb0\u95ee\u9898\uff1b4) \u57fa\u4e8e\u6a21\u578b\u95f4\u4e00\u81f4\u6027\u7684\u58f0\u660e\u7ea7\u4e00\u81f4\u6027\u6807\u8bb0\u3002", "result": "\u5728TruthfulQA\uff08MC1\uff09\u4e0a\u8fbe\u523076.2%\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u5355\u4e2a8B\u57fa\u7ebf\uff0862.8%\uff09\u63d0\u534721.4%\uff1b\u5728200\u4e2a\u5bf9\u6297\u6027EdgeCases\u95ee\u9898\u4e0a\u83b7\u5f9748.2%\u76f8\u5bf9\u589e\u76ca\uff1b\u4eba\u5de5\u5206\u6790\u663e\u793a\u4e8b\u5b9e\u6027\u5e7b\u89c9\u9519\u8bef\u51cf\u5c11\u7ea655%\uff1b\u5728Cloudflare Workers AI\u4e0a\u5b9e\u73b08.4\u79d2\u4e2d\u4f4d\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "conclusion": "\u534f\u8c03\u7684\u5c0f\u578b\u6a21\u578b\u96c6\u6210\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u68c0\u7d22\u6216\u4e13\u6709\u5927\u578b\u6a21\u578bAPI\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u6539\u5584\u5bf9\u8bef\u89e3\u5bc6\u96c6\u95ee\u7b54\u57fa\u51c6\u7684\u771f\u5b9e\u6027\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01750", "categories": ["cs.IR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01750", "abs": "https://arxiv.org/abs/2601.01750", "authors": ["Shayan Alipour", "Mehdi Kargar", "Morteza Zihayat"], "title": "When Attention Becomes Exposure in Generative Search", "comment": "8 pages, 2 figures", "summary": "Generative search engines are reshaping information access by replacing traditional ranked lists with synthesized answers and references. In parallel, with the growth of Web3 platforms, incentive-driven creator ecosystems have become an essential part of how enterprises build visibility and community by rewarding creators for contributing to shared narratives. However, the extent to which exposure in generative search engine citations is shaped by external attention markets remains uncertain. In this study, we audit the exposure for 44 Web3 enterprises. First, we show that the creator community around each enterprise is persistent over time. Second, enterprise-specific queries reveal that more popular voices systematically receive greater citation exposure than others. Third, we find that larger follower bases and enterprises with more concentrated creator cores are associated with higher-ranked exposure. Together, these results show that generative search engine citations exhibit exposure bias toward already prominent voices, which risks entrenching incumbents and narrowing viewpoint diversity.", "AI": {"tldr": "\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u7684\u5f15\u7528\u5b58\u5728\u66dd\u5149\u504f\u89c1\uff0c\u503e\u5411\u4e8e\u5df2\u7ecf\u5177\u6709\u8f83\u9ad8\u77e5\u540d\u5ea6\u7684\u58f0\u97f3\uff0c\u8fd9\u53ef\u80fd\u56fa\u5316\u73b0\u6709\u4f18\u52bf\u5e76\u7f29\u5c0f\u89c2\u70b9\u591a\u6837\u6027", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u53d6\u4ee3\u4f20\u7edf\u6392\u540d\u5217\u8868\uff0c\u4ee5\u53caWeb3\u5e73\u53f0\u6fc0\u52b1\u9a71\u52a8\u521b\u4f5c\u8005\u751f\u6001\u7cfb\u7edf\u7684\u5174\u8d77\uff0c\u9700\u8981\u7814\u7a76\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u5f15\u7528\u4e2d\u7684\u66dd\u5149\u662f\u5426\u53d7\u5230\u5916\u90e8\u6ce8\u610f\u529b\u5e02\u573a\u7684\u5f71\u54cd", "method": "\u5bf944\u5bb6Web3\u4f01\u4e1a\u8fdb\u884c\u5ba1\u8ba1\u7814\u7a76\uff1a1\uff09\u5206\u6790\u4f01\u4e1a\u521b\u4f5c\u8005\u793e\u533a\u7684\u6301\u4e45\u6027\uff1b2\uff09\u901a\u8fc7\u4f01\u4e1a\u7279\u5b9a\u67e5\u8be2\u8bc4\u4f30\u4e0d\u540c\u58f0\u97f3\u7684\u5f15\u7528\u66dd\u5149\u5dee\u5f02\uff1b3\uff09\u5206\u6790\u7c89\u4e1d\u57fa\u7840\u548c\u521b\u4f5c\u8005\u6838\u5fc3\u96c6\u4e2d\u5ea6\u4e0e\u66dd\u5149\u6392\u540d\u7684\u5173\u7cfb", "result": "1\uff09\u4f01\u4e1a\u521b\u4f5c\u8005\u793e\u533a\u968f\u65f6\u95f4\u4fdd\u6301\u7a33\u5b9a\uff1b2\uff09\u66f4\u53d7\u6b22\u8fce\u7684\u58f0\u97f3\u7cfb\u7edf\u6027\u5730\u83b7\u5f97\u66f4\u591a\u5f15\u7528\u66dd\u5149\uff1b3\uff09\u66f4\u5927\u7684\u7c89\u4e1d\u57fa\u7840\u548c\u66f4\u96c6\u4e2d\u7684\u521b\u4f5c\u8005\u6838\u5fc3\u4e0e\u66f4\u9ad8\u7684\u66dd\u5149\u6392\u540d\u76f8\u5173", "conclusion": "\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\u5f15\u7528\u5b58\u5728\u5411\u5df2\u5177\u77e5\u540d\u5ea6\u58f0\u97f3\u7684\u66dd\u5149\u504f\u89c1\uff0c\u53ef\u80fd\u56fa\u5316\u73b0\u6709\u4f18\u52bf\u5730\u4f4d\u5e76\u51cf\u5c11\u89c2\u70b9\u591a\u6837\u6027"}}
{"id": "2601.00843", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00843", "abs": "https://arxiv.org/abs/2601.00843", "authors": ["Ayda Aghaei Nia"], "title": "OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification", "comment": "16 pages, 7 figures, 3 tables. Source code and implementation available at: https://github.com/ayda-aghaei/OmniNeuro. Highlights the use of LLMs (Gemini) and Quantum probability formalism for real-time BCI explainability", "summary": "While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the \"Black Box\" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the \"trial-and-error\" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.", "AI": {"tldr": "OmniNeuro\u662f\u4e00\u4e2a\u65b0\u578bHCI\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u3001\u6df7\u6c8c\u548c\u91cf\u5b50\u542f\u53d1\u7684\u53ef\u89e3\u91ca\u6027\u5f15\u64ce\uff0c\u5c06BCI\u4ece\u9ed1\u76d2\u89e3\u7801\u5668\u8f6c\u53d8\u4e3a\u900f\u660e\u7684\u53cd\u9988\u4f19\u4f34\uff0c\u63d0\u9ad8\u7528\u6237\u7406\u89e3\u548c\u795e\u7ecf\u53ef\u5851\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u867d\u7136\u63d0\u9ad8\u4e86\u8111\u673a\u63a5\u53e3\u7684\u89e3\u7801\u7cbe\u5ea6\uff0c\u4f46\u5176\"\u9ed1\u76d2\"\u7279\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\uff0c\u5bfc\u81f4\u7528\u6237\u632b\u6298\u611f\u548c\u795e\u7ecf\u53ef\u5851\u6027\u7ed3\u679c\u4e0d\u4f73\u3002\u9700\u8981\u53ef\u89e3\u91ca\u7684\u53cd\u9988\u7cfb\u7edf\u6765\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u63d0\u51faOmniNeuro\u6846\u67b6\uff0c\u96c6\u6210\u4e09\u4e2a\u53ef\u89e3\u91ca\u6027\u5f15\u64ce\uff1a1) \u7269\u7406\uff08\u80fd\u91cf\uff09\u5206\u6790\uff0c2) \u6df7\u6c8c\uff08\u5206\u5f62\u590d\u6742\u5ea6\uff09\u5206\u6790\uff0c3) \u91cf\u5b50\u542f\u53d1\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002\u8fd9\u4e9b\u6307\u6807\u9a71\u52a8\u5b9e\u65f6\u795e\u7ecf\u58f0\u6ce2\u5316\u548c\u751f\u6210\u5f0fAI\u4e34\u5e8a\u62a5\u544a\uff0c\u7cfb\u7edf\u4e0e\u89e3\u7801\u5668\u65e0\u5173\u3002", "result": "\u5728PhysioNet\u6570\u636e\u96c6\uff08N=109\uff09\u4e0a\u8fbe\u5230\u5e73\u5747\u51c6\u786e\u738758.52%\u3002\u5b9a\u6027\u8bd5\u70b9\u7814\u7a76\uff08N=3\uff09\u8bc1\u5b9e\u53ef\u89e3\u91ca\u53cd\u9988\u5e2e\u52a9\u7528\u6237\u8c03\u8282\u5fc3\u7406\u52aa\u529b\uff0c\u51cf\u5c11\"\u8bd5\u9519\"\u9636\u6bb5\u3002", "conclusion": "OmniNeuro\u4f5c\u4e3a\u4efb\u4f55\u6700\u5148\u8fdb\u67b6\u6784\u7684\u5fc5\u9700\u53ef\u89e3\u91ca\u6027\u5c42\uff0c\u5c06BCI\u4ece\u6c89\u9ed8\u89e3\u7801\u5668\u8f6c\u53d8\u4e3a\u900f\u660e\u53cd\u9988\u4f19\u4f34\uff0c\u6709\u671b\u6539\u5584\u4e34\u5e8a\u91c7\u7528\u548c\u795e\u7ecf\u53ef\u5851\u6027\u7ed3\u679c\u3002"}}
{"id": "2601.00853", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00853", "abs": "https://arxiv.org/abs/2601.00853", "authors": ["Sameer Rahil", "Zain Abdullah Ahmad", "Talha Asif"], "title": "FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments", "comment": "13 pages, 27 figures", "summary": "Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \\textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.", "AI": {"tldr": "FedSCAM\uff1a\u4e00\u79cd\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u6311\u6218\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574SAM\u6270\u52a8\u534a\u5f84\u548c\u57fa\u4e8e\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u5206\u6570\u7684\u805a\u5408\u6743\u91cd\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u534f\u4f5c\u8bad\u7ec3\u65f6\u9762\u4e34\u7edf\u8ba1\u5f02\u8d28\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6807\u7b7e\u5206\u5e03\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5bf9\u6240\u6709\u5ba2\u6237\u7aef\u4f7f\u7528\u7edf\u4e00\u7684\u6270\u52a8\u534a\u5f84\uff0c\u5ffd\u7565\u4e86\u5ba2\u6237\u7aef\u7279\u5b9a\u7684\u5f02\u8d28\u6027\uff0c\u8fd9\u4f1a\u5f71\u54cd\u6536\u655b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faFedSCAM\u7b97\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1\uff09\u57fa\u4e8e\u5ba2\u6237\u7aef\u5f02\u8d28\u6027\u5206\u6570\u52a8\u6001\u8c03\u6574SAM\u6270\u52a8\u534a\u5f84\uff0c\u5bf9\u9ad8\u65b9\u5dee\u5ba2\u6237\u7aef\u4f7f\u7528\u8f83\u5c0f\u6270\u52a8\u534a\u5f84\u4ee5\u9632\u6b62\u5168\u5c40\u6a21\u578b\u4e0d\u7a33\u5b9a\uff1b2\uff09\u5f15\u5165\u5f02\u8d28\u6027\u611f\u77e5\u7684\u52a0\u6743\u805a\u5408\u673a\u5236\uff0c\u4f18\u5148\u8003\u8651\u4e0e\u5168\u5c40\u4f18\u5316\u65b9\u5411\u4e00\u81f4\u7684\u5ba2\u6237\u7aef\u66f4\u65b0\u3002", "result": "\u5728CIFAR-10\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u57fa\u4e8e\u72c4\u5229\u514b\u96f7\u5206\u5e03\u7684\u4e0d\u540c\u7a0b\u5ea6\u6807\u7b7e\u504f\u659c\u8fdb\u884c\u5b9e\u9a8c\uff0cFedSCAM\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6d4b\u8bd5\u51c6\u786e\u7387\u65b9\u9762\u8fbe\u5230\u4e86\u4e0eFedSAM\u3001FedLESAM\u7b49\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "FedSCAM\u901a\u8fc7\u8003\u8651\u5ba2\u6237\u7aef\u7279\u5b9a\u5f02\u8d28\u6027\u6765\u8c03\u6574SAM\u6270\u52a8\u534a\u5f84\u548c\u805a\u5408\u6743\u91cd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u7edf\u8ba1\u5f02\u8d28\u6027\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u975eIID\u6570\u636e\u73af\u5883\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2601.01751", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01751", "abs": "https://arxiv.org/abs/2601.01751", "authors": ["Samaneh Mohtadi", "Gianluca Demartini"], "title": "Query-Document Dense Vectors for LLM Relevance Judgment Bias Analysis", "comment": "Accepted for presentation at the ECIR 2026 Full Papers track", "summary": "Large Language Models (LLMs) have been used as relevance assessors for Information Retrieval (IR) evaluation collection creation due to reduced cost and increased scalability as compared to human assessors. While previous research has looked at the reliability of LLMs as compared to human assessors, in this work, we aim to understand if LLMs make systematic mistakes when judging relevance, rather than just understanding how good they are on average. To this aim, we propose a novel representational method for queries and documents that allows us to analyze relevance label distributions and compare LLM and human labels to identify patterns of disagreement and localize systematic areas of disagreement. We introduce a clustering-based framework that embeds query-document (Q-D) pairs into a joint semantic space, treating relevance as a relational property. Experiments on TREC Deep Learning 2019 and 2020 show that systematic disagreement between humans and LLMs is concentrated in specific semantic clusters rather than distributed randomly. Query-level analyses reveal recurring failures, most often in definition-seeking, policy-related, or ambiguous contexts. Queries with large variation in agreement across their clusters emerge as disagreement hotspots, where LLMs tend to under-recall relevant content or over-include irrelevant material. This framework links global diagnostics with localized clustering to uncover hidden weaknesses in LLM judgments, enabling bias-aware and more reliable IR evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6846\u67b6\u6765\u5206\u6790LLM\u4f5c\u4e3a\u76f8\u5173\u6027\u8bc4\u4f30\u8005\u65f6\u7684\u7cfb\u7edf\u6027\u9519\u8bef\uff0c\u53d1\u73b0LLM\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u7684\u5206\u6b67\u96c6\u4e2d\u5728\u7279\u5b9a\u8bed\u4e49\u7c07\u800c\u975e\u968f\u673a\u5206\u5e03\uff0c\u63ed\u793a\u4e86LLM\u5728\u5b9a\u4e49\u67e5\u8be2\u3001\u653f\u7b56\u76f8\u5173\u548c\u6a21\u7cca\u8bed\u5883\u4e2d\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\u3002", "motivation": "\u867d\u7136LLM\u5df2\u88ab\u7528\u4f5c\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u4e2d\u7684\u76f8\u5173\u6027\u8bc4\u4f30\u8005\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u76f8\u6bd4\u7684\u53ef\u9760\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3LLM\u5728\u5224\u65ad\u76f8\u5173\u6027\u65f6\u662f\u5426\u72af\u7cfb\u7edf\u6027\u9519\u8bef\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e86\u89e3\u5176\u5e73\u5747\u8868\u73b0\u5982\u4f55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u67e5\u8be2-\u6587\u6863\u5bf9\u5d4c\u5165\u5230\u8054\u5408\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u5c06\u76f8\u5173\u6027\u89c6\u4e3a\u5173\u7cfb\u5c5e\u6027\u3002\u5f15\u5165\u57fa\u4e8e\u805a\u7c7b\u7684\u6846\u67b6\u6765\u5206\u6790\u76f8\u5173\u6027\u6807\u7b7e\u5206\u5e03\uff0c\u6bd4\u8f83LLM\u548c\u4eba\u7c7b\u6807\u7b7e\u4ee5\u8bc6\u522b\u5206\u6b67\u6a21\u5f0f\u5e76\u5b9a\u4f4d\u7cfb\u7edf\u6027\u5206\u6b67\u533a\u57df\u3002", "result": "\u5728TREC Deep Learning 2019\u548c2020\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4eba\u7c7b\u4e0eLLM\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5206\u6b67\u96c6\u4e2d\u5728\u7279\u5b9a\u8bed\u4e49\u7c07\u4e2d\uff0c\u800c\u975e\u968f\u673a\u5206\u5e03\u3002\u67e5\u8be2\u7ea7\u5206\u6790\u63ed\u793a\u4e86\u5728\u5b9a\u4e49\u67e5\u8be2\u3001\u653f\u7b56\u76f8\u5173\u6216\u6a21\u7cca\u8bed\u5883\u4e2d\u7684\u91cd\u590d\u6027\u5931\u8d25\u3002\u5177\u6709\u8de8\u7c07\u4e00\u81f4\u6027\u5dee\u5f02\u5927\u7684\u67e5\u8be2\u6210\u4e3a\u5206\u6b67\u70ed\u70b9\uff0cLLM\u503e\u5411\u4e8e\u4f4e\u4f30\u76f8\u5173\u5185\u5bb9\u6216\u9ad8\u4f30\u65e0\u5173\u5185\u5bb9\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u8bca\u65ad\u4e0e\u5c40\u90e8\u805a\u7c7b\u76f8\u7ed3\u5408\uff0c\u63ed\u793a\u4e86LLM\u5224\u65ad\u4e2d\u7684\u9690\u85cf\u5f31\u70b9\uff0c\u4e3a\u5b9e\u73b0\u504f\u5dee\u611f\u77e5\u548c\u66f4\u53ef\u9760\u7684\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b9\u6cd5\u3002"}}
{"id": "2601.00845", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00845", "abs": "https://arxiv.org/abs/2601.00845", "authors": ["Lili Chen", "Wensheng Gan", "Shuang Liang", "Philip S. Yu"], "title": "Enhancing Temporal Awareness in LLMs for Temporal Point Processes", "comment": "preprint", "summary": "Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL", "AI": {"tldr": "TPP-TAL\u662f\u4e00\u4e2a\u589e\u5f3aLLMs\u4e2d\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u65f6\u95f4\u52a8\u6001\u4e0e\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u6539\u5584\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u4e2d\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u957f\u7a0b\u4ea4\u4e92\u3002", "motivation": "\u65f6\u95f4\u70b9\u8fc7\u7a0b\u5728\u91d1\u878d\u3001\u533b\u7597\u3001\u793e\u4ea4\u7cfb\u7edf\u7b49\u9886\u57df\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u65f6\u95f4\u4fe1\u606f\u4e0e\u8bed\u4e49\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u9650\u5236\u4e86LLMs\u5728\u65f6\u95f4\u70b9\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faTPP-TAL\u6846\u67b6\uff0c\u4e0d\u91c7\u7528\u7b80\u5355\u62fc\u63a5\u4e8b\u4ef6\u65f6\u95f4\u548c\u7c7b\u578b\u5d4c\u5165\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u800c\u662f\u5728\u5c06\u4fe1\u606f\u8f93\u5165LLM\u4e4b\u524d\u663e\u5f0f\u5bf9\u9f50\u65f6\u95f4\u52a8\u6001\u4e0e\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u4e8b\u4ef6\u95f4\u957f\u7a0b\u4ea4\u4e92\u7684\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTPP-TAL\u5728\u65f6\u95f4\u4f3c\u7136\u4f30\u8ba1\u548c\u4e8b\u4ef6\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u589e\u5f3aLLMs\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u5bf9\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002", "conclusion": "TPP-TAL\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u65f6\u95f4\u52a8\u6001\u4e0e\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u65f6\u95f4\u70b9\u8fc7\u7a0b\u5efa\u6a21\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00857", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00857", "abs": "https://arxiv.org/abs/2601.00857", "authors": ["Yuchi Ma", "Yawen Shen", "Anu Swatantran", "David B. Lobell"], "title": "Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks", "comment": null, "summary": "Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86AlphaEarth Foundation (AEF)\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u519c\u4e1a\u76d1\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u3001\u8015\u4f5c\u5236\u56fe\u548c\u8986\u76d6\u4f5c\u7269\u5236\u56fe\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u4e0e\u4f20\u7edf\u9065\u611f\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u867d\u7136\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b(GFMs)\u5982AEF\u5728\u571f\u5730\u8986\u76d6\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u519c\u4e1a\u76d1\u6d4b\u9886\u57df\u7684\u5e94\u7528\u7f3a\u4e4f\u6df1\u5165\u8bc4\u4f30\u3002\u9700\u8981\u5168\u9762\u6bd4\u8f83AEF\u6a21\u578b\u4e0e\u4f20\u7edf\u9065\u611f\u6a21\u578b\u5728\u4e0d\u540c\u519c\u4e1a\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528AEF\u9884\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u5e74\u5ea6\u5168\u7403\u5d4c\u5165\u6570\u636e\u96c6\uff0c\u5728\u7f8e\u56fd\u7684\u4e09\u4e2a\u519c\u4e1a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff1a\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u3001\u8015\u4f5c\u5236\u56fe\u548c\u8986\u76d6\u4f5c\u7269\u5236\u56fe\u3002\u4ece\u516c\u5171\u548c\u79c1\u4eba\u6765\u6e90\u7f16\u8bd1\u6570\u636e\u96c6\uff0c\u5728\u4e0d\u540c\u5c3a\u5ea6\u548c\u4f4d\u7f6e\u8bc4\u4f30AEF\u5d4c\u5165\uff0c\u540c\u65f6\u8bad\u7ec3\u4f20\u7edf\u9065\u611f\u6a21\u578b\u4f5c\u4e3a\u5bf9\u6bd4\u57fa\u51c6\u3002", "result": "AEF\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5728\u4ea7\u91cf\u9884\u6d4b\u548c\u53bf\u7ea7\u8015\u4f5c\u5236\u56fe\u4efb\u52a1\u4e2d\uff0c\u5f53\u4f7f\u7528\u672c\u5730\u6570\u636e\u8bad\u7ec3\u65f6\uff0c\u4e0e\u4e13\u95e8\u6784\u5efa\u7684\u9065\u611f\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\u3002\u4f46AEF\u5d4c\u5165\u5b58\u5728\u7a7a\u95f4\u53ef\u8fc1\u79fb\u6027\u6709\u9650\u3001\u53ef\u89e3\u91ca\u6027\u4f4e\u3001\u65f6\u95f4\u654f\u611f\u6027\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\u3002", "conclusion": "AEF\u5d4c\u5165\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u65f6\u95f4\u654f\u611f\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u8981\u6c42\u8f83\u9ad8\u7684\u519c\u4e1a\u9886\u57df\u9700\u8981\u8c28\u614e\u4f7f\u7528\u3002\u5f53\u524dAEF\u5d4c\u5165\u5b58\u5728\u7a7a\u95f4\u8f6c\u79fb\u80fd\u529b\u6709\u9650\u3001\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\uff0c\u5efa\u8bae\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u6ce8\u610f\u8fd9\u4e9b\u9650\u5236\u3002"}}
{"id": "2601.01753", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01753", "abs": "https://arxiv.org/abs/2601.01753", "authors": ["Hyunsoo Kim", "Jaewan Moon", "Seongmin Park", "Jongwuk Lee"], "title": "MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation", "comment": "Accepted by KDD 2026", "summary": "Modern recommender systems trained on domain-specific data often struggle to generalize across multiple domains. Cross-domain sequential recommendation has emerged as a promising research direction to address this challenge; however, existing approaches face fundamental limitations, such as reliance on overlapping users or items across domains, or unrealistic assumptions that ignore privacy constraints. In this work, we propose a new framework, MergeRec, based on model merging under a new and realistic problem setting termed data-isolated cross-domain sequential recommendation, where raw user interaction data cannot be shared across domains. MergeRec consists of three key components: (1) merging initialization, (2) pseudo-user data construction, and (3) collaborative merging optimization. First, we initialize a merged model using training-free merging techniques. Next, we construct pseudo-user data by treating each item as a virtual sequence in each domain, enabling the synthesis of meaningful training samples without relying on real user interactions. Finally, we optimize domain-specific merging weights through a joint objective that combines a recommendation loss, which encourages the merged model to identify relevant items, and a distillation loss, which transfers collaborative filtering signals from the fine-tuned source models. Extensive experiments demonstrate that MergeRec not only preserves the strengths of the original models but also significantly enhances generalizability to unseen domains. Compared to conventional model merging methods, MergeRec consistently achieves superior performance, with average improvements of up to 17.21% in Recall@10, highlighting the potential of model merging as a scalable and effective approach for building universal recommender systems. The source code is available at https://github.com/DIALLab-SKKU/MergeRec.", "AI": {"tldr": "MergeRec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u878d\u5408\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u636e\u9694\u79bb\u7684\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u95ee\u9898\uff0c\u65e0\u9700\u5171\u4eab\u539f\u59cb\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u901a\u8fc7\u4f2a\u7528\u6237\u6570\u636e\u6784\u5efa\u548c\u534f\u540c\u878d\u5408\u4f18\u5316\u5b9e\u73b0\u8de8\u57df\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u8de8\u57df\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u8981\u4e48\u4f9d\u8d56\u8de8\u57df\u91cd\u53e0\u7528\u6237/\u7269\u54c1\uff0c\u8981\u4e48\u5ffd\u89c6\u9690\u79c1\u7ea6\u675f\u3002\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u4e0d\u540c\u9886\u57df\u7684\u539f\u59cb\u7528\u6237\u4ea4\u4e92\u6570\u636e\u7531\u4e8e\u9690\u79c1\u539f\u56e0\u65e0\u6cd5\u5171\u4eab\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MergeRec\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u878d\u5408\u521d\u59cb\u5316\uff1a\u4f7f\u7528\u514d\u8bad\u7ec3\u7684\u878d\u5408\u6280\u672f\u521d\u59cb\u5316\u878d\u5408\u6a21\u578b\uff1b2) \u4f2a\u7528\u6237\u6570\u636e\u6784\u5efa\uff1a\u5c06\u6bcf\u4e2a\u7269\u54c1\u89c6\u4e3a\u865a\u62df\u5e8f\u5217\uff0c\u5408\u6210\u6709\u610f\u4e49\u7684\u8bad\u7ec3\u6837\u672c\uff1b3) \u534f\u540c\u878d\u5408\u4f18\u5316\uff1a\u901a\u8fc7\u63a8\u8350\u635f\u5931\u548c\u84b8\u998f\u635f\u5931\u7684\u8054\u5408\u76ee\u6807\u4f18\u5316\u9886\u57df\u7279\u5b9a\u878d\u5408\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMergeRec\u4e0d\u4ec5\u4fdd\u7559\u4e86\u539f\u59cb\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u8fd8\u663e\u8457\u589e\u5f3a\u4e86\u5bf9\u672a\u89c1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u5728Recall@10\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u5347\u9ad8\u8fbe17.21%\u3002", "conclusion": "\u6a21\u578b\u878d\u5408\u662f\u6784\u5efa\u901a\u7528\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u9014\u5f84\u3002MergeRec\u5728\u6570\u636e\u9694\u79bb\u7684\u8de8\u57df\u5e8f\u5217\u63a8\u8350\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u8de8\u57df\u63a8\u8350\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.01836", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01836", "abs": "https://arxiv.org/abs/2601.01836", "authors": ["Dasol Choi", "DongGeon Lee", "Brigitta Jesica Kartono", "Helena Berndt", "Taeyoun Kwon", "Joonwon Jang", "Haon Park", "Hwanjo Yu", "Minsuk Kahng"], "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs", "comment": null, "summary": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.", "AI": {"tldr": "COMPASS\u6846\u67b6\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30LLM\u662f\u5426\u7b26\u5408\u7ec4\u7ec7\u767d\u540d\u5355/\u9ed1\u540d\u5355\u653f\u7b56\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5904\u7406\u5408\u6cd5\u8bf7\u6c42\u65f6\u8868\u73b0\u826f\u597d\uff08>95%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728\u6267\u884c\u7981\u4ee4\u65f6\u4e25\u91cd\u5931\u8d25\uff08\u4ec5\u62d2\u7edd13-40%\u7684\u5bf9\u6297\u6027\u8fdd\u89c4\uff09", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f01\u4e1a\u9ad8\u98ce\u9669\u5e94\u7528\uff08\u533b\u7597\u3001\u91d1\u878d\u7b49\uff09\u4e2d\u7684\u90e8\u7f72\uff0c\u786e\u4fdd\u6a21\u578b\u9075\u5b88\u7ec4\u7ec7\u7279\u5b9a\u653f\u7b56\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4ec5\u5173\u6ce8\u666e\u904d\u5371\u5bb3\uff0c\u7f3a\u4e4f\u5bf9\u7ec4\u7ec7\u653f\u7b56\u5408\u89c4\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u63d0\u51faCOMPASS\u6846\u67b6\uff0c\u5e94\u7528\u4e8e8\u4e2a\u4e0d\u540c\u884c\u4e1a\u573a\u666f\uff0c\u751f\u6210\u5e76\u9a8c\u8bc15,920\u4e2a\u67e5\u8be2\uff0c\u6d4b\u8bd5\u5e38\u89c4\u5408\u89c4\u6027\u548c\u901a\u8fc7\u7b56\u7565\u8bbe\u8ba1\u7684\u8fb9\u7f18\u6848\u4f8b\u8fdb\u884c\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f307\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u53d1\u73b0\u57fa\u672c\u4e0d\u5bf9\u79f0\u6027\uff1a\u6a21\u578b\u53ef\u9760\u5904\u7406\u5408\u6cd5\u8bf7\u6c42\uff08>95%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728\u6267\u884c\u7981\u4ee4\u65f6\u707e\u96be\u6027\u5931\u8d25\uff0c\u4ec5\u62d2\u7edd13-40%\u7684\u5bf9\u6297\u6027\u9ed1\u540d\u5355\u8fdd\u89c4\u3002", "conclusion": "\u5f53\u524dLLM\u7f3a\u4e4f\u653f\u7b56\u5173\u952e\u90e8\u7f72\u6240\u9700\u7684\u9c81\u68d2\u6027\uff0cCOMPASS\u6210\u4e3a\u7ec4\u7ec7AI\u5b89\u5168\u7684\u91cd\u8981\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.00905", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00905", "abs": "https://arxiv.org/abs/2601.00905", "authors": ["Eliot Park", "Abhi Kumar", "Pranav Rajpurkar"], "title": "Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems", "comment": "x", "summary": "While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86GPT-4o\u3001GPT-4o-mini\u548cClaude 3.5\u7b49\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u7269\u54c1\u53ef\u56de\u6536\u6027\u65b9\u9762\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5339\u914d\u7269\u54c1\u5230\u5408\u9002\u56de\u6536\u7bb1\u7684\u80fd\u529b\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5728\u4f4d\u7f6e\u7279\u5b9a\u6307\u5357\u3001\u6c61\u67d3/\u635f\u574f\u548c\u591a\u6750\u6599\u7269\u54c1\u7b49\u6311\u6218\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u867d\u7136\u9ad8\u6548\u56de\u6536\u7684\u91cd\u8981\u6027\u88ab\u5e7f\u6cdb\u8ba4\u53ef\uff0c\u4f46\u516c\u4f17\u51c6\u786e\u5224\u65ad\u7269\u54c1\u53ef\u56de\u6536\u6027\u548c\u6b63\u786e\u5904\u7406\u65b9\u5f0f\u4ecd\u7136\u662f\u4e00\u9879\u590d\u6742\u4efb\u52a1\u3002\u9700\u8981\u63a2\u7d22\u5148\u8fdbAI\u6a21\u578b\u5728\u6539\u5584\u56de\u6536\u5b9e\u8df5\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u7cbe\u5fc3\u7b56\u5212\u7684\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8bc4\u4f30GPT-4o\u3001GPT-4o-mini\u548cClaude 3.5\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5305\u62ec\uff1a1\uff09\u5339\u914d\u7269\u54c1\u5230\u5408\u9002\u56de\u6536\u7bb1\u5e76\u5224\u65ad\u7269\u7406\u9002\u914d\u6027\uff1b2\uff09\u6d4b\u8bd5\u4f4d\u7f6e\u7279\u5b9a\u56de\u6536\u6307\u5357\u4e0b\u7684\u9884\u6d4b\u8c03\u6574\uff1b3\uff09\u8003\u8651\u6c61\u67d3\u6216\u7ed3\u6784\u635f\u574f\u60c5\u51b5\uff1b4\uff09\u5904\u7406\u591a\u6750\u6599\u7269\u54c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u76f8\u6bd4\u5148\u524d\u7248\u672c\u6709\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002\u6a21\u578b\u5728\u5339\u914d\u7269\u54c1\u5230\u56de\u6536\u7bb1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u4f4d\u7f6e\u7279\u5b9a\u6307\u5357\u3001\u6c61\u67d3/\u635f\u574f\u8bc4\u4f30\u548c\u591a\u6750\u6599\u7269\u54c1\u5206\u89e3\u7b49\u590d\u6742\u573a\u666f\u65f6\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578b\u7684\u6301\u7eed\u6539\u8fdb\u5bf9\u4e8e\u63d0\u5347\u516c\u4f17\u56de\u6536\u5b9e\u8df5\u548c\u63a8\u8fdb\u73af\u5883\u53ef\u6301\u7eed\u6027\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u5f53\u524d\u6a21\u578b\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u56de\u6536\u573a\u666f\u65f6\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2601.00860", "categories": ["cs.LG", "cs.AI", "physics.app-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00860", "abs": "https://arxiv.org/abs/2601.00860", "authors": ["Xidi Wang"], "title": "Path Integral Solution for Dissipative Generative Dynamics", "comment": "6 pages, 2 figures, 2 tables, along with 2 supplementary materials", "summary": "Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u7eaf\u673a\u68b0\u7cfb\u7edf\u901a\u8fc7\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\u548c\u975e\u5c40\u90e8\u4e0a\u4e0b\u6587\u805a\u5408\u53ef\u4ee5\u751f\u6210\u667a\u80fd\u8bed\u8a00\uff0c\u800c\u5b88\u6052\u5b9a\u5f8b\u4f1a\u5bfc\u81f4\u6839\u672c\u6027\u5931\u8d25\u3002\u8bed\u8a00\u751f\u6210\u672c\u8d28\u4e0a\u662f\u8017\u6563\u91cf\u5b50\u573a\u8bba\uff0c\u667a\u80fd\u901a\u8fc7\u8017\u6563\u548c\u975e\u5c40\u57df\u6027\u83b7\u5f97\uff0c\u800c\u975e\u5b88\u6052\u3002", "motivation": "\u63a2\u7d22\u7eaf\u673a\u68b0\u7cfb\u7edf\u662f\u5426\u80fd\u591f\u751f\u6210\u667a\u80fd\u8bed\u8a00\uff0c\u7814\u7a76\u91cf\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u8017\u6563\u4e0e\u975e\u5c40\u57df\u6027\u5728\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u4f5c\u7528\uff0c\u6311\u6218\u4f20\u7edf\u8ba4\u4e3a\u5b88\u6052\u5b9a\u5f8b\u662f\u667a\u80fd\u7cfb\u7edf\u57fa\u7840\u7684\u89c2\u70b9\u3002", "method": "\u91c7\u7528\u5177\u6709\u5c01\u95ed\u5f62\u5f0f\u8def\u5f84\u79ef\u5206\u4f20\u64ad\u5b50\u7684Koopman\u7b97\u5b50\u65b9\u6cd5\uff0c\u5206\u6790\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u975e\u5c40\u90e8\u4e0a\u4e0b\u6587\u805a\u5408\u673a\u5236\uff0c\u901a\u8fc7\u8c31\u5206\u6790\u63ed\u793a\u7279\u5f81\u503c\u7ed3\u6784\uff08\u8870\u51cf\u6a21\u5f0f\u3001\u589e\u957f\u6a21\u5f0f\u3001\u4e2d\u6027\u6a21\u5f0f\uff09\u3002", "result": "\u8bc1\u660e\u8017\u6563\u91cf\u5b50\u52a8\u529b\u5b66\u80fd\u591f\u4ea7\u751f\u8fde\u8d2f\u7684\u6587\u672c\u751f\u6210\uff0c\u800c\u54c8\u5bc6\u987f\u7ea6\u675f\u4f1a\u6d88\u9664\u8017\u6563\u6a21\u5f0f\u5e76\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u8c31\u5206\u6790\u663e\u793a\u7279\u5f81\u503c\u7ed3\u6784\u5206\u79bb\u4e3a\u8870\u51cf\u6a21\u5f0f\uff08\u9057\u5fd8\uff09\u3001\u589e\u957f\u6a21\u5f0f\uff08\u653e\u5927\uff09\u548c\u4e2d\u6027\u6a21\u5f0f\uff08\u4fdd\u6301\uff09\uff0c\u8fd9\u4e9b\u662f\u5b9a\u5411\u4fe1\u606f\u6d41\u7684\u57fa\u672c\u8981\u7d20\u3002", "conclusion": "\u8bed\u8a00\u751f\u6210\u672c\u8d28\u4e0a\u662f\u8017\u6563\u91cf\u5b50\u573a\u8bba\uff0c\u673a\u68b0\u7cfb\u7edf\u901a\u8fc7\u8017\u6563\u548c\u975e\u5c40\u57df\u6027\u7684\u7ec4\u5408\u83b7\u5f97\u667a\u80fd\uff0c\u800c\u975e\u901a\u8fc7\u5b88\u6052\u5b9a\u5f8b\u3002\u53d7\u63a7\u4fe1\u606f\u8017\u6563\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\u805a\u5408\u662f\u667a\u80fd\u8bed\u8a00\u751f\u6210\u7684\u6839\u672c\u8981\u6c42\u3002"}}
{"id": "2601.01785", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01785", "abs": "https://arxiv.org/abs/2601.01785", "authors": ["Rajiv Chaitanya Muttur"], "title": "SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines", "comment": "Presented at ICEdge 2025; nominated for Best Paper Award", "summary": "Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.", "AI": {"tldr": "SRAS\u662f\u4e00\u79cd\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907RAG\u7cfb\u7edf\u7684\u8f7b\u91cf\u7ea7\u6587\u6863\u9009\u62e9\u5668\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4ec50.76MB\u5927\u5c0f\uff0c\u5728CPU\u4e0a\u5ef6\u8fdf\u5c0f\u4e8e1\u79d2\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u4f7f\u7528\u56fa\u5b9a\u7684top-k\u6587\u6863\u9009\u62e9\u673a\u5236\uff0c\u5ffd\u7565\u4e86\u751f\u6210\u8d28\u91cf\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e0d\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5ef6\u8fdf\u611f\u77e5\u7684\u6587\u6863\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSRAS\uff08\u7a00\u758f\u5956\u52b1\u611f\u77e5\u9009\u62e9\u5668\uff09\uff0c\u4f7f\u7528PPO\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7d27\u51d1\u7b56\u7565\uff08~0.76MB\uff09\uff0c\u91c7\u7528\u7ed3\u5408Relaxed F1\u548cBERTScore\u7684\u6df7\u5408\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u548c\u4ee4\u724c\u7ea6\u675f\u4e0b\u8fd0\u884c\u3002", "result": "\u5728\u5408\u6210QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u76d1\u7763\u548c\u968f\u673a\u9009\u62e9\u5668\uff0c\u5728SQuAD v2\u4e0a\u8fbe\u52300.8546\u7684BERTScore F1\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8c03\u4f18\uff0c\u5ef6\u8fdf\u5c0f\u4e8e1\u79d2\u3002", "conclusion": "\u9996\u6b21\u8bc1\u660e\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6587\u6863\u9009\u62e9\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8f7b\u91cf\u7ea7\u3001\u5ef6\u8fdf\u611f\u77e5\uff0c\u5e76\u6709\u6548\u7528\u4e8e\u8bbe\u5907\u7aefRAG\u6d41\u6c34\u7ebf\uff0c\u4e3a\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01878", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.01878", "abs": "https://arxiv.org/abs/2601.01878", "authors": ["Farzan Karimi-Malekabadi", "Suhaib Abdurahman", "Zhivar Sourati", "Jackson Trager", "Morteza Dehghani"], "title": "Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs", "comment": null, "summary": "Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u8ba4\u77e5\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u8bc4\u4f30-\u90e8\u7f72\u5dee\u8ddd\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u660e\u786e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u88ab\u8fc7\u5ea6\u6cdb\u5316\u3002\u4f5c\u8005\u63d0\u51fa\"\u7406\u8bba\u8ffd\u8e2a\u5361\"\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5206\u5f88\u9ad8\uff0c\u4f46\u8fd9\u4e9b\u5206\u6570\u5f80\u5f80\u65e0\u6cd5\u9884\u6d4b\u771f\u5b9e\u4e16\u754c\u884c\u4e3a\u3002\u73b0\u6709\u7814\u7a76\u5c06\u8fd9\u79cd\u5dee\u8ddd\u5f52\u56e0\u4e8e\u6d4b\u91cf\u548c\u6548\u5ea6\u95ee\u9898\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u66f4\u6df1\u5c42\u7684\u539f\u56e0\u662f\u7f3a\u4e4f\u660e\u786e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u5bf9\u80fd\u529b\u8bc4\u4f30\u7684\u8fc7\u5ea6\u6cdb\u5316\u3002", "method": "\u9996\u5148\u8bca\u65ad\u5e76\u5f62\u5f0f\u5316\"\u7406\u8bba\u5dee\u8ddd\"\u95ee\u9898\uff0c\u7136\u540e\u63d0\u51fa\"\u7406\u8bba\u8ffd\u8e2a\u5361\"\u8fd9\u4e00\u8f7b\u91cf\u7ea7\u6587\u6863\u5de5\u5177\uff0c\u660e\u786e\u8bb0\u5f55\u8bc4\u4f30\u7684\u7406\u8bba\u57fa\u7840\u3001\u76ee\u6807\u80fd\u529b\u7ec4\u4ef6\u3001\u64cd\u4f5c\u5316\u8fc7\u7a0b\u53ca\u5c40\u9650\u6027\u3002", "result": "\u7406\u8bba\u8ffd\u8e2a\u5361\u80fd\u591f\u589e\u5f3a\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u91cd\u7528\u6027\uff0c\u901a\u8fc7\u660e\u786e\u7406\u8bba\u3001\u4efb\u52a1\u64cd\u4f5c\u5316\u3001\u8bc4\u5206\u548c\u5c40\u9650\u6027\u4e4b\u95f4\u7684\u5b8c\u6574\u6548\u5ea6\u94fe\uff0c\u800c\u4e0d\u9700\u8981\u4fee\u6539\u57fa\u51c6\u6d4b\u8bd5\u6216\u8981\u6c42\u7406\u8bba\u7edf\u4e00\u3002", "conclusion": "\u89e3\u51b3\u8bc4\u4f30-\u90e8\u7f72\u5dee\u8ddd\u9700\u8981\u586b\u8865\u7406\u8bba\u7a7a\u767d\uff0c\u7406\u8bba\u8ffd\u8e2a\u5361\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u793e\u4f1a\u8ba4\u77e5\u8bc4\u4f30\u7684\u900f\u660e\u5ea6\u548c\u6709\u6548\u6027\uff0c\u51cf\u5c11\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u7684\u7cfb\u7edf\u6027\u8fc7\u5ea6\u89e3\u8bfb\u3002"}}
{"id": "2601.00913", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00913", "abs": "https://arxiv.org/abs/2601.00913", "authors": ["Subhankar Mishra"], "title": "Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs", "AI": {"tldr": "Clean-GS\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f\u8bed\u4e49\u63a9\u7801\u53bb\u96643D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u7684\u80cc\u666f\u6742\u6ce2\u548c\u6f02\u6d6e\u7269\uff0c\u5b9e\u73b060-80%\u6a21\u578b\u538b\u7f29\uff0c\u4fdd\u6301\u76ee\u6807\u7269\u4f53\u8d28\u91cf", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u4f1a\u4ea7\u751f\u5927\u91cf\u865a\u5047\u9ad8\u65af\uff08\u6f02\u6d6e\u7269\uff09\uff0c\u8fd9\u4e9b\u4f2a\u5f71\u4f1a\u906e\u6321\u611f\u5174\u8da3\u7269\u4f53\u5e76\u589e\u52a0\u6a21\u578b\u5927\u5c0f\uff0c\u963b\u788d\u5728\u5e26\u5bbd\u53d7\u9650\u5e94\u7528\u4e2d\u7684\u90e8\u7f72", "method": "\u7ed3\u5408\u767d\u540d\u5355\u7a7a\u95f4\u8fc7\u6ee4\u3001\u989c\u8272\u5f15\u5bfc\u9a8c\u8bc1\u548c\u79bb\u7fa4\u503c\u53bb\u9664\u7684\u591a\u9636\u6bb5\u65b9\u6cd5\uff1a1)\u901a\u8fc7\u6295\u5f71\u5230\u63a9\u7801\u533a\u57df\u8fdb\u884c\u767d\u540d\u5355\u8fc7\u6ee4\uff1b2)\u6df1\u5ea6\u7f13\u51b2\u989c\u8272\u9a8c\u8bc1\uff1b3)\u57fa\u4e8e\u90bb\u5c45\u7684\u79bb\u7fa4\u503c\u53bb\u9664", "result": "\u5728Tanks and Temples\u6570\u636e\u96c6\u4e0a\uff0c\u6587\u4ef6\u5927\u5c0f\u4ece125MB\u51cf\u5c11\u523047MB\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\uff0c\u4f7f3DGS\u6a21\u578b\u9002\u7528\u4e8eWeb\u90e8\u7f72\u548cAR/VR\u5e94\u7528", "conclusion": "Clean-GS\u4f7f\u7528\u4ec53\u4e2a\u5206\u5272\u63a9\u7801\uff081%\u7684\u89c6\u56fe\uff09\u5c31\u80fd\u6709\u6548\u53bb\u9664\u80cc\u666f\u6742\u6ce2\u548c\u6f02\u6d6e\u7269\uff0c\u76f8\u6bd4\u4f9d\u8d56\u5168\u5c40\u91cd\u8981\u6027\u6307\u6807\u7684\u73b0\u67093DGS\u4fee\u526a\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u8bed\u4e49\u5f15\u5bfc\u538b\u7f29"}}
{"id": "2601.00856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00856", "abs": "https://arxiv.org/abs/2601.00856", "authors": ["Milos Stankovic", "Ella Hirche", "Sarah Kollatzsch", "Julia Nadine Doetsch"], "title": "Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks", "comment": "Comment on arXiv:2506.08872", "summary": "Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5bf9Kosmyna\u7b49\u4eba(2025)\u5173\u4e8e\u4f7f\u7528ChatGPT\u8fdb\u884c\u8bba\u6587\u5199\u4f5c\u4efb\u52a1\u4e2d\u8ba4\u77e5\u503a\u52a1\u79ef\u7d2f\u7814\u7a76\u7684\u8bc4\u8bba\u6587\u7ae0\uff0c\u6307\u51fa\u4e86\u539f\u7814\u7a76\u5728\u6837\u672c\u91cf\u3001\u53ef\u91cd\u590d\u6027\u3001EEG\u5206\u6790\u65b9\u6cd5\u3001\u7ed3\u679c\u62a5\u544a\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u7b49\u65b9\u9762\u7684\u95ee\u9898\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u5bf9Kosmyna\u7b49\u4eba\u5173\u4e8eAI\u52a9\u624b\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u5f71\u54cd\u7684\u7814\u7a76\u63d0\u4f9b\u5efa\u8bbe\u6027\u8bc4\u8bba\uff0c\u5e2e\u52a9\u6539\u8fdb\u8be5\u7814\u7a76\u4ee5\u9002\u5408\u540c\u884c\u8bc4\u5ba1\u53d1\u8868\uff0c\u56e0\u4e3a\u539f\u7814\u7a76\u7684\u4e00\u4e9b\u7ed3\u679c\u53ef\u80fd\u9700\u8981\u66f4\u4fdd\u5b88\u7684\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u6279\u5224\u6027\u5206\u6790\u539f\u7814\u7a76\u7684\u65b9\u6cd5\u8bba\uff0c\u91cd\u70b9\u5173\u6ce8\u4e94\u4e2a\u4e3b\u8981\u65b9\u9762\uff1a\u7814\u7a76\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u6837\u672c\u91cf\u9650\u5236\uff09\u3001\u5206\u6790\u7684\u53ef\u91cd\u590d\u6027\u3001EEG\u5206\u6790\u65b9\u6cd5\u95ee\u9898\u3001\u7ed3\u679c\u62a5\u544a\u7684\u4e0d\u4e00\u81f4\u6027\u4ee5\u53ca\u7814\u7a76\u8fc7\u7a0b\u548c\u53d1\u73b0\u7684\u900f\u660e\u5ea6\u4e0d\u8db3\u3002", "result": "\u8bc4\u8bba\u6587\u7ae0\u6307\u51fa\u4e86\u539f\u7814\u7a76\u5b58\u5728\u7684\u591a\u4e2a\u65b9\u6cd5\u8bba\u548c\u62a5\u544a\u95ee\u9898\uff0c\u5305\u62ec\u6837\u672c\u91cf\u4e0d\u8db3\u53ef\u80fd\u5f71\u54cd\u7edf\u8ba1\u6548\u529b\u3001\u5206\u6790\u65b9\u6cd5\u53ef\u80fd\u4e0d\u591f\u7a33\u5065\u3001\u7ed3\u679c\u62a5\u544a\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u6574\u4f53\u7814\u7a76\u900f\u660e\u5ea6\u6709\u5f85\u63d0\u9ad8\u3002", "conclusion": "\u867d\u7136\u8ba4\u53efKosmyna\u7b49\u4eba\u7814\u7a76\u7684\u91cd\u8981\u6027\u548c\u4ef7\u503c\uff0c\u4f46\u5efa\u8bae\u5728\u53d1\u8868\u524d\u89e3\u51b3\u8fd9\u4e9b\u65b9\u6cd5\u8bba\u548c\u62a5\u544a\u95ee\u9898\uff0c\u4ee5\u4fbf\u66f4\u51c6\u786e\u5730\u89e3\u91ca\u7ed3\u679c\u5e76\u63d0\u9ad8\u7814\u7a76\u7684\u79d1\u5b66\u4e25\u8c28\u6027\u3002"}}
{"id": "2601.01897", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01897", "abs": "https://arxiv.org/abs/2601.01897", "authors": ["Lilu Cheng", "Jingjun Lu", "Yi Xuan Chan", "Quoc Khai Nguyen", "John Bi", "Sean Ho"], "title": "A Hybrid Architecture for Multi-Stage Claim Document Understanding: Combining Vision-Language Models and Machine Learning for Real-Time Processing", "comment": "19 pages, 3 figures, 3 tables", "summary": "Claims documents are fundamental to healthcare and insurance operations, serving as the basis for reimbursement, auditing, and compliance. However, these documents are typically not born digital; they often exist as scanned PDFs or photographs captured under uncontrolled conditions. Consequently, they exhibit significant content heterogeneity, ranging from typed invoices to handwritten medical reports, as well as linguistic diversity. This challenge is exemplified by operations at Fullerton Health, which handles tens of millions of claims annually across nine markets, including Singapore, the Philippines, Indonesia, Malaysia, Mainland China, Hong Kong, Vietnam, Papua New Guinea, and Cambodia. Such variability, coupled with inconsistent image quality and diverse layouts, poses a significant obstacle to automated parsing and structured information extraction.\n  This paper presents a robust multi-stage pipeline that integrates the multilingual optical character recognition (OCR) engine PaddleOCR, a traditional Logistic Regression classifier, and a compact Vision-Language Model (VLM), Qwen 2.5-VL-7B, to achieve efficient and accurate field extraction from large-scale claims data. The proposed system achieves a document-type classification accuracy of over 95 percent and a field-level extraction accuracy of approximately 87 percent, while maintaining an average processing latency of under 2 seconds per document. Compared to manual processing, which typically requires around 10 minutes per claim, our system delivers a 300x improvement in efficiency. These results demonstrate that combining traditional machine learning models with modern VLMs enables production-grade accuracy and speed for real-world automation. The solution has been successfully deployed in our mobile application and is currently processing tens of thousands of claims weekly from Vietnam and Singapore.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u4ece\u533b\u7597\u7406\u8d54\u6587\u6863\u4e2d\u9ad8\u6548\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u5904\u7406\u901f\u5ea6\u6bd4\u4eba\u5de5\u5feb300\u500d\u3002", "motivation": "\u533b\u7597\u7406\u8d54\u6587\u6863\u901a\u5e38\u4ee5\u626b\u63cfPDF\u6216\u7167\u7247\u5f62\u5f0f\u5b58\u5728\uff0c\u5b58\u5728\u5185\u5bb9\u5f02\u8d28\u6027\uff08\u6253\u5370/\u624b\u5199\uff09\u3001\u8bed\u8a00\u591a\u6837\u6027\u3001\u56fe\u50cf\u8d28\u91cf\u4e0d\u4e00\u548c\u5e03\u5c40\u5dee\u5f02\u7b49\u95ee\u9898\uff0c\u7ed9\u81ea\u52a8\u5316\u89e3\u6790\u548c\u4fe1\u606f\u63d0\u53d6\u5e26\u6765\u5de8\u5927\u6311\u6218\u3002Fullerton Health\u6bcf\u5e74\u5904\u7406\u6570\u5343\u4e07\u4efd\u7406\u8d54\uff0c\u8de8\u8d8a\u4e5d\u4e2a\u5e02\u573a\uff0c\u4e9f\u9700\u9ad8\u6548\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a1) \u4f7f\u7528\u591a\u8bed\u8a00OCR\u5f15\u64cePaddleOCR\u8fdb\u884c\u6587\u672c\u8bc6\u522b\uff1b2) \u4f20\u7edf\u903b\u8f91\u56de\u5f52\u5206\u7c7b\u5668\u8fdb\u884c\u6587\u6863\u7c7b\u578b\u5206\u7c7b\uff1b3) \u7d27\u51d1\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578bQwen 2.5-VL-7B\u8fdb\u884c\u5b57\u6bb5\u63d0\u53d6\u3002\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e0e\u73b0\u4ee3VLM\u6280\u672f\u3002", "result": "\u6587\u6863\u7c7b\u578b\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u5b57\u6bb5\u7ea7\u63d0\u53d6\u51c6\u786e\u7387\u7ea687%\uff0c\u5e73\u5747\u5904\u7406\u5ef6\u8fdf\u4f4e\u4e8e2\u79d2/\u6587\u6863\u3002\u76f8\u6bd4\u4eba\u5de5\u5904\u7406\u6bcf\u4efd\u7406\u8d54\u7ea610\u5206\u949f\uff0c\u6548\u7387\u63d0\u5347300\u500d\u3002\u7cfb\u7edf\u5df2\u5728\u79fb\u52a8\u5e94\u7528\u4e2d\u90e8\u7f72\uff0c\u6bcf\u5468\u5904\u7406\u8d8a\u5357\u548c\u65b0\u52a0\u5761\u6570\u4e07\u4efd\u7406\u8d54\u3002", "conclusion": "\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u751f\u4ea7\u7ea7\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u7684\u81ea\u52a8\u5316\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u7cfb\u7edf\u5df2\u6210\u529f\u90e8\u7f72\u5e76\u5904\u7406\u5927\u89c4\u6a21\u7406\u8d54\u6570\u636e\u3002"}}
{"id": "2601.00869", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00869", "abs": "https://arxiv.org/abs/2601.00869", "authors": ["Huang Junyao", "Situ Ruimin", "Ye Renqin"], "title": "Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery", "comment": "19 pages, 5 tables. Dataset and code available at https://github.com/zhizibianjie-omniedge/geo-cultural-encoding", "summary": "As artificial intelligence systems increasingly mediate consumer information discovery,\n  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large\n  Language Models (LLMs) -- systematic differences in brand recommendations arising from\n  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,\n  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6\n  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,\n  p<.001). This disparity persists in identical English queries, indicating training data\n  geography -- not language -- drives the effect. We introduce the Existence Gap: brands\n  absent from LLM training corpora lack \"existence\" in AI responses regardless of quality.\n  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%\n  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how\n  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we\n  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic\n  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility\n  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization\n  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats\n  through semantic coverage, technical depth, and cultural localization. Our findings reveal\n  that in AI-mediated markets, the limits of a brand's \"Data Boundaries\" define the limits\n  of its \"Market Frontiers.\"", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u8bad\u7ec3\u6570\u636e\u7684\u5730\u7406\u5206\u5e03\u5bfc\u81f4\u54c1\u724c\u63a8\u8350\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u4e2d\u6587LLM\u7684\u54c1\u724c\u63d0\u53ca\u7387\u6bd4\u56fd\u9645LLM\u9ad830.6\u4e2a\u767e\u5206\u70b9\uff0c\u63ed\u793a\u4e86\"\u5b58\u5728\u9e3f\u6c9f\"\u73b0\u8c61\u548c\u8bed\u8a00\u8fb9\u754c\u969c\u788d\u5bf9\u5e02\u573a\u8fdb\u5165\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u4ecb\u5165\u6d88\u8d39\u8005\u4fe1\u606f\u53d1\u73b0\u8fc7\u7a0b\uff0c\u54c1\u724c\u9762\u4e34\u7b97\u6cd5\u4e0d\u53ef\u89c1\u6027\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u6784\u6210\u5dee\u5f02\u5bfc\u81f4\u7684\u54c1\u724c\u63a8\u8350\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "method": "\u5206\u6790\u4e861,909\u4e2a\u7eaf\u82f1\u6587\u67e5\u8be2\uff0c\u8986\u76d66\u4e2aLLM\uff08GPT-4o\u3001Claude\u3001Gemini\u3001Qwen3\u3001DeepSeek\u3001Doubao\uff09\u548c30\u4e2a\u54c1\u724c\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e2d\u6587LLM\u548c\u56fd\u9645LLM\u7684\u54c1\u724c\u63d0\u53ca\u7387\u5dee\u5f02\uff0c\u5e76\u5f15\u5165\u77e5\u7ec7\u8fb9\u754c\uff08Zhizibianjie/OmniEdge\uff09\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u4e2d\u6587LLM\u7684\u54c1\u724c\u63d0\u53ca\u7387\u6bd4\u56fd\u9645LLM\u9ad830.6\u4e2a\u767e\u5206\u70b9\uff0888.9% vs. 58.3%\uff0cp<.001\uff09\uff0c\u8fd9\u79cd\u5dee\u5f02\u5728\u76f8\u540c\u7684\u82f1\u6587\u67e5\u8be2\u4e2d\u6301\u7eed\u5b58\u5728\uff0c\u8868\u660e\u8bad\u7ec3\u6570\u636e\u7684\u5730\u7406\u5206\u5e03\u800c\u975e\u8bed\u8a00\u662f\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002\u77e5\u7ec7\u8fb9\u754c\u6848\u4f8b\u663e\u793a\u4e2d\u6587LLM\u63d0\u53ca\u7387\u4e3a65.6%\uff0c\u800c\u56fd\u9645\u6a21\u578b\u4e3a0%\uff08p<.001\uff09\u3002", "conclusion": "\u63d0\u51fa\u4e86\"\u6570\u636e\u62a4\u57ce\u6cb3\"\u6846\u67b6\uff0c\u5c06AI\u53ef\u89c1\u5185\u5bb9\u89c6\u4e3aVRIN\u6218\u7565\u8d44\u6e90\uff0c\u5e76\u5b9a\u4e49\u4e86\"\u7b97\u6cd5\u65e0\u5904\u4e0d\u5728\"\u4f5c\u4e3a\u751f\u6210\u5f15\u64ce\u4f18\u5316\u7684\u6218\u7565\u76ee\u6807\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5728AI\u4e2d\u4ecb\u7684\u5e02\u573a\u4e2d\uff0c\u54c1\u724c\u7684\"\u6570\u636e\u8fb9\u754c\"\u51b3\u5b9a\u4e86\u5176\"\u5e02\u573a\u8fb9\u754c\"\u3002"}}
{"id": "2601.01930", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01930", "abs": "https://arxiv.org/abs/2601.01930", "authors": ["Dongfang Zhao"], "title": "MCGI: Manifold-Consistent Graph Indexing for Billion-Scale Disk-Resident Vector Search", "comment": null, "summary": "Graph-based Approximate Nearest Neighbor (ANN) search often suffers from performance degradation in high-dimensional spaces due to the ``Euclidean-Geodesic mismatch,'' where greedy routing diverges from the underlying data manifold. To address this, we propose Manifold-Consistent Graph Indexing (MCGI), a geometry-aware and disk-resident indexing method that leverages Local Intrinsic Dimensionality (LID) to dynamically adapt search strategies to the data's intrinsic geometry. Unlike standard algorithms that treat dimensions uniformly, MCGI modulates its beam search budget based on in situ geometric analysis, eliminating dependency on static hyperparameters. Theoretical analysis confirms that MCGI enables improved approximation guarantees by preserving manifold-consistent topological connectivity. Empirically, MCGI achieves 5.8$\\times$ higher throughput at 95\\% recall on high-dimensional GIST1M compared to state-of-the-art DiskANN. On the billion-scale SIFT1B dataset, MCGI further validates its scalability by reducing high-recall query latency by 3$\\times$, while maintaining performance parity on standard lower-dimensional datasets.", "AI": {"tldr": "MCGI\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5f62\u4e00\u81f4\u6027\u7684\u56fe\u7d22\u5f15\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u672c\u5f81\u7ef4\u5ea6\u52a8\u6001\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff0c\u89e3\u51b3\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u56fe\u641c\u7d22\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\u7684\u67e5\u8be2\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56fe\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u5b58\u5728\"\u6b27\u51e0\u91cc\u5f97-\u6d4b\u5730\u7ebf\u4e0d\u5339\u914d\"\u95ee\u9898\uff0c\u8d2a\u5a6a\u8def\u7531\u4f1a\u504f\u79bb\u5e95\u5c42\u6570\u636e\u6d41\u5f62\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u6570\u636e\u5185\u5728\u51e0\u4f55\u7ed3\u6784\u7684\u7d22\u5f15\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6d41\u5f62\u4e00\u81f4\u56fe\u7d22\u5f15(MCGI)\uff0c\u8fd9\u662f\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u78c1\u76d8\u9a7b\u7559\u7d22\u5f15\u65b9\u6cd5\u3002\u5229\u7528\u5c40\u90e8\u672c\u5f81\u7ef4\u5ea6(LID)\u8fdb\u884c\u539f\u4f4d\u51e0\u4f55\u5206\u6790\uff0c\u52a8\u6001\u8c03\u6574\u6ce2\u675f\u641c\u7d22\u9884\u7b97\uff0c\u6d88\u9664\u5bf9\u9759\u6001\u8d85\u53c2\u6570\u7684\u4f9d\u8d56\uff0c\u4fdd\u6301\u6d41\u5f62\u4e00\u81f4\u7684\u62d3\u6251\u8fde\u63a5\u6027\u3002", "result": "\u5728\u9ad8\u7ef4GIST1M\u6570\u636e\u96c6\u4e0a\uff0cMCGI\u572895%\u53ec\u56de\u7387\u4e0b\u5b9e\u73b0\u4e865.8\u500d\u4e8eDiskANN\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002\u5728\u5341\u4ebf\u89c4\u6a21\u7684SIFT1B\u6570\u636e\u96c6\u4e0a\uff0c\u5c06\u9ad8\u53ec\u56de\u67e5\u8be2\u5ef6\u8fdf\u964d\u4f4e\u4e863\u500d\uff0c\u540c\u65f6\u5728\u6807\u51c6\u4f4e\u7ef4\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "MCGI\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u7d22\u5f15\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4ANN\u641c\u7d22\u4e2d\u7684\u6d41\u5f62\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7406\u8bba\u5206\u6790\u8bc1\u5b9e\u4e86\u5176\u6539\u8fdb\u7684\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u4fdd\u6301\u4f4e\u7ef4\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u573a\u666f\u4e0b\u7684\u641c\u7d22\u6548\u7387\u3002"}}
{"id": "2601.00880", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00880", "abs": "https://arxiv.org/abs/2601.00880", "authors": ["Anthony Mikinka"], "title": "Universal Conditional Logic: A Formal Language for Prompt Engineering", "comment": "25 pages, 15 figures, 5 tables. Includes appendices with variable reference, pattern library, and O_s calculation examples. Supplementary materials: V1-V4.1 prompt source code and 305 model responses available at GitHub repositories", "summary": "We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.", "AI": {"tldr": "UCL\u662f\u4e00\u4e2a\u5c06\u63d0\u793a\u5de5\u7a0b\u4ece\u542f\u53d1\u5f0f\u5b9e\u8df5\u8f6c\u53d8\u4e3a\u7cfb\u7edf\u4f18\u5316\u7684\u6570\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\u80fd\u663e\u8457\u51cf\u5c1129.8%\u7684token\u4f7f\u7528\u5e76\u964d\u4f4e\u6210\u672c\uff0c\u5176\u6027\u80fd\u5dee\u5f02\u53ef\u901a\u8fc7\u7ed3\u6784\u5f00\u9500\u51fd\u6570\u548c\u8fc7\u6307\u5b9a\u6096\u8bba\u6765\u89e3\u91ca\u3002", "motivation": "\u5f53\u524d\u63d0\u793a\u5de5\u7a0b\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u5b9e\u8df5\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u4f18\u5316\u6846\u67b6\u3002\u4f5c\u8005\u65e8\u5728\u5c06\u63d0\u793a\u5de5\u7a0b\u4ece\u7ecf\u9a8c\u6027\u5b9e\u8df5\u8f6c\u53d8\u4e3a\u57fa\u4e8e\u6570\u5b66\u6846\u67b6\u7684\u7cfb\u7edf\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8LLM\u4ea4\u4e92\u7684\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u63d0\u51fa\u901a\u7528\u6761\u4ef6\u903b\u8f91\uff08UCL\uff09\u6846\u67b6\uff0c\u5305\u542b\u6307\u793a\u51fd\u6570\u3001\u7ed3\u6784\u5f00\u9500\u51fd\u6570\u3001\u65e9\u671f\u7ed1\u5b9a\u7b49\u6838\u5fc3\u673a\u5236\u3002\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\uff08N=305\uff0c11\u4e2a\u6a21\u578b\uff0c4\u6b21\u8fed\u4ee3\uff09\u9a8c\u8bc1\u6846\u67b6\u6548\u679c\uff0c\u5e76\u5f15\u5165\u8fc7\u6307\u5b9a\u6096\u8bba\u89e3\u91ca\u6027\u80fd\u5dee\u5f02\u3002", "result": "UCL\u80fd\u663e\u8457\u51cf\u5c1129.8%\u7684token\u4f7f\u7528\uff08t(10)=6.36, p<0.001, Cohen's d=2.01\uff09\uff0c\u76f8\u5e94\u964d\u4f4e\u6210\u672c\u3002\u53d1\u73b0\u8fc7\u6307\u5b9a\u9608\u503cS*=0.509\uff0c\u8d85\u8fc7\u6b64\u9608\u503c\u540e\u989d\u5916\u6307\u5b9a\u4f1a\u4e8c\u6b21\u964d\u4f4e\u6027\u80fd\u3002\u6700\u4f18UCL\u914d\u7f6e\u56e0\u6a21\u578b\u67b6\u6784\u800c\u5f02\u3002", "conclusion": "UCL\u4e3a\u9ad8\u6548\u7684LLM\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6821\u51c6\u7684\u4f18\u5316\u6846\u67b6\uff0c\u6a21\u578b\u65cf\u7279\u5b9a\u7684\u4f18\u5316\u662f\u672a\u6765\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\u8be5\u6846\u67b6\u5c06\u63d0\u793a\u5de5\u7a0b\u4ece\u542f\u53d1\u5f0f\u5b9e\u8df5\u8f6c\u53d8\u4e3a\u7cfb\u7edf\u5316\u6570\u5b66\u4f18\u5316\u3002"}}
{"id": "2601.01997", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01997", "abs": "https://arxiv.org/abs/2601.01997", "authors": ["Dario Di Palma", "Giovanni Maria Biancofiore", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "title": "Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations", "comment": null, "summary": "ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization.\n  This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30ChatGPT-3.5\u548cChatGPT-4\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u548c\u6d41\u884c\u5ea6\u504f\u5dee\u8868\u73b0\uff0c\u53d1\u73b0\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u867d\u7136ChatGPT\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5bf9\u5176\u5728\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u548c\u6d41\u884c\u5ea6\u504f\u5dee\u7b49\u591a\u7ef4\u5ea6\u6027\u80fd\u7684\u5168\u9762\u5206\u6790\u4ecd\u7f3a\u4e4f\uff0c\u9700\u8981\u7406\u89e3\u8fd9\u4e9b\u65b9\u9762\u4ee5\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u5b9e\u73b0\u957f\u671f\u4e2a\u6027\u5316", "method": "\u8bc4\u4f30ChatGPT-3.5\u548cChatGPT-4\u5728\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u5728Top-N\u63a8\u8350\u548c\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u548c\u6d41\u884c\u5ea6\u504f\u5dee\u4e09\u4e2a\u7ef4\u5ea6", "result": "ChatGPT-4\u5339\u914d\u6216\u8d85\u8d8a\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u63a8\u8350\u4e2d\u5e73\u8861\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\uff1b\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\uff0cChatGPT\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u5408\u65b0\u7528\u6237", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86ChatGPT\u63a8\u8350\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u5728\u8d85\u8d8a\u51c6\u786e\u6027\u6307\u6807\u65b9\u9762\u7684\u63a8\u8350\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u7279\u522b\u662f\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
{"id": "2601.00928", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00928", "abs": "https://arxiv.org/abs/2601.00928", "authors": ["Luis Yoichi Morales", "Francesco Zanlungo", "David M. Woollard"], "title": "Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store", "comment": null, "summary": "Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u4ece3D\u8f68\u8ff9\u6570\u636e\u4e2d\u63d0\u53d6\u987e\u5ba2\"\u8d27\u67b6\u8bbf\u95ee\"\u884c\u4e3a\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u96f6\u552e\u5e97\u4e2d\u7684\u987e\u5ba2\u6d4f\u89c8\u610f\u56fe\uff0c\u5e76\u5728\u4e0d\u540c\u5546\u5e97\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u96f6\u552e\u4e1a\u5ba2\u6237\u670d\u52a1\u89d2\u8272\u4e2d\u7684\u90e8\u7f72\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u7406\u89e3\u987e\u5ba2\u8d2d\u7269\u610f\u56fe\u7684\u6280\u672f\uff0c\u7279\u522b\u662f\u5206\u6790\u987e\u5ba2\u5728\u5b9e\u4f53\u5e97\u4e2d\u7684\u6d4f\u89c8\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u57fa\u4e8e\u673a\u5668\u89c6\u89c9\u76843D\u8ddf\u8e2a\u548c\u5934\u9876\u6444\u50cf\u5934\u83b7\u53d6\u7684\u8f68\u8ff9\u6570\u636e\u4e2d\u8ba1\u7b97\u987e\u5ba2\"\u8d27\u67b6\u8bbf\u95ee\"\u7684\u7b97\u6cd5\u3002\u5728\u4e24\u4e2a\u4e0d\u540c\u5546\u5e97\u6536\u96c6\u4e868138\u6761\u548c15129\u6761\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u7531\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u6821\u51c6\uff0c\u5e76\u5728\u6821\u51c6\u96c6\u5916\u7684\u6570\u636e\u4e0a\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u5728\u4e0e\u6821\u51c6\u73af\u5883\u4e0d\u540c\u7684\u5546\u5e97\u4e2d\u6709\u6548\u8bc6\u522b\u987e\u5ba2\u7684\u6d4f\u89c8\u6d3b\u52a8\u3002\u7814\u7a76\u8fd8\u5229\u7528\u8be5\u6a21\u578b\u5206\u6790\u4e86\u5927\u91cf\u8f68\u8ff9\u4e2d\u7684\u987e\u5ba2\"\u6d4f\u89c8\u6a21\u5f0f\"\u53ca\u5176\u4e0e\u5b9e\u9645\u8d2d\u4e70\u884c\u4e3a\u7684\u5173\u7cfb\u3002", "conclusion": "\u8d27\u67b6\u6d4f\u89c8\u4fe1\u606f\u53ef\u7528\u4e8e\u96f6\u552e\u89c4\u5212\u548c\u4eba\u673a\u4ea4\u4e92\u573a\u666f\uff0c\u4e3a\u96f6\u552e\u4e1a\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u987e\u5ba2\u884c\u4e3a\u6d1e\u5bdf\uff0c\u652f\u6301\u673a\u5668\u4eba\u66f4\u597d\u5730\u7406\u89e3\u987e\u5ba2\u610f\u56fe\u3002"}}
{"id": "2601.00885", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00885", "abs": "https://arxiv.org/abs/2601.00885", "authors": ["Mandar Parab"], "title": "Counterfactual Self-Questioning for Stable Policy Optimization in Language Models", "comment": null, "summary": "Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.", "AI": {"tldr": "\u63d0\u51faCounterfactual Self-Questioning\u6846\u67b6\uff0c\u8ba9\u5355\u4e2a\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e76\u8bc4\u4f30\u81ea\u8eab\u63a8\u7406\u7684\u53cd\u4e8b\u5b9e\u6279\u8bc4\uff0c\u901a\u8fc7\u5185\u90e8\u751f\u6210\u7684\u76d1\u7763\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u81ea\u6211\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6279\u8bc4\u8005\u3001\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u6216\u96c6\u6210\u91c7\u6837\uff0c\u589e\u52a0\u4e86\u590d\u6742\u6027\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u66f4\u7b80\u5355\u6709\u6548\u7684\u81ea\u6211\u76d1\u7763\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u81ea\u6211\u63d0\u95ee\u6846\u67b6\uff1a1) \u751f\u6210\u521d\u59cb\u63a8\u7406\u8f68\u8ff9\uff1b2) \u9488\u5bf9\u6f5c\u5728\u5931\u8d25\u70b9\u63d0\u51fa\u9488\u5bf9\u6027\u95ee\u9898\uff1b3) \u751f\u6210\u66b4\u9732\u9519\u8bef\u5047\u8bbe\u6216\u65e0\u6548\u6b65\u9aa4\u7684\u66ff\u4ee3\u63a8\u7406\u8f68\u8ff9\uff1b4) \u4f7f\u7528\u8fd9\u4e9b\u53cd\u4e8b\u5b9e\u8f68\u8ff9\u4f5c\u4e3a\u7ed3\u6784\u5316\u76f8\u5bf9\u53cd\u9988\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u53cd\u4e8b\u5b9e\u81ea\u6211\u63d0\u95ee\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8f83\u5c0f\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u5185\u90e8\u751f\u6210\u7684\u76d1\u7763\u5c31\u80fd\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u81ea\u6211\u6539\u8fdb\u3002", "conclusion": "Counterfactual Self-Questioning\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\uff0c\u65e0\u9700\u5916\u90e8\u6a21\u578b\u6216\u590d\u6742\u67b6\u6784\uff0c\u901a\u8fc7\u5185\u90e8\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u6279\u8bc4\u5b9e\u73b0\u7a33\u5b9a\u7684\u81ea\u6211\u76d1\u7763\u5b66\u4e60\u3002"}}
{"id": "2601.02002", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02002", "abs": "https://arxiv.org/abs/2601.02002", "authors": ["Antonio Colacicco", "Vito Guida", "Dario Di Palma", "Fedelucio Narducci", "Tommaso Di Noia"], "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u81ea\u52a8\u68c0\u6d4b\u548c\u63d0\u53d6\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8bb0\u5fc6\u7684\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u8d8a\u72f1\u63d0\u793a\u5de5\u7a0b\u3001\u65e0\u76d1\u7763\u6f5c\u5728\u77e5\u8bc6\u53d1\u73b0\u548c\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u53d1\u73b0\u81ea\u52a8\u4f18\u5316\u63d0\u793a\u662f\u6700\u6709\u524d\u666f\u7684\u7b56\u7565\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u573a\u666f\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u8bad\u7ec3\u6570\u636e\u4e0d\u516c\u5f00\uff0c\u5b58\u5728\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002\u5148\u524d\u7814\u7a76\u8868\u660eMovieLens-1M\u6570\u636e\u96c6\u88abLLaMA\u548cOpenAI\u6a21\u578b\u8bb0\u5fc6\uff0c\u4f46\u63d0\u53d6\u8fd9\u4e9b\u8bb0\u5fc6\u6570\u636e\u76ee\u524d\u4ec5\u4f9d\u8d56\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u66f4\u5148\u8fdb\u7684\u65b9\u6cd5\u589e\u5f3a\u6570\u636e\u6cc4\u9732\u68c0\u6d4b\u548c\u63d0\u53d6\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a(1) \u8d8a\u72f1\u63d0\u793a\u5de5\u7a0b\uff1b(2) \u65e0\u76d1\u7763\u6f5c\u5728\u77e5\u8bc6\u53d1\u73b0\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e00\u81f4\u641c\u7d22(CCS)\u548c\u805a\u7c7b\u8303\u6570\u63a2\u6d4b\u5185\u90e8\u6fc0\u6d3b\uff1b(3) \u81ea\u52a8\u63d0\u793a\u5de5\u7a0b(APE)\uff0c\u5c06\u63d0\u793a\u53d1\u73b0\u6784\u5efa\u4e3a\u5143\u5b66\u4e60\u8fc7\u7a0b\uff0c\u8fed\u4ee3\u4f18\u5316\u5019\u9009\u6307\u4ee4\u3002\u5728LLaMA\u6a21\u578b\u4e0a\u4f7f\u7528MovieLens-1M\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a\u8d8a\u72f1\u63d0\u793a\u672a\u80fd\u6539\u5584\u8bb0\u5fc6\u9879\u76ee\u7684\u68c0\u7d22\u4e14\u7ed3\u679c\u4e0d\u4e00\u81f4\uff1bCCS\u80fd\u53ef\u9760\u533a\u5206\u771f\u5b9e\u4e0e\u4f2a\u9020\u7684\u7535\u5f71\u6807\u9898\uff0c\u4f46\u5bf9\u6570\u503c\u7528\u6237\u548c\u8bc4\u5206\u6570\u636e\u65e0\u6548\uff1bAPE\u5728\u9879\u76ee\u7ea7\u4fe1\u606f\u68c0\u7d22\u65b9\u9762\u53d6\u5f97\u4e2d\u7b49\u6210\u529f\uff0c\u4f46\u5728\u6062\u590d\u6570\u503c\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u81ea\u52a8\u4f18\u5316\u63d0\u793a\u662f\u63d0\u53d6\u8bb0\u5fc6\u6837\u672c\u6700\u6709\u524d\u666f\u7684\u7b56\u7565\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u5728\u6570\u503c\u6570\u636e\u63d0\u53d6\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3002"}}
{"id": "2601.00939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00939", "abs": "https://arxiv.org/abs/2601.00939", "authors": ["Feng Luo", "Hongbo Pan", "Xiang Yang", "Baoyu Jiang", "Fengqing Liu", "Tao Huang"], "title": "ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.", "AI": {"tldr": "ShadowGS\u662f\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u536b\u661f\u56fe\u50cf\u9634\u5f71\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u6e32\u67d3\u65b9\u7a0b\u548c\u5149\u7ebf\u884c\u8fdb\u6280\u672f\u89e3\u51b3\u591a\u65f6\u76f8\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u9634\u5f71\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u53473D\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u591a\u65f6\u76f8\u536b\u661f\u56fe\u50cf\u4e2d\uff0c\u7531\u4e8e\u5149\u7167\u6761\u4ef6\u53d8\u5316\u5bfc\u81f4\u7684\u9634\u5f71\u4e0d\u4e00\u81f4\u95ee\u9898\u4e25\u91cd\u5f71\u54cd3D\u91cd\u5efa\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u5efa\u6a21\u51e0\u4f55\u4e00\u81f4\u9634\u5f71\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u7ed3\u5408\u9065\u611f\u7269\u7406\u6e32\u67d3\u65b9\u7a0b\u548c\u9ad8\u6548\u5149\u7ebf\u884c\u8fdb\u6280\u672f\uff0c\u5f15\u5165\u9634\u5f71\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u9634\u5f71\u56fe\u5148\u9a8c\uff0c\u5b9e\u73b0\u9634\u5f71\u89e3\u8026\u548c\u51e0\u4f55\u7cbe\u786e\u5efa\u6a21\u3002", "result": "ShadowGS\u5728\u9634\u5f71\u89e3\u8026\u7cbe\u5ea6\u30013D\u91cd\u5efa\u7cbe\u5ea6\u548c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ec5\u9700\u51e0\u5206\u949f\u8bad\u7ec3\uff0c\u5728RGB\u3001\u5168\u8272\u878d\u5408\u548c\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u4e0b\u5747\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "ShadowGS\u901a\u8fc7\u7269\u7406\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u65f6\u76f8\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u9634\u5f71\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u91cd\u5efa\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2601.00923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00923", "abs": "https://arxiv.org/abs/2601.00923", "authors": ["Josef Ott"], "title": "Context Collapse: In-Context Learning and Model Collapse", "comment": "Master's thesis", "summary": "This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LLMs\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u578b\u5d29\u6e83\u73b0\u8c61\uff0c\u5728\u7ebf\u6027\u53d8\u6362\u5668\u4e2d\u5206\u6790\u4e86ICL\u7684\u76f8\u53d8\u7279\u6027\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u5d29\u6e83\u7684\u5fc5\u7136\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5d29\u6e83\u7684\u65b0\u6982\u5ff5\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u4e24\u4e2a\u5173\u952e\u73b0\u8c61\uff1a\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u548c\u6a21\u578b\u5d29\u6e83\uff0c\u65e8\u5728\u7406\u89e3ICL\u7684\u673a\u5236\u3001\u6a21\u578b\u5d29\u6e83\u7684\u6761\u4ef6\uff0c\u5e76\u63ed\u793a\u957f\u671f\u751f\u6210\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "1. \u5728\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u4e0a\u8bad\u7ec3\u5e26\u6743\u91cd\u7ed1\u5b9a\u7684\u7ebf\u6027\u53d8\u6362\u5668\u7814\u7a76ICL\uff1b2. \u5c06\u524d\u5411\u4f20\u64ad\u7b80\u5316\u4e3a\u9884\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\u5e76\u5206\u6790\u6700\u4f18\u9884\u6761\u4ef6\u5668\uff1b3. \u4f7f\u7528\u9785\u548c\u968f\u673a\u6e38\u8d70\u7406\u8bba\u5206\u6790\u7ebf\u6027\u56de\u5f52\u548c\u9ad8\u65af\u62df\u5408\u7684\u7b80\u5316\u8bbe\u7f6e\uff1b4. \u63d0\u51fa\u4e0a\u4e0b\u6587\u5d29\u6e83\u6982\u5ff5\u5206\u6790\u957f\u5e8f\u5217\u751f\u6210\u95ee\u9898\u3002", "result": "1. ICL\u5b58\u5728\u76f8\u53d8\uff1a\u8d85\u8fc7\u4e34\u754c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\uff0c\u89e3\u51fa\u73b0\u659c\u5bf9\u79f0\u5206\u91cf\uff1b2. \u6a21\u578b\u5d29\u6e83\u51e0\u4e4e\u5fc5\u7136\u53d1\u751f\uff0c\u9664\u975e\u6570\u636e\u5feb\u901f\u589e\u957f\u6216\u957f\u671f\u4fdd\u7559\uff1b3. \u53d1\u73b0\u4e86\u4e0a\u4e0b\u6587\u5d29\u6e83\u73b0\u8c61\uff0c\u7279\u522b\u662f\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u3002", "conclusion": "ICL\u548c\u6a21\u578b\u5d29\u6e83\u662fLLMs\u4e2d\u7684\u6838\u5fc3\u73b0\u8c61\uff0cICL\u7684\u76f8\u53d8\u7279\u6027\u4e0e\u9884\u6761\u4ef6\u5668\u76f8\u5173\uff0c\u6a21\u578b\u5d29\u6e83\u4e0d\u53ef\u907f\u514d\uff0c\u4e0a\u4e0b\u6587\u5d29\u6e83\u63ed\u793a\u4e86ICL\u52a8\u6001\u4e0e\u751f\u6210\u6a21\u578b\u957f\u671f\u7a33\u5b9a\u6027\u6311\u6218\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2601.02306", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2601.02306", "abs": "https://arxiv.org/abs/2601.02306", "authors": ["Shivam Verma", "Hannes Karlbom", "Yu Zhao", "Nick Topping", "Vivian Chen", "Kieran Stanley", "Bharath Rengarajan"], "title": "Cold-Starting Podcast Ads and Promotions with Multi-Task Learning on Spotify", "comment": "Accepted at WSDM 2026", "summary": "We present a unified multi-objective model for targeting both advertisements and promotions within the Spotify podcast ecosystem. Our approach addresses key challenges in personalization and cold-start initialization, particularly for new advertising objectives. By leveraging transfer learning from large-scale ad and content interactions within a multi-task learning (MTL) framework, a single joint model can be fine-tuned or directly applied to new or low-data targeting tasks, including in-app promotions. This multi-objective design jointly optimizes podcast outcomes such as streams, clicks, and follows for both ads and promotions using a shared representation over user, content, context, and creative features, effectively supporting diverse business goals while improving user experience. Online A/B tests show up to a 22% reduction in effective Cost-Per-Stream (eCPS), particularly for less-streamed podcasts, and an 18-24% increase in podcast stream rates. Offline experiments and ablations highlight the contribution of ancillary objectives and feature groups to cold-start performance. Our experience shows that a unified modeling strategy improves maintainability, cold-start performance, and coverage, while breaking down historically siloed targeting pipelines. We discuss practical trade-offs of such joint models in a real-world advertising system.", "AI": {"tldr": "Spotify\u63d0\u51fa\u7edf\u4e00\u591a\u76ee\u6807\u6a21\u578b\uff0c\u540c\u65f6\u4f18\u5316\u5e7f\u544a\u548c\u63a8\u5e7f\u6d3b\u52a8\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u89e3\u51b3\u4e2a\u6027\u5316\u4e0e\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u548c\u63d0\u5347\u6d41\u5a92\u4f53\u7387\u3002", "motivation": "\u89e3\u51b3\u64ad\u5ba2\u751f\u6001\u7cfb\u7edf\u4e2d\u5e7f\u544a\u548c\u63a8\u5e7f\u6d3b\u52a8\u7684\u4e2a\u6027\u5316\u4e0e\u51b7\u542f\u52a8\u521d\u59cb\u5316\u6311\u6218\uff0c\u7279\u522b\u662f\u9488\u5bf9\u65b0\u7684\u5e7f\u544a\u76ee\u6807\uff0c\u6253\u7834\u5386\u53f2\u4e0a\u5b64\u7acb\u7684\u5b9a\u5411\u7ba1\u9053\uff0c\u63d0\u9ad8\u7cfb\u7edf\u53ef\u7ef4\u62a4\u6027\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5927\u89c4\u6a21\u5e7f\u544a\u548c\u5185\u5bb9\u4ea4\u4e92\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u6784\u5efa\u5171\u4eab\u7528\u6237\u3001\u5185\u5bb9\u3001\u4e0a\u4e0b\u6587\u548c\u521b\u610f\u7279\u5f81\u7684\u7edf\u4e00\u8868\u793a\uff0c\u8054\u5408\u4f18\u5316\u5e7f\u544a\u548c\u63a8\u5e7f\u7684\u6d41\u5a92\u4f53\u3001\u70b9\u51fb\u548c\u5173\u6ce8\u7b49\u591a\u9879\u76ee\u6807\u3002", "result": "\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u6709\u6548\u6bcf\u6d41\u6210\u672c\u964d\u4f4e22%\uff08\u7279\u522b\u662f\u8f83\u5c11\u6d41\u5a92\u4f53\u7684\u64ad\u5ba2\uff09\uff0c\u64ad\u5ba2\u6d41\u5a92\u4f53\u7387\u63d0\u534718-24%\uff1b\u79bb\u7ebf\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8f85\u52a9\u76ee\u6807\u548c\u7279\u5f81\u7ec4\u5bf9\u51b7\u542f\u52a8\u6027\u80fd\u7684\u8d21\u732e\u3002", "conclusion": "\u7edf\u4e00\u5efa\u6a21\u7b56\u7565\u63d0\u9ad8\u4e86\u53ef\u7ef4\u62a4\u6027\u3001\u51b7\u542f\u52a8\u6027\u80fd\u548c\u8986\u76d6\u7387\uff0c\u6253\u7834\u4e86\u5b64\u7acb\u7684\u5b9a\u5411\u7ba1\u9053\uff0c\u8ba8\u8bba\u4e86\u5b9e\u9645\u5e7f\u544a\u7cfb\u7edf\u4e2d\u8054\u5408\u6a21\u578b\u7684\u5b9e\u7528\u6743\u8861\u3002"}}
{"id": "2601.00940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00940", "abs": "https://arxiv.org/abs/2601.00940", "authors": ["Jonas Li", "Michelle Li", "Luke Liu", "Heng Fan"], "title": "Learning to Segment Liquids in Real-world Images", "comment": "9 pages, 7 figures", "summary": "Different types of liquids such as water, wine and medicine appear in all aspects of daily life. However, limited attention has been given to the task, hindering the ability of robots to avoid or interact with liquids safely. The segmentation of liquids is difficult because liquids come in diverse appearances and shapes; moreover, they can be both transparent or reflective, taking on arbitrary objects and scenes from the background or surroundings. To take on this challenge, we construct a large-scale dataset of liquids named LQDS consisting of 5000 real-world images annotated into 14 distinct classes, and design a novel liquid detection model named LQDM, which leverages cross-attention between a dedicated boundary branch and the main segmentation branch to enhance segmentation predictions. Extensive experiments demonstrate the effectiveness of LQDM on the test set of LQDS, outperforming state-of-the-art methods and establishing a strong baseline for the semantic segmentation of liquids.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6db2\u4f53\u5206\u5272\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6LQDS\u548c\u65b0\u7684\u6db2\u4f53\u68c0\u6d4b\u6a21\u578bLQDM\uff0c\u901a\u8fc7\u8fb9\u754c\u5206\u652f\u4e0e\u5206\u5272\u5206\u652f\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6db2\u4f53\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u6db2\u4f53\uff08\u5982\u6c34\u3001\u9152\u3001\u836f\u7269\uff09\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u65e0\u5904\u4e0d\u5728\uff0c\u4f46\u673a\u5668\u4eba\u9886\u57df\u5bf9\u6db2\u4f53\u5206\u5272\u4efb\u52a1\u5173\u6ce8\u6709\u9650\uff0c\u8fd9\u963b\u788d\u4e86\u673a\u5668\u4eba\u5b89\u5168\u907f\u514d\u6216\u4e0e\u6db2\u4f53\u4ea4\u4e92\u7684\u80fd\u529b\u3002\u6db2\u4f53\u5206\u5272\u7684\u6311\u6218\u5728\u4e8e\u6db2\u4f53\u5916\u89c2\u548c\u5f62\u72b6\u591a\u6837\uff0c\u65e2\u53ef\u4ee5\u662f\u900f\u660e\u4e5f\u53ef\u4ee5\u662f\u53cd\u5c04\u6027\u7684\uff0c\u4f1a\u5448\u73b0\u80cc\u666f\u6216\u5468\u56f4\u73af\u5883\u7684\u4efb\u610f\u7269\u4f53\u548c\u573a\u666f\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b5000\u5f20\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u3001\u6807\u6ce8\u4e3a14\u4e2a\u4e0d\u540c\u7c7b\u522b\u7684\u5927\u89c4\u6a21\u6db2\u4f53\u6570\u636e\u96c6LQDS\uff1b\u8bbe\u8ba1\u4e86\u65b0\u9896\u7684\u6db2\u4f53\u68c0\u6d4b\u6a21\u578bLQDM\uff0c\u5229\u7528\u4e13\u95e8\u7684\u8fb9\u754c\u5206\u652f\u4e0e\u4e3b\u8981\u5206\u5272\u5206\u652f\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6765\u589e\u5f3a\u5206\u5272\u9884\u6d4b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eLQDM\u5728LQDS\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4e3a\u6db2\u4f53\u8bed\u4e49\u5206\u5272\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6db2\u4f53\u6570\u636e\u96c6\u548c\u63d0\u51fa\u521b\u65b0\u7684\u6db2\u4f53\u68c0\u6d4b\u6a21\u578b\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u4e0e\u6db2\u4f53\u4ea4\u4e92\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u6db2\u4f53\u5206\u5272\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2601.00943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00943", "abs": "https://arxiv.org/abs/2601.00943", "authors": ["Megha Mariam K. M", "Aditya Arun", "Zakaria Laskar", "C. V. Jawahar"], "title": "PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education", "comment": "Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026", "summary": "Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6a21\u578b\u5728\u7269\u7406\u6559\u80b2\u4e2d\u751f\u6210\u89e3\u91ca\u6027\u89c6\u9891\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u80fd\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u7684\u89c6\u9891\uff0c\u4f46\u6982\u5ff5\u51c6\u786e\u6027\u6709\u5f85\u63d0\u9ad8\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6a21\u578b\uff0c\u7279\u522b\u662f\u6587\u672c\u5230\u89c6\u9891\u7cfb\u7edf\uff0c\u4e3a\u79d1\u5b66\u6559\u80b2\u63d0\u4f9b\u4e86\u901a\u8fc7\u81ea\u52a8\u521b\u5efa\u5f15\u4eba\u5165\u80dc\u7684\u89c6\u89c9\u89e3\u91ca\u6765\u6539\u53d8\u6559\u5b66\u65b9\u5f0f\u7684\u6f5c\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u7269\u7406\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u89e3\u91ca\u6027\u89c6\u9891\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u7269\u7406\u6982\u5ff5\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u6559\u5b66\u70b9\uff0c\u6bcf\u4e2a\u6559\u5b66\u70b9\u90fd\u914d\u6709\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\uff0c\u7528\u4e8e\u8bc4\u4f30T2V\u6a21\u578b\u6839\u636e\u8fd9\u4e9b\u63d0\u793a\u751f\u6210\u51c6\u786e\u89c6\u9891\u7684\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6a21\u578b\u80fd\u751f\u6210\u89c6\u89c9\u8fde\u8d2f\u3001\u8fd0\u52a8\u5e73\u6ed1\u3001\u95ea\u70c1\u8f83\u5c11\u7684\u89c6\u9891\uff0c\u4f46\u6982\u5ff5\u51c6\u786e\u6027\u4e0d\u591f\u53ef\u9760\u3002\u5728\u529b\u5b66\u3001\u6d41\u4f53\u548c\u5149\u5b66\u9886\u57df\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u7535\u78c1\u5b66\u548c\u70ed\u529b\u5b66\u7b49\u9700\u8981\u63cf\u7ed8\u62bd\u8c61\u76f8\u4e92\u4f5c\u7528\u7684\u9886\u57df\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6559\u80b2\u89c6\u9891\u751f\u6210\u4e2d\u89c6\u89c9\u8d28\u91cf\u548c\u6982\u5ff5\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e0c\u671b\u8be5\u57fa\u51c6\u6d4b\u8bd5\u80fd\u5e2e\u52a9\u793e\u533a\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63a8\u52a8\u5f00\u53d1\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u51c6\u786e\u3001\u4e0e\u8bfe\u7a0b\u5bf9\u9f50\u7684\u7269\u7406\u5185\u5bb9\u7684T2V\u7cfb\u7edf\u3002"}}
{"id": "2601.01195", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01195", "abs": "https://arxiv.org/abs/2601.01195", "authors": ["Wuzhenghong Wen", "Chao Xue", "Su Pan", "Yuwei Sun", "Minlong Peng"], "title": "Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering", "comment": "11 pages, 2 figures", "summary": "Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.", "AI": {"tldr": "MRE\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u524d\u5411\u548c\u540e\u5411\u63a8\u7406\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u6811\u7ed3\u6784\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u591a\u8df3\u63a8\u7406\u7684\u5168\u5c40\u6700\u4f18\u8f68\u8ff9\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6bcf\u8df3\u63a8\u7406\u65f6\u4f1a\u68c0\u7d22\u5927\u91cf\u65f6\u95f4\u76f8\u4f3c\u4e14\u8bed\u4e49\u590d\u6742\u7684\u5173\u7cfb\u5b50\u56fe\uff0c\u589e\u52a0\u4e86\u6b21\u4f18\u51b3\u7b56\u548c\u9519\u8bef\u4f20\u64ad\u7684\u98ce\u9669\uff0c\u9700\u8981\u6539\u8fdb\u591a\u8df3\u63a8\u7406\u7684\u5168\u5c40\u4f18\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u591a\u8df3\u63a8\u7406\u589e\u5f3a\u6846\u67b6MRE\uff1a1\uff09\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u5f15\u5bfcLLM\u751f\u6210\u591a\u6837\u5316\u7684\u63a8\u7406\u8f68\u8ff9\uff1b2\uff09\u9009\u62e9\u6709\u6548\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u4f5c\u4e3a\u51b7\u542f\u52a8\u7b56\u7565\uff1b3\uff09\u5f15\u5165\u6811\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u91c7\u7528\u9012\u5f52\u6811\u7ed3\u6784\u63a2\u7d22\u5b66\u4e60\uff0c\u6bcf\u8df3\u63a2\u7d22\u4e0e\u524d\u8df3\u5efa\u7acb\u5f3a\u56e0\u679c\u4f9d\u8d56\uff0c\u8bc4\u4f30\u5219\u57fa\u4e8e\u540e\u7eed\u8df3\u7684\u591a\u8def\u5f84\u63a2\u7d22\u53cd\u9988\u3002", "result": "\u5728\u4e24\u4e2aTKGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMRE\u6a21\u578b\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u5904\u7406\u590d\u6742\u591a\u8df3\u67e5\u8be2\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u663e\u793a\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5bf9\u566a\u58f0\u65f6\u95f4\u6807\u6ce8\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MRE\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u524d\u5411\u548c\u540e\u5411\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u591a\u8df3\u63a8\u7406\u7684\u5168\u5c40\u4f18\u5316\u80fd\u529b\uff0c\u5728\u590d\u6742\u67e5\u8be2\u5904\u7406\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2601.01024", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01024", "abs": "https://arxiv.org/abs/2601.01024", "authors": ["Tien-Huy Nguyen", "Huu-Loc Tran", "Thanh Duc Ngo"], "title": "ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval", "comment": "Accepted at WACV Main Track 2026", "summary": "Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself", "AI": {"tldr": "\u672c\u6587\u63d0\u51faITSELF\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u9690\u5f0f\u5c40\u90e8\u5bf9\u9f50\u65b9\u6cd5\u89e3\u51b3\u6587\u672c\u4eba\u7269\u641c\u7d22\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u5373\u53ef\u5b66\u4e60\u56fe\u50cf\u4e0e\u6587\u672c\u95f4\u7684\u7cbe\u7ec6\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5c40\u90e8\u5bf9\u9f50\u89e3\u51b3\u6587\u672c\u4eba\u7269\u641c\u7d22\u95ee\u9898\uff0c\u4f46\u5bb9\u6613\u9677\u5165\u6377\u5f84\u5b66\u4e60\u548c\u865a\u5047\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u9519\u4f4d\u3002\u540c\u65f6\uff0c\u6ce8\u5165\u5148\u9a8c\u77e5\u8bc6\u53ef\u80fd\u626d\u66f2\u6a21\u6001\u5185\u7ed3\u6784\u3002\u7814\u7a76\u53d1\u73b0\u7f16\u7801\u5668\u6ce8\u610f\u529b\u4ece\u8bad\u7ec3\u65e9\u671f\u5c31\u80fd\u63d0\u4f9b\u7a7a\u95f4\u7cbe\u786e\u7684\u8bc1\u636e\uff0c\u8fd9\u542f\u53d1\u4e86\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u9690\u5f0f\u5c40\u90e8\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51faITSELF\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) GRAB\u5c06\u6a21\u578b\u81ea\u8eab\u6ce8\u610f\u529b\u8f6c\u6362\u4e3a\u9ad8\u663e\u8457\u6027\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5e93\uff0c\u5e76\u5728\u8be5\u5e93\u4e0a\u5e94\u7528\u5c40\u90e8\u76ee\u6807\uff1b2) MARS\u805a\u5408\u8de8\u5c42\u6ce8\u610f\u529b\u5e76\u8fdb\u884c\u591a\u6837\u6027\u611f\u77e5\u7684top-k\u9009\u62e9\uff1b3) ATS\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ece\u7c97\u5230\u7ec6\u8c03\u5ea6\u4fdd\u7559\u9884\u7b97\uff0c\u65e9\u671f\u4fdd\u7559\u4e0a\u4e0b\u6587\uff0c\u9010\u6b65\u805a\u7126\u5224\u522b\u6027\u7ec6\u8282\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684TBPS\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u589e\u52a0\u989d\u5916\u5148\u9a8c\u76d1\u7763\u60c5\u51b5\u4e0b\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ITSELF\u6846\u67b6\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u9690\u5f0f\u5c40\u90e8\u5bf9\u9f50\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u4eba\u7269\u641c\u7d22\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u5373\u53ef\u5b66\u4e60\u56fe\u50cf\u4e0e\u6587\u672c\u95f4\u7684\u7cbe\u7ec6\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.00963", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00963", "abs": "https://arxiv.org/abs/2601.00963", "authors": ["Bishwajit Saha", "Dmitry Krotov", "Mohammed J. Zaki", "Parikshit Ram"], "title": "Deep Clustering with Associative Memories", "comment": null, "summary": "Deep clustering - joint representation learning and latent space clustering - is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization task, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. In this work, we propose a novel loss function utilizing energy-based dynamics via Associative Memories to formulate a new deep clustering method, DCAM, which ties together the representation learning and clustering aspects more intricately in a single objective. Our experiments showcase the advantage of DCAM, producing improved clustering quality for various architecture choices (convolutional, residual or fully-connected) and data modalities (images or text).", "AI": {"tldr": "DCAM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u52a8\u529b\u5b66\u548c\u8054\u60f3\u8bb0\u5fc6\u7684\u65b0\u578b\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00\u76ee\u6807\u51fd\u6570\u5c06\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u66f4\u7d27\u5bc6\u5730\u7ed3\u5408\u5728\u4e00\u8d77\u3002", "motivation": "\u6df1\u5ea6\u805a\u7c7b\u4e2d\u8868\u793a\u5b66\u4e60\u662f\u53ef\u5fae\u5206\u7684\uff0c\u4f46\u805a\u7c7b\u672c\u8d28\u4e0a\u662f\u79bb\u6563\u4f18\u5316\u95ee\u9898\uff0c\u9700\u8981\u5404\u79cd\u8fd1\u4f3c\u548c\u6b63\u5219\u5316\u624d\u80fd\u878d\u5165\u6807\u51c6\u53ef\u5fae\u6d41\u7a0b\uff0c\u5bfc\u81f4\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002", "method": "\u63d0\u51faDCAM\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u4e8e\u80fd\u91cf\u52a8\u529b\u5b66\u7684\u8054\u60f3\u8bb0\u5fc6\u6784\u5efa\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u5355\u4e00\u76ee\u6807\u51fd\u6570\u66f4\u7d27\u5bc6\u5730\u7ed3\u5408\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u3002", "result": "DCAM\u5728\u4e0d\u540c\u67b6\u6784\u9009\u62e9\uff08\u5377\u79ef\u3001\u6b8b\u5dee\u6216\u5168\u8fde\u63a5\uff09\u548c\u6570\u636e\u6a21\u6001\uff08\u56fe\u50cf\u6216\u6587\u672c\uff09\u4e0a\u90fd\u5c55\u73b0\u51fa\u4f18\u52bf\uff0c\u4ea7\u751f\u4e86\u6539\u8fdb\u7684\u805a\u7c7b\u8d28\u91cf\u3002", "conclusion": "DCAM\u901a\u8fc7\u80fd\u91cf\u52a8\u529b\u5b66\u548c\u8054\u60f3\u8bb0\u5fc6\u5b9e\u73b0\u4e86\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u7684\u66f4\u7d27\u5bc6\u96c6\u6210\uff0c\u4e3a\u6df1\u5ea6\u805a\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.00964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00964", "abs": "https://arxiv.org/abs/2601.00964", "authors": ["Md. Maksudul Haque", "Rahnuma Akter", "A S M Ahsanul Sarkar Akib", "Abdul Hasib"], "title": "A Deep Learning Approach for Automated Skin Lesion Diagnosis with Explainable AI", "comment": null, "summary": "Skin cancer is also one of the most common and dangerous types of cancer in the world that requires timely and precise diagnosis. In this paper, a deep-learning architecture of the multi-class skin lesion classification on the HAM10000 dataset will be described. The system suggested combines high-quality data balancing methods, large-scale data augmentation, hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach. Moreover, we also use explainable AI (XAI) techniques such as Grad-CAM and saliency maps to come up with intelligible visual representations of model predictions. Our strategy is with a total accuracy of 91.15 per cent, macro F1 of 85.45\\% and micro-average AUC of 99.33\\%. The model has shown high performance in all the seven lesion classes with specific high performance of melanoma and melanocytic nevi. In addition to enhancing diagnostic transparency, XAI also helps to find out the visual characteristics that cause the classifications, which enhances clinical trustworthiness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u5e73\u8861\u3001\u6570\u636e\u589e\u5f3a\u3001EfficientNetV2-L\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u548c\u4e09\u9636\u6bb5\u6e10\u8fdb\u5b66\u4e60\u65b9\u6cd5\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8eHAM10000\u6570\u636e\u96c6\u4e0a\u7684\u591a\u7c7b\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\uff0c\u8fbe\u523091.15%\u51c6\u786e\u7387\uff0c\u5e76\u4f7f\u7528XAI\u6280\u672f\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u76ae\u80a4\u764c\u662f\u5168\u7403\u6700\u5e38\u89c1\u4e14\u5371\u9669\u7684\u764c\u75c7\u7c7b\u578b\u4e4b\u4e00\uff0c\u9700\u8981\u53ca\u65f6\u7cbe\u786e\u7684\u8bca\u65ad\u3002\u73b0\u6709\u8bca\u65ad\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6027\u80fd\u4e14\u53ef\u4fe1\u8d56\u7684\u81ea\u52a8\u8bca\u65ad\u7cfb\u7edf\u3002", "method": "1. \u9ad8\u8d28\u91cf\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b2. \u5927\u89c4\u6a21\u6570\u636e\u589e\u5f3a\u6280\u672f\uff1b3. \u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u7684\u6df7\u5408EfficientNetV2-L\u6846\u67b6\uff1b4. \u4e09\u9636\u6bb5\u6e10\u8fdb\u5b66\u4e60\u7b56\u7565\uff1b5. \u4f7f\u7528Grad-CAM\u548c\u663e\u8457\u6027\u56fe\u7b49XAI\u6280\u672f\u63d0\u4f9b\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "result": "\u6a21\u578b\u5728HAM10000\u6570\u636e\u96c6\u4e0a\u53d6\u5f9791.15%\u7684\u603b\u51c6\u786e\u7387\u300185.45%\u7684\u5b8fF1\u5206\u6570\u548c99.33%\u7684\u5fae\u5e73\u5747AUC\u3002\u5728\u6240\u6709\u4e03\u4e2a\u75c5\u53d8\u7c7b\u522b\u4e2d\u90fd\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ed1\u8272\u7d20\u7624\u548c\u9ed1\u8272\u7d20\u7ec6\u80de\u75e3\u5206\u7c7b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7XAI\u6280\u672f\u589e\u5f3a\u4e86\u8bca\u65ad\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u76ae\u80a4\u764c\u7684\u81ea\u52a8\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00883", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00883", "abs": "https://arxiv.org/abs/2601.00883", "authors": ["Zhongyang Shen"], "title": "Outlier Detection Using Vector Cosine Similarity by Adding a Dimension", "comment": "This is an updated version of the paper originally published in ICAIIC 2024 (DOI: 10.1109/ICAIIC60209.2024.10463442). Changes include minor typographical and grammatical corrections, as well as an added description of an optimized open-source Python implementation (MDOD) available on PyPI at https://pypi.org/project/mdod/", "summary": "We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5411\u91cf\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u591a\u7ef4\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u539f\u6570\u636e\u6dfb\u52a0\u96f6\u503c\u7ef4\u5ea6\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u5229\u7528\u89c2\u6d4b\u70b9\u4e0e\u6d4b\u91cf\u70b9\u7684\u5411\u91cf\u76f8\u4f3c\u5ea6\u6bd4\u8f83\u6765\u8bc6\u522b\u5f02\u5e38\u503c", "motivation": "\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u591a\u7ef4\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u9ad8\u7ef4\u6570\u636e\u5206\u5e03\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u6548\u679c\u4e0d\u4f73", "method": "\u5728\u539f\u6570\u636e\u6dfb\u52a0\u4e00\u4e2a\u96f6\u503c\u7ef4\u5ea6\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u9009\u62e9\u6d4b\u91cf\u70b9\u540e\u521b\u5efa\u89c2\u6d4b\u70b9\uff08\u539f\u70b9\uff09\uff0c\u89c2\u6d4b\u70b9\u4e0e\u6d4b\u91cf\u70b9\u4ec5\u5728\u65b0\u7ef4\u5ea6\u6709\u975e\u96f6\u503c\u5dee\u5f02\uff0c\u7136\u540e\u8ba1\u7b97\u4ece\u89c2\u6d4b\u70b9\u5230\u6d4b\u91cf\u70b9\u53ca\u5176\u4ed6\u70b9\u7684\u5411\u91cf\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fdb\u884c\u6bd4\u8f83", "result": "\u5f00\u53d1\u4e86\u4f18\u5316\u5b9e\u73b0MDOD\u5e76\u5df2\u5728PyPI\u53d1\u5e03\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u591a\u7ef4\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u503c", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5411\u91cf\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3a\u591a\u7ef4\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u5de5\u5177\uff0cMDOD\u7684\u5b9e\u73b0\u4fbf\u4e8e\u5b9e\u9645\u5e94\u7528"}}
{"id": "2601.00988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00988", "abs": "https://arxiv.org/abs/2601.00988", "authors": ["Lin Xi", "Yingliang Ma", "Xiahai Zhuang"], "title": "Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss", "comment": null, "summary": "We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684FSVOS\u6a21\u578b\uff0c\u91c7\u7528\u5c40\u90e8\u5339\u914d\u7b56\u7565\u9650\u5236\u641c\u7d22\u7a7a\u95f4\uff0c\u901a\u8fc7\u65b9\u5411\u91c7\u6837\u5b9e\u73b0\u52a8\u6001\u53ef\u53d8\u91c7\u6837\u533a\u57df\uff0c\u7ed3\u5408\u65f6\u7a7a\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u5e76\u5728X\u5c04\u7ebf\u8840\u7ba1\u9020\u5f71\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u6807\u51c6im2col\u5b9e\u73b0\uff08\u7a7a\u95f4\u5377\u79ef\u3001\u6df1\u5ea6\u5377\u79ef\u3001\u7279\u5f81\u79fb\u4f4d\u673a\u5236\uff09\u6548\u7387\u4f4e\u4e0b\uff0c\u6216\u4f9d\u8d56\u786c\u4ef6\u7279\u5b9a\u7684CUDA\u5185\u6838\uff08\u53ef\u53d8\u5f62\u548c\u90bb\u57df\u6ce8\u610f\u529b\uff09\u5728\u975eCUDA\u8bbe\u5907\u4e0a\u53ef\u79fb\u690d\u6027\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u53ef\u79fb\u690d\u7684\u89c6\u9891\u5206\u5272\u65b9\u6cd5\u3002", "method": "1. \u91c7\u7528\u5c40\u90e8\u5339\u914d\u7b56\u7565\u9650\u5236\u641c\u7d22\u7a7a\u95f4\u5230\u6700\u76f8\u5173\u7684\u76f8\u90bb\u50cf\u7d20\uff1b2. \u901a\u8fc7\u65b9\u5411\u91c7\u6837\u89c6\u89d2\u91cd\u65b0\u7ec4\u7ec7\u5c40\u90e8\u91c7\u6837\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u975e\u53c2\u6570\u91c7\u6837\u673a\u5236\uff0c\u652f\u6301\u52a8\u6001\u53ef\u53d8\u91c7\u6837\u533a\u57df\uff1b3. \u8bbe\u8ba1\u76d1\u7763\u5f0f\u65f6\u7a7a\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848\uff0c\u589e\u5f3a\u8de8\u5e27\u7279\u5f81\u4e00\u81f4\u6027\uff1b4. \u521b\u5efaMOSXAV\u6570\u636e\u96c6\u7528\u4e8eX\u5c04\u7ebf\u8840\u7ba1\u9020\u5f71\u89c6\u9891\u7684\u591a\u76ee\u6807\u5206\u5272\u3002", "result": "\u5728CADICA\u3001XACV\u548cMOSXAV\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684FSVOS\u65b9\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff08\u5305\u62ec\u5df2\u89c1\u548c\u672a\u89c1\u7c7b\u522b\uff09\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u9891\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u7075\u6d3b\u6027\uff0c\u65e0\u9700\u53c2\u6570\u5c42\u7684\u8ba1\u7b97\u6210\u672c\u548c\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728X\u5c04\u7ebf\u8840\u7ba1\u9020\u5f71\u89c6\u9891\u5206\u6790\u9886\u57df\u3002"}}
{"id": "2601.01330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01330", "abs": "https://arxiv.org/abs/2601.01330", "authors": ["Shengji Tang", "Weihao Lin", "Jingqi Ye", "Hao Li", "Bo Zhang", "Shuyue Hu", "Tao Chen", "Wangli Ouyang", "Lei Bai", "Peng Ye"], "title": "Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale", "comment": "12 pages", "summary": "Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).", "AI": {"tldr": "JiSi\u6846\u67b6\u901a\u8fc7\u67e5\u8be2-\u54cd\u5e94\u6df7\u5408\u8def\u7531\u3001\u652f\u6301\u96c6\u805a\u5408\u5668\u9009\u62e9\u548c\u81ea\u9002\u5e94\u8def\u7531-\u805a\u5408\u5207\u6362\u4e09\u5927\u521b\u65b0\uff0c\u4f7f10\u4e2a\u5f00\u6e90LLM\u534f\u4f5c\u4ee547%\u6210\u672c\u8d85\u8d8aGemini-3-Pro\uff0c\u8bc1\u660e\u96c6\u4f53\u667a\u80fd\u662f\u901a\u5411AGI\u7684\u65b0\u8def\u5f84\u3002", "motivation": "\u5f53\u524dLLM\u8def\u7531\u548c\u805a\u5408\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u74f6\u9888\uff1a1) \u57fa\u4e8e\u67e5\u8be2\u7684\u8def\u7531\u5668\u4ec5\u5173\u6ce8\u6587\u672c\u76f8\u4f3c\u6027\uff1b2) \u805a\u5408\u65b9\u6cd5\u9759\u6001\uff0c\u65e0\u6cd5\u4e3a\u4e0d\u540c\u4efb\u52a1\u9009\u62e9\u5408\u9002\u805a\u5408\u5668\uff1b3) \u8def\u7531\u4e0e\u805a\u5408\u7684\u4e92\u8865\u6027\u672a\u5145\u5206\u5229\u7528\u3002\u9700\u8981\u91ca\u653eLLM\u534f\u4f5c\u7684\u5b8c\u6574\u6f5c\u529b\u3002", "method": "\u63d0\u51faJiSi\u6846\u67b6\uff0c\u5305\u542b\u4e09\u5927\u521b\u65b0\uff1a1) \u67e5\u8be2-\u54cd\u5e94\u6df7\u5408\u8def\u7531\uff0c\u540c\u65f6\u6355\u6349\u8bed\u4e49\u4fe1\u606f\u548c\u95ee\u9898\u96be\u5ea6\uff1b2) \u57fa\u4e8e\u652f\u6301\u96c6\u7684\u805a\u5408\u5668\u9009\u62e9\uff0c\u8054\u5408\u8bc4\u4f30\u805a\u5408\u80fd\u529b\u548c\u9886\u57df\u80fd\u529b\uff1b3) \u81ea\u9002\u5e94\u8def\u7531-\u805a\u5408\u5207\u6362\uff0c\u52a8\u6001\u5229\u7528\u8def\u7531\u548c\u805a\u5408\u7684\u4f18\u52bf\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJiSi\u901a\u8fc7\u534f\u8c0310\u4e2a\u5f00\u6e90LLM\uff0c\u4ee5\u4ec547%\u7684\u6210\u672c\u8d85\u8d8a\u4e86Gemini-3-Pro\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u96c6\u4f53\u667a\u80fd\u4ee3\u8868\u4e86\u901a\u5411\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u7684\u65b0\u8def\u5f84\uff0c\u901a\u8fc7\u6709\u6548\u534f\u8c03\u591a\u4e2aLLM\u7684\u534f\u4f5c\u53ef\u4ee5\u8d85\u8d8a\u5355\u4e00\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002"}}
{"id": "2601.00889", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00889", "abs": "https://arxiv.org/abs/2601.00889", "authors": ["Nalin Dhiman"], "title": "FANoS: Friction-Adaptive Nos\u00e9--Hoover Symplectic Momentum for Stiff Objectives", "comment": "13 pages, 5 figures, 4 tables", "summary": "We study a physics-inspired optimizer, \\emph{FANoS} (Friction-Adaptive Nos\u00e9--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nos\u00e9--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.\n  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\\times 10^{-3}$, and L-BFGS reaches $\\approx 4.4\\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.\n  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.", "AI": {"tldr": "FANoS\u662f\u4e00\u79cd\u53d7\u7269\u7406\u542f\u53d1\u7684\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u4e86\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u3001Nos\u00e9-Hoover\u6052\u6e29\u5668\u548c\u8f9b\u79ef\u5206\u5668\uff0c\u5728Rosenbrock-100D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eAdamW\u548cSGD+momentum\uff0c\u4f46\u5728\u5176\u4ed6\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "motivation": "\u5c06\u5206\u5b50\u52a8\u529b\u5b66\u4e2d\u7684\u7ed3\u6784\u4fdd\u6301\u79ef\u5206\u548c\u6052\u6e29\u5668\u601d\u60f3\u5e94\u7528\u4e8e\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u79cd\u7269\u7406\u542f\u53d1\u7684\u4f18\u5316\u5668\uff0c\u65e8\u5728\u5904\u7406\u4e00\u4e9b\u975e\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u56f0\u96be\u533a\u57df\u3002", "method": "FANoS\u7ed3\u5408\u4e86\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u4f5c\u4e3a\u79bb\u6563\u5316\u4e8c\u9636\u52a8\u529b\u7cfb\u7edf\u7684\u52a8\u91cf\u66f4\u65b0\uff1b(2) \u4f7f\u7528\u52a8\u80fd\u53cd\u9988\u81ea\u9002\u5e94\u8c03\u6574\u6469\u64e6\u7cfb\u6570\u7684Nos\u00e9-Hoover\u7c7b\u6052\u6e29\u5668\u53d8\u91cf\uff1b(3) \u534a\u9690\u5f0f\uff08\u8f9b-\u6b27\u62c9\uff09\u79ef\u5206\u5668\uff0c\u53ef\u9009\u914d\u5bf9\u89d2RMS\u9884\u5904\u7406\u5668\u3002", "result": "\u5728Rosenbrock-100D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFANoS-RMS\u8fbe\u5230\u5e73\u5747\u6700\u7ec8\u76ee\u6807\u503c1.74\u00d710\u207b\u00b2\uff0c\u663e\u8457\u4f18\u4e8e\u672a\u88c1\u526a\u7684AdamW\uff0848.50\uff09\u548cSGD+momentum\uff0890.76\uff09\u3002\u4f46\u5728\u75c5\u6001\u51f8\u4e8c\u6b21\u95ee\u9898\u548c\u5c0f\u578bPINN\u9884\u70ed\u6d4b\u8bd5\u4e2d\uff0cFANoS\u8868\u73b0\u4e0d\u5982AdamW\u4e14\u4e0d\u7a33\u5b9a\u3002", "conclusion": "FANoS\u662f\u73b0\u6709\u601d\u60f3\u7684\u53ef\u89e3\u91ca\u6027\u7efc\u5408\uff0c\u5728\u67d0\u4e9b\u975e\u51f8\u5c71\u8c37\u95ee\u9898\u4e0a\u53ef\u80fd\u6709\u5e2e\u52a9\uff0c\u4f46\u4e0d\u662f\u73b0\u4ee3\u57fa\u7ebf\u7684\u901a\u7528\u66ff\u4ee3\u54c1\uff0c\u5176\u884c\u4e3a\u5bf9\u6e29\u5ea6\u8c03\u5ea6\u548c\u8d85\u53c2\u6570\u9009\u62e9\u654f\u611f\u3002"}}
{"id": "2601.00991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00991", "abs": "https://arxiv.org/abs/2601.00991", "authors": ["Joshua Kawaguchi", "Saad Manzur", "Emily Gao Wang", "Maitreyi Sinha", "Bryan Vela", "Yunxi Wang", "Brandon Vela", "Wayne B. Hayes"], "title": "UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data", "comment": "CVPR 2026 submission. Introduces UnrealPose-1M dataset and UnrealPose-Gen pipeline", "summary": "Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted \"coherent\" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.", "AI": {"tldr": "UnrealPose-Gen\u662f\u4e00\u4e2a\u57fa\u4e8eUnreal Engine 5\u7684\u79bb\u7ebf\u6e32\u67d3\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u4eba\u4f53\u59ff\u6001\u6570\u636e\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b\u7ea6100\u4e07\u5e27\u7684UnrealPose-1M\u6570\u636e\u96c6\u3002", "motivation": "\u771f\u5b9e3D\u4eba\u4f53\u59ff\u6001\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u53d7\u9650\u4e8e\u5de5\u4f5c\u5ba4\u73af\u5883\uff0c\u800c\u91ce\u5916\u6570\u636e\u96c6\u7f3a\u4e4f\u51c6\u786e\u7684\u5730\u9762\u771f\u503c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u4f7f\u7528Unreal Engine 5\u6784\u5efa\u7684UnrealPose-Gen\u6d41\u6c34\u7ebf\uff0c\u57fa\u4e8eMovie Render Queue\u8fdb\u884c\u9ad8\u8d28\u91cf\u79bb\u7ebf\u6e32\u67d3\u3002\u751f\u6210\u7684\u6570\u636e\u5305\u62ec\uff1a3D\u5173\u8282\u5750\u6807\uff08\u4e16\u754c\u548c\u76f8\u673a\u5750\u6807\u7cfb\uff09\u30012D\u6295\u5f71\u548cCOCO\u98ce\u683c\u5173\u952e\u70b9\uff08\u542b\u906e\u6321\u548c\u53ef\u89c1\u6027\u6807\u8bb0\uff09\u3001\u4eba\u7269\u8fb9\u754c\u6846\u3001\u76f8\u673a\u5185\u5916\u53c2\u6570\u3002", "result": "\u521b\u5efa\u4e86UnrealPose-1M\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea6100\u4e07\u5e27\uff0c\u5206\u4e3a8\u4e2a\u5e8f\u5217\uff1a5\u4e2a\u811a\u672c\u5316\u7684\"\u8fde\u8d2f\"\u5e8f\u5217\uff085\u4e2a\u573a\u666f\u3001\u7ea640\u4e2a\u52a8\u4f5c\u30015\u4e2a\u4e3b\u4f53\uff09\u548c3\u4e2a\u968f\u673a\u5316\u5e8f\u5217\uff083\u4e2a\u573a\u666f\u3001\u7ea6100\u4e2a\u52a8\u4f5c\u30015\u4e2a\u4e3b\u4f53\uff09\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u89c6\u89d2\u8986\u76d6\u3002\u901a\u8fc7\u56db\u4e2a\u4efb\u52a1\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\uff1a\u56fe\u50cf\u52303D\u59ff\u6001\u30012D\u5173\u952e\u70b9\u68c0\u6d4b\u30012D\u52303D\u63d0\u5347\u3001\u4eba\u7269\u68c0\u6d4b/\u5206\u5272\u3002", "conclusion": "\u867d\u7136\u65f6\u95f4\u548c\u8d44\u6e90\u9650\u5236\u4e86\u65e0\u9650\u6570\u636e\u96c6\u7684\u751f\u6210\uff0c\u4f46\u4f5c\u8005\u53d1\u5e03\u4e86UnrealPose-1M\u6570\u636e\u96c6\u548cUnrealPose-Gen\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u7b2c\u4e09\u65b9\u751f\u6210\u4eba\u4f53\u59ff\u6001\u6570\u636e\uff0c\u4e3a\u89e3\u51b33D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u3002"}}
{"id": "2601.01363", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01363", "abs": "https://arxiv.org/abs/2601.01363", "authors": ["Xiaomeng Yang", "Zhiyu Tan", "Xiaohui Zhong", "Mengping Yang", "Qiusheng Huang", "Lei Chen", "Libo Wu", "Hao Li"], "title": "A unified multimodal understanding and generation model for cross-disciplinary scientific research", "comment": null, "summary": "Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25\u00b0 resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.", "AI": {"tldr": "FuXi-Uni\u662f\u4e00\u4e2a\u539f\u751f\u7edf\u4e00\u7684\u591a\u6a21\u6001\u79d1\u5b66\u6a21\u578b\uff0c\u80fd\u591f\u5728\u5355\u4e00\u67b6\u6784\u5185\u8de8\u79d1\u5b66\u9886\u57df\u8fdb\u884c\u79d1\u5b66\u7406\u89e3\u548c\u9ad8\u8d28\u91cf\u751f\u6210\uff0c\u7279\u522b\u9488\u5bf9\u5730\u7403\u79d1\u5b66\u548c\u751f\u7269\u533b\u5b66\u9886\u57df\u3002", "motivation": "\u79d1\u5b66\u53d1\u73b0\u65e5\u76ca\u4f9d\u8d56\u4e8e\u8de8\u5b66\u79d1\u6574\u5408\u5f02\u6784\u9ad8\u7ef4\u6570\u636e\uff0c\u4f46\u73b0\u6709AI\u6a21\u578b\u901a\u5e38\u662f\u9886\u57df\u7279\u5b9a\u7684\uff0c\u7f3a\u4e4f\u540c\u65f6\u7406\u89e3\u548c\u751f\u6210\u591a\u6a21\u6001\u79d1\u5b66\u6570\u636e\u7684\u80fd\u529b\uff0c\u800c\u8bb8\u591a\u5168\u7403\u6311\u6218\u548c\u79d1\u5b66\u95ee\u9898\u672c\u8d28\u4e0a\u662f\u8de8\u5b66\u79d1\u7684\uff0c\u9700\u8981\u591a\u4e2a\u9886\u57df\u7684\u534f\u540c\u8fdb\u5c55\u3002", "method": "FuXi-Uni\u5c06\u8de8\u5b66\u79d1\u79d1\u5b66\u6807\u8bb0\u4e0e\u81ea\u7136\u8bed\u8a00\u6807\u8bb0\u5bf9\u9f50\uff0c\u5e76\u4f7f\u7528\u79d1\u5b66\u89e3\u7801\u5668\u91cd\u5efa\u79d1\u5b66\u6807\u8bb0\uff0c\u4ece\u800c\u540c\u65f6\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u548c\u79d1\u5b66\u6570\u503c\u9884\u6d4b\u3002\u6a21\u578b\u5728\u539f\u751f\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7edf\u4e00\u5f02\u6784\u79d1\u5b66\u6a21\u6001\u3002", "result": "\u5728\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u4e2d\uff1a1\uff09\u751f\u621010\u5929\u5168\u74030.25\u00b0\u5206\u8fa8\u7387\u9884\u62a5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7269\u7406\u9884\u62a5\u7cfb\u7edf\uff1b2\uff09\u70ed\u5e26\u6c14\u65cb\u8def\u5f84\u548c\u5f3a\u5ea6\u9884\u6d4b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7269\u7406\u6a21\u578b\uff1b3\uff09\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u533a\u57df\u5929\u6c14\u573a\u8d85\u8d8a\u6807\u51c6\u63d2\u503c\u57fa\u7ebf\u3002\u5728\u751f\u7269\u533b\u5b66\u4e2d\uff1a\u5728\u591a\u4e2a\u751f\u7269\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u4f18\u4e8e\u9886\u5148\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5728\u539f\u751f\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7edf\u4e00\u5f02\u6784\u79d1\u5b66\u6a21\u6001\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u9886\u57df\u7279\u5b9a\u6027\u80fd\uff0cFuXi-Uni\u5411\u66f4\u901a\u7528\u7684\u591a\u6a21\u6001\u79d1\u5b66\u6a21\u578b\u8fc8\u8fdb\u4e86\u4e00\u6b65\u3002"}}
{"id": "2601.00892", "categories": ["cs.LG", "cs.CV", "physics.data-an", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00892", "abs": "https://arxiv.org/abs/2601.00892", "authors": ["Ana Carpio", "Gema Duro"], "title": "Hierarchical topological clustering", "comment": "not peer reviewed, reviewed version to appear in Soft Computing", "summary": "Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c42\u6b21\u5316\u62d3\u6251\u805a\u7c7b\u7b97\u6cd5\uff0c\u53ef\u5904\u7406\u4efb\u610f\u5f62\u72b6\u7684\u805a\u7c7b\u548c\u79bb\u7fa4\u70b9\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u3001\u533b\u7597\u548c\u7ecf\u6d4e\u6570\u636e", "motivation": "\u62d3\u6251\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u5047\u8bbe\u6570\u636e\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u63a2\u7d22\u6570\u636e\u4e91\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4efb\u610f\u5f62\u72b6\u805a\u7c7b\u548c\u79bb\u7fa4\u70b9\u7684\u805a\u7c7b\u7b97\u6cd5", "method": "\u63d0\u51fa\u5c42\u6b21\u5316\u62d3\u6251\u805a\u7c7b\u7b97\u6cd5\uff0c\u53ef\u4f7f\u7528\u4efb\u610f\u8ddd\u79bb\u5ea6\u91cf\uff0c\u901a\u8fc7\u5c42\u6b21\u7ed3\u6784\u63a8\u65ad\u79bb\u7fa4\u70b9\u548c\u4efb\u610f\u5f62\u72b6\u805a\u7c7b\u7684\u6301\u7eed\u6027", "result": "\u5728\u5305\u542b\u56fe\u50cf\u3001\u533b\u7597\u548c\u7ecf\u6d4e\u6570\u636e\u7684\u9009\u5b9a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u7b97\u6cd5\u7684\u6f5c\u529b\uff0c\u8fd9\u4e9b\u6570\u636e\u4e2d\u79bb\u7fa4\u70b9\u8d77\u91cd\u8981\u4f5c\u7528", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5176\u4ed6\u6280\u672f\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u805a\u7c7b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2601.00993", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00993", "abs": "https://arxiv.org/abs/2601.00993", "authors": ["Julian D. Santamaria", "Claudia Isaza", "Jhony H. Giraldo"], "title": "WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift", "comment": null, "summary": "Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.", "AI": {"tldr": "WildIng\u6a21\u578b\u901a\u8fc7\u6574\u5408\u6587\u672c\u63cf\u8ff0\u4e0e\u56fe\u50cf\u7279\u5f81\uff0c\u63d0\u5347\u91ce\u751f\u52a8\u7269\u8bc6\u522b\u6a21\u578b\u5728\u5730\u7406\u57df\u8fc1\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u65b0\u5730\u7406\u533a\u57df\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u91ce\u751f\u52a8\u7269\u8bc6\u522b\u6a21\u578b\u5728\u8bad\u7ec3\u5730\u7406\u533a\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65b0\u5730\u7406\u533a\u57df\u6d4b\u8bd5\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u56fe\u50cf\u7279\u5f81\uff0c\u5bf9\u80cc\u666f\u3001\u5149\u7167\u3001\u73af\u5883\u6761\u4ef6\u7b49\u5730\u7406\u5206\u5e03\u53d8\u5316\u654f\u611f\u3002", "method": "\u63d0\u51faWildIng\u6a21\u578b\uff0c\u6574\u5408\u6587\u672c\u63cf\u8ff0\u4e0e\u56fe\u50cf\u7279\u5f81\uff0c\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u6355\u6349\u7269\u79cd\u5916\u89c2\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u521b\u5efa\u5bf9\u5730\u7406\u57df\u8fc1\u79fb\u66f4\u9c81\u68d2\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWildIng\u5c06BioCLIP\u7b49\u57fa\u7840\u6a21\u578b\u5728\u5730\u7406\u57df\u8fc1\u79fb\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e8630%\uff0c\u5728\u975e\u6d32\u548c\u7f8e\u56fd\u4e24\u4e2a\u4e0d\u540c\u5730\u7406\u533a\u57df\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u6587\u672c\u63cf\u8ff0\u4e0e\u56fe\u50cf\u7279\u5f81\uff0cWildIng\u663e\u8457\u63d0\u5347\u4e86\u91ce\u751f\u52a8\u7269\u8bc6\u522b\u6a21\u578b\u5728\u5730\u7406\u57df\u8fc1\u79fb\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u4e2d\u7684\u5730\u7406\u5206\u5e03\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.00998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00998", "abs": "https://arxiv.org/abs/2601.00998", "authors": ["Yue Zhou", "Jue Chen", "Zilun Zhang", "Penghui Huang", "Ran Ding", "Zhentao Zou", "PengFei Gao", "Yuchen Wei", "Ke Li", "Xue Yang", "Xue Jiang", "Hongxin Yang", "Jonathan Li"], "title": "DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models", "comment": "20 pages, 17 figures", "summary": "Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DVGBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u65e0\u4eba\u673a\u7684\u9ad8\u8d28\u91cf\u9690\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d66\u4e2a\u4e3b\u8981\u5e94\u7528\u573a\u666f\uff0c\u5e76\u8bbe\u8ba1\u4e86DroneVG-R1\u6a21\u578b\uff0c\u91c7\u7528\u9690\u5f0f\u5230\u663e\u5f0f\u601d\u7ef4\u94fe\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u6765\u63d0\u5347\u65e0\u4eba\u673a\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u53c2\u7167\u8868\u8fbe\uff08\u5982\u76f8\u5bf9\u4f4d\u7f6e\u3001\u5927\u5c0f\u3001\u989c\u8272\u7b49\uff09\uff0c\u9650\u5236\u4e86\u5728\u9700\u8981\u9886\u57df\u77e5\u8bc6\u7684\u9690\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u9690\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c6\u6765\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "method": "1) \u6784\u5efaDVGBench\u6570\u636e\u96c6\uff0c\u5305\u542b6\u4e2a\u4e3b\u8981\u65e0\u4eba\u673a\u5e94\u7528\u573a\u666f\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u540c\u65f6\u63d0\u4f9b\u663e\u5f0f\u548c\u9690\u5f0f\u67e5\u8be2\uff1b2) \u8bbe\u8ba1DroneVG-R1\u6a21\u578b\uff0c\u91c7\u7528\u9690\u5f0f\u5230\u663e\u5f0f\u601d\u7ef4\u94fe\uff08I2E-CoT\uff09\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\uff0c\u5229\u7528\u573a\u666f\u7279\u5b9a\u4e13\u4e1a\u77e5\u8bc6\u5c06\u9690\u5f0f\u53c2\u7167\u8f6c\u6362\u4e3a\u663e\u5f0f\u53c2\u7167\u3002", "result": "\u5bf9\u4e3b\u6d41\u6a21\u578b\u5728\u663e\u5f0f\u548c\u9690\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u4e3a\u63d0\u5347\u65e0\u4eba\u673a\u667a\u80fd\u4f53\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "conclusion": "DVGBench\u6570\u636e\u96c6\u548cDroneVG-R1\u6a21\u578b\u4e3a\u65e0\u4eba\u673a\u9690\u5f0f\u89c6\u89c9\u5b9a\u4f4d\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u901a\u8fc7\u9690\u5f0f\u5230\u663e\u5f0f\u601d\u7ef4\u94fe\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u6709\u6548\u964d\u4f4e\u4e86\u89c6\u89c9\u5b9a\u4f4d\u96be\u5ea6\uff0c\u63a8\u52a8\u4e86\u9065\u611f\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.01378", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01378", "abs": "https://arxiv.org/abs/2601.01378", "authors": ["Han Yuan", "Yilin Wu", "Li Zhang", "Zheng Ma"], "title": "Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification", "comment": null, "summary": "Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAAAI\u4e09\u9636\u6bb5\u6d41\u7a0b\uff08\u5173\u8054\u8bc6\u522b\u3001\u81ea\u52a8\u68c0\u6d4b\u3001\u81ea\u9002\u5e94\u63a8\u7406\uff09\uff0c\u901a\u8fc7\u7f13\u89e3\u4e8b\u5b9e\u5e7b\u89c9\u6765\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u56e0\u5176\u5feb\u901f\u63a8\u7406\u548c\u672c\u5730\u90e8\u7f72\u4f18\u52bf\u5728\u91d1\u878d\u5206\u7c7b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\uff0cSLMs\u66f4\u5bb9\u6613\u4ea7\u751f\u4e8b\u5b9e\u5e7b\u89c9\u4e14\u5206\u7c7b\u6027\u80fd\u8f83\u5f31\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7f13\u89e3\u4e8b\u5b9e\u5e7b\u89c9\u662f\u5426\u80fd\u63d0\u5347SLMs\u7684\u91d1\u878d\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51faAAAI\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u5173\u8054\u8bc6\u522b\uff08Association Identification\uff09\u8bc6\u522b\u4e8b\u5b9e\u5e7b\u89c9\u4e0e\u5206\u7c7b\u9519\u8bef\u7684\u5173\u7cfb\uff1b2) \u81ea\u52a8\u68c0\u6d4b\uff08Automated Detection\uff09\u4f7f\u7528\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u9a8c\u8bc1\u5668\u68c0\u6d4b\u4e8b\u5b9e\u5e7b\u89c9\uff1b3) \u81ea\u9002\u5e94\u63a8\u7406\uff08Adaptive Inference\uff09\u901a\u8fc7\u4e8b\u5b9e\u9519\u8bef\u53cd\u9988\u4f7fSLMs\u80fd\u591f\u81ea\u9002\u5e94\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u5728\u4e09\u4e2a\u4ee3\u8868\u6027SLMs\u4e0a\u663e\u793a\uff1a1) \u4e8b\u5b9e\u5e7b\u89c9\u4e0e\u9519\u8bef\u5206\u7c7b\u5448\u6b63\u76f8\u5173\uff1b2) \u57fa\u4e8e\u7f16\u7801\u5668\u7684\u9a8c\u8bc1\u5668\u80fd\u6709\u6548\u68c0\u6d4b\u4e8b\u5b9e\u5e7b\u89c9\uff1b3) \u7ed3\u5408\u4e8b\u5b9e\u9519\u8bef\u53cd\u9988\u7684\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "AAAI\u6d41\u7a0b\u901a\u8fc7\u7f13\u89e3\u4e8b\u5b9e\u5e7b\u89c9\u6709\u6548\u63d0\u5347\u4e86SLMs\u5728\u91d1\u878d\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u4e3aSLMs\u5728\u91d1\u878d\u9886\u57df\u7684\u53ef\u4fe1\u8d56\u548c\u6709\u6548\u5e94\u7528\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00898", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00898", "abs": "https://arxiv.org/abs/2601.00898", "authors": ["Ruiming Liang", "Yinan Zheng", "Kexin Zheng", "Tianyi Tan", "Jianxiong Li", "Liyuan Mao", "Zhihao Wang", "Guang Chen", "Hangjun Ye", "Jingjing Liu", "Jinqiao Wang", "Xianyuan Zhan"], "title": "Dichotomous Diffusion Policy Optimization", "comment": null, "summary": "Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.", "AI": {"tldr": "DIPOLE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u6700\u4f18\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u5bf9\u7a33\u5b9a\u5b66\u4e60\u7684\u4e8c\u5206\u7b56\u7565\uff08\u4e00\u4e2a\u6700\u5927\u5316\u5956\u52b1\uff0c\u4e00\u4e2a\u6700\u5c0f\u5316\u5956\u52b1\uff09\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u5728RL\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u95ee\u9898\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u5b9e\u73b0\u8d2a\u5a6a\u5ea6\u7684\u7075\u6d3b\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u7b56\u7565\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u76f4\u63a5\u6700\u5927\u5316\u4ef7\u503c\u76ee\u6807\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u6216\u4f9d\u8d56\u7c97\u7cd9\u7684\u9ad8\u65af\u4f3c\u7136\u8fd1\u4f3c\u9700\u8981\u5927\u91cf\u5c0f\u53bb\u566a\u6b65\u9aa4\u5bfc\u81f4\u8ba1\u7b97\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u7a33\u5b9a\u8bad\u7ec3\u53c8\u80fd\u7075\u6d3b\u63a7\u5236\u63a8\u7406\u751f\u6210\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDIPOLE\u7b97\u6cd5\uff1a1) \u91cd\u65b0\u5ba1\u89c6RL\u4e2d\u7684KL\u6b63\u5219\u5316\u76ee\u6807\uff0c\u63d0\u4f9b\u6269\u6563\u7b56\u7565\u63d0\u53d6\u7684\u52a0\u6743\u56de\u5f52\u76ee\u6807\uff1b2) \u8bbe\u8ba1\u8d2a\u5a6a\u5316\u7b56\u7565\u6b63\u5219\u5316\u65b9\u6848\uff0c\u5c06\u6700\u4f18\u7b56\u7565\u5206\u89e3\u4e3a\u4e00\u5bf9\u4e8c\u5206\u7b56\u7565\uff08\u5956\u52b1\u6700\u5927\u5316\u548c\u6700\u5c0f\u5316\u7b56\u7565\uff09\uff1b3) \u5728\u63a8\u7406\u65f6\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u4e8c\u5206\u7b56\u7565\u7684\u5206\u6570\u6765\u751f\u6210\u4f18\u5316\u52a8\u4f5c\uff0c\u5b9e\u73b0\u8d2a\u5a6a\u5ea6\u7684\u7075\u6d3b\u63a7\u5236\u3002", "result": "\u5728ExORL\u548cOGBench\u7684\u79bb\u7ebf\u548c\u79bb\u7ebf\u5230\u5728\u7ebfRL\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u8fd8\u6210\u529f\u8bad\u7ec3\u4e86\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6NAVSIM\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "DIPOLE\u901a\u8fc7\u4e8c\u5206\u7b56\u7565\u5206\u89e3\u548c\u7ebf\u6027\u7ec4\u5408\u673a\u5236\uff0c\u4e3a\u6269\u6563\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u53ef\u63a7\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2601.01467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01467", "abs": "https://arxiv.org/abs/2601.01467", "authors": ["Romuald Kwessy Mouona", "Blaise Bl\u00e9riot Koguep Njionou", "Etienne Romuald Temgoua Alomo", "Rokia Missaoui", "Leonard Kwuida"], "title": "A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts", "comment": "26 pages", "summary": "This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.", "AI": {"tldr": "\u7814\u7a76\u4e09\u5143\u80cc\u666f\u4e2d\u7684\u8574\u542b\u5173\u7cfb\uff0c\u91cd\u70b9\u5206\u6790Ganter\u548cObiedkov\u63d0\u51fa\u7684\u6761\u4ef6\u5c5e\u6027\u8574\u542b\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u542b\uff0c\u76ee\u6807\u662f\u6784\u5efa\u8fd9\u4e9b\u8574\u542b\u7684\u6700\u4f18\u57fa", "motivation": "\u4e09\u5143\u80cc\u666f\u4e2d\u7684\u8574\u542b\u5173\u7cfb\u5206\u6790\u662f\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\u7684\u91cd\u8981\u6269\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e9b\u8574\u542b\u5173\u7cfb\u7684\u7279\u6027\u5e76\u6784\u5efa\u6700\u4f18\u8868\u793a\u57fa", "method": "\u7814\u7a76Ganter\u548cObiedkov\u63d0\u51fa\u7684\u6761\u4ef6\u5c5e\u6027\u8574\u542b\u548c\u5c5e\u6027\u6761\u4ef6\u8574\u542b\uff0c\u5206\u6790\u5176\u6570\u5b66\u7279\u6027\uff0c\u6784\u5efa\u8fd9\u4e9b\u8574\u542b\u5173\u7cfb\u7684\u6700\u4f18\u57fa", "result": "\u5efa\u7acb\u4e86\u4e09\u5143\u80cc\u666f\u4e2d\u8574\u542b\u5173\u7cfb\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u6784\u5efa\u6700\u4f18\u57fa\u7684\u65b9\u6cd5", "conclusion": "\u6210\u529f\u5206\u6790\u4e86\u4e09\u5143\u80cc\u666f\u4e2d\u7684\u8574\u542b\u5173\u7cfb\uff0c\u4e3a\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\u5728\u4e09\u5143\u6570\u636e\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2601.00908", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.00908", "abs": "https://arxiv.org/abs/2601.00908", "authors": ["Chorok Lee"], "title": "Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment", "comment": null, "summary": "Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5728\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u4fdd\u5f62\u9884\u6d4b\u7684\u8986\u76d6\u4fdd\u8bc1\u4f1a\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u7279\u5f81\u4e25\u91cd\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u3002\u901a\u8fc7COVID-19\u4f9b\u5e94\u94fe\u4efb\u52a1\u5206\u6790\u53d1\u73b0\uff0c\u8986\u76d6\u4e0b\u964d\u7a0b\u5ea6\u5dee\u5f02\u5de8\u5927\uff080%-86.7%\uff09\uff0c\u4e0e\u5355\u7279\u5f81\u4f9d\u8d56\u6027\u9ad8\u5ea6\u76f8\u5173\u3002\u63d0\u51fa\u57fa\u4e8eSHAP\u7279\u5f81\u91cd\u8981\u6027\u96c6\u4e2d\u5ea6\u7684\u51b3\u7b56\u6846\u67b6\u6765\u6307\u5bfc\u6a21\u578b\u90e8\u7f72\u548c\u91cd\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u4fdd\u5f62\u9884\u6d4b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u4fdd\u8bc1\u4f1a\u4e0b\u964d\uff0c\u4f46\u5177\u4f53\u4e0b\u964d\u7a0b\u5ea6\u548c\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5229\u7528COVID-19\u4f5c\u4e3a\u81ea\u7136\u5b9e\u9a8c\uff0c\u7814\u7a76\u4f9b\u5e94\u94fe\u4efb\u52a1\u4e2d\u7279\u5f81\u53d8\u5316\u5bf9\u4fdd\u5f62\u9884\u6d4b\u8986\u76d6\u4fdd\u8bc1\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u63a2\u7d22\u4e3a\u4ec0\u4e48\u4e0d\u540c\u4efb\u52a1\u5728\u76f8\u540c\u7279\u5f81\u53d8\u5316\u7a0b\u5ea6\u4e0b\u8868\u73b0\u5dee\u5f02\u5982\u6b64\u5de8\u5927\u3002", "method": "\u4f7f\u7528COVID-19\u4f5c\u4e3a\u81ea\u7136\u5b9e\u9a8c\uff0c\u5206\u67908\u4e2a\u4f9b\u5e94\u94fe\u4efb\u52a1\u3002\u901a\u8fc7Jaccard\u76f8\u4f3c\u5ea6\u8861\u91cf\u7279\u5f81\u53d8\u5316\u7a0b\u5ea6\uff0c\u4f7f\u7528SHAP\u5206\u6790\u7279\u5f81\u91cd\u8981\u6027\u5206\u5e03\u3002\u6bd4\u8f83\u4e0d\u540c\u4efb\u52a1\u5728\u76f8\u540c\u7279\u5f81\u4e25\u91cd\u53d8\u5316\uff08Jaccard\u22480\uff09\u4e0b\u7684\u8986\u76d6\u4e0b\u964d\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u76f8\u5173\u6027\u5206\u6790\uff08rho=0.714, p=0.047\uff09\u9a8c\u8bc1\u5355\u7279\u5f81\u4f9d\u8d56\u6027\u4e0e\u707e\u96be\u6027\u5931\u8d25\u7684\u5173\u7cfb\u3002\u5bf9\u707e\u96be\u6027\u4efb\u52a1\u8fdb\u884c\u5b63\u5ea6\u91cd\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u5e76\u6269\u5c55\u5206\u67904\u4e2a\u4e2d\u7b49\u7279\u5f81\u7a33\u5b9a\u6027\u4efb\u52a1\u3002", "result": "\u5728\u76f8\u540c\u7279\u5f81\u4e25\u91cd\u53d8\u5316\u4e0b\uff0c\u4e0d\u540c\u4efb\u52a1\u7684\u8986\u76d6\u4e0b\u964d\u5dee\u5f02\u5de8\u5927\uff080%-86.7%\uff09\u3002\u707e\u96be\u6027\u5931\u8d25\u4e0e\u5355\u7279\u5f81\u4f9d\u8d56\u6027\u9ad8\u5ea6\u76f8\u5173\uff08rho=0.714\uff09\uff0c\u707e\u96be\u6027\u4efb\u52a1\u7684\u7279\u5f81\u91cd\u8981\u6027\u96c6\u4e2d\u5728\u5355\u4e00\u7279\u5f81\uff08\u589e\u52a04.5\u500d\uff09\uff0c\u800c\u7a33\u5065\u4efb\u52a1\u5219\u5206\u6563\u5230\u591a\u4e2a\u7279\u5f81\uff0810-20\u500d\uff09\u3002\u5b63\u5ea6\u91cd\u8bad\u7ec3\u53ef\u5c06\u707e\u96be\u6027\u4efb\u52a1\u8986\u76d6\u4ece22%\u63d0\u5347\u523041%\uff0c\u4f46\u5bf9\u7a33\u5065\u4efb\u52a1\u65e0\u76ca\uff08\u4fdd\u630199.8%\u8986\u76d6\uff09\u3002\u4e2d\u7b49\u7279\u5f81\u7a33\u5b9a\u6027\u4efb\u52a1\u7684\u5206\u6790\u8868\u660e\uff0c\u7279\u5f81\u7a33\u5b9a\u6027\u800c\u975e\u96c6\u4e2d\u5ea6\u51b3\u5b9a\u7a33\u5065\u6027\u3002", "conclusion": "\u63d0\u51fa\u57fa\u4e8eSHAP\u7279\u5f81\u91cd\u8981\u6027\u96c6\u4e2d\u5ea6\u7684\u51b3\u7b56\u6846\u67b6\uff1a\u90e8\u7f72\u524d\u76d1\u63a7SHAP\u96c6\u4e2d\u5ea6\uff1b\u5982\u679c\u96c6\u4e2d\u5ea6>40%\uff08\u6613\u53d7\u653b\u51fb\uff09\u5219\u8fdb\u884c\u5b63\u5ea6\u91cd\u8bad\u7ec3\uff1b\u5982\u679c\u7a33\u5065\u5219\u8df3\u8fc7\u91cd\u8bad\u7ec3\u3002\u7279\u5f81\u96c6\u4e2d\u5ea6\u6548\u5e94\u4e3b\u8981\u9002\u7528\u4e8e\u4e25\u91cd\u5206\u5e03\u504f\u79fb\u60c5\u51b5\uff0c\u800c\u7279\u5f81\u7a33\u5b9a\u6027\u662f\u51b3\u5b9a\u6a21\u578b\u7a33\u5065\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2601.00915", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00915", "abs": "https://arxiv.org/abs/2601.00915", "authors": ["Jacquelyn Shelton", "Przemyslaw Polewski", "Alexander Robel", "Matthew Hoffman", "Stephen Price"], "title": "Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles", "comment": "draft / preliminary", "summary": "Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.", "AI": {"tldr": "\u63d0\u51faLC-CVAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u6f5c\u5728\u7a7a\u95f4\u5728\u5171\u4eab\"\u951a\u70b9\"\u4f4d\u7f6e\u7684\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4f20\u7edfCVAE\u5728\u6c14\u5019\u6a21\u578b\u751f\u6210\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u6709\u9650\u6c14\u5019\u6a21\u62df\u4e2d\u751f\u6210\u7edf\u8ba1\u4e00\u81f4\u7684\u65f6\u7a7a\u53d8\u91cf\u65b0\u5b9e\u73b0\u3002", "motivation": "\u5927\u578b\u6c14\u5019\u6a21\u578b\u96c6\u5408\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f46\u8bb8\u591a\u4e0b\u6e38\u5206\u6790\u9700\u8981\u66f4\u591a\u7edf\u8ba1\u4e00\u81f4\u7684\u65f6\u7a7a\u6c14\u5019\u53d8\u91cf\u5b9e\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u6a21\u62df\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u751f\u6210\u672a\u89c1\u8fc7\u7684\u96c6\u5408\u6210\u5458\u3002", "method": "\u63d0\u51fa\u6f5c\u5728\u7ea6\u675f\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668(LC-CVAE)\uff0c\u5728\u5171\u4eab\u5730\u7406\"\u951a\u70b9\"\u4f4d\u7f6e\u5f3a\u5236\u6f5c\u5728\u5d4c\u5165\u7684\u8de8\u5b9e\u73b0\u540c\u8d28\u6027\u3002\u7136\u540e\u4f7f\u7528\u591a\u8f93\u51fa\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5728\u6f5c\u5728\u7a7a\u95f4\u9884\u6d4b\u65b0\u5b9e\u73b0\u4e2d\u672a\u91c7\u6837\u4f4d\u7f6e\u7684\u6f5c\u5728\u5750\u6807\uff0c\u6700\u540e\u89e3\u7801\u751f\u6210\u5b8c\u6574\u65f6\u95f4\u5e8f\u5217\u573a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u5728\u5355\u4e2a\u5b9e\u73b0\u4e0a\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff1b2) \u7eb3\u5165\u7ea65\u4e2a\u5b9e\u73b0\u540e\u6536\u76ca\u9012\u51cf\uff1b3) \u7a7a\u95f4\u8986\u76d6\u8303\u56f4\u4e0e\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u8fd9\u4e0e\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5e73\u5747\u90bb\u8fd1\u8ddd\u79bb\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "LC-CVAE\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7ea6\u675f\u89e3\u51b3\u4e86\u4f20\u7edfCVAE\u5728\u6c14\u5019\u6a21\u578b\u751f\u6210\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u80fd\u591f\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u751f\u6210\u7edf\u8ba1\u4e00\u81f4\u7684\u65b0\u6c14\u5019\u5b9e\u73b0\uff0c\u4e3a\u6c14\u5019\u6a21\u578b\u96c6\u5408\u6269\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2601.00919", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00919", "abs": "https://arxiv.org/abs/2601.00919", "authors": ["Zichuan Fu", "Wentao Song", "Guojing Li", "Yejing Wang", "Xian Wu", "Yimin Deng", "Hanyu Yan", "Yefeng Zheng", "Xiangyu Zhao"], "title": "Attention Needs to Focus: A Unified Perspective on Attention Allocation", "comment": "ICLR 2026 conference", "summary": "The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLazy Attention\u673a\u5236\uff0c\u901a\u8fc7\u7edf\u4e00\u89c6\u89d2\u89e3\u51b3Transformer\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u6ce8\u610f\u529b\u8fc7\u8f7d\u548c\u6ce8\u610f\u529b\u6b20\u8f7d\uff0c\u4ece\u800c\u63d0\u9ad8\u6ce8\u610f\u529b\u5206\u5e03\u7684\u805a\u7126\u6027\u3002", "motivation": "Transformer\u67b6\u6784\u4e2d\u7684\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u4e24\u4e2a\u5df2\u77e5\u95ee\u9898\uff1a\u8868\u793a\u5d29\u6e83\u548c\u6ce8\u610f\u529b\u6c89\u6ca1\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5b64\u7acb\u5730\u5904\u7406\u8fd9\u4e9b\u95ee\u9898\uff0c\u800c\u672c\u6587\u8ba4\u4e3a\u5b83\u4eec\u6709\u5171\u540c\u7684\u6839\u6e90\u2014\u2014\u4e0d\u6070\u5f53\u7684\u6ce8\u610f\u529b\u5206\u914d\u3002", "method": "\u63d0\u51faLazy Attention\u673a\u5236\uff1a1) \u9488\u5bf9\u6ce8\u610f\u529b\u8fc7\u8f7d\uff0c\u91c7\u7528\u8de8\u5934\u548c\u7ef4\u5ea6\u7684\u4f4d\u7f6e\u533a\u5206\u6765\u589e\u5f3atoken\u533a\u5206\u5ea6\uff1b2) \u9488\u5bf9\u6ce8\u610f\u529b\u6b20\u8f7d\uff0c\u5f15\u5165Elastic-Softmax\u5f52\u4e00\u5316\u51fd\u6570\uff0c\u653e\u677e\u6807\u51c6softmax\u7ea6\u675f\u4ee5\u6291\u5236\u5bf9\u65e0\u5173token\u7684\u5173\u6ce8\u3002", "result": "\u5728FineWeb-Edu\u8bed\u6599\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLazy Attention\u6210\u529f\u7f13\u89e3\u4e86\u6ce8\u610f\u529b\u6c89\u6ca1\u95ee\u9898\uff0c\u5728\u4e5d\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u6807\u51c6\u6ce8\u610f\u529b\u548c\u73b0\u4ee3\u67b6\u6784\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u8fbe\u5230\u9ad8\u8fbe59.58%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5ea6\u3002", "conclusion": "Lazy Attention\u901a\u8fc7\u7edf\u4e00\u89c6\u89d2\u89e3\u51b3\u6ce8\u610f\u529b\u5206\u914d\u95ee\u9898\uff0c\u4e3aTransformer\u67b6\u6784\u63d0\u4f9b\u4e86\u66f4\u805a\u7126\u7684\u6ce8\u610f\u529b\u5206\u5e03\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\u3002"}}
{"id": "2601.00920", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00920", "abs": "https://arxiv.org/abs/2601.00920", "authors": ["Xingsheng Chen", "Regina Zhang", "Bo Gao", "Xingwei He", "Xiaofeng Liu", "Pietro Lio", "Kwok-Yan Lam", "Siu-Ming Yiu"], "title": "MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs", "comment": "12 pages, 6 tables", "summary": "Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.", "AI": {"tldr": "MODE\uff1a\u4e00\u4e2a\u7ed3\u5408\u4f4e\u79e9\u795e\u7ecfODE\u548c\u589e\u5f3aMamba\u67b6\u6784\u7684\u7edf\u4e00\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u5e73\u8861\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faMODE\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u7ebf\u6027\u6807\u8bb0\u5316\u5c42\u5904\u7406\u8f93\u5165\u5e8f\u5217\uff1b2\uff09\u901a\u8fc7\u591a\u4e2a\u589e\u5f3aMamba\u7f16\u7801\u5668\u5757\u5904\u7406\uff0c\u6bcf\u4e2a\u5757\u5305\u542b\u56e0\u679c\u5377\u79ef\u3001SiLU\u6fc0\u6d3b\u548c\u4f4e\u79e9\u795e\u7ecfODE\u589e\u5f3a\uff1b3\uff09\u91c7\u7528\u5206\u6bb5\u9009\u62e9\u6027\u626b\u63cf\u673a\u5236\uff0c\u53d7\u4f2aODE\u52a8\u529b\u5b66\u542f\u53d1\uff0c\u81ea\u9002\u5e94\u5173\u6ce8\u91cd\u8981\u5b50\u5e8f\u5217\uff1b4\uff09\u4f4e\u79e9\u516c\u5f0f\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMODE\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MODE\u4e3a\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u9ad8\u6548\u67b6\u6784\uff0c\u901a\u8fc7\u5c06Mamba\u7684\u9009\u62e9\u6027\u626b\u63cf\u4e0e\u4f4e\u79e9\u795e\u7ecfODE\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u65f6\u95f4\u8868\u793a\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u52a8\u6001\u9009\u62e9\u6027\u626b\u63cf\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.01036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01036", "abs": "https://arxiv.org/abs/2601.01036", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Kien Nguyen Do Trung", "Duc Dung Nguyen"], "title": "Mono3DV: Monocular 3D Object Detection with 3D-Aware Bipartite Matching and Variational Query DeNoising", "comment": null, "summary": "While DETR-like architectures have demonstrated significant potential for monocular 3D object detection, they are often hindered by a critical limitation: the exclusion of 3D attributes from the bipartite matching process. This exclusion arises from the inherent ill-posed nature of 3D estimation from monocular image, which introduces instability during training. Consequently, high-quality 3D predictions can be erroneously suppressed by 2D-only matching criteria, leading to suboptimal results. To address this, we propose Mono3DV, a novel Transformer-based framework. Our approach introduces three key innovations. First, we develop a 3D-Aware Bipartite Matching strategy that directly incorporates 3D geometric information into the matching cost, resolving the misalignment caused by purely 2D criteria. Second, it is important to stabilize the Bipartite Matching to resolve the instability occurring when integrating 3D attributes. Therefore, we propose 3D-DeNoising scheme in the training phase. Finally, recognizing the gradient vanishing issue associated with conventional denoising techniques, we propose a novel Variational Query DeNoising mechanism to overcome this limitation, which significantly enhances model performance. Without leveraging any external data, our method achieves state-of-the-art results on the KITTI 3D object detection benchmark.", "AI": {"tldr": "Mono3DV\uff1a\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc73D\u611f\u77e5\u4e8c\u5206\u5339\u914d\u30013D\u53bb\u566a\u548c\u53d8\u5206\u67e5\u8be2\u53bb\u566a\u673a\u5236\u89e3\u51b3\u4f20\u7edfDETR\u67b6\u6784\u4e2d3D\u5c5e\u6027\u88ab\u6392\u9664\u5728\u5339\u914d\u8fc7\u7a0b\u4e4b\u5916\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfDETR\u7c7b\u67b6\u6784\u5728\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u4e2d\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u7531\u4e8e\u5355\u76ee\u56fe\u50cf3D\u4f30\u8ba1\u7684\u56fa\u6709\u4e0d\u9002\u5b9a\u6027\uff0c3D\u5c5e\u6027\u88ab\u6392\u9664\u5728\u4e8c\u5206\u5339\u914d\u8fc7\u7a0b\u4e4b\u5916\uff0c\u5bfc\u81f4\u9ad8\u8d28\u91cf3D\u9884\u6d4b\u53ef\u80fd\u88ab\u4ec5\u57fa\u4e8e2D\u7684\u5339\u914d\u6807\u51c6\u9519\u8bef\u6291\u5236\uff0c\u9020\u6210\u6b21\u4f18\u7ed3\u679c\u3002", "method": "\u63d0\u51faMono3DV\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff093D\u611f\u77e5\u4e8c\u5206\u5339\u914d\u7b56\u7565\uff0c\u5c063D\u51e0\u4f55\u4fe1\u606f\u76f4\u63a5\u7eb3\u5165\u5339\u914d\u6210\u672c\uff1b2\uff093D\u53bb\u566a\u65b9\u6848\uff0c\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u4e8c\u5206\u5339\u914d\uff1b3\uff09\u53d8\u5206\u67e5\u8be2\u53bb\u566a\u673a\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u53bb\u566a\u6280\u672f\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "result": "\u5728\u4e0d\u4f7f\u7528\u4efb\u4f55\u5916\u90e8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728KITTI 3D\u76ee\u6807\u68c0\u6d4b\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "Mono3DV\u901a\u8fc7\u5c063D\u51e0\u4f55\u4fe1\u606f\u6574\u5408\u5230\u5339\u914d\u8fc7\u7a0b\u4e2d\u5e76\u89e3\u51b3\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e863D\u611f\u77e5\u5339\u914d\u548c\u7a33\u5b9a\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.01546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01546", "abs": "https://arxiv.org/abs/2601.01546", "authors": ["Letian Kong", "Qianran", "Jin", "Renyu Zhang"], "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation", "comment": "39 pages, 2 figures, 3 tables", "summary": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\u6539\u5584LLM\u5728\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u5bf9\u9f50\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e0a\u4e0b\u6587\u5f62\u6210\u660e\u786e\u6307\u5b9a\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4e0a\u4e0b\u6587\u5bfc\u822a\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002\u9a8c\u8bc1\u8868\u660e\u590d\u6742\u51b3\u7b56\u9700\u8981\u4e24\u9636\u6bb5\uff0c\u7b80\u5355\u4efb\u52a1\u4ec5\u9700\u7b2c\u4e00\u9636\u6bb5\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u7528\u4e8e\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u5b9e\u9a8c\uff0c\u4f46\u5728\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\uff08\u9700\u8981\u9884\u6d4b\u4ed6\u4eba\u884c\u52a8\u548c\u57fa\u4e8e\u89c2\u5bdf\u884c\u4e3a\u5f62\u6210\u4fe1\u5ff5\uff09\u4e0e\u4eba\u7c7b\u51b3\u7b56\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u9700\u8981\u6539\u8fdb\u884c\u4e3a\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4e0a\u4e0b\u6587\u5f62\u6210\u9636\u6bb5\uff1a\u660e\u786e\u6307\u5b9a\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5efa\u7acb\u51b3\u7b56\u4efb\u52a1\u548c\u4e0a\u4e0b\u6587\u7684\u51c6\u786e\u8868\u5f81\uff1b2\uff09\u4e0a\u4e0b\u6587\u5bfc\u822a\u9636\u6bb5\uff1a\u5728\u8be5\u8868\u5f81\u5185\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u505a\u51fa\u51b3\u7b56\u3002\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff1a\u5e8f\u5217\u8d2d\u4e70\u6e38\u620f\u3001\u4f17\u7b79\u6e38\u620f\u548c\u9700\u6c42\u4f30\u8ba1\u4efb\u52a1\u3002", "result": "\u5728\u56db\u4e2aSOTA\u6a21\u578b\uff08GPT-4o\u3001GPT-5\u3001Claude-4.0-Sonnet-Thinking\u3001DeepSeek-R1\uff09\u4e0a\u9a8c\u8bc1\u53d1\u73b0\uff1a\u590d\u6742\u51b3\u7b56\u73af\u5883\u9700\u8981\u4e24\u9636\u6bb5\u624d\u80fd\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u57fa\u51c6\u7684\u884c\u4e3a\u5bf9\u9f50\uff0c\u800c\u7b80\u5355\u7684\u9700\u6c42\u4f30\u8ba1\u4efb\u52a1\u4ec5\u9700\u4e0a\u4e0b\u6587\u5f62\u6210\u9636\u6bb5\u3002", "conclusion": "\u9610\u660e\u4e86\u6bcf\u4e2a\u9636\u6bb5\u4f55\u65f6\u5fc5\u8981\uff0c\u4e3a\u8bbe\u8ba1\u548c\u8bca\u65adLLM\u793e\u4f1a\u6a21\u62df\u4f5c\u4e3a\u884c\u4e3a\u7814\u7a76\u4e2d\u4eba\u7c7b\u53d7\u8bd5\u8005\u7684\u8865\u5145\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86LLM\u5728\u590d\u6742\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2601.00921", "categories": ["cs.LG", "cs.AI", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.00921", "abs": "https://arxiv.org/abs/2601.00921", "authors": ["Azadeh Alavi", "Hamidreza Khalili", "Stanley H. Chan", "Fatemeh Kouchmeshki", "Ross Vlahos"], "title": "Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease", "comment": "24 pages, 4 figures", "summary": "Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u91cf\u5b50\u6838\u56de\u5f52\u548c\u51e0\u4f55\u611f\u77e5\u65b9\u6cd5\u9884\u6d4bCOPD\u5c0f\u9f20\u6a21\u578b\u7684\u808c\u8089\u529f\u80fd\u6307\u6807\uff0c\u5728\u4f4e\u6570\u636e\u3001\u4f4e\u7279\u5f81\u751f\u7269\u533b\u5b66\u9884\u6d4b\u4e2d\u663e\u793a\u51fa\u4f18\u52bf\u3002", "motivation": "\u6162\u6027\u963b\u585e\u6027\u80ba\u75be\u75c5\uff08COPD\uff09\u7684\u9aa8\u9abc\u808c\u529f\u80fd\u969c\u788d\u4e0e\u5168\u8eab\u548c\u6c14\u9053\u708e\u75c7\u5bc6\u5207\u76f8\u5173\uff0c\u9700\u8981\u4ece\u5fae\u521b\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u808c\u8089\u7ed3\u5c40\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5c0f\u6837\u672c\u4e34\u5e8a\u524d\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528213\u53ea\u52a8\u7269\uff08Sham\u7ec4\u4e0e\u9999\u70df\u70df\u96fe\u66b4\u9732\u7ec4\uff09\u7684\u5c0f\u6837\u672c\u4e34\u5e8a\u524d\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u8c03\u4f18\u7684\u7ecf\u5178\u57fa\u7ebf\u6a21\u578b\u3001\u51e0\u4f55\u611f\u77e5\u5bf9\u79f0\u6b63\u5b9a\u63cf\u8ff0\u7b26\u4e0eStein\u6563\u5ea6\u3001\u4ee5\u53ca\u9488\u5bf9\u4f4e\u7ef4\u8868\u683c\u6570\u636e\u8bbe\u8ba1\u7684\u91cf\u5b50\u6838\u6a21\u578b\u3002\u91cf\u5b50\u6838\u5cad\u56de\u5f52\u4f7f\u7528\u56db\u4e2a\u53ef\u89e3\u91ca\u8f93\u5165\uff08\u8840\u6db2C\u53cd\u5e94\u86cb\u767d\u3001\u4e2d\u6027\u7c92\u7ec6\u80de\u8ba1\u6570\u3001\u652f\u6c14\u7ba1\u80ba\u6ce1\u704c\u6d17\u7ec6\u80de\u6027\u548c\u6761\u4ef6\uff09\u9884\u6d4b\u808c\u8089\u91cd\u91cf\u3002", "result": "\u5728\u808c\u8089\u91cd\u91cf\u9884\u6d4b\u4e2d\uff0c\u91cf\u5b50\u6838\u5cad\u56de\u5f52\u7684\u6d4b\u8bd5\u5747\u65b9\u6839\u8bef\u5dee\u4e3a4.41 mg\uff0c\u51b3\u5b9a\u7cfb\u6570\u4e3a0.605\uff0c\u4f18\u4e8e\u76f8\u540c\u7279\u5f81\u96c6\u7684\u5339\u914d\u5cad\u56de\u5f52\u57fa\u7ebf\uff084.70 mg\u548c0.553\uff09\u3002\u51e0\u4f55\u611f\u77e5\u7684Stein\u6563\u5ea6\u539f\u578b\u8ddd\u79bb\u5728\u4ec5\u4f7f\u7528\u751f\u7269\u6807\u5fd7\u7269\u7684\u8bbe\u7f6e\u4e2d\u4e5f\u83b7\u5f97\u4e86\u4e00\u81f4\u4f46\u8f83\u5c0f\u7684\u6539\u8fdb\uff084.55 mg vs 4.79 mg\uff09\u3002\u901a\u8fc7\u9608\u503c\u5316\u8fde\u7eed\u7ed3\u679c\u8fdb\u884c\u7b5b\u67e5\u5f0f\u8bc4\u4f30\uff0c\u68c0\u6d4b\u4f4e\u808c\u8089\u91cd\u91cf\u7684ROC-AUC\u9ad8\u8fbe0.90\u3002", "conclusion": "\u51e0\u4f55\u548c\u91cf\u5b50\u6838\u63d0\u5347\u65b9\u6cd5\u5728\u4f4e\u6570\u636e\u3001\u4f4e\u7279\u5f81\u7684\u751f\u7269\u533b\u5b66\u9884\u6d4b\u95ee\u9898\u4e2d\u80fd\u63d0\u4f9b\u53ef\u6d4b\u91cf\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u7684\u6a21\u578b\u9009\u62e9\u3002"}}
{"id": "2601.01562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01562", "abs": "https://arxiv.org/abs/2601.01562", "authors": ["Mingyu Xu", "Cheng Fang", "Keyue Jiang", "Yuqian Zheng", "Yanghua Xiao", "Baojian Zhou", "Qifang Zhao", "Suhang Zheng", "Xiuwen Zhu", "Jiyang Tang", "Yongchi Zhao", "Yijia Luo", "Zhiqi Bai", "Yuchi Xu", "Wenbo Su", "Wei Wang", "Bing Zhao", "Lin Qu", "Xiaoxiao Xu"], "title": "Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement", "comment": null, "summary": "We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.", "AI": {"tldr": "Logics-STEM\u662f\u4e00\u4e2a\u9488\u5bf9STEM\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4f18\u5316\u7684\u5148\u8fdb\u63a8\u7406\u6a21\u578b\uff0c\u57fa\u4e8e1000\u4e07\u89c4\u6a21\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5728STEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53474.68%\u6027\u80fd\uff0c\u901a\u8fc7\u6570\u636e\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5f53\u524dSTEM\u9886\u57df\u9700\u8981\u66f4\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728STEM\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6765\u63d0\u5347\u6a21\u578b\u5728\u79d1\u5b66\u3001\u6280\u672f\u3001\u5de5\u7a0b\u548c\u6570\u5b66\u9886\u57df\u7684\u63a8\u7406\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6570\u636e\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5f15\u64ce\uff1a\u6570\u636e\u65b9\u9762\uff0c\u901a\u8fc75\u9636\u6bb5\u6570\u636e\u7b56\u5212\u5f15\u64ce\uff08\u6807\u6ce8\u3001\u53bb\u91cd\u3001\u53bb\u6c61\u67d3\u3001\u84b8\u998f\u3001\u5206\u5c42\u91c7\u6837\uff09\u6784\u5efaLogics-STEM-SFT-Dataset\uff1b\u7b97\u6cd5\u65b9\u9762\uff0c\u4f7f\u7528\u5931\u8d25\u9a71\u52a8\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u77e5\u8bc6\u68c0\u7d22\u548c\u5931\u8d25\u533a\u57df\u6570\u636e\u5408\u6210\u6765\u6307\u5bfc\u7b2c\u4e8c\u9636\u6bb5SFT\u6216\u5f3a\u5316\u5b66\u4e60\u3002", "result": "Logics-STEM\u5728STEM\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u540c\u89c4\u6a218B\u6a21\u578b\u5e73\u5747\u63d0\u53474.68%\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u5f00\u6e90\u6570\u636e\u4e0e\u7cbe\u5fc3\u8bbe\u8ba1\u5408\u6210\u6570\u636e\u7ed3\u5408\u7684\u6f5c\u529b\u3002\u6a21\u578b\u63d0\u4f9b8B\u548c32B\u7248\u672c\uff0c\u6570\u636e\u96c6\u63d0\u4f9b1000\u4e07\u548c220\u4e07\u7248\u672c\u3002", "conclusion": "\u6570\u636e\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5bf9\u4e8e\u901a\u8fc7\u540e\u8bad\u7ec3\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u5927\u89c4\u6a21\u5f00\u6e90\u6570\u636e\u4e0e\u7cbe\u5fc3\u8bbe\u8ba1\u5408\u6210\u6570\u636e\u7684\u7ed3\u5408\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002\u7814\u7a76\u516c\u5f00\u4e86\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u652f\u6301\u5f00\u6e90\u793e\u533a\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2601.00924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00924", "abs": "https://arxiv.org/abs/2601.00924", "authors": ["Rares Folea", "Radu Iacob", "Emil Slusanschi", "Traian Rebedea"], "title": "Complexity-based code embeddings", "comment": null, "summary": "This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7b97\u6cd5\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u5d4c\u5165\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u6790\u7a0b\u5e8f\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u4e3a\u5206\u6790\u6307\u6807\u5b9a\u5236\u591a\u4e2a\u901a\u7528\u590d\u6742\u5ea6\u51fd\u6570\uff0c\u57fa\u4e8er-Complexity\u6784\u5efa\u4ee3\u7801\u5d4c\u5165\uff0c\u5e76\u5728Codeforces\u771f\u5b9e\u4ee3\u7801\u7247\u6bb5\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86XGBoost\u7b97\u6cd5\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u65b9\u6cd5\u6765\u5c06\u4e0d\u540c\u7b97\u6cd5\u7684\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u5d4c\u5165\uff0c\u4ee5\u4fbf\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u5904\u7406\u548c\u5206\u6790\u3002\u901a\u8fc7\u52a8\u6001\u5206\u6790\u7a0b\u5e8f\u884c\u4e3a\u548c\u4f7f\u7528\u590d\u6742\u5ea6\u51fd\u6570\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u6355\u6349\u4ee3\u7801\u7684\u8bed\u4e49\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6e90\u4ee3\u7801\u8f6c\u6362\u65b9\u6cd5\uff1a1\uff09\u52a8\u6001\u5206\u6790\u8ba1\u7b97\u673a\u7a0b\u5e8f\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u7684\u884c\u4e3a\uff1b2\uff09\u4e3a\u5206\u6790\u6307\u6807\u5b9a\u5236\u591a\u4e2a\u901a\u7528\u590d\u6742\u5ea6\u51fd\u6570\uff1b3\uff09\u57fa\u4e8er-Complexity\u6784\u5efa\u7b97\u6cd5\u5d4c\u5165\uff1b4\uff09\u4f7f\u7528\u8fd9\u4e9b\u5d4c\u5165\u5b9e\u73b0XGBoost\u7b97\u6cd5\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\u3002", "result": "\u5728\u57fa\u4e8eCodeforces\u5e73\u53f0\u771f\u5b9e\u4ee3\u7801\u7247\u6bb5\u6784\u5efa\u768411\u7c7b\u591a\u6807\u7b7e\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u63d0\u51fa\u7684\u4ee3\u7801\u5d4c\u5165\u5b9e\u73b0\u7684XGBoost\u7b97\u6cd5\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u5747F1\u5206\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c06\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3a\u6570\u503c\u5d4c\u5165\uff0c\u4e3a\u4ee3\u7801\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7279\u5f81\u8868\u793a\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7684\u7f16\u7a0b\u7ade\u8d5b\u4ee3\u7801\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.01044", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01044", "abs": "https://arxiv.org/abs/2601.01044", "authors": ["Jin Wang", "Angelo De Castro", "Yuxi Zhang", "Lucas Basolli Borsatto", "Yuechen Guo", "Victoria Bastos Primo", "Ana Beatriz Montevecchio Bernardino", "Gota Morota", "Ricardo C Chebel", "Haipeng Yu"], "title": "Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data", "comment": null, "summary": "Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728\u5976\u725b\u4f53\u91cd\u9884\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u6bd4\u8f83\u4e86\u6df1\u5ea6\u56fe\u50cf\u548c\u70b9\u4e91\u4e24\u79cd\u6570\u636e\u6a21\u6001\uff0c\u53d1\u73b0\u8fc1\u79fb\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u89c4\u6a21\u519c\u573a\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4e14\u4e24\u79cd\u6a21\u6001\u8868\u73b0\u76f8\u5f53\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u4e3a\u5976\u725b\u76d1\u6d4b\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u3001\u975e\u4fb5\u5165\u6027\u548c\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u4f46\u8fc1\u79fb\u5b66\u4e60\u5728\u755c\u7267\u5e94\u7528\u4e2d\u7684\u6548\u679c\u548c\u6700\u4f73\u5fae\u8c03\u7b56\u7565\u5c1a\u4e0d\u660e\u786e\uff0c\u4e14\u6df1\u5ea6\u56fe\u50cf\u4e0e\u70b9\u4e91\u6570\u636e\u5728\u5976\u725b\u4f53\u91cd\u9884\u6d4b\u4e2d\u7684\u76f4\u63a5\u6bd4\u8f83\u6709\u9650\u3002", "method": "\u7814\u7a76\u4ece\u5927\u3001\u4e2d\u3001\u5c0f\u4e09\u4e2a\u519c\u573a\u6536\u96c6\u4e861201\u3001215\u548c58\u5934\u5976\u725b\u7684\u4fef\u89c6\u6df1\u5ea6\u56fe\u50cf\u548c\u70b9\u4e91\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1aConvNeXt\u548cMobileViT\u7528\u4e8e\u6df1\u5ea6\u56fe\u50cf\uff0cPointNet\u548cDGCNN\u7528\u4e8e\u70b9\u4e91\uff0c\u6bd4\u8f83\u4e86\u8fc1\u79fb\u5b66\u4e60\u3001\u5355\u6e90\u5b66\u4e60\u548c\u8054\u5408\u5b66\u4e60\u4e09\u79cd\u5b9e\u9a8c\u8bbe\u8ba1\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u519c\u573a\u7684\u4f53\u91cd\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u6240\u6709\u56db\u79cd\u6a21\u578b\u4e2d\u90fd\u4f18\u4e8e\u5355\u6e90\u5b66\u4e60\uff0c\u4e14\u6548\u679c\u4e0e\u8054\u5408\u5b66\u4e60\u76f8\u5f53\u6216\u66f4\u597d\u3002\u6df1\u5ea6\u56fe\u50cf\u548c\u70b9\u4e91\u6a21\u578b\u4e4b\u95f4\u6ca1\u6709\u4e00\u81f4\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u7279\u522b\u9002\u5408\u6570\u636e\u6709\u9650\u7684\u5c0f\u519c\u573a\u9884\u6d4b\u573a\u666f\uff0c\u56e0\u4e3a\u5b83\u53ea\u9700\u8981\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u800c\u975e\u539f\u59cb\u6570\u636e\uff0c\u907f\u514d\u4e86\u8de8\u519c\u573a\u6570\u636e\u5171\u4eab\u7684\u9690\u79c1\u3001\u7269\u6d41\u6216\u653f\u7b56\u9650\u5236\u95ee\u9898\u3002"}}
{"id": "2601.01050", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.01050", "abs": "https://arxiv.org/abs/2601.01050", "authors": ["Hongming Fu", "Wenjia Wang", "Xiaozhen Qiao", "Shuo Yang", "Zheng Liu", "Bo Zhao"], "title": "EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos", "comment": null, "summary": "We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.", "AI": {"tldr": "EgoGrasp\uff1a\u9996\u4e2a\u4ece\u52a8\u6001\u5355\u76ee\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e2d\u91cd\u5efa\u4e16\u754c\u7a7a\u95f4\u624b\u7269\u4ea4\u4e92\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u52a8\u6001\u548c\u5168\u5c40\u8f68\u8ff9\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3", "motivation": "\u51c6\u786e\u7684\u4e16\u754c\u7a7a\u95f4\u624b\u7269\u4ea4\u4e92\u91cd\u5efa\u5bf9\u4e8e\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u548c\u5b9e\u73b0\u5177\u8eab\u667a\u80fd\u3001\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u56fe\u50cf\u6216\u76f8\u673a\u5750\u6807\u7cfb\uff0c\u65e0\u6cd5\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\u6216\u4e00\u81f4\u7684\u5168\u5c40\u8f68\u8ff9\uff0c\u4e14\u5728\u5267\u70c8\u76f8\u673a\u8fd0\u52a8\u548c\u9891\u7e41\u906e\u6321\u7684\u91ce\u5916\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u591a\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u65b0\u5f00\u53d1\u7684\u7a7a\u95f4\u667a\u80fd\u6a21\u578b\u7684\u9c81\u68d2\u9884\u5904\u7406\u6d41\u7a0b\uff1b2\uff09\u57fa\u4e8e\u89e3\u8026\u6269\u6563\u6a21\u578b\u7684\u5168\u8eab\u624b\u7269\u4ea4\u4e92\u5148\u9a8c\u6a21\u578b\uff08\u6a21\u677f\u65e0\u5173\u4e14\u53ef\u6269\u5c55\u5230\u591a\u7269\u4f53\uff09\uff1b3\uff09\u591a\u76ee\u6807\u6d4b\u8bd5\u65f6\u4f18\u5316\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4e16\u754c\u7a7a\u95f4\u624b\u7269\u4ea4\u4e92\u91cd\u5efa\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "EgoGrasp\u662f\u9996\u4e2a\u80fd\u591f\u4ece\u52a8\u6001\u5355\u76ee\u7b2c\u4e00\u4eba\u79f0\u91ce\u5916\u89c6\u9891\u4e2d\u91cd\u5efa\u4e16\u754c\u7a7a\u95f4\u624b\u7269\u4ea4\u4e92\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u591a\u9636\u6bb5\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u548c\u5b9e\u73b0\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.01609", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01609", "abs": "https://arxiv.org/abs/2601.01609", "authors": ["Albert Sadowski", "Jaros\u0142aw A. Chudziak"], "title": "Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration", "comment": null, "summary": "Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u548c\u7b26\u53f7\u63a8\u7406\u7684\u6846\u67b6\uff1a\u7528LLM\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u6362\u4e3aABox\u65ad\u8a00\uff0c\u518d\u7528SWRL\u63a8\u7406\u5668\u8fdb\u884c\u786e\u5b9a\u6027\u89c4\u5219\u5e94\u7528\uff0c\u5728\u4e09\u4e2a\u9886\u57df\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u4f18\u4e8efew-shot\u63d0\u793a\u3002", "motivation": "\u5728\u9700\u8981\u53ef\u5ba1\u8ba1\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u7684\u9886\u57df\uff08\u5982\u4e34\u5e8a\u534f\u8bae\u3001\u6cd5\u5f8b\u8bc1\u636e\u89c4\u5219\u3001\u79d1\u5b66\u6807\u51c6\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1aLLM\u5177\u6709\u7075\u6d3b\u6027\u4f46\u65e0\u6cd5\u4fdd\u8bc1\u89c4\u5219\u5e94\u7528\u7684\u4e00\u81f4\u6027\uff0c\u7b26\u53f7\u7cfb\u7edf\u80fd\u63d0\u4f9b\u5f62\u5f0f\u5316\u4fdd\u8bc1\u4f46\u9700\u8981\u7ed3\u6784\u5316\u8f93\u5165\u3002", "method": "\u63d0\u51fa\u96c6\u6210\u6a21\u5f0f\uff1aLLM\u4f5c\u4e3a\u672c\u4f53\u586b\u5145\u5f15\u64ce\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u6362\u4e3a\u57fa\u4e8e\u4e13\u5bb6\u7f16\u5199TBox\u89c4\u8303\u7684ABox\u65ad\u8a00\uff0cSWRL\u63a8\u7406\u5668\u63d0\u4f9b\u786e\u5b9a\u6027\u89c4\u5219\u5e94\u7528\u4fdd\u8bc1\u3002\u6846\u67b6\u5c06\u63a8\u7406\u5206\u89e3\u4e3a\u5b9e\u4f53\u8bc6\u522b\u3001\u65ad\u8a00\u63d0\u53d6\u548c\u7b26\u53f7\u9a8c\u8bc1\u4e09\u4e2a\u6b65\u9aa4\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\uff08\u6cd5\u5f8b\u4f20\u95fb\u8bc1\u636e\u5224\u5b9a\u3001\u79d1\u5b66\u65b9\u6cd5\u4efb\u52a1\u5e94\u7528\u3001\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\uff09\u548c11\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u6784\u5316\u5206\u89e3\u5728\u603b\u4f53\u4e0a\u6bd4few-shot\u63d0\u793a\u6709\u7edf\u8ba1\u663e\u8457\u6539\u8fdb\uff0c\u4e09\u4e2a\u9886\u57df\u90fd\u89c2\u5bdf\u5230\u589e\u76ca\u3002\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u7b26\u53f7\u9a8c\u8bc1\u6bd4\u5355\u7eaf\u7ed3\u6784\u5316\u63d0\u793a\u6709\u5b9e\u8d28\u597d\u5904\u3002", "conclusion": "\u8be5\u6846\u67b6\u7ed3\u5408\u4e86LLM\u7684\u7075\u6d3b\u6027\u548c\u7b26\u53f7\u63a8\u7406\u7684\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u586b\u5145\u7684ABox\u53ef\u4e0e\u6807\u51c6\u8bed\u4e49\u7f51\u5de5\u5177\u96c6\u6210\uff0c\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u4e3a\u9700\u8981\u53ef\u5ba1\u8ba1\u51b3\u7b56\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01718", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01718", "abs": "https://arxiv.org/abs/2601.01718", "authors": ["YuanLab. ai", ":", "Shawn Wu", "Sean Wang", "Louie Li", "Darcy Chen", "Allen Wang", "Jiangang Luo", "Xudong Zhao", "Joseph Shen", "Gawain Ma", "Jasper Jia", "Marcus Mao", "Claire Wang", "Hunter He", "Carol Wang", "Zera Zhang", "Jason Wang", "Chonly Shen", "Leo Zhang", "Logan Chen", "Qasim Meng", "James Gong", "Danied Zhao", "Penn Zheng", "Owen Zhu", "Tong Yu"], "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications", "comment": null, "summary": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.", "AI": {"tldr": "Yuan3.0 Flash\u662f\u4e00\u4e2a\u5f00\u6e90\u7684MoE\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u62e5\u670937\u4ebf\u6fc0\u6d3b\u53c2\u6570\u548c400\u4ebf\u603b\u53c2\u6570\uff0c\u4e13\u4e3a\u4f01\u4e1a\u4efb\u52a1\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u4efb\u52a1\u7ade\u4e89\u529b\u3002\u901a\u8fc7RAPO\u7b97\u6cd5\u89e3\u51b3\u5927\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u4f01\u4e1a\u5bfc\u5411\u4efb\u52a1\uff08\u5982RAG\u3001\u590d\u6742\u8868\u683c\u7406\u89e3\u3001\u6458\u8981\uff09\u7684\u6027\u80fd\u63d0\u5347\u9700\u6c42\uff0c\u540c\u65f6\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u5e38\u89c1\u7684\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u5f00\u53d1\u4e00\u4e2a\u65e2\u9ad8\u6548\u53c8\u5177\u5907\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684\u5f00\u6e90\u6a21\u578b\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u63d0\u51fa\u53cd\u5c04\u611f\u77e5\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\uff08RAPO\uff09\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b97\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u8c03\u8282\u8fc7\u5ea6\u601d\u8003\u884c\u4e3a\u3002", "result": "\u5728\u4f01\u4e1a\u5bfc\u5411\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u6570\u5b66\u3001\u79d1\u5b66\u7b49\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u5f3a\u5927\uff0c\u8fbe\u5230\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4ec5\u9700\u7ea61/4\u52301/2\u7684\u5e73\u5747token\u6570\u91cf\u3002", "conclusion": "Yuan3.0 Flash\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u4f01\u4e1a\u5bfc\u5411\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7RAPO\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u4f01\u4e1a\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5df2\u5b8c\u5168\u5f00\u6e90\u4f9b\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2601.00942", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00942", "abs": "https://arxiv.org/abs/2601.00942", "authors": ["Kabir Grover"], "title": "Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures", "comment": null, "summary": "The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.", "AI": {"tldr": "\u7a00\u758fMoE\u67b6\u6784\u5728\u968f\u673a\u89e3\u7801\u4e0b\u7684\u53ef\u9760\u6027\u7814\u7a76\u8868\u660e\uff0c\u6307\u4ee4\u5fae\u8c03\u800c\u975e\u67b6\u6784\u7a00\u758f\u6027\u662f\u51b3\u5b9a\u6a21\u578b\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u9c81\u68d2\u6027\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "motivation": "\u968f\u7740\u7a00\u758fMoE\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u7814\u7a76\u5176\u5728\u968f\u673a\u89e3\u7801\u4e0b\u7684\u53ef\u9760\u6027\u3002\u867d\u7136\u6761\u4ef6\u8ba1\u7b97\u5e26\u6765\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u63d0\u5347\uff0c\u4f46\u7a00\u758f\u8def\u7531\u4e0e\u57fa\u4e8e\u6e29\u5ea6\u7684\u91c7\u6837\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u662f\u5426\u4f1a\u635f\u5bb3\u8f93\u51fa\u7a33\u5b9a\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u8bc4\u4f30\u4e09\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\uff1aOLMoE-7B\uff08\u7a00\u758f\u57fa\u7840\u6a21\u578b\uff09\u3001Mixtral-8x7B\uff08\u7a00\u758f\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff09\u548cQwen2.5-3B\uff08\u5bc6\u96c6\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff09\u3002\u5728\u786e\u5b9a\u6027\u7b97\u672f\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6db5\u76d6\u56db\u79cd\u89e3\u7801\u914d\u7f6e\uff08\u4ece\u8d2a\u5a6a\u89e3\u7801\u5230T=1.0\uff09\uff0c\u8bc4\u4f30\u51c6\u786e\u6027\u3001\u683c\u5f0f\u5408\u89c4\u6027\u3001\u91cd\u590d\u751f\u6210\u7684\u4e00\u81f4\u6027\u548c\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u603b\u8ba19,360\u6b21\u6a21\u578b\u751f\u6210\u3002", "result": "\u7a00\u758f\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5728\u6240\u6709\u89e3\u7801\u6e29\u5ea6\u4e0b\u8868\u73b0\u51fa\u4e0e\u5bc6\u96c6\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u76f8\u5f53\u7684\u7a33\u5b9a\u6027\uff0c\u800c\u7a00\u758f\u57fa\u7840\u6a21\u578b\u968f\u7740\u6e29\u5ea6\u5347\u9ad8\u51fa\u73b0\u7cfb\u7edf\u6027\u6027\u80fd\u4e0b\u964d\u3002\u8fd9\u8868\u660e\u6307\u4ee4\u5fae\u8c03\u800c\u975e\u67b6\u6784\u7a00\u758f\u6027\u662f\u51b3\u5b9a\u6a21\u578b\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e2d\u5bf9\u89e3\u7801\u968f\u673a\u6027\u9c81\u68d2\u6027\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u662f\u786e\u4fdd\u7a00\u758f\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u9760\u6027\u5173\u952e\u5e94\u7528\u4e2d\u7a33\u5b9a\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u7a00\u758f\u67b6\u6784\u53ef\u4ee5\u88ab\u5b89\u5168\u91c7\u7528\u800c\u4e0d\u727a\u7272\u8f93\u51fa\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.01064", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01064", "abs": "https://arxiv.org/abs/2601.01064", "authors": ["Jianan Li", "Wangcai Zhao", "Tingfa Xu"], "title": "Efficient Hyperspectral Image Reconstruction Using Lightweight Separate Spectral Transformers", "comment": null, "summary": "Hyperspectral imaging (HSI) is essential across various disciplines for its capacity to capture rich spectral information. However, efficiently reconstructing hyperspectral images from compressive sensing measurements presents significant challenges. To tackle these, we adopt a divide-and-conquer strategy that capitalizes on the unique spectral and spatial characteristics of hyperspectral images. We introduce the Lightweight Separate Spectral Transformer (LSST), an innovative architecture tailored for efficient hyperspectral image reconstruction. This architecture consists of Separate Spectral Transformer Blocks (SSTB) for modeling spectral relationships and Lightweight Spatial Convolution Blocks (LSCB) for spatial processing. The SSTB employs Grouped Spectral Self-attention and a Spectrum Shuffle operation to effectively manage both local and non-local spectral relationships. Simultaneously, the LSCB utilizes depth-wise separable convolutions and strategic ordering to enhance spatial information processing. Furthermore, we implement the Focal Spectrum Loss, a novel loss weighting mechanism that dynamically adjusts during training to improve reconstruction across spectrally complex bands. Extensive testing demonstrates that our LSST achieves superior performance while requiring fewer FLOPs and parameters, underscoring its efficiency and effectiveness. The source code is available at: https://github.com/wcz1124/LSST.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u79bb\u5149\u8c31\u53d8\u6362\u5668\uff08LSST\uff09\u7528\u4e8e\u9ad8\u6548\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\uff0c\u91c7\u7528\u5206\u800c\u6cbb\u4e4b\u7b56\u7565\u5904\u7406\u5149\u8c31\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u91cf\u548c\u53c2\u6570\u7684\u540c\u65f6\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u9ad8\u5149\u8c31\u6210\u50cf\u5728\u591a\u4e2a\u9886\u57df\u90fd\u5f88\u91cd\u8981\uff0c\u4f46\u4ece\u538b\u7f29\u611f\u77e5\u6d4b\u91cf\u4e2d\u9ad8\u6548\u91cd\u5efa\u9ad8\u5149\u8c31\u56fe\u50cf\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u9700\u8981\u5229\u7528\u9ad8\u5149\u8c31\u56fe\u50cf\u72ec\u7279\u7684\u5149\u8c31\u548c\u7a7a\u95f4\u7279\u6027\u6765\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u800c\u6cbb\u4e4b\u7b56\u7565\uff0c\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5206\u79bb\u5149\u8c31\u53d8\u6362\u5668\uff08LSST\uff09\u67b6\u6784\uff0c\u5305\u542b\u5206\u79bb\u5149\u8c31\u53d8\u6362\u5757\uff08SSTB\uff09\u7528\u4e8e\u5efa\u6a21\u5149\u8c31\u5173\u7cfb\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u5377\u79ef\u5757\uff08LSCB\uff09\u7528\u4e8e\u7a7a\u95f4\u5904\u7406\u3002SSTB\u4f7f\u7528\u5206\u7ec4\u5149\u8c31\u81ea\u6ce8\u610f\u529b\u548c\u5149\u8c31\u6d17\u724c\u64cd\u4f5c\uff0cLSCB\u4f7f\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u548c\u7b56\u7565\u6027\u6392\u5e8f\u3002\u8fd8\u5f15\u5165\u4e86\u7126\u70b9\u5149\u8c31\u635f\u5931\uff0c\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6743\u91cd\u7684\u635f\u5931\u673a\u5236\u3002", "result": "\u5927\u91cf\u6d4b\u8bd5\u8868\u660e\uff0cLSST\u5728\u51cf\u5c11FLOPs\u548c\u53c2\u6570\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "conclusion": "LSST\u67b6\u6784\u901a\u8fc7\u6709\u6548\u5904\u7406\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u5149\u8c31\u548c\u7a7a\u95f4\u7279\u6027\uff0c\u5728\u9ad8\u6548\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u538b\u7f29\u611f\u77e5\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01743", "abs": "https://arxiv.org/abs/2601.01743", "authors": ["Bin Xu"], "title": "AI Agent Systems: Architectures, Applications, and Evaluation", "comment": null, "summary": "AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\\ multi-agent; centralized vs.\\ decentralized coordination), and deployment settings (offline analysis vs.\\ online interactive assistance; safety-critical vs.\\ open-ended tasks). We discuss key design trade-offs -- latency vs.\\ accuracy, autonomy vs.\\ controllability, and capability vs.\\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9AI\u667a\u80fd\u4f53\u67b6\u6784\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6db5\u76d6\u63a8\u7406\u3001\u89c4\u5212\u3001\u5de5\u5177\u8c03\u7528\u7b49\u6838\u5fc3\u7ec4\u4ef6\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u6743\u8861\u3001\u8bc4\u4f30\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u4e0e\u63a8\u7406\u3001\u89c4\u5212\u3001\u8bb0\u5fc6\u548c\u5de5\u5177\u4f7f\u7528\u76f8\u7ed3\u5408\uff0cAI\u667a\u80fd\u4f53\u6b63\u6210\u4e3a\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u4e0e\u73b0\u5b9e\u4e16\u754c\u8ba1\u7b97\u7684\u5b9e\u7528\u63a5\u53e3\u3002\u9700\u8981\u5bf9\u8fd9\u4e9b\u65b0\u5174\u7684\u667a\u80fd\u4f53\u67b6\u6784\u8fdb\u884c\u7cfb\u7edf\u6027\u6574\u7406\u548c\u5206\u7c7b\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u5de5\u4f5c\u7ec4\u7ec7\u6210\u7edf\u4e00\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d6\uff1a1\uff09\u667a\u80fd\u4f53\u7ec4\u4ef6\uff08\u7b56\u7565/LLM\u6838\u5fc3\u3001\u8bb0\u5fc6\u3001\u4e16\u754c\u6a21\u578b\u3001\u89c4\u5212\u5668\u3001\u5de5\u5177\u8def\u7531\u5668\u3001\u6279\u8bc4\u5668\uff09\uff1b2\uff09\u7f16\u6392\u6a21\u5f0f\uff08\u5355\u667a\u80fd\u4f53vs.\u591a\u667a\u80fd\u4f53\uff0c\u96c6\u4e2d\u5f0fvs.\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\uff09\uff1b3\uff09\u90e8\u7f72\u8bbe\u7f6e\uff08\u79bb\u7ebf\u5206\u6790vs.\u5728\u7ebf\u4ea4\u4e92\u8f85\u52a9\uff0c\u5b89\u5168\u5173\u952evs.\u5f00\u653e\u4efb\u52a1\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684AI\u667a\u80fd\u4f53\u67b6\u6784\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u8bbe\u8ba1\u6743\u8861\uff08\u5ef6\u8fdfvs.\u51c6\u786e\u6027\u3001\u81ea\u4e3b\u6027vs.\u53ef\u63a7\u6027\u3001\u80fd\u529bvs.\u53ef\u9760\u6027\uff09\uff0c\u603b\u7ed3\u4e86\u8bc4\u4f30\u5b9e\u8df5\uff08\u4efb\u52a1\u5957\u4ef6\u3001\u4eba\u7c7b\u504f\u597d\u6307\u6807\u3001\u7ea6\u675f\u4e0b\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u8bc4\u4f30\u9762\u4e34\u7684\u6311\u6218\uff08\u975e\u786e\u5b9a\u6027\u3001\u957f\u671f\u4fe1\u7528\u5206\u914d\u3001\u5de5\u5177\u548c\u73af\u5883\u53d8\u5f02\u6027\u3001\u9690\u85cf\u6210\u672c\uff09\u3002", "conclusion": "AI\u667a\u80fd\u4f53\u67b6\u6784\u7814\u7a76\u4ecd\u9762\u4e34\u8bf8\u591a\u5f00\u653e\u6311\u6218\uff0c\u5305\u62ec\u5de5\u5177\u52a8\u4f5c\u7684\u9a8c\u8bc1\u548c\u5b89\u5168\u62a4\u680f\u3001\u53ef\u6269\u5c55\u7684\u8bb0\u5fc6\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u667a\u80fd\u4f53\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u53ca\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u53ef\u91cd\u590d\u8bc4\u4f30\u3002\u9700\u8981\u5728\u8fd9\u4e9b\u65b9\u5411\u4e0a\u8fdb\u4e00\u6b65\u63a8\u8fdb\u7814\u7a76\u3002"}}
{"id": "2601.01084", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01084", "abs": "https://arxiv.org/abs/2601.01084", "authors": ["Adari Rama Sukanya", "Puvvula Roopesh Naga Sri Sai", "Kota Moses", "Rimalapudi Sarvendranath"], "title": "A UAV-Based Multispectral and RGB Dataset for Multi-Stage Paddy Crop Monitoring in Indian Agricultural Fields", "comment": "10-page dataset explanation paper", "summary": "We present a large-scale unmanned aerial vehicle (UAV)-based RGB and multispectral image dataset collected over paddy fields in the Vijayawada region, Andhra Pradesh, India, covering nursery to harvesting stages. We used a 20-megapixel RGB camera and a 5-megapixel four-band multispectral camera capturing red, green, red-edge, and near-infrared bands. Standardised operating procedure (SOP) and checklists were developed to ensure repeatable data acquisition. Our dataset comprises of 42,430 raw images (415 GB) captured over 5 acres with 1 cm/pixel ground sampling distance (GSD) with associated metadata such as GPS coordinates, flight altitude, and environmental conditions. Captured images were validated using Pix4D Fields to generate orthomosaic maps and vegetation index maps, such as normalised difference vegetation index (NDVI) and normalised difference red-edge (NDRE) index. Our dataset is one of the few datasets that provide high-resolution images with rich metadata that cover all growth stages of Indian paddy crops. The dataset is available on IEEE DataPort with DOI, . It can support studies on targeted spraying, disease analysis, and yield estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u5927\u89c4\u6a21\u6c34\u7a3b\u7530RGB\u548c\u591a\u5149\u8c31\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8986\u76d6\u4e86\u4ece\u80b2\u82d7\u5230\u6536\u83b7\u7684\u6240\u6709\u751f\u957f\u9636\u6bb5\uff0c\u5305\u542b\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u4e30\u5bcc\u5143\u6570\u636e\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u8986\u76d6\u5370\u5ea6\u6c34\u7a3b\u4f5c\u7269\u6240\u6709\u751f\u957f\u9636\u6bb5\u7684\u9ad8\u5206\u8fa8\u7387\u65e0\u4eba\u673a\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u9700\u8981\u8fd9\u6837\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u7cbe\u51c6\u519c\u4e1a\u5e94\u7528\uff0c\u5982\u9776\u5411\u55b7\u6d12\u3001\u75c5\u5bb3\u5206\u6790\u548c\u4ea7\u91cf\u4f30\u7b97\u3002", "method": "\u4f7f\u752820\u5146\u50cf\u7d20RGB\u76f8\u673a\u548c5\u5146\u50cf\u7d20\u56db\u6ce2\u6bb5\u591a\u5149\u8c31\u76f8\u673a\uff08\u7ea2\u3001\u7eff\u3001\u7ea2\u8fb9\u3001\u8fd1\u7ea2\u5916\uff09\u91c7\u96c6\u6570\u636e\uff0c\u5236\u5b9a\u4e86\u6807\u51c6\u5316\u64cd\u4f5c\u7a0b\u5e8f\u548c\u68c0\u67e5\u6e05\u5355\u786e\u4fdd\u6570\u636e\u91c7\u96c6\u7684\u53ef\u91cd\u590d\u6027\uff0c\u8986\u76d65\u82f1\u4ea9\u571f\u5730\uff0c\u5730\u9762\u91c7\u6837\u8ddd\u79bb\u4e3a1\u5398\u7c73/\u50cf\u7d20\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b42,430\u5f20\u539f\u59cb\u56fe\u50cf\uff08415GB\uff09\u7684\u6570\u636e\u96c6\uff0c\u9644\u5e26GPS\u5750\u6807\u3001\u98de\u884c\u9ad8\u5ea6\u548c\u73af\u5883\u6761\u4ef6\u7b49\u5143\u6570\u636e\uff0c\u4f7f\u7528Pix4D Fields\u9a8c\u8bc1\u5e76\u751f\u6210\u6b63\u5c04\u5f71\u50cf\u56fe\u548c\u690d\u88ab\u6307\u6570\u56fe\uff08NDVI\u548cNDRE\uff09\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u662f\u5c11\u6570\u51e0\u4e2a\u63d0\u4f9b\u8986\u76d6\u5370\u5ea6\u6c34\u7a3b\u4f5c\u7269\u6240\u6709\u751f\u957f\u9636\u6bb5\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u4e30\u5bcc\u5143\u6570\u636e\u7684\u8d44\u6e90\u4e4b\u4e00\uff0c\u5df2\u5728IEEE DataPort\u4e0a\u516c\u5f00\uff0c\u53ef\u7528\u4e8e\u7cbe\u51c6\u519c\u4e1a\u7814\u7a76\u3002"}}
{"id": "2601.01765", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01765", "abs": "https://arxiv.org/abs/2601.01765", "authors": ["Yao Lu", "Shang Liu", "Hangan Zhou", "Wenji Fang", "Qijun Zhang", "Zhiyao Xie"], "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization", "comment": null, "summary": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.", "AI": {"tldr": "RTL-OPT\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728RTL\u4ee3\u7801\u4f18\u5316\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b36\u4e2a\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u5b57\u7535\u8def\u8bbe\u8ba1\uff0c\u8986\u76d6\u591a\u79cd\u5b9e\u73b0\u7c7b\u522b\uff0c\u5e76\u63d0\u4f9b\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u548c\u91cf\u5316PPA\u6539\u8fdb\u3002", "motivation": "\u5f53\u524dAI\u5728\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u5173\u6ce8RTL\u4ee3\u7801\u751f\u6210\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u8bc4\u4f30\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u529f\u8017\u3001\u6027\u80fd\u548c\u9762\u79ef\uff08PPA\uff09\u4f18\u5316\u8d28\u91cf\u7684\u8bc4\u4f30\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30LLM\u5728RTL\u4f18\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efaRTL-OPT\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b36\u4e2a\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u5b57\u7535\u8def\u8bbe\u8ba1\uff0c\u8986\u76d6\u7ec4\u5408\u903b\u8f91\u3001\u6d41\u6c34\u7ebf\u6570\u636e\u901a\u8def\u3001\u6709\u9650\u72b6\u6001\u673a\u548c\u5b58\u50a8\u5668\u63a5\u53e3\u7b49\u7c7b\u522b\u3002\u6bcf\u4e2a\u4efb\u52a1\u63d0\u4f9b\u4e00\u5bf9RTL\u4ee3\u7801\uff1a\u6b21\u4f18\u7248\u672c\u548c\u4eba\u5de5\u4f18\u5316\u7684\u53c2\u8003\u7248\u672c\uff0c\u540e\u8005\u4f53\u73b0\u4e86\u884c\u4e1a\u9a8c\u8bc1\u7684\u4f18\u5316\u6a21\u5f0f\u3002\u540c\u65f6\u96c6\u6210\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u9a8c\u8bc1\u529f\u80fd\u6b63\u786e\u6027\u548c\u91cf\u5316PPA\u6539\u8fdb\u3002", "result": "RTL-OPT\u57fa\u51c6\u6d4b\u8bd5\u6210\u529f\u5efa\u7acb\uff0c\u80fd\u591f\u6807\u51c6\u5316\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u5728\u786c\u4ef6\u8bbe\u8ba1\u4f18\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u4f53\u7cfb\u4e2d\u5bf9PPA\u4f18\u5316\u8d28\u91cf\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "conclusion": "RTL-OPT\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728RTL\u4ee3\u7801\u4f18\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u3001\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AI\u5728\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u4f18\u5316\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.00968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00968", "abs": "https://arxiv.org/abs/2601.00968", "authors": ["Longwei Wang", "Mohammad Navid Nayyem", "Abdullah Al Rakin", "KC Santosh", "Chaowei Zhang", "Yang Zhou"], "title": "Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks", "comment": "8pages,4 figures", "summary": "The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLIME\u89e3\u91ca\u7684\u4e3b\u52a8\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6291\u5236\u865a\u5047\u7279\u5f81\u6765\u540c\u65f6\u63d0\u5347\u6a21\u578b\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u5728\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u540c\u65f6\u5177\u5907\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u51b3\u7b56\u900f\u660e\u5ea6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7LIME\u8bc6\u522b\u7684\u865a\u5047\u3001\u4e0d\u7a33\u5b9a\u6216\u8bed\u4e49\u65e0\u5173\u7279\u5f81\u4f1a\u663e\u8457\u589e\u52a0\u5bf9\u6297\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u5f52\u56e0\u5f15\u5bfc\u7684\u7cbe\u70bc\u6846\u67b6\uff0c\u5c06LIME\u4ece\u88ab\u52a8\u8bca\u65ad\u5de5\u5177\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u8bad\u7ec3\u4fe1\u53f7\u3002\u901a\u8fc7\u7279\u5f81\u63a9\u853d\u3001\u654f\u611f\u611f\u77e5\u6b63\u5219\u5316\u548c\u5bf9\u6297\u589e\u5f3a\u7684\u95ed\u73af\u7cbe\u70bc\u6d41\u7a0b\uff0c\u7cfb\u7edf\u6027\u5730\u6291\u5236\u865a\u5047\u7279\u5f81\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-10-C\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u96c6\u6216\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "\u5efa\u7acb\u4e86\u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u901a\u8fc7\u4e3b\u52a8\u5229\u7528\u89e3\u91ca\u4fe1\u606f\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2601.01085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01085", "abs": "https://arxiv.org/abs/2601.01085", "authors": ["Jiayi Xu", "Zhang Zhang", "Yuanrui Zhang", "Ruitao Chen", "Yixian Xu", "Tianyu He", "Di He"], "title": "Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models", "comment": null, "summary": "In this paper, we introduce \\emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \\emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.", "AI": {"tldr": "Luminark\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6982\u7387\u8ba4\u8bc1\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u901a\u7528\u89c6\u89c9\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u5757\u7ea7\u4eae\u5ea6\u7edf\u8ba1\u5b9e\u73b0\u53ef\u8ba4\u8bc1\u68c0\u6d4b\uff0c\u901a\u8fc7\u6c34\u5370\u5f15\u5bfc\u6280\u672f\u5b9e\u73b0\u8de8\u6a21\u578b\u517c\u5bb9", "motivation": "\u4e3a\u89c6\u89c9\u751f\u6210\u6a21\u578b\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u65e2\u80fd\u4fdd\u62a4\u6a21\u578b\u8f93\u51fa\u7248\u6743\uff0c\u53c8\u4e0d\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u8ba4\u8bc1\u7684\u68c0\u6d4b\u4fdd\u8bc1", "method": "\u57fa\u4e8e\u5757\u7ea7\u4eae\u5ea6\u7edf\u8ba1\u5b9a\u4e49\u6c34\u5370\uff0c\u9884\u5b9a\u4e49\u4e8c\u8fdb\u5236\u6a21\u5f0f\u548c\u76f8\u5e94\u9608\u503c\uff1b\u5229\u7528\u5e7f\u6cdb\u91c7\u7528\u7684\u5f15\u5bfc\u6280\u672f\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u673a\u5236\uff0c\u5f00\u53d1\u6c34\u5370\u5f15\u5bfc\uff1b\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u63a7\u5236\u8bef\u62a5\u7387", "result": "\u5728\u6269\u6563\u3001\u81ea\u56de\u5f52\u548c\u6df7\u5408\u6846\u67b6\u7b499\u4e2a\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cLuminark\u59cb\u7ec8\u8868\u73b0\u51fa\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3001\u5bf9\u5e38\u89c1\u56fe\u50cf\u53d8\u6362\u7684\u5f3a\u9c81\u68d2\u6027\u4ee5\u53ca\u826f\u597d\u7684\u89c6\u89c9\u8d28\u91cf", "conclusion": "Luminark\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u3001\u6982\u7387\u8ba4\u8bc1\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u5177\u6709\u8de8\u6a21\u578b\u901a\u7528\u6027\uff0c\u4e0d\u635f\u5bb3\u56fe\u50cf\u8d28\u91cf\uff0c\u4e3a\u89c6\u89c9\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7248\u6743\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01088", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01088", "abs": "https://arxiv.org/abs/2601.01088", "authors": ["Haq Nawaz Malik"], "title": "600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script", "comment": null, "summary": "This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.", "AI": {"tldr": "\u8be5\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86600K-KS-OCR\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u7ea660.2\u4e07\u4e2a\u8bcd\u7ea7\u5206\u5272\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u5408\u6210\u8bed\u6599\u5e93\uff0c\u4e13\u95e8\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u9488\u5bf9\u514b\u4ec0\u7c73\u5c14\u6587\u5b57\u7684OCR\u7cfb\u7edf\u3002", "motivation": "\u514b\u4ec0\u7c73\u5c14\u8bed\u662f\u4e00\u79cd\u6fd2\u5371\u7684\u8fbe\u5c14\u5fb7\u8bed\u7cfb\u8bed\u8a00\uff0c\u4f7f\u7528\u6539\u826f\u7684\u6ce2\u65af-\u963f\u62c9\u4f2f\u6587\u5b57\u7cfb\u7edf\uff0c\u7ea6\u6709700\u4e07\u4eba\u4f7f\u7528\u3002\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u514b\u4ec0\u7c73\u5c14\u6587\u5b57\u7684OCR\u8bad\u7ec3\u8d44\u6e90\uff0c\u8be5\u6570\u636e\u96c6\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u5173\u952e\u8d44\u6e90\u7a7a\u767d\u3002", "method": "\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u5305\u62ec\uff1a\u4f7f\u7528\u4e09\u79cd\u4f20\u7edf\u514b\u4ec0\u7c73\u5c14\u5b57\u4f53\u6e32\u67d3\u56fe\u50cf\uff08256x64\u50cf\u7d20\uff09\uff1b\u5b9e\u65bd\u5168\u9762\u7684\u6570\u636e\u589e\u5f3a\u6a21\u62df\u771f\u5b9e\u6587\u6863\u9000\u5316\uff1b\u91c7\u7528\u591a\u6837\u5316\u80cc\u666f\u7eb9\u7406\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff1b\u63d0\u4f9b\u591a\u79cd\u683c\u5f0f\u7684\u771f\u5b9e\u6807\u6ce8\u4ee5\u517c\u5bb9CRNN\u3001TrOCR\u7b49\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u7ea6602,000\u4e2a\u8bcd\u7ea7\u5206\u5272\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5206\u5e03\u572810\u4e2a\u5206\u533a\u5b58\u6863\u4e2d\uff0c\u603b\u5927\u5c0f\u7ea610.6 GB\u3002\u6570\u636e\u96c6\u91c7\u7528CC-BY-4.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u4fbf\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00OCR\u7814\u7a76\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u514b\u4ec0\u7c73\u5c14\u8bedOCR\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u6fd2\u5371\u8bed\u8a00\u7684\u6280\u672f\u4fdd\u62a4\u548c\u7814\u7a76\uff0c\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00OCR\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.01003", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01003", "abs": "https://arxiv.org/abs/2601.01003", "authors": ["Amin Abyaneh", "Charlotte Morissette", "Mohamad H. Danesh", "Anas El Houssaini", "David Meger", "Gregory Dudek", "Hsiu-Chin Lin"], "title": "Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations", "comment": "Under review at ICLR 2026", "summary": "Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6536\u7f29\u6269\u6563\u7b56\u7565\uff08CDPs\uff09\uff0c\u901a\u8fc7\u5728\u6269\u6563\u91c7\u6837\u52a8\u529b\u5b66\u4e2d\u5f15\u5165\u6536\u7f29\u884c\u4e3a\u6765\u589e\u5f3a\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u6c42\u89e3\u5668\u548c\u5206\u6570\u5339\u914d\u8bef\u5dee\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5176\u57fa\u4e8e\u5206\u6570\u7684SDE\u5efa\u6a21\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u4f1a\u7d2f\u79ef\u6c42\u89e3\u5668\u8bef\u5dee\u3001\u5206\u6570\u5339\u914d\u8bef\u5dee\uff0c\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u5e76\u5bfc\u81f4\u52a8\u4f5c\u751f\u6210\u4e0d\u4e00\u81f4\u3002\u8fd9\u4e9b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u4e0d\u592a\u5173\u952e\u7684\u95ee\u9898\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u4f1a\u7d2f\u79ef\u5bfc\u81f4\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u6536\u7f29\u6269\u6563\u7b56\u7565\uff08CDPs\uff09\uff0c\u5728\u6269\u6563\u91c7\u6837\u52a8\u529b\u5b66\u4e2d\u5f15\u5165\u6536\u7f29\u884c\u4e3a\uff0c\u4f7f\u76f8\u90bb\u6d41\u7ebf\u76f8\u4e92\u9760\u8fd1\uff0c\u4ece\u800c\u589e\u5f3a\u5bf9\u6c42\u89e3\u5668\u548c\u5206\u6570\u5339\u914d\u8bef\u5dee\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u52a8\u4f5c\u65b9\u5dee\u3002\u63d0\u4f9b\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8df5\u5b9e\u73b0\u65b9\u6848\uff0c\u53ef\u6700\u5c0f\u4fee\u6539\u5730\u96c6\u6210\u5230\u73b0\u6709\u6269\u6563\u7b56\u7565\u67b6\u6784\u4e2d\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cCDPs\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901a\u5e38\u4f18\u4e8e\u57fa\u7ebf\u7b56\u7565\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u660e\u663e\u7684\u4f18\u52bf\u3002", "conclusion": "\u6536\u7f29\u6269\u6563\u7b56\u7565\u901a\u8fc7\u5f15\u5165\u6536\u7f29\u884c\u4e3a\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2601.01095", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01095", "abs": "https://arxiv.org/abs/2601.01095", "authors": ["Hyeonjeong Ha", "Jinjin Ge", "Bo Feng", "Kaixin Ma", "Gargi Chakraborty"], "title": "NarrativeTrack: Evaluating Video Language Models Beyond the Frame", "comment": "VideoLLM Fine-Grained Evaluation", "summary": "Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.", "AI": {"tldr": "NarrativeTrack\u662f\u9996\u4e2a\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u4e2d\u5fc3\u63a8\u7406\u8bc4\u4f30MLLMs\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u4f7f\u7528\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6CRP\u9010\u6b65\u589e\u52a0\u53d9\u4e8b\u590d\u6742\u6027\uff0c\u63ed\u793aMLLMs\u5728\u5b9e\u4f53\u8ffd\u8e2a\u548c\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5bf9\u89c6\u9891\u4e2d\u968f\u65f6\u95f4\u5c55\u5f00\u7684\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u771f\u6b63\u7684\u53d9\u4e8b\u7406\u89e3\u9700\u8981\u6a21\u578b\u80fd\u591f\u8ffd\u8e2a\"\u8c01\u5728\u4f55\u65f6\u4f55\u5730\u505a\u4ec0\u4e48\"\uff0c\u5e76\u5728\u52a8\u6001\u89c6\u89c9\u548c\u65f6\u5e8f\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u8fde\u8d2f\u7684\u5b9e\u4f53\u8868\u793a\u3002", "method": "\u5f15\u5165NarrativeTrack\u57fa\u51c6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u4e2d\u5fc3\u63a8\u7406\u8bc4\u4f30\u53d9\u4e8b\u7406\u89e3\u3002\u91c7\u7528\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6CRP\uff08\u7ec4\u5408\u63a8\u7406\u8fdb\u5c55\uff09\uff0c\u5728\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u9010\u6b65\u589e\u52a0\u53d9\u4e8b\u590d\u6742\u6027\uff1a\u5b9e\u4f53\u5b58\u5728\u6027\u3001\u5b9e\u4f53\u53d8\u5316\u548c\u5b9e\u4f53\u6a21\u7cca\u6027\u3002\u4f7f\u7528\u5168\u81ea\u52a8\u5b9e\u4f53\u4e2d\u5fc3\u7ba1\u9053\u63d0\u53d6\u65f6\u5e8f\u57fa\u7840\u5b9e\u4f53\u8868\u793a\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u6700\u5148\u8fdb\u7684MLLMs\u5728\u89c6\u89c9\u8f6c\u6362\u548c\u65f6\u5e8f\u52a8\u6001\u4e2d\u65e0\u6cd5\u7a33\u5065\u8ffd\u8e2a\u5b9e\u4f53\uff0c\u7ecf\u5e38\u5728\u4e0a\u4e0b\u6587\u53d8\u5316\u65f6\u4ea7\u751f\u8eab\u4efd\u5e7b\u89c9\u3002\u5f00\u6e90\u901a\u7528MLLMs\u8868\u73b0\u51fa\u5f3a\u611f\u77e5\u57fa\u7840\u4f46\u5f31\u65f6\u5e8f\u8fde\u8d2f\u6027\uff0c\u800c\u89c6\u9891\u4e13\u7528MLLMs\u80fd\u6355\u6349\u65f6\u5e8f\u4e0a\u4e0b\u6587\u4f46\u5e7b\u89c9\u5b9e\u4f53\u4e0a\u4e0b\u6587\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u611f\u77e5\u57fa\u7840\u548c\u65f6\u5e8f\u63a8\u7406\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002", "conclusion": "\u53d9\u4e8b\u7406\u89e3\u4ec5\u4ece\u611f\u77e5\u57fa\u7840\u548c\u65f6\u5e8f\u63a8\u7406\u7684\u6574\u5408\u4e2d\u4ea7\u751f\u3002NarrativeTrack\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u5316\u6846\u67b6\u6765\u8bca\u65ad\u548c\u63a8\u8fdbMLLMs\u4e2d\u65f6\u5e8f\u57fa\u7840\u7684\u53d9\u4e8b\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2601.01816", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01816", "abs": "https://arxiv.org/abs/2601.01816", "authors": ["Chris Duffey"], "title": "Admissibility Alignment", "comment": "24 pages, 2 figures, 2 tables.. Decision-theoretic alignment under uncertainty", "summary": "This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.\n  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u53ef\u91c7\u7eb3\u6027\u5bf9\u9f50\"\u6982\u5ff5\uff0c\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5bf9\u7ed3\u679c\u5206\u5e03\u8fdb\u884c\u53ef\u91c7\u7eb3\u884c\u52a8\u548c\u51b3\u7b56\u9009\u62e9\u7684\u5c5e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86MAP-AI\u7cfb\u7edf\u67b6\u6784\u6765\u5b9e\u65bd\u8fd9\u4e00\u6982\u5ff5\u3002", "motivation": "\u4f20\u7edfAI\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u88ab\u89c6\u4e3a\u9759\u6001\u6216\u4e8c\u5143\u6761\u4ef6\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u786e\u5b9a\u6027\u3001\u5e72\u9884\u6548\u5e94\u3001\u4ef7\u503c\u6a21\u7cca\u6027\u548c\u6cbb\u7406\u7ea6\u675f\u7684\u660e\u786e\u5efa\u6a21\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bc4\u4f30\u51b3\u7b56\u7b56\u7565\u5728\u591a\u79cd\u53ef\u80fd\u672a\u6765\u4e2d\u8868\u73b0\u7684\u65b9\u6cd5\uff0c\u5c06\u5bf9\u9f50\u89c6\u4e3a\u6982\u7387\u6027\u3001\u51b3\u7b56\u7406\u8bba\u5c5e\u6027\u800c\u975e\u7b80\u5355\u51c6\u786e\u5ea6\u6307\u6807\u3002", "method": "\u63d0\u51faMAP-AI\uff08\u8499\u7279\u5361\u6d1b\u5bf9\u9f50\u7b56\u7565\uff09\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u7ed3\u679c\u5206\u5e03\u548c\u53ef\u91c7\u7eb3\u6027\u63a7\u5236\u7684\u7b56\u7565\u9009\u62e9\u6765\u5b9e\u65bd\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u8bc4\u4f30\u51b3\u7b56\u7b56\u7565\u5728\u5408\u7406\u672a\u6765\u96c6\u5408\u4e2d\u7684\u8868\u73b0\uff0c\u660e\u786e\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3001\u5e72\u9884\u6548\u5e94\u3001\u4ef7\u503c\u6a21\u7cca\u6027\u548c\u6cbb\u7406\u7ea6\u675f\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684AI\u7cfb\u7edf\u6cbb\u7406\u57fa\u7840\uff0c\u5176\u5f71\u54cd\u4e0d\u662f\u7531\u4e2a\u4f53\u9884\u6d4b\u51b3\u5b9a\uff0c\u800c\u662f\u7531\u7b56\u7565\u5728\u5206\u5e03\u548c\u5c3e\u90e8\u4e8b\u4ef6\u4e2d\u7684\u884c\u4e3a\u51b3\u5b9a\u3002\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5bf9\u9f50\u8bc4\u4f30\u6574\u5408\u5230\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u4ea7\u751f\u53ef\u91c7\u7eb3\u6027\u63a7\u5236\u7684\u884c\u52a8\u9009\u62e9\u673a\u5236\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u5e95\u5c42\u6a21\u578b\u3002", "conclusion": "\u53ef\u91c7\u7eb3\u6027\u5bf9\u9f50\u6846\u67b6\u5c06AI\u5bf9\u9f50\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u6982\u7387\u6027\u51b3\u7b56\u7406\u8bba\u5c5e\u6027\uff0c\u63d0\u4f9b\u4e86\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u590d\u6742\u7ea6\u675f\u4e0b\u8bc4\u4f30\u548c\u786e\u4fddAI\u7cfb\u7edf\u5bf9\u9f50\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f01\u4e1a\u548c\u673a\u6784AI\u7cfb\u7edf\u7684\u4fe1\u4efb\u8bc4\u4f30\u3002"}}
{"id": "2601.01844", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01844", "abs": "https://arxiv.org/abs/2601.01844", "authors": ["Udiptaman Das", "Krishnasai B. Atmakuri", "Duy Ho", "Chi Lee", "Yugyung Lee"], "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation", "comment": "13 pages, 5 tables, 4 figures", "summary": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u63d0\u793a\u548c\u6a21\u5f0f\u7ea6\u675f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08KG-RAG\uff09\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4e34\u5e8a\u81ea\u7531\u6587\u672c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u7279\u522b\u9488\u5bf9\u80bf\u7624\u5b66\u9886\u57df\uff0c\u65e0\u9700\u4f9d\u8d56\u9ec4\u91d1\u6807\u51c6\u6807\u6ce8\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u53d9\u8ff0\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u7f3a\u4e4f\u5bf9\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u7a33\u5065\u9a8c\u8bc1\uff0c\u8fd9\u5728\u80bf\u7624\u5b66\u9886\u57df\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u62ec\uff1a(1) \u63d0\u793a\u9a71\u52a8\u7684\u5b9e\u4f53\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u63d0\u53d6\uff1b(2) \u57fa\u4e8e\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\uff1b(3) \u672c\u4f53\u5bf9\u9f50\u7684RDF/OWL\u6a21\u5f0f\u751f\u6210\uff1b(4) \u591aLLM\u5171\u8bc6\u9a8c\u8bc1\u7528\u4e8e\u5e7b\u89c9\u68c0\u6d4b\u548c\u8bed\u4e49\u7cbe\u70bc\u3002\u91c7\u7528\u591a\u667a\u80fd\u4f53\u63d0\u793a\u548c\u6a21\u5f0f\u7ea6\u675f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b56\u7565\u3002", "result": "\u5e94\u7528\u4e8e\u4e24\u4e2a\u80bf\u7624\u5b66\u961f\u5217\uff08PDAC\u548cBRCA\uff09\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u4e86\u53ef\u89e3\u91ca\u3001SPARQL\u517c\u5bb9\u4e14\u4e34\u5e8a\u57fa\u7840\u7684\u77e5\u8bc6\u56fe\u8c31\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u7cbe\u786e\u5ea6\u3001\u76f8\u5173\u6027\u548c\u672c\u4f53\u5408\u89c4\u6027\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u6301\u7eed\u7cbe\u70bc\u548c\u81ea\u76d1\u7763\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u56fe\u8c31\u8d28\u91cf\u7684\u8fed\u4ee3\u6539\u8fdb\uff0c\u4e3a\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u63d0\u4f9b\u4e86\u65e0\u9700\u9ec4\u91d1\u6807\u51c6\u6807\u6ce8\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01857", "abs": "https://arxiv.org/abs/2601.01857", "authors": ["Defei Xia", "Bingfeng Pi", "Shenbin Zhang", "Song Hua", "Yunfei Wei", "Lei Zuo"], "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios", "comment": null, "summary": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faJenius-Agent\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u751f\u6210\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u5de5\u5177\u7f16\u6392\u548c\u5206\u5c42\u5185\u5b58\u673a\u5236\u4e09\u5927\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u53d1\u5c55\uff0c\u63d0\u5347\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u5de5\u5177\u4f7f\u7528\u548c\u54cd\u5e94\u751f\u6210\u65b9\u9762\u7684\u4efb\u52a1\u6027\u80fd\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u5c3d\u7ba1\u5148\u524d\u7814\u7a76\u6539\u8fdb\u4e86LLM\u667a\u80fd\u4f53\u7684\u6574\u4f53\u8bbe\u8ba1\uff0c\u4f46\u5bf9\u5176\u5185\u90e8\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\u7684\u7cfb\u7edf\u6027\u4f18\u5316\u4ecd\u663e\u4e0d\u8db3\u3002", "method": "\u63d0\u51faJenius-Agent\u6846\u67b6\uff0c\u5305\u542b\u4e09\u5927\u5173\u952e\u6280\u672f\uff1a(1) \u81ea\u9002\u5e94\u63d0\u793a\u751f\u6210\u7b56\u7565\uff0c\u6839\u636e\u667a\u80fd\u4f53\u72b6\u6001\u548c\u4efb\u52a1\u76ee\u6807\u8c03\u6574\u63d0\u793a\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff1b(2) \u4e0a\u4e0b\u6587\u611f\u77e5\u5de5\u5177\u7f16\u6392\u6a21\u5757\uff0c\u57fa\u4e8e\u7528\u6237\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u8fdb\u884c\u5de5\u5177\u5206\u7c7b\u3001\u8bed\u4e49\u68c0\u7d22\u548c\u81ea\u9002\u5e94\u8c03\u7528\uff1b(3) \u5206\u5c42\u5185\u5b58\u673a\u5236\uff0c\u6574\u5408\u4f1a\u8bdd\u5185\u5b58\u3001\u4efb\u52a1\u5386\u53f2\u548c\u5916\u90e8\u6458\u8981\uff0c\u901a\u8fc7\u52a8\u6001\u6458\u8981\u548c\u538b\u7f29\u63d0\u9ad8\u76f8\u5173\u6027\u548c\u6548\u7387\u3002\u6846\u67b6\u96c6\u6210\u4e86\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u7684\u5de5\u5177\u3001\u6587\u4ef6\u8f93\u5165/\u8f93\u51fa\u548c\u6267\u884c\u53cd\u9988\u7b49\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u534720%\uff0c\u540c\u65f6\u964d\u4f4e\u4e86token\u6210\u672c\u3001\u54cd\u5e94\u5ef6\u8fdf\u548c\u8c03\u7528\u5931\u8d25\u7387\u3002\u8be5\u6846\u67b6\u5df2\u5728Jenius\u5e73\u53f0\u90e8\u7f72\uff0c\u4e3a\u5065\u58ee\u3001\u534f\u8bae\u517c\u5bb9\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Jenius-Agent\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u667a\u80fd\u4f53\u7684\u5185\u90e8\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01021", "abs": "https://arxiv.org/abs/2601.01021", "authors": ["Dai Shi", "Lequan Lin", "Andi Han", "Luke Thompson", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Zhiyong Wang", "Junbin Gao"], "title": "Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations", "comment": null, "summary": "Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.", "AI": {"tldr": "\u57fa\u4e8eWiener\u6df7\u6c8c\u5c55\u5f00\u7684\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff0c\u7528\u4e8e\u5b66\u4e60SDE/SPDE\u7684\u89e3\u7b97\u5b50\uff0c\u901a\u8fc7\u6b63\u4ea4Wick Hermite\u7279\u5f81\u6295\u5f71\u566a\u58f0\u8def\u5f84\uff0c\u7528\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u786e\u5b9a\u6027\u6df7\u6c8c\u7cfb\u6570\uff0c\u5b9e\u73b0\u4ece\u566a\u58f0\u5230\u5b8c\u6574\u89e3\u8f68\u8ff9\u7684\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u3002", "motivation": "\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u548c\u968f\u673a\u504f\u5fae\u5206\u65b9\u7a0b\u662f\u5efa\u6a21\u968f\u673a\u52a8\u529b\u5b66\u7684\u57fa\u7840\u5de5\u5177\uff0c\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fd1\u4f3c\u5176\u89e3\u7b97\u5b50\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u5feb\u901f\u5b9e\u7528\u7684\u6c42\u89e3\u5668\uff0c\u8fd8\u80fd\u4e3a\u7ecf\u5178\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178Wiener\u6df7\u6c8c\u5c55\u5f00\u8bbe\u8ba1\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff1a\u5c06\u9a71\u52a8\u566a\u58f0\u8def\u5f84\u6295\u5f71\u5230\u6b63\u4ea4Wick Hermite\u7279\u5f81\u4e0a\uff0c\u7528\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u786e\u5b9a\u6027\u6df7\u6c8c\u7cfb\u6570\uff0c\u5b9e\u73b0\u4ece\u566a\u58f0\u5230\u5b8c\u6574\u89e3\u8f68\u8ff9\u7684\u5355\u6b21\u524d\u5411\u8ba1\u7b97\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u9898\u4e0a\u9a8c\u8bc1\u6a21\u578b\uff1a\u7ecf\u5178SPDE\u57fa\u51c6\u6d4b\u8bd5\u3001\u56fe\u50cf\u6269\u6563\u4e00\u6b65\u91c7\u6837\u3001\u56fe\u62d3\u6251\u63d2\u503c\u3001\u91d1\u878d\u5916\u63a8\u3001\u53c2\u6570\u4f30\u8ba1\u4ee5\u53ca\u6d2a\u6c34\u9884\u6d4b\u7684\u6d41\u5f62SDE\uff0c\u5c55\u793a\u4e86\u7ade\u4e89\u6027\u7cbe\u5ea6\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u57fa\u4e8eWiener\u6df7\u6c8c\u5c55\u5f00\u7684\u795e\u7ecf\u7b97\u5b50\u4e3a\u5b66\u4e60SDE/SPDE\u89e3\u7b97\u5b50\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.01176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01176", "abs": "https://arxiv.org/abs/2601.01176", "authors": ["Andr\u00e9s Bell-Navas", "Jes\u00fas Garicano-Mena", "Antonella Ausiello", "Soledad Le Clainche", "Mar\u00eda Villalba-Orero", "Enrique Lara-Pezzi"], "title": "CardioMOD-Net: A Modal Decomposition-Neural Network Framework for Diagnosis and Prognosis of HFpEF from Echocardiography Cine Loops", "comment": "9 pages; 1 figure; letter", "summary": "Introduction: Heart failure with preserved ejection fraction (HFpEF) arises from diverse comorbidities and progresses through prolonged subclinical stages, making early diagnosis and prognosis difficult. Current echocardiography-based Artificial Intelligence (AI) models focus primarily on binary HFpEF detection in humans and do not provide comorbidity-specific phenotyping or temporal estimates of disease progression towards decompensation. We aimed to develop a unified AI framework, CardioMOD-Net, to perform multiclass diagnosis and continuous prediction of HFpEF onset directly from standard echocardiography cine loops in preclinical models.\n  Methods: Mouse echocardiography videos from four groups were used: control (CTL), hyperglycaemic (HG), obesity (OB), and systemic arterial hypertension (SAH). Two-dimensional parasternal long-axis cine loops were decomposed using Higher Order Dynamic Mode Decomposition (HODMD) to extract temporal features for downstream analysis. A shared latent representation supported Vision Transformers, one for a classifier for diagnosis and another for a regression module for predicting the age at HFpEF onset.\n  Results: Overall diagnostic accuracy across the four groups was 65%, with all classes exceeding 50% accuracy. Misclassifications primarily reflected early-stage overlap between OB or SAH and CTL. The prognostic module achieved a root-mean-square error of 21.72 weeks for time-to-HFpEF prediction, with OB and SAH showing the most accurate estimates. Predicted HFpEF onset closely matched true distributions in all groups.\n  Discussion: This unified framework demonstrates that multiclass phenotyping and continuous HFpEF onset prediction can be obtained from a single cine loop, even under small-data conditions. The approach offers a foundation for integrating diagnostic and prognostic modelling in preclinical HFpEF research.", "AI": {"tldr": "\u5f00\u53d1\u4e86CardioMOD-Net AI\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u9f20\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u8fdb\u884cHFpEF\u591a\u7c7b\u522b\u8bca\u65ad\u548c\u8fde\u7eed\u53d1\u75c5\u65f6\u95f4\u9884\u6d4b", "motivation": "HFpEF\u75c5\u56e0\u590d\u6742\u4e14\u8fdb\u5c55\u7f13\u6162\uff0c\u73b0\u6709AI\u6a21\u578b\u4ec5\u5173\u6ce8\u4e8c\u5143\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u5171\u75c5\u7279\u5f02\u6027\u8868\u578b\u5206\u6790\u548c\u75be\u75c5\u8fdb\u5c55\u65f6\u95f4\u9884\u6d4b", "method": "\u4f7f\u7528\u5c0f\u9f20\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\uff0c\u901a\u8fc7\u9ad8\u9636\u52a8\u6001\u6a21\u6001\u5206\u89e3\u63d0\u53d6\u65f6\u95f4\u7279\u5f81\uff0c\u6784\u5efa\u5171\u4eab\u6f5c\u5728\u8868\u793a\u7684Vision Transformers\uff0c\u5206\u522b\u7528\u4e8e\u5206\u7c7b\u8bca\u65ad\u548c\u56de\u5f52\u9884\u6d4bHFpEF\u53d1\u75c5\u65f6\u95f4", "result": "\u56db\u7ec4\u8bca\u65ad\u603b\u4f53\u51c6\u786e\u738765%\uff0c\u6240\u6709\u7c7b\u522b\u8d85\u8fc750%\u51c6\u786e\u7387\uff1b\u9884\u540e\u6a21\u5757\u9884\u6d4bHFpEF\u53d1\u75c5\u65f6\u95f4\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4e3a21.72\u5468\uff0cOB\u548cSAH\u7ec4\u9884\u6d4b\u6700\u51c6\u786e", "conclusion": "\u8be5\u7edf\u4e00\u6846\u67b6\u8bc1\u660e\u5373\u4f7f\u5728\u5c0f\u6570\u636e\u6761\u4ef6\u4e0b\uff0c\u4e5f\u80fd\u4ece\u5355\u4e00\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u83b7\u5f97\u591a\u7c7b\u522b\u8868\u578b\u5206\u6790\u548c\u8fde\u7eedHFpEF\u53d1\u75c5\u9884\u6d4b\uff0c\u4e3a\u4e34\u5e8a\u524dHFpEF\u7814\u7a76\u63d0\u4f9b\u8bca\u65ad\u548c\u9884\u540e\u5efa\u6a21\u57fa\u7840"}}
{"id": "2601.01023", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01023", "abs": "https://arxiv.org/abs/2601.01023", "authors": ["Jo\u00e3o Morais", "Sadjad Alikhani", "Akshay Malhotra", "Shahab Hamidi-Rad", "Ahmed Alkhateeb"], "title": "Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning", "comment": "resources available in: https://www.wi-lab.net/research/dataset-similarity", "summary": "This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4efb\u52a1\u548c\u6a21\u578b\u611f\u77e5\u7684\u65e0\u7ebf\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u8ddd\u79bb\u9884\u6d4b\u6a21\u578b\u8de8\u6570\u636e\u96c6\u53ef\u8fc1\u79fb\u6027\uff0c\u5728CSI\u538b\u7f29\u548c\u6ce2\u675f\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u4e2d\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u652f\u6301\u6570\u636e\u96c6\u9009\u62e9/\u589e\u5f3a\u3001\u4eff\u771f\u5230\u771f\u5b9e\u573a\u666f\u6bd4\u8f83\u3001\u4efb\u52a1\u7279\u5b9a\u5408\u6210\u6570\u636e\u751f\u6210\u7b49\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4efb\u52a1\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u548c\u6a21\u578b\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u8ddd\u79bb\u9884\u6d4b\u6a21\u578b\u8de8\u6570\u636e\u96c6\u53ef\u8fc1\u79fb\u6027\u3002\u4f7f\u7528UMAP\u5d4c\u5165\u7ed3\u5408Wasserstein\u548c\u6b27\u6c0f\u8ddd\u79bb\u5ea6\u91cf\u65e0\u76d1\u7763\u4efb\u52a1\uff0c\u5728\u76d1\u7763\u4efb\u52a1\u4e2d\u96c6\u6210\u76d1\u7763UMAP\u548c\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u60e9\u7f5a\u3002", "result": "\u5728CSI\u538b\u7f29\u4efb\u52a1\u4e2d\uff0cUMAP\u5d4c\u5165\u7ed3\u5408Wasserstein\u548c\u6b27\u6c0f\u8ddd\u79bb\u7684\u5ea6\u91cf\u65b9\u6cd5\u8fbe\u5230\u8d85\u8fc70.85\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff1b\u5728\u6ce2\u675f\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6807\u7b7e\u611f\u77e5\u8ddd\u79bb\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\uff0c\u4e0e\u6a21\u578b\u53ef\u8fc1\u79fb\u6027\u76f8\u5173\u6027\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5ea6\u91cf\u65e0\u7ebf\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u652f\u6301\u4efb\u52a1\u76f8\u5173\u7684\u6570\u636e\u96c6\u6bd4\u8f83\uff0c\u4e3a\u6570\u636e\u96c6\u9009\u62e9\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u9002\u5e94\u65b0\u90e8\u7f72\u63d0\u4f9b\u51b3\u7b56\u4f9d\u636e\u3002"}}
{"id": "2601.01181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01181", "abs": "https://arxiv.org/abs/2601.01181", "authors": ["Chenglizhao Chen", "Shaojiang Yuan", "Xiaoxue Lu", "Mengke Song", "Jia Song", "Zhenyu Wu", "Wenfeng Song", "Shuai Li"], "title": "GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation", "comment": null, "summary": "Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGenCAMO\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5408\u6210\u9ad8\u8d28\u91cf\u4f2a\u88c5\u56fe\u50cf\u5bc6\u96c6\u6807\u6ce8\u6570\u636e\uff0c\u89e3\u51b3\u4f2a\u88c5\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u4f2a\u88c5\u573a\u666f\u4e0b\u7684\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f2a\u88c5\u5bc6\u96c6\u9884\u6d4b\uff08\u7279\u522b\u662fRGB-D\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u548c\u5f00\u653e\u8bcd\u6c47\u4f2a\u88c5\u76ee\u6807\u5206\u5272\uff09\u5bf9\u4e8e\u7406\u89e3\u590d\u6742\u4f2a\u88c5\u573a\u666f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u9ad8\u8d28\u91cf\u3001\u5927\u89c4\u6a21\u7684\u4f2a\u88c5\u6570\u636e\u96c6\u7a00\u7f3a\u3002", "method": "\u63d0\u51faGenCAMO-DB\u5927\u89c4\u6a21\u4f2a\u88c5\u6570\u636e\u96c6\uff08\u5305\u542b\u6df1\u5ea6\u56fe\u3001\u573a\u666f\u56fe\u3001\u5c5e\u6027\u63cf\u8ff0\u548c\u6587\u672c\u63d0\u793a\u7b49\u591a\u6a21\u6001\u6807\u6ce8\uff09\u548cGenCAMO\u6846\u67b6\uff08\u73af\u5883\u611f\u77e5\u3001\u65e0\u63a9\u7801\u7684\u751f\u6210\u6846\u67b6\uff09\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u5408\u6210\u903c\u771f\u7684\u4f2a\u88c5\u56fe\u50cf\u5bc6\u96c6\u6570\u636e\u3002", "result": "\u591a\u6a21\u6001\u5b9e\u9a8c\u8868\u660e\uff0cGenCAMO\u901a\u8fc7\u63d0\u4f9b\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4f2a\u88c5\u573a\u666f\u4e0b\u7684\u5bc6\u96c6\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5408\u6210\u4f2a\u88c5\u56fe\u50cf\u5bc6\u96c6\u6807\u6ce8\u6570\u636e\u662f\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0cGenCAMO\u6846\u67b6\u80fd\u591f\u4e3a\u4f2a\u88c5\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.01045", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01045", "abs": "https://arxiv.org/abs/2601.01045", "authors": ["Tatsuaki Tsuruyama"], "title": "Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI", "comment": null, "summary": "Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.\n  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u4fe1\u606f\u8bbaLyapunov\u51fd\u6570\u6846\u67b6\u79fb\u690d\u5230\u751f\u6210\u6a21\u578b\u7684\u9006\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u63d0\u51faV-delta\u6295\u5f71\u9006\u5411\u6269\u6563\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5bf9\u7c97\u7c92\u5ea6\u7edf\u8ba1\u91cf\u7684\u663e\u5f0f\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u63cf\u8ff0\u7c97\u7c92\u5ea6\u7edf\u8ba1\u91cf\uff08\u5982\u56fe\u50cf\u5206\u5757\u540e\u7684\u5757\u5f3a\u5ea6\u6216\u7c7b\u522b\u6bd4\u4f8b\uff09\u5728\u9006\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u5982\u4f55\u4fdd\u6301\u548c\u6f14\u5316\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u5c06\u4fe1\u606f\u8bbaLyapunov\u51fd\u6570V\u79fb\u690d\u5230\u751f\u6210\u6a21\u578b\u7684\u9006\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u63d0\u51faV-delta\u6295\u5f71\u9006\u5411\u6269\u6563\u65b9\u6848\uff1b\u6269\u5c55V\u7684\u5355\u8c03\u6027\u5230\u65f6\u95f4\u975e\u9f50\u6b21\u5757\u4fdd\u6301\u9a6c\u5c14\u53ef\u592b\u6838\uff1b\u901a\u8fc7\u5757\u5e38\u6570\u56fe\u50cf\u548c\u7b80\u5316\u9006\u5411\u6838\u7684\u73a9\u5177\u6a21\u578b\u8fdb\u884c\u6570\u503c\u9a8c\u8bc1\u3002", "result": "\u5728\u5c0f\u7684\u6cc4\u6f0f\u548cV-delta\u6295\u5f71\u6761\u4ef6\u4e0b\uff0cV-delta\u53ef\u4f5c\u4e3a\u8fd1\u4f3cLyapunov\u51fd\u6570\uff1b\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5c06\u5757\u8d28\u91cf\u8bef\u5dee\u548c\u6cc4\u6f0f\u5bb9\u5fcd\u52bf\u4fdd\u6301\u5728\u9884\u8bbe\u5bb9\u5dee\u5185\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u975e\u6295\u5f71\u52a8\u6001\u76f8\u5f53\u7684\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u751f\u6210\u91c7\u6837\u91cd\u65b0\u89e3\u91ca\u4e3a\u4ece\u566a\u58f0\u5230\u6570\u636e\u7684\u4fe1\u606f\u52bf\u4e0b\u964d\u8fc7\u7a0b\uff0c\u4e3a\u5177\u6709\u7c97\u7c92\u5ea6\u7edf\u8ba1\u91cf\u663e\u5f0f\u63a7\u5236\u7684\u9006\u5411\u6269\u6563\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2601.01192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01192", "abs": "https://arxiv.org/abs/2601.01192", "authors": ["Hao Lu", "Xuhui Zhu", "Wenjing Zhang", "Yanan Li", "Xiang Bai"], "title": "Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors", "comment": "Journal Extension of arXiv:2506.13067", "summary": "Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can underperform in congested scenes such as metro commuting. To address this, we build WuhanMetroCrowd, one of the first VIC datasets that characterize crowded, dynamic pedestrian flows. It features sparse-to-dense density levels, short-to-long video clips, slow-to-fast flow variations, front-to-back appearance changes, and light-to-heavy occlusions. To better adapt VIC approaches to crowds, we rethink the nature of VIC and recognize two informative priors: i) the social grouping prior that indicates pedestrians tend to gather in groups and ii) the spatial-temporal displacement prior that informs an individual cannot teleport physically. The former inspires us to relax the standard one-to-one (O2O) matching used by VIC to one-to-many (O2M) matching, implemented by an implicit context generator and a O2M matcher; the latter facilitates the design of a displacement prior injector, which strengthens not only O2M matching but also feature extraction and model training. These designs jointly form a novel and strong VIC baseline OMAN++. Extensive experiments show that OMAN++ not only outperforms state-of-the-art VIC baselines on the standard SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, but also indicates a clear advantage in crowded scenes, with a 38.12% error reduction on our WuhanMetroCrowd dataset. Code, data, and pretrained models are available at https://github.com/tiny-smart/OMAN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOMAN++\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u793e\u4ea4\u5206\u7ec4\u5148\u9a8c\u548c\u65f6\u7a7a\u4f4d\u79fb\u5148\u9a8c\uff0c\u5c06\u6807\u51c6\u7684\u4e00\u5bf9\u4e00\u5339\u914d\u653e\u5bbd\u4e3a\u4e00\u5bf9\u591a\u5339\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u62e5\u6324\u573a\u666f\u4e0b\u7684\u89c6\u9891\u4e2a\u4f53\u8ba1\u6570\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4e2a\u4f53\u8ba1\u6570\u65b9\u6cd5\u5728\u62e5\u6324\u573a\u666f\uff08\u5982\u5730\u94c1\u901a\u52e4\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u62e5\u6324\u52a8\u6001\u4eba\u6d41\u7684\u6570\u636e\u96c6\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u6784\u5efaWuhanMetroCrowd\u6570\u636e\u96c6\uff0c\u5f15\u5165\u793e\u4ea4\u5206\u7ec4\u5148\u9a8c\uff08\u653e\u5bbd\u4e00\u5bf9\u4e00\u5339\u914d\u4e3a\u4e00\u5bf9\u591a\u5339\u914d\uff09\u548c\u65f6\u7a7a\u4f4d\u79fb\u5148\u9a8c\uff0c\u8bbe\u8ba1\u9690\u5f0f\u4e0a\u4e0b\u6587\u751f\u6210\u5668\u3001\u4e00\u5bf9\u591a\u5339\u914d\u5668\u548c\u4f4d\u79fb\u5148\u9a8c\u6ce8\u5165\u5668\u3002", "result": "OMAN++\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\uff08SenseCrowd\u3001CroHD\u3001MovingDroneCrowd\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728WuhanMetroCrowd\u6570\u636e\u96c6\u4e0a\u8bef\u5dee\u964d\u4f4e38.12%\u3002", "conclusion": "\u63d0\u51fa\u7684OMAN++\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u793e\u4ea4\u5206\u7ec4\u548c\u65f6\u7a7a\u4f4d\u79fb\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u62e5\u6324\u573a\u666f\u4e0b\u7684\u89c6\u9891\u4e2a\u4f53\u8ba1\u6570\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2601.01910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01910", "abs": "https://arxiv.org/abs/2601.01910", "authors": ["Minh Hieu Ha", "Khanh Ly Ta", "Hung Phan", "Tung Doan", "Tung Dao", "Dao Tran", "Huynh Thi Thanh Binh"], "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning", "comment": null, "summary": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.\n  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.", "AI": {"tldr": "MMP-A*\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u548c\u81ea\u9002\u5e94\u8870\u51cf\u673a\u5236\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u4f20\u7edfA*\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u590d\u6742\u73af\u5883\u4e2d\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u5728\u62d3\u6251\u590d\u6742\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u7684\u8def\u5f84\u70b9\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "MMP-A*\u6846\u67b6\u6574\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u548c\u65b0\u9896\u7684\u81ea\u9002\u5e94\u8870\u51cf\u673a\u5236\u3002\u8be5\u673a\u5236\u52a8\u6001\u8c03\u8282\u4e0d\u786e\u5b9a\u8def\u5f84\u70b9\u5728\u542f\u53d1\u5f0f\u51fd\u6570\u4e2d\u7684\u5f71\u54cd\uff0c\u786e\u4fdd\u51e0\u4f55\u6709\u6548\u6027\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u3002", "result": "\u5728\u5177\u6709\u4e25\u91cd\u6742\u4e71\u548c\u62d3\u6251\u590d\u6742\u6027\u7684\u6311\u6218\u6027\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cMMP-A*\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u64cd\u4f5c\u6210\u672c\u3002", "conclusion": "MMP-A*\u5c55\u793a\u4e86\u4e00\u4e2a\u611f\u77e5\u57fa\u7840\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u81ea\u4e3b\u5bfc\u822a\u8303\u5f0f\u6f5c\u529b\uff0c\u901a\u8fc7\u5c06\u9ad8\u5c42\u63a8\u7406\u951a\u5b9a\u5728\u7269\u7406\u51e0\u4f55\u4e2d\uff0c\u89e3\u51b3\u4e86\u7eaf\u6587\u672c\u89c4\u5212\u5668\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01061", "categories": ["cs.LG", "cs.AI", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.01061", "abs": "https://arxiv.org/abs/2601.01061", "authors": ["Yajing Liu", "Erkao Bao", "Linqi Song"], "title": "A UCB Bandit Algorithm for General ML-Based Estimators", "comment": "15 pages, 4 figures, 1 table, Multi-Arm bandit, psi-UCB, generalized machine learning models", "summary": "We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB", "AI": {"tldr": "ML-UCB\u7b97\u6cd5\u5c06\u4efb\u610f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96c6\u6210\u5230\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u76f4\u63a5\u5efa\u6a21\u5e95\u5c42\u4f30\u8ba1\u5668\u7684\u5b66\u4e60\u66f2\u7ebf\u884c\u4e3a\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u53ef\u5904\u7406\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u90e8\u7f72\u590d\u6742ML\u6a21\u578b\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u7528\u4e8e\u539f\u5219\u6027\u63a2\u7d22\u7684\u53ef\u5904\u7406\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faML-UCB\u7b97\u6cd5\uff0c\u5047\u8bbe\u5747\u65b9\u8bef\u5dee\u968f\u8bad\u7ec3\u6837\u672c\u6570\u5448\u5e42\u5f8b\u4e0b\u964d\uff0c\u63a8\u5bfc\u51fa\u5e7f\u4e49\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u3002\u8be5\u6846\u67b6\u5141\u8bb8\u96c6\u6210\u4efb\u4f55\u5b66\u4e60\u66f2\u7ebf\u53ef\u7ecf\u9a8c\u8868\u5f81\u7684ML\u6a21\u578b\uff0c\u65e0\u9700\u6a21\u578b\u7279\u5b9a\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u8bc1\u660e\u4e86ML-UCB\u80fd\u591f\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u3002\u5728\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u7cfb\u7edf\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u5728\u7ebf\u77e9\u9635\u5206\u89e3\u548c\u6a21\u62df\u7b80\u5316\u53cc\u5854\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\uff0c\u76f8\u6bd4LinUCB\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "ML-UCB\u4e3a\u5c06\u4efb\u610f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u539f\u5219\u6027\u5730\u96c6\u6210\u5230\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u4e2d\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5efa\u6a21\u5b66\u4e60\u66f2\u7ebf\u884c\u4e3a\u514b\u670d\u4e86\u4f20\u7edf\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u7684\u9650\u5236\u3002"}}
{"id": "2601.01939", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01939", "abs": "https://arxiv.org/abs/2601.01939", "authors": ["Victor Sanchez", "Chris Reinke", "Ahamed Mohamed", "Xavier Alameda-Pineda"], "title": "OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation", "comment": null, "summary": "In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.", "AI": {"tldr": "OpenSocInt\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u4eff\u771f\u5668\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u67b6\u6784\u8bad\u7ec3\u793e\u4ea4\u667a\u80fd\u4f53\uff0c\u5df2\u5e94\u7528\u4e8e\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1", "motivation": "\u4e3a\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u5f00\u6e90\u4eff\u771f\u5e73\u53f0\uff0c\u652f\u6301\u63a2\u7d22\u4e0d\u540c\u611f\u77e5\u7279\u5f81\u3001\u7f16\u7801\u878d\u5408\u65b9\u6cd5\u548c\u667a\u80fd\u4f53\u8bbe\u8ba1", "method": "\u5f00\u53d1\u4e86OpenSocInt\u8f6f\u4ef6\u5305\uff0c\u5305\u542b\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u4eff\u771f\u5668\u548c\u6a21\u5757\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u5c55\u793a\u5176\u529f\u80fd", "result": "\u8f6f\u4ef6\u5df2\u516c\u5f00\u53ef\u7528\uff08GPL\u8bb8\u53ef\uff09\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5b9e\u9a8c\u534f\u8bae\uff0c\u80fd\u591f\u652f\u6301\u4e0d\u540c\u611f\u77e5\u7279\u5f81\u3001\u7f16\u7801\u878d\u5408\u548c\u667a\u80fd\u4f53\u7684\u7814\u7a76", "conclusion": "OpenSocInt\u4e3a\u793e\u4ea4\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u591a\u6a21\u6001\u793e\u4ea4\u4ea4\u4e92\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2601.01062", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01062", "abs": "https://arxiv.org/abs/2601.01062", "authors": ["Yunlin Zeng"], "title": "SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models", "comment": "14 pages, 3 figures. Accepted to WVAQ 2026, WACV 2026", "summary": "Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\\% win rate) and narrative depth (+50\\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u64ad\u5ba2\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5fae\u8c03Qwen3-VL-32B\u6a21\u578b\uff0c\u4f7f\u7528\u5408\u6210\u5230\u771f\u5b9e\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u57284000\u4e2a\u56fe\u50cf-\u5bf9\u8bdd\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bf4\u8bdd\u8005\u64ad\u5ba2\u5bf9\u8bdd\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u63cf\u8ff0\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u5f15\u4eba\u5165\u80dc\u7684\u957f\u7bc7\u53d9\u4e8b\uff08\u7279\u522b\u662f\u591a\u8bf4\u8bdd\u8005\u64ad\u5ba2\u5bf9\u8bdd\uff09\u65b9\u9762\u4ecd\u6709\u5f85\u63a2\u7d22\u4e14\u96be\u4ee5\u8bc4\u4f30\u3002\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6355\u6349\u5bf9\u8bdd\u81ea\u7136\u5ea6\u3001\u4e2a\u6027\u548c\u53d9\u4e8b\u6d41\u7545\u6027\u7b49\u7ec6\u5fae\u5dee\u522b\u3002", "method": "1. \u5f00\u53d1\u7aef\u5230\u7aef\u89c6\u89c9\u64ad\u5ba2\u751f\u6210\u7ba1\u9053\uff1b2. \u57284000\u4e2a\u56fe\u50cf-\u5bf9\u8bdd\u5bf9\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Qwen3-VL-32B\u6a21\u578b\uff1b3. \u91c7\u7528\u5408\u6210\u5230\u771f\u5b9e\u7684\u8bad\u7ec3\u7b56\u7565\uff1a\u5728SPoRC\u7684\u9ad8\u8d28\u91cf\u64ad\u5ba2\u5bf9\u8bdd\u4e0e\u5408\u6210\u751f\u6210\u56fe\u50cf\u4e0a\u8bad\u7ec3\uff0c\u5728VIST\u7684\u771f\u5b9e\u7167\u7247\u5e8f\u5217\u4e0a\u8bc4\u4f30\uff1b4. \u63d0\u51fa\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528AI\u4f5c\u4e3a\u8bc4\u5224\u5458\u548c\u65b0\u578b\u98ce\u683c\u6307\u6807\u3002", "result": "\u5fae\u8c03\u768432B\u6a21\u578b\u5728\u5bf9\u8bdd\u81ea\u7136\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e235B\u57fa\u7840\u6a21\u578b\uff08\u80dc\u7387>80%\uff09\uff0c\u53d9\u4e8b\u6df1\u5ea6\u589e\u52a050%\uff08\u8f6e\u6b21\u957f\u5ea6\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u89c6\u89c9\u57fa\u7840\u80fd\u529b\uff08CLIPScore: 20.39\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u76f8\u5bf9\u8f83\u5c0f\u7684\u6a21\u578b\uff0832B\uff09\u53ef\u4ee5\u5728\u751f\u6210\u5f15\u4eba\u5165\u80dc\u7684\u591a\u8bf4\u8bdd\u8005\u64ad\u5ba2\u5bf9\u8bdd\u65b9\u9762\u8d85\u8d8a\u66f4\u5927\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4e3a\u89c6\u89c9\u53d9\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2601.01202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01202", "abs": "https://arxiv.org/abs/2601.01202", "authors": ["Jiazhu Dai", "Huihui Jiang"], "title": "RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models", "comment": null, "summary": "Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRefSR-Adv\u5bf9\u6297\u653b\u51fb\uff0c\u901a\u8fc7\u4ec5\u6270\u52a8\u53c2\u8003\u56fe\u50cf\u6765\u964d\u4f4e\u57fa\u4e8e\u53c2\u8003\u7684\u8d85\u5206\u8fa8\u7387\u7cfb\u7edf\u6027\u80fd\uff0c\u63ed\u793a\u4e86RefSR\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u8003\u7279\u5f81\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8RefSR\u7684\u540e\u95e8\u653b\u51fb\uff0c\u800c\u9488\u5bf9RefSR\u7684\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u63ed\u793aRefSR\u7cfb\u7edf\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faRefSR-Adv\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5bf9\u6297\u8f93\u51fa\u4e0e\u5e72\u51c0\u8f93\u51fa\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ec5\u5bf9\u53c2\u8003\u56fe\u50cf\u6dfb\u52a0\u6270\u52a8\u6765\u964d\u4f4e\u8d85\u5206\u8fa8\u7387\u8f93\u51fa\u8d28\u91cf\u3002", "result": "RefSR-Adv\u5728CNN\u3001Transformer\u548cMamba\u67b6\u6784\u4e0a\u5747\u80fd\u663e\u8457\u964d\u4f4e\u6027\u80fd\u5e76\u4ea7\u751f\u4e25\u91cd\u4f2a\u5f71\uff0c\u5b9e\u9a8c\u8bc1\u5b9e\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u4e0e\u53c2\u8003\u56fe\u50cf\u7684\u76f8\u4f3c\u5ea6\u4e0e\u653b\u51fb\u6548\u679c\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "RefSR\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u8003\u7279\u5f81\u662f\u4e00\u4e2a\u5173\u952e\u5b89\u5168\u7f3a\u9677\uff0c\u672c\u7814\u7a76\u63ed\u793a\u4e86RefSR\u7cfb\u7edf\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u65e8\u5728\u4fc3\u4f7f\u7814\u7a76\u8005\u5173\u6ce8RefSR\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002"}}
{"id": "2601.01976", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01976", "abs": "https://arxiv.org/abs/2601.01976", "authors": ["Yasmine Souissi", "Fabrice Boissier", "Nida Meddouri"], "title": "CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes", "comment": null, "summary": "Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u7684\u5206\u7c7b\u5668\u8fdb\u884c\u4e86\u6700\u65b0\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u540d\u4e49\u6570\u636e\u8ba1\u7b97\u95ed\u5305\u7b97\u5b50\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5173\u6ce8\u6700\u76f8\u5173\u6982\u5ff5\u7684\u90e8\u5206\u6982\u5ff5\u683c\u3002", "motivation": "\u77e5\u8bc6\u53d1\u73b0\uff08KDD\uff09\u65e8\u5728\u4ece\u6d77\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u9690\u85cf\u7684\u6709\u610f\u4e49\u77e5\u8bc6\uff0c\u5176\u4e2d\u5206\u7c7b\u662f\u6838\u5fc3\u6570\u636e\u6316\u6398\u6280\u672f\u4e4b\u4e00\u3002\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u56e0\u5176\u57fa\u4e8e\u6982\u5ff5\u683c\u7684\u6570\u5b66\u7ed3\u6784\uff0c\u80fd\u591f\u751f\u6210\u5f62\u5f0f\u6982\u5ff5\u5e76\u53d1\u73b0\u9690\u85cf\u5173\u7cfb\uff0c\u88ab\u8ba4\u4e3a\u662f\u4e00\u79cd\u6709\u6548\u7684\u53ef\u89e3\u91ca\u548c\u53ef\u89e3\u91ca\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "1. \u5bf9\u57fa\u4e8eFCA\u7684\u5206\u7c7b\u5668\u8fdb\u884c\u4e86\u6700\u65b0\u7efc\u8ff0\uff1b2. \u63a2\u7d22\u4e86\u4ece\u540d\u4e49\u6570\u636e\u8ba1\u7b97\u95ed\u5305\u7b97\u5b50\u7684\u5404\u79cd\u65b9\u6cd5\uff1b3. \u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u90e8\u5206\u6982\u5ff5\u683c\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u6982\u5ff5\u683c\u4e13\u6ce8\u4e8e\u6700\u76f8\u5173\u7684\u6982\u5ff5\u3002", "result": "\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u7ed3\u679c\u6765\u8bc1\u660e\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86FCA\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u6982\u5ff5\u683c\u6784\u5efa\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u540d\u4e49\u6570\u636e\u5e76\u5173\u6ce8\u6700\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u4e3a\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2601.01065", "categories": ["cs.LG", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01065", "abs": "https://arxiv.org/abs/2601.01065", "authors": ["Achraf Hsain", "Yahya Zaki", "Othman Abaakil", "Hibat-allah Bekkar", "Yousra Chtouki"], "title": "Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco", "comment": "Published in IEEE GCAIoT 2024", "summary": "Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u57fa\u4e8eTinyML\u7684\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u96c6\u6210\u5230\u6c34\u4ea7\u517b\u6b96\u7cfb\u7edf\u4e2d\uff0c\u5b9e\u73b0\u5b9e\u65f6\u81ea\u52a8\u5316\u76d1\u6d4b\u548c\u63a7\u5236\uff0c\u4ee5\u89e3\u51b3\u6c34\u8d28\u6ce2\u52a8\u3001\u75be\u75c5\u7206\u53d1\u548c\u9972\u6599\u7ba1\u7406\u6548\u7387\u4f4e\u7b49\u6311\u6218\u3002", "motivation": "\u6c34\u4ea7\u517b\u6b96\u4e1a\u9762\u4e34\u6c34\u8d28\u6ce2\u52a8\u3001\u75be\u75c5\u7206\u53d1\u548c\u9972\u6599\u7ba1\u7406\u6548\u7387\u4f4e\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u52b3\u52a8\u4e14\u8017\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4\u95ee\u9898\u5904\u7406\u5ef6\u8fdf\u3002\u9700\u8981\u66f4\u667a\u80fd\u3001\u5b9e\u65f6\u7684\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210\u57fa\u4e8eTinyML\u7684\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u5b9e\u65f6\u6536\u96c6pH\u503c\u3001\u6e29\u5ea6\u3001\u6eb6\u89e3\u6c27\u3001\u6c28\u6c2e\u6c34\u5e73\u7b49\u53c2\u6570\u6570\u636e\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u76d1\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u62a5\u8b66\u548c\u63a7\u5236\u529f\u80fd\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u76d1\u6d4b\u6c34\u8d28\u53c2\u6570\uff0c\u81ea\u52a8\u89e6\u53d1\u8b66\u62a5\uff0c\u51cf\u5c11\u4eba\u5de5\u9700\u6c42\uff0c\u6536\u96c6\u7684\u6570\u636e\u53ef\u7528\u4e8e\u4f18\u5316\u6c34\u5904\u7406\u8fc7\u7a0b\u3001\u9972\u6599\u5206\u914d\u548c\u9972\u6599\u6548\u7387\u5206\u6790\uff0c\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u3002", "conclusion": "TinyML\u6280\u672f\u5728\u6c34\u4ea7\u517b\u6b96\u76d1\u6d4b\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u80fd\u591f\u4fc3\u8fdb\u66f4\u53ef\u6301\u7eed\u548c\u9ad8\u6548\u7684\u517b\u6b96\u5b9e\u8df5\u53d1\u5c55\uff0c\u4f46\u9700\u8981\u8003\u8651\u4f20\u611f\u5668\u9009\u62e9\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u786c\u4ef6\u7ea6\u675f\u548c\u4f26\u7406\u56e0\u7d20\u3002"}}
{"id": "2601.01204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01204", "abs": "https://arxiv.org/abs/2601.01204", "authors": ["Zunhai Su", "Weihao Ye", "Hansen Feng", "Keyu Fan", "Jing Zhang", "Dahai Yu", "Zhengwu Liu", "Ngai Wong"], "title": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression", "comment": null, "summary": "Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.", "AI": {"tldr": "XStreamVGGT\u901a\u8fc7\u8054\u5408\u526a\u679d\u548c\u91cf\u5316\u538b\u7f29KV\u7f13\u5b58\uff0c\u89e3\u51b3\u4e86StreamVGGT\u5728\u6d41\u5f0f3D\u91cd\u5efa\u4e2dKV\u7f13\u5b58\u65e0\u9650\u589e\u957f\u5bfc\u81f4\u5185\u5b58\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\u589e\u52a0\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u6d41\u5f0f\u63a8\u7406\u3002", "motivation": "StreamVGGT\u867d\u7136\u5229\u7528\u5e27\u7ea7\u56e0\u679c\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6d41\u5f0f\u91cd\u5efa\uff0c\u4f46\u5b58\u5728KV\u7f13\u5b58\u65e0\u9650\u589e\u957f\u7684\u95ee\u9898\uff0c\u968f\u7740\u8f93\u5165\u5e27\u7684\u79ef\u7d2f\u5bfc\u81f4\u5185\u5b58\u6d88\u8017\u548c\u63a8\u7406\u5ef6\u8fdf\u4e0d\u65ad\u589e\u52a0\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u65e0\u9700\u8c03\u4f18\u7684XStreamVGGT\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u526a\u679d\u548c\u91cf\u5316\u7cfb\u7edf\u6027\u5730\u538b\u7f29KV\u7f13\u5b58\uff1a1\uff09\u901a\u8fc7\u9ad8\u6548\u4ee4\u724c\u91cd\u8981\u6027\u8bc6\u522b\u526a\u9664\u591a\u89c6\u56fe\u8f93\u5165\u4ea7\u751f\u7684\u5197\u4f59KV\uff0c\u5b9e\u73b0\u56fa\u5b9a\u5185\u5b58\u9884\u7b97\uff1b2\uff09\u5229\u7528KV\u5f20\u91cf\u7684\u72ec\u7279\u5206\u5e03\u7279\u6027\uff0c\u5f15\u5165KV\u91cf\u5316\u8fdb\u4e00\u6b65\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "result": "\u8bc4\u4f30\u663e\u793aXStreamVGGT\u5728\u6027\u80fd\u635f\u5931\u51e0\u4e4e\u53ef\u5ffd\u7565\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u5c06\u5185\u5b58\u4f7f\u7528\u51cf\u5c114.42\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53475.48\u500d\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u6d41\u5f0f3D\u5e94\u7528\u3002", "conclusion": "XStreamVGGT\u901a\u8fc7\u7cfb\u7edf\u6027\u7684KV\u7f13\u5b58\u538b\u7f29\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6d41\u5f0f3D\u89c6\u89c9\u51e0\u4f55\u6a21\u578b\u4e2dKV\u7f13\u5b58\u65e0\u9650\u589e\u957f\u7684\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u6d41\u5f0f3D\u5e94\u7528\u63d0\u4f9b\u4e86\u5185\u5b58\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01982", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01982", "abs": "https://arxiv.org/abs/2601.01982", "authors": ["Noel Thomas"], "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems", "comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)", "summary": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.", "AI": {"tldr": "ChaosBench-Logic\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u9886\u57df\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b30\u4e2a\u7cfb\u7edf\u300111\u4e2a\u8bed\u4e49\u8c13\u8bcd\u548c621\u4e2a\u95ee\u9898\uff0c\u6db5\u76d67\u79cd\u63a8\u7406\u7c7b\u578b\uff0c\u7ed3\u679c\u663e\u793a\u524d\u6cbfLLMs\u5728\u5355\u9879\u51c6\u786e\u7387\u4e0a\u8fbe\u523091-94%\uff0c\u4f46\u5728\u7ec4\u5408\u63a8\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u903b\u8f91\u548c\u7b26\u53f7\u63a8\u7406\u7684\u9886\u57df\u4ecd\u7136\u8106\u5f31\u3002\u6df7\u6c8c\u52a8\u529b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7279\u522b\u4e25\u683c\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u56e0\u4e3a\u6df7\u6c8c\u662f\u786e\u5b9a\u6027\u7684\uff0c\u4f46\u7ecf\u5e38\u88ab\u8bef\u89e3\u4e3a\u968f\u673a\u6027\u6216\u590d\u6742\u6027\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLMs\u5728\u79d1\u5b66\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165ChaosBench-Logic\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u4e00\u9636\u903b\u8f91\u672c\u4f53\u8bba\u8bc4\u4f3030\u4e2a\u4e0d\u540c\u7684\u52a8\u529b\u7cfb\u7edf\u3002\u6bcf\u4e2a\u7cfb\u7edf\u752811\u4e2a\u8bed\u4e49\u8c13\u8bcd\u7684\u771f\u503c\u5206\u914d\u8fdb\u884c\u6807\u6ce8\uff0c\u751f\u6210621\u4e2a\u95ee\u9898\uff0c\u6db5\u76d67\u79cd\u63a8\u7406\u7c7b\u522b\uff1a\u591a\u8df3\u63a8\u7406\u3001\u8de8\u7cfb\u7edf\u7c7b\u6bd4\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u504f\u89c1\u63a2\u6d4b\u548c\u591a\u8f6e\u5bf9\u8bdd\u7b49\u3002\u5b9a\u4e49\u4e86\u903b\u8f91\u51c6\u786e\u6027\u3001\u8574\u542b\u4e00\u81f4\u6027\u3001\u5bf9\u8bdd\u8fde\u8d2f\u6027\u548c\u77db\u76fe\u6027\u7b49\u6307\u6807\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90\u8bc4\u4f30\u7ba1\u9053\u3002", "result": "\u524d\u6cbfLLMs\uff08GPT-4\u3001Claude 3.5 Sonnet\u3001Gemini 2.5 Flash\u548c\u5f00\u6e90LLaMA-3 70B\uff09\u5728\u5355\u9879\u51c6\u786e\u7387\u4e0a\u8fbe\u523091-94%\uff0c\u4f46\u5728\u7ec4\u5408\u63a8\u7406\u9879\u76ee\u4e0a\u5f97\u5206\u4e3a0%\uff0c\u8868\u73b0\u51fa\u8106\u5f31\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3002\u5bf9\u8bdd\u7ea7\u51c6\u786e\u7387\u4ece53.1%\uff08GPT-4 CoT\uff09\u523075.5%\uff08LLaMA-3\u96f6\u6837\u672c\uff09\u4e0d\u7b49\u3002", "conclusion": "ChaosBench-Logic\u4e3a\u8bca\u65adLLMs\u5728\u903b\u8f91\u63a8\u7406\u65b9\u9762\u7684\u5931\u8d25\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u4e3a\u5f00\u53d1\u80fd\u591f\u6539\u5584LLMs\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5c3d\u7ba1LLMs\u5728\u5355\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u7ec4\u5408\u63a8\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7684\u590d\u6742\u903b\u8f91\u4efb\u52a1\u4e0a\u4ecd\u7136\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\u3002"}}
{"id": "2601.01069", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01069", "abs": "https://arxiv.org/abs/2601.01069", "authors": ["Jing Wang", "Peng Zhao", "Zhi-Hua Zhou"], "title": "Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs", "comment": "accepted by IEEE Transactions on Information Theory. arXiv admin note: substantial text overlap with arXiv:2303.02691", "summary": "Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \\emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\\tilde{O}(k_\u03bc^{5/4} c_\u03bc^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\\tilde{O}(k_\u03bc^{2} c_\u03bc^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_\u03bc$ and $c_\u03bc$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u52a0\u6743\u7b56\u7565\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u7b80\u5316\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u5e76\u63d0\u5347\u4e86\u7406\u8bba\u6027\u80fd\u3002", "motivation": "\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u4e2d\uff0c\u52a0\u6743\u7b56\u7565\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u5206\u6790\u590d\u6742\u4e14\u7b97\u6cd5\u6548\u7387\u4f4e\u6216\u7edf\u8ba1\u6027\u80fd\u4e0d\u4f18\u3002\u4f5c\u8005\u53d1\u73b0\u8fd9\u662f\u56e0\u4e3a\u73b0\u6709\u5206\u6790\u6846\u67b6\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7b97\u6cd5\u8bbe\u8ba1\u8fc7\u4e8e\u590d\u6742\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cbe\u70bc\u7684\u5206\u6790\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u52a0\u6743\u7b56\u7565\u7684\u63a8\u5bfc\u8fc7\u7a0b\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u8bbe\u8ba1\u4e86\u66f4\u7b80\u5355\u7684\u52a0\u6743\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u3001\u81ea\u534f\u8c03\u8d4c\u535a\u673a\u4ee5\u53ca\u5177\u6709\u51fd\u6570\u8fd1\u4f3c\u7684\u975e\u5e73\u7a33\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5728\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\uff0c\u65b0\u7b97\u6cd5\u4e0e\u7a97\u53e3/\u91cd\u542f\u7b97\u6cd5\u4e00\u6837\u9ad8\u6548\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u9057\u61be\u754c\u3002\u5728\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\u4e2d\uff0c\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u9057\u61be\u754c\uff1a$\\tilde{O}(k_\u03bc^{5/4} c_\u03bc^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684$\\tilde{O}(k_\u03bc^{2} c_\u03bc^{-1}d^{9/10} P_T^{1/5}T^{4/5})$\u3002\u6846\u67b6\u8fd8\u6210\u529f\u6269\u5c55\u5230\u7ebf\u6027\u6df7\u5408MDP\u548c\u591a\u9879Logit\u6df7\u5408MDP\u3002", "conclusion": "\u65b0\u7684\u5206\u6790\u6846\u67b6\u89e3\u51b3\u4e86\u52a0\u6743\u7b56\u7565\u5728\u975e\u5e73\u7a33\u53c2\u6570\u5316\u8d4c\u535a\u673a\u4e2d\u7684\u7406\u8bba\u5206\u6790\u96be\u9898\uff0c\u7b80\u5316\u4e86\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u4e2d\u3002"}}
{"id": "2601.01993", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01993", "abs": "https://arxiv.org/abs/2601.01993", "authors": ["Dong Xue", "Jicheng Tu", "Ming Wang", "Xin Yan", "Fangzhou Liu", "Jie Hu"], "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support", "comment": "33 pages, 16 figures", "summary": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.", "AI": {"tldr": "MindChat\u662f\u4e00\u4e2a\u4fdd\u62a4\u9690\u79c1\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u914d\u5408MindCorpus\u5408\u6210\u5fc3\u7406\u54a8\u8be2\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u548c\u5dee\u5206\u9690\u79c1\u51cf\u5c11\u9690\u79c1\u98ce\u9669\uff0c\u5728\u5fc3\u7406\u54a8\u8be2\u80fd\u529b\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u8bad\u7ec3\u53d7\u5230\u771f\u5b9e\u5fc3\u7406\u54a8\u8be2\u5bf9\u8bdd\u7a00\u7f3a\u6027\u548c\u654f\u611f\u6027\u7684\u9650\u5236\uff0c\u9700\u8981\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\u3002", "method": "1. \u5f00\u53d1MindCorpus\u5408\u6210\u6570\u636e\u96c6\uff1a\u91c7\u7528\u591a\u667a\u80fd\u4f53\u89d2\u8272\u626e\u6f14\u6846\u67b6\uff0c\u5305\u542b\u56de\u5408\u7ea7\u6279\u8bc4\u4fee\u8ba2\u548c\u4f1a\u8bdd\u7ea7\u7b56\u7565\u7cbe\u5316\u7684\u53cc\u95ed\u73af\u53cd\u9988\u8bbe\u8ba1\uff1b2. \u8bad\u7ec3MindChat\u6a21\u578b\uff1a\u4f7f\u7528\u8054\u90a6\u5b66\u4e60\u914d\u5408\u53c2\u6570\u9ad8\u6548\u7684LoRA\u9002\u914d\u5668\uff0c\u5e76\u52a0\u5165\u5dee\u5206\u9690\u79c1\u4f18\u5316\u51cf\u5c11\u9690\u79c1\u98ce\u9669\u3002", "result": "MindCorpus\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u679c\uff0cMindChat\u5728\u81ea\u52a8LLM\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u4e0e\u73b0\u6709\u901a\u7528\u548c\u5fc3\u7406\u54a8\u8be2\u5bfc\u5411\u7684\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u6210\u5458\u63a8\u7406\u653b\u51fb\u4e0b\u8868\u73b0\u51fa\u51cf\u5c11\u7684\u9690\u79c1\u6cc4\u9732\u3002", "conclusion": "\u63d0\u51fa\u7684MindChat\u6a21\u578b\u548cMindCorpus\u6570\u636e\u96c6\u4e3a\u89e3\u51b3\u5fc3\u7406\u5065\u5eb7\u652f\u6301LLM\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u5fc3\u7406\u54a8\u8be2\u80fd\u529b\u3002"}}
{"id": "2601.01213", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01213", "abs": "https://arxiv.org/abs/2601.01213", "authors": ["Riccardo Gelato", "Carlo Sgaravatti", "Jakob Grahn", "Giacomo Boracchi", "Filippo Maria Bianchi"], "title": "Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation", "comment": null, "summary": "Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eSegment Anything Model (SAM)\u7684SAR\u56fe\u50cf\u96ea\u5d29\u6807\u6ce8\u5de5\u5177\uff0c\u901a\u8fc7\u9886\u57df\u9002\u914d\u3001\u591a\u901a\u9053\u7f16\u7801\u5668\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u9ad8\u6548\u8bad\u7ec3\u7b97\u6cd5\uff0c\u663e\u8457\u52a0\u5feb\u4e86Sentinel-1 SAR\u56fe\u50cf\u7684\u96ea\u5d29\u6807\u6ce8\u901f\u5ea6\u3002", "motivation": "SAR\u56fe\u50cf\u96ea\u5d29\u5206\u5272\u548c\u5236\u56fe\u5bf9\u5c71\u533a\u98ce\u9669\u9884\u6d4b\u548c\u51cf\u707e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bad\u7ec3\u6709\u6548\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u6781\u5176\u8017\u65f6\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdbSAM\u6a21\u578b\u6765\u52a0\u901fSAR\u56fe\u50cf\u7684\u96ea\u5d29\u6807\u6ce8\u8fc7\u7a0b\u3002", "method": "\u9488\u5bf9SAR\u56fe\u50cf\u4e0e\u81ea\u7136\u56fe\u50cf\u7684\u9886\u57df\u5dee\u5f02\uff0c\u91c7\u7528\u9002\u914d\u5668\u7f13\u89e3\u9886\u57df\u5dee\u8ddd\uff1b\u4f7f\u7528\u591a\u7f16\u7801\u5668\u5904\u7406\u591a\u901a\u9053SAR\u8f93\u5165\uff1b\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u63d0\u9ad8\u96ea\u5d29\u5b9a\u4f4d\u7cbe\u5ea6\uff1b\u8bbe\u8ba1\u9ad8\u6548\u8bad\u7ec3\u7b97\u6cd5\u9650\u5236\u7f16\u7801\u5668\u8bad\u7ec3\u65f6\u95f4\u3002\u6700\u7ec8\u5c06\u6539\u8fdb\u7684\u6a21\u578b\u96c6\u6210\u5230\u6807\u6ce8\u5de5\u5177\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u52a0\u901fSAR\u56fe\u50cf\u7684\u96ea\u5d29\u6807\u6ce8\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u9886\u57df\u4e0d\u5339\u914d\u3001\u591a\u901a\u9053\u8f93\u5165\u3001\u4e0d\u7cbe\u786e\u63d0\u793a\u5f71\u54cd\u5206\u5272\u8d28\u91cf\u4ee5\u53ca\u8bad\u7ec3\u6548\u7387\u4f4e\u7b49\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u5236\u5316\u6539\u8fdbSAM\u6a21\u578b\u5e76\u96c6\u6210\u5230\u6807\u6ce8\u5de5\u5177\u4e2d\uff0c\u6210\u529f\u5b9e\u73b0\u4e86SAR\u56fe\u50cf\u96ea\u5d29\u6807\u6ce8\u7684\u52a0\u901f\uff0c\u4e3a\u5c71\u533a\u96ea\u5d29\u98ce\u9669\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2601.02008", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02008", "abs": "https://arxiv.org/abs/2601.02008", "authors": ["Midhat Urooj", "Ayan Banerjee", "Sandeep Gupta"], "title": "XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging", "comment": "Accepted at AAAI Bridge Program 2026", "summary": "Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.", "AI": {"tldr": "XAIMeD\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u533b\u7597AI\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u6574\u5408\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\uff0c\u63d0\u5347\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u7f55\u89c1\u7c7b\u522b\u654f\u611f\u6027\uff0c\u63d0\u4f9b\u900f\u660e\u7684\u4e34\u5e8a\u5bf9\u9f50\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u533b\u7597AI\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u3001\u9886\u57df\u6cdb\u5316\u548c\u7f55\u89c1\u7c7b\u522b\u53ef\u9760\u6027\u7b49\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u6df1\u5ea6\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u4e0b\u7ecf\u5e38\u5931\u8d25\uff0c\u5e76\u5bf9\u4e0d\u9891\u7e41\u7684\u4e34\u5e8a\u6761\u4ef6\u8868\u73b0\u51fa\u504f\u89c1\u3002", "method": "\u63d0\u51faXAIMeD\u6846\u67b6\uff0c\u5c06\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u7f16\u7801\u4e3a\u539f\u5b50\u533b\u5b66\u547d\u9898\u7684\u903b\u8f91\u8fde\u63a5\uff0c\u8f6c\u5316\u4e3a\u673a\u5668\u53ef\u68c0\u67e5\u7684\u7c7b\u522b\u7279\u5b9a\u89c4\u5219\u3002\u901a\u8fc7\u52a0\u6743\u7279\u5f81\u6ee1\u8db3\u5ea6\u5206\u6570\u91cf\u5316\u8bca\u65ad\u6548\u7528\uff0c\u521b\u5efa\u7b26\u53f7\u63a8\u7406\u5206\u652f\u8865\u5145\u795e\u7ecf\u9884\u6d4b\u3002\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u52a0\u6743\u878d\u5408\u6574\u5408\u7b26\u53f7\u548c\u6df1\u5ea6\u8f93\u51fa\uff0c\u5e76\u91c7\u7528\u53d7Hunt\u542f\u53d1\u7684\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\uff0c\u7531\u71b5\u4e0d\u5e73\u8861\u589e\u76ca\u548c\u7f55\u89c1\u7c7b\u522b\u57fa\u5c3c\u7cfb\u6570\u6307\u5bfc\u3002", "result": "\u5728\u56db\u4e2a\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u8bc4\u4f30\u663e\u793a\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1a\u8de8\u9886\u57df\u6cdb\u5316\u63d0\u53476%\uff0c\u7f55\u89c1\u7c7b\u522bF1\u5206\u6570\u63d0\u534710%\uff0c\u8fdc\u8d85\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e34\u5e8a\u57fa\u7840\u7684\u7b26\u53f7\u7ec4\u4ef6\u4f5c\u4e3a\u6709\u6548\u7684\u6b63\u5219\u5316\u5668\uff0c\u786e\u4fdd\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "XAIMeD\u4e3a\u591a\u6a21\u6001\u533b\u7597AI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u3001\u4e34\u5e8a\u5fe0\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u533b\u7597\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2601.01082", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01082", "abs": "https://arxiv.org/abs/2601.01082", "authors": ["Bryon Tjanaka", "Henry Chen", "Matthew C. Fontaine", "Stefanos Nikolaidis"], "title": "Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces", "comment": "Source code available at https://github.com/icaros-usc/discount-models", "summary": "Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.", "AI": {"tldr": "DMS\u7b97\u6cd5\u901a\u8fc7\u4f7f\u7528\u5e73\u6ed1\u8fde\u7eed\u6298\u6263\u503c\u6a21\u578b\u89e3\u51b3\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2dQD\u4f18\u5316\u7684\u5931\u771f\u95ee\u9898\uff0c\u8d85\u8d8a\u73b0\u6709\u7b97\u6cd5\u5e76\u652f\u6301\u56fe\u50cf\u4f5c\u4e3a\u5ea6\u91cf\u7a7a\u95f4\u7684\u65b0\u5e94\u7528", "motivation": "\u4f20\u7edfQD\u7b97\u6cd5\u5728\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u5b58\u5728\u5931\u771f\u95ee\u9898\uff0c\u5373\u8bb8\u591a\u89e3\u6620\u5c04\u5230\u76f8\u4f3c\u7684\u5ea6\u91cf\u503c\uff0c\u5bfc\u81f4\u63a2\u7d22\u505c\u6ede\u3002CMA-MAE\u7b49\u7b97\u6cd5\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u56e0\u4f7f\u7528\u76f4\u65b9\u56fe\u8bb0\u5f55\u6298\u6263\u503c\u800c\u5931\u6548\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u652f\u6301\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u7684\u5e94\u7528", "method": "\u63d0\u51fa\u6298\u6263\u6a21\u578b\u641c\u7d22\uff08DMS\uff09\uff0c\u4f7f\u7528\u63d0\u4f9b\u5e73\u6ed1\u8fde\u7eed\u6298\u6263\u503c\u8868\u793a\u7684\u6a21\u578b\u6765\u6307\u5bfc\u63a2\u7d22\u3002\u8be5\u6a21\u578b\u80fd\u591f\u533a\u5206\u5177\u6709\u76f8\u4f3c\u5ea6\u91cf\u7684\u89e3\uff0c\u4ece\u800c\u5728\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7ee7\u7eed\u63a2\u7d22", "result": "DMS\u5728\u9ad8\u7ef4\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u65b0\u9886\u57df\uff08\u4ee5\u56fe\u50cf\u4f5c\u4e3a\u5ea6\u91cf\u7a7a\u95f4\uff09\u4e2d\u8868\u73b0\u4f18\u4e8eCMA-MAE\u548c\u5176\u4ed6\u9ed1\u76d2QD\u7b97\u6cd5\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u63d0\u4f9b\u56fe\u50cf\u6570\u636e\u96c6\u800c\u975e\u624b\u52a8\u8bbe\u8ba1\u5ea6\u91cf\u51fd\u6570\u6765\u6307\u5b9a\u6240\u9700\u5ea6\u91cf", "conclusion": "DMS\u901a\u8fc7\u8fde\u7eed\u6298\u6263\u6a21\u578b\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u7684\u5931\u771f\u95ee\u9898\uff0c\u6269\u5c55\u4e86QD\u7b97\u6cd5\u7684\u5e94\u7528\u8303\u56f4\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u4f5c\u4e3a\u5ea6\u91cf\u7a7a\u95f4\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2601.02043", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02043", "abs": "https://arxiv.org/abs/2601.02043", "authors": ["Hendrik Kempt", "Alon Lavie"], "title": "Simulated Reasoning is Reasoning", "comment": "21 pages", "summary": "Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., \"symbolic reasoning\". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can \"reason\" by way of imitating the process of \"thinking out loud\", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the \"stochastic parrot\" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6a21\u4eff\"\u5927\u58f0\u601d\u8003\"\u8fc7\u7a0b\u3001\u6d4b\u8bd5\u751f\u6210\u8def\u5f84\u5e76\u8fed\u4ee3\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u4e8e\u4eba\u7c7b\u7b26\u53f7\u63a8\u7406\u7684\u65b0\u578b\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u6539\u53d8\u4e86\u6211\u4eec\u5bf9\u63a8\u7406\u5fc5\u8981\u6761\u4ef6\u7684\u7406\u89e3\uff0c\u5e76\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\"\u968f\u673a\u9e66\u9e49\"\u6bd4\u55bb\u7684\u9002\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u63a8\u7406\u662f\u901a\u8fc7\u7b26\u53f7\u63a8\u7406\u5b9e\u73b0\u7406\u89e3\u7684\u8def\u5f84\uff0c\u4f46\u57fa\u7840\u6a21\u578b\u5c55\u793a\u4e86\u65e0\u9700\u7b26\u53f7\u63a8\u7406\u4e5f\u80fd\u5b8c\u6210\u63a8\u7406\u4efb\u52a1\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u65b0\u578b\u63a8\u7406\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u63a8\u7406\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u7f3a\u4e4f\u57fa\u7840\u5e38\u8bc6\u4e14\u5177\u6709\u8106\u5f31\u6027\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u63a8\u7406\u7684\u672c\u8d28\u53ca\u5176\u5fc5\u8981\u6761\u4ef6\u3002", "method": "\u8bba\u6587\u91c7\u7528\u54f2\u5b66\u5206\u6790\u65b9\u6cd5\uff0c\u63a2\u8ba8\u57fa\u7840\u6a21\u578b\u63a8\u7406\u73b0\u8c61\u7684\u591a\u91cd\u54f2\u5b66\u89e3\u91ca\uff0c\u8bba\u8bc1\"\u968f\u673a\u9e66\u9e49\"\u6bd4\u55bb\u5df2\u5931\u53bb\u76f8\u5173\u6027\uff0c\u5e76\u53cd\u601d\u7531\u8fd9\u4e9b\u63a8\u7406\u6a21\u578b\u53ca\u5176\u589e\u957f\u80fd\u529b\u5f15\u53d1\u7684\u5b89\u5168\u6027\u548c\u9002\u5f53\u6027\u89c4\u8303\u8003\u91cf\u3002", "result": "\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u6a21\u4eff\"\u5927\u58f0\u601d\u8003\"\u8fc7\u7a0b\u3001\u6d4b\u8bd5\u751f\u6210\u8def\u5f84\u5e76\u8fed\u4ee3\uff0c\u80fd\u591f\u72ec\u7acb\u6216\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u8fd9\u79cd\u63a8\u7406\u7f3a\u4e4f\u57fa\u7840\u5e38\u8bc6\u548c\u7a33\u5b9a\u6027\uff0c\u4e0e\u4eba\u7c7b\u7b26\u53f7\u63a8\u7406\u6709\u6839\u672c\u5dee\u5f02\u3002\u8fd9\u6539\u53d8\u4e86\u6211\u4eec\u5bf9\u63a8\u7406\u5fc5\u8981\u6761\u4ef6\u7684\u8bc4\u4f30\uff0c\u5e76\u9700\u8981\u65b0\u7684\u5b89\u5168\u9632\u5fa1\u65b9\u6cd5\u6765\u5e94\u5bf9\u5176\u8106\u5f31\u6027\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u5c55\u793a\u7684\u65b0\u578b\u63a8\u7406\u80fd\u529b\u6311\u6218\u4e86\u4f20\u7edf\u7b26\u53f7\u63a8\u7406\u7684\u5fc5\u8981\u6027\u5047\u8bbe\uff0c\"\u968f\u673a\u9e66\u9e49\"\u6bd4\u55bb\u5df2\u4e0d\u518d\u9002\u7528\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003\u63a8\u7406\u7684\u672c\u8d28\uff0c\u5e76\u5efa\u7acb\u9488\u5bf9\u8fd9\u79cd\u65b0\u578b\u63a8\u7406\u8106\u5f31\u6027\u7684\u5b89\u5168\u6846\u67b6\u548c\u89c4\u8303\u8003\u91cf\uff0c\u4ee5\u5e94\u5bf9\u5176\u65e5\u76ca\u589e\u957f\u7684\u80fd\u529b\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2601.01224", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01224", "abs": "https://arxiv.org/abs/2601.01224", "authors": ["Bac Nguyen", "Yuhta Takida", "Naoki Murata", "Chieh-Hsin Lai", "Toshimitsu Uesaka", "Stefano Ermon", "Yuki Mitsufuji"], "title": "Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment", "comment": null, "summary": "Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.", "AI": {"tldr": "CODA\u901a\u8fc7\u5f15\u5165\u5bc4\u5b58\u5668\u69fd\u548c\u5bf9\u6bd4\u5bf9\u9f50\u635f\u5931\uff0c\u89e3\u51b3\u4e86Slot Attention\u5728\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u4e2d\u7684\u69fd\u7ea0\u7f20\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u53d1\u73b0\u548c\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "motivation": "Slot Attention\u4e0e\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7ed3\u5408\u5728\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u69fd\u7ea0\u7f20\u548c\u5bf9\u8c61\u69fd\u4e0e\u56fe\u50cf\u5185\u5bb9\u5bf9\u9f50\u5f31\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faCODA\u6846\u67b6\uff1a(1)\u4f7f\u7528\u5bc4\u5b58\u5668\u69fd\u5438\u6536\u6b8b\u5dee\u6ce8\u610f\u529b\uff0c\u51cf\u5c11\u5bf9\u8c61\u69fd\u95f4\u7684\u5e72\u6270\uff1b(2)\u5e94\u7528\u5bf9\u6bd4\u5bf9\u9f50\u635f\u5931\u663e\u5f0f\u9f13\u52b1\u69fd-\u56fe\u50cf\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f5c\u4e3a\u6700\u5927\u5316\u69fd\u4e0e\u8f93\u5165\u95f4\u4e92\u4fe1\u606f\u7684\u53ef\u5904\u7406\u66ff\u4ee3\u76ee\u6807\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6(MOVi-C/E)\u548c\u771f\u5b9e\u6570\u636e\u96c6(VOC, COCO)\u4e0a\uff0cCODA\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u53d1\u73b0\u6027\u80fd(\u5982COCO\u4e0aFG-ARI\u63d0\u53476.1%)\u3001\u5c5e\u6027\u9884\u6d4b\u548c\u7ec4\u5408\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0c\u5bc4\u5b58\u5668\u69fd\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "CODA\u4f5c\u4e3a\u6709\u6548\u7684\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u7b80\u5355\u6269\u5c55\u89e3\u51b3\u4e86\u69fd\u7ea0\u7f20\u548c\u5bf9\u9f50\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.01119", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01119", "abs": "https://arxiv.org/abs/2601.01119", "authors": ["Muhammad Ashad Kabir", "Sirajam Munira", "Dewan Tasnia Azad", "Saleh Mohammed Ikram", "Mohammad Habibur Rahman Sarker", "Syed Manzoor Ahmed Hanifi"], "title": "Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings", "comment": "27 pages", "summary": "Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u4eba\u7fa4\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u793e\u533a\u65e9\u671f\u6162\u6027\u80be\u75c5\u7b5b\u67e5\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u3002", "motivation": "\u73b0\u6709\u6162\u6027\u80be\u75c5\u7b5b\u67e5\u5de5\u5177\u4e3b\u8981\u57fa\u4e8e\u9ad8\u6536\u5165\u56fd\u5bb6\u4eba\u7fa4\u5f00\u53d1\uff0c\u5728\u5b5f\u52a0\u62c9\u56fd\u548c\u5357\u4e9a\u5730\u533a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5927\u591a\u4f9d\u8d56\u7b80\u5355\u7684\u52a0\u6cd5\u8bc4\u5206\u51fd\u6570\uff0c\u57fa\u4e8e\u665a\u671f\u80be\u75c5\u6570\u636e\uff0c\u65e0\u6cd5\u6355\u6349\u98ce\u9669\u56e0\u7d20\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u96be\u4ee5\u9884\u6d4b\u65e9\u671f\u80be\u75c5\u3002", "method": "\u4f7f\u7528\u5b5f\u52a0\u62c9\u56fd\u793e\u533a\u6570\u636e\u96c6\uff08\u5357\u4e9a\u9996\u4e2a\u6b64\u7c7b\u6570\u636e\u96c6\uff09\uff0c\u8bc4\u4f3012\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u5e94\u752810\u79cd\u4e92\u8865\u7279\u5f81\u9009\u62e9\u6280\u672f\u8bc6\u522b\u7a33\u5065\u9884\u6d4b\u56e0\u5b50\uff0c\u91c7\u752810\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5728\u5370\u5ea6\u3001\u963f\u8054\u914b\u548c\u5b5f\u52a0\u62c9\u56fd\u7684\u4e09\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\uff0c\u4f7f\u7528SHAP\u63d0\u4f9b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "RFECV\u7279\u5f81\u5b50\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5e73\u8861\u51c6\u786e\u7387\u8fbe90.40%\uff0c\u6700\u5c0f\u975e\u75c5\u7406\u6d4b\u8bd5\u7279\u5f81\u96c6\u5e73\u8861\u51c6\u786e\u7387\u8fbe89.23%\uff0c\u5e38\u4f18\u4e8e\u66f4\u5927\u6216\u5b8c\u6574\u7279\u5f81\u96c6\u3002\u76f8\u6bd4\u73b0\u6709\u7b5b\u67e5\u5de5\u5177\uff0c\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u663e\u8457\u63d0\u9ad8\u4e14\u8f93\u5165\u66f4\u5c11\u3002\u5916\u90e8\u9a8c\u8bc1\u663e\u793a78%-98%\u654f\u611f\u6027\uff0cSHAP\u8bc6\u522b\u51fa\u4e0e\u5df2\u77e5CKD\u98ce\u9669\u56e0\u7d20\u4e00\u81f4\u7684\u4e34\u5e8a\u6709\u610f\u4e49\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u5f00\u53d1\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e3a\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u793e\u533a\u65e9\u671f\u6162\u6027\u80be\u75c5\u7b5b\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9488\u5bf9\u5357\u4e9a\u4eba\u7fa4\u5b9a\u5236\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2601.02071", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02071", "abs": "https://arxiv.org/abs/2601.02071", "authors": ["Adeshola Okubena", "Yusuf Ali Mohammed", "Moe Elbadawi"], "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations", "comment": null, "summary": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u836f\u72693D\u6253\u5370\u914d\u65b9\u5f00\u53d1\uff0c\u901a\u8fc7\u5fae\u8c03\u56db\u79cdLLM\u67b6\u6784\u57281400\u591a\u4e2aFDM\u914d\u65b9\u6570\u636e\u96c6\u4e0a\uff0c\u8bc4\u4f30\u5176\u5728\u63a8\u8350\u8f85\u6599\u548c\u9884\u6d4b\u4e1d\u6750\u673a\u68b0\u6027\u80fd\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dAI\u9a71\u52a8\u7684\u836f\u72693D\u6253\u5370\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u72ed\u7a84\u9886\u57df\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u8be5\u6280\u672f\u56fa\u6709\u7684\u5e7f\u6cdb\u914d\u65b9\u6311\u6218\u3002\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5411\u901a\u7528\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\uff0c\u9700\u8981\u63a2\u7d22LLM\u5728\u836f\u7269\u914d\u65b9\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u5fae\u8c03\u4e86\u56db\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u4f7f\u7528\u5305\u542b1400\u591a\u4e2a\u7194\u878d\u6c89\u79ef\u6210\u578b\u914d\u65b9\u7684\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5fae\u8c03\u548c\u751f\u6210\u53c2\u6570\u914d\u7f6e\u3002\u6a21\u578b\u88ab\u8bad\u7ec3\u7528\u4e8e\u57fa\u4e8eAPI\u5242\u91cf\u63a8\u8350\u5408\u9002\u7684\u8f85\u6599\uff0c\u5e76\u9884\u6d4b\u4e1d\u6750\u7684\u673a\u68b0\u6027\u80fd\u3002", "result": "Llama2\u6a21\u578b\u5728\u63a8\u8350FDM\u914d\u65b9\u8f85\u6599\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002\u6a21\u578b\u9009\u62e9\u548c\u53c2\u6570\u5316\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u8f83\u5c0f\u7684LLM\u51fa\u73b0\u4e86\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\u3002\u7814\u7a76\u53d1\u73b0\uff1a\u5373\u4f7f\u76f8\u5bf9\u8f83\u5c0f\u76841400\u591a\u4e2a\u914d\u65b9\u6570\u636e\u96c6\u4e5f\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u707e\u96be\u6027\u9057\u5fd8\uff1b\u6807\u51c6LLM\u6307\u6807\u4ec5\u8bc4\u4f30\u8bed\u8a00\u6027\u80fd\u800c\u975e\u914d\u65b9\u53ef\u52a0\u5de5\u6027\uff1b\u57fa\u4e8e\u751f\u7269\u533b\u5b66\u76f8\u5173\u6570\u636e\u8bad\u7ec3\u7684LLM\u5e76\u4e0d\u603b\u662f\u4ea7\u751f\u6700\u4f73\u7ed3\u679c\u3002", "conclusion": "\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u5bf9\u4e8e\u63a8\u52a8LLM\u8d85\u8d8a\u8bed\u8a00\u719f\u7ec3\u5ea6\uff0c\u53d1\u5c55\u6210\u4e3a\u836f\u7269\u914d\u65b9\u5f00\u53d1\u7684\u53ef\u9760\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u6765\u786e\u4fddLLM\u5728\u836f\u72693D\u6253\u5370\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u3002"}}
{"id": "2601.01240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01240", "abs": "https://arxiv.org/abs/2601.01240", "authors": ["Ziqian Guan", "Xieyi Fu", "Yuting Wang", "Haowen Xiao", "Jiarui Zhu", "Yingying Zhu", "Yongtao Liu", "Lin Gu"], "title": "RFAssigner: A Generic Label Assignment Strategy for Dense Object Detection", "comment": null, "summary": "Label assignment is a critical component in training dense object detectors. State-of-the-art methods typically assign each training sample a positive and a negative weight, optimizing the assignment scheme during training. However, these strategies often assign an insufficient number of positive samples to small objects, leading to a scale imbalance during training. To address this limitation, we introduce RFAssigner, a novel assignment strategy designed to enhance the multi-scale learning capabilities of dense detectors. RFAssigner first establishes an initial set of positive samples using a point-based prior. It then leverages a Gaussian Receptive Field (GRF) distance to measure the similarity between the GRFs of unassigned candidate locations and the ground-truth objects. Based on this metric, RFAssigner adaptively selects supplementary positive samples from the unassigned pool, promoting a more balanced learning process across object scales. Comprehensive experiments on three datasets with distinct object scale distributions validate the effectiveness and generalizability of our method. Notably, a single FCOS-ResNet-50 detector equipped with RFAssigner achieves state-of-the-art performance across all object scales, consistently outperforming existing strategies without requiring auxiliary modules or heuristics.", "AI": {"tldr": "RFAssigner\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6807\u7b7e\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u9ad8\u65af\u611f\u53d7\u91ce\u8ddd\u79bb\u5ea6\u91cf\u5019\u9009\u4f4d\u7f6e\u4e0e\u771f\u5b9e\u7269\u4f53\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u81ea\u9002\u5e94\u8865\u5145\u6b63\u6837\u672c\uff0c\u89e3\u51b3\u5bc6\u96c6\u76ee\u6807\u68c0\u6d4b\u5668\u4e2d\u5c3a\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bc6\u96c6\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u6807\u7b7e\u5206\u914d\u7b56\u7565\u901a\u5e38\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u5206\u914d\u6b63\u8d1f\u6743\u91cd\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5f80\u5f80\u4e3a\u5c0f\u7269\u4f53\u5206\u914d\u7684\u6b63\u6837\u672c\u6570\u91cf\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5c3a\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "RFAssigner\u9996\u5148\u4f7f\u7528\u57fa\u4e8e\u70b9\u7684\u5148\u9a8c\u5efa\u7acb\u521d\u59cb\u6b63\u6837\u672c\u96c6\uff0c\u7136\u540e\u5229\u7528\u9ad8\u65af\u611f\u53d7\u91ce\u8ddd\u79bb\u5ea6\u91cf\u672a\u5206\u914d\u5019\u9009\u4f4d\u7f6e\u4e0e\u771f\u5b9e\u7269\u4f53\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u57fa\u4e8e\u6b64\u5ea6\u91cf\u81ea\u9002\u5e94\u5730\u4ece\u672a\u5206\u914d\u6c60\u4e2d\u9009\u62e9\u8865\u5145\u6b63\u6837\u672c\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u4e0d\u540c\u7269\u4f53\u5c3a\u5ea6\u5206\u5e03\u7684\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u914d\u5907RFAssigner\u7684\u5355\u4e2aFCOS-ResNet-50\u68c0\u6d4b\u5668\u5728\u6240\u6709\u7269\u4f53\u5c3a\u5ea6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7b56\u7565\uff0c\u4e14\u65e0\u9700\u8f85\u52a9\u6a21\u5757\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "RFAssigner\u901a\u8fc7\u81ea\u9002\u5e94\u8865\u5145\u6b63\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5bc6\u96c6\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5c3a\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5c3a\u5ea6\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u5bc6\u96c6\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u5e73\u8861\u7684\u6807\u7b7e\u5206\u914d\u65b9\u6848\u3002"}}
{"id": "2601.01127", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01127", "abs": "https://arxiv.org/abs/2601.01127", "authors": ["Golbahar Amanpour", "Benyamin Ghojogh"], "title": "Wittgenstein's Family Resemblance Clustering Algorithm", "comment": null, "summary": "This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ef4\u7279\u6839\u65af\u5766\u5bb6\u65cf\u76f8\u4f3c\u6027\u54f2\u5b66\u6982\u5ff5\u7684\u805a\u7c7b\u7b97\u6cd5WFR\uff0c\u65e0\u9700\u9884\u5148\u6307\u5b9a\u805a\u7c7b\u6570\u91cf\u6216\u5047\u8bbe\u805a\u7c7b\u5f62\u72b6\uff0c\u901a\u8fc7\u6784\u5efa\u76f8\u4f3c\u6027\u56fe\u5b9e\u73b0\u975e\u7ebf\u6027\u805a\u7c7b\u3002", "motivation": "\u53d7\u7ef4\u7279\u6839\u65af\u5766\u54f2\u5b66\u4e2d\"\u5bb6\u65cf\u76f8\u4f3c\u6027\"\u6982\u5ff5\u7684\u542f\u53d1\uff0c\u8be5\u6982\u5ff5\u8ba4\u4e3a\u7c7b\u522b\u6210\u5458\u901a\u8fc7\u91cd\u53e0\u7684\u76f8\u4f3c\u6027\u800c\u975e\u5355\u4e00\u5171\u540c\u7279\u5f81\u76f8\u8054\u7cfb\uff0c\u8fd9\u79cd\u601d\u60f3\u81ea\u7136\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u56fe\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u65b0\u7684\u805a\u7c7b\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86WFR\uff08\u7ef4\u7279\u6839\u65af\u5766\u5bb6\u65cf\u76f8\u4f3c\u6027\uff09\u805a\u7c7b\u7b97\u6cd5\u53ca\u5176\u6838\u53d8\u4f53kernel WFR\u3002\u7b97\u6cd5\u8ba1\u7b97\u76f8\u90bb\u6570\u636e\u5b9e\u4f8b\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u5f97\u5206\uff0c\u901a\u8fc7\u9608\u503c\u5904\u7406\u540e\u6784\u5efa\u76f8\u4f3c\u6027\u56fe\uff0c\u8be5\u56fe\u7684\u8fde\u901a\u5206\u91cf\u5373\u5f62\u6210\u6700\u7ec8\u805a\u7c7b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0cWFR\u662f\u4e00\u79cd\u6709\u6548\u7684\u975e\u7ebf\u6027\u805a\u7c7b\u7b97\u6cd5\uff0c\u4e0d\u9700\u8981\u9884\u5148\u77e5\u9053\u805a\u7c7b\u6570\u91cf\uff0c\u4e5f\u4e0d\u9700\u8981\u5bf9\u805a\u7c7b\u5f62\u72b6\u505a\u51fa\u5047\u8bbe\u3002", "conclusion": "\u5c06\u54f2\u5b66\u6982\u5ff5\uff08\u7ef4\u7279\u6839\u65af\u5766\u7684\u5bb6\u65cf\u76f8\u4f3c\u6027\uff09\u4e0e\u673a\u5668\u5b66\u4e60\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u6210\u529f\u5f00\u53d1\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u805a\u7c7b\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u8de8\u5b66\u79d1\u65b9\u6cd5\u5728\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.01260", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01260", "abs": "https://arxiv.org/abs/2601.01260", "authors": ["Hamad Khan", "Saddam Hussain Khan"], "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance", "comment": "28 Pages, Tables 12, Figure 09", "summary": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.", "AI": {"tldr": "\u63d0\u51faMambaFormer\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u7ed3\u5408Transformer\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e13\u5bb6\uff0c\u901a\u8fc7\u667a\u80fd\u8def\u7531\u673a\u5236\u5728\u533b\u7597\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u8ba1\u7b97\u6210\u672c\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u8ba1\u7b97\u6210\u672c\u4e0e\u7ebf\u6027\u65f6\u95f4\u6a21\u578b\u6548\u7387\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLLM-based MambaFormer\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u5305\u542b\u8f7b\u91cf\u7ea7\u95e8\u63a7\u673a\u5236\u8fdb\u884ctoken\u7ea7\u52a8\u6001\u8def\u7531\uff1a\u590d\u6742\u77ed\u67e5\u8be2\u8def\u7531\u5230Transformer\u4e13\u5bb6(ET5)\uff0c\u957f\u9ad8\u541e\u5410\u5e8f\u5217\u8def\u7531\u5230\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e13\u5bb6(EMamba)\u3002\u91c7\u7528\u65b0\u9896\u7684\u6548\u7528\u5f15\u5bfc\u591a\u76ee\u6807\u635f\u5931\u8054\u5408\u4f18\u5316\u8def\u7531\u51b3\u7b56\u3001\u53c2\u6570\u548c\u884c\u4e3a\u3002", "result": "\u5728DentalQA\u548cPubMedQA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cMambaFormer\u83b7\u5f97BERTScore=0.9180\uff0c\u8d85\u4f4e\u5ef6\u8fdf0.077\u79d2\uff0c\u6bd4T5-Large\u5feb24.4\u500d\uff0c\u5728\u63a8\u7406\u5ef6\u8fdf\u548c\u9884\u6d4b\u51c6\u786e\u6027\u4e4b\u95f4\u5b9e\u73b0\u5e15\u7d2f\u6258\u6700\u4f18\u6743\u8861\u3002", "conclusion": "MambaFormer\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u4e13\u5bb6\u8def\u7531\u673a\u5236\uff0c\u5728\u533b\u7597\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6700\u4f73\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02170", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02170", "abs": "https://arxiv.org/abs/2601.02170", "authors": ["Haolang Lu", "Minghui Pan", "Ripeng Li", "Guoshun Nan", "Jialin Zhuang", "Zijie Zhao", "Zhongxiang Sun", "Kun Wang", "Yang Liu"], "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning", "comment": null, "summary": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u89c6\u4e3a\u6f14\u5316\u6f5c\u72b6\u6001\u800c\u975e\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\uff0c\u5f15\u5165\u7d2f\u79ef\u524d\u7f00\u7ea7\u5e7b\u89c9\u4fe1\u53f7\u6765\u8ffd\u8e2a\u63a8\u7406\u72b6\u6001\u7684\u5168\u5c40\u6f14\u5316\uff0c\u5b9e\u73b0\u6d41\u5f0f\u5e7b\u89c9\u68c0\u6d4b", "motivation": "\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u867d\u7136\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5176\u4e2d\u7684\u5e7b\u89c9\u5f80\u5f80\u4ee5\u5fae\u5999\u65b9\u5f0f\u51fa\u73b0\u5e76\u5728\u63a8\u7406\u6b65\u9aa4\u95f4\u4f20\u64ad\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u5e7b\u89c9\u89c6\u4e3a\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u66f4\u9002\u5408\u88ab\u7406\u89e3\u4e3a\u6f14\u5316\u4e2d\u7684\u6f5c\u72b6\u6001\uff0c\u9700\u8981\u5168\u5c40\u89c6\u89d2\u6765\u8ffd\u8e2a\u5176\u6f14\u53d8\u8fc7\u7a0b", "method": "\u5c06\u6b65\u9aa4\u7ea7\u5e7b\u89c9\u5224\u65ad\u89c6\u4e3a\u5c40\u90e8\u89c2\u6d4b\uff0c\u5f15\u5165\u7d2f\u79ef\u524d\u7f00\u7ea7\u5e7b\u89c9\u4fe1\u53f7\u6765\u8ffd\u8e2a\u6574\u4e2a\u63a8\u7406\u8f68\u8ff9\u4e2d\u63a8\u7406\u72b6\u6001\u7684\u5168\u5c40\u6f14\u5316\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u6d41\u5f0f\u5e7b\u89c9\u68c0\u6d4b\uff0c\u63d0\u4f9b\u5b9e\u65f6\u3001\u53ef\u89e3\u91ca\u7684\u8bc1\u636e", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u6d41\u5f0f\u5e7b\u89c9\u68c0\u6d4b\uff0c\u63d0\u4f9b\u5b9e\u65f6\u76d1\u63a7\u548c\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u7406\u89e3\u548c\u5904\u7406\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5e7b\u89c9\u6f14\u5316", "conclusion": "\u5c06\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u89c6\u4e3a\u6f14\u5316\u6f5c\u72b6\u6001\u800c\u975e\u4e00\u6b21\u6027\u9519\u8bef\u4e8b\u4ef6\uff0c\u901a\u8fc7\u7d2f\u79ef\u524d\u7f00\u7ea7\u4fe1\u53f7\u8ffd\u8e2a\u5168\u5c40\u6f14\u5316\uff0c\u4e3a\u6d41\u5f0f\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u589e\u5f3a\u4e86\u5b9e\u65f6\u76d1\u63a7\u548c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2601.01146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01146", "abs": "https://arxiv.org/abs/2601.01146", "authors": ["Anusree M", "Akhila Henry", "Pramod P Nair"], "title": "Self-Training the Neurochaos Learning Algorithm", "comment": null, "summary": "In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u4e0e\u81ea\u8bad\u7ec3\u7684\u534a\u76d1\u7763\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u83b7\u53d6\u5927\u91cf\u6807\u6ce8\u6570\u636e\u65e2\u56f0\u96be\u53c8\u6602\u8d35\uff0c\u800c\u672a\u6807\u6ce8\u6570\u636e\u5219\u5bb9\u6613\u83b7\u53d6\u3002\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u7a00\u5c11\u6216\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u534a\u76d1\u7763\u5b66\u4e60\u67b6\u6784\uff0c\u5c06\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u4e0e\u57fa\u4e8e\u9608\u503c\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u5c06\u8f93\u5165\u7279\u5f81\u8f6c\u6362\u4e3a\u6df7\u6c8c\u53d1\u653e\u7387\u8868\u793a\uff0c\u6355\u6349\u6570\u636e\u4e2d\u7684\u975e\u7ebf\u6027\u5173\u7cfb\uff1b\u81ea\u8bad\u7ec3\u65b9\u6cd5\u5219\u5229\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u4f2a\u6807\u6ce8\u6837\u672c\u9010\u6b65\u6269\u5927\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c5\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5176\u4e2d85%\u7684\u8bad\u7ec3\u6570\u636e\u88ab\u89c6\u4e3a\u672a\u6807\u6ce8\uff0c\u4ec515%\u7528\u4f5c\u6807\u6ce8\u6570\u636e\u3002\u63d0\u51fa\u7684\u81ea\u8bad\u7ec3\u795e\u7ecf\u6df7\u6c8c\u5b66\u4e60\u67b6\u6784\u76f8\u6bd4\u72ec\u7acb\u7684\u81ea\u8bad\u7ec3\u6a21\u578b\u83b7\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u3001\u975e\u7ebf\u6027\u548c\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u5982Iris\uff08188.66%\uff09\u3001Wine\uff08158.58%\uff09\u548cGlass Identification\uff08110.48%\uff09\u3002", "conclusion": "\u5c06\u57fa\u4e8e\u6df7\u6c8c\u7684\u7279\u5f81\u63d0\u53d6\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u80fd\u591f\u6539\u5584\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u4e3a\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.01150", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01150", "abs": "https://arxiv.org/abs/2601.01150", "authors": ["Wenbin Pei", "Ruohao Dai", "Bing Xue", "Mengjie Zhang", "Qiang Zhang", "Yiu-Ming Cheung"], "title": "Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification", "comment": null, "summary": "Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.", "AI": {"tldr": "Evo-TFS\uff1a\u4e00\u79cd\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u7684\u8fdb\u5316\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u7c7b\u578b\u9057\u4f20\u7f16\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u6837\u672c\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u5206\u5e03\u5e73\u8861\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u5f80\u5f80\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u5c11\u6570\u7c7b\u88ab\u5ffd\u7565\u3002\u73b0\u6709\u7684\u8fc7\u91c7\u6837\u65b9\u6cd5\u4f9d\u8d56\u7ebf\u6027\u63d2\u503c\uff0c\u96be\u4ee5\u4fdd\u6301\u65f6\u95f4\u52a8\u6001\u7279\u6027\u548c\u751f\u6210\u591a\u6837\u6837\u672c\u3002", "method": "\u63d0\u51faEvo-TFS\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f3a\u7c7b\u578b\u9057\u4f20\u7f16\u7a0b\u540c\u65f6\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u6765\u8fdb\u5316\u751f\u6210\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u65f6\u95f4\u5e8f\u5217\u6837\u672c\u3002\u901a\u8fc7\u5305\u542b\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u7684\u9002\u5e94\u5ea6\u51fd\u6570\u6765\u6307\u5bfc\u8fdb\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEvo-TFS\u4f18\u4e8e\u73b0\u6709\u7684\u8fc7\u91c7\u6837\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u57df\u548c\u9891\u57df\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "Evo-TFS\u901a\u8fc7\u8fdb\u5316\u65b9\u6cd5\u7ed3\u5408\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5c11\u6570\u7c7b\u6837\u672c\uff0c\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\u3002"}}
{"id": "2601.02346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02346", "abs": "https://arxiv.org/abs/2601.02346", "authors": ["Falcon LLM Team", "Iheb Chaabane", "Puneesh Khanna", "Suhail Mohmad", "Slim Frikha", "Shi Hu", "Abdalgader Abubaker", "Reda Alami", "Mikhail Lubinets", "Mohamed El Amine Seddik", "Hakim Hacid"], "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling", "comment": null, "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.", "AI": {"tldr": "Falcon-H1R\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u63a8\u7406\u4f18\u5316\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u6bd4\u5b83\u59272-7\u500d\u7684SOTA\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u7b56\u5c55\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u5f53\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5e76\u884c\u67b6\u6784\u8bbe\u8ba1\u5b9e\u73b0\u66f4\u5feb\u63a8\u7406\uff0c\u901a\u8fc7\u9ad8\u6548\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u8fdb\u884c\u9488\u5bf9\u6027\u8bad\u7ec3\uff0c\u5229\u7528DeepConf\u65b9\u6cd5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6548\u7387\uff0c\u7ed3\u5408\u6570\u636e\u7b56\u5c55\u548c\u67b6\u6784\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFalcon-H1R-7B\u6a21\u578b\u4e00\u81f4\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u6bd4\u5b83\u59272-7\u500d\u7684SOTA\u63a8\u7406\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u6548\u7387\u76843D\u6781\u9650\uff08\u66f4\u5feb\u63a8\u7406\u3001\u66f4\u9ad8token\u6548\u7387\u3001\u66f4\u9ad8\u51c6\u786e\u6027\uff09\u3002", "conclusion": "\u7d27\u51d1\u6a21\u578b\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u67b6\u6784\u9009\u62e9\uff0c\u80fd\u591f\u63d0\u4f9b\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u63a8\u7406\u6027\u80fd\uff0cFalcon-H1R-7B\u6210\u4e3a\u6269\u5c55\u9ad8\u7ea7\u63a8\u7406\u7cfb\u7edf\u7684\u5b9e\u7528\u9aa8\u5e72\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5927\u91cf\u601d\u7ef4\u94fe\u751f\u6210\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u573a\u666f\u3002"}}
{"id": "2601.01162", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01162", "abs": "https://arxiv.org/abs/2601.01162", "authors": ["Zihua Yang", "Xin Liao", "Yiqun Zhang", "Yiu-ming Cheung"], "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models", "comment": "Submitted to ICPR 2026", "summary": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE", "AI": {"tldr": "ARISE\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5916\u90e8\u8bed\u4e49\u77e5\u8bc6\u589e\u5f3a\u5206\u7c7b\u6570\u636e\u7684\u805a\u7c7b\u6548\u679c\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u8868\u793a\u5f25\u8865\u4f20\u7edf\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u7684\u4e0d\u8db3\uff0c\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u76f8\u6bd47\u4e2a\u4ee3\u8868\u6027\u65b9\u6cd5\u63d0\u534719-27%", "motivation": "\u5206\u7c7b\u6570\u636e\u805a\u7c7b\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u662f\u7f3a\u4e4f\u56fa\u6709\u6392\u5e8f\u6216\u8ddd\u79bb\u7684\u5c5e\u6027\u503c\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u96c6\u5185\u5171\u73b0\u6a21\u5f0f\u63a8\u65ad\u503c\u5173\u7cfb\uff0c\u4f46\u5728\u6837\u672c\u6709\u9650\u65f6\u4e0d\u53ef\u9760\uff0c\u5bfc\u81f4\u8bed\u4e49\u4e0a\u4e0b\u6587\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9020\u6210\u8bed\u4e49\u9e3f\u6c9f\u5e76\u964d\u4f4e\u805a\u7c7b\u8d28\u91cf\u3002", "method": "\u63d0\u51faARISE\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5916\u90e8\u8bed\u4e49\u77e5\u8bc6\u6784\u5efa\u8bed\u4e49\u611f\u77e5\u8868\u793a\uff0c\u8865\u5145\u5206\u7c7b\u6570\u636e\u7684\u5ea6\u91cf\u7a7a\u95f4\u3002\u5177\u4f53\u91c7\u7528LLM\u63cf\u8ff0\u5c5e\u6027\u503c\u4ee5\u589e\u5f3a\u8868\u793a\uff0c\u5c06LLM\u589e\u5f3a\u7684\u5d4c\u5165\u4e0e\u539f\u59cb\u6570\u636e\u7ed3\u5408\uff0c\u63a2\u7d22\u8bed\u4e49\u663e\u8457\u7684\u805a\u7c7b\u7ed3\u6784\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd47\u4e2a\u4ee3\u8868\u6027\u5bf9\u6bd4\u65b9\u6cd5\uff0cARISE\u5b9e\u73b0\u4e8619-27%\u7684\u6301\u7eed\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u5916\u90e8\u8bed\u4e49\u77e5\u8bc6\u589e\u5f3a\u5206\u7c7b\u6570\u636e\u805a\u7c7b\u7684\u6709\u6548\u6027\u3002", "conclusion": "ARISE\u901a\u8fc7\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5916\u90e8\u8bed\u4e49\u77e5\u8bc6\uff0c\u6709\u6548\u5f25\u8865\u4e86\u5206\u7c7b\u6570\u636e\u805a\u7c7b\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u8d28\u91cf\uff0c\u4e3a\u6709\u9650\u6837\u672c\u4e0b\u7684\u5206\u7c7b\u6570\u636e\u805a\u7c7b\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01312", "abs": "https://arxiv.org/abs/2601.01312", "authors": ["Kailash A. Hambarde", "Hugo Proen\u00e7a", "Md Rashidunnabi", "Pranita Samale", "Qiwei Yang", "Pingping Zhang", "Zijing Gong", "Yuhao Wang", "Xi Zhang", "Ruoshui Qu", "Qiaoyun He", "Yuhang Zhang", "Thi Ngoc Ha Nguyen", "Tien-Dung Mai", "Cheng-Jun Kang", "Yu-Fan Lin", "Jin-Hui Jiang", "Chih-Chung Hsu", "Tam\u00e1s Endrei", "Gy\u00f6rgy Cserey", "Ashwat Rajbhandari"], "title": "VReID-XFD: Video-based Person Re-identification at Extreme Far Distance Challenge Results", "comment": null, "summary": "Person re-identification (ReID) across aerial and ground views at extreme far distances introduces a distinct operating regime where severe resolution degradation, extreme viewpoint changes, unstable motion cues, and clothing variation jointly undermine the appearance-based assumptions of existing ReID systems. To study this regime, we introduce VReID-XFD, a video-based benchmark and community challenge for extreme far-distance (XFD) aerial-to-ground person re-identification. VReID-XFD is derived from the DetReIDX dataset and comprises 371 identities, 11,288 tracklets, and 11.75 million frames, captured across altitudes from 5.8 m to 120 m, viewing angles from oblique (30 degrees) to nadir (90 degrees), and horizontal distances up to 120 m. The benchmark supports aerial-to-aerial, aerial-to-ground, and ground-to-aerial evaluation under strict identity-disjoint splits, with rich physical metadata. The VReID-XFD-25 Challenge attracted 10 teams with hundreds of submissions. Systematic analysis reveals monotonic performance degradation with altitude and distance, a universal disadvantage of nadir views, and a trade-off between peak performance and robustness. Even the best-performing SAS-PReID method achieves only 43.93 percent mAP in the aerial-to-ground setting. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/ .", "AI": {"tldr": "VReID-XFD\u662f\u4e00\u4e2a\u9488\u5bf9\u6781\u7aef\u8fdc\u8ddd\u79bb\uff08XFD\uff09\u7a7a\u4e2d\u5230\u5730\u9762\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u548c\u793e\u533a\u6311\u6218\uff0c\u5305\u542b371\u4e2a\u8eab\u4efd\u300111,288\u4e2a\u8f68\u8ff9\u548c1175\u4e07\u5e27\uff0c\u8986\u76d65.8\u7c73\u5230120\u7c73\u9ad8\u5ea6\u300130\u5ea6\u523090\u5ea6\u89c6\u89d2\u548c120\u7c73\u6c34\u5e73\u8ddd\u79bb\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u968f\u9ad8\u5ea6\u548c\u8ddd\u79bb\u5355\u8c03\u4e0b\u964d\u7684\u8d8b\u52bf\u3002", "motivation": "\u73b0\u6709\u884c\u4eba\u91cd\u8bc6\u522b\u7cfb\u7edf\u57fa\u4e8e\u5916\u89c2\u7684\u5047\u8bbe\u5728\u6781\u7aef\u8fdc\u8ddd\u79bb\u7684\u7a7a\u4e2d\u5230\u5730\u9762\u573a\u666f\u4e2d\u9762\u4e34\u4e25\u91cd\u6311\u6218\uff0c\u5305\u62ec\u5206\u8fa8\u7387\u4e25\u91cd\u9000\u5316\u3001\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u3001\u4e0d\u7a33\u5b9a\u8fd0\u52a8\u7ebf\u7d22\u548c\u670d\u88c5\u53d8\u5316\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7814\u7a76\u57fa\u51c6\u6765\u63a2\u7d22\u8fd9\u4e00\u72ec\u7279\u64cd\u4f5c\u673a\u5236\u3002", "method": "\u57fa\u4e8eDetReIDX\u6570\u636e\u96c6\u6784\u5efaVReID-XFD\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b371\u4e2a\u8eab\u4efd\u300111,288\u4e2a\u8f68\u8ff9\u548c11.75\u767e\u4e07\u5e27\uff0c\u8986\u76d65.8-120\u7c73\u9ad8\u5ea6\u300130-90\u5ea6\u89c6\u89d2\u548c120\u7c73\u6c34\u5e73\u8ddd\u79bb\uff0c\u652f\u6301\u7a7a\u4e2d\u5230\u7a7a\u4e2d\u3001\u7a7a\u4e2d\u5230\u5730\u9762\u548c\u5730\u9762\u5230\u7a7a\u4e2d\u7684\u4e25\u683c\u8eab\u4efd\u5206\u79bb\u8bc4\u4f30\uff0c\u5e76\u4e3e\u529eVReID-XFD-25\u6311\u6218\u8d5b\u5438\u5f1510\u4e2a\u56e2\u961f\u53c2\u4e0e\u3002", "result": "\u7cfb\u7edf\u5206\u6790\u663e\u793a\u6027\u80fd\u968f\u9ad8\u5ea6\u548c\u8ddd\u79bb\u5355\u8c03\u4e0b\u964d\uff0c\u5929\u5e95\u89c6\u89d2\u666e\u904d\u5904\u4e8e\u52a3\u52bf\uff0c\u5b58\u5728\u5cf0\u503c\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u5373\u4f7f\u5728\u6700\u4f73\u8868\u73b0\u7684SAS-PReID\u65b9\u6cd5\u4e2d\uff0c\u7a7a\u4e2d\u5230\u5730\u9762\u8bbe\u7f6e\u7684mAP\u4e5f\u4ec5\u4e3a43.93%\u3002", "conclusion": "VReID-XFD\u4e3a\u6781\u7aef\u8fdc\u8ddd\u79bb\u7a7a\u4e2d\u5230\u5730\u9762\u884c\u4eba\u91cd\u8bc6\u522b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u4e25\u5cfb\u6311\u6218\uff0c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u4e00\u72ec\u7279\u64cd\u4f5c\u673a\u5236\u4e0b\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\uff0c\u76f8\u5173\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2601.01206", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.01206", "abs": "https://arxiv.org/abs/2601.01206", "authors": ["Soroush Elyasi", "Arya VarastehNezhad", "Fattaneh Taghiyareh"], "title": "MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches", "comment": null, "summary": "Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.", "AI": {"tldr": "\u4f7f\u7528\u591a\u7c7b\u578b\u4e25\u8083\u6e38\u620f\u548c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u8f6f\u4ef6\u5f00\u53d1\u5c97\u4f4d\u9002\u5408\u5ea6\uff0c\u901a\u8fc7\u6e38\u620f\u884c\u4e3a\u7279\u5f81\u800c\u975e\u4f20\u7edf\u95ee\u5377\u8fbe\u523097%\u7cbe\u5ea6\u548c94%\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u804c\u4e1a\u8bc4\u4f30\u4e2d\u7684\u4eba\u683c\u95ee\u5377\u5b58\u5728\u56de\u5e94\u504f\u5dee\u3001\u75b2\u52b3\u548c\u6545\u610f\u626d\u66f2\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u4e14\u51cf\u5c11\u504f\u89c1\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u5b9e\u8bc1\u7814\u7a76\u786e\u5b9a\u8f6f\u4ef6\u5f00\u53d1\u76f8\u5173\u4eba\u683c\u548c\u884c\u4e3a\u7279\u5f81\uff0c\u8bbe\u8ba1\u5b9a\u5236\u79fb\u52a8\u6e38\u620f\u6536\u96c6\u95ee\u9898\u89e3\u51b3\u3001\u89c4\u5212\u3001\u9002\u5e94\u6027\u7b49\u884c\u4e3a\u6570\u636e\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5efa\u6a21\u7b56\u7565\u4ece\u6e38\u620f\u884c\u4e3a\u7279\u5f81\u9884\u6d4b\u9002\u5408\u5ea6", "result": "\u6a21\u578b\u8fbe\u523097%\u7cbe\u5ea6\u548c94%\u51c6\u786e\u7387\uff1b\u5408\u9002\u5019\u9009\u4eba\u8868\u73b0\u51fa\u72ec\u7279\u6e38\u620f\u6a21\u5f0f\uff1a\u89e3\u8c1c\u6e38\u620f\u83b7\u80dc\u66f4\u591a\u3001\u5b8c\u6210\u66f4\u591a\u652f\u7ebf\u6311\u6218\u3001\u66f4\u9891\u7e41\u5bfc\u822a\u83dc\u5355\u3001\u8f83\u5c11\u6682\u505c/\u91cd\u8bd5/\u653e\u5f03\u884c\u4e3a", "conclusion": "\u6e38\u620f\u8fc7\u7a0b\u4e2d\u6355\u83b7\u7684\u9690\u5f0f\u884c\u4e3a\u75d5\u8ff9\u80fd\u6709\u6548\u9884\u6d4b\u8f6f\u4ef6\u5f00\u53d1\u9002\u5408\u5ea6\uff0c\u652f\u6301\u4e25\u8083\u6e38\u620f\u4f5c\u4e3a\u804c\u4e1a\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u3001\u5438\u5f15\u4eba\u4e14\u504f\u89c1\u8f83\u5c11\u7684\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2601.01322", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01322", "abs": "https://arxiv.org/abs/2601.01322", "authors": ["Hongjie Wang", "Niraj K. Jha"], "title": "LinMU: Multimodal Understanding Made Linear", "comment": "23 pages, 7 figures", "summary": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.", "AI": {"tldr": "LinMU\u662f\u4e00\u79cd\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7M-MATE\u6a21\u5757\u66ff\u6362\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf/\u957f\u89c6\u9891\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\uff0c\u5e76\u4e14\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u957f\u89c6\u9891\u65f6\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51faLinMU\u67b6\u6784\uff0c\u4f7f\u7528M-MATE\u6a21\u5757\uff08\u5305\u542b\u53cc\u5411\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684Flex-MA\u5206\u652f\u548c\u5c40\u90e8\u7a97\u53e3\u6ce8\u610f\u529b\u7684Local-Swin\u5206\u652f\uff09\u66ff\u6362\u6240\u6709\u81ea\u6ce8\u610f\u529b\u5c42\uff0c\u5e76\u901a\u8fc7\u4e09\u9636\u6bb5\u84b8\u998f\u6846\u67b6\u5c06\u9884\u8bad\u7ec3VLM\u8f6c\u6362\u4e3aLinMU\u67b6\u6784\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLinMU\u4e0e\u6559\u5e08\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u5728\u5206\u949f\u7ea7\u89c6\u9891\u4e0a\uff0c\u9996\u8bcd\u751f\u6210\u65f6\u95f4\u51cf\u5c112.7\u500d\uff0ctoken\u541e\u5410\u91cf\u63d0\u9ad89.0\u500d\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u84b8\u998f\u9636\u6bb5\u548cM-MATE\u53cc\u5206\u652f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u65e0\u9700\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e5f\u80fd\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e3a\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u957f\u89c6\u9891\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.01339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01339", "abs": "https://arxiv.org/abs/2601.01339", "authors": ["Weihang You", "Hanqi Jiang", "Yi Pan", "Junhao Chen", "Tianming Liu", "Fei Dou"], "title": "Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning", "comment": null, "summary": "Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.", "AI": {"tldr": "NeuroAlign\u662f\u4e00\u4e2a\u65b0\u9896\u7684fMRI-\u89c6\u9891\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5206\u5c42\u7ec4\u7ec7\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5c06\u795e\u7ecf\u89e3\u7801\u7b80\u5316\u4e3a\u751f\u6210\u4efb\u52a1\u6216\u7b80\u5355\u76f8\u5173\u6027\u5206\u6790\uff0c\u65e0\u6cd5\u53cd\u6620\u5927\u8111\u89c6\u89c9\u5904\u7406\u7684\u5206\u5c42\u548c\u65f6\u95f4\u8fc7\u7a0b\u3002\u7531\u4e8e\u5927\u8111\u8868\u5f81\u7684\u56fa\u6709\u590d\u6742\u6027\u4ee5\u53ca\u795e\u7ecf\u6570\u636e\u4e0e\u89c6\u89c9\u8f93\u5165\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\uff0c\u7406\u89e3\u795e\u7ecf\u5bf9\u89c6\u89c9\u523a\u6fc0\u7684\u53cd\u5e94\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faNeuroAlign\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u673a\u5236\u6a21\u62df\u751f\u7269\u89c6\u89c9\u901a\u8def\uff1a1) \u901a\u8fc7\u795e\u7ecf-\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60(NTCL)\u5b9e\u73b0\u5168\u5c40\u8bed\u4e49\u7406\u89e3\uff0c\u901a\u8fc7\u53cc\u5411\u9884\u6d4b\u5efa\u6a21\u65f6\u95f4\u52a8\u6001\uff1b2) \u901a\u8fc7\u589e\u5f3a\u5411\u91cf\u91cf\u5316\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u5339\u914d\uff1b\u4f7f\u7528DynaSyncMM-EMA\u65b9\u6cd5\u5b9e\u73b0\u52a8\u6001\u591a\u6a21\u6001\u878d\u5408\u548c\u81ea\u9002\u5e94\u52a0\u6743\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNeuroAlign\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u7406\u89e3\u89c6\u89c9\u8ba4\u77e5\u673a\u5236\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "NeuroAlign\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u5206\u5c42\u7ec4\u7ec7\uff0c\u6210\u529f\u89e3\u51b3\u4e86fMRI-\u89c6\u9891\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u4e3a\u7406\u89e3\u89c6\u89c9\u8ba4\u77e5\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2601.01223", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01223", "abs": "https://arxiv.org/abs/2601.01223", "authors": ["Marzieh Amiri Shahbazi", "Ali Baheri", "Nasibeh Azadeh-Fard"], "title": "Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data", "comment": null, "summary": "Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u8d1d\u53f6\u65af-\u4fdd\u5f62\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u5c42\u6b21\u968f\u673a\u68ee\u6797\u4e0e\u7ec4\u611f\u77e5\u4fdd\u5f62\u6821\u51c6\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u5206\u5e03\u81ea\u7531\u7684\u8986\u76d6\u4fdd\u8bc1\u548c\u98ce\u9669\u81ea\u9002\u5e94\u7cbe\u5ea6", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u8981\u6c42\u540c\u65f6\u6ee1\u8db3\u5206\u5e03\u81ea\u7531\u7684\u8986\u76d6\u4fdd\u8bc1\u548c\u98ce\u9669\u81ea\u9002\u5e94\u7cbe\u5ea6\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u8981\u6c42", "method": "\u6df7\u5408\u8d1d\u53f6\u65af-\u4fdd\u5f62\u6846\u67b6\uff1a\u96c6\u6210\u8d1d\u53f6\u65af\u5c42\u6b21\u968f\u673a\u68ee\u6797\u4e0e\u7ec4\u611f\u77e5\u4fdd\u5f62\u6821\u51c6\uff0c\u4f7f\u7528\u540e\u9a8c\u4e0d\u786e\u5b9a\u6027\u5bf9\u4fdd\u5f62\u5206\u6570\u8fdb\u884c\u52a0\u6743\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u8986\u76d6\u6709\u6548\u6027", "result": "\u572861,538\u4f8b\u5165\u9662\u60a3\u8005\uff083,793\u5bb6\u7f8e\u56fd\u533b\u9662\uff0c4\u4e2a\u5730\u533a\uff09\u4e0a\u8bc4\u4f30\uff1a\u8fbe\u5230\u76ee\u6807\u8986\u76d6\u7387\uff0894.3% vs 95%\u76ee\u6807\uff09\uff0c\u4f4e\u4e0d\u786e\u5b9a\u6027\u75c5\u4f8b\u533a\u95f4\u5bbd\u5ea6\u51cf\u5c1121%\uff0c\u9ad8\u98ce\u9669\u9884\u6d4b\u9002\u5f53\u52a0\u5bbd\u3002\u7eaf\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u4e25\u91cd\u4e0d\u8db3\uff0814.1%\u8986\u76d6\uff09", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u98ce\u9669\u5206\u5c42\u4e34\u5e8a\u534f\u8bae\u3001\u9ad8\u6548\u8d44\u6e90\u89c4\u5212\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\uff0c\u4e3a\u4e0d\u786e\u5b9a\u75c5\u4f8b\u63d0\u4f9b\u4fdd\u5b88\u5206\u914d\u548c\u589e\u5f3a\u76d1\u7763\uff0c\u4e3a\u591a\u6837\u5316\u533b\u7597\u73af\u5883\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u51b3\u7b56\u652f\u6301"}}
{"id": "2601.01364", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01364", "abs": "https://arxiv.org/abs/2601.01364", "authors": ["Mostofa Rafid Uddin", "Mahek Vora", "Qifeng Wu", "Muyuan Chen", "Min Xu"], "title": "Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography", "comment": null, "summary": "Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u51b7\u51bb\u7535\u955c\u65ad\u5c42\u626b\u63cf\u6570\u636e\u4e2d\u5206\u79bbSE(3)\u53d8\u6362\u4e0e\u5f62\u6001\u5185\u5bb9\uff0c\u901a\u8fc7\u591a\u9009\u62e9\u5b66\u4e60\u6a21\u5757\u5904\u7406\u9ad8\u566a\u58f0\u6570\u636e\uff0c\u80fd\u591f\u53d1\u73b0\u65b0\u7684\u5206\u5b50\u5f62\u6001\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u671f\u671b\u6700\u5927\u5316\u7684\u65b9\u6cd5\u7ecf\u5e38\u9057\u6f0f\u7f55\u89c1\u4f46\u91cd\u8981\u7684\u5206\u5b50\u5f62\u6001\uff0c\u4e14\u9700\u8981\u5927\u91cf\u624b\u52a8\u8d85\u53c2\u6570\u8c03\u6574\u3002\u51b7\u51bb\u7535\u955c\u65ad\u5c42\u626b\u63cf\u63d0\u4f9b\u4e86\u7ec6\u80de\u5185\u5206\u5b50\u7684\u76f4\u63a53D\u53ef\u89c6\u5316\uff0c\u4f46\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5206\u6790\u5176\u539f\u4f4d\u5f62\u6001\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5206\u79bbSE(3)\u53d8\u6362\u4e0e\u5f62\u6001\u5185\u5bb9\u3002\u5305\u542b\u65b0\u9896\u7684\u591a\u9009\u62e9\u5b66\u4e60\u6a21\u5757\uff0c\u4e13\u95e8\u5904\u7406\u9ad8\u566a\u58f0\u7684\u51b7\u51bb\u7535\u955c\u65ad\u5c42\u626b\u63cf\u6570\u636e\uff0c\u5229\u7528\u5b66\u4e60\u7684\u5f62\u6001\u5185\u5bb9\u751f\u6210\u6a21\u677f\u5f62\u6001\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u51b7\u51bb\u7535\u955c\u65ad\u5c42\u626b\u63cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u660e\u663e\u6539\u8fdb\uff0c\u5305\u62ec\u53d1\u73b0\u4e86\u5148\u524d\u672a\u8bc6\u522b\u7684\u5927\u5206\u5b50\u5f62\u6001\u3002", "conclusion": "\u8be5\u89e3\u8026\u8868\u793a\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u51b7\u51bb\u7535\u955c\u65ad\u5c42\u626b\u63cf\u6570\u636e\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5206\u79bb\u53d8\u6362\u4e0e\u5f62\u6001\u7279\u5f81\uff0c\u4e3a\u53d1\u73b0\u65b0\u7684\u5206\u5b50\u5f62\u6001\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.01290", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01290", "abs": "https://arxiv.org/abs/2601.01290", "authors": ["Harshita Narnoli", "Mihai Surdeanu"], "title": "The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification", "comment": "International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025", "summary": "In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6bd4\u8f83\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u76d1\u7763\u5206\u7c7b\u5668\u7684\u884c\u4e3a\uff0c\u63a2\u7a76LLMs\u5982\u4f55\u901a\u8fc7\u793a\u4f8b\u8fdb\u884c\u5b66\u4e60\uff0c\u53d1\u73b0\u5f53\u793a\u4f8b\u76f8\u5173\u6027\u9ad8\u65f6\uff0cLLMs\u884c\u4e3a\u7c7b\u4f3ckNN\u5206\u7c7b\u5668\uff0c\u76f8\u5173\u6027\u4f4e\u65f6LLMs\u8868\u73b0\u66f4\u597d", "motivation": "\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u5b66\u4e60\u5728\u5b9e\u8df5\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\u4e14\u6709\u6548\uff0c\u4f46\u4eba\u4eec\u5bf9\u5176\u5de5\u4f5c\u539f\u7406\u4ecd\u7f3a\u4e4f\u771f\u6b63\u7684\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76LLMs\u901a\u8fc7\u4e0a\u4e0b\u6587\u793a\u4f8b\u5b66\u4e60\u65f6\uff0c\u5176\u5185\u90e8\u673a\u5236\u4e0e\u54ea\u4e9b\u4f20\u7edf\u5206\u7c7b\u5668\u66f4\u76f8\u4f3c", "method": "\u4f7f\u7528\u6587\u672c\u5206\u7c7b\u4f5c\u4e3a\u7528\u4f8b\uff0c\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u4e09\u4e2aLLMs\u4e0a\uff0c\u6bd4\u8f83\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u57fa\u4e8e\u76f8\u540c\u793a\u4f8b\u8bad\u7ec3\u7684\u76d1\u7763\u5206\u7c7b\u5668\uff08\u68af\u5ea6\u4e0b\u964d\u548ck\u8fd1\u90bb\uff09\u7684\u884c\u4e3a\u5dee\u5f02", "result": "\u5f53\u793a\u4f8b\u76f8\u5173\u6027\u9ad8\u65f6\uff0cLLMs\u884c\u4e3a\u4e0e\u8fd9\u4e9b\u5206\u7c7b\u5668\u76f8\u4f3c\uff0c\u4e14\u66f4\u63a5\u8fd1kNN\u800c\u975e\u903b\u8f91\u56de\u5f52\uff1b\u5f53\u793a\u4f8b\u76f8\u5173\u6027\u4f4e\u65f6\uff0cLLMs\u8868\u73b0\u4f18\u4e8e\u8fd9\u4e9b\u5206\u7c7b\u5668\uff0c\u56e0\u4e3aLLMs\u53ef\u4ee5\u4f9d\u8d56\u5176\u53c2\u6570\u5316\u8bb0\u5fc6", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u5728\u793a\u4f8b\u76f8\u5173\u6027\u9ad8\u65f6\u7c7b\u4f3ckNN\u5206\u7c7b\u5668\uff0c\u5728\u76f8\u5173\u6027\u4f4e\u65f6LLMs\u80fd\u5229\u7528\u53c2\u6570\u5316\u8bb0\u5fc6\u83b7\u5f97\u4f18\u52bf\uff0c\u8fd9\u4e3a\u7406\u89e3LLMs\u5982\u4f55\u901a\u8fc7\u4e0a\u4e0b\u6587\u793a\u4f8b\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bc1\u636e"}}
{"id": "2601.01386", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01386", "abs": "https://arxiv.org/abs/2601.01386", "authors": ["Xiaobao Wei", "Zhangjie Ye", "Yuxiang Gu", "Zunjie Zhu", "Yunfei Guo", "Yingying Shen", "Shan Zhao", "Ming Lu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Rongfeng Lu", "Hangjun Ye"], "title": "ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking", "comment": null, "summary": "Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian", "AI": {"tldr": "\u63d0\u51fa\u4e86ParkGaussian\u6846\u67b6\uff0c\u9996\u6b21\u5c063D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5e94\u7528\u4e8e\u505c\u8f66\u573a\u573a\u666f\u91cd\u5efa\uff0c\u5e76\u521b\u5efa\u4e86ParkRecon3D\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u69fd\u4f4d\u611f\u77e5\u91cd\u5efa\u7b56\u7565\u63d0\u5347\u4e0b\u6e38\u505c\u8f66\u4f4d\u68c0\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u505c\u8f66\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u57282D\u505c\u8f66\u4f4d\u611f\u77e5\u3001\u5efa\u56fe\u548c\u5b9a\u4f4d\uff0c\u800c3D\u91cd\u5efa\u7814\u7a76\u4e0d\u8db3\u3002\u505c\u8f66\u573a\u573a\u666f\u5177\u6709\u590d\u6742\u7684\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\uff0c\u4f46\u5355\u7eaf\u63d0\u9ad8\u91cd\u5efa\u89c6\u89c9\u8d28\u91cf\u5e76\u4e0d\u80fd\u76f4\u63a5\u63d0\u5347\u81ea\u52a8\u505c\u8f66\u6027\u80fd\uff0c\u56e0\u4e3a\u505c\u8f66\u7684\u5173\u952e\u5165\u53e3\u662f\u505c\u8f66\u4f4d\u611f\u77e5\u6a21\u5757\u3002", "method": "1. \u521b\u5efaParkRecon3D\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u56db\u4e2a\u73af\u89c6\u9c7c\u773c\u76f8\u673a\u7684\u4f20\u611f\u5668\u6570\u636e\u3001\u6807\u5b9a\u5916\u53c2\u548c\u5bc6\u96c6\u505c\u8f66\u4f4d\u6807\u6ce8\uff1b2. \u63d0\u51faParkGaussian\u6846\u67b6\uff0c\u9996\u6b21\u5c063D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u7528\u4e8e\u505c\u8f66\u573a\u573a\u666f\u91cd\u5efa\uff1b3. \u5f15\u5165\u69fd\u4f4d\u611f\u77e5\u91cd\u5efa\u7b56\u7565\uff0c\u5229\u7528\u73b0\u6709\u505c\u8f66\u611f\u77e5\u65b9\u6cd5\u589e\u5f3a\u69fd\u4f4d\u533a\u57df\u7684\u5408\u6210\u8d28\u91cf\u3002", "result": "\u5728ParkRecon3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cParkGaussian\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u4e0e\u4e0b\u6e38\u4efb\u52a1\u7684\u611f\u77e5\u4e00\u81f4\u6027\u3002", "conclusion": "ParkGaussian\u662f\u9996\u4e2a\u5c063D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5e94\u7528\u4e8e\u505c\u8f66\u573a\u573a\u666f\u91cd\u5efa\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u69fd\u4f4d\u611f\u77e5\u91cd\u5efa\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u4e0e\u4e0b\u6e38\u505c\u8f66\u4f4d\u68c0\u6d4b\u4efb\u52a1\u7684\u4e00\u81f4\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u505c\u8f66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u597d\u76843D\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2601.01297", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01297", "abs": "https://arxiv.org/abs/2601.01297", "authors": ["Anantha Sharma"], "title": "ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System", "comment": "26 pages", "summary": "Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.\n  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.\n  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.", "AI": {"tldr": "Argus\u6846\u67b6\u5c06\u9ad8\u7ef4\u6570\u636e\u6d41\u4e2d\u7684\u5206\u5e03\u6f02\u79fb\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5728\u6570\u636e\u6d41\u5f62\u56fa\u5b9a\u7a7a\u95f4\u5212\u5206\u4e0a\u8ddf\u8e2a\u5c40\u90e8\u7edf\u8ba1\u91cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u51e0\u4f55\u7ed3\u6784\u4fdd\u6301\u548c\u8eab\u4efd\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u6d41\u4e2d\u7684\u5206\u5e03\u6f02\u79fb\u68c0\u6d4b\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u5168\u5c40\u6bd4\u8f83\u65b9\u6cd5\u6269\u5c55\u6027\u5dee\uff0c\u57fa\u4e8e\u6295\u5f71\u7684\u65b9\u6cd5\u4e22\u5931\u51e0\u4f55\u7ed3\u6784\uff0c\u91cd\u65b0\u805a\u7c7b\u65b9\u6cd5\u5b58\u5728\u8eab\u4efd\u4e0d\u7a33\u5b9a\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u7ef4\u7ed3\u6784\u53c8\u5177\u6709\u8ba1\u7b97\u6548\u7387\u7684\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "Argus\u6846\u67b6\u901a\u8fc7\u56fa\u5b9a\u7a7a\u95f4\u5212\u5206\uff08Voronoi\u7ec6\u5206\uff09\u8ddf\u8e2a\u5c40\u90e8\u7edf\u8ba1\u91cf\u6765\u68c0\u6d4b\u6f02\u79fb\u3002\u4f7f\u7528\u89c4\u8303\u6b63\u4ea4\u57fa\u4e0a\u7684Voronoi\u7ec6\u5206\u786e\u4fdd\u6f02\u79fb\u5ea6\u91cf\u5bf9\u6b63\u4ea4\u53d8\u6362\u4e0d\u53d8\uff0c\u5f15\u5165\u56fe\u8bba\u65b9\u6cd5\u533a\u5206\u76f8\u5e72\u5206\u5e03\u6f02\u79fb\u548c\u5b64\u7acb\u6270\u52a8\uff0c\u5e76\u901a\u8fc7\u4e58\u79ef\u91cf\u5316\u7ec6\u5206\u6269\u5c55\u5230\u8d85\u9ad8\u7ef4\u5ea6\uff08d>500\uff09\u3002", "result": "\u7406\u8bba\u8bc1\u660eVoronoi\u7ec6\u5206\u5728\u89c4\u8303\u6b63\u4ea4\u57fa\u4e0a\u4ea7\u751f\u5bf9\u6b63\u4ea4\u53d8\u6362\u4e0d\u53d8\u7684\u6f02\u79fb\u5ea6\u91cf\uff0c\u6846\u67b6\u5b9e\u73b0O(N)\u590d\u6742\u5ea6\u5e76\u63d0\u4f9b\u5355\u5143\u7ea7\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u80fd\u6b63\u786e\u8bc6\u522b\u5750\u6807\u65cb\u8f6c\u4e0b\u7684\u6f02\u79fb\u800c\u73b0\u6709\u65b9\u6cd5\u4f1a\u4ea7\u751f\u8bef\u62a5\u3002", "conclusion": "Argus\u4e3a\u5206\u5e03\u76d1\u63a7\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u51e0\u4f55\u57fa\u7840\uff0c\u65e2\u80fd\u4fdd\u6301\u9ad8\u7ef4\u7ed3\u6784\u53c8\u907f\u514d\u4e86\u6210\u5bf9\u6bd4\u8f83\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u901a\u8fc7\u56fa\u5b9a\u7a7a\u95f4\u5212\u5206\u548c\u5c40\u90e8\u7edf\u8ba1\u8ddf\u8e2a\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u6f02\u79fb\u68c0\u6d4b\u3002"}}
{"id": "2601.01298", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01298", "abs": "https://arxiv.org/abs/2601.01298", "authors": ["Jorge L. Ruiz Williams"], "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware", "comment": null, "summary": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.", "AI": {"tldr": "Warp Cortex\u662f\u4e00\u4e2a\u5f02\u6b65\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u667a\u80fd\u4f53\u903b\u8f91\u4e0e\u7269\u7406\u5185\u5b58\uff0c\u5b9e\u73b0\u4e86\u767e\u4e07\u7ea7\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u6269\u5c55\uff0c\u5c06\u5185\u5b58\u590d\u6742\u5ea6\u4eceO(N*L)\u964d\u4f4e\u5230O(1)\u6743\u91cd\u548cO(N*k)\u4e0a\u4e0b\u6587\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u5b58\u5728\u7ebf\u6027\u5185\u5b58\u6269\u5c55\u95ee\u9898\uff0c\u4f7f\u5f97\"\u7cfb\u7edf2\"\u5e76\u884c\u63a8\u7406\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u4e0d\u5207\u5b9e\u9645\u3002\u9700\u8981\u89e3\u51b3\u5185\u5b58\u74f6\u9888\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u534f\u540c\u63a8\u7406\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u67b6\u6784\uff0c\u901a\u8fc7\u5355\u4f8b\u6743\u91cd\u5171\u4eab\u548c\u62d3\u6251\u7a81\u89e6\u6280\u672f\uff08\u53d7\u62d3\u6251\u6570\u636e\u5206\u6790\u542f\u53d1\uff09\uff0c\u5c06KV\u7f13\u5b58\u89c6\u4e3a\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u70b9\u4e91\uff0c\u5e94\u7528\u89c1\u8bc1\u590d\u6742\u7a00\u758f\u5316\u6765\u4fdd\u6301\u4e0a\u4e0b\u6587\u6d41\u5f62\u7684\u6301\u4e45\u540c\u8c03\u7279\u5f81\u3002\u5f15\u5165\u53c2\u8003\u6ce8\u5165\u673a\u5236\u5b9e\u73b0\u975e\u4fb5\u5165\u5f0fKV\u7f13\u5b58\u66f4\u65b0\u3002", "result": "\u5728\u5355\u4e2aNVIDIA RTX 4090\u4e0a\u5b9e\u73b0\u4e86100\u4e2a\u5e76\u53d1\u667a\u80fd\u4f53\uff0c\u4ec5\u5360\u75282.2GB\u603bVRAM\uff0c\u7406\u8bba\u5bb9\u91cf\u8d85\u8fc71000\u4e2a\u667a\u80fd\u4f53\uff08\u8ba1\u7b97\u5ef6\u8fdf\u6210\u4e3a\u74f6\u9888\u524d\uff09\u3002\u5185\u5b58\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "Warp Cortex\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u4f18\u5316\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7684\u5185\u5b58\u6269\u5c55\u74f6\u9888\uff0c\u4e3a\u5b9e\u73b0\u5927\u89c4\u6a21\u5e76\u884c\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2601.01306", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.01306", "abs": "https://arxiv.org/abs/2601.01306", "authors": ["John Zhao"], "title": "Towards a Principled Muon under $\u03bc\\mathsf{P}$: Ensuring Spectral Conditions throughout Training", "comment": "21 pages, 0 figures", "summary": "The $\u03bc$-parameterization ($\u03bc$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $\u03bc$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $\u03bc$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $\u03bc$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $\u03bc$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $\u03bc$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.", "AI": {"tldr": "\u03bcP\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u4f46\u77e9\u9635\u4f18\u5316\u5668Muon\u5728\u5b9e\u9645\u8bad\u7ec3\u4e2d\u96be\u4ee5\u6ee1\u8db3\u5176\u8c31\u6761\u4ef6\u8981\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u4fdd\u8bc1\u5168\u7a0b\u6ee1\u8db3\u6761\u4ef6\uff0c\u8981\u4e48\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u672c\u6587\u63d0\u51faMuon++\uff0c\u901a\u8fc7\u4ec5\u63a7\u5236\u4f18\u5316\u5668\u66f4\u65b0\u7684\u8c31\u6761\u4ef6\u6765\u4fdd\u8bc1\u03bcP\u517c\u5bb9\u6027\uff0c\u65e0\u9700\u663e\u5f0f\u6743\u91cd\u8c31\u5f52\u4e00\u5316\u3002", "motivation": "\u03bc-\u53c2\u6570\u5316\uff08\u03bcP\uff09\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u4f46\u5b9e\u9645\u8bad\u7ec3\u4e2d\u77e9\u9635\u4f18\u5316\u5668Muon\u96be\u4ee5\u6ee1\u8db3\u03bcP\u8981\u6c42\u7684\u8c31\u6761\u4ef6\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u65e0\u6cd5\u4fdd\u8bc1\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8c31\u6761\u4ef6\u59cb\u7ec8\u6ee1\u8db3\uff0c\u8981\u4e48\u9700\u8981\u9891\u7e41\u7684\u8c31\u5f52\u4e00\u5316\u64cd\u4f5c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u5b9e\u7528\u6027\u5dee\u3002", "method": "\u63d0\u51faMuon++\u4f18\u5316\u5668\u53d8\u4f53\uff0c\u5173\u952e\u89c1\u89e3\u662f\u5bf9\u4e8e\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\uff0c\u4ec5\u9700\u5728\u4f18\u5316\u5668\u66f4\u65b0\u5c42\u9762\u7ef4\u6301\u8c31\u63a7\u5236\u5373\u53ef\u4fdd\u6301\u03bcP\u517c\u5bb9\u7684\u7f29\u653e\u7279\u6027\uff0c\u65e0\u9700\u663e\u5f0f\u5bf9\u6743\u91cd\u8fdb\u884c\u8c31\u5f52\u4e00\u5316\u3002\u540c\u65f6\u9996\u6b21\u5f15\u5165\u6570\u636e\u4f9d\u8d56\u7684\u81ea\u9002\u5e94\u8c31\u6761\u4ef6\uff0c\u66f4\u9002\u5408\u957f\u65f6\u7a0bLLM\u8bad\u7ec3\u3002", "result": "\u5f00\u53d1\u51faMuon++\uff0c\u80fd\u591f\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u9760\u5730\u4fdd\u8bc1\u03bcP\u6240\u9700\u7684\u8c31\u6761\u4ef6\uff0c\u5f25\u5408\u4e86\u03bcP\u7406\u8bba\u627f\u8bfa\u4e0e\u77e9\u9635\u4f18\u5316\u5668\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u901a\u8fc7\u4ec5\u63a7\u5236\u4f18\u5316\u5668\u66f4\u65b0\u7684\u8c31\u6761\u4ef6\uff0c\u53ef\u4ee5\u5728\u4e0d\u663e\u5f0f\u5f52\u4e00\u5316\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u4fdd\u8bc1\u03bcP\u517c\u5bb9\u6027\uff0c\u4e3a\u77e9\u9635\u4f18\u5316\u5668\u5728\u957f\u65f6\u7a0bLLM\u8bad\u7ec3\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u8c31\u6761\u4ef6\u4ee5\u9002\u5e94\u6570\u636e\u4f9d\u8d56\u6548\u5e94\u3002"}}
{"id": "2601.01416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01416", "abs": "https://arxiv.org/abs/2601.01416", "authors": ["Yue Zhou", "Ran Ding", "Xue Yang", "Xue Jiang", "Xingzhao Liu"], "title": "AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval", "comment": "12 pages, 9 figures", "summary": "Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86AirSpatial\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5f00\u53d1\u4e86\u80fd\u591f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8f66\u8f86\u5c5e\u6027\u8bc6\u522b\u548c\u68c0\u7d22\u7684\u7a7a\u4e2d\u667a\u80fd\u4f53AirSpatialBot\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002\u7279\u522b\u662f\u9488\u5bf9\u65e0\u4eba\u673a\u62cd\u6444\u7684\u8f66\u8f86\u56fe\u50cf\uff0c\u9700\u8981\u66f4\u597d\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u6765\u652f\u6301\u66f4\u590d\u6742\u7684\u4efb\u52a1\u3002", "method": "1) \u521b\u5efa\u5305\u542b\u8d85\u8fc720.6\u4e07\u6761\u6307\u4ee4\u7684AirSpatial\u6570\u636e\u96c6\uff0c\u5f15\u5165\u7a7a\u95f4\u5b9a\u4f4d\u548c\u7a7a\u95f4\u95ee\u7b54\u4e24\u4e2a\u65b0\u4efb\u52a1\uff0c\u9996\u6b21\u63d0\u4f9b3D\u8fb9\u754c\u6846\u6807\u6ce8\uff1b2) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u56fe\u50cf\u7406\u89e3\u9884\u8bad\u7ec3\u548c\u7a7a\u95f4\u7406\u89e3\u5fae\u8c03\uff1b3) \u5f00\u53d1AirSpatialBot\u667a\u80fd\u4f53\uff0c\u52a8\u6001\u6574\u5408\u4efb\u52a1\u89c4\u5212\u3001\u56fe\u50cf\u7406\u89e3\u3001\u7a7a\u95f4\u7406\u89e3\u548c\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u63ed\u793a\u4e86\u73b0\u6709VLMs\u7684\u7a7a\u95f4\u5c40\u9650\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002AirSpatialBot\u80fd\u591f\u9002\u5e94\u591a\u6837\u5316\u7684\u67e5\u8be2\u9700\u6c42\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8f66\u8f86\u5c5e\u6027\u8bc6\u522b\u548c\u68c0\u7d22\u3002", "conclusion": "\u901a\u8fc7AirSpatial\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u63d0\u5347\u4e86\u9065\u611f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u7a7a\u4e2d\u667a\u80fd\u4f53\uff0c\u4e3a\u9065\u611f\u9886\u57df\u7684\u7a7a\u95f4\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01313", "abs": "https://arxiv.org/abs/2601.01313", "authors": ["Vladimer Khasia"], "title": "Spectral-Window Hybrid (SWH)", "comment": null, "summary": "Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \\textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \\textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\\mathcal{O}(T \\log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH", "AI": {"tldr": "SWH\u662f\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u5168\u5c40\u5206\u652f\uff08\u5229\u7528\u5377\u79ef\u5b9a\u7406\u5efa\u6a21\u957f\u7a0b\u8870\u51cf\u52a8\u6001\uff09\u548c\u5c40\u90e8\u5206\u652f\uff08\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff09\u6765\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u8fbe\u80fd\u529b\uff0c\u5b9e\u73b0\u7ebf\u6027\u6269\u5c55\u81f3\u957f\u5e8f\u5217\u3002", "motivation": "\u5c06\u5e8f\u5217\u5efa\u6a21\u6269\u5c55\u5230\u6781\u957f\u4e0a\u4e0b\u6587\u65f6\uff0c\u9700\u8981\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002Transformer\u867d\u7136\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u7cbe\u786e\u68c0\u7d22\uff0c\u4f46\u5176\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSpectral-Window Hybrid (SWH)\u67b6\u6784\uff0c\u5c06\u5e8f\u5217\u5efa\u6a21\u89e3\u8026\u4e3a\u4e24\u4e2a\u5e76\u884c\u6d41\uff1a1) \u5168\u5c40\u5206\u652f\u5229\u7528\u5377\u79ef\u5b9a\u7406\u5728O(T log T)\u65f6\u95f4\u5185\u5efa\u6a21\u957f\u7a0b\u8870\u51cf\u52a8\u6001\uff1b2) \u5c40\u90e8\u5206\u652f\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u5904\u7406\u6709\u9650\u4e0a\u4e0b\u6587\u5185\u7684token\u4ea4\u4e92\u3002\u901a\u8fc7\u805a\u5408\u8fd9\u4e24\u79cd\u8868\u793a\uff0c\u907f\u514d\u5168\u5c40\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u74f6\u9888\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u7cbe\u5ea6\u3002", "result": "SWH\u5728\u77ed\u4e0a\u4e0b\u6587\u4e0a\u80fd\u8fbe\u5230\u6807\u51c6Transformer\u7684\u56f0\u60d1\u5ea6\u6c34\u5e73\uff0c\u540c\u65f6\u80fd\u591f\u9ad8\u6548\u5730\u7ebf\u6027\u6269\u5c55\u5230\u957f\u5e8f\u5217\u3002", "conclusion": "SWH\u67b6\u6784\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u9891\u8c31\u5efa\u6a21\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u8fbe\u80fd\u529b\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u6781\u7aef\u4e0a\u4e0b\u6587\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01425", "abs": "https://arxiv.org/abs/2601.01425", "authors": ["Xu Guo", "Fulong Ye", "Xinghui Li", "Pengqi Tu", "Pengze Zhang", "Qichao Sun", "Songtao Zhao", "Xiangwang Hou", "Qian He"], "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "comment": "Project: https://guoxu1233.github.io/DreamID-V/", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "AI": {"tldr": "DreamID-V\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u7ba1\u9053\u3001\u6a21\u6001\u611f\u77e5\u6761\u4ef6\u6ce8\u5165\u548c\u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\uff0c\u5728\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u3001\u5c5e\u6027\u4fdd\u7559\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u9700\u8981\u5c06\u56fe\u50cf\u4eba\u8138\u4ea4\u6362\u7684\u4f18\u52bf\u65e0\u7f1d\u8fc1\u79fb\u5230\u89c6\u9891\u9886\u57df\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "1) \u63d0\u51faSyncID-Pipe\u6570\u636e\u7ba1\u9053\uff0c\u9884\u8bad\u7ec3\u8eab\u4efd\u951a\u5b9a\u89c6\u9891\u5408\u6210\u5668\u5e76\u4e0eIFS\u6a21\u578b\u7ed3\u5408\u6784\u5efa\u53cc\u5411ID\u56db\u5143\u7ec4\u8fdb\u884c\u663e\u5f0f\u76d1\u7763\uff1b2) \u9996\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u6846\u67b6DreamID-V\uff0c\u91c7\u7528\u6a21\u6001\u611f\u77e5\u6761\u4ef6\u6a21\u5757\u533a\u5206\u6027\u5730\u6ce8\u5165\u591a\u6a21\u6001\u6761\u4ef6\uff1b3) \u5408\u6210\u5230\u771f\u5b9e\u7684\u8bfe\u7a0b\u673a\u5236\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff1b4) \u5f15\u5165IDBench-V\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eDreamID-V\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u901a\u7528\u6027\uff0c\u53ef\u4ee5\u65e0\u7f1d\u9002\u5e94\u5404\u79cd\u4ea4\u6362\u76f8\u5173\u4efb\u52a1\u3002", "conclusion": "DreamID-V\u901a\u8fc7\u521b\u65b0\u7684\u6846\u67b6\u8bbe\u8ba1\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u4e2d\u8eab\u4efd\u76f8\u4f3c\u6027\u3001\u5c5e\u6027\u4fdd\u7559\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u4eba\u8138\u4ea4\u6362\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01347", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01347", "abs": "https://arxiv.org/abs/2601.01347", "authors": ["Yuyan Pi", "Min Jin", "Wentao Xie", "Xinhua Liu"], "title": "From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion", "comment": "34 pages,5 figures", "summary": "Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.", "AI": {"tldr": "GM-MLG\uff1a\u57fa\u4e8e\u56fe-\u57fa\u5e8f\u7279\u5f81\u878d\u5408\u548c\u591a\u6807\u7b7e\u751f\u6210\u7684\u5f00\u653e\u5f0f\u836f\u7269\u4e0d\u826f\u53cd\u5e94\u9884\u6d4b\u65b0\u8303\u5f0f\uff0c\u5c06\u4f20\u7edf\u591a\u6807\u7b7e\u5206\u7c7b\u8f6c\u5316\u4e3aTransformer\u89e3\u7801\u5668\u9a71\u52a8\u7684\u591a\u6807\u7b7e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u5e76\u6269\u5c55\u9884\u6d4b\u7a7a\u95f4", "motivation": "\u5f53\u524d\u836f\u7269\u4e0d\u826f\u53cd\u5e94\u9884\u6d4b\u65b9\u6cd5\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u836f\u7269\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u51b7\u542f\u52a8\u95ee\u9898\u3001\u5c01\u95ed\u6807\u7b7e\u96c6\u9650\u5236\u3001\u4ee5\u53ca\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u4e0d\u8db3\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u8ba1\u7b97\u751f\u7269\u5b66\u5728\u964d\u4f4e\u65b0\u836f\u7814\u53d1\u6210\u672c\u65b9\u9762\u7684\u6f5c\u529b\u53d1\u6325\u3002", "method": "1. \u6784\u5efa\u53cc\u56fe\u8868\u793a\u67b6\u6784\uff1a\u539f\u5b50\u7ea7\u3001\u5c40\u90e8\u5206\u5b50\u7ea7\uff08BRICS\u7b97\u6cd5\u52a8\u6001\u63d0\u53d6\u7ec6\u7c92\u5ea6\u57fa\u5e8f\uff09\u3001\u5168\u5c40\u5206\u5b50\u7ea7\n2. \u5c06ADR\u9884\u6d4b\u4ece\u591a\u6807\u7b7e\u5206\u7c7b\u8f6c\u5316\u4e3a\u57fa\u4e8eTransformer\u89e3\u7801\u5668\u7684\u591a\u6807\u7b7e\u751f\u6210\n3. \u5c06ADR\u6807\u7b7e\u89c6\u4e3a\u79bb\u6563\u6807\u8bb0\u5e8f\u5217\uff0c\u4f7f\u7528\u4f4d\u7f6e\u5d4c\u5165\u663e\u5f0f\u6355\u83b7\u5927\u89c4\u6a21\u6807\u7b7e\u7a7a\u95f4\u4e2d\u7684\u4f9d\u8d56\u548c\u5171\u73b0\u5173\u7cfb\n4. \u901a\u8fc7\u81ea\u56de\u5f52\u89e3\u7801\u751f\u6210\u9884\u6d4b\uff0c\u52a8\u6001\u6269\u5c55\u9884\u6d4b\u7a7a\u95f4", "result": "1. \u6027\u80fd\u63d0\u5347\uff1a\u6700\u9ad8\u8fbe38%\u6539\u8fdb\uff0c\u5e73\u5747\u589e\u76ca20%\n2. \u9884\u6d4b\u7a7a\u95f4\u6269\u5c55\uff1a\u4ece200\u79cd\u6269\u5c55\u5230\u8d85\u8fc710,000\u79cd\n3. \u901a\u8fc7\u9006\u5411\u5408\u6210\u57fa\u5e8f\u5206\u6790\u9610\u660eADR\u4e0e\u57fa\u5e8f\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u7ed3\u6784-\u6d3b\u6027\u5173\u7cfb\n4. \u4e3a\u7cfb\u7edf\u6027\u964d\u4f4e\u836f\u7269\u5b89\u5168\u98ce\u9669\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u521b\u65b0\u652f\u6301", "conclusion": "GM-MLG\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5f00\u653e\u5f0fADR\u9884\u6d4b\u8303\u5f0f\uff0c\u901a\u8fc7\u56fe-\u57fa\u5e8f\u7279\u5f81\u878d\u5408\u548c\u591a\u6807\u7b7e\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u5e76\u5927\u5e45\u6269\u5c55\u4e86\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u836f\u7269\u5b89\u5168\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u53ef\u89e3\u91ca\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2601.01431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01431", "abs": "https://arxiv.org/abs/2601.01431", "authors": ["Weiqi Yu", "Yiyang Yao", "Lin He", "Jianming Lv"], "title": "EdgeNeRF: Edge-Guided Regularization for Neural Radiance Fields from Sparse Views", "comment": "PRCV 2025", "summary": "Neural Radiance Fields (NeRF) achieve remarkable performance in dense multi-view scenarios, but their reconstruction quality degrades significantly under sparse inputs due to geometric artifacts. Existing methods utilize global depth regularization to mitigate artifacts, leading to the loss of geometric boundary details. To address this problem, we propose EdgeNeRF, an edge-guided sparse-view 3D reconstruction algorithm. Our method leverages the prior that abrupt changes in depth and normals generate edges. Specifically, we first extract edges from input images, then apply depth and normal regularization constraints to non-edge regions, enhancing geometric consistency while preserving high-frequency details at boundaries. Experiments on LLFF and DTU datasets demonstrate EdgeNeRF's superior performance, particularly in retaining sharp geometric boundaries and suppressing artifacts. Additionally, the proposed edge-guided depth regularization module can be seamlessly integrated into other methods in a plug-and-play manner, significantly improving their performance without substantially increasing training time. Code is available at https://github.com/skyhigh404/edgenerf.", "AI": {"tldr": "EdgeNeRF\uff1a\u4e00\u79cd\u8fb9\u7f18\u5f15\u5bfc\u7684\u7a00\u758f\u89c6\u56fe3D\u91cd\u5efa\u7b97\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u8f93\u5165\u56fe\u50cf\u8fb9\u7f18\u5e76\u5728\u975e\u8fb9\u7f18\u533a\u57df\u65bd\u52a0\u6df1\u5ea6\u548c\u6cd5\u7ebf\u6b63\u5219\u5316\u7ea6\u675f\uff0c\u5728\u4fdd\u6301\u51e0\u4f55\u8fb9\u754c\u9ad8\u9891\u7ec6\u8282\u7684\u540c\u65f6\u51cf\u5c11\u4f2a\u5f71\u3002", "motivation": "\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u5728\u5bc6\u96c6\u591a\u89c6\u56fe\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7a00\u758f\u8f93\u5165\u4e0b\u91cd\u5efa\u8d28\u91cf\u663e\u8457\u4e0b\u964d\uff0c\u51fa\u73b0\u51e0\u4f55\u4f2a\u5f71\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5168\u5c40\u6df1\u5ea6\u6b63\u5219\u5316\u6765\u51cf\u8f7b\u4f2a\u5f71\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u51e0\u4f55\u8fb9\u754c\u7ec6\u8282\u7684\u4e22\u5931\u3002", "method": "\u63d0\u51faEdgeNeRF\u7b97\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u548c\u6cd5\u7ebf\u7a81\u53d8\u4ea7\u751f\u8fb9\u7f18\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u9996\u5148\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u63d0\u53d6\u8fb9\u7f18\uff0c\u7136\u540e\u5728\u975e\u8fb9\u7f18\u533a\u57df\u5e94\u7528\u6df1\u5ea6\u548c\u6cd5\u7ebf\u6b63\u5219\u5316\u7ea6\u675f\uff0c\u589e\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\u540c\u65f6\u4fdd\u6301\u8fb9\u754c\u7684\u9ad8\u9891\u7ec6\u8282\u3002", "result": "\u5728LLFF\u548cDTU\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEdgeNeRF\u5728\u4fdd\u6301\u9510\u5229\u51e0\u4f55\u8fb9\u754c\u548c\u6291\u5236\u4f2a\u5f71\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u6240\u63d0\u51fa\u7684\u8fb9\u7f18\u5f15\u5bfc\u6df1\u5ea6\u6b63\u5219\u5316\u6a21\u5757\u53ef\u4ee5\u5373\u63d2\u5373\u7528\u5730\u96c6\u6210\u5230\u5176\u4ed6\u65b9\u6cd5\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u800c\u4e0d\u663e\u8457\u589e\u52a0\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "EdgeNeRF\u901a\u8fc7\u8fb9\u7f18\u5f15\u5bfc\u7684\u7a00\u758f\u89c6\u56fe3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86NeRF\u5728\u7a00\u758f\u8f93\u5165\u4e0b\u7684\u51e0\u4f55\u4f2a\u5f71\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51e0\u4f55\u8fb9\u754c\u7ec6\u8282\u7684\u540c\u65f6\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.01439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01439", "abs": "https://arxiv.org/abs/2601.01439", "authors": ["Wenqi Ren", "Weijie Wang", "Meng Zheng", "Ziyan Wu", "Yang Tang", "Zhun Zhong", "Nicu Sebe"], "title": "In defense of the two-stage framework for open-set domain adaptive semantic segmentation", "comment": null, "summary": "Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) presents a significant challenge, as it requires both domain adaptation for known classes and the distinction of unknowns. Existing methods attempt to address both tasks within a single unified stage. We question this design, as the annotation imbalance between known and unknown classes often leads to negative transfer of known classes and underfitting for unknowns. To overcome these issues, we propose SATS, a Separating-then-Adapting Training Strategy, which addresses OSDA-SS through two sequential steps: known/unknown separation and unknown-aware domain adaptation. By providing the model with more accurate and well-aligned unknown classes, our method ensures a balanced learning of discriminative features for both known and unknown classes, steering the model toward discovering truly unknown objects. Additionally, we present hard unknown exploration, an innovative data augmentation method that exposes the model to more challenging unknowns, strengthening its ability to capture more comprehensive understanding of target unknowns. We evaluate our method on public OSDA-SS benchmarks. Experimental results demonstrate that our method achieves a substantial advancement, with a +3.85% H-Score improvement for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSATS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb-\u9002\u5e94\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u5f00\u653e\u96c6\u57df\u9002\u5e94\u8bed\u4e49\u5206\u5272\u95ee\u9898\uff0c\u907f\u514d\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\u4e4b\u95f4\u7684\u8d1f\u8fc1\u79fb\u548c\u6b20\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u96c6\u57df\u9002\u5e94\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5355\u9636\u6bb5\u7edf\u4e00\u5904\u7406\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\uff0c\u4f46\u7531\u4e8e\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\u4e4b\u95f4\u7684\u6807\u6ce8\u4e0d\u5e73\u8861\uff0c\u5f80\u5f80\u5bfc\u81f4\u5df2\u77e5\u7c7b\u7684\u8d1f\u8fc1\u79fb\u548c\u672a\u77e5\u7c7b\u7684\u6b20\u62df\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51faSATS\uff08Separating-then-Adapting Training Strategy\uff09\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1\uff09\u5df2\u77e5/\u672a\u77e5\u5206\u79bb\u9636\u6bb5\uff1b2\uff09\u672a\u77e5\u611f\u77e5\u7684\u57df\u9002\u5e94\u9636\u6bb5\u3002\u540c\u65f6\u63d0\u51fa\u786c\u672a\u77e5\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u8ba9\u6a21\u578b\u63a5\u89e6\u66f4\u5177\u6311\u6218\u6027\u7684\u672a\u77e5\u6837\u672c\u3002", "result": "\u5728\u516c\u5f00\u7684OSDA-SS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1aGTA5-to-Cityscapes\u4e0aH-Score\u63d0\u5347+3.85%\uff0cSYNTHIA-to-Cityscapes\u4e0a\u63d0\u5347+18.64%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5206\u79bb\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\u5e76\u5206\u522b\u8fdb\u884c\u57df\u9002\u5e94\uff0cSATS\u65b9\u6cd5\u80fd\u591f\u5e73\u8861\u5b66\u4e60\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\u7684\u5224\u522b\u7279\u5f81\uff0c\u6709\u6548\u53d1\u73b0\u771f\u6b63\u7684\u672a\u77e5\u5bf9\u8c61\uff0c\u663e\u8457\u63d0\u5347\u5f00\u653e\u96c6\u57df\u9002\u5e94\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2601.01368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01368", "abs": "https://arxiv.org/abs/2601.01368", "authors": ["Mujin Zhou", "Junzhe Zhang"], "title": "Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach", "comment": null, "summary": "Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ef-GAN\u6846\u67b6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u5904\u7406\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u6570\u636e\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u81ea\u7531\u80fd\u5b66\u4e60\u56e0\u679c\u7ed3\u6784\uff0c\u4f7f\u7528Gumbel-Softmax\u677e\u5f1b\u5728\u79bb\u6563\u56fe\u7a7a\u95f4\u8fdb\u884c\u68af\u5ea6\u641c\u7d22\u3002", "motivation": "\u4ece\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u6570\u636e\u4e2d\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u72ec\u7acb\u4e8e\u7279\u5b9a\u6743\u91cd\u503c\u5b66\u4e60\u4e8c\u5143\u56e0\u679c\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u7ed3\u6784\u5b66\u4e60\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u6700\u5c0f\u5316\u8d1d\u53f6\u65af\u81ea\u7531\u80fd\uff0c\u8bc1\u660e\u8be5\u95ee\u9898\u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316\u771f\u5b9e\u6570\u636e\u5206\u5e03\u4e0e\u6a21\u578b\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684f-\u6563\u5ea6\u3002\u5229\u7528f-GAN\u6846\u67b6\u5c06\u6b64\u76ee\u6807\u8f6c\u5316\u4e3a\u6700\u5c0f-\u6700\u5927\u5bf9\u6297\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4f7f\u7528Gumbel-Softmax\u677e\u5f1b\u5728\u79bb\u6563\u56fe\u7a7a\u95f4\u5b9e\u73b0\u68af\u5ea6\u641c\u7d22\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u56e0\u679c\u7ed3\u6784\uff0c\u901a\u8fc7\u5bf9\u6297\u4f18\u5316\u6846\u67b6\u6709\u6548\u5904\u7406\u79bb\u6563\u56fe\u7a7a\u95f4\u7684\u641c\u7d22\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8ef-GAN\u6846\u67b6\u7684\u65b9\u6cd5\u4e3a\u5b58\u5728\u672a\u6d4b\u91cf\u6df7\u6742\u56e0\u7d20\u7684\u56e0\u679c\u53d1\u73b0\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u548c\u5bf9\u6297\u4f18\u5316\u5b9e\u73b0\u4e86\u56e0\u679c\u7ed3\u6784\u7684\u6709\u6548\u5b66\u4e60\u3002"}}
{"id": "2601.01383", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01383", "abs": "https://arxiv.org/abs/2601.01383", "authors": ["Yen-Chia Chen", "Hsing-Kuo Pao", "Hanjuan Huang"], "title": "Data Complexity-aware Deep Model Performance Forecasting", "comment": "12 pages, 12 figures", "summary": "Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u53ef\u5728\u8bad\u7ec3\u524d\u6839\u636e\u6570\u636e\u96c6\u7279\u6027\u548c\u6a21\u578b\u7ed3\u6784\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u907f\u514d\u91cd\u590d\u8bd5\u9519", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9009\u62e9\u901a\u5e38\u4f9d\u8d56\u91cd\u590d\u8bd5\u9519\u8fc7\u7a0b\uff0c\u8017\u65f6\u8017\u8d44\u6e90\u4e14\u96be\u4ee5\u81ea\u52a8\u5316\u3002\u73b0\u6709\u6027\u80fd\u9884\u6d4b\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u5f00\u9500\uff0c\u8981\u4e48\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u6570\u636e\u96c6\u7684\u53ef\u6d4b\u91cf\u5c5e\u6027\u9884\u6d4b\u57fa\u7ebf\u6027\u80fd\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7ed3\u5408\u6a21\u578b\u67b6\u6784\u548c\u8d85\u53c2\u6570\u7ec6\u8282\u8c03\u6574\u4f30\u8ba1\u3002\u6846\u67b6\u53ef\u6cdb\u5316\u5230\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u7c7b\u578b\u3002", "result": "\u6846\u67b6\u4e0d\u4ec5\u80fd\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u80fd\u6307\u5bfc\u67b6\u6784\u9009\u62e9\u3001\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5e76\u5728\u8bad\u7ec3\u524d\u68c0\u6d4b\u6f5c\u5728\u95ee\u9898\u6570\u636e\u96c6\u3002\u53d1\u73b0\u6570\u636e\u96c6\u65b9\u5dee\u7b49\u7279\u5f81\u53ef\u4f5c\u4e3a\u6570\u636e\u8d28\u91cf\u7684\u65e9\u671f\u6307\u6807\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6a21\u578b\u6027\u80fd\u9884\u6d4b\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u4f20\u7edf\u8bd5\u9519\u8fc7\u7a0b\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u6570\u636e\u5904\u7406\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2601.01456", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01456", "abs": "https://arxiv.org/abs/2601.01456", "authors": ["Wentao Bian", "Fenglei Xu"], "title": "Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration", "comment": "10 pages, 4 figures, 3 tables", "summary": "In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in \"Fuse-then-Refine\" paradigms: the \"Plasticity-Stability Dilemma.\" In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.", "AI": {"tldr": "DA-FSS\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u4e13\u5bb6\u4ef2\u88c1\u7684\u5c11\u6837\u672c3D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\"\u5148\u878d\u5408\u540e\u7ec6\u5316\"\u8303\u5f0f\u4e2d\u7684\"\u53ef\u5851\u6027-\u7a33\u5b9a\u6027\u56f0\u5883\"\u548cCLIP\u7684\u7c7b\u95f4\u6df7\u6dc6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\u5b58\u5728\"\u53ef\u5851\u6027-\u7a33\u5b9a\u6027\u56f0\u5883\"\uff1a\u51e0\u4f55\u8def\u5f84\u9700\u8981\u53ef\u5851\u6027\u4ee5\u9002\u5e94\u65b0\u7c7b\u522b\uff0c\u800c\u8bed\u4e49\u8def\u5f84\u9700\u8981\u7a33\u5b9a\u6027\u4ee5\u907f\u514d\u8fc7\u62df\u5408\u3002\u540c\u65f6\uff0cCLIP\u7684\u7c7b\u95f4\u6df7\u6dc6\u4f1a\u5bfc\u81f4\u8bed\u4e49\u76f2\u533a\u3002", "method": "\u63d0\u51faDA-FSS\u6a21\u578b\uff0c\u5305\u542b\uff1a1) \u5e76\u884c\u4e13\u5bb6\u7ec6\u5316\u6a21\u5757\u751f\u6210\u5404\u6a21\u6001\u76f8\u5173\u6027\uff1b2) \u5806\u53e0\u4ef2\u88c1\u6a21\u5757\u8fdb\u884c\u5377\u79ef\u878d\u5408\u548c\u6a21\u6001\u8def\u5f84\u4ef2\u88c1\uff1b3) \u89e3\u8026\u4e13\u5bb6\u8def\u5f84\uff1a\u51e0\u4f55\u4e13\u5bb6\u4fdd\u6301\u53ef\u5851\u6027\uff0c\u8bed\u4e49\u4e13\u5bb6\u786e\u4fdd\u7a33\u5b9a\u6027\uff1b4) \u89e3\u8026\u5bf9\u9f50\u6a21\u5757\u5728\u4e0d\u4f20\u64ad\u6df7\u6dc6\u7684\u60c5\u51b5\u4e0b\u4f20\u9012\u77e5\u8bc6\u3002", "result": "\u5728S3DIS\u548cScanNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDA-FSS\u4f18\u4e8eMM-FSS\u57fa\u51c6\u65b9\u6cd5\uff0c\u5728\u51e0\u4f55\u8fb9\u754c\u3001\u5b8c\u6574\u6027\u548c\u7eb9\u7406\u533a\u5206\u65b9\u9762\u90fd\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u548c\u8bed\u4e49\u8def\u5f84\u5e76\u76f8\u4e92\u6b63\u5219\u5316\u68af\u5ea6\uff0cDA-FSS\u80fd\u591f\u66f4\u597d\u5730\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u7a7a\u95f4\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5c11\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2601.01387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01387", "abs": "https://arxiv.org/abs/2601.01387", "authors": ["Yongzhe Li", "Lin Guan", "Zihan Cai", "Zuxian Lin", "Jiyu Huang", "Liukai Chen"], "title": "Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning", "comment": null, "summary": "Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c3a\u5ea6\u81ea\u9002\u5e94\u591a\u4efb\u52a1\u6f6e\u6d41\u5206\u6790\u6846\u67b6\uff08SaMPFA\uff09\uff0c\u901a\u8fc7\u5c40\u90e8\u62d3\u6251\u5207\u7247\u91c7\u6837\u6280\u672f\u548c\u65e0\u53c2\u8003\u591a\u4efb\u52a1\u56fe\u5b66\u4e60\u6a21\u578b\uff0c\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u53d8\u62d3\u6251\u7ed3\u6784\u4e0b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f00\u53d1\u5177\u6709\u5f3a\u62d3\u6251\u9002\u5e94\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u6f6e\u6d41\u5206\u6790\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u610f\u4e49\u3002\u73b0\u6709\u6a21\u578b\u5728\u53d8\u7cfb\u7edf\u89c4\u6a21\u548c\u5206\u652f\u529f\u7387\u9884\u6d4b\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u589e\u5f3a\u6a21\u578b\u7684\u8de8\u5c3a\u5ea6\u5b66\u4e60\u80fd\u529b\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "1. \u63d0\u51fa\u5c3a\u5ea6\u81ea\u9002\u5e94\u591a\u4efb\u52a1\u6f6e\u6d41\u5206\u6790\u6846\u67b6\uff08SaMPFA\uff09\uff1b2. \u5f15\u5165\u5c40\u90e8\u62d3\u6251\u5207\u7247\uff08LTS\uff09\u91c7\u6837\u6280\u672f\uff0c\u4ece\u5b8c\u6574\u7535\u7f51\u63d0\u53d6\u4e0d\u540c\u5c3a\u5ea6\u7684\u5b50\u56fe\uff1b3. \u8bbe\u8ba1\u65e0\u53c2\u8003\u591a\u4efb\u52a1\u56fe\u5b66\u4e60\uff08RMGL\uff09\u6a21\u578b\uff0c\u9884\u6d4b\u6bcd\u7ebf\u7535\u538b\u548c\u5206\u652f\u529f\u7387\u800c\u975e\u76f8\u89d2\uff1b4. \u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165\u9f13\u52b1\u6a21\u578b\u6355\u6349\u76f8\u89d2\u5dee\u548c\u529f\u7387\u4f20\u8f93\u7269\u7406\u6a21\u5f0f\u7684\u989d\u5916\u9879\u3002", "result": "\u5728IEEE 39\u8282\u70b9\u7cfb\u7edf\u548c\u5b9e\u9645\u7701\u7ea7\u7535\u7f51\u4e0a\u7684\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u6a21\u578b\u5728\u53d8\u7cfb\u7edf\u5c3a\u5ea6\u4e0b\u5177\u6709\u4f18\u5f02\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u4e864.47%\u548c36.82%\u3002", "conclusion": "SaMPFA\u6846\u67b6\u901a\u8fc7\u5c40\u90e8\u62d3\u6251\u5207\u7247\u91c7\u6837\u548c\u65e0\u53c2\u8003\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u53d8\u62d3\u6251\u7ed3\u6784\u4e0b\u7684\u6f6e\u6d41\u5206\u6790\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u7269\u7406\u4e00\u81f4\u6027\u548c\u8de8\u5c3a\u5ea6\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2601.01403", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01403", "abs": "https://arxiv.org/abs/2601.01403", "authors": ["Zewei Yu", "Jianqiu Xu", "Caimin Li"], "title": "A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble", "comment": "8 pages", "summary": "With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.", "AI": {"tldr": "GDME\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u65e0\u76d1\u7763\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u6c60\u548c\u56fe\u7ed3\u6784\u8fdb\u884c\u6a21\u578b\u96c6\u6210\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784\u6d41\u6570\u636e\u5e76\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\u3002", "motivation": "\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u6d41\u6570\u636e\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u79bb\u7ebf\u8bbe\u8ba1\u6216\u96be\u4ee5\u6709\u6548\u5904\u7406\u5f02\u6784\u6d41\u6570\u636e\uff0c\u4e14\u6570\u636e\u6a21\u5f0f\u591a\u6837\u4e14\u5feb\u901f\u6f14\u53d8\uff0c\u8fd9\u7ed9\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faGDME\u6846\u67b6\uff1a1\uff09\u7ef4\u62a4\u52a8\u6001\u6a21\u578b\u6c60\uff0c\u6301\u7eed\u66f4\u65b0\uff08\u4fee\u526a\u8868\u73b0\u4e0d\u4f73\u7684\u6a21\u578b\u5e76\u5f15\u5165\u65b0\u6a21\u578b\uff09\uff1b2\uff09\u4f7f\u7528\u52a8\u6001\u56fe\u7ed3\u6784\u8868\u793a\u6a21\u578b\u95f4\u5173\u7cfb\uff1b3\uff09\u5728\u56fe\u4e0a\u8fdb\u884c\u793e\u533a\u68c0\u6d4b\u4ee5\u9009\u62e9\u9002\u5f53\u7684\u5b50\u96c6\u8fdb\u884c\u96c6\u6210\uff1b4\uff09\u901a\u8fc7\u76d1\u63a7\u56fe\u7ed3\u6784\u53d8\u5316\u6765\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\uff0c\u4f7f\u6846\u67b6\u80fd\u591f\u9002\u5e94\u6f14\u5316\u7684\u6d41\u6570\u636e\u3002", "result": "\u5728\u4e03\u4e2a\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGDME\u4f18\u4e8e\u73b0\u6709\u7684\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6539\u8fdb\u5e45\u5ea6\u9ad8\u8fbe24%\u3002\u5176\u96c6\u6210\u7b56\u7565\u76f8\u6bd4\u5355\u4e2a\u6a21\u578b\u548c\u5e73\u5747\u96c6\u6210\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u5177\u6709\u7ade\u4e89\u529b\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "GDME\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u96c6\u6210\u548c\u56fe\u7ed3\u6784\u5206\u6790\u5904\u7406\u5f02\u6784\u6d41\u6570\u636e\uff0c\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.01460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01460", "abs": "https://arxiv.org/abs/2601.01460", "authors": ["Mohd Usama", "Belal Ahmad", "Christer Gronlund", "Faleh Menawer R Althiyabi"], "title": "Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network", "comment": "15 pages, 9 figures, 4 tables", "summary": "Deep learning has been extensively used in medical imaging applications, assuming that the test and training datasets belong to the same probability distribution. However, a common challenge arises when working with medical images generated by different systems or even the same system with different parameter settings. Such images contain diverse textures and reverberation noise that violate the aforementioned assumption. Consequently, models trained on data from one device or setting often struggle to perform effectively with data from other devices or settings. In addition, retraining models for each specific device or setting is labor-intensive and costly. To address these issues in ultrasound images, we propose a novel Generative Adversarial Network (GAN)-based model. We formulated the domain adaptation tasks as an image-to-image translation task, in which we modified the texture patterns and removed reverberation noise in the test data images from the source domain to align with those in the target domain images while keeping the image content unchanged. We applied the proposed method to two datasets containing carotid ultrasound images from three different domains. The experimental results demonstrate that the model successfully translated the texture pattern of images and removed reverberation noise from the ultrasound images. Furthermore, we evaluated the CycleGAN approaches for a comparative study with the proposed model. The experimental findings conclusively demonstrated that the proposed model achieved domain adaptation (histogram correlation (0.960 (0.019), & 0.920 (0.043) and bhattacharya distance (0.040 (0.020), & 0.085 (0.048)), compared to no adaptation (0.916 (0.062) & 0.890 (0.077), 0.090 (0.070) & 0.121 (0.095)) for both datasets.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGAN\u7684\u533b\u5b66\u56fe\u50cf\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u7edf\u4e00\u4e0d\u540c\u8d85\u58f0\u8bbe\u5907\u7684\u7eb9\u7406\u6a21\u5f0f\u5e76\u53bb\u9664\u6df7\u54cd\u566a\u58f0\uff0c\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u8bbe\u5907\u5dee\u5f02\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u4e0d\u540c\u8bbe\u5907\u6216\u53c2\u6570\u8bbe\u7f6e\u4ea7\u751f\u7684\u56fe\u50cf\u5b58\u5728\u7eb9\u7406\u548c\u566a\u58f0\u5dee\u5f02\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e8e\u67d0\u4e00\u8bbe\u5907\u7684\u6a21\u578b\u5728\u5176\u4ed6\u8bbe\u5907\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u9488\u5bf9\u6bcf\u4e2a\u8bbe\u5907\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eGAN\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6a21\u578b\uff0c\u5c06\u6e90\u57df\u56fe\u50cf\u8f6c\u6362\u4e3a\u76ee\u6807\u57df\u98ce\u683c\uff0c\u4fdd\u6301\u56fe\u50cf\u5185\u5bb9\u4e0d\u53d8\u7684\u540c\u65f6\u7edf\u4e00\u7eb9\u7406\u6a21\u5f0f\u5e76\u53bb\u9664\u6df7\u54cd\u566a\u58f0\u3002", "result": "\u5728\u5305\u542b\u4e09\u4e2a\u4e0d\u540c\u57df\u7684\u9888\u52a8\u8109\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u6210\u529f\u8f6c\u6362\u7eb9\u7406\u6a21\u5f0f\u5e76\u53bb\u9664\u566a\u58f0\uff0c\u5728\u76f4\u65b9\u56fe\u76f8\u5173\u6027(0.960 vs 0.916)\u548c\u5df4\u6c0f\u8ddd\u79bb(0.040 vs 0.090)\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u65e0\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684GAN\u57fa\u57df\u9002\u5e94\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u8bbe\u5907\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u6bd4CycleGAN\u7b49\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2601.01481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01481", "abs": "https://arxiv.org/abs/2601.01481", "authors": ["Mohammad Hassan Saghafi", "Seyed Majid Noorhosseini", "Seyed Abolfazl Seyed Javadein", "Hadi Khalili"], "title": "Robust Ship Detection and Tracking Using Modified ViBe and Backwash Cancellation Algorithm", "comment": null, "summary": "In this paper, we propose a robust real time detection and tracking method for detecting ships in a coastal video sequences. Since coastal scenarios are unpredictable and scenes have dynamic properties it is essential to apply detection methods that are robust to these conditions. This paper presents modified ViBe for moving object detection which detects ships and backwash. In the modified ViBe the probability of losing ships is decreased in comparison with the original ViBe. It is robust to natural sea waves and variation of lights and is capable of quickly updating the background. Based on geometrical properties of ship and some concepts such as brightness distortion, a new method for backwash cancellation is proposed. Experimental results demonstrate that the proposed strategy and methods have outstanding performance in ship detection and tracking. These results also illustrate real time and precise performance of the proposed strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u6d77\u5cb8\u89c6\u9891\u5e8f\u5217\u4e2d\u8239\u8236\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7684\u9c81\u68d2\u5b9e\u65f6\u65b9\u6cd5\uff0c\u5305\u62ec\u6539\u8fdb\u7684ViBe\u8fd0\u52a8\u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\u548c\u5c3e\u6d41\u6d88\u9664\u6280\u672f", "motivation": "\u6d77\u5cb8\u573a\u666f\u5177\u6709\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u52a8\u6001\u7279\u6027\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u8fd9\u4e9b\u6761\u4ef6\u7684\u9c81\u68d2\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u81ea\u7136\u6d77\u6d6a\u3001\u5149\u7ebf\u53d8\u5316\u7b49\u590d\u6742\u73af\u5883\u4e0b\u51c6\u786e\u68c0\u6d4b\u8239\u8236", "method": "1. \u6539\u8fdb\u7684ViBe\u7b97\u6cd5\u7528\u4e8e\u8fd0\u52a8\u7269\u4f53\u68c0\u6d4b\uff0c\u964d\u4f4e\u8239\u8236\u4e22\u5931\u6982\u7387\uff0c\u5bf9\u81ea\u7136\u6d77\u6d6a\u548c\u5149\u7ebf\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u5feb\u901f\u66f4\u65b0\u80cc\u666f\uff1b2. \u57fa\u4e8e\u8239\u8236\u51e0\u4f55\u7279\u6027\u548c\u4eae\u5ea6\u5931\u771f\u7b49\u6982\u5ff5\uff0c\u63d0\u51fa\u65b0\u7684\u5c3e\u6d41\u6d88\u9664\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u548c\u65b9\u6cd5\u5728\u8239\u8236\u68c0\u6d4b\u548c\u8ddf\u8e2a\u65b9\u9762\u5177\u6709\u4f18\u5f02\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u548c\u7cbe\u786e\u7684\u5904\u7406\u6548\u679c", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6d77\u5cb8\u89c6\u9891\u5e8f\u5217\u4e2d\u7684\u590d\u6742\u73af\u5883\uff0c\u5728\u8239\u8236\u68c0\u6d4b\u548c\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u7684\u8981\u6c42"}}
{"id": "2601.01483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01483", "abs": "https://arxiv.org/abs/2601.01483", "authors": ["Xinyu Qiu", "Heng Jia", "Zhengwen Zeng", "Shuheng Shen", "Changhua Meng", "Yi Yang", "Linchao Zhu"], "title": "Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization", "comment": null, "summary": "Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.", "AI": {"tldr": "ADPO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u5355\u4e00\u7b56\u7565\u4e2d\u8054\u5408\u5b66\u4e60\u7b54\u6848\u751f\u6210\u548c\u81ea\u6211\u9a8c\u8bc1\uff0c\u901a\u8fc7\u504f\u597d\u9a8c\u8bc1\u5956\u52b1\u548c\u89e3\u8026\u4f18\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u9a8c\u8bc1\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u9700\u8981\u5206\u522b\u8bad\u7ec3\u751f\u6210\u548c\u9a8c\u8bc1\u6a21\u578b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\u6765\u540c\u65f6\u5b66\u4e60\u751f\u6210\u548c\u9a8c\u8bc1\u80fd\u529b\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faADPO\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u521b\u65b0\uff1a1\uff09\u504f\u597d\u9a8c\u8bc1\u5956\u52b1\uff0c\u901a\u8fc7\u6b63\u8d1f\u6837\u672c\u7684\u5e73\u5747\u9a8c\u8bc1\u5206\u6570\u4f5c\u4e3a\u51b3\u7b56\u9608\u503c\uff0c\u5f53\u9884\u6d4b\u6b63\u786e\u6027\u4e0e\u7b54\u6848\u6b63\u786e\u6027\u4e00\u81f4\u65f6\u63d0\u4f9b\u6b63\u53cd\u9988\uff1b2\uff09\u4f18\u52bf\u89e3\u8026\u4f18\u5316\uff0c\u4e3a\u751f\u6210\u548c\u9a8c\u8bc1\u5206\u522b\u8ba1\u7b97\u4f18\u52bf\uff0c\u5e94\u7528token\u63a9\u7801\u9694\u79bb\u68af\u5ea6\uff0c\u7ed3\u5408\u63a9\u7801GRPO\u76ee\u6807\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u6821\u51c6\u9a8c\u8bc1\u5206\u6570\u3002", "result": "ADPO\u5b9e\u73b0\u4e86\u9a8c\u8bc1AUC\u63d0\u5347\u9ad8\u8fbe+34.1%\uff0c\u63a8\u7406\u65f6\u95f4\u964d\u4f4e-53.5%\uff0c\u5728MathVista/MMMU\u4e0a\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347+2.8%/+1.4%\uff0cReasonSeg\u4e0acIoU\u63d0\u5347+1.9\uff0cAndroidControl/GUI Odyssey\u4e0a\u6b65\u9aa4\u6210\u529f\u7387\u5206\u522b\u63d0\u5347+1.7%/+1.0%\u3002", "conclusion": "ADPO\u901a\u8fc7\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u548c\u9a8c\u8bc1\u5206\u79bb\u5e26\u6765\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u9a8c\u8bc1\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01452", "abs": "https://arxiv.org/abs/2601.01452", "authors": ["Jian Feng", "Zhihong Huang"], "title": "Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models", "comment": "19 pages, 1 figures, 4 tables", "summary": "Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/\u03b3$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\\times$--1.08$\\times$ of MeZO).", "AI": {"tldr": "BSZO\u662f\u4e00\u79cd\u8d1d\u53f6\u65af\u5b50\u7a7a\u95f4\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u7ed3\u5408\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u7684\u6709\u9650\u5dee\u5206\u4fe1\u606f\uff0c\u76f8\u6bd4\u4f20\u7edfZO\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u5728\u4fdd\u6301\u4f4e\u5185\u5b58\u6d88\u8017\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u968f\u673a\u6270\u52a8\u7684\u4e00\u6b65\u68af\u5ea6\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u4f18\u5316\u6548\u7387\u548c\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5229\u7528\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u4fe1\u606f\u3001\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u5b50\u7a7a\u95f4\u96f6\u9636\u4f18\u5316(BSZO)\uff0c\u5c06\u6bcf\u4e2a\u6709\u9650\u5dee\u5206\u6d4b\u91cf\u89c6\u4e3a\u566a\u58f0\u89c2\u6d4b\uff0c\u901a\u8fc7\u5361\u5c14\u66fc\u6ee4\u6ce2\u6784\u5efa\u6295\u5f71\u68af\u5ea6\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u63a8\u65ad\u66f4\u65b0\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u6b8b\u5dee\u7684\u81ea\u9002\u5e94\u673a\u5236\u8c03\u6574\u6270\u52a8\u5c3a\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793aBSZO\u6bd4\u6807\u51c6ZO\u65b9\u6cd5\u7684\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e86k/\u03b3\u500d\u3002\u5728RoBERTa\u3001Mistral\u548cOPT\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBSZO\u4f18\u4e8eMeZO\u3001MeZO-Adam\u548cHiZOO\uff0c\u5728OPT-13B\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad86.67%\u7684\u7edd\u5bf9\u5e73\u5747\u63d0\u5347\uff0c\u5185\u5b58\u4f7f\u7528\u63a5\u8fd1\u4ec5\u63a8\u7406\u57fa\u7ebf\uff08MeZO\u76841.00-1.08\u500d\uff09\u3002", "conclusion": "BSZO\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u6709\u6548\u6574\u5408\u591a\u4e2a\u6270\u52a8\u65b9\u5411\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u9636\u4f18\u5316\u7684\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u5185\u5b58\u6d88\u8017\u7684\u4f18\u52bf\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.01485", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01485", "abs": "https://arxiv.org/abs/2601.01485", "authors": ["Zobia Batool", "Diala Lteif", "Vijaya B. Kolachalama", "Huseyin Ozkan", "Erchan Aptoula"], "title": "Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease", "comment": null, "summary": "Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faExtended MixStyle (EM)\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u9ad8\u9636\u7279\u5f81\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u6765\u6a21\u62df\u591a\u6837\u5316\u7684\u5206\u5e03\u53d8\u5316\uff0c\u4ee5\u89e3\u51b3\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u5355\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53472.4\u4e2a\u767e\u5206\u70b9\u7684\u5b8fF1\u5206\u6570\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u57fa\u4e8e\u7ed3\u6784\u78c1\u5171\u632f\u6210\u50cf\u8bad\u7ec3\u7684\u6a21\u578b\u7531\u4e8e\u626b\u63cf\u4eea\u3001\u534f\u8bae\u548c\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u7684\u5dee\u5f02\u5bfc\u81f4\u57df\u504f\u79fb\uff0c\u5728\u65b0\u961f\u5217\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4f5c\u4e3a\u75f4\u5446\u75c7\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u901a\u8fc7\u840e\u7f29\u548c\u8111\u5ba4\u6269\u5f20\u7b49\u6e10\u8fdb\u6027\u8ba4\u77e5\u548c\u795e\u7ecf\u89e3\u5256\u5b66\u53d8\u5316\u8868\u73b0\u51fa\u6765\uff0c\u56e0\u6b64\u9700\u8981\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u7684\u5206\u7c7b\u65b9\u6cd5\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002\u5355\u57df\u6cdb\u5316\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6570\u636e\u96c6\u7684\u788e\u7247\u5316\u80cc\u666f\u4e0b\u81f3\u5173\u91cd\u8981\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faExtended MixStyle (EM)\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u9ad8\u9636\u7279\u5f81\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u6765\u6a21\u62df\u591a\u6837\u5316\u7684\u5206\u5e03\u53d8\u5316\u3002\u4f7f\u7528\u56fd\u5bb6\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u534f\u8c03\u4e2d\u5fc3\u7684\u7ed3\u6784\u78c1\u5171\u632f\u6210\u50cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u533a\u5206\u6b63\u5e38\u8ba4\u77e5\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u60a3\u8005\uff0c\u5e76\u5728\u4e09\u4e2a\u672a\u89c1\u961f\u5217\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5728\u4e09\u4e2a\u672a\u89c1\u961f\u5217\uff08\u603b\u6837\u672c\u91cf3,126\uff09\u4e0a\uff0cEM\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u6027\u80fd\uff0c\u5e73\u5747\u5b8fF1\u5206\u6570\u6bd4\u6700\u5148\u8fdb\u7684\u5355\u57df\u6cdb\u5316\u57fa\u51c6\u63d0\u9ad8\u4e862.4\u4e2a\u767e\u5206\u70b9\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5f02\u8d28\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u73b0\u4e0d\u53d8\u3001\u53ef\u9760\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u7684\u6f5c\u529b\u3002", "conclusion": "Extended MixStyle\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u9ad8\u9636\u7279\u5f81\u77e9\u6709\u6548\u89e3\u51b3\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u4e2d\u7684\u5355\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5f02\u8d28\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u53ef\u9760\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01487", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01487", "abs": "https://arxiv.org/abs/2601.01487", "authors": ["Ziyue Zhang", "Luxi Lin", "Xiaolin Hu", "Chao Chang", "HuaiXi Wang", "Yiyi Zhou", "Rongrong Ji"], "title": "DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion", "comment": null, "summary": "Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.", "AI": {"tldr": "\u63d0\u51faDeepInv\u81ea\u76d1\u7763\u6269\u6563\u53cd\u6f14\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u76ee\u6807\u548c\u6570\u636e\u589e\u5f3a\u751f\u6210\u4f2a\u566a\u58f0\uff0c\u8bad\u7ec3\u53c2\u6570\u5316\u53cd\u6f14\u6c42\u89e3\u5668\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u56fe\u50cf\u5230\u566a\u58f0\u6620\u5c04\u3002", "motivation": "\u6269\u6563\u53cd\u6f14\u4efb\u52a1\u7f3a\u4e4f\u53ef\u884c\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u57fa\u4e8e\u8fd1\u4f3c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f80\u5f80\u5728\u6027\u80fd\u6216\u6548\u7387\u4e0a\u6709\u6240\u727a\u7272\u3002", "method": "\u63d0\u51fa\u81ea\u76d1\u7763\u6269\u6563\u53cd\u6f14\u65b9\u6cd5DeepInv\uff1a1) \u5f15\u5165\u81ea\u76d1\u7763\u76ee\u6807\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u566a\u58f0\uff1b2) \u91c7\u7528\u8fed\u4ee3\u591a\u5c3a\u5ea6\u8bad\u7ec3\u673a\u5236\u8bad\u7ec3\u53c2\u6570\u5316\u53cd\u6f14\u6c42\u89e3\u5668\uff1b3) \u9996\u6b21\u63d0\u51fa\u53ef\u8bad\u7ec3\u7684\u5206\u6b65\u9884\u6d4b\u53cd\u6f14\u566a\u58f0\u6c42\u89e3\u5668\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0cDeepInv\u6bd4EasyInv SSIM\u63d0\u534740.435%\uff0c\u6bd4ReNoise\u63a8\u7406\u901f\u5ea6\u63d0\u53479887.5%\uff0c\u5728\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "DeepInv\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u53ef\u8bad\u7ec3\u6c42\u89e3\u5668\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u6269\u6563\u53cd\u6f14\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u601d\u8def\u3002"}}
{"id": "2601.01475", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01475", "abs": "https://arxiv.org/abs/2601.01475", "authors": ["Ruofeng Yang", "Yongcan Li", "Bo Jiang", "Cheng Chen", "Shuai Li"], "title": "Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts", "comment": null, "summary": "Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \\times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\\sqrt{\u03a3_{k=1}^Kn_k}\\sqrt{\u03a3_{k=1}^Kn_kd_k}/\\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faMoLR-MoG\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4f4e\u79e9\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6355\u6349\u6570\u636e\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u751f\u6210\u6548\u679c\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u9762\u4e34\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\uff08n^{-1/D}\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u8003\u8651\u4e86\u6570\u636e\u7684\u591a\u6d41\u5f62\u7279\u6027\uff0c\u4f46\u4f7f\u7528\u9ad8\u65af\u6f5c\u5728\u53d8\u91cf\u65e0\u6cd5\u6355\u6349\u6f5c\u5728\u6d41\u5f62\u7684\u591a\u6a21\u6001\u7279\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u53cd\u6620\u591a\u6d41\u5f62\u7ed3\u6784\u53c8\u80fd\u6355\u6349\u591a\u6a21\u6001\u7279\u6027\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMoLR-MoG\uff08\u6df7\u5408\u4f4e\u79e9\u9ad8\u65af\u6df7\u5408\uff09\u5efa\u6a21\uff1a\u5c06\u76ee\u6807\u6570\u636e\u5efa\u6a21\u4e3aK\u4e2a\u7ebf\u6027\u5b50\u7a7a\u95f4\u7684\u5e76\u96c6\uff0c\u6bcf\u4e2a\u5b50\u7a7a\u95f4\u91c7\u7528\u9ad8\u65af\u6df7\u5408\u6f5c\u5728\u53d8\u91cf\uff08n_k\u4e2a\u6a21\u6001\uff0c\u7ef4\u5ea6d_k\uff09\u3002\u5bf9\u5e94\u7684\u5f97\u5206\u51fd\u6570\u5177\u6709\u6df7\u5408\u4e13\u5bb6\u7ed3\u6784\uff0c\u80fd\u6355\u6349\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u5305\u542b\u975e\u7ebf\u6027\u7279\u6027\u3002", "result": "1. \u5b9e\u9a8c\u8868\u660eMoE-latent MoG NN\u7684\u751f\u6210\u6548\u679c\u663e\u8457\u4f18\u4e8eMoE-latent Gaussian score\uff1b2. MoE-latent MoG NN\u4e0e\u53c2\u6570\u591a10\u500d\u7684MoE-latent Unet\u6027\u80fd\u76f8\u5f53\uff1b3. \u7406\u8bba\u5206\u6790\u5f97\u5230R^4\u221a(\u03a3n_k)\u221a(\u03a3n_kd_k)/\u221an\u7684\u4f30\u8ba1\u8bef\u5dee\uff0c\u6446\u8131\u4e86\u7ef4\u5ea6\u8bc5\u5492\uff1b4. \u8bc1\u660e\u4e86MoLR-MoG\u5efa\u6a21\u4e0b\u7684\u4f18\u5316\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "MoLR-MoG\u5efa\u6a21\u5408\u7406\u4e14\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u6269\u6563\u6a21\u578b\u53ea\u9700\u5c11\u91cf\u8bad\u7ec3\u6837\u672c\u548c\u5feb\u901f\u4f18\u5316\u8fc7\u7a0b\u5c31\u80fd\u83b7\u5f97\u4f18\u5f02\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6570\u636e\u7ed3\u6784\u6446\u8131\u7ef4\u5ea6\u8bc5\u5492\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2601.01513", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01513", "abs": "https://arxiv.org/abs/2601.01513", "authors": ["Gen Li", "Peiyu Liu"], "title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation", "comment": null, "summary": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.", "AI": {"tldr": "VideoSpeculateRAG\uff1a\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\u7684\u9ad8\u6548\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u751f\u6210\u5019\u9009\u7b54\u6848\uff0c\u518d\u7531\u91cd\u91cf\u7ea7\u6a21\u578b\u9a8c\u8bc1\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u65b9\u9762\u4ecd\u6709\u56f0\u96be\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u7b54\u6848\u8d28\u91cf\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVideoSpeculateRAG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1\uff09\u63a8\u6d4b\u89e3\u7801\u6d41\u6c34\u7ebf\uff1a\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u5feb\u901f\u751f\u6210\u591a\u4e2a\u5019\u9009\u7b54\u6848\uff0c\u518d\u7531\u51c6\u786e\u7684\u91cd\u91cf\u7ea7\u6a21\u578b\u9a8c\u8bc1\u548c\u4f18\u5316\uff1b2\uff09\u76f8\u4f3c\u6027\u8fc7\u6ee4\u7b56\u7565\uff1a\u89e3\u51b3\u68c0\u7d22\u77e5\u8bc6\u4e2d\u5b9e\u4f53\u8bc6\u522b\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u5347\u5b9e\u4f53\u5bf9\u9f50\u548c\u7b54\u6848\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVideoSpeculateRAG\u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u6807\u51c6RAG\u65b9\u6cd5\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u7ea62\u500d\uff0c\u6709\u6548\u5e73\u8861\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5c06\u63a8\u6d4b\u89e3\u7801\u4e0e\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u76f8\u7ed3\u5408\u5728\u590d\u6742\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u6574\u5408\u65b9\u9762\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.01526", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01526", "abs": "https://arxiv.org/abs/2601.01526", "authors": ["Hongbing Li", "Linhui Xiao", "Zihan Zhao", "Qi Shen", "Yixiang Huang", "Bo Xiao", "Zhanyu Ma"], "title": "BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding", "comment": null, "summary": "Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.", "AI": {"tldr": "BARE\u662f\u4e00\u4e2a\u7528\u4e8e\u5355\u5854\u89c6\u89c9\u5b9a\u4f4d\u7684\u504f\u7f6e\u611f\u77e5\u548c\u63a8\u7406\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u7559\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u548c\u6784\u5efa\u6307\u79f0\u8bed\u4e49\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u6a21\u6001\u504f\u7f6e\u548c\u8bed\u4e49\u63a8\u7406\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5355\u5854\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u8fc7\u5ea6\u7ea0\u7f20\u7684\u591a\u6a21\u6001\u8868\u793a\u52a0\u5267\u4e86\u6b3a\u9a97\u6027\u6a21\u6001\u504f\u7f6e\uff1b2\uff09\u8bed\u4e49\u63a8\u7406\u4e0d\u8db3\u963b\u788d\u4e86\u6307\u79f0\u7ebf\u7d22\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51faBARE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u65b0\u6a21\u5757\uff1a\u8bed\u8a00\u663e\u8457\u6027\u8c03\u5236\u5668\u3001\u89c6\u89c9\u504f\u7f6e\u6821\u6b63\u548c\u6307\u79f0\u5173\u7cfb\u589e\u5f3a\uff0c\u5171\u540c\u51cf\u8f7b\u591a\u6a21\u6001\u5e72\u6270\u5e76\u589e\u5f3a\u6307\u79f0\u7406\u89e3\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBARE\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u800c\u4e14\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "BARE\u901a\u8fc7\u504f\u7f6e\u611f\u77e5\u548c\u63a8\u7406\u589e\u5f3a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u6a21\u6001\u504f\u7f6e\u548c\u8bed\u4e49\u63a8\u7406\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.01501", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01501", "abs": "https://arxiv.org/abs/2601.01501", "authors": ["Fan Xu", "Wei Gong", "Hao Wu", "Lilan Peng", "Nan Wang", "Qingsong Wen", "Xian Wu", "Kun Wang", "Xibin Zhao"], "title": "Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE", "comment": null, "summary": "Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.", "AI": {"tldr": "HiGO\u6846\u67b6\u901a\u8fc7\u591a\u5c42\u7ea7\u56fe\u7ed3\u6784\u548c\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u7403\u91ce\u706b\u957f\u671f\u9884\u6d4b\u6027\u80fd", "motivation": "\u91ce\u706b\u4f5c\u4e3a\u5730\u7403\u7cfb\u7edf\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u53d7\u5230\u5927\u6c14\u3001\u6d77\u6d0b\u548c\u9646\u5730\u8fc7\u7a0b\u5728\u5e7f\u6cdb\u65f6\u7a7a\u5c3a\u5ea6\u4e0a\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u5f71\u54cd\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u5728\u5168\u7403\u5929\u6c14\u9884\u62a5\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u5728\u5168\u7403\u91ce\u706b\u884c\u4e3a\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u56feODE\uff08HiGO\uff09\u6846\u67b6\uff1a1\uff09\u5c06\u5730\u7403\u7cfb\u7edf\u8868\u793a\u4e3a\u591a\u5c42\u7ea7\u56fe\u5c42\u6b21\u7ed3\u6784\uff1b2\uff09\u63d0\u51fa\u81ea\u9002\u5e94\u6ee4\u6ce2\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u7528\u4e8e\u5c42\u7ea7\u5185\u548c\u5c42\u7ea7\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\uff1b3\uff09\u5728\u591a\u4e2a\u5c42\u7ea7\u4e0a\u96c6\u6210GNN\u53c2\u6570\u5316\u7684\u795e\u7ecfODE\u6a21\u5757\uff0c\u663e\u5f0f\u5b66\u4e60\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u8fde\u7eed\u52a8\u529b\u5b66\u3002", "result": "\u5728SeasFire Cube\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHiGO\u5728\u957f\u671f\u91ce\u706b\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002\u5176\u8fde\u7eed\u65f6\u95f4\u9884\u6d4b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u89c2\u6d4b\u4e00\u81f4\u6027\uff0c\u7a81\u663e\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002", "conclusion": "HiGO\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u591a\u5c3a\u5ea6\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\uff0c\u4e3a\u5168\u7403\u91ce\u706b\u884c\u4e3a\u9884\u6d4b\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.01528", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01528", "abs": "https://arxiv.org/abs/2601.01528", "authors": ["Yang Zhou", "Hao Shao", "Letian Wang", "Zhuofan Zong", "Hongsheng Li", "Steven L. Waslander"], "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving", "comment": "10 pages, 4 figures; Project Website: https://drivinggen-bench.github.io/", "summary": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.", "AI": {"tldr": "DrivingGen\u662f\u9996\u4e2a\u7528\u4e8e\u751f\u6210\u5f0f\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u65b0\u6307\u6807\u8bc4\u4f30\u89c6\u89c9\u771f\u5b9e\u6027\u3001\u8f68\u8ff9\u5408\u7406\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u5f53\u524d\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u7814\u7a76\u7f3a\u4e4f\u4e25\u8c28\u7684\u57fa\u51c6\u6765\u6d4b\u91cf\u8fdb\u5c55\u548c\u6307\u5bfc\u4f18\u5148\u7ea7\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u591a\u4e2a\u5c40\u9650\u6027\uff1a\u901a\u7528\u89c6\u9891\u6307\u6807\u5ffd\u89c6\u5b89\u5168\u5173\u952e\u56e0\u7d20\uff1b\u8f68\u8ff9\u5408\u7406\u6027\u5f88\u5c11\u91cf\u5316\uff1b\u65f6\u95f4\u548c\u667a\u80fd\u4f53\u7ea7\u522b\u4e00\u81f4\u6027\u88ab\u5ffd\u7565\uff1b\u5bf9\u81ea\u6211\u6761\u4ef6\u63a7\u5236\u7684\u53ef\u63a7\u6027\u88ab\u5ffd\u89c6\uff1b\u4e14\u5f53\u524d\u6570\u636e\u96c6\u672a\u80fd\u8986\u76d6\u73b0\u5b9e\u90e8\u7f72\u6240\u9700\u7684\u591a\u6837\u6027\u6761\u4ef6\u3002", "method": "\u63d0\u51faDrivingGen\u57fa\u51c6\uff0c\u7ed3\u5408\u4ece\u9a7e\u9a76\u6570\u636e\u96c6\u548c\u4e92\u8054\u7f51\u89c4\u6a21\u89c6\u9891\u6e90\u4e2d\u7b56\u5212\u7684\u591a\u6837\u5316\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u5929\u6c14\u3001\u65f6\u95f4\u3001\u5730\u7406\u533a\u57df\u548c\u590d\u6742\u64cd\u4f5c\uff0c\u5e76\u914d\u5907\u4e00\u5957\u65b0\u6307\u6807\u6765\u8054\u5408\u8bc4\u4f30\u89c6\u89c9\u771f\u5b9e\u6027\u3001\u8f68\u8ff9\u5408\u7406\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u3002", "result": "\u5bf914\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u660e\u786e\u7684\u6743\u8861\uff1a\u901a\u7528\u6a21\u578b\u770b\u8d77\u6765\u66f4\u597d\u4f46\u8fdd\u53cd\u7269\u7406\u89c4\u5f8b\uff0c\u800c\u9a7e\u9a76\u4e13\u7528\u6a21\u578b\u80fd\u771f\u5b9e\u6355\u6349\u8fd0\u52a8\u4f46\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u843d\u540e\u3002DrivingGen\u4e3a\u8bc4\u4f30\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\u3002", "conclusion": "DrivingGen\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u4fc3\u8fdb\u53ef\u9760\u3001\u53ef\u63a7\u548c\u53ef\u90e8\u7f72\u7684\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6a21\u62df\u3001\u89c4\u5212\u548c\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2601.01558", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01558", "abs": "https://arxiv.org/abs/2601.01558", "authors": ["Pengfei Qu", "Wenyu Ouyang", "Chi Zhang", "Yikai Chai", "Shuolong Xu", "Lei Ye", "Yongri Piao", "Miao Zhang", "Huchuan Lu"], "title": "Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings", "comment": "12 pages, 11 figures", "summary": "Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.", "AI": {"tldr": "\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u7684\u73af\u5883\u5d4c\u5165\u6bd4\u4f20\u7edf\u6d41\u57df\u5c5e\u6027\u66f4\u80fd\u6709\u6548\u9884\u6d4b\u65e0\u5f84\u6d41\u8bb0\u5f55\u5730\u533a\u7684\u6cb3\u6d41\u6d41\u91cf", "motivation": "\u4f20\u7edf\u6d41\u57df\u5c5e\u6027\u65e0\u6cd5\u5b8c\u5168\u63cf\u8ff0\u81ea\u7136\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8868\u5f81\u6d41\u57df\u7279\u5f81\u4ee5\u9884\u6d4b\u65e0\u5f84\u6d41\u8bb0\u5f55\u5730\u533a\u7684\u6cb3\u6d41\u6d41\u91cf", "method": "\u4f7f\u7528AlphaEarth Foundation\u5d4c\u5165\uff08\u4ece\u5927\u91cf\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u7684\u73af\u5883\u8868\u793a\uff09\u6765\u63cf\u8ff0\u6d41\u57df\u7279\u5f81\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u9009\u62e9\u9002\u5f53\u7684\u4f9b\u4f53\u6d41\u57df\u6765\u9884\u6d4b\u65e0\u6d4b\u7ad9\u5730\u533a\u7684\u6d41\u91cf", "result": "\u57fa\u4e8e\u5d4c\u5165\u7684\u6a21\u578b\u5728\u9884\u6d4b\u672a\u53c2\u4e0e\u8bad\u7ec3\u7684\u6d41\u57df\u6d41\u91cf\u65f6\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u8868\u660e\u8fd9\u4e9b\u5d4c\u5165\u6bd4\u4f20\u7edf\u5c5e\u6027\u66f4\u80fd\u6355\u6349\u5173\u952e\u7684\u7269\u7406\u5dee\u5f02\uff1b\u57fa\u4e8e\u5d4c\u5165\u76f8\u4f3c\u6027\u9009\u62e9\u4f9b\u4f53\u6d41\u57df\u80fd\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd", "conclusion": "\u536b\u661f\u56fe\u50cf\u5b66\u4e60\u7684\u73af\u5883\u8868\u793a\u53ef\u4ee5\u589e\u5f3a\u6c34\u6587\u9884\u6d4b\u80fd\u529b\uff0c\u652f\u6301\u5f00\u53d1\u66f4\u9002\u5e94\u4e0d\u540c\u666f\u89c2\u7684\u6c34\u6587\u6a21\u578b"}}
{"id": "2601.01535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01535", "abs": "https://arxiv.org/abs/2601.01535", "authors": ["Zixuan Fu", "Lanqing Guo", "Chong Wang", "Binbin Song", "Ding Liu", "Bihan Wen"], "title": "Improving Flexible Image Tokenizers for Autoregressive Image Generation", "comment": null, "summary": "Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \\textbf{ReToK}, a flexible tokenizer with \\underline{Re}dundant \\underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \\textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \\textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \\href{https://github.com/zfu006/ReTok}{https://github.com/zfu006/ReTok}", "AI": {"tldr": "ReToK\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7075\u6d3b\u56fe\u50cf\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u5197\u4f59\u4ee4\u724c\u586b\u5145\u548c\u5206\u5c42\u8bed\u4e49\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5d4c\u5957\u4e22\u5f03\u65b9\u6cd5\u4e2d\u4fe1\u606f\u8fc7\u5ea6\u96c6\u4e2d\u5728\u65e9\u671f\u4ee4\u724c\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7075\u6d3b\u56fe\u50cf\u5206\u8bcd\u5668\u4f7f\u7528\u5d4c\u5957\u4e22\u5f03\uff08\u5c3e\u90e8\u622a\u65ad\uff09\u7b56\u7565\uff0c\u5bfc\u81f4\u56fe\u50cf\u4fe1\u606f\u8fc7\u5ea6\u96c6\u4e2d\u5728\u65e9\u671f\u4ee4\u724c\u4e2d\uff0c\u9650\u5236\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5f53\u4ee4\u724c\u957f\u5ea6\u589e\u52a0\u65f6\u3002", "method": "\u63d0\u51faReToK\u65b9\u6cd5\uff1a1\uff09\u5197\u4f59\u4ee4\u724c\u586b\u5145\uff1a\u66f4\u9891\u7e41\u5730\u6fc0\u6d3b\u5c3e\u90e8\u4ee4\u724c\uff0c\u7f13\u89e3\u4fe1\u606f\u8fc7\u5ea6\u96c6\u4e2d\u95ee\u9898\uff1b2\uff09\u5206\u5c42\u8bed\u4e49\u6b63\u5219\u5316\uff1a\u5bf9\u9f50\u65e9\u671f\u4ee4\u724c\u7684\u89e3\u7801\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u540c\u65f6\u5411\u5c3e\u90e8\u9010\u6e10\u51cf\u5c11\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u5141\u8bb8\u66f4\u7cbe\u7ec6\u7684\u4f4e\u7ea7\u7ec6\u8282\u91cd\u5efa\u3002", "result": "\u5728ImageNet 256\u00d7256\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReToK\u76f8\u6bd4\u7075\u6d3b\u548c\u56fa\u5b9a\u957f\u5ea6\u5206\u8bcd\u5668\u90fd\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u751f\u6210\u6027\u80fd\u3002", "conclusion": "ReToK\u901a\u8fc7\u5197\u4f59\u4ee4\u724c\u586b\u5145\u548c\u5206\u5c42\u8bed\u4e49\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7075\u6d3b\u56fe\u50cf\u5206\u8bcd\u5668\u4e2d\u4fe1\u606f\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u3002"}}
{"id": "2601.01605", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01605", "abs": "https://arxiv.org/abs/2601.01605", "authors": ["Xin Di", "Xinglin Piao", "Fei Wang", "Guodong Jing", "Yong Zhang"], "title": "REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training", "comment": null, "summary": "Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.", "AI": {"tldr": "\u63d0\u51faREE-TTT\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\u6539\u8fdb\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\uff0c\u589e\u5f3a\u8de8\u533a\u57df\u6781\u7aef\u964d\u6c34\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u8d28\u91cf\u672c\u5730\u8bad\u7ec3\u6570\u636e\u548c\u9759\u6001\u6a21\u578b\u53c2\u6570\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u533a\u57df\u548c\u6781\u7aef\u4e8b\u4ef6", "method": "\u63d0\u51faREE-TTT\u6a21\u578b\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\uff0c\u8bbe\u8ba1\u65f6\u7a7a\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u5757\uff0c\u7528\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u673a\u5236\u66ff\u4ee3\u6807\u51c6\u7ebf\u6027\u6295\u5f71\uff0c\u589e\u5f3a\u5bf9\u975e\u5e73\u7a33\u6c14\u8c61\u5206\u5e03\u7684\u9002\u5e94\u80fd\u529b", "result": "\u5728\u8de8\u533a\u57df\u6781\u7aef\u964d\u6c34\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0cREE-TTT\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u5bf9\u6570\u636e\u5206\u5e03\u53d8\u5316\u7684\u51fa\u8272\u9002\u5e94\u6027", "conclusion": "REE-TTT\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u96f7\u8fbe\u56de\u6ce2\u5916\u63a8\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01547", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01547", "abs": "https://arxiv.org/abs/2601.01547", "authors": ["Tianjun Gu", "Chenghua Gong", "Jingyu Gong", "Zhizhong Zhang", "Yuan Xie", "Lizhuang Ma", "Xin Tan"], "title": "EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding", "comment": null, "summary": "The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTeleo-Spatial Intelligence (TSI)\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u7269\u7406\u52a8\u6001\u63a8\u7406\u548c\u610f\u56fe\u9a71\u52a8\u63a8\u7406\uff0c\u5e76\u521b\u5efaEscherVerse\u57fa\u51c6\u5957\u4ef6\u6765\u8bc4\u4f30AI\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7a7a\u95f4\u63a8\u7406\u7814\u7a76\u5ffd\u7565\u4e86\u4eba\u7c7b\u610f\u56fe\u5728\u7a7a\u95f4\u53d8\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u9700\u8981\u4ece\u88ab\u52a8\u573a\u666f\u63cf\u8ff0\u8f6c\u5411\u76ee\u7684\u9a71\u52a8\u7684\u6574\u4f53\u4e16\u754c\u7406\u89e3\u3002", "method": "\u63d0\u51faTSI\u8303\u5f0f\uff0c\u5305\u542b\u7269\u7406\u52a8\u6001\u63a8\u7406\u548c\u610f\u56fe\u9a71\u52a8\u63a8\u7406\u4e24\u4e2a\u652f\u67f1\uff1b\u521b\u5efaEscherVerse\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u62ecEscher-Bench\u57fa\u51c6\u3001Escher-35k\u6570\u636e\u96c6\u548cEscher\u7cfb\u5217\u6a21\u578b\uff1b\u5f00\u53d1\u4e86\u65b0\u9896\u7684\u6570\u636e\u6574\u7406\u6d41\u7a0b\u3002", "result": "EscherVerse\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u610f\u56fe\u9a71\u52a8\u63a8\u7406\u7684\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u89c6\u9891\uff0c\u8bc4\u4f30\u7269\u4f53\u6052\u5b58\u6027\u3001\u72b6\u6001\u8f6c\u6362\u548c\u8f68\u8ff9\u9884\u6d4b\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u53d7\u9650\u73af\u5883\u8bbe\u7f6e\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4ece\u88ab\u52a8\u573a\u666f\u63cf\u8ff0\u5411\u6574\u4f53\u76ee\u7684\u9a71\u52a8\u4e16\u754c\u7406\u89e3\u63a8\u8fdb\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u5f00\u542f\u4e86\u7a7a\u95f4\u63a8\u7406\u7814\u7a76\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2601.01616", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.01616", "abs": "https://arxiv.org/abs/2601.01616", "authors": ["Md Istiauk Hossain Rifat", "Moin Khan", "Mohammad Zunaed"], "title": "Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry", "comment": "9 pages, 9 figures", "summary": "The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u5b5f\u52a0\u62c9\u56fd\u7eba\u7ec7\u4e1a\u5f00\u53d1\u4e86\u57fa\u4e8eNILM\u7684\u5b9e\u65f6\u80fd\u8017\u76d1\u6d4b\u6846\u67b6\uff0c\u9488\u5bf9\u76f8\u540c\u7535\u673a\u8d1f\u8f7d\uff08\u7eba\u7ec7\u5207\u5272\u673a\uff09\u8fdb\u884c\u76d1\u63a7\uff0c\u7ed3\u679c\u663e\u793a\u603b\u4f53\u80fd\u8017\u4f30\u8ba1\u51c6\u786e\uff0c\u4f46\u76f8\u540c\u8bbe\u5907\u540c\u65f6\u8fd0\u884c\u65f6\u5206\u89e3\u7cbe\u5ea6\u4e0b\u964d\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u7eba\u7ec7\u4e1a\u662f\u80fd\u6e90\u5bc6\u96c6\u578b\u884c\u4e1a\uff0c\u4f46\u73b0\u6709\u76d1\u6d4b\u65b9\u6cd5\u8fc7\u65f6\uff0c\u5bfc\u81f4\u80fd\u6e90\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u548c\u8fd0\u8425\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u5f00\u53d1\u5b9e\u65f6\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u7535\u538b\u7535\u6d41\u4f20\u611f\u5668\u3001Arduino Mega\u548cESP8266\u7684\u786c\u4ef6\u7cfb\u7edf\uff0c\u91c7\u96c6\u603b\u8d1f\u8f7d\u548c\u5355\u4e2a\u8d1f\u8f7d\u6570\u636e\uff0c\u5728\u4e91\u5e73\u53f0\u5904\u7406\uff1b\u521b\u5efa\u4e86\u5305\u542b\u4e09\u4e2a\u76f8\u540c\u611f\u5e94\u7535\u673a\u548c\u8f85\u52a9\u8d1f\u8f7d\u7684\u65b0\u6570\u636e\u96c6\uff08\u8d85\u8fc718\u4e07\u4e2a\u6837\u672c\uff09\uff0c\u4f7f\u7528\u6700\u5148\u8fdb\u7684MATNILM\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u603b\u4f53\u80fd\u8017\u4f30\u8ba1\u8f83\u4e3a\u51c6\u786e\uff0c\u4f46\u5355\u4e2a\u8bbe\u5907\u5206\u89e3\u9762\u4e34\u56f0\u96be\uff0c\u7279\u522b\u662f\u591a\u4e2a\u76f8\u540c\u673a\u5668\u540c\u65f6\u8fd0\u884c\u65f6\uff1b\u96c6\u6210\u7cfb\u7edf\u901a\u8fc7Blynk\u5e94\u7528\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u5b9e\u65f6\u8fdc\u7a0b\u76d1\u63a7\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86NILM\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u5efa\u8bae\u672a\u6765\u6539\u8fdb\u5305\u62ec\u66f4\u9ad8\u9891\u7387\u6570\u636e\u91c7\u96c6\u3001\u66f4\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ee5\u53ca\u5904\u7406\u76f8\u540c\u8d1f\u8f7d\u7684\u5148\u8fdb\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.01593", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.01593", "abs": "https://arxiv.org/abs/2601.01593", "authors": ["Haonan Cai", "Yuxuan Luo", "Zhouhui Lian"], "title": "Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation", "comment": "25 pages", "summary": "Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.", "AI": {"tldr": "GAR-Font\u662f\u4e00\u4e2a\u7528\u4e8e\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u7684\u65b0\u578b\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u611f\u77e5\u5206\u8bcd\u5668\u3001\u591a\u6a21\u6001\u98ce\u683c\u7f16\u7801\u5668\u548c\u540e\u5904\u7406\u7ec6\u5316\u7ba1\u9053\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u4f7f\u7528\u8865\u4e01\u7ea7\u5206\u8bcd\uff0c\u5ffd\u7565\u4e86\u5b57\u4f53\u5408\u6210\u6240\u9700\u7684\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\uff1b2\uff09\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u56fe\u50cf\u5230\u56fe\u50cf\u8303\u5f0f\uff0c\u4ec5\u4f9d\u8d56\u89c6\u89c9\u53c2\u8003\u800c\u5ffd\u89c6\u4e86\u8bed\u8a00\u5728\u4f20\u8fbe\u5b57\u4f53\u8bbe\u8ba1\u98ce\u683c\u610f\u56fe\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86GAR-Font\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u5168\u5c40\u611f\u77e5\u5206\u8bcd\u5668\uff0c\u540c\u65f6\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u548c\u5168\u5c40\u98ce\u683c\u6a21\u5f0f\uff1b2\uff09\u591a\u6a21\u6001\u98ce\u683c\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bed\u8a00\u98ce\u683c\u9002\u914d\u5668\u5b9e\u73b0\u7075\u6d3b\u7684\u98ce\u683c\u63a7\u5236\uff0c\u65e0\u9700\u5bc6\u96c6\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\uff1b3\uff09\u540e\u5904\u7406\u7ec6\u5316\u7ba1\u9053\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGAR-Font\u5728\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u98ce\u683c\u5fe0\u5b9e\u5ea6\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u4e14\u901a\u8fc7\u6587\u672c\u98ce\u683c\u6307\u5bfc\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u7ed3\u679c\u3002", "conclusion": "GAR-Font\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u611f\u77e5\u5efa\u6a21\u548c\u591a\u6a21\u6001\u98ce\u683c\u63a7\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u4e2d\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u4e3a\u5b57\u4f53\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2601.01649", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01649", "abs": "https://arxiv.org/abs/2601.01649", "authors": ["Umesh Vangapally", "Wenhan Wu", "Chen Chen", "Zhishuai Guo"], "title": "Communication-Efficient Federated AUC Maximization with Cyclic Client Participation", "comment": "Accepted to Transactions on Machine Learning Research (TMLR)", "summary": "Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-\u0141ojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\\widetilde{O}(1/\u03b5^{1/2})$ and iteration complexity of $\\widetilde{O}(1/\u03b5)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/\u03b5^3)$ and an iteration complexity of $O(1/\u03b5^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\\widetilde{O}(1/\u03b5^{1/2})$ and iteration complexity of $\\widetilde{O}(1/\u03b5)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u5468\u671f\u6027\u5ba2\u6237\u7aef\u53c2\u4e0e\u573a\u666f\u4e0b\u7684AUC\u6700\u5927\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5206\u522b\u5728\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\u548c\u4e00\u822c\u6210\u5bf9\u635f\u5931\u4e0b\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u901a\u4fe1\u548c\u8fed\u4ee3\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u8054\u90a6AUC\u6700\u5927\u5316\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5ba2\u6237\u7aef\u5b8c\u5168\u53ef\u7528\uff0c\u4f46\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5ba2\u6237\u7aef\u901a\u5e38\u6309\u56fa\u5b9a\u5468\u671f\u5faa\u73af\u53c2\u4e0e\u8bad\u7ec3\u3002\u8fd9\u79cd\u5468\u671f\u6027\u53c2\u4e0e\u7ed9\u975e\u53ef\u5206\u89e3\u7684AUC\u76ee\u6807\u5e26\u6765\u4e86\u72ec\u7279\u7684\u4f18\u5316\u6311\u6218\uff0c\u9700\u8981\u4e13\u95e8\u7684\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "\u9488\u5bf9\u4e24\u79cd\u8bbe\u7f6e\u5f00\u53d1\u7b97\u6cd5\uff1a1\uff09\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\u7684AUC\u6700\u5927\u5316\uff0c\u5c06\u5176\u91cd\u6784\u4e3a\u975e\u51f8-\u5f3a\u51f9\u6781\u5c0f\u6781\u5927\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528Polyak-\u0141ojasiewicz\u6761\u4ef6\uff1b2\uff09\u4e00\u822c\u6210\u5bf9AUC\u635f\u5931\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u8003\u8651\u4e86\u5468\u671f\u6027\u5ba2\u6237\u7aef\u53c2\u4e0e\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u901a\u4fe1\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5e73\u65b9\u66ff\u4ee3\u635f\u5931\u4e0b\u5b9e\u73b0\u4e86\u901a\u4fe1\u590d\u6742\u5ea6$\\widetilde{O}(1/\u03b5^{1/2})$\u548c\u8fed\u4ee3\u590d\u6742\u5ea6$\\widetilde{O}(1/\u03b5)$\uff1b\u4e00\u822c\u6210\u5bf9\u635f\u5931\u4e0b\u901a\u4fe1\u590d\u6742\u5ea6$O(1/\u03b5^3)$\u548c\u8fed\u4ee3\u590d\u6742\u5ea6$O(1/\u03b5^4)$\uff0c\u5728PL\u6761\u4ef6\u4e0b\u6539\u8fdb\u4e3a$\\widetilde{O}(1/\u03b5^{1/2})$\u548c$\\widetilde{O}(1/\u03b5)$\u3002\u5b9e\u9a8c\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u533b\u5b66\u5f71\u50cf\u548c\u6b3a\u8bc8\u68c0\u6d4b\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u89e3\u51b3\u4e86\u5468\u671f\u6027\u5ba2\u6237\u7aef\u53c2\u4e0e\u4e0b\u7684\u8054\u90a6AUC\u6700\u5927\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u901a\u4fe1\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01608", "abs": "https://arxiv.org/abs/2601.01608", "authors": ["Felix Krause", "Stefan Andreas Baumann", "Johannes Schusterbauer", "Olga Grebenkova", "Ming Gui", "Vincent Tao Hu", "Bj\u00f6rn Ommer"], "title": "Guiding Token-Sparse Diffusion Models", "comment": null, "summary": "Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.", "AI": {"tldr": "\u63d0\u51faSparse Guidance (SG)\u65b9\u6cd5\u89e3\u51b3\u7a00\u758f\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u65f6Classifier-free Guidance\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u901a\u8fc7token\u7ea7\u7a00\u758f\u6027\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4f4e\u8ba1\u7b97\u91cf\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u7a00\u758f\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u867d\u7136\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u5bf9Classifier-free Guidance\u54cd\u5e94\u4e0d\u4f73\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u63a8\u7406\u6027\u80fd\u95ee\u9898\u3002", "method": "\u63d0\u51faSparse Guidance (SG)\u65b9\u6cd5\uff0c\u4f7f\u7528token\u7ea7\u7a00\u758f\u6027\u66ff\u4ee3\u4f20\u7edf\u7684\u6761\u4ef6dropout\u4f5c\u4e3a\u5f15\u5bfc\u4fe1\u53f7\uff0c\u5728\u63a8\u7406\u65f6\u4fdd\u6301\u6761\u4ef6\u9884\u6d4b\u7684\u9ad8\u65b9\u5dee\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "result": "\u5728ImageNet-256\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52301.58 FID\uff0c\u51cf\u5c1125% FLOPs\uff1b\u5728\u5339\u914d\u57fa\u7ebf\u8d28\u91cf\u65f6\u53ef\u8282\u770158% FLOPs\uff1b2.5B\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u6784\u56fe\u548c\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "Sparse Guidance\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u56fe\u50cf\u751f\u6210\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.01613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01613", "abs": "https://arxiv.org/abs/2601.01613", "authors": ["Kazi Ramisa Rifa", "Jie Zhang", "Abdullah Imran"], "title": "CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment", "comment": "18 pages, 9 figures, 5 tables", "summary": "Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.", "AI": {"tldr": "\u63d0\u51fa\u4e86CAP-IQA\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\u5f15\u5bfc\u548c\u56e0\u679c\u53bb\u504f\u6280\u672f\uff0c\u7ed3\u5408\u6587\u672c\u5148\u9a8c\u548c\u5b9e\u4f8b\u7ea7\u4e0a\u4e0b\u6587\u63d0\u793a\uff0c\u7528\u4e8eCT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff0c\u5728LDCTIQA\u6311\u6218\u57fa\u51c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd54.24%\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728CT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u4e14\u901a\u5e38\u5f15\u5165\u7406\u60f3\u5316\u5b9a\u4e49\u5e26\u6765\u7684\u504f\u5dee\uff0c\u65e0\u6cd5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u566a\u58f0\u3001\u8fd0\u52a8\u4f2a\u5f71\u548c\u626b\u63cf\u4eea\u53d8\u5f02\u7b49\u9000\u5316\u60c5\u51b5\u3002", "method": "\u63d0\u51faCAP-IQA\u6846\u67b6\uff0c\u7ed3\u5408CNN\u89c6\u89c9\u7f16\u7801\u5668\u548c\u9886\u57df\u7279\u5b9a\u6587\u672c\u7f16\u7801\u5668\uff0c\u96c6\u6210\u6587\u672c\u7ea7\u5148\u9a8c\u4e0e\u5b9e\u4f8b\u7ea7\u4e0a\u4e0b\u6587\u63d0\u793a\uff0c\u5e94\u7528\u56e0\u679c\u53bb\u504f\u6280\u672f\u5206\u79bb\u7406\u60f3\u5316\u77e5\u8bc6\u4e0e\u5b9e\u9645\u56fe\u50cf\u9000\u5316\u7279\u5f81\uff0c\u4f7f\u7528\u653e\u5c04\u5b66\u98ce\u683c\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u878d\u5408\u5bf9\u9f50\u8bed\u4e49\u548c\u611f\u77e5\u8868\u793a\u3002", "result": "\u57282023\u5e74LDCTIQA\u6311\u6218\u57fa\u51c6\u4e0a\u83b7\u5f972.8590\u7684\u603b\u76f8\u5173\u5206\u6570\uff08PLCC\u3001SROCC\u548cKROCC\u4e4b\u548c\uff09\uff0c\u8d85\u8d8a\u6392\u884c\u699c\u6700\u4f73\u56e2\u961f4.24%\uff1b\u5728\u5305\u542b91,514\u5f20\u513f\u79d1CT\u56fe\u50cf\u7684\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CAP-IQA\u6846\u67b6\u901a\u8fc7\u63d0\u793a\u5f15\u5bfc\u878d\u5408\u548c\u7b80\u5316\u7f16\u7801\u5668\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u7279\u5f81\u5bf9\u9f50\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728CT\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.01663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01663", "abs": "https://arxiv.org/abs/2601.01663", "authors": ["He Sun", "Jiwoong Shin", "Ravi Dhar"], "title": "Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths", "comment": null, "summary": "We study generative modeling of \\emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \\emph{distribution matching} for trajectory-derived statistics. We propose \\textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.", "AI": {"tldr": "\u63d0\u51fa\u957f\u5ea6\u611f\u77e5\u91c7\u6837\uff08LAS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u8f68\u8ff9\u957f\u5ea6\u5206\u7ec4\u91c7\u6837\u6765\u51cf\u5c11\u6279\u6b21\u5185\u957f\u5ea6\u5f02\u8d28\u6027\uff0c\u6539\u5584\u8f68\u8ff9\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5206\u5e03\u5339\u914d\u95ee\u9898", "motivation": "\u5728\u53ef\u53d8\u957f\u5ea6\u8f68\u8ff9\u7684\u751f\u6210\u5efa\u6a21\u4e2d\uff0c\u6807\u51c6\u5c0f\u6279\u91cf\u8bad\u7ec3\u5728\u8f68\u8ff9\u957f\u5ea6\u9ad8\u5ea6\u5f02\u8d28\u65f6\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u4f1a\u964d\u4f4e\u8f68\u8ff9\u6d3e\u751f\u7edf\u8ba1\u91cf\u7684\u5206\u5e03\u5339\u914d\u8d28\u91cf", "method": "\u63d0\u51fa\u957f\u5ea6\u611f\u77e5\u91c7\u6837\uff08LAS\uff09\u7b56\u7565\uff1a\u6309\u8f68\u8ff9\u957f\u5ea6\u5206\u7ec4\uff0c\u4ece\u5355\u4e00\u957f\u5ea6\u6876\u4e2d\u91c7\u6837\u6279\u6b21\uff0c\u51cf\u5c11\u6279\u6b21\u5185\u957f\u5ea6\u5f02\u8d28\u6027\uff1b\u7ed3\u5408\u6761\u4ef6\u8f68\u8ff9GAN\u548c\u8f85\u52a9\u65f6\u95f4\u5bf9\u9f50\u635f\u5931", "result": "LAS\u5728\u8d2d\u7269\u8005\u8f68\u8ff9\u6570\u636e\u96c6\u548c\u591a\u79cd\u516c\u5171\u5e8f\u5217\u6570\u636e\u96c6\uff08GPS\u3001\u6559\u80b2\u3001\u7535\u5b50\u5546\u52a1\u3001\u7535\u5f71\uff09\u4e0a\u4e00\u81f4\u6539\u5584\u6d3e\u751f\u53d8\u91cf\u5206\u5e03\u7684\u5339\u914d\uff0c\u4f18\u4e8e\u968f\u673a\u91c7\u6837", "conclusion": "LAS\u901a\u8fc7\u79fb\u9664\u4ec5\u4f9d\u8d56\u957f\u5ea6\u7684\u6377\u5f84\u6279\u8bc4\u5668\u5e76\u9488\u5bf9\u6876\u5185\u5dee\u5f02\uff0c\u6539\u5584\u4e86\u5206\u5e03\u5339\u914d\uff0c\u4e3a\u8f68\u8ff9\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565"}}
{"id": "2601.01639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01639", "abs": "https://arxiv.org/abs/2601.01639", "authors": ["Gaurav Sekar"], "title": "An Empirical Study of Monocular Human Body Measurement Under Weak Calibration", "comment": "The paper consists of 8 pages, 2 figures (on pages 4 and 7), and 2 tables (both on page 6)", "summary": "Estimating human body measurements from monocular RGB imagery remains challenging due to scale ambiguity, viewpoint sensitivity, and the absence of explicit depth information. This work presents a systematic empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, evaluated under semi-constrained conditions using consumer-grade cameras. Rather than pursuing state-of-the-art accuracy, the study analyzes how differing calibration assumptions influence measurement behavior, robustness, and failure modes across varied body types. The results reveal a clear trade-off between user effort during calibration and the stability of resulting circumferential quantities. This paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e09\u79cd\u5f31\u6807\u5b9a\u5355\u76eeRGB\u56fe\u50cf\u4eba\u4f53\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u6807\u5b9a\u5047\u8bbe\u5bf9\u6d4b\u91cf\u884c\u4e3a\u3001\u9c81\u68d2\u6027\u548c\u5931\u8d25\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u4e3a\u6d88\u8d39\u8bbe\u5907\u4e0a\u7684\u8f7b\u91cf\u7ea7\u4eba\u4f53\u6d4b\u91cf\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u8bc1\u8bbe\u8ba1\u53c2\u8003\u3002", "motivation": "\u4ece\u5355\u76eeRGB\u56fe\u50cf\u4f30\u8ba1\u4eba\u4f53\u5c3a\u5bf8\u9762\u4e34\u5c3a\u5ea6\u6a21\u7cca\u3001\u89c6\u89d2\u654f\u611f\u548c\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\u7b49\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u540c\u5f31\u6807\u5b9a\u7b56\u7565\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u548c\u6743\u8861\u3002", "method": "\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76\u4e09\u79cd\u5f31\u6807\u5b9a\u5355\u76ee\u7b56\u7565\uff1a\u57fa\u4e8e\u5730\u6807\u7684\u51e0\u4f55\u65b9\u6cd5\u3001\u59ff\u6001\u9a71\u52a8\u7684\u56de\u5f52\u65b9\u6cd5\u3001\u7269\u4f53\u6807\u5b9a\u7684\u8f6e\u5ed3\u65b9\u6cd5\uff0c\u5728\u6d88\u8d39\u7ea7\u76f8\u673a\u534a\u7ea6\u675f\u6761\u4ef6\u4e0b\u8bc4\u4f30\uff0c\u91cd\u70b9\u5206\u6790\u4e0d\u540c\u6807\u5b9a\u5047\u8bbe\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6807\u5b9a\u8fc7\u7a0b\u4e2d\u7684\u7528\u6237\u52aa\u529b\u7a0b\u5ea6\u4e0e\u6240\u5f97\u5468\u957f\u6d4b\u91cf\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u6743\u8861\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u4f53\u578b\u4e0a\u7684\u6d4b\u91cf\u884c\u4e3a\u3001\u9c81\u68d2\u6027\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6d88\u8d39\u8bbe\u5907\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7\u5355\u76ee\u4eba\u4f53\u6d4b\u91cf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bbe\u8ba1\u53c2\u8003\uff0c\u5f3a\u8c03\u7406\u89e3\u4e0d\u540c\u6807\u5b9a\u5047\u8bbe\u5bf9\u6d4b\u91cf\u6027\u80fd\u7684\u5f71\u54cd\u6bd4\u8ffd\u6c42\u6700\u9ad8\u7cbe\u5ea6\u66f4\u4e3a\u91cd\u8981\u3002"}}
{"id": "2601.01664", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01664", "abs": "https://arxiv.org/abs/2601.01664", "authors": ["Amichai Painsky"], "title": "Who is the Winning Algorithm? Rank Aggregation for Comparative Studies", "comment": null, "summary": "Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u6846\u67b6\uff0c\u5229\u7528\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u6392\u540d\u4fe1\u606f\uff08\u4e0d\u4ec5\u662f\u83b7\u80dc\u6b21\u6570\uff0c\u8fd8\u5305\u62ec\u7b2c\u4e8c\u3001\u7b2c\u4e09\u7b49\u6392\u540d\uff09\u6765\u4f30\u8ba1\u6bcf\u4e2a\u7b97\u6cd5\u5728\u672a\u6765\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u83b7\u80dc\u7684\u6982\u7387\u3002", "motivation": "\u4f20\u7edf\u6700\u5927\u4f3c\u7136\u65b9\u6cd5\u4ec5\u7edf\u8ba1\u6bcf\u4e2a\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u83b7\u80dc\u6b21\u6570\uff0c\u4f46\u5ffd\u7565\u4e86\u5b8c\u6574\u7684\u6392\u540d\u4fe1\u606f\uff08\u5982\u7b2c\u4e8c\u3001\u7b2c\u4e09\u540d\u7b49\uff09\u3002\u8fd9\u4e9b\u989d\u5916\u4fe1\u606f\u53ef\u80fd\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7b97\u6cd5\u5728\u672a\u6765\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u65b0\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5229\u7528m\u4e2a\u7ade\u4e89\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u6392\u540d\u4fe1\u606f\uff08\u5305\u62ec\u6bcf\u4e2a\u7b97\u6cd5\u83b7\u5f97\u7b2c\u4e00\u3001\u7b2c\u4e8c\u3001\u7b2c\u4e09\u7b49\u540d\u6b21\u7684\u6b21\u6570\uff09\uff0c\u6765\u4f30\u8ba1\u6bcf\u4e2a\u7b97\u6cd5\u5728\u672a\u6765\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u83b7\u80dc\u7684\u6982\u7387\u3002", "result": "\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u4e2d\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u5df2\u77e5\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7b97\u6cd5\u5728\u672a\u6765\u6570\u636e\u96c6\u4e0a\u7684\u83b7\u80dc\u6982\u7387\u3002", "conclusion": "\u5229\u7528\u7b97\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b8c\u6574\u6392\u540d\u4fe1\u606f\uff08\u800c\u4e0d\u4ec5\u4ec5\u662f\u83b7\u80dc\u6b21\u6570\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u7b97\u6cd5\u5728\u672a\u6765\u6570\u636e\u96c6\u4e0a\u83b7\u80dc\u6982\u7387\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2601.01665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01665", "abs": "https://arxiv.org/abs/2601.01665", "authors": ["Wei Liu", "Yaoxin Wu", "Yingqian Zhang", "Thomas B\u00e4ck", "Yingjie Fan"], "title": "Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives", "comment": null, "summary": "Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9762\u5411\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u7edf\u4e00\u9c81\u68d2\u6027\u6846\u67b6\uff0c\u5305\u62ec\u504f\u597d\u5bf9\u6297\u653b\u51fb\u751f\u6210\u56f0\u96be\u5b9e\u4f8b\uff0c\u4ee5\u53ca\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u795e\u7ecf\u6c42\u89e3\u5668\u9c81\u68d2\u6027\u7684\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u5728\u591a\u6837\u590d\u6742\u95ee\u9898\u5206\u5e03\u4e0b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u63d0\u5347\u5b66\u4e60\u6c42\u89e3\u5668\u7684\u7a33\u5065\u6027\u3002", "method": "1) \u63d0\u51fa\u7edf\u4e00\u9c81\u68d2\u6027\u6846\u67b6\uff1b2) \u5f00\u53d1\u57fa\u4e8e\u504f\u597d\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u751f\u6210\u56f0\u96be\u5b9e\u4f8b\uff1b3) \u901a\u8fc7\u5e15\u7d2f\u6258\u524d\u6cbf\u8d28\u91cf\u9000\u5316\u91cf\u5316\u653b\u51fb\u5f71\u54cd\uff1b4) \u5f15\u5165\u9632\u5fa1\u7b56\u7565\uff0c\u5c06\u96be\u5ea6\u611f\u77e5\u504f\u597d\u9009\u62e9\u96c6\u6210\u5230\u5bf9\u6297\u8bad\u7ec3\u4e2d\uff0c\u51cf\u5c11\u5bf9\u53d7\u9650\u504f\u597d\u533a\u57df\u7684\u8fc7\u62df\u5408\u3002", "result": "\u5728\u591a\u76ee\u6807\u65c5\u884c\u5546\u95ee\u9898\u3001\u591a\u76ee\u6807\u5bb9\u91cf\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u548c\u591a\u76ee\u6807\u80cc\u5305\u95ee\u9898\u4e0a\u9a8c\u8bc1\uff1a\u653b\u51fb\u65b9\u6cd5\u6210\u529f\u4e3a\u4e0d\u540c\u6c42\u89e3\u5668\u751f\u6210\u56f0\u96be\u5b9e\u4f8b\uff1b\u9632\u5fa1\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u795e\u7ecf\u6c42\u89e3\u5668\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u56f0\u96be\u6216\u5206\u5e03\u5916\u5b9e\u4f8b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u9c81\u68d2\u6027\u6846\u67b6\u6709\u6548\u8bc4\u4f30\u548c\u63d0\u5347\u4e86\u591a\u76ee\u6807\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6c42\u89e3\u5668\u7684\u7a33\u5065\u6027\uff0c\u653b\u51fb\u65b9\u6cd5\u80fd\u66b4\u9732\u6c42\u89e3\u5668\u5f31\u70b9\uff0c\u9632\u5fa1\u7b56\u7565\u80fd\u589e\u5f3a\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.01676", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01676", "abs": "https://arxiv.org/abs/2601.01676", "authors": ["Jin Yao", "Radowan Mahmud Redoy", "Sebastian Elbaum", "Matthew B. Dwyer", "Zezhou Cheng"], "title": "LabelAny3D: Label Any Object 3D in the Wild", "comment": "NeurIPS 2025. Project page: https://uva-computer-vision-lab.github.io/LabelAny3D/", "summary": "Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \\emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.", "AI": {"tldr": "LabelAny3D\u662f\u4e00\u4e2a\u901a\u8fc7\u5206\u6790\u5408\u6210\u6846\u67b6\u4ece2D\u56fe\u50cf\u91cd\u5efa\u5b8c\u65743D\u573a\u666f\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf3D\u8fb9\u754c\u6846\u6807\u6ce8\u7684\u7cfb\u7edf\uff0c\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86COCO3D\u57fa\u51c6\uff0c\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u5355\u76ee3D\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u76ee3D\u68c0\u6d4b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f3D\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c3D\u6807\u6ce8\u7684\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf3D\u6807\u6ce8\u7684\u65b9\u6cd5\u6765\u6269\u5c553D\u8bc6\u522b\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86LabelAny3D\u5206\u6790\u5408\u6210\u6846\u67b6\uff0c\u4ece2D\u56fe\u50cf\u91cd\u5efa\u5b8c\u65743D\u573a\u666f\uff0c\u4ee5\u6b64\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf3D\u8fb9\u754c\u6846\u6807\u6ce8\u3002\u57fa\u4e8e\u6b64\u7ba1\u9053\u6784\u5efa\u4e86COCO3D\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u6e90\u81eaMS-COCO\u6570\u636e\u96c6\uff0c\u8986\u76d6\u4e86\u73b0\u67093D\u6570\u636e\u96c6\u4e2d\u7f3a\u5931\u7684\u5e7f\u6cdb\u5bf9\u8c61\u7c7b\u522b\u3002", "result": "LabelAny3D\u751f\u6210\u7684\u6807\u6ce8\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u5355\u76ee3D\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u8d28\u91cf\u4e0a\u4f18\u4e8e\u5148\u524d\u7684\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u3002COCO3D\u57fa\u51c6\u4e3a\u5f00\u653e\u8bcd\u6c47\u5355\u76ee3D\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u57fa\u7840\u6a21\u578b\u9a71\u52a8\u7684\u6807\u6ce8\u65b9\u6cd5\u5728\u6269\u5c55\u771f\u5b9e\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u76843D\u8bc6\u522b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u89e3\u51b33D\u6807\u6ce8\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01678", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01678", "abs": "https://arxiv.org/abs/2601.01678", "authors": ["Siba Smarak Panigrahi", "Jovana Videnovi\u0107", "Maria Brbi\u0107"], "title": "HeurekaBench: A Benchmarking Framework for AI Co-scientist", "comment": "33 pages, 5 figures, 7 tables. Code available at https://github.com/mlbio-epfl/HeurekaBench", "summary": "LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.", "AI": {"tldr": "HeurekaBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u667a\u80fd\u4f53\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u534a\u81ea\u52a8\u6d41\u7a0b\u521b\u5efa\u57fa\u4e8e\u771f\u5b9e\u79d1\u5b66\u7814\u7a76\u7684\u5f00\u653e\u5f0f\u7814\u7a76\u95ee\u9898\uff0c\u5e76\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u5b9e\u4f8b\u5316\u4e3asc-HeurekaBench\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u4f18\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u79d1\u5b66\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u73b0\u5b9e\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u8981\u80fd\u591f\u6574\u5408\u6570\u636e\u5206\u6790\u3001\u89e3\u91ca\u548c\u4ece\u5b9e\u9a8c\u6570\u636e\u751f\u6210\u65b0\u89c1\u89e3\u7684\u771f\u5b9e\u7814\u7a76\u573a\u666f\u6765\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86HeurekaBench\u6846\u67b6\uff0c\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\uff1a\u57fa\u4e8e\u771f\u5b9e\u79d1\u5b66\u7814\u7a76\u53ca\u5176\u4ee3\u7801\u4ed3\u5e93\u521b\u5efa\u5f00\u653e\u5f0f\u7814\u7a76\u95ee\u9898\uff0c\u5229\u7528\u591a\u4e2aLLM\u63d0\u53d6\u89c1\u89e3\u5e76\u751f\u6210\u5019\u9009\u5de5\u4f5c\u6d41\uff0c\u7136\u540e\u4e0e\u62a5\u544a\u7ed3\u679c\u8fdb\u884c\u9a8c\u8bc1\u3002\u5728\u5355\u7ec6\u80de\u751f\u7269\u5b66\u9886\u57df\u5b9e\u4f8b\u5316\u4e3asc-HeurekaBench\u57fa\u51c6\u3002", "result": "\u4f7f\u7528sc-HeurekaBench\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u5355\u7ec6\u80de\u667a\u80fd\u4f53\uff0c\u53d1\u73b0\u6dfb\u52a0\u6279\u8bc4\u6a21\u5757\u53ef\u4ee5\u5c06\u5f00\u6e90LLM\u667a\u80fd\u4f53\u7684\u4e0d\u826f\u54cd\u5e94\u6539\u5584\u9ad8\u8fbe22%\uff0c\u7f29\u5c0f\u4e0e\u95ed\u6e90\u667a\u80fd\u4f53\u7684\u5dee\u8ddd\u3002\u5c55\u793a\u4e86\u57fa\u51c6\u5728\u5b9a\u91cf\u5206\u6790\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u9009\u62e9\u65b9\u9762\u7684\u4ef7\u503c\u3002", "conclusion": "HeurekaBench\u4e3a\u79d1\u5b66\u667a\u80fd\u4f53\u7684\u4e25\u683c\u7aef\u5230\u7aef\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c06\u57fa\u51c6\u6784\u5efa\u624e\u6839\u4e8e\u771f\u5b9e\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u667a\u80fd\u4f53\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.01677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01677", "abs": "https://arxiv.org/abs/2601.01677", "authors": ["Zhengsen Xu", "Lanying Wang", "Sibo Cheng", "Xue Rui", "Kyle Gao", "Yimin Zhu", "Mabel Heffring", "Zack Dewis", "Saeid Taleghanidoozdoozan", "Megan Greenwood", "Motasem Alkayid", "Quinn Ledingham", "Hongjie He", "Jonathan Li", "Lincoln Linlin Xu"], "title": "Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada", "comment": null, "summary": "In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u957f\u5e8f\u5217\u591a\u5c3a\u5ea6\u65f6\u5e8f\u5efa\u6a21\u7684\u53ef\u4fe1\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u6846\u67b6\uff0c\u5728\u52a0\u62ff\u5927\u897f\u90e82023-2024\u5e74\u91ce\u706b\u5b63\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u65700.90\uff0cPR-AUC 0.98\uff0c\u540c\u65f6\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u4f9b\u8fc7\u7a0b\u7ea7\u89e3\u91ca\u3002", "motivation": "\u52a0\u62ff\u5927\u897f\u90e8\u91ce\u706b\u6d3b\u52a8\u52a0\u5267\u5bfc\u81f4\u91cd\u5927\u793e\u4f1a\u7ecf\u6d4e\u548c\u73af\u5883\u635f\u5931\uff0c\u4f46\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u9762\u4e34\u70b9\u706b\u548c\u8513\u5ef6\u7684\u968f\u673a\u6027\u3001\u71c3\u6599\u6761\u4ef6\u3001\u6c14\u8c61\u3001\u6c14\u5019\u53d8\u7387\u3001\u5730\u5f62\u548c\u4eba\u7c7b\u6d3b\u52a8\u7b49\u591a\u56e0\u7d20\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u7684\u6311\u6218\uff0c\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u53ef\u4fe1\u6570\u636e\u9a71\u52a8\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u6846\u67b6\uff0c\u57fa\u4e8e\u957f\u5e8f\u5217\u591a\u5c3a\u5ea6\u65f6\u5e8f\u5efa\u6a21\uff0c\u6574\u5408\u5f02\u8d28\u9a71\u52a8\u56e0\u7d20\uff0c\u540c\u65f6\u663e\u5f0f\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5e76\u652f\u6301\u8fc7\u7a0b\u7ea7\u89e3\u91ca\u3002\u4f7f\u7528SHAP\u65b9\u6cd5\u8fdb\u884c\u673a\u5236\u89e3\u91ca\u3002", "result": "\u5728\u52a0\u62ff\u5927\u897f\u90e82023\u548c2024\u5e74\u521b\u7eaa\u5f55\u91ce\u706b\u5b63\u8bc4\u4f30\u4e2d\uff0c\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\uff0cF1\u5206\u65700.90\uff0cPR-AUC 0.98\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u3002\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u663e\u793a\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u7ed3\u6784\u5316\u7a7a\u95f4\u548c\u5b63\u8282\u6a21\u5f0f\uff0cSHAP\u89e3\u91ca\u63ed\u793a\u6e29\u5ea6\u76f8\u5173\u9a71\u52a8\u56e0\u7d20\u5728\u4e24\u5e74\u5747\u4e3b\u5bfc\u91ce\u706b\u98ce\u9669\uff0c\u800c2024\u5e74\u6e7f\u5ea6\u76f8\u5173\u7ea6\u675f\u5728\u5851\u9020\u7a7a\u95f4\u548c\u571f\u5730\u8986\u76d6\u7279\u5b9a\u5bf9\u6bd4\u65b9\u9762\u4f5c\u7528\u66f4\u5f3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u4fe1\u91ce\u706b\u98ce\u9669\u9884\u6d4b\u6846\u67b6\u5728\u51c6\u786e\u9884\u6d4b\u91ce\u706b\u98ce\u9669\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u673a\u5236\u89e3\u91ca\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u91ce\u706b\u63a7\u5236\u56e0\u7d20\uff0c\u4e3a\u91ce\u706b\u7ba1\u7406\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u4f9d\u636e\u3002"}}
{"id": "2601.01688", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01688", "abs": "https://arxiv.org/abs/2601.01688", "authors": ["Yash Thesia", "Meera Suthar"], "title": "DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors", "comment": "8 pages, 3 figures, 4 tables", "summary": "Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the \"Cold Start\" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique \"optimization trajectory\" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.", "AI": {"tldr": "DiMEx\u5229\u7528\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u901a\u8fc7\u968f\u673a\u5d4c\u5165\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5408\u6210\u9ad8\u8d28\u91cf\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u65e0\u6570\u636e\u6a21\u578b\u7a83\u53d6\u6548\u7387\uff1b\u540c\u65f6\u63d0\u51fa\u6df7\u5408\u72b6\u6001\u96c6\u6210\u9632\u5fa1\u6765\u68c0\u6d4b\u6b64\u7c7b\u653b\u51fb\u7684\u4f18\u5316\u8f68\u8ff9\u7279\u5f81\u3002", "motivation": "\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1\u4e2d\u7684\u6a21\u578b\u7a83\u53d6\u653b\u51fb\u5a01\u80c1\uff0c\u7279\u522b\u662f\u65e0\u6570\u636e\u6a21\u578b\u63d0\u53d6\u9762\u4e34\u7684\"\u51b7\u542f\u52a8\"\u95ee\u9898\u2014\u2014\u57fa\u4e8eGAN\u7684\u653b\u51fb\u8005\u9700\u8981\u5927\u91cf\u67e5\u8be2\u4ece\u968f\u673a\u566a\u58f0\u6536\u655b\u5230\u6709\u610f\u4e49\u6570\u636e\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faDiMEx\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u4e30\u5bcc\u8bed\u4e49\u5148\u9a8c\uff0c\u901a\u8fc7\u968f\u673a\u5d4c\u5165\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u751f\u6210\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u76f4\u63a5\u5408\u6210\u9ad8\u4fdd\u771f\u67e5\u8be2\uff0c\u7ed5\u8fc7\u521d\u59cb\u5316\u969c\u788d\u3002", "result": "\u5728SVHN\u6570\u636e\u96c6\u4e0a\u4ec5\u75282000\u6b21\u67e5\u8be2\u5c31\u8fbe\u523052.1%\u7684\u534f\u8bae\u7387\uff0c\u6bd4\u6700\u5148\u8fdb\u7684GAN\u57fa\u7ebf\u9ad8\u51fa16%\u4ee5\u4e0a\uff1b\u540c\u65f6\u63d0\u51fa\u7684\u6df7\u5408\u72b6\u6001\u96c6\u6210\u9632\u5fa1\u80fd\u5c06\u653b\u51fb\u6210\u529f\u7387\u6291\u5236\u523021.6%\u3002", "conclusion": "DiMEx\u5c55\u793a\u4e86\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8bed\u4e49\u5148\u9a8c\u8fdb\u884c\u9ad8\u6548\u6a21\u578b\u7a83\u53d6\u7684\u53ef\u884c\u6027\uff0c\u800c\u6df7\u5408\u72b6\u6001\u96c6\u6210\u9632\u5fa1\u901a\u8fc7\u68c0\u6d4b\u6f5c\u5728\u7a7a\u95f4\u653b\u51fb\u7684\u72ec\u7279\u4f18\u5316\u8f68\u8ff9\u7279\u5f81\uff0c\u80fd\u6709\u6548\u5bf9\u6297\u8fd9\u79cd\u9ad8\u8bed\u4e49\u5a01\u80c1\u3002"}}
{"id": "2601.01692", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01692", "abs": "https://arxiv.org/abs/2601.01692", "authors": ["Erfan Hajihashemi", "Yanning Shen"], "title": "Enhanced Multi-model Online Conformal Prediction", "comment": null, "summary": "Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u578b\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u4e8c\u5206\u56fe\u9009\u62e9\u6709\u6548\u6a21\u578b\u5b50\u96c6\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u9884\u6d4b\u6548\u7387", "motivation": "\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u4f9d\u8d56\u5355\u4e00\u56fa\u5b9a\u6a21\u578b\uff0c\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u53ef\u80fd\u8868\u73b0\u4e0d\u7a33\u5b9a\uff1b\u73b0\u6709\u591a\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u53d7\u6027\u80fd\u8f83\u5dee\u6a21\u578b\u5f71\u54cd", "method": "\u5f00\u53d1\u4e86\u591a\u6a21\u578b\u5728\u7ebf\u5171\u5f62\u9884\u6d4b\u7b97\u6cd5\uff0c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u751f\u6210\u4e8c\u5206\u56fe\u6765\u8bc6\u522b\u6709\u6548\u6a21\u578b\u5b50\u96c6\uff0c\u4ece\u4e2d\u9009\u62e9\u6a21\u578b\u6784\u5efa\u9884\u6d4b\u96c6", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u96c6\u5927\u5c0f\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u6a21\u578b\u5171\u5f62\u9884\u6d4b\u6280\u672f", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u578b\u5171\u5f62\u9884\u6d4b\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u9884\u6d4b\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5728\u7ebf\u73af\u5883\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848"}}
{"id": "2601.01687", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01687", "abs": "https://arxiv.org/abs/2601.01687", "authors": ["Abdur R. Fayjie", "Pankhi Kashyap", "Jutika Borah", "Patrick Vandewalle"], "title": "FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation", "comment": "20 pages, 6 figures, 7 tables", "summary": "Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.", "AI": {"tldr": "FALCON\u662f\u4e00\u4e2a\u8de8\u57df\u5c11\u6837\u672c\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5c063D\u533b\u5b66\u4f53\u79ef\u6570\u636e\u4f5c\u4e3a2D\u5207\u7247\u5904\u7406\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u5272\uff0c\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u6570\u636e\u9700\u6c42\u3001\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u8fb9\u754c\u7cbe\u5ea6", "motivation": "3D\u533b\u5b66\u4f53\u79ef\u4e2d\u89e3\u5256\u548c\u75c5\u7406\u7ed3\u6784\u7684\u7cbe\u786e\u5206\u5272\u5bf9\u8bca\u65ad\u3001\u624b\u672f\u89c4\u5212\u548c\u75be\u75c5\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709AI\u65b9\u6cd5\u9762\u4e343D\u6807\u6ce8\u7a00\u7f3a\u3001\u60a3\u8005\u7279\u5f02\u6027\u53d8\u5f02\u3001\u6570\u636e\u9690\u79c1\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7b49\u6311\u6218", "method": "\u63d0\u51faFALCON\u6846\u67b6\uff1a\u5148\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8fdb\u884c\u5143\u8bad\u7ec3\u5b66\u4e60\u901a\u7528\u5206\u5272\u5148\u9a8c\uff0c\u7136\u540e\u901a\u8fc7\u5bf9\u6297\u6027\u5fae\u8c03\u548c\u8fb9\u754c\u611f\u77e5\u5b66\u4e60\u8fc1\u79fb\u5230\u533b\u5b66\u9886\u57df\uff0c\u4efb\u52a1\u611f\u77e5\u63a8\u7406\u6839\u636e\u652f\u6301\u7ebf\u7d22\u52a8\u6001\u9002\u5e94\u60a3\u8005\u7279\u5f02\u6027\u53d8\u5f02", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFALCON\u59cb\u7ec8\u83b7\u5f97\u6700\u4f4e\u7684Hausdorff\u8ddd\u79bb\u5206\u6570\uff08\u8868\u660e\u8fb9\u754c\u7cbe\u5ea6\u4f18\u8d8a\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u7684Dice\u76f8\u4f3c\u7cfb\u6570\uff0c\u4e14\u4f7f\u7528\u66f4\u5c11\u6807\u6ce8\u6570\u636e\u3001\u65e0\u6570\u636e\u589e\u5f3a\u3001\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e", "conclusion": "FALCON\u901a\u8fc7\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u548c2D\u5207\u7247\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u533b\u5b66\u5206\u5272\u4e2d\u7684\u6807\u6ce8\u7a00\u7f3a\u3001\u8ba1\u7b97\u5f00\u9500\u548c\u60a3\u8005\u53d8\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e34\u5e8a\u53ef\u884c\u7684\u7cbe\u786e\u5206\u5272"}}
{"id": "2601.01689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01689", "abs": "https://arxiv.org/abs/2601.01689", "authors": ["Afzal Hossain", "Stephanie Schuckers"], "title": "Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data", "comment": null, "summary": "Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5408\u6210\u4eba\u8138\u6570\u636e\u80fd\u5426\u4f5c\u4e3a\u7eb5\u5411\u7a33\u5b9a\u5668\uff0c\u6539\u5584\u513f\u7ae5\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u65f6\u95f4\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u5fae\u8c03\u80fd\u663e\u8457\u964d\u4f4e\u9519\u8bef\u7387\u3002", "motivation": "\u513f\u7ae5\u9762\u90e8\u8bc6\u522b\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u9762\u90e8\u5feb\u901f\u975e\u7ebf\u6027\u751f\u957f\u5bfc\u81f4\u6a21\u677f\u6f02\u79fb\u548c\u968f\u65f6\u95f4\u589e\u52a0\u7684\u9a8c\u8bc1\u9519\u8bef\u3002\u9700\u8981\u5bfb\u627e\u65b9\u6cd5\u63d0\u9ad8\u513f\u7ae5\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u65f6\u95f4\u9c81\u68d2\u6027\u3002", "method": "\u5728YFA\u6570\u636e\u96c6\u4e0a\u91c7\u7528\u8eab\u4efd\u5206\u79bb\u534f\u8bae\uff0c\u8bc4\u4f30\u4e09\u79cd\u8bbe\u7f6e\uff1a1) \u9884\u8bad\u7ec3MagFace\u5d4c\u5165\uff1b2) \u4ec5\u4f7f\u7528\u771f\u5b9e\u8bad\u7ec3\u4eba\u8138\u5fae\u8c03MagFace\uff1b3) \u4f7f\u7528\u771f\u5b9e\u548c\u5408\u6210\u751f\u6210\u8bad\u7ec3\u4eba\u8138\u7ec4\u5408\u5fae\u8c03MagFace\u3002\u4f7f\u7528StyleGAN2 ADA\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u5e94\u7528\u540e\u751f\u6210\u8fc7\u6ee4\u6b65\u9aa4\u51cf\u8f7b\u8eab\u4efd\u6cc4\u6f0f\u548c\u53bb\u9664\u4f2a\u5f71\u6837\u672c\u3002", "result": "\u57286\u523036\u4e2a\u6708\u7684\u6ce8\u518c\u9a8c\u8bc1\u95f4\u9694\u5b9e\u9a8c\u4e2d\uff0c\u5408\u6210\u589e\u5f3a\u7684\u5fae\u8c03\u76f8\u5bf9\u4e8e\u9884\u8bad\u7ec3\u57fa\u7ebf\u548c\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u7684\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9519\u8bef\u7387\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u589e\u5f3a\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u513f\u7ae5\u4eba\u8138\u8bc6\u522b\u4e2d\u7684\u8eab\u4efd\u6301\u4e45\u6027\uff0c\u4e3a\u6539\u5584\u513f\u79d1\u4eba\u8138\u8bc6\u522b\u63d0\u4f9b\u4e86\u98ce\u9669\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2601.01714", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01714", "abs": "https://arxiv.org/abs/2601.01714", "authors": ["Kareem Ahmed", "Sameer Singh"], "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning", "comment": null, "summary": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.", "AI": {"tldr": "EPIC\u662f\u4e00\u79cd\u8d85\u53c2\u6570\u81ea\u7531\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u672a\u6765\u8f68\u8ff9\u7684\u71b5\u7eb3\u5165\u8bed\u8a00\u6a21\u578b\u89e3\u7801\uff0c\u5728\u6bcf\u4e00\u6b65\u751f\u6210\u65f6\u663e\u5f0f\u8c03\u8282\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u91c7\u6837\u5206\u5e03\u7684\u71b5\u4e0e\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u7b97\u6cd5\u4f9d\u8d56\u8d2a\u5a6a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5bfc\u81f4\u77ed\u89c6\u5931\u771f\uff0c\u4ea7\u751f\u540c\u8d28\u5316\u3001\u91cd\u590d\u4e14\u4e0d\u8fde\u8d2f\u7684\u53e5\u5b50\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u66f4\u597d\u5730\u5bf9\u9f50\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u7684\u89e3\u7801\u65b9\u6cd5\u3002", "method": "EPIC\u901a\u8fc7\u71b5\u611f\u77e5\u60f0\u6027Gumbel-Max\u91c7\u6837\uff0c\u5c06\u672a\u6765\u8f68\u8ff9\u7684\u71b5\u7eb3\u5165\u89e3\u7801\u8fc7\u7a0b\uff0c\u663e\u5f0f\u8c03\u8282\u6bcf\u4e00\u6b65\u751f\u6210\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u91c7\u6837\u5206\u5e03\u7684\u71b5\u4e0e\u6570\u636e\uff08\u5076\u7136\uff09\u4e0d\u786e\u5b9a\u6027\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u7cbe\u786e\u4e14\u9ad8\u6548\uff0c\u6bcf\u6b65\u4ec5\u9700\u4e9a\u7ebf\u6027\u6b21\u6570\u7684\u71b5\u8bc4\u4f30\u3002", "result": "\u5728\u521b\u610f\u5199\u4f5c\u548c\u6458\u8981\u4efb\u52a1\u4e2d\uff0cEPIC\u5728LM-as-judge\u504f\u597d\u80dc\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u89e3\u7801\u7b56\u7565\u3002\u81ea\u52a8\u6307\u6807\u663e\u793aEPIC\u4ea7\u751f\u66f4\u591a\u6837\u5316\u7684\u751f\u6210\u548c\u66f4\u5fe0\u5b9e\u7684\u6458\u8981\u3002\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cEPIC\u4e5f\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "EPIC\u901a\u8fc7\u5c06\u71b5\u610f\u8bc6\u7eb3\u5165\u89e3\u7801\u8fc7\u7a0b\uff0c\u80fd\u591f\u4ea7\u751f\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u591a\u6837\u5316\u4e14\u66f4\u5fe0\u5b9e\u7684\u8bed\u8a00\u751f\u6210\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u89e3\u7801\u7b56\u7565\u3002"}}
{"id": "2601.01695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01695", "abs": "https://arxiv.org/abs/2601.01695", "authors": ["Ruiyu Mao", "Baoming Zhang", "Nicholas Ruozzi", "Yunhui Guo"], "title": "Learnability-Driven Submodular Optimization for Active Roadside 3D Detection", "comment": "10 pages, 7 figures. Submitted to CVPR 2026", "summary": "Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.", "AI": {"tldr": "\u63d0\u51faLH3D\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u65e2\u4fe1\u606f\u4e30\u5bcc\u53c8\u53ef\u53ef\u9760\u6807\u6ce8\u7684\u8def\u8fb9\u5355\u76ee3D\u68c0\u6d4b\u573a\u666f\uff0c\u6291\u5236\u56fa\u6709\u6a21\u7cca\u6837\u672c\uff0c\u5728\u4ec5\u4f7f\u752825%\u6807\u6ce8\u9884\u7b97\u4e0b\u8fbe\u5230\u63a5\u8fd1\u5168\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5b9e\u9645\u90e8\u7f72\u4e2d\u901a\u5e38\u53ea\u80fd\u6807\u6ce8\u7eaf\u8def\u8fb9\u6570\u636e\uff08\u65e0\u8f66\u8f86\u7aef\u6570\u636e\uff09\uff0c\u4f46\u8bb8\u591a\u8def\u8fb9\u573a\u666f\u5b58\u5728\u8ddd\u79bb\u8fdc\u3001\u6a21\u7cca\u6216\u88ab\u906e\u6321\u7684\u7269\u4f53\uff0c\u51763D\u5c5e\u6027\u4ece\u5355\u89c6\u89d2\u770b\u5177\u6709\u56fa\u6709\u6a21\u7cca\u6027\uff0c\u53ea\u80fd\u901a\u8fc7\u8f66\u8f86-\u8def\u8fb9\u914d\u5bf9\u5e27\u4ea4\u53c9\u9a8c\u8bc1\u624d\u80fd\u53ef\u9760\u6807\u6ce8\u3002\u8fd9\u4e9b\u56fa\u6709\u6a21\u7cca\u6837\u672c\u589e\u52a0\u4e86\u6807\u6ce8\u96be\u5ea6\u548c\u6210\u672c\uff0c\u63ed\u793a\u4e86\u6839\u672c\u7684\u53ef\u5b66\u4e60\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faLH3D\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u8def\u8fb9\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u7684\u4e3b\u52a8\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u9009\u62e9\u65e2\u4fe1\u606f\u4e30\u5bcc\u53c8\u53ef\u53ef\u9760\u6807\u6ce8\u7684\u573a\u666f\uff0c\u6291\u5236\u56fa\u6709\u6a21\u7cca\u6837\u672c\u540c\u65f6\u786e\u4fdd\u8986\u76d6\u8303\u56f4\u3002\u901a\u8fc7\u53ef\u5b66\u4e60\u6027\u9a71\u52a8\u7684\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u800c\u975e\u4f20\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u65b9\u6cd5\u3002", "result": "\u5728DAIR-V2X-I\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u752825%\u7684\u6807\u6ce8\u9884\u7b97\uff0cLH3D\u65b9\u6cd5\u5bf9\u8f66\u8f86\u3001\u884c\u4eba\u548c\u9a91\u884c\u8005\u5206\u522b\u8fbe\u5230\u5168\u6027\u80fd\u768486.06%\u300167.32%\u548c78.67%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u5b9e\u5bf9\u4e8e\u8def\u8fb93D\u611f\u77e5\u4efb\u52a1\uff0c\u53ef\u5b66\u4e60\u6027\u800c\u975e\u4e0d\u786e\u5b9a\u6027\u624d\u662f\u5173\u952e\u56e0\u7d20\u3002LH3D\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u56fa\u6709\u6a21\u7cca\u6837\u672c\u7684\u6807\u6ce8\u6d6a\u8d39\uff0c\u540c\u65f6\u83b7\u5f97\u9ad8\u6027\u80fd\u6a21\u578b\u3002"}}
{"id": "2601.01754", "categories": ["cs.LG", "cs.CC", "cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2601.01754", "abs": "https://arxiv.org/abs/2601.01754", "authors": ["Selim Jerad", "Anej Svete", "Sophie Hao", "Ryan Cotterell", "William Merrill"], "title": "Context-Free Recognition with Transformers", "comment": null, "summary": "Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\\mathcal{O}(\\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\\mathcal{O}(\\log n)$ looping layers and $\\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u5faa\u73afTransformer\u901a\u8fc7O(log n)\u5faa\u73af\u5c42\u548cO(n^6)\u586b\u5145token\u53ef\u4ee5\u8bc6\u522b\u6240\u6709\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\uff0c\u4f46\u5bf9\u4e8e\u81ea\u7136\u5b50\u7c7b\u5982\u65e0\u6b67\u4e49CFL\uff0c\u4ec5\u9700O(n^3)\u586b\u5145token\uff0c\u4f7f\u8bc6\u522b\u66f4\u9ad8\u6548\u3002", "motivation": "Transformer\u5728\u5904\u7406\u81ea\u7136\u8bed\u8a00\u548c\u4ee3\u7801\u7b49\u7b26\u5408\u8bed\u6cd5\u89c4\u5219\u7684\u8f93\u5165\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5904\u7406\u8bed\u6cd5\u7ed3\u6784\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002\u73b0\u6709\u7814\u7a76\u8868\u660e\u6807\u51c6Transformer\u65e0\u6cd5\u8bc6\u522b\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\uff08CFL\uff09\uff0c\u751a\u81f3\u65e0\u6cd5\u8bc6\u522b\u5176\u5b50\u7c7b\u6b63\u5219\u8bed\u8a00\u3002\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u8bc1\u660eO(log n)\u5faa\u73af\u5c42\u53ef\u4f7fTransformer\u8bc6\u522b\u6b63\u5219\u8bed\u8a00\uff0c\u4f46CFL\u8bc6\u522b\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u5faa\u73afTransformer\u67b6\u6784\uff0c\u4f7f\u7528O(log n)\u5faa\u73af\u5c42\u548cO(n^6)\u586b\u5145token\u6765\u5b9e\u73b0CFL\u8bc6\u522b\u3002\u7279\u522b\u5730\uff0c\u5bf9\u4e8e\u65e0\u6b67\u4e49CFL\u7b49\u81ea\u7136\u5b50\u7c7b\uff0c\u5c06\u586b\u5145token\u9700\u6c42\u964d\u4f4e\u5230O(n^3)\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u9700\u8981\u5bf9\u6570\u6df1\u5ea6\u7684\u8bed\u8a00\u4e0a\u6d4b\u8bd5\u5faa\u73af\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5faa\u73afTransformer\u80fd\u591f\u8bc6\u522b\u6240\u6709CFL\uff0c\u4f46\u9700\u8981\u5927\u91cf\u586b\u5145token\uff08O(n^6)\uff09\u3002\u5bf9\u4e8e\u65e0\u6b67\u4e49CFL\u7b49\u81ea\u7136\u5b50\u7c7b\uff0c\u586b\u5145\u9700\u6c42\u663e\u8457\u964d\u4f4e\u81f3O(n^3)\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u5faa\u73af\u673a\u5236\u5728\u9700\u8981\u5bf9\u6570\u6df1\u5ea6\u7684\u8bed\u8a00\u4e0a\u786e\u5b9e\u6709\u6548\u3002", "conclusion": "Transformer\u8bc6\u522bCFL\u5b58\u5728\u590d\u6742\u6027\uff1a\u901a\u7528\u8bc6\u522b\u53ef\u80fd\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u5927\u91cf\u586b\u5145token\uff0c\u4f46\u901a\u8fc7\u81ea\u7136\u7ea6\u675f\uff08\u5982\u65e0\u6b67\u4e49\u6027\uff09\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u8bc6\u522b\u7b97\u6cd5\u3002\u5faa\u73af\u673a\u5236\u4e3aTransformer\u5904\u7406\u8bed\u6cd5\u7ed3\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2601.01696", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01696", "abs": "https://arxiv.org/abs/2601.01696", "authors": ["Yian Liu", "Xiong Wang", "Ping Xu", "Lei Zhu", "Ming Yan", "Linyun Xue"], "title": "Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems", "comment": null, "summary": "Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5b9e\u65f6\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u534f\u65b9\u5dee\u5206\u5e03\u4f18\u5316\uff08CDO\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u80fd\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u800c\u4e0d\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\u3002", "motivation": "\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u8f66\u9053\u7ebf\u68c0\u6d4b\u9762\u4e34\u91cd\u5927\u6311\u6218\uff1aRGB\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u4fe1\u53f7\u7a00\u758f\u4e14\u5fae\u5999\uff0c\u540c\u65f6\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\u548c\u529f\u8017\u3002\u5c3d\u7ba1\u5b58\u5728\u5206\u5272\u3001\u951a\u70b9\u548c\u66f2\u7ebf\u4e09\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8f66\u9053\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u4e3a\u4f4e\u529f\u8017\u5d4c\u5165\u5f0f\u73af\u5883\u8bbe\u8ba1\u7684\u901a\u7528\u4f18\u5316\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u534f\u65b9\u5dee\u5206\u5e03\u4f18\u5316\uff08CDO\uff09\u6a21\u5757\uff0c\u4e13\u95e8\u4e3a\u9ad8\u6548\u5b9e\u65f6\u5e94\u7528\u8bbe\u8ba1\u3002\u8be5\u6a21\u5757\u901a\u8fc7\u4f7f\u8f66\u9053\u7279\u5f81\u5206\u5e03\u4e0e\u771f\u5b9e\u6807\u7b7e\u5bf9\u9f50\u6765\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4e0d\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u800c\u65e0\u9700\u7ed3\u6784\u4fee\u6539\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u8981\u6570\u636e\u96c6\uff08CULane\u3001TuSimple\u3001LLAMAS\uff09\u4e0a\u5bf9\u516d\u79cd\u4e0d\u540c\u6a21\u578b\uff08\u6db5\u76d6\u6240\u6709\u4e09\u79cd\u65b9\u6cd5\u7c7b\u522b\uff0c\u5305\u62ec\u4e24\u79cd\u5b9e\u65f6\u4f18\u5316\u6a21\u578b\u548c\u56db\u79cdSOTA\u6a21\u578b\uff09\u8fdb\u884c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u7cbe\u5ea6\u63d0\u5347\u8303\u56f4\u57280.01%\u52301.5%\u4e4b\u95f4\u3002", "conclusion": "CDO\u6a21\u5757\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u3001\u80fd\u6548\u548c\u64cd\u4f5c\u7075\u6d3b\u6027\u4f18\u52bf\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8f66\u9053\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2601.01720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01720", "abs": "https://arxiv.org/abs/2601.01720", "authors": ["Xijie Huang", "Chengming Xu", "Donghao Luo", "Xiaobin Hu", "Peng Tang", "Xu Peng", "Jiangning Zhang", "Chengjie Wang", "Yanwei Fu"], "title": "FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing", "comment": null, "summary": "First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.", "AI": {"tldr": "\u63d0\u51faFFP-300K\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u89c6\u9891\u6570\u636e\u96c6\u548c\u65b0\u578b\u65e0\u5f15\u5bfcFFP\u6846\u67b6\uff0c\u901a\u8fc7AST-RoPE\u4f4d\u7f6e\u7f16\u7801\u548c\u81ea\u84b8\u998f\u7b56\u7565\u89e3\u51b3\u5916\u89c2\u4fdd\u6301\u4e0e\u8fd0\u52a8\u4fdd\u7559\u7684\u51b2\u7a81\uff0c\u5728EditVerseBench\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709FFP\u65b9\u6cd5\u4f9d\u8d56\u7e41\u7410\u7684\u8fd0\u884c\u65f6\u5f15\u5bfc\uff0c\u6839\u672c\u539f\u56e0\u5728\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u4e0d\u8db3\uff1a\u89c6\u9891\u8fc7\u77ed\u3001\u5206\u8fa8\u7387\u4f4e\u3001\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u591f\uff0c\u65e0\u6cd5\u5b66\u4e60\u9c81\u68d2\u7684\u65f6\u95f4\u5148\u9a8c\u3002", "method": "1) \u6784\u5efaFFP-300K\u6570\u636e\u96c6\uff1a30\u4e07\u5bf9720p\u5206\u8fa8\u7387\u300181\u5e27\u7684\u9ad8\u4fdd\u771f\u89c6\u9891\u5bf9\uff0c\u91c7\u7528\u53cc\u8f68\u7ba1\u9053\u652f\u6301\u591a\u6837\u5c40\u90e8\u548c\u5168\u5c40\u7f16\u8f91\uff1b2) \u63d0\u51fa\u65e0\u5f15\u5bfcFFP\u6846\u67b6\uff1a\u5f15\u5165AST-RoPE\u52a8\u6001\u91cd\u6620\u5c04\u4f4d\u7f6e\u7f16\u7801\u4ee5\u89e3\u8026\u5916\u89c2\u548c\u8fd0\u52a8\u53c2\u8003\uff1b3) \u91c7\u7528\u81ea\u84b8\u998f\u7b56\u7565\uff1a\u8eab\u4efd\u4f20\u64ad\u4efb\u52a1\u4f5c\u4e3a\u6b63\u5219\u5316\u5668\u786e\u4fdd\u957f\u671f\u65f6\u95f4\u7a33\u5b9a\u6027\u548c\u9632\u6b62\u8bed\u4e49\u6f02\u79fb\u3002", "result": "\u5728EditVerseBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5b66\u672f\u548c\u5546\u4e1a\u6a21\u578b\uff0c\u83b7\u5f97\u7ea60.2 PickScore\u548c0.3 VLM\u5206\u6570\u7684\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u63d0\u51fa\u521b\u65b0\u7684\u65e0\u5f15\u5bfcFFP\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86FFP\u4e2d\u5916\u89c2\u4fdd\u6301\u4e0e\u8fd0\u52a8\u4fdd\u7559\u7684\u5173\u952e\u77db\u76fe\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u53ef\u63a7\u89c6\u9891\u7f16\u8f91\u3002"}}
{"id": "2601.01792", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.01792", "abs": "https://arxiv.org/abs/2601.01792", "authors": ["NAVER Cloud HyperCLOVA X Team"], "title": "HyperCLOVA X 8B Omni", "comment": "Technical Report", "summary": "In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.", "AI": {"tldr": "HyperCLOVA X 8B Omni\u662f\u9996\u4e2a\u652f\u6301\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u4efb\u610f\u8f93\u5165\u8f93\u51fa\u7684\u5168\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5e8f\u5217\u5904\u7406\u5b9e\u73b0\u7406\u89e3\u548c\u751f\u6210\u529f\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u4efb\u610f\u5230\u4efb\u610f\u5168\u6a21\u6001\u52a9\u624b\uff0c\u907f\u514d\u4f20\u7edf\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u5206\u79bb\u7684\u6a21\u6001\u7279\u5b9a\u5904\u7406\u6d41\u7a0b\uff0c\u5b9e\u73b0\u66f4\u5b9e\u7528\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u5171\u4eab\u7684\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u63a5\u53e3\u5904\u7406\u4ea4\u9519\u7684\u591a\u6a21\u6001\u5e8f\u5217\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u97f3\u9891\u7f16\u7801\u5668\u6ce8\u5165\u8fde\u7eed\u5d4c\u5165\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7406\u89e3\u548c\u63a5\u5730\u3002", "result": "\u5728\u97e9\u8bed\u548c\u82f1\u8bed\u7684\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u591a\u79cd\u8f93\u5165\u8f93\u51fa\u7ec4\u5408\u4e2d\uff0c\u76f8\u6bd4\u540c\u7b49\u89c4\u6a21\u6a21\u578b\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "HyperCLOVA X 8B Omni\u4f5c\u4e3a8B\u89c4\u6a21\u7684\u5168\u6a21\u6001\u63a2\u7d22\u70b9\uff0c\u5176\u5f00\u6e90\u6743\u91cd\u5c06\u652f\u6301\u5e7f\u6cdb\u7684\u7814\u7a76\u548c\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2601.01746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01746", "abs": "https://arxiv.org/abs/2601.01746", "authors": ["Lintong Wei", "Jian Lu", "Haozhe Cheng", "Jihua Zhu", "Kaibing Zhang"], "title": "Point-SRA: Self-Representation Alignment for 3D Representation Learning", "comment": "This is an AAAI 2026 accepted paper titled \"Point-SRA: Self-Representation Alignment for 3D Representation Learning\", spanning 13 pages in total. The submission includes 7 figures (fig1 to fig7) that visually support the technical analysis", "summary": "Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.", "AI": {"tldr": "Point-SRA\u662f\u4e00\u79cd\u901a\u8fc7\u81ea\u84b8\u998f\u548c\u6982\u7387\u5efa\u6a21\u5bf9\u9f50\u8868\u793a\u76843D\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u4f20\u7edfMAE\u7684\u56fa\u5b9a\u63a9\u7801\u6bd4\u7387\u548c\u70b9\u7ea7\u91cd\u5efa\u5047\u8bbe\uff0c\u5728\u591a\u6a21\u6001\u8868\u793a\u5bf9\u9f50\u548c\u6982\u7387\u91cd\u5efa\u65b9\u9762\u6709\u521b\u65b0\u3002", "motivation": "\u73b0\u6709MAE\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u56fa\u5b9a\u63a9\u7801\u6bd4\u7387\u5ffd\u89c6\u4e86\u591a\u7ea7\u8868\u793a\u76f8\u5173\u6027\uff1b2\uff09\u70b9\u7ea7\u91cd\u5efa\u5047\u8bbe\u4e0e\u70b9\u4e91\u591a\u6837\u6027\u51b2\u7a81\uff1b3\uff09\u672a\u80fd\u5145\u5206\u5229\u7528\u51e0\u4f55\u7ed3\u6784\u4fe1\u606f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u4e92\u8865\u51e0\u4f55\u8bed\u4e49\u4fe1\u606f\u5e76\u652f\u6301\u591a\u6837\u5316\u6982\u7387\u91cd\u5efa\u7684\u65b9\u6cd5\u3002", "method": "1\uff09\u4f7f\u7528\u4e0d\u540c\u63a9\u7801\u6bd4\u7387\u7684MAE\u6355\u6349\u4e92\u8865\u4fe1\u606f\uff1b2\uff09MeanFlow Transformer\u5229\u7528\u8de8\u6a21\u6001\u6761\u4ef6\u5d4c\u5165\u5b9e\u73b0\u591a\u6837\u5316\u6982\u7387\u91cd\u5efa\uff1b3\uff09\u5728MAE\u548cMFT\u4e24\u4e2a\u5c42\u6b21\u63d0\u51fa\u53cc\u81ea\u8868\u793a\u5bf9\u9f50\u673a\u5236\uff1b4\uff09\u8bbe\u8ba1\u6d41\u6761\u4ef6\u5fae\u8c03\u67b6\u6784\u5145\u5206\u5229\u7528\u5b66\u4e60\u5230\u7684\u70b9\u4e91\u5206\u5e03\u3002", "result": "1\uff09\u5728ScanObjectNN\u4e0a\u6bd4Point-MAE\u63d0\u53475.37%\uff1b2\uff09\u9885\u5185\u52a8\u8109\u7624\u5206\u5272\u4e2d\u52a8\u8109\u8fbe\u523096.07%\u5e73\u5747IoU\uff0c\u52a8\u8109\u7624\u8fbe\u523086.87%\uff1b3\uff093D\u76ee\u6807\u68c0\u6d4b\u8fbe\u523047.3% AP@50\uff0c\u6bd4MaskPoint\u63d0\u53475.12%\u3002", "conclusion": "Point-SRA\u901a\u8fc7\u591a\u7ea7\u8868\u793a\u5bf9\u9f50\u548c\u6982\u7387\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfMAE\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u4e2a3D\u89c6\u89c9\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6355\u6349\u51e0\u4f55\u8bed\u4e49\u4fe1\u606f\u548c\u5904\u7406\u70b9\u4e91\u591a\u6837\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2601.01793", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01793", "abs": "https://arxiv.org/abs/2601.01793", "authors": ["Shamik Bhattacharyya", "Rachel Kalpana Kalaimani"], "title": "Distributed Federated Learning by Alternating Periods of Training", "comment": null, "summary": "Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u670d\u52a1\u5668\u67b6\u6784\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4e2d\u5355\u70b9\u6545\u969c\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86DFL\u7b97\u6cd5\u5b9e\u73b0\u670d\u52a1\u5668\u95f4\u7684\u534f\u540c\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u4f9d\u8d56\u5355\u4e00\u4e2d\u592e\u670d\u52a1\u5668\uff0c\u5728\u9762\u5bf9\u5927\u91cf\u5ba2\u6237\u7aef\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5e76\u5b58\u5728\u5355\u70b9\u6545\u969c\u98ce\u9669\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u9650\u5236\uff0c\u63d0\u9ad8\u7cfb\u7edf\u7684\u5bb9\u9519\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u542b\u591a\u4e2a\u5177\u6709\u670d\u52a1\u5668\u95f4\u901a\u4fe1\u80fd\u529b\u7684\u670d\u52a1\u5668\u3002\u6bcf\u4e2a\u670d\u52a1\u5668\u5173\u8054\u4e00\u7ec4\u4e0d\u76f8\u4ea4\u7684\u5ba2\u6237\u7aef\uff0c\u4fdd\u6301\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u901a\u4fe1\u80fd\u529b\u3002\u8bbe\u8ba1\u4e86DFL\u7b97\u6cd5\uff0c\u4ea4\u66ff\u8fdb\u884c\u5ba2\u6237\u7aef\u6570\u636e\u7684\u672c\u5730\u8bad\u7ec3\u548c\u670d\u52a1\u5668\u95f4\u7684\u5168\u5c40\u8bad\u7ec3\u3002", "result": "\u5728\u9002\u5f53\u53c2\u6570\u9009\u62e9\u4e0b\uff0cDFL\u7b97\u6cd5\u786e\u4fdd\u6240\u6709\u670d\u52a1\u5668\u6536\u655b\u5230\u5171\u540c\u6a21\u578b\u503c\uff0c\u4e0e\u7406\u60f3\u6a21\u578b\u7684\u8bef\u5dee\u5728\u8f83\u5c0f\u5bb9\u5fcd\u8303\u56f4\u5185\uff0c\u6709\u6548\u6574\u5408\u4e86\u672c\u5730\u548c\u5168\u5c40\u8bad\u7ec3\u6a21\u578b\u3002\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u7406\u8bba\u4e3b\u5f20\u3002", "conclusion": "\u5206\u5e03\u5f0f\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u670d\u52a1\u5668\u67b6\u6784\u548cDFL\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u4e3a\u5927\u89c4\u6a21\u8054\u90a6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.01749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01749", "abs": "https://arxiv.org/abs/2601.01749", "authors": ["Lei Zhu", "Lijian Lin", "Ye Zhu", "Jiahao Wu", "Xuehan Hou", "Yu Li", "Yunfei Liu", "Jie Chen"], "title": "MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement", "comment": "20 pages, 11i figures", "summary": "Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.", "AI": {"tldr": "MANGO\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u7eaf\u56fe\u50cf\u7ea7\u76d1\u7763\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u53cc\u4eba3D\u5bf9\u8bdd\u5934\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u81ea\u7136\u542c-\u8bf4\u4ea4\u4e92\u548c\u7cbe\u7ec6\u9762\u90e8\u52a8\u6001\u6355\u6349\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u9a71\u52a8\u76843D\u5934\u90e8\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u8bf4\u8bdd\u4eba\u573a\u666f\uff0c\u7f3a\u4e4f\u81ea\u7136\u7684\u53cc\u5411\u542c-\u8bf4\u4ea4\u4e92\u3002\u73b0\u67093D\u5bf9\u8bdd\u5934\u50cf\u65b9\u6cd5\u4f9d\u8d56\u5bb9\u6613\u51fa\u9519\u7684\u4f2a3D\u6807\u7b7e\uff0c\u65e0\u6cd5\u6355\u6349\u7cbe\u7ec6\u7684\u9762\u90e8\u52a8\u6001\uff0c\u5b9e\u73b0\u6d41\u7545\u7684\u5bf9\u8bdd\u884c\u4e3a\u8f6c\u6362\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6MANGO\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684transformer\u548c\u53cc\u97f3\u9891\u4ea4\u4e92\u6a21\u5757\u4ece\u591a\u8bf4\u8bdd\u4eba\u97f3\u9891\u4e2d\u5efa\u6a21\u81ea\u71363D\u8fd0\u52a8\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5feb\u901f3D\u9ad8\u65af\u6e32\u67d3\u5668\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\uff0c\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u4e3a3D\u8fd0\u52a8\u63d0\u4f9b2D\u7ea7\u5149\u5ea6\u76d1\u7763\u3002\u540c\u65f6\u5f15\u5165\u4e86MANGO-Dialog\u6570\u636e\u96c6\uff0c\u5305\u542b500+\u8eab\u4efd\u8d85\u8fc750\u5c0f\u65f6\u7684\u5bf9\u9f502D-3D\u5bf9\u8bdd\u6570\u636e\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5efa\u6a21\u53cc\u4eba3D\u5bf9\u8bdd\u8fd0\u52a8\u65b9\u9762\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u611f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u9a71\u52a8\u8bf4\u8bdd\u5934\u50cf\u7684\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "MANGO\u6846\u67b6\u901a\u8fc7\u7eaf\u56fe\u50cf\u7ea7\u76d1\u7763\u548c\u4ea4\u66ff\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f2a3D\u6807\u7b7e\u5f15\u5165\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e0e\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u884c\u4e3a\u66f4\u597d\u7684\u5bf9\u9f50\uff0c\u63a8\u52a8\u4e863D\u5bf9\u8bdd\u5934\u50cf\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.01769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01769", "abs": "https://arxiv.org/abs/2601.01769", "authors": ["Hao Lu", "Ziniu Qian", "Yifu Li", "Yang Zhou", "Bingzheng Wei", "Yan Xu"], "title": "CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology", "comment": "The paper has been accepted by BIBM 2025", "summary": "In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u4e34\u5e8a\u8bca\u65ad\u6a21\u677f\u7684\u75c5\u7406\u4fe1\u606f\u7ed3\u6784\u5316\u6536\u96c6\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86CTIS-Align\u6570\u636e\u96c6\u548cCTIS-Bench\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u4e86CTIS-QA\u6a21\u578b\u7528\u4e8e\u75c5\u7406\u5207\u7247\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u75c5\u7406\u62a5\u544a\u4fe1\u606f\u5206\u6563\u4e14\u975e\u7ed3\u6784\u5316\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u548c\u6807\u51c6\u5316\u75c5\u7406\u7279\u5f81\uff0c\u4ee5\u652f\u6301\u57fa\u4e8e\u75c5\u7406\u5207\u7247\u7684\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u548c\u95ee\u7b54\u4efb\u52a1\u3002", "method": "1. \u8bbe\u8ba1\u4e34\u5e8a\u75c5\u7406\u62a5\u544a\u6a21\u677f(CPRT)\u6807\u51c6\u5316\u63d0\u53d6\u75c5\u7406\u7279\u5f81\uff1b2. \u6784\u5efaCTIS-Align\u6570\u636e\u96c6(8\u4e07\u5207\u7247-\u63cf\u8ff0\u5bf9)\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\uff1b3. \u521b\u5efaCTIS-Bench\u57fa\u51c6(977\u5f20WSI\uff0c14,879\u4e2aQA\u5bf9)\uff1b4. \u63d0\u51faCTIS-QA\u53cc\u6d41\u67b6\u6784\u6a21\u578b\uff0c\u7ed3\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u533a\u57df\u5173\u6ce8\u3002", "result": "\u5728TCGA-BRCA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6d41\u7a0b\u6709\u6548\u6027\u3002CTIS-QA\u5728WSI-VQA\u3001CTIS-Bench\u548c\u5207\u7247\u7ea7\u8bca\u65ad\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u4e00\u81f4\u4f18\u8d8a\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e34\u5e8a\u8bca\u65ad\u6a21\u677f\u6d41\u7a0b\u80fd\u6709\u6548\u7ed3\u6784\u5316\u75c5\u7406\u4fe1\u606f\uff0cCTIS-QA\u6a21\u578b\u901a\u8fc7\u6a21\u62df\u75c5\u7406\u5b66\u5bb6\u8bca\u65ad\u65b9\u6cd5\uff0c\u5728\u75c5\u7406\u5207\u7247\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e34\u5e8a\u75c5\u7406\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.01803", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01803", "abs": "https://arxiv.org/abs/2601.01803", "authors": ["Dennis Jabs", "Aditya Mohan", "Marius Lindauer"], "title": "Moments Matter:Stabilizing Policy Optimization using Return Distributions", "comment": "Workshop paper at RLDM'25", "summary": "Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(\u03b8)$, obtained by repeatedly sampling minibatches, updating $\u03b8$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(\u03b8)$ can improve stability, directly estimating $R(\u03b8)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(\u03b8)$. In such cases, our moment-based correction narrows $R(\u03b8)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u8bc4\u8bba\u5bb6\u9ad8\u9636\u77e9\u7684PPO\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u56de\u62a5\u5206\u5e03\u6765\u63d0\u5347\u7b56\u7565\u7a33\u5b9a\u6027", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u76f8\u540c\u56de\u62a5\u4e0b\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u884c\u4e3a\uff0c\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u6e90\u4e8e\u73af\u5883\u548c\u7b97\u6cd5\u56e0\u7d20\u3002\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u5373\u4f7f\u5c0f\u7684\u53c2\u6570\u53d8\u5316\u4e5f\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u6b65\u6001\uff0c\u8fd9\u7ed9\u7b97\u6cd5\u6bd4\u8f83\u548c\u5b9e\u9645\u5e94\u7528\u5e26\u6765\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u5206\u5e03\u8bc4\u8bba\u5bb6\u5efa\u6a21\u72b6\u6001-\u52a8\u4f5c\u56de\u62a5\u5206\u5e03\uff0c\u5229\u7528\u8be5\u5206\u5e03\u7684\u9ad8\u9636\u77e9\uff08\u504f\u5ea6\u548c\u5cf0\u5ea6\uff09\u5bf9PPO\u7684\u4f18\u52bf\u51fd\u6570\u8fdb\u884c\u504f\u7f6e\u4fee\u6b63\uff0c\u901a\u8fc7\u60e9\u7f5a\u6781\u7aef\u5c3e\u90e8\u884c\u4e3a\u6765\u907f\u514d\u7b56\u7565\u8fdb\u5165\u4e0d\u7a33\u5b9a\u53c2\u6570\u533a\u57df\u3002", "result": "\u5728Walker2D\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u7b56\u7565\u7a33\u5b9a\u6027\u63d0\u5347\u4e8675%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u8f83\u7684\u8bc4\u4f30\u56de\u62a5\u3002", "conclusion": "\u5229\u7528\u73af\u5883\u968f\u673a\u6027\u548c\u5206\u5e03\u8bc4\u8bba\u5bb6\u7684\u9ad8\u9636\u77e9\u4fe1\u606f\u53ef\u4ee5\u6709\u6548\u6539\u5584PPO\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u540e\u66f4\u65b0\u8bc4\u8bba\u5bb6\u503c\u4e0e\u540e\u66f4\u65b0\u56de\u62a5\u5bf9\u9f50\u4e0d\u4f73\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u7f29\u5c0f\u56de\u62a5\u5206\u5e03\u8303\u56f4\uff0c\u63d0\u5347\u7b56\u7565\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.01781", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01781", "abs": "https://arxiv.org/abs/2601.01781", "authors": ["Lakshay Sharma", "Alex Marin"], "title": "Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery", "comment": "Accepted at CV4EO Workshop at WACV 2026", "summary": "Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \\href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u2014\u2014\u5b50\u56fe\u50cf\u91cd\u53e0\u9884\u6d4b\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u8f83\u5c11\u7684\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u9884\u6d4b\u5b50\u56fe\u50cf\u5728\u539f\u56fe\u4e2d\u7684\u4f4d\u7f6e\u6765\u5b66\u4e60\u7279\u5f81\uff0c\u663e\u8457\u52a0\u5feb\u6536\u655b\u901f\u5ea6\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u9065\u611f\u56fe\u50cf\u9886\u57df\u83b7\u53d6\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9700\u8981\u8f83\u5c11\u9884\u8bad\u7ec3\u6570\u636e\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u5b50\u56fe\u50cf\u91cd\u53e0\u9884\u6d4b\u4efb\u52a1\uff1a\u4ece\u539f\u59cb\u56fe\u50cf\u4e2d\u63d0\u53d6\u4e00\u4e2a\u5b50\u56fe\u50cf\uff0c\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u8be5\u5b50\u56fe\u50cf\u5728\u539f\u59cb\u56fe\u50cf\u4e2d\u7684\u4f4d\u7f6e\u8bed\u4e49\u63a9\u7801\u3002\u8fd9\u79cd\u65b9\u6cd5\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u52a0\u5feb\u4e86\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5728\u591a\u4e2a\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u540c\u7b49\u6216\u66f4\u597d\u7684mIoU\u6027\u80fd\u3002\u5f53\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u51cf\u5c11\u65f6\uff0c\u6027\u80fd\u4f18\u52bf\u66f4\u52a0\u660e\u663e\uff0c\u4e14\u76f8\u6bd4\u5176\u4ed6\u81ea\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u66f4\u5c11\u7684\u9884\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u5b50\u56fe\u50cf\u91cd\u53e0\u9884\u6d4b\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\uff0c\u80fd\u591f\u5728\u51cf\u5c11\u9884\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2601.01829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01829", "abs": "https://arxiv.org/abs/2601.01829", "authors": ["Peiyan Hu", "Haodong Feng", "Hongyuan Liu", "Tongtong Yan", "Wenhao Deng", "Tianrun Gao", "Rong Zheng", "Haoren Zheng", "Chenglei Yu", "Chuanrui Wang", "Kaiwen Li", "Zhi-Ming Ma", "Dezhi Zhou", "Xingcai Lu", "Dixia Fan", "Tailin Wu"], "title": "RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data", "comment": "46 pages, 21 figures", "summary": "Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.", "AI": {"tldr": "RealPDEBench\u662f\u9996\u4e2a\u6574\u5408\u771f\u5b9e\u4e16\u754c\u6d4b\u91cf\u6570\u636e\u4e0e\u914d\u5bf9\u6570\u503c\u6a21\u62df\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u57fa\u51c6\uff0c\u5305\u542b5\u4e2a\u6570\u636e\u96c6\u30013\u4e2a\u4efb\u52a1\u30018\u4e2a\u6307\u6807\u548c10\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u79d1\u5b66ML\u4e2d\u771f\u5b9e\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9762\u4e34\u7684\u4e3b\u8981\u74f6\u9888\u662f\u7f3a\u4e4f\u6602\u8d35\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u5bfc\u81f4\u5927\u591a\u6570\u6a21\u578b\u53ea\u80fd\u5728\u6a21\u62df\u6570\u636e\u4e0a\u8bad\u7ec3\u548c\u9a8c\u8bc1\uff0c\u8fd9\u4e0d\u4ec5\u9650\u5236\u4e86\u79d1\u5b66ML\u7684\u53d1\u5c55\u8bc4\u4f30\uff0c\u4e5f\u963b\u788d\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u7b49\u5173\u952e\u4efb\u52a1\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b5\u4e2a\u771f\u5b9e\u4e16\u754c\u6d4b\u91cf\u6570\u636e\u96c6\u53ca\u5176\u914d\u5bf9\u6a21\u62df\u6570\u636e\u96c6\u7684\u57fa\u51c6\uff0c\u5b9a\u4e49\u4e863\u4e2a\u4efb\u52a1\uff08\u5141\u8bb8\u771f\u5b9e\u4e0e\u6a21\u62df\u6570\u636e\u6bd4\u8f83\u5e76\u4fc3\u8fdb\u4e24\u8005\u6865\u63a5\u65b9\u6cd5\u5f00\u53d1\uff09\uff0c\u8bbe\u8ba1\u4e868\u4e2a\u8bc4\u4f30\u6307\u6807\uff08\u6db5\u76d6\u6570\u636e\u5bfc\u5411\u548c\u7269\u7406\u5bfc\u5411\u6307\u6807\uff09\uff0c\u5e76\u57fa\u51c6\u6d4b\u8bd5\u4e8610\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u6a21\u62df\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u540c\u65f6\u8868\u660e\u4f7f\u7528\u6a21\u62df\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u80fd\u6301\u7eed\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6536\u655b\u6027\u3002", "conclusion": "RealPDEBench\u901a\u8fc7\u63d0\u4f9b\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u89c1\u89e3\uff0c\u63a8\u52a8\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u5411\u6865\u63a5\u6a21\u62df\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u548c\u5b9e\u9645\u90e8\u7f72\u65b9\u5411\u53d1\u5c55\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u57fa\u51c6\u5e73\u53f0\u3002"}}
{"id": "2601.01784", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.01784", "abs": "https://arxiv.org/abs/2601.01784", "authors": ["Boyang Zhao", "Xin Liao", "Jiaxin Chen", "Xiaoshuai Wu", "Yufeng Wu"], "title": "DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization", "comment": null, "summary": "The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \\emph{local view}, failing to capture global anomalies. To address this, we propose a \\underline{d}ual-stream graph learning and \\underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \\emph{Temporal Distance Stream} for local artifacts and a \\emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\\% in AP@0.95, with significant improvements in cross-domain robustness.", "AI": {"tldr": "DDNet\uff1a\u57fa\u4e8e\u53cc\u6d41\u56fe\u5b66\u4e60\u548c\u89e3\u7f20\u7684\u65f6\u5e8f\u4f2a\u9020\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u5c40\u90e8\u4f2a\u5f71\u548c\u8bed\u4e49\u5185\u5bb9\u6d41\uff0c\u7ed3\u5408\u8f68\u8ff9\u89e3\u7f20\u548c\u8de8\u7ea7\u7279\u5f81\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u4f2a\u9020\u7247\u6bb5\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8de8\u57df\u9c81\u68d2\u6027\u3002", "motivation": "AIGC\u6280\u672f\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u4ec5\u7be1\u6539\u89c6\u9891\u5c0f\u7247\u6bb5\u5c31\u80fd\u8bef\u5bfc\u89c2\u4f17\uff0c\u800c\u89c6\u9891\u7ea7\u68c0\u6d4b\u4e0d\u51c6\u786e\u4e14\u7f3a\u4e4f\u8bf4\u670d\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u5c40\u90e8\u89c6\u89d2\uff0c\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u5f02\u5e38\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65f6\u5e8f\u4f2a\u9020\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDDNet\u53cc\u6d41\u56fe\u5b66\u4e60\u6846\u67b6\uff1a1) \u65f6\u95f4\u8ddd\u79bb\u6d41\u6355\u6349\u5c40\u90e8\u4f2a\u5f71\uff1b2) \u8bed\u4e49\u5185\u5bb9\u6d41\u5efa\u7acb\u957f\u7a0b\u8fde\u63a5\u3002\u5f15\u5165\u8f68\u8ff9\u89e3\u7f20\u4e0e\u9002\u5e94(TDA)\u5206\u79bb\u901a\u7528\u4f2a\u9020\u6307\u7eb9\uff0c\u4ee5\u53ca\u8de8\u7ea7\u7279\u5f81\u5d4c\u5165(CLFE)\u901a\u8fc7\u5c42\u6b21\u7279\u5f81\u6df1\u5ea6\u878d\u5408\u6784\u5efa\u9c81\u68d2\u7279\u5f81\u57fa\u7840\u3002", "result": "\u5728ForgeryNet\u548cTVIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDDNet\u5728AP@0.95\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u5347\u7ea69%\uff0c\u5728\u8de8\u57df\u9c81\u68d2\u6027\u65b9\u9762\u4e5f\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "DDNet\u901a\u8fc7\u53cc\u6d41\u534f\u8c03\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5c40\u90e8\u5e73\u6ed1\u6df9\u6ca1\u5168\u5c40\u7ebf\u7d22\u7684\u95ee\u9898\uff0c\u7ed3\u5408\u89e3\u7f20\u6280\u672f\u548c\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u65f6\u5e8f\u4f2a\u9020\u5b9a\u4f4d\u548c\u66f4\u5f3a\u7684\u8de8\u57df\u9002\u5e94\u6027\u3002"}}
{"id": "2601.01833", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01833", "abs": "https://arxiv.org/abs/2601.01833", "authors": ["Chenyu Hu", "Qiming Hu", "Sinan Chen", "Nianyu Li", "Mingyue Zhang", "Jialong Li"], "title": "FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks", "comment": null, "summary": "Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.", "AI": {"tldr": "FAROS\uff1a\u4e00\u4e2a\u589e\u5f3a\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5dee\u5206\u7f29\u653e\u548c\u9c81\u68d2\u6838\u5fc3\u96c6\u8ba1\u7b97\u6765\u9632\u5fa1\u540e\u95e8\u653b\u51fb\uff0c\u76f8\u6bd4\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u4e3b\u4efb\u52a1\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u540e\u95e8\u653b\u51fb\u7684\u4e25\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u53c2\u6570\uff0c\u5b58\u5728\u5355\u70b9\u6545\u969c\u98ce\u9669\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u653b\u51fb\u8005\u7b56\u7565\u3002", "method": "\u63d0\u51faFAROS\u6846\u67b6\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5dee\u5206\u7f29\u653e\uff08ADS\uff09\u673a\u5236\u548c\u9c81\u68d2\u6838\u5fc3\u96c6\u8ba1\u7b97\uff08RCC\uff09\u3002ADS\u6839\u636e\u5ba2\u6237\u7aef\u4e0a\u4f20\u68af\u5ea6\u7684\u79bb\u6563\u5ea6\u52a8\u6001\u8c03\u6574\u9632\u5fa1\u7075\u654f\u5ea6\uff1bRCC\u901a\u8fc7\u8ba1\u7b97\u9ad8\u7f6e\u4fe1\u5ea6\u5ba2\u6237\u7aef\u6838\u5fc3\u96c6\u7684\u8d28\u5fc3\u6765\u964d\u4f4e\u5355\u70b9\u6545\u969c\u98ce\u9669\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u653b\u51fb\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u4e3b\u4efb\u52a1\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "FAROS\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u9632\u5fa1\u673a\u5236\u6709\u6548\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01798", "abs": "https://arxiv.org/abs/2601.01798", "authors": ["Syed Abdul Hannan", "Hazim Bukhari", "Thomas Cantalapiedra", "Eman Ansar", "Massa Baali", "Rita Singh", "Bhiksha Raj"], "title": "VerLM: Explaining Face Verification Using Natural Language", "comment": null, "summary": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u521b\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u4eba\u8138\u9a8c\u8bc1\uff0c\u4e0d\u4ec5\u80fd\u51c6\u786e\u5224\u65ad\u4e24\u5f20\u4eba\u8138\u56fe\u50cf\u662f\u5426\u4e3a\u540c\u4e00\u4eba\uff0c\u8fd8\u80fd\u660e\u786e\u89e3\u91ca\u51b3\u7b56\u4f9d\u636e\uff0c\u4f7f\u7528\u4e24\u79cd\u4e92\u8865\u7684\u89e3\u91ca\u98ce\u683c\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u4eba\u8138\u9a8c\u8bc1\u7cfb\u7edf\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u901a\u5e38\u7f3a\u4e4f\u51b3\u7b56\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\uff0c\u7528\u6237\u65e0\u6cd5\u7406\u89e3\u7cfb\u7edf\u505a\u51fa\u5224\u65ad\u7684\u5177\u4f53\u539f\u56e0\u3002", "method": "\u91c7\u7528\u521b\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8bad\u7ec3\u65f6\u4f7f\u7528\u4e24\u79cd\u4e92\u8865\u7684\u89e3\u91ca\u98ce\u683c\uff1a\u7b80\u6d01\u603b\u7ed3\u5173\u952e\u56e0\u7d20\u548c\u8be6\u7ec6\u63cf\u8ff0\u56fe\u50cf\u95f4\u5177\u4f53\u5dee\u5f02\u3002\u5c06\u539f\u672c\u4e3a\u97f3\u9891\u533a\u5206\u8bbe\u8ba1\u7684\u5148\u8fdb\u5efa\u6a21\u65b9\u6cd5\u9002\u5e94\u5e76\u589e\u5f3a\u4ee5\u6709\u6548\u5904\u7406\u89c6\u89c9\u8f93\u5165\u3002", "result": "\u63d0\u51fa\u7684VLM\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u57fa\u7ebf\u65b9\u6cd5\u548c\u73b0\u6709\u6a21\u578b\uff0c\u8de8\u6a21\u6001\u8fc1\u79fb\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u8138\u9a8c\u8bc1\u9886\u57df\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u900f\u660e\u3001\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u4eba\u8138\u9a8c\u8bc1\u7cfb\u7edf\u3002"}}
{"id": "2601.01840", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01840", "abs": "https://arxiv.org/abs/2601.01840", "authors": ["Qiantao Yang", "Liquan Chen", "Mingfu Xue", "Songze Li"], "title": "Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack", "comment": "Accepted in AAAI 2026", "summary": "Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.", "AI": {"tldr": "FedCSPACK\uff1a\u4e00\u79cd\u57fa\u4e8e\u4f59\u5f26\u7a00\u758f\u5316\u53c2\u6570\u6253\u5305\u548c\u53cc\u6743\u91cd\u805a\u5408\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u6253\u5305\u548c\u9009\u62e9\u6027\u5171\u4eab\u51cf\u5c11\u5e26\u5bbd\u9700\u6c42\uff0c\u5229\u7528\u63a9\u7801\u77e9\u9635\u548c\u6743\u91cd\u805a\u5408\u673a\u5236\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u8fb9\u7f18\u5ba2\u6237\u7aef\u7684\u6570\u636e\u5f02\u6784\u6027\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u6a21\u578b\u5206\u5272\u548c\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u6a21\u578b\u517c\u5bb9\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u5ba2\u6237\u7aef\u6709\u9650\u7684\u901a\u4fe1\u5e26\u5bbd\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u672a\u80fd\u6709\u6548\u5e73\u8861\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u548c\u9002\u5e94\u6709\u9650\u5ba2\u6237\u7aef\u8d44\u6e90\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u63d0\u51faFedCSPACK\u65b9\u6cd5\uff1a1\uff09\u5ba2\u6237\u7aef\u6253\u5305\u6a21\u578b\u53c2\u6570\uff0c\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u9009\u62e9\u8d21\u732e\u6700\u5927\u7684\u53c2\u6570\u5305\u8fdb\u884c\u5171\u4eab\uff0c\u51cf\u5c11\u5e26\u5bbd\u9700\u6c42\uff1b2\uff09\u5ba2\u6237\u7aef\u751f\u6210\u57fa\u4e8e\u5171\u4eab\u53c2\u6570\u5305\u7684\u63a9\u7801\u77e9\u9635\uff0c\u63d0\u9ad8\u7a00\u758f\u66f4\u65b0\u5728\u670d\u52a1\u5668\u4e0a\u7684\u5bf9\u9f50\u548c\u805a\u5408\u6548\u7387\uff1b3\uff09\u5728\u63a9\u7801\u4e2d\u5d4c\u5165\u65b9\u5411\u548c\u5206\u5e03\u8ddd\u79bb\u6743\u91cd\uff0c\u5b9e\u73b0\u52a0\u6743\u5f15\u5bfc\u805a\u5408\u673a\u5236\uff0c\u589e\u5f3a\u5168\u5c40\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u5341\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFedCSPACK\u5728\u4fdd\u6301\u9ad8\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "FedCSPACK\u901a\u8fc7\u53c2\u6570\u6253\u5305\u3001\u9009\u62e9\u6027\u5171\u4eab\u548c\u53cc\u6743\u91cd\u805a\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u548c\u6709\u9650\u5ba2\u6237\u7aef\u8d44\u6e90\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2601.01804", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01804", "abs": "https://arxiv.org/abs/2601.01804", "authors": ["Zhengjian Kang", "Qi Chen", "Rui Liu", "Kangtong Mo", "Xingyu Zhang", "Xiaoyu Deng", "Ye Zhang"], "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs", "comment": "7 pages, 4 figures", "summary": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.", "AI": {"tldr": "V-CORE\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u65f6\u95f4\u987a\u5e8f\u7ea6\u675f\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u65f6\u95f4\u6392\u5e8f\u548c\u56e0\u679c\u4e00\u81f4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7a7a\u95f4\u805a\u5408\u548c\u56e0\u679c\u611f\u77e5\u65f6\u95f4\u6295\u5f71\u5668\u6765\u786e\u4fdd\u5355\u5411\u4fe1\u606f\u6d41\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u4e00\u81f4\u65f6\u95f4\u6392\u5e8f\u548c\u56e0\u679c\u8fde\u8d2f\u6027\u7684\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u6311\u6218\uff0c\u8bb8\u591a\u53c2\u6570\u9ad8\u6548\u6a21\u578b\u4f7f\u7528\u65e0\u7ea6\u675f\u7684\u53cc\u5411\u6295\u5f71\u5668\u6765\u5efa\u6a21\u5e27\u95f4\u4ea4\u4e92\uff0c\u8fd9\u4f1a\u6a21\u7cca\u65f6\u95f4\u987a\u5e8f\uff0c\u56e0\u4e3a\u5141\u8bb8\u540e\u7eed\u5e27\u5f71\u54cd\u5148\u524d\u8868\u793a\uff0c\u7f3a\u4e4f\u5c0a\u91cd\u89c6\u9891\u63a8\u7406\u65b9\u5411\u6027\u7684\u663e\u5f0f\u67b6\u6784\u673a\u5236\u3002", "method": "V-CORE\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u53ef\u5b66\u4e60\u7a7a\u95f4\u805a\u5408(LSA)\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u663e\u8457\u7a7a\u95f4\u6807\u8bb0\u4ee5\u51cf\u5c11\u5197\u4f59\uff1b2) \u56e0\u679c\u611f\u77e5\u65f6\u95f4\u6295\u5f71\u5668(CATP)\uff0c\u901a\u8fc7\u5757\u56e0\u679c\u6ce8\u610f\u529b\u548c\u4f5c\u4e3a\u56e0\u679c\u6c47\u7684\u7ec8\u7aef\u52a8\u6001\u6458\u8981\u6807\u8bb0\u6765\u5f3a\u5236\u6267\u884c\u7ed3\u6784\u5316\u5355\u5411\u4fe1\u606f\u6d41\uff0c\u5728\u4fdd\u6301\u5e27\u5185\u7a7a\u95f4\u4ea4\u4e92\u7684\u540c\u65f6\u786e\u4fdd\u65f6\u95f4\u4fe1\u606f\u6309\u4e25\u683c\u987a\u5e8f\u805a\u5408\u3002", "result": "V-CORE\u5728\u5177\u6709\u6311\u6218\u6027\u7684NExT-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523061.2%\u7684\u51c6\u786e\u7387\uff0c\u5728MSVD-QA\u3001MSRVTT-QA\u548cTGIF-QA\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5728\u65f6\u95f4\u548c\u56e0\u679c\u63a8\u7406\u5b50\u7c7b\u522b\u4e0a\u5206\u522b\u83b7\u5f97+3.5%\u548c+5.2%\u7684\u589e\u76ca\uff0c\u76f4\u63a5\u9a8c\u8bc1\u4e86\u663e\u5f0f\u65f6\u95f4\u987a\u5e8f\u7ea6\u675f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "V-CORE\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u65f6\u95f4\u987a\u5e8f\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u7406\u89e3\u4e2d\u65f6\u95f4\u6392\u5e8f\u548c\u56e0\u679c\u4e00\u81f4\u6027\u7684\u5173\u952e\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u53c2\u6570\u9ad8\u6548\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5c0a\u91cd\u89c6\u9891\u63a8\u7406\u65b9\u5411\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.01860", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.01860", "abs": "https://arxiv.org/abs/2601.01860", "authors": ["Shuta Kikuchi", "Shu Tanaka"], "title": "High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation", "comment": "6 pages, 2 figures", "summary": "Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u5b50\u5206\u89e3\u673a\u548c\u4e8c\u6b21\u4f18\u5316\u9000\u706b\u7684FMQA\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u68c0\u6d4b\u9ad8\u9636\u4e0a\u4f4d\u6027\u76f8\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u4f20\u7edfMDR\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u591a\u56e0\u5b50\u964d\u7ef4\u65b9\u6cd5\u5728\u68c0\u6d4b\u9ad8\u9636\u4e0a\u4f4d\u6027\u65f6\u9762\u4e34\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7b97\u6cd5\u6765\u8bc6\u522b\u9057\u4f20\u5173\u8054\u7814\u7a76\u4e2d\u7684\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u5c06\u4e0a\u4f4d\u6027\u68c0\u6d4b\u95ee\u9898\u5b9a\u4e49\u4e3a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u56e0\u5b50\u5206\u89e3\u673a\u7ed3\u5408\u4e8c\u6b21\u4f18\u5316\u9000\u706b\u7b97\u6cd5\uff0c\u4ee5MDR\u8ba1\u7b97\u7684\u5206\u7c7b\u9519\u8bef\u7387\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728\u6a21\u62df\u75c5\u4f8b\u5bf9\u7167\u6570\u636e\u96c6\u4e0a\u6210\u529f\u8bc6\u522b\u51fa\u9884\u5b9a\u4e49\u7684\u9ad8\u9636\u4e0a\u4f4d\u6027\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u4e0d\u540c\u4ea4\u4e92\u9636\u6570\u548c\u9057\u4f20\u4f4d\u70b9\u6570\u91cf\u4e0b\u90fd\u80fd\u5728\u6709\u9650\u8fed\u4ee3\u6b21\u6570\u5185\u627e\u5230\u771f\u5b9e\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684FMQA\u65b9\u6cd5\u5bf9\u4e8e\u9ad8\u9636\u4e0a\u4f4d\u6027\u68c0\u6d4b\u65e2\u6709\u6548\u53c8\u8ba1\u7b97\u9ad8\u6548\uff0c\u4e3a\u89e3\u51b3\u9057\u4f20\u5173\u8054\u7814\u7a76\u4e2d\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.01807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01807", "abs": "https://arxiv.org/abs/2601.01807", "authors": ["Ubaidullah", "Muhammad Abid Hussain", "Mohsin Raza Jafri", "Rozi Khan", "Moid Sandhu", "Abd Ullah Khan", "Hyundong Shin"], "title": "Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification", "comment": null, "summary": "Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.", "AI": {"tldr": "LUMPNet\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u7684\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\u65e9\u671f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528YOLOv11\u68c0\u6d4b\u76ae\u80a4\u7ed3\u8282\uff0cEfficientNet\u5206\u7c7b\uff0c\u7ed3\u5408\u65b0\u578b\u81ea\u9002\u5e94\u6df7\u5408\u4f18\u5316\u5668\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u8bad\u7ec3\u51c6\u786e\u7387\u548c98%\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "motivation": "\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\uff08LSD\uff09\u662f\u4e00\u79cd\u4f20\u67d3\u6027\u75c5\u6bd2\u6027\u75be\u75c5\uff0c\u4e25\u91cd\u5a01\u80c1\u755c\u7267\u4e1a\u5065\u5eb7\u548c\u5168\u7403\u7cae\u98df\u5b89\u5168\u3002\u7531\u4e8e\u5176\u5feb\u901f\u4f20\u64ad\u7279\u6027\uff0c\u65e9\u671f\u7cbe\u786e\u8bc6\u522b\u5bf9\u4e8e\u9884\u9632\u75ab\u60c5\u7206\u53d1\u548c\u786e\u4fdd\u53ca\u65f6\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faLUMPNet\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u4f7f\u7528YOLOv11\u68c0\u6d4b\u548c\u5b9a\u4f4d\u725b\u56fe\u50cf\u4e2d\u7684\u76ae\u80a4\u7ed3\u8282\u548c\u75c5\u53d8\uff1b2\uff09\u5229\u7528\u57fa\u4e8eEfficientNet\u7684CNN\u5206\u7c7b\u5668\u5bf9\u5b9a\u4f4d\u540e\u7684\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff08LSD\u611f\u67d3\u6216\u5065\u5eb7\uff09\uff1b3\uff09\u63d0\u51fa\u5e76\u91c7\u7528\u65b0\u578b\u81ea\u9002\u5e94\u6df7\u5408\u4f18\u5316\u5668\u6765\u7a33\u5b9a\u548c\u52a0\u901fYOLOv11\u4e0eEfficientNet\u6df7\u5408\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cLUMPNet\u8fbe\u523099%\u7684LSD\u68c0\u6d4b\u8bad\u7ec3\u51c6\u786e\u7387\u548c98%\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\u4f18\u5316\u540e\u7684EfficientNet-B0\u6a21\u578b\u4e0eAdamW\u4f18\u5316\u5668\uff0cLUMPNet\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "LUMPNet\u4e3a\u725b\u7ed3\u8282\u6027\u76ae\u80a4\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u548c\u5206\u7c7b\u76ae\u80a4\u7ed3\u8282\uff0c\u6709\u52a9\u4e8e\u53ca\u65f6\u5e72\u9884\u548c\u75ab\u60c5\u63a7\u5236\u3002"}}
{"id": "2601.01887", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01887", "abs": "https://arxiv.org/abs/2601.01887", "authors": ["Jiawen Zhang", "Lipeng He", "Kejia Chen", "Jian Lou", "Jian Liu", "Xiaohu Yang", "Ruoxi Jia"], "title": "Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance", "comment": null, "summary": "Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.", "AI": {"tldr": "\u4ec5\u9700\u5355\u4e2a\u5b89\u5168\u6837\u672c\u5373\u53ef\u5b8c\u5168\u6062\u590d\u5b89\u5168\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u727a\u7272\u5b9e\u7528\u6027\uff0c\u6210\u672c\u6781\u4f4e", "motivation": "\u4f20\u7edf\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5b89\u5168\u6837\u672c\u6216\u6821\u51c6\u96c6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u6a21\u578b\u5b9e\u7528\u6027\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u5b89\u5168\u5bf9\u9f50\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u6062\u590d", "method": "\u4f7f\u7528\u5355\u4e2a\u5b89\u5168\u793a\u4f8b\u8fdb\u884c\u5fae\u8c03\u6062\u590d\u5b89\u5168\u5bf9\u9f50\uff0c\u63ed\u793a\u5b89\u5168\u68af\u5ea6\u7684\u4f4e\u79e9\u7ed3\u6784\uff0c\u8bc1\u660e\u9ad8\u6548\u4fee\u6b63\u7684\u53ef\u80fd\u6027", "result": "\u65e0\u8bba\u6709\u5bb3\u793a\u4f8b\u6570\u91cf\u6216\u6a21\u578b\u5927\u5c0f\uff0c\u4ec5\u9700\u5355\u4e2a\u5b89\u5168\u6837\u672c\u5373\u53ef\u5728\u51e0\u4e2aepoch\u5185\u6709\u6548\u6062\u590d\u5b89\u5168\u5bf9\u9f50\uff0c\u5728\u4e94\u4e2a\u5b89\u5168\u5bf9\u9f50LLM\u548c\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027", "conclusion": "\u5b89\u5168\u5bf9\u9f50\u53ef\u4ee5\u901a\u8fc7\u6781\u4f4e\u6210\u672c\u9ad8\u6548\u6062\u590d\uff0c\u5b89\u5168\u68af\u5ea6\u7684\u4f4e\u79e9\u7ed3\u6784\u89e3\u91ca\u4e86\u8fd9\u79cd\u9ad8\u6548\u4fee\u6b63\u7684\u53ef\u80fd\u6027\uff0c\u4e3aLLM\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2601.01818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01818", "abs": "https://arxiv.org/abs/2601.01818", "authors": ["Sungjune Park", "Hongda Mao", "Qingshuang Chen", "Yong Man Ro", "Yelin Kim"], "title": "Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning", "comment": "11 pages, 7 figures, 4 tables", "summary": "As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bed\u8a00\u5f15\u5bfc\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u6ce8\u610f\u529b\u9884\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u6ce8\u610f\u529b\u805a\u7126\u673a\u5236\uff0c\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u5206\u6790\u9700\u6c42\u7684\u589e\u957f\uff0c\u9884\u6d4b\u6444\u50cf\u673a\u4f69\u6234\u8005\u7684\u6ce8\u610f\u529b\u533a\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u4f46\u7531\u4e8e\u7b2c\u4e00\u4eba\u79f0\u573a\u666f\u7684\u52a8\u6001\u6027\u548c\u590d\u6742\u6027\uff0c\u8fd9\u4e00\u4efb\u52a1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7814\u7a76\u8868\u660e\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5728\u8c03\u8282\u4eba\u7c7b\u6ce8\u610f\u529b\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5229\u7528\u573a\u666f\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bed\u8a00\u5f15\u5bfc\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u611f\u77e5\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bed\u8a00\u7684\u573a\u666f\u63cf\u8ff0\u6765\u603b\u7ed3\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89c6\u9891\u8868\u793a\uff1b2\uff09\u5f15\u5165\u4e86\u4e24\u4e2a\u8bad\u7ec3\u76ee\u6807\uff1a\u9f13\u52b1\u6846\u67b6\u805a\u7126\u4e8e\u76ee\u6807\u5174\u8da3\u533a\u57df\uff0c\u540c\u65f6\u6291\u5236\u4e0d\u592a\u53ef\u80fd\u5438\u5f15\u7b2c\u4e00\u4eba\u79f0\u6ce8\u610f\u529b\u7684\u65e0\u5173\u533a\u57df\u7684\u5e72\u6270\u3002", "result": "\u5728Ego4D\u548cAria Everyday Activities (AEA)\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6837\u5316\u3001\u52a8\u6001\u7684\u7b2c\u4e00\u4eba\u79f0\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u8bed\u8a00\u5f15\u5bfc\u573a\u666f\u4e0a\u4e0b\u6587\u611f\u77e5\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u6ce8\u610f\u529b\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u63cf\u8ff0\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u66f4\u597d\u5730\u6a21\u62df\u4eba\u7c7b\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002"}}
{"id": "2601.01901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01901", "abs": "https://arxiv.org/abs/2601.01901", "authors": ["Yuexuan Xia", "Yinghao Zhang", "Yalin Liu", "Hong-Ning Dai", "Yong Xia"], "title": "FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data", "comment": null, "summary": "Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.", "AI": {"tldr": "FedBiCross\u662f\u4e00\u4e2a\u7528\u4e8e\u975eIID\u533b\u7597\u6570\u636e\u7684\u4e2a\u6027\u5316\u5355\u8f6e\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u3001\u53cc\u5c42\u8de8\u96c6\u7fa4\u4f18\u5316\u548c\u4e2a\u6027\u5316\u84b8\u998f\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u975eIID\u6570\u636e\u4e0b\u9884\u6d4b\u51b2\u7a81\u5bfc\u81f4\u76d1\u7763\u4fe1\u53f7\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u8f6e\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u975eIID\u6570\u636e\u4e0b\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u5c06\u6240\u6709\u5ba2\u6237\u7aef\u7684\u9884\u6d4b\u805a\u5408\u5f62\u6210\u5168\u5c40\u6559\u5e08\u6a21\u578b\u65f6\uff0c\u51b2\u7a81\u7684\u9884\u6d4b\u5728\u5e73\u5747\u8fc7\u7a0b\u4e2d\u76f8\u4e92\u62b5\u6d88\uff0c\u4ea7\u751f\u63a5\u8fd1\u5747\u5300\u5206\u5e03\u7684\u8f6f\u6807\u7b7e\uff0c\u5bfc\u81f4\u84b8\u998f\u76d1\u7763\u4fe1\u53f7\u5f31\u3002", "method": "FedBiCross\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u57fa\u4e8e\u6a21\u578b\u8f93\u51fa\u76f8\u4f3c\u6027\u5bf9\u5ba2\u6237\u7aef\u8fdb\u884c\u805a\u7c7b\uff0c\u5f62\u6210\u4e00\u81f4\u7684\u5b50\u96c6\u6210\uff1b2\uff09\u53cc\u5c42\u8de8\u96c6\u7fa4\u4f18\u5316\uff0c\u5b66\u4e60\u81ea\u9002\u5e94\u6743\u91cd\u4ee5\u9009\u62e9\u6027\u5730\u5229\u7528\u6709\u76ca\u7684\u8de8\u96c6\u7fa4\u77e5\u8bc6\uff0c\u540c\u65f6\u6291\u5236\u8d1f\u8fc1\u79fb\uff1b3\uff09\u4e2a\u6027\u5316\u84b8\u998f\u8fdb\u884c\u5ba2\u6237\u7aef\u7279\u5b9a\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u533b\u7597\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedBiCross\u5728\u4e0d\u540c\u975eIID\u7a0b\u5ea6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedBiCross\u901a\u8fc7\u805a\u7c7b\u548c\u9009\u62e9\u6027\u77e5\u8bc6\u8f6c\u79fb\u6709\u6548\u89e3\u51b3\u4e86\u975eIID\u6570\u636e\u4e0b\u5355\u8f6e\u8054\u90a6\u5b66\u4e60\u4e2d\u9884\u6d4b\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u7684\u533b\u7597\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01835", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01835", "abs": "https://arxiv.org/abs/2601.01835", "authors": ["Rashid Iqbal", "Saddam Hussain Khan"], "title": "RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images", "comment": "15 Pages, 7 Figures, 4 Tables", "summary": "In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRSwinV2\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8eMpox\uff08\u7334\u75d8\uff09\u8bca\u65ad\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u7684\u6b8b\u5deeSwinTransformerV2\u67b6\u6784\u589e\u5f3a\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u80fd\u529b\uff0c\u5728Kaggle\u6570\u636e\u96c6\u4e0a\u8fbe\u523096.21%\u51c6\u786e\u7387\u548c95.62% F1\u5206\u6570\u3002", "motivation": "\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u5de5\u5177\u6765\u51c6\u786e\u8bca\u65adMpox\uff08\u7334\u75d8\uff09\u76ae\u80a4\u75c5\u53d8\uff0c\u7279\u522b\u662f\u8981\u533a\u5206Mpox\u4e0e\u5176\u4ed6\u7c7b\u4f3c\u75be\u75c5\u5982\u6c34\u75d8\u3001\u9ebb\u75b9\u548c\u725b\u75d8\uff0c\u4ee5\u5e94\u5bf9\u8bca\u65ad\u6311\u6218\u5e76\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u5b9a\u5236\u5316\u7684RSwinV2\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u8f93\u5165\u7ef4\u5ea6\u3001\u5d4c\u5165\u7ed3\u6784\u548c\u8f93\u51fa\u76ee\u6807\u5b9a\u5236\u5206\u5c42Transformer\u7ed3\u6784\uff1b2\uff09\u5c06\u8f93\u5165\u56fe\u50cf\u5206\u5272\u6210\u975e\u91cd\u53e0\u5757\uff0c\u4f7f\u7528\u79fb\u4f4d\u7a97\u53e3\u548c\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\uff1b3\uff09\u5305\u542b\u8865\u4e01\u548c\u4f4d\u7f6e\u5d4c\u5165\u4ee5\u5229\u7528Transformer\u7684\u5168\u5c40\u94fe\u63a5\u80fd\u529b\uff1b4\uff09\u5f15\u5165\u9006\u6b8b\u5dee\u5757\uff08IRB\uff09\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u7ed3\u5408\u5377\u79ef\u8df3\u8dc3\u8fde\u63a5\uff1b5\uff09\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u5f0f\u94fe\u63a5\u63d0\u9ad8\u75c5\u53d8\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u5728Kaggle\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cRSwinV2\u8fbe\u523096.21%\u7684\u51c6\u786e\u7387\u548c95.62%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u6807\u51c6CNN\u6a21\u578b\u548cSwinTransformers\uff0c\u8bc1\u660e\u5176\u4f5c\u4e3aMpox\u75c5\u53d8\u89c2\u5bdf\u89e3\u91ca\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "conclusion": "RSwinV2\u901a\u8fc7\u7ed3\u5408Transformer\u7684\u5168\u5c40\u94fe\u63a5\u80fd\u529b\u548cIRB\u7684\u5c40\u90e8\u6a21\u5f0f\u5904\u7406\uff0c\u6210\u529f\u63d0\u9ad8\u4e86Mpox\u75c5\u53d8\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u6709\u6548\u533a\u5206Mpox\u3001\u6c34\u75d8\u3001\u9ebb\u75b9\u548c\u725b\u75d8\uff0c\u4e3aMpox\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2601.01847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01847", "abs": "https://arxiv.org/abs/2601.01847", "authors": ["Chuhang Ma", "Shuai Tan", "Ye Pan", "Jiaolong Yang", "Xin Tong"], "title": "ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting", "comment": "13 pages, 10 figures", "summary": "Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.", "AI": {"tldr": "ESGaussianFace\uff1a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u60c5\u611f\u5316\u98ce\u683c\u5316\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u97f3\u9891\u5f15\u5bfc\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\u548c3D\u9ad8\u65af\u53d8\u5f62\u9884\u6d4b\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u60c5\u611f\u98ce\u683c\u5316\u9762\u90e8\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e2d\u6027\u60c5\u611f\u89c6\u9891\u751f\u6210\uff0c\u867d\u7136\u5df2\u6709\u7814\u7a76\u5904\u7406\u60c5\u611f\u97f3\u9891\u9a71\u52a8\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u751f\u6210\u540c\u65f6\u5305\u542b\u60c5\u611f\u8868\u8fbe\u548c\u98ce\u683c\u7279\u5f81\u7684\u9ad8\u8d28\u91cf\u8bf4\u8bdd\u5934\u90e8\u89c6\u9891\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "1. \u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u91cd\u5efa3D\u573a\u666f\u5e76\u6e32\u67d3\u89c6\u9891\uff1b2. \u63d0\u51fa\u60c5\u611f\u97f3\u9891\u5f15\u5bfc\u7684\u7a7a\u95f4\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u6709\u6548\u6574\u5408\u60c5\u611f\u7279\u5f81\u4e0e\u97f3\u9891\u5185\u5bb9\u7279\u5f81\uff1b3. \u5f15\u5165\u4e24\u4e2a3D\u9ad8\u65af\u53d8\u5f62\u9884\u6d4b\u5668\u5b9e\u73b0\u60c5\u611f\u548c\u98ce\u683c\u5316\u53d8\u5f62\uff1b4. \u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u9010\u6b65\u5b66\u4e60\u5634\u5507\u8fd0\u52a8\u3001\u60c5\u611f\u53d8\u5316\u548c\u98ce\u683c\u7279\u5f81\u3002", "result": "\u751f\u6210\u7ed3\u679c\u5177\u6709\u9ad8\u6548\u7387\u3001\u9ad8\u8d28\u91cf\u548c3D\u4e00\u81f4\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5634\u5507\u8fd0\u52a8\u51c6\u786e\u6027\u3001\u8868\u60c5\u53d8\u5316\u548c\u98ce\u683c\u7279\u5f81\u8868\u73b0\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u3002", "conclusion": "ESGaussianFace\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u60c5\u611f\u5316\u98ce\u683c\u5316\u97f3\u9891\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u76843D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u60c5\u611f\u98ce\u683c\u5316\u9762\u90e8\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2601.01904", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01904", "abs": "https://arxiv.org/abs/2601.01904", "authors": ["Yuxuan Li", "Harshith Reddy Kethireddy", "Srijita Das"], "title": "Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning", "comment": null, "summary": "Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.\n  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.\n  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u5f3a\u5316\u5b66\u4e60\u504f\u597d\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u6982\u5ff5\uff0c\u53d1\u73b0\u5728\u67d0\u4e9b\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u8bbe\u7f6e\u4e0b\uff0c\u6700\u5148\u8fdb\u7684\u566a\u58f0\u9c81\u68d2\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u65e0\u663e\u5f0f\u53bb\u566a\u7684\u65b9\u6cd5\u53cd\u800c\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u504f\u597d\u5b66\u4e60\uff08PbRL\uff09\u5728\u5956\u52b1\u51fd\u6570\u4e0d\u6613\u83b7\u5f97\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u504f\u597d\u6570\u636e\u901a\u5e38\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u566a\u58f0\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u68c0\u6d4b\u566a\u58f0\uff0c\u4f46\u566a\u58f0\u7c7b\u578b\u6709\u9650\u4e14\u5927\u591a\u4e0e\u89c2\u6d4b\u65e0\u5173\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u95ee\u9898\u3002", "method": "\u5f62\u5f0f\u5316\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u6982\u5ff5\uff0c\u63d0\u51fa\u591a\u79cd\u53d8\u4f53\uff1a\u8f68\u8ff9\u7279\u5f81\u566a\u58f0\u3001\u8f68\u8ff9\u76f8\u4f3c\u6027\u566a\u58f0\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u566a\u58f0\u548c\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u3002\u5728DMControl\u548cMeta-world\u7684\u590d\u6742\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8bc4\u4f30\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u67d0\u4e9b\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u8bbe\u7f6e\u4e0b\uff0c\u6700\u5148\u8fdb\u7684\u566a\u58f0\u9c81\u68d2PbRL\u65b9\u6cd5\u7684\u5b66\u4e60\u6027\u80fd\u663e\u8457\u6076\u5316\uff0c\u800c\u65e0\u663e\u5f0f\u53bb\u566a\u7684PbRL\u65b9\u6cd5\u5728\u591a\u6570\u8bbe\u7f6e\u4e2d\u610f\u5916\u5730\u4f18\u4e8e\u566a\u58f0\u9c81\u68d2\u65b9\u6cd5\u3002\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u8868\u73b0\u51fa\u4e0e\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u76f8\u4f3c\u7684\u7279\u5f81\u3002", "conclusion": "\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u5bf9\u73b0\u6709\u566a\u58f0\u9c81\u68d2\u65b9\u6cd5\u6784\u6210\u6311\u6218\uff0c\u8bed\u8a00\u6a21\u578b\u566a\u58f0\u6a21\u62df\u4e86\u771f\u5b9e\u4eba\u7c7b\u504f\u597d\u566a\u58f0\u7684\u7279\u5f81\u3002\u7814\u7a76\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u9c81\u68d2\u5730\u5b66\u4e60\u7279\u5f81\u4f9d\u8d56\u6027\u566a\u58f0\u3002"}}
{"id": "2601.01856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01856", "abs": "https://arxiv.org/abs/2601.01856", "authors": ["Joongwon Chae", "Lihui Luo", "Yang Liu", "Runming Wang", "Dongmei Yu", "Zeming Liang", "Xi Yuan", "Dayan Zhang", "Zhenglin Chen", "Peiwu Qin", "Ilmoon Chae"], "title": "GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection", "comment": null, "summary": "Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.\n  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.\n  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR", "AI": {"tldr": "GCR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u4e00\u81f4\u8def\u7531\u7684\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u7a33\u5b9a\u4efb\u52a1\u65e0\u5173\u7684\u6301\u7eed\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u6700\u8fd1\u539f\u578b\u8ddd\u79bb\u8fdb\u884c\u8def\u7531\u51b3\u7b56\uff0c\u907f\u514d\u8de8\u4e13\u5bb6\u5206\u6570\u53ef\u6bd4\u6027\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u57fa\u4e8e\u7279\u5f81\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5df2\u77e5\u7c7b\u522b\u8eab\u4efd\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9700\u8981\u4efb\u52a1\u65e0\u5173\u7684\u6301\u7eed\u7c7b\u522b\u6269\u5c55\u64cd\u4f5c\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u4e13\u5bb6\u8def\u7531\u65f6\u9762\u4e34\u5206\u6570\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8def\u7531\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u4e00\u81f4\u8def\u7531(GCR)\u6846\u67b6\uff1a1\uff09\u5728\u5171\u4eab\u7684\u51bb\u7ed3\u8865\u4e01\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u6784\u5efa\u539f\u578b\u5e93\uff1b2\uff09\u901a\u8fc7\u6700\u5c0f\u5316\u7d2f\u79ef\u6700\u8fd1\u539f\u578b\u8ddd\u79bb\u5c06\u6d4b\u8bd5\u56fe\u50cf\u8def\u7531\u5230\u5408\u9002\u7684\u4e13\u5bb6\uff1b3\uff09\u4ec5\u5728\u8def\u7531\u5230\u7684\u4e13\u5bb6\u5185\u90e8\u4f7f\u7528\u6807\u51c6\u57fa\u4e8e\u539f\u578b\u7684\u8bc4\u5206\u89c4\u5219\u8ba1\u7b97\u5f02\u5e38\u56fe\u3002", "result": "\u5728MVTec AD\u548cVisA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u51e0\u4f55\u4e00\u81f4\u8def\u7531\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u7531\u7a33\u5b9a\u6027\uff0c\u7f13\u89e3\u4e86\u6301\u7eed\u6027\u80fd\u5d29\u6e83\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u96f6\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\u3002", "conclusion": "\u8bb8\u591a\u5148\u524d\u5f52\u56e0\u4e8e\u8868\u793a\u9057\u5fd8\u7684\u5931\u8d25\u5b9e\u9645\u4e0a\u53ef\u4ee5\u89e3\u91ca\u4e3a\u8de8\u4e13\u5bb6\u8def\u7531\u4e2d\u51b3\u7b56\u89c4\u5219\u7684\u4e0d\u7a33\u5b9a\u6027\u3002GCR\u901a\u8fc7\u5206\u79bb\u8de8\u4e13\u5bb6\u51b3\u7b56\u548c\u4e13\u5bb6\u5185\u5f02\u5e38\u8bc4\u5206\uff0c\u65e0\u9700\u7aef\u5230\u7aef\u8868\u793a\u5b66\u4e60\u5373\u53ef\u89e3\u51b3\u8def\u7531\u7a33\u5b9a\u6027\u95ee\u9898\u3002"}}
{"id": "2601.01917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01917", "abs": "https://arxiv.org/abs/2601.01917", "authors": ["Ryo Iwaki", "Takayuki Osogami"], "title": "Distorted Distributional Policy Evaluation for Offline Reinforcement Learning", "comment": "The preprint version of the paper accepted to ICONIP2025. The Version of Record is available online at https://link.springer.com/chapter/10.1007/978-981-95-4091-4_35", "summary": "While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5206\u4f4d\u6570\u626d\u66f2\u6982\u5ff5\u5b9e\u73b0\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u5747\u5300\u4f4e\u4f30\u56de\u62a5\u5206\u4f4d\u6570\u800c\u5bfc\u81f4\u7684\u8fc7\u5ea6\u4fdd\u5b88\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u79bb\u7ebf\u573a\u666f\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u5b83\u4eec\u91c7\u7528\u5747\u5300\u4f4e\u4f30\u56de\u62a5\u5206\u4f4d\u6570\u7684\u65b9\u6cd5\uff0c\u8fd9\u79cd\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u4f1a\u5bfc\u81f4\u8fc7\u4e8e\u4fdd\u5b88\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u4ece\u800c\u963b\u788d\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u63d0\u5347\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u6982\u5ff5\u2014\u2014\u5206\u4f4d\u6570\u626d\u66f2\uff0c\u901a\u8fc7\u57fa\u4e8e\u652f\u6301\u6570\u636e\u7684\u53ef\u7528\u6027\u8c03\u6574\u4fdd\u5b88\u7a0b\u5ea6\uff0c\u5b9e\u73b0\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7406\u8bba\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u65b9\u6cd5\u6709\u66f4\u597d\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5206\u4f4d\u6570\u626d\u66f2\u5b9e\u73b0\u975e\u5747\u5300\u60b2\u89c2\u4e3b\u4e49\u662f\u89e3\u51b3\u79bb\u7ebf\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u4e2d\u8fc7\u5ea6\u4fdd\u5b88\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u5347\u79bb\u7ebf\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2601.01865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01865", "abs": "https://arxiv.org/abs/2601.01865", "authors": ["Wenlong Yang", "Canran Jin", "Weihang Yuan", "Chao Wang", "Lifeng Sun"], "title": "RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations", "comment": null, "summary": "With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.", "AI": {"tldr": "RRNet\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53ef\u914d\u7f6e\u7684\u5b9e\u65f6\u89c6\u9891\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1\u865a\u62df\u5149\u6e90\u53c2\u6570\u5b9e\u73b0\u5c40\u90e8\u91cd\u7167\u660e\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u5b9e\u65f6\u89c6\u9891\u589e\u5f3a\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u6709\u6548\u66dd\u5149\u63a7\u5236\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u4e0d\u5747\u5300\u5149\u7167\u6761\u4ef6\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u53c8\u80fd\u5b9e\u65f6\u8fd0\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRRNet\u6846\u67b6\uff1a1) \u4f30\u8ba1\u5c11\u91cf\u865a\u62df\u5149\u6e90\u53c2\u6570\uff1b2) \u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u6e32\u67d3\u6a21\u5757\u5b9e\u73b0\u5c40\u90e8\u91cd\u7167\u660e\uff1b3) \u4f7f\u7528\u6d41\u7ebf\u578b\u7f16\u7801\u5668\u548c\u8f7b\u91cf\u7ea7\u9884\u6d4b\u5934\uff1b4) \u91c7\u7528\u751f\u6210\u5f0fAI\u521b\u5efa\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "RRNet\u5728\u4f4e\u5149\u589e\u5f3a\u3001\u5c40\u90e8\u5149\u7167\u8c03\u6574\u548c\u7729\u5149\u53bb\u9664\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u5904\u7406\uff0c\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u7279\u5f81\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u7684\u5149\u7167\u63a7\u5236\u3002", "conclusion": "RRNet\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u53ef\u914d\u7f6e\u8bbe\u8ba1\uff0c\u5728\u89c6\u9891\u4f1a\u8bae\u3001AR\u4eba\u50cf\u589e\u5f3a\u548c\u79fb\u52a8\u6444\u5f71\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2601.01870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01870", "abs": "https://arxiv.org/abs/2601.01870", "authors": ["Wenyu Shao", "Hongbo Liu", "Yunchuan Ma", "Ruili Wang"], "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.", "AI": {"tldr": "\u63d0\u51faEGMT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f53\u5f15\u5bfc\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u8fdb\u884c\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\uff0c\u5229\u7528\u5b9e\u4f53\u7ea7\u6587\u672c\u4fe1\u606f\u6d88\u9664\u8bed\u4e49\u566a\u58f0\uff0c\u63d0\u5347\u878d\u5408\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u5bc6\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u53e5\u5b50\u7ea7\u6587\u672c\u4fe1\u606f\uff0c\u5b58\u5728\u8bed\u4e49\u566a\u58f0\u95ee\u9898\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u6587\u672c\u7684\u6df1\u5c42\u8bed\u4e49\u4ef7\u503c\u3002", "method": "\u63d0\u51faEGMT\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u7ec4\u4ef6\uff1a1) \u4ece\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u4e2d\u63d0\u53d6\u5b9e\u4f53\u7ea7\u6587\u672c\u4fe1\u606f\uff1b2) \u6784\u5efa\u5e76\u884c\u591a\u4efb\u52a1\u5b66\u4e60\u67b6\u6784\uff0c\u5c06\u56fe\u50cf\u878d\u5408\u4e0e\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u7ed3\u5408\uff1b3) \u5f00\u53d1\u5b9e\u4f53\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u5757\uff0c\u4fc3\u8fdb\u89c6\u89c9\u4e0e\u5b9e\u4f53\u7ea7\u6587\u672c\u7279\u5f81\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u3002", "result": "\u5728TNO\u3001RoadScene\u3001M3FD\u548cMSRS\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eEGMT\u5728\u4fdd\u7559\u663e\u8457\u76ee\u6807\u3001\u7eb9\u7406\u7ec6\u8282\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "EGMT\u65b9\u6cd5\u901a\u8fc7\u5b9e\u4f53\u5f15\u5bfc\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u7684\u8d28\u91cf\uff0c\u5e76\u53d1\u5e03\u4e86\u5b9e\u4f53\u6807\u6ce8\u7248\u672c\u7684\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8be5\u6846\u67b6\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2601.01931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01931", "abs": "https://arxiv.org/abs/2601.01931", "authors": ["Willem R\u00f6pke", "Samuel Coward", "Andrei Lupu", "Thomas Foster", "Tim Rockt\u00e4schel", "Jakob Foerster"], "title": "D\u00e9j\u00e0Q: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems", "comment": null, "summary": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce D\u00e9j\u00e0Q, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of D\u00e9j\u00e0Q, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.", "AI": {"tldr": "D\u00e9j\u00e0Q\u6846\u67b6\u901a\u8fc7\u8fdb\u5316\u5408\u6210\u6570\u5b66\u95ee\u9898\u6765\u589e\u5f3a\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\uff0c\u907f\u514d\u9759\u6001\u6570\u636e\u96c6\u5bfc\u81f4\u7684\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u7684\u53d8\u5f02\u7b56\u7565\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u6570\u636e", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u5927\u591a\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8bb0\u5fc6\u5316\u800c\u975e\u6cdb\u5316\u80fd\u529b\u7684\u63d0\u5347\uff0c\u9650\u5236\u4e86\u6570\u5b66\u63a8\u7406\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55", "method": "\u63d0\u51faD\u00e9j\u00e0Q\u6846\u67b6\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8054\u5408\u8fdb\u5316\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u5b66\u95ee\u9898\u96c6\uff0c\u91c7\u7528\u4e24\u79cdLLM\u9a71\u52a8\u7684\u53d8\u5f02\u7b56\u7565\uff1a\u6539\u53d8\u4e0a\u4e0b\u6587\u7ec6\u8282\u6216\u76f4\u63a5\u4fee\u6539\u95ee\u9898\u7ed3\u6784", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u65b0\u9896\u4e14\u6709\u610f\u4e49\u7684\u6570\u5b66\u95ee\u9898\uff0cLLM\u9a71\u52a8\u7684\u53d8\u5f02\u7b56\u7565\u6539\u5584\u4e86\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u679c\uff0c\u751f\u6210\u7684\u6570\u5b66\u95ee\u9898\u5177\u6709\u6709\u6548\u6027\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u63a7", "conclusion": "\u52a8\u6001\u8fdb\u5316\u8bad\u7ec3\u6570\u636e\u80fd\u6709\u6548\u589e\u5f3a\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u4f5c\u8005\u5c06\u5f00\u6e90\u4ee3\u7801\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76"}}
{"id": "2601.01874", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01874", "abs": "https://arxiv.org/abs/2601.01874", "authors": ["Shuhang Chen", "Yunqiu Xu", "Junjie Xie", "Aojun Lu", "Tao Feng", "Zeying Huang", "Ning Zhang", "Yi Sun", "Yi Yang", "Hangjie Yuan"], "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving", "comment": null, "summary": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.", "AI": {"tldr": "CogFlow\u662f\u4e00\u4e2a\u53d7\u8ba4\u77e5\u542f\u53d1\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u2192\u5185\u5316\u2192\u63a8\u7406\u7684\u5c42\u6b21\u6d41\u7a0b\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u89c6\u89c9\u611f\u77e5\u74f6\u9888\u548c\u89c6\u89c9\u7ebf\u7d22\u6574\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u867d\u7136\u4e00\u4e9b\u5de5\u4f5c\u8ba4\u8bc6\u5230\u89c6\u89c9\u611f\u77e5\u662f\u74f6\u9888\uff0c\u4f46\u89e3\u51b3\u65b9\u6848\u4ec5\u9650\u4e8e\u6539\u8fdb\u89c6\u89c9\u8f93\u5165\u7684\u63d0\u53d6\u548c\u89e3\u91ca\uff0c\u5ffd\u7565\u4e86\u63d0\u53d6\u7684\u89c6\u89c9\u7ebf\u7d22\u662f\u5426\u88ab\u5fe0\u5b9e\u6574\u5408\u5e76\u6b63\u786e\u7528\u4e8e\u540e\u7eed\u63a8\u7406\u7684\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faCogFlow\u8ba4\u77e5\u542f\u53d1\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u611f\u77e5\u9636\u6bb5\uff1a\u8bbe\u8ba1\u534f\u540c\u89c6\u89c9\u5956\u52b1\uff0c\u5728\u53c2\u6570\u548c\u8bed\u4e49\u7a7a\u95f4\u4e2d\u63d0\u5347\u611f\u77e5\u80fd\u529b\uff1b2\uff09\u5185\u5316\u9636\u6bb5\uff1a\u5f15\u5165\u77e5\u8bc6\u5185\u5316\u5956\u52b1\u6a21\u578b\uff0c\u8fde\u63a5\u611f\u77e5\u548c\u63a8\u7406\uff1b3\uff09\u63a8\u7406\u9636\u6bb5\uff1a\u8bbe\u8ba1\u89c6\u89c9\u95e8\u63a7\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u786e\u4fdd\u63a8\u7406\u57fa\u4e8e\u89c6\u89c9\u77e5\u8bc6\u3002\u540c\u65f6\u8d21\u732e\u4e86\u5305\u542b12\u4e07+\u9ad8\u8d28\u91cf\u611f\u77e5-\u63a8\u7406\u5bf9\u9f50\u6807\u6ce8\u7684MathCog\u6570\u636e\u96c6\u3002", "result": "\u5728\u5e38\u7528\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u548c\u5206\u6790\u9a8c\u8bc1\u4e86CogFlow\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "CogFlow\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u7684\u5c42\u6b21\u6d41\u7a0b\uff0c\u5168\u9762\u589e\u5f3a\u611f\u77e5\u3001\u5185\u5316\u548c\u63a8\u7406\u9636\u6bb5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u89c6\u89c9\u611f\u77e5\u74f6\u9888\u548c\u89c6\u89c9\u7ebf\u7d22\u6574\u5408\u95ee\u9898\u3002"}}
{"id": "2601.01943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01943", "abs": "https://arxiv.org/abs/2601.01943", "authors": ["Tieu-Long Phan", "Nhu-Ngoc Nguyen Song", "Peter F. Stadler"], "title": "SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling", "comment": "31 pages (including references), 3 figures, 7 tables", "summary": "We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.", "AI": {"tldr": "SynRXN\u662f\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u548c\u5f00\u653e\u6570\u636e\u8d44\u6e90\uff0c\u5c06\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u4e94\u4e2a\u4efb\u52a1\u65cf\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6d41\u7a0b\u548c\u53ef\u590d\u73b0\u7684\u6784\u5efa\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6570\u636e\u96c6\u5f02\u6784\u4e14\u8bc4\u4f30\u6807\u51c6\u4e0d\u7edf\u4e00\uff0c\u96be\u4ee5\u8fdb\u884c\u516c\u5e73\u7684\u65b9\u6cd5\u6bd4\u8f83\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u5c06\u7aef\u5230\u7aef\u5408\u6210\u89c4\u5212\u5206\u89e3\u4e3a\u4e94\u4e2a\u4efb\u52a1\u65cf\uff1a\u53cd\u5e94\u5e73\u8861\u3001\u539f\u5b50\u6620\u5c04\u3001\u53cd\u5e94\u5206\u7c7b\u3001\u53cd\u5e94\u6027\u8d28\u9884\u6d4b\u548c\u5408\u6210\u8def\u7ebf\u8bbe\u8ba1\uff1b\u4ece\u5f02\u6784\u516c\u5171\u6765\u6e90\u6536\u96c6\u53cd\u5e94\u6570\u636e\u5e76\u7edf\u4e00\u8868\u793a\uff1b\u63d0\u4f9b\u900f\u660e\u7684\u6570\u636e\u5206\u5272\u51fd\u6570\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7a0b\u548c\u6307\u6807\u5957\u4ef6\uff1b\u91c7\u7528\u811a\u672c\u5316\u6784\u5efa\u65b9\u6cd5\u786e\u4fdd\u53ef\u590d\u73b0\u6027\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7248\u672c\u5316\u6570\u636e\u96c6\u3001\u660e\u786e\u5143\u6570\u636e\u3001\u8bb8\u53ef\u6807\u7b7e\u548c\u673a\u5668\u53ef\u8bfb\u6e05\u5355\u7684\u8d44\u6e90\uff1b\u63d0\u4f9b\u4e86\u6cc4\u6f0f\u611f\u77e5\u7684\u6570\u636e\u5206\u5272\u548c\u654f\u611f\u4efb\u52a1\u7684\u8bc4\u4f30\u96c6\uff1b\u652f\u6301\u516c\u5e73\u7684\u7eb5\u5411\u65b9\u6cd5\u6bd4\u8f83\u548c\u4e25\u8c28\u7684\u6d88\u878d\u5b9e\u9a8c\u3002", "conclusion": "SynRXN\u901a\u8fc7\u6d88\u9664\u6570\u636e\u96c6\u5f02\u8d28\u6027\u5e76\u63d0\u4f9b\u900f\u660e\u53ef\u91cd\u7528\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u65b9\u6cd5\u7684\u516c\u5e73\u6bd4\u8f83\uff0c\u964d\u4f4e\u4e86\u4ece\u4e1a\u8005\u83b7\u53d6\u7a33\u5065\u53ef\u6bd4\u6027\u80fd\u8bc4\u4f30\u7684\u95e8\u69db\u3002"}}
{"id": "2601.01891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01891", "abs": "https://arxiv.org/abs/2601.01891", "authors": ["Niloufar Alipour Talemi", "Julia Boone", "Fatemeh Afghah"], "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems", "comment": "Accepted to the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, GeoCV Workshop", "summary": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u9065\u611f\u9886\u57df\u4e2d\u7684\u667a\u80fd\u4f53AI\uff0c\u63d0\u51fa\u7edf\u4e00\u5206\u7c7b\u4f53\u7cfb\uff0c\u5206\u6790\u67b6\u6784\u57fa\u7840\uff0c\u5e76\u5c55\u671b\u81ea\u4e3b\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u5206\u6790\u8303\u5f0f\u6b63\u4ece\u9759\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8f6c\u5411\u81ea\u4e3b\u667a\u80fd\u4f53AI\u3002\u5c3d\u7ba1\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8868\u793a\u5b66\u4e60\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u590d\u6742\u5730\u7406\u7a7a\u95f4\u5de5\u4f5c\u6d41\u6240\u9700\u7684\u5e8f\u5217\u89c4\u5212\u548c\u5de5\u5177\u7f16\u6392\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8be5\u65b0\u5174\u9886\u57df\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u63d0\u51fa\u7edf\u4e00\u5206\u7c7b\u4f53\u7cfb\u533a\u5206\u5355\u667a\u80fd\u4f53\u534f\u540c\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5206\u6790\u89c4\u5212\u673a\u5236\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u8bb0\u5fc6\u7ed3\u6784\u7b49\u67b6\u6784\u57fa\u7840\uff0c\u5e76\u8bc4\u4f30\u4ece\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u5230\u8f68\u8ff9\u611f\u77e5\u63a8\u7406\u6b63\u786e\u6027\u7684\u65b0\u5174\u57fa\u51c6\u3002", "result": "\u5efa\u7acb\u4e86\u9065\u611f\u9886\u57df\u667a\u80fd\u4f53AI\u7684\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u67b6\u6784\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8bc6\u522b\u4e86\u5728\u57fa\u7840\u3001\u5b89\u5168\u6027\u548c\u7f16\u6392\u65b9\u9762\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u9c81\u68d2\u3001\u81ea\u4e3b\u7684\u5730\u7406\u7a7a\u95f4\u667a\u80fd\u63d0\u4f9b\u4e86\u6218\u7565\u8def\u7ebf\u56fe\uff0c\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u63a8\u52a8\u5730\u7403\u89c2\u6d4b\u5206\u6790\u5411\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2601.01966", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01966", "abs": "https://arxiv.org/abs/2601.01966", "authors": ["Bo Yin", "Qi Li", "Runpeng Yu", "Xinchao Wang"], "title": "Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior", "comment": null, "summary": "Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRePro\u6846\u67b6\uff0c\u7528\u4e8e\u63a8\u65ad\u6307\u4ee4\u8c03\u4f18\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u793a\u662f\u5426\u7ecf\u8fc7LLM\u6539\u5199\uff0c\u89e3\u51b3\u6570\u636e\u96c6\u6cbb\u7406\u548c\u4e89\u8bae\u4e2d\u7684\u5b9e\u4f8b\u7ea7\u5ba1\u8ba1\u95ee\u9898\u3002", "motivation": "\u6307\u4ee4\u8c03\u4f18\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56LLM\u8fdb\u884c\u63d0\u793a\u6539\u5199\uff0c\u8fd9\u5f15\u53d1\u4e86\u5b9e\u4f8b\u7ea7\u5ba1\u8ba1\u9700\u6c42\uff1a\u5bf9\u4e8e\u5fae\u8c03\u6a21\u578b\u548c\u8bad\u7ec3\u63d0\u793a-\u54cd\u5e94\u5bf9\uff0c\u80fd\u5426\u63a8\u65ad\u6a21\u578b\u662f\u5728\u539f\u59cb\u63d0\u793a\u8fd8\u662f\u5176LLM\u6539\u5199\u7248\u672c\u4e0a\u8bad\u7ec3\u7684\uff1f\u8fd9\u5bf9\u6570\u636e\u96c6\u6cbb\u7406\u548c\u8bad\u7ec3\u6570\u636e\u4e89\u8bae\u89e3\u51b3\u5f88\u91cd\u8981\u3002", "method": "\u5c06\u5ba1\u8ba1\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3aRefinement Provenance Inference (RPI)\uff0c\u63d0\u51faRePro\u6846\u67b6\uff1a\u5229\u7528\u63d0\u793a\u6539\u5199\u5bfc\u81f4\u7684\u6559\u5e08\u5f3a\u5236token\u5206\u5e03\u7a33\u5b9a\u504f\u79fb\uff0c\u878d\u5408\u6559\u5e08\u5f3a\u5236\u4f3c\u7136\u7279\u5f81\u548clogit\u6392\u5e8f\u4fe1\u53f7\uff0c\u901a\u8fc7\u5f71\u5b50\u5fae\u8c03\u5b66\u4e60\u53ef\u8fc1\u79fb\u8868\u793a\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5934\u8fdb\u884c\u63a8\u65ad\u3002", "result": "RePro\u5728\u5b9e\u8bc1\u4e2d\u8868\u73b0\u5f3a\u52b2\u4e14\u80fd\u5f88\u597d\u5730\u8de8\u6539\u5199\u5668\u8fc1\u79fb\uff0c\u8868\u660e\u5176\u5229\u7528\u4e86\u6539\u5199\u5668\u65e0\u5173\u7684\u5206\u5e03\u504f\u79fb\u800c\u975e\u6539\u5199\u98ce\u683c\u4f2a\u5f71\u3002", "conclusion": "\u63d0\u793a\u6539\u5199\u4f1a\u4ea7\u751f\u53ef\u68c0\u6d4b\u7684\u5206\u5e03\u504f\u79fb\uff0cRePro\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u6307\u4ee4\u8c03\u4f18\u8bad\u7ec3\u6570\u636e\u7684\u5b9e\u4f8b\u7ea7\u5ba1\u8ba1\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.01892", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01892", "abs": "https://arxiv.org/abs/2601.01892", "authors": ["Arjun Ramesh Kaushik", "Naresh Kumar Devulapally", "Vishnu Suresh Lokhande", "Nalini K. Ratha", "Venu Govindaraju"], "title": "Forget Less by Learning from Parents Through Hierarchical Relationships", "comment": "Accepted at AAAI-26", "summary": "Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.", "AI": {"tldr": "FLLP\u6846\u67b6\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u7236\u5b50\u6982\u5ff5\u5b66\u4e60\u673a\u5236\uff0c\u89e3\u51b3\u5b9a\u5236\u6269\u6563\u6a21\u578b\u5728\u987a\u5e8f\u5b66\u4e60\u65b0\u6982\u5ff5\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898", "motivation": "\u5b9a\u5236\u6269\u6563\u6a21\u578b\u5728\u987a\u5e8f\u5b66\u4e60\u65b0\u6982\u5ff5\u65f6\u5bb9\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6700\u5c0f\u5316\u6982\u5ff5\u95f4\u5e72\u6270\uff0c\u4f46\u5ffd\u89c6\u4e86\u6982\u5ff5\u95f4\u6f5c\u5728\u7684\u79ef\u6781\u4ea4\u4e92\u4f5c\u7528", "method": "\u63d0\u51faFLLP\u6846\u67b6\uff0c\u5728\u6d1b\u4f26\u5179\u6d41\u5f62\uff08\u53cc\u66f2\u7a7a\u95f4\uff09\u4e2d\u5f15\u5165\u7236\u5b50\u6982\u5ff5\u5b66\u4e60\u673a\u5236\uff0c\u5c06\u5148\u524d\u5b66\u4e60\u7684\u6982\u5ff5\u4f5c\u4e3a\u7236\u6982\u5ff5\u6307\u5bfc\u65b0\u6982\u5ff5\u7684\u5b66\u4e60\uff0c\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u81ea\u7136\u9002\u5408\u5efa\u6a21\u6811\u72b6\u5c42\u6b21\u7ed3\u6784\u7684\u7279\u6027", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5408\u6210\u57fa\u51c6\u4e0a\u9a8c\u8bc1\uff0cFLLP\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6301\u7eed\u6539\u8fdb", "conclusion": "FLLP\u901a\u8fc7\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u7236\u5b50\u6982\u5ff5\u5b66\u4e60\u673a\u5236\uff0c\u4e0d\u4ec5\u80fd\u591f\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd8\u652f\u6301\u65b0\u6982\u5ff5\u7684\u6301\u7eed\u96c6\u6210\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5b9a\u5236\u6269\u6563\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898"}}
{"id": "2601.01979", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.01979", "abs": "https://arxiv.org/abs/2601.01979", "authors": ["Julie Keisler", "Anastase Alexandre Charantonis", "Yannig Goude", "Boutheina Oueslati", "Claire Monteleoni"], "title": "SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition", "comment": null, "summary": "Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.", "AI": {"tldr": "SerpentFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u7684\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5206\u89e3\u4e3a\u5171\u4eab\u7ed3\u6784\u548c\u57df\u7279\u5b9a\u7ec4\u4ef6\uff0c\u5229\u7528\u5171\u4eab\u8868\u793a\u4e0e\u76ee\u6807\u57df\u6837\u672c\u6784\u5efa\u5408\u6210\u8bad\u7ec3\u5bf9\uff0c\u5b9e\u73b0\u6761\u4ef6\u751f\u6210\u6a21\u578b\u5728\u65e0\u914d\u5bf9\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002", "motivation": "\u5728\u65e0\u914d\u5bf9\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u57df\u5bf9\u9f50\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8de8\u57df\u7684\u76f4\u63a5\u76d1\u7763\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5171\u4eab\u7ed3\u6784\u6a21\u5f0f\u4f46\u5177\u4f53\u5b9e\u73b0\u4e0d\u540c\u7684\u57df\u4e4b\u95f4\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "method": "SerpentFlow\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5c06\u6570\u636e\u5206\u89e3\u4e3a\u5171\u4eab\u7ec4\u4ef6\u548c\u57df\u7279\u5b9a\u7ec4\u4ef6\u3002\u901a\u8fc7\u9694\u79bb\u5171\u4eab\u7ed3\u6784\u5e76\u7528\u968f\u673a\u566a\u58f0\u66ff\u6362\u57df\u7279\u5b9a\u7ec4\u4ef6\uff0c\u6784\u5efa\u5171\u4eab\u8868\u793a\u4e0e\u76ee\u6807\u57df\u6837\u672c\u4e4b\u95f4\u7684\u5408\u6210\u8bad\u7ec3\u5bf9\uff0c\u4ece\u800c\u652f\u6301\u6761\u4ef6\u751f\u6210\u6a21\u578b\u7684\u5e94\u7528\u3002\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\uff0c\u5171\u4eab\u7ec4\u4ef6\u5bf9\u5e94\u4f4e\u9891\u5185\u5bb9\uff0c\u9ad8\u9891\u7ec6\u8282\u6355\u83b7\u57df\u7279\u5b9a\u53d8\u5f02\u6027\uff0c\u4f7f\u7528\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u51c6\u5219\u81ea\u52a8\u786e\u5b9a\u5206\u79bb\u9891\u7387\u3002", "result": "\u5728\u5408\u6210\u56fe\u50cf\u3001\u7269\u7406\u8fc7\u7a0b\u6a21\u62df\u548c\u6c14\u5019\u964d\u5c3a\u5ea6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u91cd\u5efa\u4e0e\u5e95\u5c42\u4f4e\u9891\u6a21\u5f0f\u4e00\u81f4\u7684\u9ad8\u9891\u7ed3\u6784\uff0c\u652f\u6301\u5171\u4eab\u7ed3\u6784\u5206\u89e3\u4f5c\u4e3a\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u7684\u6709\u6548\u7b56\u7565\u3002", "conclusion": "\u5171\u4eab\u7ed3\u6784\u5206\u89e3\u4e3a\u65e0\u914d\u5bf9\u57df\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5408\u6210\u8bad\u7ec3\u5bf9\u4f7f\u4f20\u7edf\u9700\u8981\u914d\u5bf9\u6570\u636e\u7684\u6761\u4ef6\u751f\u6210\u6a21\u578b\u80fd\u591f\u5728\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u5e94\u7528\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.01908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01908", "abs": "https://arxiv.org/abs/2601.01908", "authors": ["Jingjing Wang", "Qianglin Liu", "Zhuo Xiao", "Xinning Yao", "Bo Liu", "Lu Li", "Lijuan Niu", "Fugen Zhou"], "title": "Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection", "comment": null, "summary": "Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.", "AI": {"tldr": "Nodule-DETR\u662f\u4e00\u79cd\u57fa\u4e8e\u68c0\u6d4b\u53d8\u6362\u5668\u7684\u65b0\u578b\u7532\u72b6\u817a\u7ed3\u8282\u68c0\u6d4b\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u5149\u8c31\u9891\u57df\u901a\u9053\u6ce8\u610f\u529b\u3001\u5206\u5c42\u7279\u5f81\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u58f0\u56fe\u50cf\u4e2d\u7532\u72b6\u817a\u7ed3\u8282\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7532\u72b6\u817a\u764c\u662f\u5168\u7403\u6700\u5e38\u89c1\u7684\u5185\u5206\u6ccc\u6076\u6027\u80bf\u7624\uff0c\u53d1\u75c5\u7387\u4e0d\u65ad\u4e0a\u5347\u3002\u8d85\u58f0\u662f\u68c0\u6d4b\u7532\u72b6\u817a\u7ed3\u8282\u7684\u9996\u9009\u6210\u50cf\u65b9\u5f0f\uff0c\u4f46\u5176\u8bca\u65ad\u51c6\u786e\u6027\u5e38\u53d7\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u4f4e\u548c\u7ed3\u8282\u8fb9\u754c\u6a21\u7cca\u7b49\u6311\u6218\u9650\u5236\u3002", "method": "\u63d0\u51faNodule-DETR\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u591a\u5149\u8c31\u9891\u57df\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5229\u7528\u9891\u7387\u5206\u6790\u589e\u5f3a\u4f4e\u5bf9\u6bd4\u5ea6\u7ed3\u8282\u7279\u5f81\uff1b2\uff09\u5206\u5c42\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u5b9e\u73b0\u9ad8\u6548\u591a\u5c3a\u5ea6\u7279\u5f81\u96c6\u6210\uff1b3\uff09\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u7075\u6d3b\u6355\u6349\u5c0f\u800c\u4e0d\u89c4\u5219\u5f62\u72b6\u7684\u7ed3\u8282\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7532\u72b6\u817a\u8d85\u58f0\u56fe\u50cf\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNodule-DETR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728mAP@0.5:0.95\u6307\u6807\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e860.149\u3002", "conclusion": "Nodule-DETR\u7684\u4f18\u8d8a\u51c6\u786e\u6027\u7a81\u663e\u4e86\u5176\u4f5c\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u7532\u72b6\u817a\u8bca\u65ad\u6709\u6548\u5de5\u5177\u7684\u91cd\u8981\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.02022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02022", "abs": "https://arxiv.org/abs/2601.02022", "authors": ["Yifan Zhu", "John C. Duchi", "Benjamin Van Roy"], "title": "Prior Diffusiveness and Regret in the Linear-Gaussian Bandit", "comment": null, "summary": "We prove that Thompson sampling exhibits $\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\\mathcal{N}(\u03bc_0, \u03a3_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\\ell_2$ norm of the actions, and $\u03c3^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \\sqrt{\\mathrm{Tr}(\u03a3_0)}$ decouples additively from the minimax (long run) regret $\u03c3d \\sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65af\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\u4e2d\u5177\u6709$\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$\u7684\u8d1d\u53f6\u65af\u9057\u61be\u754c\uff0c\u5176\u4e2d\u5148\u9a8c\u4f9d\u8d56\u7684\"\u9884\u70ed\"\u9879\u4e0e\u6781\u5c0f\u6781\u5927\u9057\u61be\u9879\u5448\u52a0\u6cd5\u5206\u79bb\u800c\u975e\u4e58\u6cd5\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65af\u8d4c\u535a\u673a\u4e2d\u7684\u9057\u61be\u754c\u4e2d\uff0c\u5148\u9a8c\u4f9d\u8d56\u7684\"\u9884\u70ed\"\u9879\u4e0e\u957f\u671f\u6781\u5c0f\u6781\u5927\u9057\u61be\u9879\u5448\u4e58\u6cd5\u5173\u7cfb\u3002\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u4e24\u4e2a\u9879\u5b9e\u9645\u4e0a\u53ef\u4ee5\u52a0\u6cd5\u5206\u79bb\uff0c\u4ece\u800c\u66f4\u7cbe\u786e\u5730\u523b\u753b\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u65b0\u7684\"\u692d\u5706\u52bf\u80fd\"\u5f15\u7406\u6765\u5206\u6790Thompson\u91c7\u6837\u7684\u9057\u61be\u754c\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5206\u79bb\u5148\u9a8c\u4f9d\u8d56\u9879\u548c\u957f\u671f\u9057\u61be\u9879\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u4e0b\u754c\u8bc1\u660e\u6765\u8868\u660e\u9884\u70ed\u9879\u662f\u4e0d\u53ef\u907f\u514d\u7684\u3002", "result": "\u8bc1\u660e\u4e86Thompson\u91c7\u6837\u5177\u6709$\\tilde{O}(\u03c3d \\sqrt{T} + d r \\sqrt{\\mathrm{Tr}(\u03a3_0)})$\u7684\u8d1d\u53f6\u65af\u9057\u61be\u754c\uff0c\u5176\u4e2d$d$\u662f\u7ef4\u5ea6\uff0c$T$\u662f\u65f6\u95f4\u8303\u56f4\uff0c$r$\u662f\u52a8\u4f5c\u7684\u6700\u5927$\\ell_2$\u8303\u6570\uff0c$\u03c3^2$\u662f\u566a\u58f0\u65b9\u5dee\u3002\u5148\u9a8c\u4f9d\u8d56\u9879\u4e0e\u957f\u671f\u9057\u61be\u9879\u5448\u52a0\u6cd5\u5173\u7cfb\u800c\u975e\u4e58\u6cd5\u5173\u7cfb\u3002", "conclusion": "Thompson\u91c7\u6837\u5728\u7ebf\u6027\u9ad8\u65af\u8d4c\u535a\u673a\u4e2d\u7684\u9057\u61be\u754c\u53ef\u4ee5\u5206\u89e3\u4e3a\u52a0\u6cd5\u5f62\u5f0f\uff1a\u5148\u9a8c\u4f9d\u8d56\u7684\u9884\u70ed\u9879$d r \\sqrt{\\mathrm{Tr}(\u03a3_0)}$\u548c\u957f\u671f\u6781\u5c0f\u6781\u5927\u9057\u61be\u9879$\u03c3d \\sqrt{T}$\u3002\u8fd9\u79cd\u52a0\u6cd5\u5206\u79bb\u6bd4\u73b0\u6709\u4e58\u6cd5\u5173\u7cfb\u66f4\u7cbe\u786e\uff0c\u4e14\u4e0b\u754c\u8868\u660e\u9884\u70ed\u9879\u662f\u4e0d\u53ef\u907f\u514d\u7684\u3002"}}
{"id": "2601.01914", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01914", "abs": "https://arxiv.org/abs/2601.01914", "authors": ["Arjun Ramesh Kaushik", "Nalini K. Ratha", "Venu Govindaraju"], "title": "Learning Action Hierarchies via Hybrid Geometric Diffusion", "comment": "Accepted at WACV-26", "summary": "Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHybridTAS\u6846\u67b6\uff0c\u5c06\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u51e0\u4f55\u7ed3\u5408\u5230\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u4e2d\uff0c\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u7684\u5c42\u6b21\u7ed3\u6784\u7279\u6027\u5b9e\u73b0\u4ece\u7c97\u5230\u7ec6\u7684\u52a8\u4f5c\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8fed\u4ee3\u7cbe\u70bc\u7684\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4eba\u7c7b\u52a8\u4f5c\u7684\u5c42\u6b21\u7ed3\u6784\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u5229\u7528\u52a8\u4f5c\u5c42\u6b21\u5173\u7cfb\u7684\u65f6\u95f4\u52a8\u4f5c\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHybridTAS\u6846\u67b6\uff0c\u5728\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\u4e2d\u878d\u5408\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u51e0\u4f55\u3002\u5229\u7528\u53cc\u66f2\u51e0\u4f55\u7684\u6811\u72b6\u5173\u7cfb\u7279\u6027\uff0c\u5728\u8f83\u9ad8\u6269\u6563\u65f6\u95f4\u6b65\u4f7f\u7528\u62bd\u8c61\u7684\u9ad8\u5c42\u52a8\u4f5c\u7c7b\u522b\uff08\u6839\u8282\u70b9\uff09\u6307\u5bfc\uff0c\u5728\u8f83\u4f4e\u65f6\u95f4\u6b65\u4f7f\u7528\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7c7b\u522b\uff08\u53f6\u8282\u70b9\uff09\u8fdb\u884c\u7cbe\u70bc\uff0c\u5b9e\u73b0\u4ece\u7c97\u5230\u7ec6\u7684\u52a8\u4f5c\u6807\u7b7e\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5728GTEA\u300150Salads\u548cBreakfast\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u53cc\u66f2\u5f15\u5bfc\u53bb\u566a\u5728\u65f6\u95f4\u52a8\u4f5c\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53cc\u66f2\u51e0\u4f55\u878d\u5165\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u52a8\u4f5c\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u65f6\u95f4\u52a8\u4f5c\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02031", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02031", "abs": "https://arxiv.org/abs/2601.02031", "authors": ["Felix Stollenwerk", "Anna Lokrantz", "Niclas Hertzberg"], "title": "Output Embedding Centering for Stable LLM Pretraining", "comment": "11 pages, 5 figures", "summary": "Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called \u03bc-centering, or a regularization method called \u03bc-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that \u03bc-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8f93\u51fa\u5d4c\u5165\u4e2d\u5fc3\u5316(OEC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u03bc-\u4e2d\u5fc3\u5316\u548c\u03bc-\u635f\u5931\u4e24\u79cd\u65b9\u5f0f\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u8f93\u51fa\u5bf9\u6570\u53d1\u6563\u95ee\u9898\uff0c\u76f8\u6bd4z-loss\u66f4\u6709\u6548\u4e14\u8d85\u53c2\u6570\u8c03\u4f18\u66f4\u7b80\u5355\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e0d\u4ec5\u6602\u8d35\uff0c\u800c\u4e14\u5bb9\u6613\u51fa\u73b0\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u3002\u5728\u8bad\u7ec3\u540e\u671f\u4f7f\u7528\u5927\u5b66\u4e60\u7387\u65f6\u7ecf\u5e38\u51fa\u73b0\u7684\u7279\u5b9a\u4e0d\u7a33\u5b9a\u6027\u662f\u8f93\u51fa\u5bf9\u6570\u53d1\u6563\u3002\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f13\u89e3\u7b56\u7565z-loss\u4ec5\u89e3\u51b3\u4e86\u95ee\u9898\u7684\u75c7\u72b6\u800c\u975e\u6839\u672c\u539f\u56e0\u3002", "method": "\u4ece\u8f93\u51fa\u5d4c\u5165\u51e0\u4f55\u89d2\u5ea6\u5206\u6790\u4e0d\u7a33\u5b9a\u6027\uff0c\u63d0\u51fa\u8f93\u51fa\u5d4c\u5165\u4e2d\u5fc3\u5316(OEC)\u4f5c\u4e3a\u65b0\u7684\u7f13\u89e3\u7b56\u7565\u3002OEC\u53ef\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u5b9e\u73b0\uff1a\u4f5c\u4e3a\u786e\u5b9a\u6027\u64cd\u4f5c\u7684\u03bc-\u4e2d\u5fc3\u5316\uff0c\u6216\u4f5c\u4e3a\u6b63\u5219\u5316\u65b9\u6cd5\u7684\u03bc-\u635f\u5931\u3002", "result": "\u4e24\u79cdOEC\u53d8\u4f53\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u7387\u654f\u611f\u6027\u65b9\u9762\u90fd\u4f18\u4e8ez-loss\u3002\u7279\u522b\u662f\uff0c\u5f53z-loss\u5931\u8d25\u65f6\uff0c\u5b83\u4eec\u786e\u4fdd\u8bad\u7ec3\u5373\u4f7f\u5728\u5927\u5b66\u4e60\u7387\u4e0b\u4e5f\u80fd\u6536\u655b\u3002\u6b64\u5916\uff0c\u03bc-\u635f\u5931\u5728\u6b63\u5219\u5316\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u9762\u6bd4z-loss\u663e\u8457\u66f4\u4e0d\u654f\u611f\u3002", "conclusion": "\u8f93\u51fa\u5d4c\u5165\u4e2d\u5fc3\u5316(OEC)\u901a\u8fc7\u89e3\u51b3\u8f93\u51fa\u5bf9\u6570\u53d1\u6563\u7684\u6839\u672c\u539f\u56e0\uff0c\u63d0\u4f9b\u4e86\u6bd4z-loss\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u03bc-\u635f\u5931\u5728\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2601.01915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01915", "abs": "https://arxiv.org/abs/2601.01915", "authors": ["Yujie Hu", "Zecheng Tang", "Xu Jiang", "Weiqi Li", "Jian Zhang"], "title": "TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing", "comment": "a Conversational Assistant for Intelligent Image Editing", "summary": "Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.", "AI": {"tldr": "TalkPhoto\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8bdd\u4ea4\u4e92\u5b9e\u73b0\u7cbe\u786e\u56fe\u50cf\u64cd\u63a7\uff0c\u5229\u7528LLM\u5206\u6790\u7528\u6237\u9700\u6c42\u5e76\u5206\u5c42\u8c03\u7528\u73b0\u6709\u5148\u8fdb\u7f16\u8f91\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u9700\u8981\u6784\u5efa\u591a\u6307\u4ee4\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u6a21\u578b\u5904\u7406\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\uff0c\u8fd9\u65e2\u8017\u65f6\u8017\u529b\u53c8\u96be\u4ee5\u83b7\u5f97\u6ee1\u610f\u7ed3\u679c\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u5b9e\u73b0\u7075\u6d3b\u53ef\u63a7\u7684\u56fe\u50cf\u7f16\u8f91\u3002", "method": "\u63d0\u51faTalkPhoto\u6846\u67b6\uff1a1) \u4f7f\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684\u63d0\u793a\u6a21\u677f\u6307\u5bfc\u5f00\u6e90LLM\u5206\u6790\u7528\u6237\u9700\u6c42\uff1b2) \u5206\u5c42\u8c03\u7528\u73b0\u6709\u9ad8\u7ea7\u7f16\u8f91\u65b9\u6cd5\uff1b3) \u5b9e\u73b0\u5373\u63d2\u5373\u7528\u3001\u9ad8\u6548\u7684\u7f16\u8f91\u65b9\u6cd5\u8c03\u7528\u673a\u5236\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u7528\u66f4\u5c11\u7684token\u6d88\u8017\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8c03\u7528\uff0c\u8fd8\u80fd\u5728\u5404\u79cd\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u7684\u7f16\u8f91\u6548\u679c\uff0c\u652f\u6301\u590d\u6742\u548c\u672a\u89c1\u8fc7\u7684\u7f16\u8f91\u4efb\u52a1\u96c6\u6210\u3002", "conclusion": "TalkPhoto\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8bdd\u4ea4\u4e92\u5b9e\u73b0\u4e86\u7a33\u5b9a\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u6548\u679c\u6709\u9650\u7684\u95ee\u9898\u3002"}}
{"id": "2601.02036", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02036", "abs": "https://arxiv.org/abs/2601.02036", "authors": ["Yiyang Wang", "Xi Chen", "Xiaogang Xu", "Yu Liu", "Hengshuang Zhao"], "title": "GDRO: Group-level Reward Post-training Suitable for Diffusion Models", "comment": null, "summary": "Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGDRO\uff08Group-level Direct Reward Optimization\uff09\uff0c\u4e00\u79cd\u9488\u5bf9\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u7684\u7fa4\u4f53\u7ea7\u5956\u52b1\u5bf9\u9f50\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u5668\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4eceLLMs\u5230\u6587\u672c\u5230\u56fe\u50cf\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5956\u52b1\u5bf9\u9f50\uff0c\u867d\u7136\u80fd\u6210\u529f\u5bf9\u9f50\u76ee\u6807\u5956\u52b1\uff0c\u4f46\u9762\u4e34\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u5668\u548c\u5956\u52b1\u9ed1\u5ba2\u7b49\u6311\u6218\u3002\u6574\u6d41\u6d41\u6a21\u578b\u4e0eLLMs\u6709\u672c\u8d28\u533a\u522b\uff1a1\uff09\u5728\u7ebf\u56fe\u50cf\u91c7\u6837\u8017\u65f6\u5927\uff1b2\uff09\u6574\u6d41\u6d41\u662f\u786e\u5b9a\u6027\u7684\uff08\u4e00\u65e6\u521d\u59cb\u566a\u58f0\u56fa\u5b9a\uff09\u3002", "method": "\u63d0\u51faGDRO\uff0c\u7ed3\u5408\u6574\u6d41\u6d41\u6a21\u578b\u7279\u6027\u7684\u7fa4\u4f53\u7ea7\u5956\u52b1\u5bf9\u9f50\u540e\u8bad\u7ec3\u8303\u5f0f\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660eGDRO\u652f\u6301\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3\uff0c\u8282\u7701\u56fe\u50cf\u91c7\u6837\u65f6\u95f4\uff1b\u6269\u6563\u91c7\u6837\u5668\u72ec\u7acb\uff0c\u65e0\u9700ODE-to-SDE\u8fd1\u4f3c\u6765\u83b7\u5f97\u968f\u673a\u6027\uff1b\u540c\u65f6\u8003\u8651\u5956\u52b1\u9ed1\u5ba2\u9677\u9631\uff0c\u5728\u8bc4\u4f30\u4e2d\u4f7f\u7528\u6821\u6b63\u5206\u6570\u3002", "result": "\u5728OCR\u548cGenEval\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGDRO\u901a\u8fc7\u7fa4\u4f53\u7ea7\u79bb\u7ebf\u4f18\u5316\u6709\u6548\u4e14\u9ad8\u6548\u5730\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u7684\u5956\u52b1\u5206\u6570\uff0c\u540c\u65f6\u5728\u7f13\u89e3\u5956\u52b1\u9ed1\u5ba2\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GDRO\u4e3a\u6574\u6d41\u6d41\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u4e14\u9c81\u68d2\u7684\u7fa4\u4f53\u7ea7\u5956\u52b1\u5bf9\u9f50\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u79bb\u7ebf\u8bad\u7ec3\u548c\u91c7\u6837\u5668\u72ec\u7acb\u6027\u3002"}}
{"id": "2601.01925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01925", "abs": "https://arxiv.org/abs/2601.01925", "authors": ["Lianjie Jia", "Yuhan Wu", "Binghao Ran", "Yifan Wang", "Lijun Wang", "Huchuan Lu"], "title": "AR-MOT: Autoregressive Multi-object Tracking", "comment": "12 pages, 5 figures", "summary": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.", "AI": {"tldr": "AR-MOT\uff1a\u57fa\u4e8e\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b0\u8303\u5f0f\uff0c\u5c06MOT\u4efb\u52a1\u8f6c\u5316\u4e3a\u5e8f\u5217\u751f\u6210\u95ee\u9898\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u5934\uff0c\u901a\u8fc7\u7075\u6d3b\u5e8f\u5217\u6784\u9020\u8f93\u51fa\u7ed3\u6784\u5316\u7ed3\u679c", "motivation": "\u73b0\u6709MOT\u65b9\u6cd5\u67b6\u6784\u50f5\u5316\u4e14\u4efb\u52a1\u7279\u5b9a\uff0c\u96be\u4ee5\u9002\u5e94\u901a\u7528\u591a\u6a21\u6001\u573a\u666f\u548c\u65b0\u7684\u8ddf\u8e2a\u4efb\u52a1\u5f62\u5f0f\uff0c\u9650\u5236\u4e86\u8de8\u4efb\u52a1\u9002\u7528\u6027\u548c\u7075\u6d3b\u6027", "method": "\u63d0\u51faAR-MOT\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u5728LLM\u6846\u67b6\u5185\u5c06MOT\u8f6c\u5316\u4e3a\u5e8f\u5217\u751f\u6210\u4efb\u52a1\uff1b\u5f15\u5165\u57fa\u4e8e\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\u7684\u5bf9\u8c61\u6807\u8bb0\u5668\u589e\u5f3a\u533a\u57df\u89c6\u89c9\u611f\u77e5\uff1b\u8bbe\u8ba1\u533a\u57df\u611f\u77e5\u5bf9\u9f50\u6a21\u5757\u7f13\u89e3\u5168\u5c40\u4e0e\u533a\u57df\u7279\u5f81\u4e0d\u5bf9\u9f50\uff1b\u8bbe\u8ba1\u65f6\u5e8f\u8bb0\u5fc6\u878d\u5408\u6a21\u5757\u7f13\u5b58\u5386\u53f2\u5bf9\u8c61\u6807\u8bb0\u652f\u6301\u957f\u671f\u8ddf\u8e2a", "result": "\u5728MOT17\u548cDanceTrack\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e3a\u66f4\u901a\u7528\u7075\u6d3b\u7684MOT\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840", "conclusion": "AR-MOT\u901a\u8fc7\u81ea\u56de\u5f52\u5e8f\u5217\u751f\u6210\u8303\u5f0f\u5b9e\u73b0\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u7684MOT\u6846\u67b6\uff0c\u65b0\u6a21\u6001\u6216\u6307\u4ee4\u53ea\u9700\u4fee\u6539\u8f93\u51fa\u5e8f\u5217\u683c\u5f0f\u800c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6269\u5c55\u6f5c\u529b"}}
{"id": "2601.01926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01926", "abs": "https://arxiv.org/abs/2601.01926", "authors": ["Zhifei Li", "Yiran Wang", "Chenyi Xiong", "Yujing Xia", "Xiaoju Hou", "Yue Zhao", "Miao Zhang", "Kui Xiao", "Bing Yang"], "title": "MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering", "comment": "Accepted to AAAI 2026", "summary": "Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.", "AI": {"tldr": "MacVQA\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u95ee\u7b54\u6301\u7eed\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5185\u5b58\u5206\u914d\u548c\u5168\u5c40\u566a\u58f0\u8fc7\u6ee4\u6765\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u7279\u5f81\u8868\u793a\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u95ee\u7b54\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u7279\u5f81\u8868\u793a\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faMacVQA\u6846\u67b6\uff0c\u878d\u5408\u89c6\u89c9\u548c\u95ee\u9898\u4fe1\u606f\u540c\u65f6\u8fc7\u6ee4\u566a\u58f0\u4ee5\u786e\u4fdd\u9c81\u68d2\u8868\u793a\uff0c\u91c7\u7528\u57fa\u4e8e\u539f\u578b\u7684\u8bb0\u5fc6\u5206\u914d\u6765\u4f18\u5316\u7279\u5f81\u8d28\u91cf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u572810\u4e2a\u6301\u7eedVQA\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMacVQA\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6807\u51c6\u4efb\u52a1\u5e73\u5747\u51c6\u786e\u738743.38%\u3001\u5e73\u5747\u9057\u5fd8\u73872.32%\uff0c\u65b0\u7ec4\u5408\u4efb\u52a1\u5e73\u5747\u51c6\u786e\u738742.53%\u3001\u5e73\u5747\u9057\u5fd8\u73873.60%\u3002", "conclusion": "MacVQA\u901a\u8fc7\u81ea\u9002\u5e94\u5185\u5b58\u5206\u914d\u548c\u566a\u58f0\u8fc7\u6ee4\u8bbe\u8ba1\uff0c\u80fd\u591f\u5e73\u8861\u6301\u7eedVQA\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u83b7\u53d6\u3001\u4fdd\u7559\u548c\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.02050", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02050", "abs": "https://arxiv.org/abs/2601.02050", "authors": ["Yanhai Gan", "Yipeng Chen", "Ning Li", "Xingguo Liu", "Junyu Dong", "Xianyao Chen"], "title": "Explore the Ideology of Deep Learning in ENSO Forecasts", "comment": "5 figures. Code available at https://github.com/liuxingguo9349/pptv-enso-env", "summary": "The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the \"dead\" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6709\u754c\u53d8\u5dee\u51fd\u6570\u7684\u6570\u5b66\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u9971\u548c\u533a\u7684\"\u6b7b\u4ea1\"\u795e\u7ecf\u5143\u6765\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u63ed\u793a\u4e86ENSO\u53ef\u9884\u6d4b\u6027\u4e3b\u8981\u6765\u81ea\u70ed\u5e26\u592a\u5e73\u6d0b\uff0c\u5e76\u5206\u6790\u4e86\u6625\u5b63\u53ef\u9884\u6d4b\u6027\u969c\u788d\u7684\u6210\u56e0\u3002", "motivation": "ENSO\u5bf9\u5168\u7403\u6c14\u5019\u53d8\u7387\u6709\u6df1\u8fdc\u5f71\u54cd\uff0c\u4f46\u5176\u9884\u6d4b\u4ecd\u662f\u91cd\u5927\u6311\u6218\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u6280\u80fd\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u7684\u4e0d\u900f\u660e\u6027\u963b\u788d\u4e86\u79d1\u5b66\u4fe1\u4efb\u548c\u4e1a\u52a1\u90e8\u7f72\u3002\u9700\u8981\u5efa\u7acb\u6570\u5b66\u4e0a\u53ef\u9760\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u6765\u7406\u89e3\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6709\u754c\u53d8\u5dee\u51fd\u6570\u7684\u6570\u5b66\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u62ef\u6551\u6fc0\u6d3b\u51fd\u6570\u9971\u548c\u533a\u7684\"\u6b7b\u4ea1\"\u795e\u7ecf\u5143\u6765\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u5bf9\u6a21\u578b\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u533a\u57df\u5bf9ENSO\u9884\u6d4b\u7684\u8d21\u732e\u3002", "result": "\u5206\u6790\u663e\u793aENSO\u53ef\u9884\u6d4b\u6027\u4e3b\u8981\u6765\u81ea\u70ed\u5e26\u592a\u5e73\u6d0b\uff0c\u5370\u5ea6\u6d0b\u548c\u5927\u897f\u6d0b\u4e5f\u6709\u8d21\u732e\uff0c\u8fd9\u4e0e\u7269\u7406\u7406\u89e3\u4e00\u81f4\u3002\u53d7\u63a7\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u7a33\u5065\u6027\u53ca\u5176\u4e0e\u5df2\u77e5\u9884\u6d4b\u56e0\u5b50\u7684\u4e00\u81f4\u6027\u3002\u7279\u522b\u53d1\u73b0\u6625\u5b63\u53ef\u9884\u6d4b\u6027\u969c\u788d\u671f\u95f4\uff0c\u5c3d\u7ba1\u654f\u611f\u6027\u6269\u5927\u4f46\u9884\u6d4b\u6027\u80fd\u4e0b\u964d\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u6b21\u4f18\u53d8\u91cf\u9009\u62e9\u3002", "conclusion": "\u8be5\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u4e3a\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60ENSO\u9884\u6d4b\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7eb3\u5165\u989d\u5916\u7684\u6d77\u6d0b-\u5927\u6c14\u53d8\u91cf\u53ef\u80fd\u6709\u52a9\u4e8e\u8d85\u8d8a\u6625\u5b63\u53ef\u9884\u6d4b\u6027\u969c\u788d\u7684\u9650\u5236\uff0c\u63a8\u8fdb\u957f\u671fENSO\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2601.01955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01955", "abs": "https://arxiv.org/abs/2601.01955", "authors": ["Zhexin Zhang", "Yifeng Zhu", "Yangyang Xu", "Long Chen", "Yong Du", "Shengfeng He", "Jun Yu"], "title": "MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization", "comment": null, "summary": "Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \\romannumeral1) explicit disentanglement of motion from appearance and \\romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.", "AI": {"tldr": "MotionAdapter\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u5185\u5bb9\u611f\u77e5\u8fd0\u52a8\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u89e3\u8026\u8fd0\u52a8\u4e0e\u5916\u89c2\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u5c06\u8fd0\u52a8\u5b9a\u5236\u5230\u76ee\u6807\u5185\u5bb9\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u89c6\u9891\u95f4\u590d\u6742\u8fd0\u52a8\u8fc1\u79fb\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u89c6\u9891\u95f4\u8fc1\u79fb\u590d\u6742\u8fd0\u52a8\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u9c81\u68d2\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\u8fc1\u79fb\u3002", "method": "MotionAdapter\u9996\u5148\u901a\u8fc7\u5206\u67903D\u5168\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u7684\u8de8\u5e27\u6ce8\u610f\u529b\u6765\u63d0\u53d6\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u8fd0\u52a8\u573a\uff0c\u5b9e\u73b0\u8fd0\u52a8\u4e0e\u5916\u89c2\u7684\u663e\u5f0f\u89e3\u8026\u3002\u7136\u540e\u5f15\u5165DINO\u5f15\u5bfc\u7684\u8fd0\u52a8\u5b9a\u5236\u6a21\u5757\uff0c\u57fa\u4e8e\u5185\u5bb9\u5bf9\u5e94\u5173\u7cfb\u91cd\u65b0\u6392\u5217\u548c\u7cbe\u70bc\u8fd0\u52a8\u573a\uff0c\u6700\u540e\u4f7f\u7528\u5b9a\u5236\u7684\u8fd0\u52a8\u573a\u6307\u5bfcDiT\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMotionAdapter\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u81ea\u7136\u5730\u652f\u6301\u590d\u6742\u8fd0\u52a8\u8fc1\u79fb\u548c\u8fd0\u52a8\u7f16\u8f91\u4efb\u52a1\uff08\u5982\u7f29\u653e\uff09\u3002", "conclusion": "MotionAdapter\u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u7684\u8fd0\u52a8\u8fc1\u79fb\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86DiT\u57fa\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e2d\u590d\u6742\u8fd0\u52a8\u8fc1\u79fb\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\u8fc1\u79fb\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2601.01957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01957", "abs": "https://arxiv.org/abs/2601.01957", "authors": ["Tianbo Wang", "Yuqing Ma", "Kewei Liao", "Zhange Zhang", "Simin Li", "Jinyang Guo", "Xianglong Liu"], "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.", "AI": {"tldr": "AFTER\u65b9\u6cd5\u901a\u8fc7\u4e8b\u5b9e\u589e\u5f3a\u7684\u6fc0\u6d3b\u5f15\u5bfc\u548c\u67e5\u8be2\u81ea\u9002\u5e94\u504f\u79fb\u4f18\u5316\uff0c\u6709\u6548\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8bed\u8a00\u504f\u89c1\u5bfc\u81f4\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5305\u62ec\u7c7b\u522b\u3001\u5c5e\u6027\u548c\u5173\u7cfb\u5e7b\u89c9\uff0c\u4e25\u91cd\u5f71\u54cd\u53ef\u4fe1AI\u5e94\u7528\u3002\u73b0\u6709\u7f16\u8f91\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e8b\u5b9e\u6587\u672c\u8bed\u4e49\u7684\u6709\u6548\u6307\u5bfc\uff0c\u96be\u4ee5\u663e\u5f0f\u7f13\u89e3\u8bed\u8a00\u504f\u89c1\u3002", "method": "\u63d0\u51faAFTER\u65b9\u6cd5\uff0c\u5305\u542b\u4e8b\u5b9e\u589e\u5f3a\u6fc0\u6d3b\u5f15\u5bfc\uff08FAS\uff09\u548c\u67e5\u8be2\u81ea\u9002\u5e94\u504f\u79fb\u4f18\u5316\uff08QAO\uff09\u3002FAS\u4e3a\u6fc0\u6d3b\u7f16\u8f91\u63d0\u4f9b\u4e8b\u5b9e\u548c\u901a\u7528\u6307\u5bfc\uff0c\u663e\u5f0f\u5efa\u6a21\u7cbe\u786e\u7684\u89c6\u89c9-\u6587\u672c\u5173\u8054\uff1bQAO\u5f15\u5165\u67e5\u8be2\u611f\u77e5\u504f\u79fb\u4f30\u8ba1\u5668\uff0c\u4ece\u901a\u7528\u5f15\u5bfc\u5411\u91cf\u5efa\u7acb\u67e5\u8be2\u7279\u5b9a\u7684\u7f16\u8f91\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6807\u51c6\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86AFTER\u7684\u6709\u6548\u6027\uff0c\u5728AMBER\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe16.3%\u7684\u5e7b\u89c9\u51cf\u5c11\u3002", "conclusion": "AFTER\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5c06\u539f\u59cb\u504f\u7f6e\u6fc0\u6d3b\u5f15\u5bfc\u81f3\u4e8b\u5b9e\u8bed\u4e49\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u53ef\u4fe1AI\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02106", "abs": "https://arxiv.org/abs/2601.02106", "authors": ["Ashish Rana", "Ammar Shaker", "Sascha Saralajew", "Takashi Suzuki", "Kosuke Yasuda", "Shintaro Kato", "Toshikazu Wada", "Toshiyuki Fujikawa", "Toru Kikutsuji"], "title": "Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI", "comment": "Accepted to the Demo Track at the IEEE International Conference on Data Mining (ICDM) 2025, where it received the Best Demo Award", "summary": "Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.", "AI": {"tldr": "ProtoPal\u6846\u67b6\u901a\u8fc7\u539f\u578b\u5b66\u4e60\u5b9e\u73b0\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\uff0c\u63d0\u4f9b\u53ef\u7406\u89e3\u548c\u53ef\u9a8c\u8bc1\u7684\u9884\u6d4b\u4e0e\u5e72\u9884\u5efa\u8bae", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5728\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\u4e2d\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e3a\u533b\u7597\u9886\u57df\u6240\u6709\u5229\u76ca\u76f8\u5173\u8005\u63d0\u4f9b\u65e2\u6613\u61c2\u53c8\u53ef\u4fe1\u7684\u9884\u6d4b\u3001\u5e72\u9884\u548c\u63a8\u8350", "method": "\u63d0\u51faProtoPal\u6846\u67b6\uff0c\u91c7\u7528\u539f\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u542b\u524d\u7aef\u548c\u540e\u7aef\u4e24\u79cd\u6a21\u5f0f\uff0c\u80fd\u591f\u76f4\u89c2\u5c55\u793a\u5e72\u9884\u63aa\u65bd\u53ca\u5176\u6a21\u62df\u7ed3\u679c", "result": "\u5728\u4fdd\u6301\u4f18\u5f02\u5b9a\u91cf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u5e72\u9884\u63aa\u65bd\u53ca\u5176\u6a21\u62df\u7ed3\u679c\u7684\u76f4\u89c2\u5448\u73b0", "conclusion": "\u539f\u578b\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4e2a\u6027\u5316\u9884\u9632\u533b\u7597\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\uff0c\u4e3a\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u900f\u660e\u53ef\u4fe1\u7684\u652f\u6301"}}
{"id": "2601.01989", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01989", "abs": "https://arxiv.org/abs/2601.01989", "authors": ["Aly R. Elkammar", "Karim M. Gamaleldin", "Catherine M. Elias"], "title": "VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis", "comment": null, "summary": "Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eTransformer/\u89c6\u9891\u89c6\u89c9Transformer\u7684\u591a\u6a21\u6001\u7b97\u6cd5\uff0c\u7528\u4e8e\u884c\u4eba\u610f\u56fe\u9884\u6d4b\uff0c\u5728JAAD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u5728\u51c6\u786e\u7387\u3001AUC\u548cF1\u5206\u6570\u7b49\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u662f\u4eceL3\u5230L4\u81ea\u52a8\u9a7e\u9a76\u8fc7\u6e21\u7684\u5173\u952e\u6280\u672f\u4e4b\u4e00\u3002\u4e3a\u4e86\u7406\u89e3\u884c\u4eba\u8fc7\u8857\u884c\u4e3a\uff0c\u9700\u8981\u8003\u8651\u591a\u79cd\u56e0\u7d20\u548c\u7279\u5f81\uff0c\u4ee5\u786e\u4fdd\u672a\u6765\u9053\u8def\u5bf9\u6240\u6709\u4eba\u90fd\u66f4\u5b89\u5168\u3002", "method": "\u5f15\u5165\u57fa\u4e8eTransformer/\u89c6\u9891\u89c6\u89c9Transformer\u7684\u4e0d\u540c\u89c4\u6a21\u7b97\u6cd5\uff0c\u4f7f\u7528\u591a\u79cd\u6570\u636e\u6a21\u6001\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u6765\u8c03\u67e5\u4e0d\u540c\u6a21\u578b\u8bbe\u8ba1\u9009\u62e9\u5e26\u6765\u7684\u4f18\u52bf\u3002", "result": "\u5728\u6d41\u884c\u7684\u884c\u4eba\u884c\u4e3a\u6570\u636e\u96c6JAAD\u4e0a\u8bc4\u4f30\u7b97\u6cd5\uff0c\u8fbe\u5230\u4e86SOTA\u6027\u80fd\uff0c\u5728\u51c6\u786e\u7387\u3001AUC\u548cF1\u5206\u6570\u7b49\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684Transformer/\u89c6\u9891\u89c6\u89c9Transformer\u591a\u6a21\u6001\u7b97\u6cd5\u5728\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u884c\u4eba\u884c\u4e3a\u7406\u89e3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01992", "abs": "https://arxiv.org/abs/2601.01992", "authors": ["Chen Zhu", "Huiwen Zhang", "Yujie Li", "Mu He", "Xiaotian Qiao"], "title": "API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning", "comment": null, "summary": "Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5757\u91cd\u8981\u6027\u611f\u77e5\uff08API\uff09\u6846\u67b6\u7528\u4e8e\u53ef\u6cdb\u5316\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u53bb\u96fe\uff0c\u5305\u542b\u81ea\u52a8\u96fe\u973e\u751f\u6210\u6a21\u5757\u548c\u5bc6\u5ea6\u611f\u77e5\u53bb\u96fe\u6a21\u5757\uff0c\u901a\u8fc7\u6df7\u5408\u6570\u636e\u589e\u5f3a\u548c\u591a\u8d1f\u6837\u672c\u5bf9\u6bd4\u635f\u5931\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u53bb\u96fe\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u590d\u6742\u771f\u5b9e\u4e16\u754c\u96fe\u973e\u573a\u666f\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u96fe\u973e\u5bc6\u5ea6\u5206\u5e03\u7684\u5185\u5728\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u5757\u91cd\u8981\u6027\u611f\u77e5\uff08API\uff09\u6846\u67b6\uff1a1\uff09\u81ea\u52a8\u96fe\u973e\u751f\u6210\uff08AHG\uff09\u6a21\u5757\u63d0\u4f9b\u6df7\u5408\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u96fe\u973e\u56fe\u50cf\u4f5c\u4e3a\u989d\u5916\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff1b2\uff09\u5bc6\u5ea6\u611f\u77e5\u53bb\u96fe\uff08DHR\uff09\u6a21\u5757\u4ee5\u81ea\u9002\u5e94\u5757\u91cd\u8981\u6027\u611f\u77e5\u65b9\u5f0f\u5904\u7406\u4e0d\u540c\u96fe\u973e\u5bc6\u5ea6\u5206\u5e03\u533a\u57df\uff1b3\uff09\u5f15\u5165\u591a\u8d1f\u6837\u672c\u5bf9\u6bd4\u53bb\u96fe\uff08MNCD\uff09\u635f\u5931\uff0c\u5145\u5206\u5229\u7528\u7a7a\u95f4\u548c\u9891\u57df\u591a\u4e2a\u8d1f\u6837\u672c\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u4e0d\u540c\u96fe\u973e\u5206\u5e03\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684API\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u6570\u636e\u589e\u5f3a\u3001\u5bc6\u5ea6\u611f\u77e5\u53bb\u96fe\u548c\u591a\u8d1f\u6837\u672c\u5bf9\u6bd4\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u53bb\u96fe\u4e2d\u7684\u8bad\u7ec3\u6570\u636e\u6709\u9650\u548c\u96fe\u973e\u5206\u5e03\u590d\u6742\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.01998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01998", "abs": "https://arxiv.org/abs/2601.01998", "authors": ["Chen Zhu", "Huiwen Zhang", "Mu He", "Yujie Li", "Xiaotian Qiao"], "title": "Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors", "comment": null, "summary": "Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u96fe\u973e\u4e0e\u4f4e\u5149\u5148\u9a8c\u4e4b\u95f4\u7684\u5185\u5728\u4e00\u81f4\u6027\uff0c\u6e10\u8fdb\u5f0f\u63d0\u5347\u591c\u95f4\u96fe\u973e\u56fe\u50cf\u7684\u53ef\u89c1\u5ea6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5904\u7406\u5355\u4e00\u7c7b\u578b\u7684\u9000\u5316\uff08\u5982\u96fe\u973e\u6216\u4f4e\u5149\uff09\uff0c\u5ffd\u7565\u4e86\u4e0d\u540c\u7c7b\u578b\u9000\u5316\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5bfc\u81f4\u53ef\u89c1\u5ea6\u63d0\u5347\u6709\u9650\u3002\u7814\u7a76\u53d1\u73b0\u4f4e\u5149\u548c\u96fe\u973e\u5148\u9a8c\u4e4b\u95f4\u7684\u9886\u57df\u77e5\u8bc6\u53ef\u4ee5\u76f8\u4e92\u5f3a\u5316\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u53ef\u89c1\u5ea6", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u4e92\u4e14\u6e10\u8fdb\u5730\u5f3a\u5316\u96fe\u973e\u548c\u4f4e\u5149\u5148\u9a8c\u4e4b\u95f4\u7684\u5185\u5728\u4e00\u81f4\u6027\u6765\u589e\u5f3a\u591c\u95f4\u96fe\u973e\u56fe\u50cf\u7684\u53ef\u89c1\u5ea6\u3002\u6a21\u578b\u91c7\u7528\u56fe\u50cf\u7ea7\u3001\u5757\u7ea7\u548c\u50cf\u7d20\u7ea7\u4e13\u5bb6\uff0c\u5728\u89c6\u89c9\u548c\u9891\u7387\u57df\u4e2d\u64cd\u4f5c\uff0c\u9010\u6b65\u6062\u590d\u5168\u5c40\u573a\u666f\u7ed3\u6784\u3001\u533a\u57df\u6a21\u5f0f\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u5f15\u5165\u9891\u7387\u611f\u77e5\u8def\u7531\u5668\u81ea\u9002\u5e94\u5730\u6307\u5bfc\u6bcf\u4e2a\u4e13\u5bb6\u7684\u8d21\u732e\uff0c\u786e\u4fdd\u9c81\u68d2\u7684\u56fe\u50cf\u6062\u590d", "result": "\u5728\u591c\u95f4\u53bb\u96fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u767d\u5929\u53bb\u96fe\u548c\u4f4e\u5149\u589e\u5f3a\u4efb\u52a1\u4e2d\u4e5f\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u96fe\u973e\u548c\u4f4e\u5149\u5148\u9a8c\u4e4b\u95f4\u7684\u5185\u5728\u4e00\u81f4\u6027\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5904\u7406\u591c\u95f4\u96fe\u973e\u56fe\u50cf\u7684\u590d\u6742\u9000\u5316\u5206\u5e03\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u89c1\u5ea6\u589e\u5f3a\u6548\u679c"}}
{"id": "2601.02193", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.02193", "abs": "https://arxiv.org/abs/2601.02193", "authors": ["Kasper Green Larsen", "Chirag Pabbaraju", "Abhishek Shetty"], "title": "Learning with Monotone Adversarial Corruptions", "comment": null, "summary": "We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a \"clean\" i.i.d. dataset, inserts additional \"corrupted\" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6807\u51c6\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u7684\u53ef\u4ea4\u6362\u6027\u548c\u72ec\u7acb\u6027\uff0c\u5728\u5355\u8c03\u5bf9\u6297\u6027\u6c61\u67d3\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u6c61\u67d3\u70b9\u88ab\u6b63\u786e\u6807\u8bb0\uff0c\u6700\u4f18\u5206\u7c7b\u7b97\u6cd5\u4ecd\u4f1a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u7684\u7b97\u6cd5\u5219\u4e0d\u53d7\u5f71\u54cd\u3002", "motivation": "\u63a2\u7a76\u6807\u51c6\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5bf9\u6570\u636e\u53ef\u4ea4\u6362\u6027\u548c\u72ec\u7acb\u6027\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u63ed\u793a\u5373\u4f7f\u5728\u770b\u4f3c\u6709\u76ca\u7684\u5355\u8c03\u6c61\u67d3\u4e0b\uff0c\u6700\u4f18\u7b97\u6cd5\u4ecd\u53ef\u80fd\u5931\u6548\u7684\u73b0\u8c61\u3002", "method": "\u5f15\u5165\u5355\u8c03\u5bf9\u6297\u6027\u6c61\u67d3\u6a21\u578b\uff1a\u653b\u51fb\u8005\u89c2\u5bdf\u5e72\u51c0i.i.d.\u6570\u636e\u96c6\u540e\uff0c\u63d2\u5165\u6309\u771f\u5b9e\u76ee\u6807\u51fd\u6570\u6807\u8bb0\u7684\u6c61\u67d3\u70b9\uff08\u5355\u8c03\u6c61\u67d3\uff09\uff0c\u5206\u6790\u4e0d\u540c\u7b97\u6cd5\u5728\u6b64\u6a21\u578b\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6240\u6709\u5df2\u77e5\u7684\u4e8c\u5143\u5206\u7c7b\u6700\u4f18\u5b66\u4e60\u7b97\u6cd5\u5728\u5355\u8c03\u6c61\u67d3\u4e0b\u90fd\u4f1a\u5728\u65b0\u6d4b\u8bd5\u70b9\u4e0a\u83b7\u5f97\u6b21\u4f18\u671f\u671b\u8bef\u5dee\uff1b\u800c\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u7684\u7b97\u6cd5\u5219\u4fdd\u6301\u539f\u6709\u6027\u80fd\u4fdd\u8bc1\u4e0d\u53d8\u3002", "conclusion": "\u6700\u4f18\u5b66\u4e60\u7b97\u6cd5\u5728\u9762\u5bf9\u770b\u4f3c\u6709\u76ca\u7684\u5355\u8c03\u6c61\u67d3\u65f6\u4f1a\u5931\u6548\uff0c\u66b4\u9732\u4e86\u5b83\u4eec\u5bf9\u6570\u636e\u53ef\u4ea4\u6362\u6027\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff1b\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u7684\u7b97\u6cd5\u66f4\u5177\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.02016", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02016", "abs": "https://arxiv.org/abs/2601.02016", "authors": ["Matthias Bartolo", "Dylan Seychell", "Gabriel Hili", "Matthew Montebello", "Carl James Debono", "Saviour Formosa", "Konstantinos Makantasis"], "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach", "comment": "Code available on GitHub: https://github.com/mbar0075/lupi-for-object-detection", "summary": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5c06\u7279\u6743\u4fe1\u606f\u5b66\u4e60\uff08LUPI\uff09\u8303\u5f0f\u6574\u5408\u5230\u76ee\u6807\u68c0\u6d4b\u4e2d\uff0c\u5229\u7528\u8bad\u7ec3\u65f6\u53ef\u7528\u4f46\u63a8\u7406\u65f6\u4e0d\u53ef\u7528\u7684\u7ec6\u7c92\u5ea6\u63cf\u8ff0\u6027\u4fe1\u606f\uff0c\u901a\u8fc7\u5e08\u751f\u67b6\u6784\u6ce8\u5165\u7279\u6743\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u800c\u4e0d\u589e\u52a0\u63a8\u7406\u590d\u6742\u5ea6\u3002", "motivation": "\u76ee\u6807\u68c0\u6d4b\u8bad\u7ec3\u65f6\u901a\u5e38\u6709\u4e30\u5bcc\u7684\u63cf\u8ff0\u6027\u4fe1\u606f\uff08\u5982\u8fb9\u754c\u6846\u63a9\u7801\u3001\u663e\u8457\u6027\u56fe\u3001\u6df1\u5ea6\u7ebf\u7d22\u7b49\uff09\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u8fd9\u4e9b\u4fe1\u606f\u4e0d\u53ef\u7528\u3002\u5982\u4f55\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u8bad\u7ec3\u65f6\u7684\u7279\u6743\u4fe1\u606f\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u800c\u4e0d\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u662f\u672c\u7814\u7a76\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e08\u751f\u67b6\u6784\u5c06\u7279\u6743\u4fe1\u606f\u6ce8\u5165\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u5668\u3002\u6559\u5e08\u6a21\u578b\u5229\u7528\u7279\u6743\u4fe1\u606f\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b66\u751f\u6a21\u578b\u5b66\u4e60\u6559\u5e08\u7684\u8f93\u51fa\uff0c\u6700\u7ec8\u5b66\u751f\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4e0d\u4f9d\u8d56\u7279\u6743\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u548c\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u65e0\u4eba\u673a\u5783\u573e\u68c0\u6d4b\u6570\u636e\u96c6\u548cPascal VOC 2012\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLUPI\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u590d\u6742\u5ea6\u6216\u6a21\u578b\u5927\u5c0f\u3002\u4e2d\u7b49\u548c\u5927\u5c3a\u5bf8\u76ee\u6807\u7684\u6027\u80fd\u63d0\u5347\u5c24\u4e3a\u660e\u663e\u3002", "conclusion": "LUPI\u6846\u67b6\u4e3a\u76ee\u6807\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u548c\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u3002\u901a\u8fc7\u5e08\u751f\u67b6\u6784\u5229\u7528\u8bad\u7ec3\u65f6\u7684\u7279\u6743\u4fe1\u606f\uff0c\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\uff0c\u4e2d\u95f4\u6743\u91cd\u5e73\u8861\u4e86\u7279\u6743\u4fe1\u606f\u548c\u6807\u51c6\u8f93\u5165\u7684\u5b66\u4e60\u3002"}}
{"id": "2601.02018", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02018", "abs": "https://arxiv.org/abs/2601.02018", "authors": ["Guangqian Guo", "Aixi Ren", "Yong Guo", "Xuehui Yu", "Jiacheng Tian", "Wenli Li", "Yaoxing Wang", "Shan Gao"], "title": "Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement", "comment": "Diffusion-based latent space enhancement helps improve the robustness of SAM", "summary": "Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.", "AI": {"tldr": "GleSAM++ \u662f\u4e00\u4e2a\u589e\u5f3a Segment Anything Models (SAMs) \u5728\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0a\u5206\u5272\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u6f5c\u5728\u7a7a\u95f4\u589e\u5f3a\u3001\u7279\u5f81\u5206\u5e03\u5bf9\u9f50\u3001\u901a\u9053\u590d\u5236\u6269\u5c55\u548c\u9000\u5316\u611f\u77e5\u81ea\u9002\u5e94\u589e\u5f3a\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u9000\u5316\u56fe\u50cf\u4e0a\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u867d\u7136 SAMs \u5728\u96f6\u6837\u672c\u5206\u5272\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e25\u91cd\u9000\u5316\u3001\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u9000\u5316\u7a0b\u5ea6\u7684\u663e\u5f0f\u6307\u5bfc\uff0c\u5bfc\u81f4\u6a21\u578b\u9700\u8981\u9690\u5f0f\u62df\u5408\u590d\u6742\u7684\u566a\u58f0\u5206\u5e03\uff0c\u589e\u52a0\u4e86\u5b66\u4e60\u8d1f\u62c5\u5e76\u5bfc\u81f4\u6b21\u4f18\u91cd\u5efa\u3002", "method": "\u63d0\u51fa\u4e86 GleSAM++ \u65b9\u6cd5\uff0c\u5305\u542b\uff1a1) \u751f\u6210\u5f0f\u6f5c\u5728\u7a7a\u95f4\u589e\u5f3a\u63d0\u5347\u4f4e\u8d28\u91cf\u56fe\u50cf\u9c81\u68d2\u6027\uff1b2) \u7279\u5f81\u5206\u5e03\u5bf9\u9f50(FDA)\u548c\u901a\u9053\u590d\u5236\u6269\u5c55(CRE)\u6539\u5584\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e0e\u5206\u5272\u6846\u67b6\u7684\u517c\u5bb9\u6027\uff1b3) \u9000\u5316\u611f\u77e5\u81ea\u9002\u5e94\u589e\u5f3a(DAE)\u673a\u5236\uff0c\u5c06\u4efb\u610f\u8d28\u91cf\u7279\u5f81\u7684\u91cd\u5efa\u8fc7\u7a0b\u89e3\u8026\u4e3a\u9000\u5316\u7ea7\u522b\u9884\u6d4b\u548c\u9000\u5316\u611f\u77e5\u91cd\u5efa\u4e24\u4e2a\u9636\u6bb5\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGleSAM++ \u5728\u590d\u6742\u9000\u5316\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u6e05\u6670\u56fe\u50cf\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u9000\u5316\u7c7b\u578b\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u3002", "conclusion": "GleSAM++ \u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u989d\u5916\u53ef\u5b66\u4e60\u53c2\u6570\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u7684 SAM \u548c SAM2\uff0c\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86 SAMs \u5728\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02020", "abs": "https://arxiv.org/abs/2601.02020", "authors": ["Shihan Peng", "Yuyang Xiong", "Hanyu Zhou", "Zhiwei Shi", "Haoyue Liu", "Gang Chen", "Luxin Yan", "Yi Chang"], "title": "Adapting Depth Anything to Adverse Imaging Conditions with Events", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51faADAE\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u5f15\u5bfc\u7684\u65f6\u7a7a\u878d\u5408\u589e\u5f3aDepth Anything\u5728\u6076\u52a3\u5149\u7167\u548c\u8fd0\u52a8\u6a21\u7cca\u573a\u666f\u4e0b\u7684\u6df1\u5ea6\u4f30\u8ba1\u80fd\u529b", "motivation": "\u5f53\u524d\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\uff08\u5982Depth Anything\uff09\u5728\u7406\u60f3\u573a\u666f\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6781\u7aef\u5149\u7167\u548c\u8fd0\u52a8\u6a21\u7cca\u7b49\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u3002\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u9700\u8981\u4ece\u5934\u8bad\u7ec3\uff0c\u65e0\u6cd5\u7ee7\u627f\u57fa\u7840\u6a21\u578b\u7684\u5f00\u653e\u4e16\u754c\u77e5\u8bc6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faADAE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u71b5\u611f\u77e5\u7a7a\u95f4\u878d\u5408 - \u4f7f\u7528\u4fe1\u606f\u71b5\u7b56\u7565\u81ea\u9002\u5e94\u878d\u5408\u5e27\u57fa\u548c\u4e8b\u4ef6\u57fa\u7279\u5f81\uff0c\u6307\u793a\u5149\u7167\u5f15\u8d77\u7684\u9000\u5316\uff1b2\uff09\u8fd0\u52a8\u5f15\u5bfc\u65f6\u95f4\u6821\u6b63 - \u5229\u7528\u4e8b\u4ef6\u8fd0\u52a8\u7ebf\u7d22\u91cd\u65b0\u6821\u51c6\u6a21\u7cca\u533a\u57df\u7684\u6a21\u7cca\u7279\u5f81\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5728\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u4e0b\u663e\u8457\u589e\u5f3a\u4e86Depth Anything\u7684\u6027\u80fd\u3002", "conclusion": "ADAE\u6846\u67b6\u901a\u8fc7\u4e8b\u4ef6\u5f15\u5bfc\u7684\u65f6\u7a7a\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u5728\u9000\u5316\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u7ee7\u627f\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5f00\u653e\u4e16\u754c\u77e5\u8bc6\u3002"}}
{"id": "2601.02029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02029", "abs": "https://arxiv.org/abs/2601.02029", "authors": ["Toshihiko Nishimura", "Hirofumi Abe", "Kazuhiko Murasaki", "Taiga Yoshida", "Ryuichi Tanida"], "title": "Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding", "comment": "19", "summary": "This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u97003D\u6807\u6ce8\u6570\u636e\u6216\u914d\u5bf9RGB\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u70b9\u4e913D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u865a\u62df\u76f8\u673a\u5c06\u70b9\u4e91\u6295\u5f71\u52302D\u56fe\u50cf\uff0c\u5229\u7528\u57fa\u78402D\u6a21\u578b\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u52a0\u6743\u6295\u7968\u5b9e\u73b03D\u5206\u5272\u3002", "motivation": "\u4f20\u7edf3D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u76843D\u8bad\u7ec3\u6570\u636e\uff0c\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u914d\u5bf9RGB\u56fe\u50cf\u6216\u76d1\u7763\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u548c\u7075\u6d3b\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u97003D\u6807\u6ce8\u3001\u65e0\u9700\u914d\u5bf9\u56fe\u50cf\u3001\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u76843D\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u3002", "method": "1) \u4f7f\u7528\u865a\u62df\u76f8\u673a\u5c063D\u70b9\u4e91\u6295\u5f71\u5230\u591a\u4e2a2D\u89c6\u89d2\u56fe\u50cf\uff1b2) \u5229\u7528\u57fa\u78402D\u6a21\u578b\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5bf9\u6bcf\u4e2a2D\u56fe\u50cf\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff1b3) \u901a\u8fc7\u52a0\u6743\u6295\u7968\u673a\u5236\u805a\u5408\u591a\u4e2a\u89c6\u89d2\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u5b9e\u73b03D\u70b9\u4e91\u5206\u5272\uff1b4) \u652f\u6301\u4efb\u610f\u6587\u672c\u67e5\u8be2\u7684\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e0\u97003D\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5206\u5272\u7cbe\u5ea6\u63a5\u8fd1\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\uff0c\u7528\u6237\u53ef\u4ee5\u4f7f\u7528\u4efb\u610f\u6587\u672c\u67e5\u8be2\u68c0\u6d4b\u5bf9\u8c61\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u7684\u7c7b\u522b\u9650\u5236\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u76843D\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5c063D\u70b9\u4e91\u6295\u5f71\u52302D\u5e76\u5229\u7528\u5f3a\u5927\u7684\u57fa\u78402D\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e0\u97003D\u6807\u6ce8\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u5206\u5272\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\uff0c\u4e3a3D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02232", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02232", "abs": "https://arxiv.org/abs/2601.02232", "authors": ["Shristi Das Biswas", "Yue Zhang", "Anwesan Pal", "Radhika Bhargava", "Kaushik Roy"], "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\\%$ and a $35\\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.", "AI": {"tldr": "ELLA\uff1a\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u539f\u5219\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u9ad8\u80fd\u91cf\u4efb\u52a1\u7279\u5b9a\u65b9\u5411\u7684\u5bf9\u9f50\u6765\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u7559\u4f4e\u80fd\u91cf\u6b8b\u5dee\u5b50\u7a7a\u95f4\u7684\u81ea\u7531\u5ea6\u4ee5\u5b9e\u73b0\u6b63\u5411\u8fc1\u79fb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u57fa\u4e8e\u91cd\u653e\u7684\u65b9\u6cd5\u4e0d\u5207\u5b9e\u9645\u4e14\u4fb5\u72af\u9690\u79c1\uff0c\u800c\u4e25\u683c\u6b63\u4ea4\u6027\u65b9\u6cd5\u5728\u89c4\u6a21\u6269\u5c55\u65f6\u4f1a\u5931\u6548\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u65b0\u4efb\u52a1\u90fd\u88ab\u6295\u5f71\u5230\u6b63\u4ea4\u8865\u7a7a\u95f4\uff0c\u9010\u6e10\u51cf\u5c11\u5269\u4f59\u81ea\u7531\u5ea6\u5e76\u7981\u6b62\u5171\u4eab\u8868\u793a\u4e2d\u7684\u91cd\u53e0\uff0c\u4ece\u800c\u6d88\u9664\u6b63\u5411\u8fc1\u79fb\u3002", "method": "ELLA\u57fa\u4e8e\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u539f\u5219\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u5668\u5bf9\u5355\u4e2a\u805a\u5408\u66f4\u65b0\u77e9\u9635\u8fdb\u884c\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u660e\u786e\u8868\u5f81\u8fc7\u53bb\u66f4\u65b0\u7684\u7ed3\u6784\uff0c\u60e9\u7f5a\u6cbf\u9ad8\u80fd\u91cf\u3001\u4efb\u52a1\u7279\u5b9a\u65b9\u5411\u7684\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u4f4e\u80fd\u91cf\u6b8b\u5dee\u5b50\u7a7a\u95f4\u7684\u81ea\u7531\u5ea6\u3002\u8fd9\u5bf9\u5e94\u4e8e\u5404\u5411\u5f02\u6027\u6536\u7f29\u7b97\u5b50\uff0c\u53ef\u9650\u5236\u5e72\u6270\uff0c\u4e14\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u6052\u5b9a\uff0c\u4e0d\u53d7\u4efb\u52a1\u5e8f\u5217\u957f\u5ea6\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u6d41\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe9.6%\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c1135\u500d\u3002\u65e0\u9700\u6570\u636e\u91cd\u653e\u3001\u67b6\u6784\u6269\u5c55\u548c\u53ef\u5ffd\u7565\u7684\u5b58\u50a8\u5f00\u9500\u3002\u5728\u4e0d\u540c\u67b6\u6784\u4e0a\u7a33\u5065\u6269\u5c55\uff0c\u5e76\u4e3b\u52a8\u589e\u5f3a\u6a21\u578b\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "ELLA\u4e3a\u6784\u5efa\u6027\u7ec8\u8eabLLM\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5b50\u7a7a\u95f4\u53bb\u76f8\u5173\u6709\u6548\u5e73\u8861\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u6b63\u5411\u8fc1\u79fb\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2601.02038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02038", "abs": "https://arxiv.org/abs/2601.02038", "authors": ["Yihan Zhu", "Mengying Ge"], "title": "AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off", "comment": null, "summary": "Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.", "AI": {"tldr": "AlignVTOFF\u662f\u4e00\u4e2a\u7528\u4e8e\u865a\u62df\u8bd5\u7a7f(VTOFF)\u4efb\u52a1\u7684\u5e76\u884cU-Net\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003U-Net\u548c\u7eb9\u7406-\u7a7a\u95f4\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u51e0\u4f55\u53d8\u5f62\u548c\u9ad8\u9891\u7eb9\u7406\u4fdd\u6301\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u4f9d\u8d56\u8f7b\u91cf\u7ea7\u6a21\u5757\u8fdb\u884c\u5feb\u901f\u7279\u5f81\u63d0\u53d6\uff0c\u96be\u4ee5\u4fdd\u6301\u7ed3\u6784\u5316\u56fe\u6848\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u5bfc\u81f4\u751f\u6210\u8fc7\u7a0b\u4e2d\u7eb9\u7406\u8870\u51cf\u3002\u9700\u8981\u89e3\u51b3\u5728\u590d\u6742\u51e0\u4f55\u53d8\u5f62\u548c\u4e30\u5bcc\u9ad8\u9891\u7eb9\u7406\u4e0b\u5408\u6210\u9ad8\u4fdd\u771f\u5e73\u94fa\u670d\u88c5\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faAlignVTOFF\u6846\u67b6\uff0c\u5305\u542b\u53c2\u8003U-Net\u548c\u7eb9\u7406-\u7a7a\u95f4\u7279\u5f81\u5bf9\u9f50(TSFA)\u6a21\u5757\u3002\u53c2\u8003U-Net\u6267\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u5e76\u589e\u5f3a\u51e0\u4f55\u4fdd\u771f\u5ea6\uff1bTSFA\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u8bbe\u8ba1\uff08\u53ef\u8bad\u7ec3\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u548c\u51bb\u7ed3\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff09\u5c06\u53c2\u8003\u670d\u88c5\u7279\u5f81\u6ce8\u5165\u51bb\u7ed3\u7684\u53bb\u566aU-Net\u4e2d\u3002", "result": "\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAlignVTOFF\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u5e73\u94fa\u670d\u88c5\u7ed3\u679c\u5177\u6709\u6539\u8fdb\u7684\u7ed3\u6784\u771f\u5b9e\u6027\u548c\u9ad8\u9891\u7ec6\u8282\u4fdd\u771f\u5ea6\u3002", "conclusion": "AlignVTOFF\u901a\u8fc7\u5e76\u884cU-Net\u6846\u67b6\u548c\u7eb9\u7406-\u7a7a\u95f4\u7279\u5f81\u5bf9\u9f50\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\u4e2d\u7684\u51e0\u4f55\u53d8\u5f62\u548c\u7eb9\u7406\u4fdd\u6301\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2601.02264", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02264", "abs": "https://arxiv.org/abs/2601.02264", "authors": ["Boris Kriuk", "Fedor Kriuk"], "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network", "comment": "8 pages, 14 figures", "summary": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.", "AI": {"tldr": "POSEIDON\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u80fd\u91cf\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u5730\u9707\u4e8b\u4ef6\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86Gutenberg-Richter\u548cOmori-Utsu\u7b49\u5730\u9707\u5b66\u539f\u7406\uff0c\u5728\u4f59\u9707\u8bc6\u522b\u3001\u6d77\u5578\u751f\u6210\u6f5c\u529b\u548c\u524d\u9707\u68c0\u6d4b\u4e09\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5730\u9707\u9884\u6d4b\u4e2d\u5f80\u5f80\u4f5c\u4e3a\u9ed1\u7bb1\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u5df2\u5efa\u7acb\u7684\u7269\u7406\u5b9a\u5f8b\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u5229\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53c8\u80fd\u5d4c\u5165\u5730\u9707\u5b66\u57fa\u672c\u539f\u7406\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u63d0\u51faPOSEIDON\u6a21\u578b\uff0c\u5c06Gutenberg-Richter\u9707\u7ea7-\u9891\u7387\u5173\u7cfb\u548cOmori-Utsu\u4f59\u9707\u8870\u51cf\u5b9a\u5f8b\u7b49\u5730\u9707\u5b66\u539f\u7406\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7ea6\u675f\u5d4c\u5165\u5230\u57fa\u4e8e\u80fd\u91cf\u7684\u5efa\u6a21\u6846\u67b6\u4e2d\u3002\u540c\u65f6\u5904\u7406\u4e09\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u9884\u6d4b\u4efb\u52a1\uff1a\u4f59\u9707\u5e8f\u5217\u8bc6\u522b\u3001\u6d77\u5578\u751f\u6210\u6f5c\u529b\u548c\u524d\u9707\u68c0\u6d4b\u3002", "result": "POSEIDON\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f18\u4e8e\u68af\u5ea6\u63d0\u5347\u3001\u968f\u673a\u68ee\u6797\u548cCNN\u57fa\u7ebf\uff0c\u5728\u6240\u6709\u6bd4\u8f83\u65b9\u6cd5\u4e2d\u83b7\u5f97\u6700\u9ad8\u7684\u5e73\u5747F1\u5206\u6570\u3002\u5b66\u4e60\u5230\u7684\u7269\u7406\u53c2\u6570\u6536\u655b\u5230\u79d1\u5b66\u53ef\u89e3\u91ca\u7684\u503c\uff1aGutenberg-Richter b\u503c\u4e3a0.752\uff0cOmori-Utsu\u53c2\u6570p=0.835\uff0cc=0.1948\u5929\uff0c\u8fd9\u4e9b\u503c\u843d\u5728\u5df2\u5efa\u7acb\u7684\u5730\u9707\u5b66\u8303\u56f4\u5185\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "POSEIDON\u6210\u529f\u5730\u5c06\u7269\u7406\u7ea6\u675f\u4e0e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u4e14\u51c6\u786e\u7684\u5730\u9707\u9884\u6d4b\u3002\u540c\u65f6\u53d1\u5e03\u7684Poseidon\u6570\u636e\u96c6\uff08\u5305\u542b280\u4e07\u6b21\u4e8b\u4ef6\uff0c\u8de8\u8d8a30\u5e74\uff09\u4e3a\u7269\u7406\u4fe1\u606f\u5730\u9707\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2601.02088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02088", "abs": "https://arxiv.org/abs/2601.02088", "authors": ["Jiahao Bao", "Huazhen Liu", "Yu Zhuang", "Leran Tao", "Xinyu Xu", "Yongtao Shi", "Mengjia Cheng", "Yiming Wang", "Congshuang Ku", "Ting Zeng", "Yilang Du", "Siyi Chen", "Shunyao Shen", "Suncheng Xiang", "Hongbo Yu"], "title": "PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction", "comment": "31 pages, 8 figures", "summary": "Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.", "AI": {"tldr": "PhysSFI-Net\uff1a\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6b63\u988c\u624b\u672f\u540e\u7684\u8f6f\u7ec4\u7ec7\u53d8\u5f62\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6b63\u988c\u624b\u672f\u9700\u8981\u51c6\u786e\u6a21\u62df\u672f\u540e\u9762\u90e8\u5f62\u6001\u4ee5\u8fdb\u884c\u672f\u524d\u89c4\u5212\u3002\u4f20\u7edf\u751f\u7269\u529b\u5b66\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u4f18\u7684\u9884\u6d4b\u6846\u67b6\u3002", "method": "PhysSFI-Net\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1\uff09\u5177\u6709\u9885\u9762\u7279\u5f81\u548c\u624b\u672f\u8ba1\u5212\u7f16\u7801\u5668\u53ca\u6ce8\u610f\u529b\u673a\u5236\u7684\u5206\u5c42\u56fe\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u53d6\u9aa8\u9abc-\u9762\u90e8\u4ea4\u4e92\u7279\u5f81\uff1b2\uff09\u57fa\u4e8eLSTM\u7684\u5e8f\u5217\u9884\u6d4b\u5668\uff0c\u7528\u4e8e\u589e\u91cf\u8f6f\u7ec4\u7ec7\u53d8\u5f62\u9884\u6d4b\uff1b3\uff09\u751f\u7269\u529b\u5b66\u542f\u53d1\u7684\u6a21\u5757\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u9762\u90e8\u8868\u9762\u91cd\u5efa\u3002", "result": "\u5728135\u540d\u60a3\u8005\u6570\u636e\u96c6\u4e0a\uff0cPhysSFI-Net\u5b9e\u73b0\u4e86\u70b9\u4e91\u5f62\u72b6\u8bef\u5dee1.070\u00b10.088mm\u3001\u8868\u9762\u504f\u5dee\u8bef\u5dee1.296\u00b10.349mm\u3001\u6807\u5fd7\u70b9\u5b9a\u4f4d\u8bef\u5dee2.445\u00b11.326mm\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5ACMT-Net\u3002", "conclusion": "PhysSFI-Net\u80fd\u591f\u4ee5\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u5206\u8fa8\u7387\u9884\u6d4b\u672f\u540e\u9762\u90e8\u5f62\u6001\uff0c\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u6b63\u988c\u624b\u672f\u89c4\u5212\u548c\u6a21\u62df\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2601.02307", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02307", "abs": "https://arxiv.org/abs/2601.02307", "authors": ["Dina El Zein", "James Henderson"], "title": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck", "comment": "11 pages, 2 figures", "summary": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with R\u00e9nyi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u672c\u6570\u636e\u5171\u4eab\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u5e26\u566a\u58f0\u7684transformer\u5d4c\u5165\u6765\u4fdd\u62a4\u654f\u611f\u4fe1\u606f\uff0c\u4f7f\u7528\u975e\u53c2\u6570\u53d8\u5206\u5dee\u5206\u9690\u79c1(NVDP)\u5728\u9690\u79c1\u548c\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9690\u85cf\u8868\u793a\u53ef\u80fd\u7f16\u7801\u8f93\u5165\u654f\u611f\u4fe1\u606f\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u6062\u590d\u539f\u59cb\u6570\u636e\uff0ctransformer\u5d4c\u5165\u5305\u542b\u591a\u4e2a\u5411\u91cf\uff08\u6bcf\u4e2atoken\u4e00\u4e2a\uff09\u4f7f\u95ee\u9898\u66f4\u52a0\u4e25\u91cd\uff0c\u9700\u8981\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6570\u636e\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u975e\u53c2\u6570\u53d8\u5206\u5dee\u5206\u9690\u79c1(NVDP)\uff0c\u5c06\u975e\u53c2\u6570\u53d8\u5206\u4fe1\u606f\u74f6\u9888(NVIB)\u5c42\u96c6\u6210\u5230transformer\u67b6\u6784\u4e2d\uff0c\u5411\u591a\u5411\u91cf\u5d4c\u5165\u6ce8\u5165\u566a\u58f0\uff0c\u4f7f\u7528R\u00e9nyi\u6563\u5ea6\u53ca\u5176\u5bf9\u5e94\u7684\u8d1d\u53f6\u65af\u5dee\u5206\u9690\u79c1(BDP)\u4fdd\u8bc1\u6765\u6d4b\u91cf\u9690\u79c1\u4fdd\u62a4\uff0c\u901a\u8fc7\u8bad\u7ec3NVIB\u5c42\u6839\u636e\u6548\u7528\u6821\u51c6\u566a\u58f0\u6c34\u5e73\u3002", "result": "\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1NVDP\uff0c\u901a\u8fc7\u8c03\u6574\u566a\u58f0\u6c34\u5e73\u5728\u9690\u79c1\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b9e\u73b0\u6709\u6548\u6743\u8861\uff0c\u8f83\u4f4e\u566a\u58f0\u6c34\u5e73\u4e0b\u6a21\u578b\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u540c\u65f6\u63d0\u4f9b\u5f3a\u9690\u79c1\u4fdd\u8bc1\u3002", "conclusion": "NVDP\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e73\u8861\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u6548\u7528\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u672c\u6570\u636e\u5171\u4eab\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02098", "abs": "https://arxiv.org/abs/2601.02098", "authors": ["Jinlong Fan", "Shanshan Zhao", "Liang Zheng", "Jing Zhang", "Yuxiang Yang", "Mingming Gong"], "title": "InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting", "comment": null, "summary": "Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.", "AI": {"tldr": "InpaintHuman\uff1a\u4e00\u79cd\u4ece\u906e\u6321\u7684\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u5b8c\u6574\u53ef\u52a8\u753b3D\u4eba\u4f53\u5316\u8eab\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6UV\u53c2\u6570\u5316\u8868\u793a\u548c\u8eab\u4efd\u4fdd\u6301\u6269\u6563\u4fee\u590d\u6a21\u5757\u89e3\u51b3\u4e25\u91cd\u906e\u6321\u4e0b\u7684\u91cd\u5efa\u95ee\u9898\u3002", "motivation": "\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u5b8c\u6574\u53ef\u52a8\u753b\u76843D\u4eba\u4f53\u5316\u8eab\u5728\u4e25\u91cd\u906e\u6321\u60c5\u51b5\u4e0b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u89c2\u6d4b\u65f6\uff0c\u5f80\u5f80\u4ea7\u751f\u635f\u574f\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u5177\u6709\u5206\u5c42\u7c97\u5230\u7ec6\u7279\u5f81\u63d2\u503c\u7684\u591a\u5c3a\u5ea6UV\u53c2\u6570\u5316\u8868\u793a\uff0c\u80fd\u591f\u9c81\u68d2\u5730\u91cd\u5efa\u906e\u6321\u533a\u57df\u540c\u65f6\u4fdd\u7559\u51e0\u4f55\u7ec6\u8282\uff1b2\uff09\u8eab\u4efd\u4fdd\u6301\u6269\u6563\u4fee\u590d\u6a21\u5757\uff0c\u5c06\u6587\u672c\u53cd\u8f6c\u4e0e\u8bed\u4e49\u6761\u4ef6\u5f15\u5bfc\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u7279\u5b9a\u4e3b\u4f53\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u8865\u5168\u3002\u4e0eSDS\u65b9\u6cd5\u4e0d\u540c\uff0c\u91c7\u7528\u76f4\u63a5\u50cf\u7d20\u7ea7\u76d1\u7763\u786e\u4fdd\u8eab\u4efd\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\uff08PeopleSnapshot\u3001ZJU-MoCap\uff09\u548c\u771f\u5b9e\u573a\u666f\uff08OcMotion\uff09\u4e0a\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u5728\u4e0d\u540c\u59ff\u6001\u548c\u89c6\u89d2\u4e0b\u91cd\u5efa\u8d28\u91cf\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "InpaintHuman\u80fd\u591f\u4ece\u906e\u6321\u7684\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5b8c\u6574\u4e14\u53ef\u52a8\u753b\u7684\u5316\u8eab\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e25\u91cd\u906e\u6321\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.02313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02313", "abs": "https://arxiv.org/abs/2601.02313", "authors": ["Hanzaleh Akbari Nodehi", "Viveck R. Cadambe", "Mohammad Ali Maddah-Ali"], "title": "Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning", "comment": null, "summary": "Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.\n  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u7f16\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7406\u6027\u5bf9\u624b\uff08\u800c\u975e\u7eaf\u7cb9\u6076\u610f\u5bf9\u624b\uff09\u7684\u573a\u666f\uff0c\u80fd\u591f\u5728\u8bda\u5b9e\u8282\u70b9\u4e0d\u5360\u591a\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u975e\u96f6\u6982\u7387\u7684\u6570\u636e\u6062\u590d\uff0c\u5e76\u5177\u6709Sybil\u62b5\u6297\u6027\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u7b49\u65b0\u5174\u5e94\u7528\u4e2d\uff0c\u53c2\u4e0e\u8282\u70b9\u56e0\u8d21\u732e\u800c\u83b7\u5f97\u5956\u52b1\uff0c\u8fd9\u50ac\u751f\u4e86\u7406\u6027\u5bf9\u624b\uff08strategic adversaries\uff09\u800c\u975e\u7eaf\u7cb9\u6076\u610f\u5bf9\u624b\u3002\u4f20\u7edf\u7f16\u7801\u7406\u8bba\u5047\u8bbe\u6700\u574f\u60c5\u51b5\u5bf9\u6297\u6a21\u578b\uff0c\u8981\u6c42\u8bda\u5b9e\u8282\u70b9\u6570\u91cf\u8d85\u8fc7\u5bf9\u6297\u8282\u70b9\uff0c\u4f46\u5728\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u8fd9\u79cd\u5047\u8bbe\u4e0d\u6210\u7acb\uff0c\u9700\u8981\u65b0\u7684\u7f16\u7801\u6846\u67b6\u6765\u5e94\u5bf9\u7406\u6027\u5bf9\u624b\u3002", "method": "\u63d0\u51fa\"\u7f16\u7801\u535a\u5f08\"\uff08game of coding\uff09\u8fd9\u4e00\u65b0\u9896\u7684\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u5c06\u7f16\u7801\u7406\u8bba\u6269\u5c55\u5230\u4fe1\u4efb\u6700\u5c0f\u5316\u7684\u8bbe\u7f6e\u4e2d\u3002\u91cd\u70b9\u7814\u7a76\u91cd\u590d\u7f16\u7801\uff08repetition coding\uff09\uff0c\u5c55\u793a\u8be5\u6846\u67b6\u7684\u4e24\u4e2a\u5173\u952e\u7279\u6027\uff1a1\uff09\u5373\u4f7f\u5bf9\u6297\u8282\u70b9\u5360\u591a\u6570\u4e5f\u80fd\u5b9e\u73b0\u975e\u96f6\u6982\u7387\u7684\u6570\u636e\u6062\u590d\uff1b2\uff09Sybil\u62b5\u6297\u6027\uff08\u5bf9\u6297\u8282\u70b9\u6570\u91cf\u589e\u52a0\u65f6\u5747\u8861\u4fdd\u6301\u4e0d\u53d8\uff09\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u8bda\u5b9e\u8282\u70b9\u4e0d\u5360\u591a\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6570\u636e\u6062\u590d\uff0c\u6253\u7834\u4e86\u4f20\u7edf\u7f16\u7801\u7406\u8bba\u7684\u9650\u5236\u3002\u91cd\u590d\u7f16\u7801\u5c55\u793a\u4e86\u5373\u4f7f\u5728\u5bf9\u6297\u8282\u70b9\u5360\u591a\u6570\u65f6\u4e5f\u80fd\u83b7\u5f97\u975e\u96f6\u6062\u590d\u6982\u7387\uff0c\u5e76\u4e14\u7cfb\u7edf\u5177\u6709Sybil\u62b5\u6297\u6027\uff0c\u5bf9\u6297\u8282\u70b9\u6570\u91cf\u589e\u52a0\u4e0d\u4f1a\u6539\u53d8\u5747\u8861\u72b6\u6001\u3002", "conclusion": "\u8bba\u6587\u4e3a\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7684\u7f16\u7801\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u535a\u5f08\u8bba\u89c6\u89d2\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7406\u6027\u5bf9\u624b\u573a\u666f\u3002\u63d0\u51fa\u7684\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u7f16\u7801\u7406\u8bba\u7684\u9650\u5236\uff0c\u4e3a\u5916\u5305\u8ba1\u7b97\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u540c\u65f6\u6307\u51fa\u4e86\u5728\u5bf9\u624b\u7b56\u7565\u672a\u77e5\u60c5\u51b5\u4e0b\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u82e5\u5e72\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2601.02102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02102", "abs": "https://arxiv.org/abs/2601.02102", "authors": ["Jiaqi Yao", "Zhongmiao Yan", "Jingyi Xu", "Songpengcheng Xia", "Yan Xiang", "Ling Pei"], "title": "360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images", "comment": null, "summary": "3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u524d\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6-\u6cd5\u5411\u51e0\u4f55\u6b63\u5219\u5316\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u6539\u55843D\u91cd\u5efa\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u5728\u7a00\u758f\u89c6\u89d2\u6216\u4f4e\u7eb9\u7406\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u9700\u8981\u9010\u573a\u666f\u4f18\u5316\u4e14\u7f3a\u4e4f\u5b9e\u65f6\u6027\uff0c\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u6ce8\u91cd\u89c6\u89c9\u8d28\u91cf\u4f46\u51e0\u4f55\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5728\u7a7a\u95f4\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u5e94\u7528", "method": "\u63d0\u51fa\u524d\u9988\u5f0f3D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u5f15\u5165\u6df1\u5ea6-\u6cd5\u5411\u51e0\u4f55\u6b63\u5219\u5316\uff0c\u5c06\u6e32\u67d3\u6df1\u5ea6\u68af\u5ea6\u4e0e\u6cd5\u5411\u4fe1\u606f\u8026\u5408\uff0c\u76d1\u7763\u9ad8\u65af\u65cb\u8f6c\u3001\u5c3a\u5ea6\u548c\u4f4d\u7f6e\u53c2\u6570\uff0c\u63d0\u5347\u70b9\u4e91\u548c\u8868\u9762\u7cbe\u5ea6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u4e3a\u7a7a\u95f4\u611f\u77e5\u4efb\u52a1\u4e2d\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51e0\u4f55\u6b63\u5219\u5316\u673a\u5236\uff0c\u57283D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u5e73\u8861\u4e86\u6e32\u67d3\u8d28\u91cf\u4e0e\u91cd\u5efa\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8eAR\u3001\u673a\u5668\u4eba\u548c\u6570\u5b57\u5b6a\u751f\u7b49\u7a7a\u95f4\u667a\u80fd\u5e94\u7528"}}
{"id": "2601.02316", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02316", "abs": "https://arxiv.org/abs/2601.02316", "authors": ["Siddharth Joshi", "Haoli Yin", "Rishabh Adiga", "Ricardo Monti", "Aldo Carranza", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Darren Teh", "David Schwab", "Fan Pan", "Haakon Mongstad", "Jack Urbanek", "Jason Lee", "Jason Telanoff", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Parth Doshi", "Paul Burstein", "Pratyush Maini", "Scott Loftin", "Spandan Das", "Tony Jiang", "Vineeth Dorna", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations", "comment": null, "summary": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u4e09\u4e2a\u7406\u60f3\u6807\u51c6\uff1a\u5fe0\u5b9e\u6027\u3001\u533a\u5206\u6027\u548c\u6548\u7387\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u5b58\u5728\u591a\u9879\u7f3a\u9677\uff0c\u901a\u8fc7\u8f6c\u6362\u548c\u8fc7\u6ee4\u65b9\u6cd5\u6539\u8fdb\u8bc4\u4f30\u8d28\u91cf\uff0c\u53d1\u5e03DatBench\u8bc4\u4f30\u5957\u4ef6\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\u5c1a\u4e0d\u6210\u719f\uff0c\u5b58\u5728\u591a\u79cd\u7f3a\u9677\uff1a\u591a\u9879\u9009\u62e9\u9898\u5f62\u5f0f\u5956\u52b1\u731c\u6d4b\u3001\u4e0d\u80fd\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u3001\u5bb9\u6613\u9971\u548c\uff1b\u90e8\u5206\u95ee\u9898\u65e0\u9700\u56fe\u50cf\u5373\u53ef\u89e3\u7b54\uff1b\u6570\u636e\u6807\u6ce8\u9519\u8bef\u6216\u6a21\u7cca\uff1b\u8bc4\u4f30\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff08\u5360\u5f00\u53d1\u8ba1\u7b97\u8d44\u6e90\u768420%\uff09\u3002\u9700\u8981\u5efa\u7acb\u66f4\u4e25\u8c28\u548c\u53ef\u6301\u7eed\u7684\u8bc4\u4f30\u5b9e\u8df5\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u73b0\u6709\u8bc4\u4f30\uff1a\u5fe0\u5b9e\u6027\u3001\u533a\u5206\u6027\u548c\u6548\u7387\u3002\u8bc6\u522b\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff0c\u63d0\u51fa\u901a\u8fc7\u8f6c\u6362\uff08\u5c06\u591a\u9879\u9009\u62e9\u9898\u8f6c\u4e3a\u751f\u6210\u4efb\u52a1\uff09\u548c\u8fc7\u6ee4\uff08\u79fb\u9664\u53ef\u76f2\u7b54\u548c\u9519\u8bef\u6807\u6ce8\u6837\u672c\uff09\u6765\u6539\u8fdb\u73b0\u6709\u57fa\u51c6\u3002\u53d1\u5e03DatBench-Full\uff0833\u4e2a\u6570\u636e\u96c6\uff09\u548cDatBench\uff08\u533a\u5206\u6027\u5b50\u96c6\uff09\u3002", "result": "\u591a\u9879\u9009\u62e9\u9898\u8f6c\u4e3a\u751f\u6210\u4efb\u52a1\u540e\uff0c\u6a21\u578b\u80fd\u529b\u4e0b\u964d\u9ad8\u8fbe35%\uff1b\u8fc7\u6ee4\u53ef\u76f2\u7b54\u548c\u9519\u8bef\u6807\u6ce8\u6837\u672c\u63d0\u9ad8\u4e86\u533a\u5206\u80fd\u529b\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1bDatBench\u5b50\u96c6\u5b9e\u73b013\u500d\u5e73\u5747\u52a0\u901f\uff08\u6700\u9ad850\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u76f8\u4f3c\u7684\u533a\u5206\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u548c\u53ef\u6301\u7eed\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u7684\u7f3a\u9677\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u5e76\u53d1\u5e03\u6e05\u7406\u540e\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u8861\u91cf\u6a21\u578b\u80fd\u529b\u5e76\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u3002"}}
{"id": "2601.02103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02103", "abs": "https://arxiv.org/abs/2601.02103", "authors": ["Yating Wang", "Yuan Sun", "Xuan Wang", "Ran Yi", "Boyao Zhou", "Yipengjing Sun", "Hongyu Liu", "Yinuo Wang", "Lizhuang Ma"], "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures", "comment": null, "summary": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.", "AI": {"tldr": "HeadLighter\uff1a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5934\u90e8\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u53ef\u89e3\u91ca\u7684\u5206\u89e3\u65b9\u6cd5\u5b9e\u73b0\u53ef\u63a7\u91cd\u5149\u7167\uff0c\u652f\u6301\u663e\u5f0f\u5149\u7167\u548c\u89c6\u89d2\u7f16\u8f91", "motivation": "\u73b0\u67093D\u611f\u77e5\u5934\u90e8\u751f\u6210\u6a21\u578b\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u3001\u903c\u771f\u4e14\u89c6\u89d2\u4e00\u81f4\u7684\u5934\u90e8\u5408\u6210\uff0c\u4f46\u5b58\u5728\u5149\u7167\u4e0e\u5185\u5728\u5916\u89c2\u6df1\u5ea6\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8fdb\u884c\u53ef\u63a7\u91cd\u5149\u7167\u3002\u73b0\u6709\u89e3\u7f20\u65b9\u6cd5\u4f9d\u8d56\u5f3a\u5047\u8bbe\u8fdb\u884c\u5f31\u76d1\u7763\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u5904\u7406\u590d\u6742\u5149\u7167\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faHeadLighter\u6846\u67b6\uff1a1\uff09\u8bbe\u8ba1\u53cc\u5206\u652f\u67b6\u6784\u5206\u522b\u5efa\u6a21\u5149\u7167\u4e0d\u53d8\u5934\u90e8\u5c5e\u6027\u548c\u7269\u7406\u57fa\u7840\u7684\u6e32\u67d3\u7ec4\u4ef6\uff1b2\uff09\u91c7\u7528\u6e10\u8fdb\u5f0f\u89e3\u7f20\u8bad\u7ec3\uff0c\u5728\u53d7\u63a7\u5149\u7167\u6761\u4ef6\u4e0b\u4f7f\u7528\u5149\u821e\u53f0\u8bbe\u7f6e\u91c7\u96c6\u7684\u591a\u89c6\u89d2\u56fe\u50cf\u8fdb\u884c\u76d1\u7763\uff1b3\uff09\u5f15\u5165\u84b8\u998f\u7b56\u7565\u751f\u6210\u9ad8\u8d28\u91cf\u6cd5\u7ebf\u4ee5\u5b9e\u73b0\u903c\u771f\u6e32\u67d3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\u548c\u5b9e\u65f6\u6e32\u67d3\u7684\u540c\u65f6\uff0c\u652f\u6301\u663e\u5f0f\u5149\u7167\u548c\u89c6\u89d2\u7f16\u8f91\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002", "conclusion": "HeadLighter\u901a\u8fc7\u7269\u7406\u53ef\u89e3\u91ca\u7684\u5206\u89e3\u65b9\u6cd5\u89e3\u51b3\u4e86\u5934\u90e8\u751f\u6210\u6a21\u578b\u4e2d\u5149\u7167\u4e0e\u5916\u89c2\u7684\u7ea0\u7f20\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u91cd\u5149\u7167\uff0c\u4e3a3D\u5934\u90e8\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u7f16\u8f91\u80fd\u529b\u3002"}}
{"id": "2601.02360", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02360", "abs": "https://arxiv.org/abs/2601.02360", "authors": ["Yazan Obeidi", "Amir Sarfi", "Joel Lidin", "Paul Janson", "Eugene Belilovsky"], "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs", "comment": null, "summary": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.", "AI": {"tldr": "SparseLoCo\u4f4e\u901a\u4fe1\u6570\u636e\u5e76\u884c\u65b9\u6cd5\u4e0e\u4f4e\u5e26\u5bbd\u6d41\u6c34\u7ebf\u6a21\u578b\u5e76\u884c\u7ed3\u5408\uff0c\u901a\u8fc7\u6fc0\u6d3b\u548c\u6fc0\u6d3b\u68af\u5ea6\u538b\u7f29\u5b9e\u73b0\u5f02\u6784\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u9700\u8981\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u4f46\u5e26\u5bbd\u9650\u5236\u4f7f\u5f97\u5728\u6570\u636e\u4e2d\u5fc3\u4e4b\u5916\u96be\u4ee5\u6269\u5c55\uff0c\u7279\u522b\u662f\u5f53\u6a21\u578b\u5e76\u884c\u9700\u8981\u9891\u7e41\u7684\u5927\u89c4\u6a21\u8bbe\u5907\u95f4\u901a\u4fe1\u65f6\u3002", "method": "\u63d0\u51fa\u5f02\u6784\u5206\u5e03\u5f0f\u8bad\u7ec3\u6846\u67b6\uff1a\u9ad8\u5e26\u5bbd\u53c2\u4e0e\u8005\u6258\u7ba1\u5b8c\u6574\u526f\u672c\uff0c\u8d44\u6e90\u6709\u9650\u53c2\u4e0e\u8005\u901a\u8fc7\u5e26\u6fc0\u6d3b\u538b\u7f29\u7684\u6d41\u6c34\u7ebf\u5e76\u884c\u5171\u540c\u5b9e\u4f8b\u5316\u526f\u672c\u3002\u5c06\u5b50\u7a7a\u95f4\u6d41\u6c34\u7ebf\u538b\u7f29\u4e0eSparseLoCo\u65b9\u6cd5\u9002\u914d\uff0c\u5b9e\u73b0\u4f4e\u901a\u4fe1\u6570\u636e\u5e76\u884c\u4e0e\u4f4e\u5e26\u5bbd\u6a21\u578b\u5e76\u884c\u7684\u7ed3\u5408\u3002", "result": "\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\uff081.78\u4ebf-10\u4ebf\u53c2\u6570\uff09\u4e2d\uff0c\u6fc0\u6d3b\u538b\u7f29\u4e0eSparseLoCo\u7ed3\u5408\u6210\u672c\u9002\u4e2d\uff0c\u9009\u62e9\u6027\uff08\u5f02\u6784\uff09\u538b\u7f29\u76f8\u6bd4\u538b\u7f29\u6240\u6709\u526f\u672c\u80fd\u6301\u7eed\u6539\u5584\u635f\u5931-\u901a\u4fe1\u6743\u8861\uff0c\u7279\u522b\u662f\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u6548\u679c\u66f4\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5c06\u4f4e\u5e26\u5bbd\u6a21\u578b\u5e76\u884c\u548c\u5f02\u6784\u53c2\u4e0e\u8005\u7eb3\u5165LLM\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u5728\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e2d\u6269\u5c55\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002"}}
{"id": "2601.02107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02107", "abs": "https://arxiv.org/abs/2601.02107", "authors": ["Jiancheng Huang", "Mingfu Yan", "Songyan Chen", "Yi Huang", "Shifeng Chen"], "title": "MagicFight: Personalized Martial Arts Combat Video Generation", "comment": "Accepted by ACM MM 2024", "summary": "Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.\n  Website: https://MingfuYAN.github.io/MagicFight/\n  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta", "AI": {"tldr": "MagicFight\u63d0\u51fa\u4e2a\u6027\u5316\u6b66\u672f\u683c\u6597\u89c6\u9891\u751f\u6210\u65b0\u4efb\u52a1\uff0c\u9488\u5bf9\u73b0\u6709\u5355\u4eba\u821e\u8e48\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u53cc\u4eba\u4ea4\u4e92\u7684\u7f3a\u9677\uff0c\u901a\u8fc7Unity\u521b\u5efa\u4e13\u7528\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u80fd\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u8fde\u8d2f\u6027\u7684\u53cc\u4eba\u683c\u6597\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4e2a\u6027\u5316\u89c6\u9891\u751f\u6210\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u4eba\u573a\u666f\uff0c\u800c\u53cc\u4eba\u4ea4\u4e92\u7279\u522b\u662f\u6b66\u672f\u683c\u6597\u9886\u57df\u5c1a\u672a\u63a2\u7d22\u3002\u73b0\u6709\u5355\u4eba\u821e\u8e48\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u53cc\u4eba\u4ea4\u4e92\u65f6\u5b58\u5728\u8eab\u4efd\u6df7\u6dc6\u3001\u80a2\u4f53\u5f02\u5e38\u548c\u52a8\u4f5c\u4e0d\u5339\u914d\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u4f7f\u7528Unity\u6e38\u620f\u7269\u7406\u5f15\u64ce\u521b\u5efa\u4e13\u95e8\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u5316\u76843D\u89d2\u8272\u3001\u6b66\u672f\u52a8\u4f5c\u548c\u573a\u666f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u6539\u8fdb\u548c\u8c03\u6574\u73b0\u6709\u6a21\u578b\u4e0e\u7b56\u7565\uff0c\u5f00\u53d1MagicFight\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53cc\u4eba\u683c\u6597\u89c6\u9891\u3002", "result": "MagicFight\u80fd\u591f\u751f\u6210\u4fdd\u6301\u4e2a\u4f53\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u52a8\u4f5c\u5e8f\u5217\u8fde\u8d2f\u6027\u7684\u9ad8\u4fdd\u771f\u53cc\u4eba\u683c\u6597\u89c6\u9891\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u89c6\u9891\u5185\u5bb9\u521b\u4f5c\u9886\u57df\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u521b\u4e86\u4e2a\u6027\u5316\u6b66\u672f\u683c\u6597\u89c6\u9891\u751f\u6210\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u4e13\u7528\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u89e3\u51b3\u4e86\u53cc\u4eba\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u7684\u72ec\u7279\u6311\u6218\uff0c\u4e3a\u672a\u6765\u4ea4\u4e92\u5f0f\u89c6\u9891\u5185\u5bb9\u521b\u65b0\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.21576", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21576", "abs": "https://arxiv.org/abs/2512.21576", "authors": ["Haoyi Zhou", "Shuo Li", "Tianyu Chen", "Qi Song", "Chonghan Gao", "Jianxin Li"], "title": "Towards Long-window Anchoring in Vision-Language Model Distillation", "comment": "Accepted by AAAI 2026", "summary": "While large vision-language models (VLMs) demonstrate strong long-context understanding, their prevalent small branches fail on linguistics-photography alignment for a limited window size. We discover that knowledge distillation improves students' capability as a complement to Rotary Position Embeddings (RoPE) on window sizes (anchored from large models). Building on this insight, we propose LAid, which directly aims at the transfer of long-range attention mechanisms through two complementary components: (1) a progressive distance-weighted attention matching that dynamically emphasizes longer position differences during training, and (2) a learnable RoPE response gain modulation that selectively amplifies position sensitivity where needed. Extensive experiments across multiple model families demonstrate that LAid-distilled models achieve up to 3.2 times longer effective context windows compared to baseline small models, while maintaining or improving performance on standard VL benchmarks. Spectral analysis also suggests that LAid successfully preserves crucial low-frequency attention components that conventional methods fail to transfer. Our work not only provides practical techniques for building more efficient long-context VLMs but also offers theoretical insights into how positional understanding emerges and transfers during distillation.", "AI": {"tldr": "LAid\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u73b0\u6bd4\u57fa\u7ebf\u5c0f\u6a21\u578b\u957f3.2\u500d\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6027\u80fd", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u5f3a\u5927\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5176\u6d41\u884c\u7684\u5c0f\u5206\u652f\u6a21\u578b\u7531\u4e8e\u7a97\u53e3\u5927\u5c0f\u6709\u9650\uff0c\u5728\u8bed\u8a00-\u89c6\u89c9\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u53d1\u73b0\u77e5\u8bc6\u84b8\u998f\u53ef\u4ee5\u8865\u5145RoPE\u5728\u7a97\u53e3\u5927\u5c0f\u65b9\u9762\u7684\u80fd\u529b", "method": "\u63d0\u51faLAid\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a(1)\u6e10\u8fdb\u5f0f\u8ddd\u79bb\u52a0\u6743\u6ce8\u610f\u529b\u5339\u914d\uff0c\u5728\u8bad\u7ec3\u4e2d\u52a8\u6001\u5f3a\u8c03\u8f83\u957f\u4f4d\u7f6e\u5dee\u5f02\uff1b(2)\u53ef\u5b66\u4e60\u7684RoPE\u54cd\u5e94\u589e\u76ca\u8c03\u5236\uff0c\u9009\u62e9\u6027\u589e\u5f3a\u9700\u8981\u7684\u4f4d\u7f6e\u654f\u611f\u6027", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLAid\u84b8\u998f\u7684\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u5c0f\u6a21\u578b\u5b9e\u73b0\u4e86\u957f\u8fbe3.2\u500d\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u540c\u65f6\u5728\u6807\u51c6VL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u6216\u6539\u8fdb\u6027\u80fd\u3002\u9891\u8c31\u5206\u6790\u663e\u793aLAid\u6210\u529f\u4fdd\u7559\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u8f6c\u79fb\u7684\u5173\u952e\u4f4e\u9891\u6ce8\u610f\u529b\u7ec4\u4ef6", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6280\u672f\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u4f4d\u7f6e\u7406\u89e3\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5982\u4f55\u51fa\u73b0\u548c\u8f6c\u79fb\u7684\u7406\u8bba\u89c1\u89e3"}}
{"id": "2601.02126", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02126", "abs": "https://arxiv.org/abs/2601.02126", "authors": ["Xavier Bou", "Elliot Vincent", "Gabriele Facciolo", "Rafael Grompone von Gioi", "Jean-Michel Morel", "Thibaud Ehret"], "title": "Remote Sensing Change Detection via Weak Temporal Supervision", "comment": null, "summary": "Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u73b0\u6709\u5355\u65f6\u76f8\u9065\u611f\u6570\u636e\u96c6\u8fdb\u884c\u5f31\u65f6\u95f4\u76d1\u7763\u7684\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u65b0\u6807\u6ce8\uff0c\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u83b7\u53d6\u591a\u65f6\u76f8\u89c2\u6d4b\uff0c\u5047\u8bbe\u771f\u5b9e\u53cc\u65f6\u76f8\u5bf9\u5927\u591a\u65e0\u53d8\u5316\uff0c\u4e0d\u540c\u4f4d\u7f6e\u56fe\u50cf\u914d\u5bf9\u751f\u6210\u53d8\u5316\u793a\u4f8b", "motivation": "\u9065\u611f\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5408\u6210\u6570\u636e\u6216\u4eba\u5de5\u751f\u6210\u53d8\u5316\u5bf9\uff0c\u4f46\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u6709\u9650", "method": "\u6269\u5c55\u5355\u65f6\u76f8\u9065\u611f\u6570\u636e\u96c6\u83b7\u53d6\u4e0d\u540c\u65f6\u95f4\u7684\u65b0\u89c2\u6d4b\uff0c\u8bad\u7ec3\u53d8\u5316\u68c0\u6d4b\u6a21\u578b\u65f6\u5047\u8bbe\u771f\u5b9e\u53cc\u65f6\u76f8\u5bf9\u5927\u591a\u65e0\u53d8\u5316\uff0c\u540c\u65f6\u914d\u5bf9\u4e0d\u540c\u4f4d\u7f6e\u56fe\u50cf\u751f\u6210\u53d8\u5316\u793a\u4f8b\u3002\u91c7\u7528\u5bf9\u8c61\u611f\u77e5\u53d8\u5316\u56fe\u751f\u6210\u548c\u8fed\u4ee3\u7cbe\u70bc\u8fc7\u7a0b\u5904\u7406\u5f31\u6807\u7b7e\u566a\u58f0", "result": "\u5728\u6269\u5c55\u7684FLAIR\u548cIAILD\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5728\u96f6\u6837\u672c\u548c\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5728\u6cd5\u56fd\u5927\u8303\u56f4\u533a\u57df\u7684\u5e94\u7528\u7ed3\u679c\uff0c\u7a81\u663e\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027", "conclusion": "\u63d0\u51fa\u7684\u5f31\u65f6\u95f4\u76d1\u7763\u7b56\u7565\u80fd\u591f\u6709\u6548\u5229\u7528\u73b0\u6709\u5355\u65f6\u76f8\u6570\u636e\u96c6\u8fdb\u884c\u53d8\u5316\u68c0\u6d4b\u8bad\u7ec3\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\uff0c\u5728\u96f6\u6837\u672c\u548c\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u6269\u5c55\u6f5c\u529b"}}
{"id": "2601.02139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02139", "abs": "https://arxiv.org/abs/2601.02139", "authors": ["Chenyang Lai", "Shuaiyu Chen", "Tianjin Huang", "Siyang Song", "Guangliang Cheng", "Chunbo Luo", "Zeyu Fu"], "title": "Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery", "comment": null, "summary": "Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.", "AI": {"tldr": "\u63d0\u51faOSCD\u4efb\u52a1\u548cTAHI\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u65f6\u76f8SAR\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u63d0\u9ad8\u6f0f\u6cb9\u8bc6\u522b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u8bef\u62a5", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5355\u5e45SAR\u56fe\u50cf\u7684\u6df1\u5ea6\u5b66\u4e60\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u771f\u5b9e\u6f0f\u6cb9\u4e0e\u89c6\u89c9\u76f8\u4f3c\u7684\u6d77\u9762\u7279\u5f81\uff08\u5982\u751f\u7269\u6cb9\u819c\u3001\u4f4e\u98ce\u533a\uff09\uff0c\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\u548c\u6709\u9650\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b", "method": "\u63d0\u51faOil Spill Change Detection (OSCD)\u53cc\u65f6\u76f8\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1Temporal-Aware Hybrid Inpainting (TAHI)\u6846\u67b6\uff0c\u5305\u542b\u9ad8\u4fdd\u771f\u6df7\u5408\u4fee\u590d\uff08\u751f\u6210\u65e0\u6cb9\u9884\u6f0f\u6cb9\u56fe\u50cf\uff09\u548c\u65f6\u95f4\u771f\u5b9e\u6027\u589e\u5f3a\uff08\u786e\u4fdd\u8f90\u5c04\u548c\u6d77\u6d0b\u72b6\u6001\u4e00\u81f4\u6027\uff09\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6", "result": "\u6784\u5efa\u9996\u4e2aOSCD\u6570\u636e\u96c6\uff0c\u57fa\u51c6\u6d4b\u8bd5\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u53d8\u5316\u68c0\u6d4b\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793aOSCD\u76f8\u6bd4\u4f20\u7edf\u5206\u5272\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8bef\u62a5\u5e76\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6", "conclusion": "\u65f6\u95f4\u611f\u77e5\u65b9\u6cd5\u5bf9\u4e8e\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u6f0f\u6cb9\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cOSCD\u4efb\u52a1\u4e3a\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84"}}
{"id": "2601.02147", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02147", "abs": "https://arxiv.org/abs/2601.02147", "authors": ["Sunny Gupta", "Shounak Das", "Amit Sethi"], "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models", "comment": "Accepted at the AAAI 2026 Workshop AIR-FM, Assessing and Improving Reliability of Foundation Models in the Real World", "summary": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.", "AI": {"tldr": "BiPrompt\uff1a\u4e00\u4e2a\u53cc\u8fb9\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u540c\u65f6\u4f18\u5316\u6765\u51cf\u5c11CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u4f9d\u8d56", "motivation": "\u73b0\u6709\u7684\u53bb\u504f\u65b9\u6cd5\u901a\u5e38\u53ea\u9488\u5bf9\u5355\u4e00\u6a21\u6001\uff08\u89c6\u89c9\u6216\u6587\u672c\uff09\uff0c\u5bfc\u81f4\u90e8\u5206\u9c81\u68d2\u6027\u548c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u4e0d\u7a33\u5b9a\u9002\u5e94\u3002\u9700\u8981\u540c\u65f6\u5904\u7406\u4e24\u4e2a\u6a21\u6001\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u4f9d\u8d56\u95ee\u9898", "method": "\u63d0\u51fa\u53cc\u8fb9\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff1a\u89c6\u89c9\u4fa7\u91c7\u7528\u7ed3\u6784\u5316\u6ce8\u610f\u529b\u5f15\u5bfc\u64e6\u9664\u6765\u6291\u5236\u80cc\u666f\u6fc0\u6d3b\u5e76\u5f3a\u5236\u56e0\u679c\u533a\u57df\u4e0e\u865a\u5047\u533a\u57df\u4e4b\u95f4\u7684\u6b63\u4ea4\u9884\u6d4b\u4e00\u81f4\u6027\uff1b\u6587\u672c\u4fa7\u5f15\u5165\u5e73\u8861\u63d0\u793a\u5f52\u4e00\u5316\uff0c\u901a\u8fc7\u5b66\u4e60\u6027\u91cd\u65b0\u4e2d\u5fc3\u5316\u673a\u5236\u5c06\u7c7b\u522b\u5d4c\u5165\u5bf9\u9f50\u5230\u5404\u5411\u540c\u6027\u8bed\u4e49\u7a7a\u95f4", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u504f\u5dee\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\uff0c\u5728\u5e73\u5747\u51c6\u786e\u7387\u548c\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb", "conclusion": "BiPrompt\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u8def\u5f84\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u9886\u57df\u76d1\u7763\uff0c\u5373\u53ef\u5b9e\u73b0\u53ef\u4fe1\u8d56\u4e14\u56e0\u679c\u57fa\u7840\u7684\u89c6\u89c9\u8bed\u8a00\u9002\u5e94"}}
{"id": "2601.02177", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.02177", "abs": "https://arxiv.org/abs/2601.02177", "authors": ["Oliver Custance", "Saad Khan", "Simon Parkinson"], "title": "Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32", "comment": null, "summary": "WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\\%, $\u03c3$=3.74\\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.", "AI": {"tldr": "\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e866\u79cd\u4fe1\u53f7\u5206\u79bb\u65b9\u6cd5\u5728\u5546\u7528ESP32 WiFi\u4f20\u611f\u5668\u4e0a\u7684\u591a\u4eba\u6b65\u6001\u8bc6\u522b\u6027\u80fd\uff0c\u53d1\u73b0\u6240\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u90fd\u5f88\u4f4e\uff0845-56%\uff09\uff0c\u8868\u660e\u786c\u4ef6\u9650\u5236\u662f\u4e3b\u8981\u74f6\u9888\u800c\u975e\u7b97\u6cd5\u95ee\u9898\u3002", "motivation": "\u867d\u7136WiFi CSI\u5728\u5355\u4eba\u6b65\u6001\u8bc6\u522b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u591a\u4eba\u8bc6\u522b\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u6602\u8d35\u7684\u5b9a\u5236\u8bbe\u5907\u3002\u6838\u5fc3\u95ee\u9898\u662f\uff1a\u591a\u4eba\u8bc6\u522b\u6027\u80fd\u5dee\u662f\u7b97\u6cd5\u9650\u5236\u8fd8\u662f\u786c\u4ef6\u7ea6\u675f\uff1f", "method": "\u4f7f\u7528\u5546\u7528ESP32 WiFi\u4f20\u611f\u5668\uff0c\u57281-10\u4eba\u573a\u666f\u4e0b\u7cfb\u7edf\u8bc4\u4f306\u79cd\u4fe1\u53f7\u5206\u79bb\u65b9\u6cd5\uff08FastICA\u3001SOBI\u3001PCA\u3001NMF\u3001\u5c0f\u6ce2\u53d8\u6362\u3001\u5f20\u91cf\u5206\u89e3\uff09\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bca\u65ad\u6307\u6807\uff08\u4e3b\u4f53\u5185\u53d8\u5f02\u6027\u3001\u4e3b\u4f53\u95f4\u53ef\u533a\u5206\u6027\u3001\u6027\u80fd\u9000\u5316\u7387\uff09\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u90fd\u5f88\u4f4e\uff0845-56%\uff0c\u6807\u51c6\u5dee3.74%\uff09\uff0c\u7edf\u8ba1\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\uff08p>0.05\uff09\u3002\u6700\u4f73\u65b9\u6cd5NMF\u4ec5\u8fbe56%\u3002\u5206\u6790\u663e\u793a\u9ad8\u4e3b\u4f53\u5185\u53d8\u5f02\u6027\u3001\u4f4e\u4e3b\u4f53\u95f4\u53ef\u533a\u5206\u6027\uff0c\u4e14\u968f\u4eba\u6570\u589e\u52a0\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "conclusion": "\u5546\u7528ESP32\u4f20\u611f\u5668\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\u53f7\u8d28\u91cf\u6765\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u4eba\u5206\u79bb\uff0c\u786c\u4ef6\u9650\u5236\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u800c\u975e\u7b97\u6cd5\u95ee\u9898\u3002\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.02198", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02198", "abs": "https://arxiv.org/abs/2601.02198", "authors": ["Alexander M\u00f6llers", "Julius Hense", "Florian Schulz", "Timo Milbich", "Maximilian Alber", "Lukas Ruff"], "title": "Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models", "comment": null, "summary": "In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u7ec4\u7ec7\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u4e2d\u653e\u5927\u500d\u6570\u91c7\u6837\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u8fde\u7eed\u91c7\u6837\u65b9\u6cd5\u4ee5\u89e3\u51b3\u4f20\u7edf\u79bb\u6563\u91c7\u6837\u5728\u4e2d\u95f4\u653e\u5927\u500d\u6570\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4f18\u5316\u91c7\u6837\u5206\u5e03\u6765\u63d0\u5347\u8de8\u653e\u5927\u500d\u6570\u7684\u8868\u793a\u8d28\u91cf\u3002", "motivation": "\u7ec4\u7ec7\u75c5\u7406\u5b66\u4e2d\uff0c\u75c5\u7406\u5b66\u5bb6\u9700\u8981\u5728\u4e0d\u540c\u653e\u5927\u500d\u6570\u4e0b\u89c2\u5bdf\u7ec4\u7ec7\u67b6\u6784\u548c\u7cbe\u7ec6\u5f62\u6001\uff0c\u4f46\u73b0\u6709\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u653e\u5927\u500d\u6570\u4e0b\u7684\u6027\u80fd\u4ee5\u53ca\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u653e\u5927\u500d\u6570\u91c7\u6837\u7b56\u7565\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u7406\u89e3\u653e\u5927\u500d\u6570\u91c7\u6837\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u66f4\u597d\u7684\u91c7\u6837\u7b56\u7565\u3002", "method": "\u5c06\u653e\u5927\u500d\u6570\u91c7\u6837\u5efa\u6a21\u4e3a\u591a\u6e90\u57df\u9002\u5e94\u95ee\u9898\uff0c\u5f00\u53d1\u7406\u8bba\u6846\u67b6\u5206\u6790\u91c7\u6837\u7b56\u7565\u7684\u7cfb\u7edf\u6027\u6743\u8861\u3002\u63d0\u51fa\u8fde\u7eed\u653e\u5927\u500d\u6570\u91c7\u6837\u65b9\u6cd5\u66ff\u4ee3\u4f20\u7edf\u7684\u79bb\u6563\u5747\u5300\u91c7\u6837\uff0c\u63a8\u5bfc\u4f18\u5316\u91c7\u6837\u5206\u5e03\u4ee5\u63d0\u5347\u8de8\u653e\u5927\u500d\u6570\u7684\u8868\u793a\u8d28\u91cf\u3002\u5f15\u5165\u4e24\u4e2a\u65b0\u57fa\u51c6\uff08TCGA-MS, BRACS-MS\uff09\u548c\u76f8\u5e94\u6307\u6807\u6765\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fde\u7eed\u91c7\u6837\u76f8\u6bd4\u79bb\u6563\u91c7\u6837\u5728\u4e2d\u95f4\u653e\u5927\u500d\u6570\u4e0a\u663e\u8457\u6539\u5584\uff0c\u5e73\u8861\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe4\u4e2a\u767e\u5206\u70b9\u3002\u4f18\u5316\u5206\u5e03\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002\u8bc4\u4f30\u73b0\u6709\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u53d1\u73b0\uff0c\u653e\u5927\u500d\u6570\u662f\u6a21\u578b\u6027\u80fd\u53d8\u5316\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8fde\u7eed\u653e\u5927\u500d\u6570\u91c7\u6837\u80fd\u6d88\u9664\u653e\u5927\u500d\u6570\u8986\u76d6\u7684\u7a7a\u767d\uff0c\u5728\u6807\u51c6\u5c3a\u5ea6\u4e0a\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u6539\u5584\u4e2d\u95f4\u653e\u5927\u500d\u6570\u7684\u8868\u73b0\u3002\u4f18\u5316\u91c7\u6837\u5206\u5e03\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u8de8\u653e\u5927\u500d\u6570\u7684\u8868\u793a\u8d28\u91cf\u3002\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u5f00\u53d1\u5728\u591a\u4e2a\u653e\u5927\u500d\u6570\u4e0a\u53ef\u9760\u8868\u73b0\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.02203", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.02203", "abs": "https://arxiv.org/abs/2601.02203", "authors": ["Oliver Custance", "Saad Khan", "Simon Parkinson", "Quan Z. Sheng"], "title": "Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules", "comment": null, "summary": "Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\\% of a full fine-tune (98.84\\% vs. 99.67\\%) while training 97.2\\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eWiFi CSI\u7684\u65e0\u8bbe\u5907\u4eba\u7fa4\u8ba1\u6570\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3CSI-ResNet-A\u83b7\u53d6\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u7ed3\u5408\u8f7b\u91cf\u9002\u914d\u5668\u5fae\u8c03\u548c\u72b6\u6001\u8ba1\u6570\u673a\uff0c\u5728\u57df\u8fc1\u79fb\u95ee\u9898\u4e0a\u53d6\u5f97\u7a81\u7834\u6027\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8eWiFi CSI\u7684\u65e0\u8bbe\u5907\u4eba\u7fa4\u8ba1\u6570\u662f\u9690\u79c1\u4fdd\u62a4\u7269\u8054\u7f51\u5e94\u7528\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u9762\u4e34\u4e25\u91cd\u7684\u57df\u8fc1\u79fb\u95ee\u9898\u2014\u2014\u5728\u4e00\u4e2a\u73af\u5883\u8bad\u7ec3\u7684\u6a21\u578b\u65e0\u6cd5\u6cdb\u5316\u5230\u5176\u4ed6\u73af\u5883\uff0c\u8fd9\u963b\u788d\u4e86\u5b9e\u7528\u5316\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528CSI-ResNet-A\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u83b7\u53d6\u57df\u4e0d\u53d8\u8868\u793a\uff1b2) \u5229\u7528\u8f7b\u91cf\u9002\u914d\u5668\u6a21\u5757\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff0c\u751f\u6210\u4e8b\u4ef6\u5e8f\u5217\u540e\u7531\u72b6\u6001\u8ba1\u6570\u673a\u5904\u7406\u5f97\u5230\u7a33\u5b9a\u5360\u7528\u4f30\u8ba1\u3002\u5f15\u5165\u6cdb\u5316\u6307\u6570(GI)\u91cf\u5316\u9c81\u68d2\u6027\u3002", "result": "\u5728WiFlow\u6570\u636e\u96c6\u4e0a\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u572810-shot\u5b66\u4e60\u573a\u666f\u4e2dMAE\u4ec50.44\uff0c\u800c\u76d1\u7763\u57fa\u7ebf\u5931\u8d25\uff1b\u6cdb\u5316\u6307\u6570\u63a5\u8fd1\u5b8c\u7f8e\uff1b\u5728\u516c\u5171WiAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523098.8%\u51c6\u786e\u7387\u7684\u65b0SOTA\u3002\u9002\u914d\u5668\u5fae\u8c03\u6027\u80fd\u63a5\u8fd1\u5168\u5fae\u8c03(98.84% vs 99.67%)\u4f46\u53c2\u6570\u51cf\u5c1197.2%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u9762\u5411\u771f\u5b9e\u4e16\u754c\u7269\u8054\u7f51\u90e8\u7f72\u7684\u9c81\u68d2\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86WiFi CSI\u4eba\u7fa4\u8ba1\u6570\u4e2d\u7684\u57df\u8fc1\u79fb\u95ee\u9898\u3002"}}
{"id": "2601.02204", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02204", "abs": "https://arxiv.org/abs/2601.02204", "authors": ["Huichao Zhang", "Liao Qu", "Yiheng Liu", "Hang Chen", "Yangyang Song", "Yongsheng Dong", "Shikun Sun", "Xian Li", "Xu Wang", "Yi Jiang", "Hu Ye", "Bo Chen", "Yiming Gao", "Peng Liu", "Akide Liu", "Zhipeng Yang", "Qili Deng", "Linjie Xing", "Jiyang Liu", "Zhao Wang", "Yang Zhou", "Mingcong Liu", "Yi Zhang", "Qian He", "Xiwei Hu", "Zhongqi Qi", "Jie Shao", "Zhiye Fu", "Shuai Wang", "Fangmin Chen", "Xuezhi Chai", "Zhihua Wu", "Yitong Wang", "Zehuan Yuan", "Daniel K. Du", "Xinglong Wu"], "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "comment": "Project page: https://github.com/ByteVisionLab/NextFlow", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "AI": {"tldr": "NextFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52Transformer\uff0c\u901a\u8fc76\u4e07\u4ebf\u6587\u672c-\u56fe\u50cf\u79bb\u6563\u6807\u8bb0\u8bad\u7ec3\uff0c\u91c7\u7528\u7edf\u4e00\u89c6\u89c9\u8868\u793a\u548c\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u5305\u62ec\u56fe\u50cf\u7f16\u8f91\u3001\u4ea4\u9519\u5185\u5bb9\u548c\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u4e0d\u540c\u6a21\u6001\u5177\u6709\u672c\u8d28\u5dee\u5f02\uff1a\u6587\u672c\u662f\u4e25\u683c\u987a\u5e8f\u7684\uff0c\u800c\u56fe\u50cf\u5177\u6709\u5c42\u6b21\u7ed3\u6784\u3002\u4f20\u7edf\u7684\u5149\u6805\u626b\u63cf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u751f\u6210\u65b9\u6cd5\u3002", "method": "1. \u7edf\u4e00\u89e3\u7801\u5668\u81ea\u56de\u5f52Transformer\u67b6\u6784\uff1b2. \u5bf9\u6587\u672c\u4fdd\u7559\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\uff0c\u5bf9\u89c6\u89c9\u91c7\u7528\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\uff1b3. \u591a\u5c3a\u5ea6\u751f\u6210\u7684\u9c81\u68d2\u8bad\u7ec3\u65b9\u6cd5\uff1b4. \u5f3a\u5316\u5b66\u4e60\u7684prefix-tuning\u7b56\u7565\u3002", "result": "1. \u57285\u79d2\u5185\u751f\u62101024x1024\u56fe\u50cf\uff0c\u6bd4\u540c\u7c7bAR\u6a21\u578b\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff1b2. \u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b3. \u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u53ef\u4e0e\u4e13\u95e8\u7684\u6269\u6563\u57fa\u7ebf\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "NextFlow\u901a\u8fc7\u7edf\u4e00\u7684\u67b6\u6784\u548c\u521b\u65b0\u7684\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u5728\u901f\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.02206", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02206", "abs": "https://arxiv.org/abs/2601.02206", "authors": ["Dachun Kai", "Zeyu Xiao", "Huyue Zhu", "Jiaxiao Wang", "Yueyi Zhang", "Xiaoyan Sun"], "title": "Seeing the Unseen: Zooming in the Dark with Event Cameras", "comment": "Accepted to AAAI 2026", "summary": "This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.", "AI": {"tldr": "RetinexEVSR\uff1a\u9996\u4e2a\u4e8b\u4ef6\u9a71\u52a8\u7684\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u5bf9\u6bd4\u5ea6\u4e8b\u4ef6\u4fe1\u53f7\u548cRetinex\u5148\u9a8c\uff0c\u901a\u8fc7\u53cc\u5411\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\u63d0\u5347\u4f4e\u5149\u89c6\u9891\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u6062\u590d\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u56e0\u4e3a\u4f4e\u5149\u6761\u4ef6\u4e0b\u5bf9\u6bd4\u5ea6\u6709\u9650\u4e14\u9ad8\u9891\u4fe1\u606f\u4e0d\u8db3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5229\u7528\u9ad8\u5bf9\u6bd4\u5ea6\u4e8b\u4ef6\u4fe1\u53f7\u5e76\u6291\u5236\u4f4e\u5149\u4f2a\u5f71\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRetinexEVSR\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5411\u8de8\u6a21\u6001\u878d\u5408\u7b56\u7565\uff1a1\uff09\u5149\u7167\u5f15\u5bfc\u7684\u4e8b\u4ef6\u589e\u5f3a\u6a21\u5757\uff0c\u5229\u7528Retinex\u6a21\u578b\u7684\u5149\u7167\u56fe\u9010\u6b65\u7cbe\u70bc\u4e8b\u4ef6\u7279\u5f81\uff1b2\uff09\u4e8b\u4ef6\u5f15\u5bfc\u7684\u53cd\u5c04\u7387\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u878d\u5408\u673a\u5236\u52a8\u6001\u6062\u590d\u53cd\u5c04\u7387\u7ec6\u8282\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728SDSD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u5148\u524d\u4e8b\u4ef6\u65b9\u6cd5\u83b7\u5f97\u6700\u9ad82.95dB\u589e\u76ca\uff0c\u540c\u65f6\u51cf\u5c1165%\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "RetinexEVSR\u901a\u8fc7\u6709\u6548\u878d\u5408\u4e8b\u4ef6\u4fe1\u53f7\u548cRGB\u5e27\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u5149\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u7ec6\u8282\u6062\u590d\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.02211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02211", "abs": "https://arxiv.org/abs/2601.02211", "authors": ["Binglei Li", "Mengping Yang", "Zhiyu Tan", "Junping Zhang", "Hao Li"], "title": "Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion", "comment": "11 pages", "summary": "Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8eMMDiT\u7684\u6269\u6563\u6a21\u578b\u5185\u90e8\u673a\u5236\uff0c\u901a\u8fc7\u79fb\u9664\u3001\u7981\u7528\u548c\u589e\u5f3a\u6587\u672c\u9690\u85cf\u72b6\u6001\u6765\u7814\u7a76\u5404\u6a21\u5757\u529f\u80fd\uff0c\u5e76\u57fa\u4e8e\u53d1\u73b0\u63d0\u51fa\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u7b56\u7565\u6765\u6539\u8fdb\u6587\u672c\u5bf9\u9f50\u3001\u7cbe\u786e\u7f16\u8f91\u548c\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eMMDiT\u7684\u6269\u6563\u6a21\u578b\uff08\u5982FLUX\u548cQwen Image\uff09\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5206\u6790\u7279\u5b9a\u7ec4\u4ef6\uff08\u5982\u4f4d\u7f6e\u7f16\u7801\u548c\u6ce8\u610f\u529b\u5c42\uff09\u7684\u5f71\u54cd\uff0c\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u6a21\u5757\u53ca\u5176\u4e0e\u6587\u672c\u6761\u4ef6\u4ea4\u4e92\u5728\u5408\u6210\u8fc7\u7a0b\u4e2d\u4f5c\u7528\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "1\uff09\u5f00\u53d1\u7cfb\u7edf\u5316\u5206\u6790\u6d41\u7a0b\uff0c\u901a\u8fc7\u79fb\u9664\u3001\u7981\u7528\u548c\u589e\u5f3a\u76f8\u5e94\u6a21\u5757\u7684\u6587\u672c\u9690\u85cf\u72b6\u6001\u6765\u5168\u9762\u7814\u7a76\u6bcf\u4e2a\u6a21\u5757\u7684\u529f\u80fd\uff1b2\uff09\u57fa\u4e8e\u5206\u6790\u53d1\u73b0\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u65b0\u7b56\u7565\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u5bf9\u9f50\u3001\u7cbe\u786e\u7f16\u8f91\u548c\u52a0\u901f\u63a8\u7406\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff1a1\uff09\u8bed\u4e49\u4fe1\u606f\u51fa\u73b0\u5728\u65e9\u671f\u6a21\u5757\uff0c\u7ec6\u8282\u5728\u540e\u671f\u6a21\u5757\u6e32\u67d3\uff1b2\uff09\u79fb\u9664\u7279\u5b9a\u6a21\u5757\u901a\u5e38\u6bd4\u7981\u7528\u6587\u672c\u6761\u4ef6\u7834\u574f\u6027\u5c0f\uff1b3\uff09\u5728\u9009\u62e9\u6027\u6a21\u5757\u589e\u5f3a\u6587\u672c\u6761\u4ef6\u80fd\u6539\u5584\u8bed\u4e49\u5c5e\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728SD3.5\u4e0a\u4f7fT2I-Combench++\u4ece56.92%\u63d0\u5347\u523063.00%\uff0cGenEval\u4ece66.42%\u63d0\u5347\u523071.63%\uff0c\u4e14\u4e0d\u727a\u7272\u5408\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u8fdb\u4e86\u5bf9MMDiT\u6a21\u578b\u7684\u7406\u89e3\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u63a8\u7406\u52a0\u901f\u65b9\u9762\u7684\u7075\u6d3b\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.02228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02228", "abs": "https://arxiv.org/abs/2601.02228", "authors": ["Duoxun Tang", "Xueyi Zhang", "Chak Hin Wang", "Xi Xiao", "Dasen Dai", "Xinhang Jiang", "Wentao Shi", "Rui Li", "Qing Li"], "title": "FMVP: Masked Flow Matching for Adversarial Video Purification", "comment": null, "summary": "Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.", "AI": {"tldr": "FMVP\u4f7f\u7528\u6d41\u5339\u914d\u548c\u63a9\u7801\u7b56\u7565\u6765\u51c0\u5316\u5bf9\u6297\u6027\u89c6\u9891\u653b\u51fb\uff0c\u901a\u8fc7\u9891\u7387\u95e8\u63a7\u635f\u5931\u5206\u79bb\u8bed\u4e49\u5185\u5bb9\u548c\u5bf9\u6297\u566a\u58f0\uff0c\u5728\u5df2\u77e5\u548c\u672a\u77e5\u653b\u51fb\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89c6\u9891\u8bc6\u522b\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u51c0\u5316\u65b9\u6cd5\u91c7\u6837\u6548\u7387\u4f4e\u4e14\u8f68\u8ff9\u5f2f\u66f2\u3002\u76f4\u63a5\u56de\u5f52\u5e72\u51c0\u89c6\u9891\u96be\u4ee5\u6062\u590d\u5fe0\u5b9e\u5185\u5bb9\uff0c\u9700\u8981\u7269\u7406\u7834\u574f\u5bf9\u6297\u7ed3\u6784\u3002", "method": "\u63d0\u51faFMVP\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u63a9\u7801\u7b56\u7565\u7269\u7406\u7834\u574f\u5168\u5c40\u5bf9\u6297\u7ed3\u6784\uff1b2) \u4f7f\u7528\u6761\u4ef6\u6d41\u5339\u914d(CFM)\u548c\u4fee\u590d\u76ee\u6807\u91cd\u5efa\u5e72\u51c0\u89c6\u9891\u52a8\u6001\uff1b3) \u8bbe\u8ba1\u9891\u7387\u95e8\u63a7\u635f\u5931(FGL)\u6291\u5236\u9ad8\u9891\u5bf9\u6297\u6b8b\u5dee\u540c\u65f6\u4fdd\u6301\u4f4e\u9891\u4fdd\u771f\u5ea6\uff1b4) \u8bbe\u8ba1\u653b\u51fb\u611f\u77e5\u548c\u901a\u7528\u8bad\u7ec3\u8303\u5f0f\u5206\u522b\u5904\u7406\u5df2\u77e5\u548c\u672a\u77e5\u5a01\u80c1\u3002", "result": "\u5728UCF-101\u548cHMDB-51\u6570\u636e\u96c6\u4e0a\uff0cFMVP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5(DiffPure\u3001DP\u3001TS\u3001FlowPure)\uff0c\u5bf9PGD\u653b\u51fb\u8fbe\u523087%\u4ee5\u4e0a\u9c81\u68d2\u51c6\u786e\u7387\uff0c\u5bf9CW\u653b\u51fb\u8fbe\u523089%\u3002\u5bf9\u81ea\u9002\u5e94\u653b\u51fb(DiffHammer)\u8868\u73b0\u51fa\u4f18\u8d8a\u9c81\u68d2\u6027\uff0c\u5e76\u53ef\u4f5c\u4e3a\u96f6\u6837\u672c\u5bf9\u6297\u68c0\u6d4b\u5668\uff0c\u5bf9PGD\u68c0\u6d4b\u51c6\u786e\u738798%\uff0c\u5bf9CW\u68c0\u6d4b\u51c6\u786e\u738779%\u3002", "conclusion": "FMVP\u901a\u8fc7\u7269\u7406\u7834\u574f\u5bf9\u6297\u7ed3\u6784\u548c\u9891\u7387\u611f\u77e5\u91cd\u5efa\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u5bf9\u6297\u51c0\u5316\u95ee\u9898\uff0c\u5728\u591a\u79cd\u653b\u51fb\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u540c\u65f6\u5177\u5907\u5bf9\u6297\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2601.02242", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02242", "abs": "https://arxiv.org/abs/2601.02242", "authors": ["Grigorii Alekseenko", "Aleksandr Gordeev", "Irina Tolstykh", "Bulat Suleimanov", "Vladimir Dokholyan", "Georgii Fedorov", "Sergey Yakubson", "Aleksandra Tsybina", "Mikhail Chernyshov", "Maksim Kuprashevich"], "title": "VIBE: Visual Instruction Based Editor", "comment": null, "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u9ad8\u6548\u7684\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u7ba1\u9053\uff0c\u4f7f\u75282B\u53c2\u6570\u7684Qwen3-VL\u6a21\u578b\u6307\u5bfc\u7f16\u8f91\u8fc7\u7a0b\uff0c\u7ed3\u54081.6B\u53c2\u6570\u7684Sana1.5\u6269\u6563\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u751f\u6210\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u867d\u7136\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u4e2d\u80fd\u8fbe\u5230\u5b9e\u9645\u5e94\u7528\u8d28\u91cf\u7684\u6709\u9650\uff0c\u4e14\u4e3b\u6d41\u7684\u6269\u6563\u6a21\u578b\u901a\u5e38\u53c2\u6570\u5e9e\u5927\uff086B-20B\uff09\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u548c\u7814\u7a76\u5e94\u7528\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u67b6\u6784\u8bbe\u8ba1\uff1a\u4f7f\u75282B\u53c2\u6570\u7684Qwen3-VL\u6a21\u578b\u4f5c\u4e3a\u7f16\u8f91\u6307\u5bfc\u5668\uff0c1.6B\u53c2\u6570\u7684Sana1.5\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u56fe\u50cf\u751f\u6210\u5668\u3002\u5728\u67b6\u6784\u3001\u6570\u636e\u5904\u7406\u3001\u8bad\u7ec3\u914d\u7f6e\u548c\u8bc4\u4f30\u7b49\u65b9\u9762\u90fd\u9488\u5bf9\u4f4e\u6210\u672c\u63a8\u7406\u548c\u4e25\u683c\u7684\u6e90\u4e00\u81f4\u6027\u8fdb\u884c\u4e86\u4f18\u5316\u8bbe\u8ba1\u3002", "result": "\u5728ImgEdit\u548cGEdit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86\u53c2\u6570\u89c4\u6a21\u5927\u6570\u500d\u3001\u63a8\u7406\u6210\u672c\u66f4\u9ad8\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u5728\u9700\u8981\u4fdd\u6301\u8f93\u5165\u56fe\u50cf\u7684\u7f16\u8f91\u4efb\u52a1\uff08\u5982\u5c5e\u6027\u8c03\u6574\u3001\u5bf9\u8c61\u79fb\u9664\u3001\u80cc\u666f\u7f16\u8f91\u548c\u76ee\u6807\u66ff\u6362\uff09\u4e0a\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\u3002\u6a21\u578b\u4ec5\u970024GB GPU\u5185\u5b58\uff0c\u5728NVIDIA H100\u4e0a\u4ee5BF16\u7cbe\u5ea6\u751f\u62102K\u5206\u8fa8\u7387\u56fe\u50cf\u7ea6\u97004\u79d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u53ef\u4ee5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u9700\u6c42\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02246", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02246", "abs": "https://arxiv.org/abs/2601.02246", "authors": ["Annoor Sharara Akhand"], "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets", "comment": null, "summary": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e09\u79cdCNN\u5e94\u7528\u8303\u5f0f\u8fdb\u884c\u4e86\u5bf9\u6bd4\u7814\u7a76\uff1a\u4ece\u5934\u8bad\u7ec3\u5c0f\u578bCNN\u3001\u4f7f\u7528\u9884\u8bad\u7ec3CNN\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u3001\u4ee5\u53ca\u901a\u8fc7\u5fae\u8c03\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002\u5b9e\u9a8c\u8868\u660e\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u6700\u4f18\uff0c\u800c\u81ea\u5b9a\u4e49CNN\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u65f6\u63d0\u4f9b\u4e86\u8f83\u597d\u7684\u6548\u7387-\u51c6\u786e\u6027\u5e73\u8861\u3002", "motivation": "\u5728\u89c6\u89c9\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u5b9e\u8df5\u8005\u901a\u5e38\u9762\u4e34\u4e09\u79cd\u9009\u62e9\uff1a\u4ece\u5934\u8bad\u7ec3\u5c0f\u578bCNN\u3001\u4f7f\u7528\u9884\u8bad\u7ec3CNN\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u3001\u6216\u901a\u8fc7\u5fae\u8c03\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5bf9\u6bd4\u8fd9\u4e09\u79cd\u8303\u5f0f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff1a\u8def\u9762\u7f3a\u9677\u8bc6\u522b\u3001\u519c\u4e1a\u54c1\u79cd\u8bc6\u522b\u3001\u6c34\u679c/\u53f6\u7247\u75c5\u5bb3\u8bc6\u522b\u3001\u4eba\u884c\u9053\u4fb5\u5360\u8bc6\u522b\u3001\u672a\u6388\u6743\u8f66\u8f86\u8bc6\u522b\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u51c6\u786e\u7387\u3001\u5b8fF1\u5206\u6570\uff0c\u4ee5\u53ca\u8bad\u7ec3\u65f6\u95f4\u3001\u53c2\u6570\u91cf\u7b49\u6548\u7387\u6307\u6807\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u6700\u5f3a\u7684\u9884\u6d4b\u6027\u80fd\u3002\u81ea\u5b9a\u4e49CNN\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u9884\u7b97\u53d7\u9650\u65f6\u63d0\u4f9b\u4e86\u6709\u5438\u5f15\u529b\u7684\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u3002\u9884\u8bad\u7ec3CNN\u4f5c\u4e3a\u56fa\u5b9a\u7279\u5f81\u63d0\u53d6\u5668\u7684\u6027\u80fd\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u662f\u83b7\u5f97\u6700\u4f73\u9884\u6d4b\u6027\u80fd\u7684\u9996\u9009\u65b9\u6cd5\uff0c\u800c\u81ea\u5b9a\u4e49CNN\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6548\u7387-\u51c6\u786e\u6027\u5e73\u8861\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684CNN\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2601.02249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02249", "abs": "https://arxiv.org/abs/2601.02249", "authors": ["Xiantai Xiang", "Guangyao Zhou", "Zixiao Wen", "Wenshuai Li", "Ben Niu", "Feng Wang", "Lijia Huang", "Qiantong Wang", "Yuhan Liu", "Zongxu Pan", "Yuxin Hu"], "title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection", "comment": null, "summary": "Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.", "AI": {"tldr": "SLGNet\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684RGB-\u7ea2\u5916\u591a\u6a21\u6001\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u9002\u914d\u5668\u548c\u8bed\u8a00\u5f15\u5bfc\u8c03\u5236\u6a21\u5757\uff0c\u5728\u51bb\u7ed3\u7684ViT\u57fa\u7840\u6a21\u578b\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u68c0\u6d4b\uff0c\u5927\u5e45\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\u5728\u5c06RGB\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u8fc1\u79fb\u5230\u591a\u6a21\u6001\u68c0\u6d4b\u4efb\u52a1\u65f6\uff0c\u5f80\u5f80\u727a\u7272\u8de8\u6a21\u6001\u7ed3\u6784\u4e00\u81f4\u6027\u4ee5\u8ffd\u6c42\u6a21\u578b\u6548\u7387\uff0c\u5bfc\u81f4\u5728\u57df\u5dee\u8ddd\u5927\u7684\u573a\u666f\uff08\u5982\u9ad8\u5bf9\u6bd4\u5ea6\u6216\u591c\u95f4\u73af\u5883\uff09\u4e2d\u4e22\u5931\u5173\u952e\u7ed3\u6784\u7ebf\u7d22\u3002\u540c\u65f6\uff0c\u4f20\u7edf\u7684\u9759\u6001\u591a\u6a21\u6001\u878d\u5408\u673a\u5236\u7f3a\u4e4f\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u5728\u590d\u6742\u52a8\u6001\u573a\u666f\u53d8\u5316\u4e0b\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSLGNet\u6846\u67b6\uff1a1\uff09\u7ed3\u6784\u611f\u77e5\u9002\u914d\u5668\u4ece\u4e24\u79cd\u6a21\u6001\u63d0\u53d6\u5c42\u6b21\u5316\u7ed3\u6784\u8868\u793a\uff0c\u5e76\u52a8\u6001\u6ce8\u5165ViT\u4ee5\u8865\u507fViT\u4e3b\u5e72\u7684\u7ed3\u6784\u9000\u5316\uff1b2\uff09\u8bed\u8a00\u5f15\u5bfc\u8c03\u5236\u6a21\u5757\u5229\u7528VLM\u9a71\u52a8\u7684\u7ed3\u6784\u5316\u63cf\u8ff0\u52a8\u6001\u91cd\u65b0\u6821\u51c6\u89c6\u89c9\u7279\u5f81\uff0c\u8d4b\u4e88\u6a21\u578b\u9c81\u68d2\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728LLVIP\u3001FLIR\u3001KAIST\u548cDroneVehicle\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSLGNet\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002\u5728LLVIP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523066.1\u7684mAP\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u5168\u5fae\u8c03\u51cf\u5c11\u7ea687%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "SLGNet\u662f\u4e00\u4e2a\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u591a\u6a21\u6001\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u5c42\u6b21\u5316\u7ed3\u6784\u5148\u9a8c\u548c\u8bed\u8a00\u5f15\u5bfc\u8c03\u5236\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2601.02256", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02256", "abs": "https://arxiv.org/abs/2601.02256", "authors": ["Shikun Sun", "Liao Qu", "Huichao Zhang", "Yiheng Liu", "Yangyang Song", "Xian Li", "Xu Wang", "Yi Jiang", "Daniel K. Du", "Xinglong Wu", "Jia Jia"], "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation", "comment": "Project page: https://github.com/ByteVisionLab/NextFlow", "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3aGRPO\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ba1\u7406VAR\u6a21\u578b\u4e2d\u7684\u5f02\u6b65\u7b56\u7565\u51b2\u7a81\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u751f\u6210\u4e2dAR\u3001\u6269\u6563\u548cVAR\u4e09\u79cd\u8303\u5f0f\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u751f\u6210\u4e2d\u7684VAR\u6a21\u578b\u5728\u751f\u6210\u6b65\u9aa4\u4e2d\u5b58\u5728\u5f02\u6784\u8f93\u5165\u7ed3\u6784\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u5f02\u6b65\u7b56\u7565\u51b2\u7a81\uff0c\u7279\u522b\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u8fd9\u4f1a\u9020\u6210\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\u7684\u6846\u67b6\uff1a1) \u7528\u4e8e\u5f15\u5bfc\u65e9\u671f\u751f\u6210\u7684\u7a33\u5b9a\u4e2d\u95f4\u5956\u52b1\uff1b2) \u7528\u4e8e\u7cbe\u786e\u4fe1\u7528\u5206\u914d\u7684\u52a8\u6001\u65f6\u95f4\u6b65\u91cd\u52a0\u6743\u65b9\u6848\uff1b3) \u57fa\u4e8e\u5956\u52b1\u53cd\u9988\u5b66\u4e60\u539f\u7406\u7684\u65b0\u578b\u63a9\u7801\u4f20\u64ad\u7b97\u6cd5\uff0c\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u9694\u79bb\u4f18\u5316\u6548\u679c\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u9f50\u65b9\u9762\u76f8\u6bd4\u539f\u59cbGRPO\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u5bf9VAR\u6a21\u578b\u7684\u7a33\u5065\u6709\u6548\u4f18\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86VAR\u6a21\u578b\u4e2d\u7684\u5f02\u6b65\u7b56\u7565\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u89c6\u89c9\u751f\u6210\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02273", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.02273", "abs": "https://arxiv.org/abs/2601.02273", "authors": ["Salim Khazem"], "title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation", "comment": null, "summary": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \\textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \\textbf{5.2\\%} of model parameters ($\\sim$4.9M). On the challenging CHASE\\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git", "AI": {"tldr": "TopoLoRA-SAM\uff1a\u57fa\u4e8e\u62d3\u6251\u611f\u77e5\u7684\u4f4e\u79e9\u9002\u914d\u6846\u67b6\uff0c\u7528\u4e8eSAM\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u5728\u89c6\u7f51\u819c\u8840\u7ba1\u7b49\u7ec6\u7ed3\u6784\u5206\u5272\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u57fa\u7840\u5206\u5272\u6a21\u578b\u5982SAM\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5c06\u5176\u9002\u914d\u5230\u7279\u5b9a\u9886\u57df\u7684\u8bed\u4e49\u5206\u5272\uff08\u7279\u522b\u662f\u7ec6\u957f\u7ed3\u6784\u5982\u89c6\u7f51\u819c\u8840\u7ba1\u548c\u566a\u58f0\u6a21\u6001\u5982SAR\u56fe\u50cf\uff09\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u5b8c\u5168\u5fae\u8c03\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\u3002", "method": "\u63d0\u51faTopoLoRA-SAM\u6846\u67b6\uff0c\u5c06\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u6ce8\u5165\u51bb\u7ed3\u7684ViT\u7f16\u7801\u5668\uff0c\u5e76\u589e\u5f3a\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u5377\u79ef\u9002\u914d\u5668\u548c\u53ef\u9009\u7684\u62d3\u6251\u611f\u77e5\u76d1\u7763\uff08\u901a\u8fc7\u53ef\u5fae\u5206clDice\u635f\u5931\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08DRIVE\u3001STARE\u3001CHASE_DB1\u3001Kvasir-SEG\u3001SL-SSDD\uff09\u4e0a\u8bc4\u4f30\uff0cTopoLoRA-SAM\u5728\u89c6\u7f51\u819c\u5e73\u5747Dice\u548c\u6574\u4f53\u5e73\u5747Dice\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4ec5\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u76845.2%\uff08\u7ea6490\u4e07\u53c2\u6570\uff09\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684CHASE_DB1\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u5206\u5272\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u62d3\u6251\u611f\u77e5\u7684\u53c2\u6570\u9ad8\u6548\u9002\u914d\u65b9\u6cd5\u80fd\u591f\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u5b8c\u5168\u5fae\u8c03\u7684\u4e13\u4e1a\u6a21\u578b\uff0c\u4e3a\u9886\u57df\u7279\u5b9a\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02281", "abs": "https://arxiv.org/abs/2601.02281", "authors": ["Shuai Yuan", "Yantai Yang", "Xiaotian Yang", "Xupeng Zhang", "Zhonghao Zhao", "Lingming Zhang", "Zhipeng Zhang"], "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams", "comment": null, "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", "AI": {"tldr": "InfiniteVGGT\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u89c6\u89c9\u51e0\u4f55\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u6709\u754c\u81ea\u9002\u5e94KV\u7f13\u5b58\u5b9e\u73b0\u65e0\u9650\u89c6\u91ce\u6d41\u5f0f\u5904\u7406\uff0c\u89e3\u51b3\u4e863D\u51e0\u4f55\u7406\u89e3\u4e2d\u53ef\u6269\u5c55\u6027\u4e0e\u957f\u671f\u7a33\u5b9a\u6027\u7684\u77db\u76fe\uff0c\u5e76\u5f15\u5165Long3D\u57fa\u51c6\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u67093D\u89c6\u89c9\u51e0\u4f55\u7406\u89e3\u65b9\u6cd5\u9762\u4e34\u53ef\u6269\u5c55\u6027\u4e0e\u957f\u671f\u7a33\u5b9a\u6027\u7684\u77db\u76fe\uff1a\u79bb\u7ebf\u6a21\u578b\uff08\u5982VGGT\uff09\u867d\u51e0\u4f55\u80fd\u529b\u5f3a\u4f46\u4e0d\u9002\u5408\u5b9e\u65f6\u7cfb\u7edf\uff1b\u6d41\u5f0f\u67b6\u6784\u8981\u4e48\u4e0d\u652f\u6301\u65e0\u9650\u89c6\u91ce\u8f93\u5165\uff0c\u8981\u4e48\u5728\u957f\u5e8f\u5217\u4e2d\u4ea7\u751f\u707e\u96be\u6027\u6f02\u79fb\u3002", "method": "\u63d0\u51faInfiniteVGGT\u56e0\u679c\u89c6\u89c9\u51e0\u4f55\u53d8\u6362\u5668\uff0c\u91c7\u7528\u6709\u754c\u81ea\u9002\u5e94KV\u7f13\u5b58\u5b9e\u73b0\u6eda\u52a8\u8bb0\u5fc6\u673a\u5236\uff0c\u7ed3\u5408\u514d\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u65e0\u5173\u526a\u679d\u7b56\u7565\uff0c\u667a\u80fd\u4e22\u5f03\u8fc7\u65f6\u4fe1\u606f\uff0c\u4e0eFlashAttention\u5b8c\u5168\u517c\u5bb9\u3002", "result": "InfiniteVGGT\u5b9e\u73b0\u4e86\u65e0\u9650\u89c6\u91ce\u6d41\u5f0f\u5904\u7406\uff0c\u5728\u957f\u671f\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6d41\u5f0f\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7Long3D\u57fa\u51c6\uff08\u7ea610,000\u5e27\u5e8f\u5217\uff09\u9996\u6b21\u5b9e\u73b0\u4e86\u771f\u6b63\u65e0\u9650\u89c6\u91ce\u7684\u4e25\u683c\u8bc4\u4f30\u3002", "conclusion": "InfiniteVGGT\u89e3\u51b3\u4e863D\u51e0\u4f55\u7406\u89e3\u4e2d\u957f\u671f\u5b58\u5728\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u7a33\u5b9a\u6027\u77db\u76fe\uff0c\u4e3a\u672a\u6765\u957f\u671f3D\u51e0\u4f55\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u6280\u672f\u548c\u8bc4\u4f30\u5e73\u53f0\u3002"}}
{"id": "2601.02289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02289", "abs": "https://arxiv.org/abs/2601.02289", "authors": ["Tom Burgert", "Leonard Hackel", "Paolo Rota", "Beg\u00fcm Demir"], "title": "Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery", "comment": "accepted for publication at IEEE/CVF Winter Conference on Applications of Computer Vision", "summary": "Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.", "AI": {"tldr": "GeoRank\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b0\u578b\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7403\u9762\u8ddd\u79bb\u5c06\u5730\u7406\u5173\u7cfb\u5d4c\u5165\u7279\u5f81\u7a7a\u95f4\uff0c\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5df2\u53d6\u5f97\u663e\u8457\u6210\u529f\uff0c\u4f46\u5e94\u7528\u4e8e\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u65f6\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u56e0\u4e3a\u9065\u611f\u6570\u636e\u5177\u6709\u5730\u7406\u548c\u65f6\u95f4\u53d8\u5f02\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5730\u7406\u5173\u7cfb\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5d4c\u5165\u5730\u7406\u4fe1\u606f\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGeoRank\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u7403\u9762\u8ddd\u79bb\u5c06\u5730\u7406\u5173\u7cfb\u5d4c\u5165\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u7279\u5f81\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff08\u5982BYOL\u3001DINO\uff09\uff0c\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86\u6570\u636e\u589e\u5f3a\u3001\u6570\u636e\u96c6\u89c4\u6a21\u3001\u56fe\u50cf\u5927\u5c0f\u548c\u65f6\u95f4\u89c6\u56fe\u7b49\u5173\u952e\u9002\u5e94\u95ee\u9898\u3002", "result": "GeoRank\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u90a3\u4e9b\u6574\u5408\u5730\u7406\u5143\u6570\u636e\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6301\u7eed\u6539\u8fdb\u591a\u79cd\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u6570\u636e\u589e\u5f3a\u7684\u6709\u6548\u6027\u3001\u6570\u636e\u96c6\u89c4\u6a21\u548c\u56fe\u50cf\u5927\u5c0f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u65f6\u95f4\u89c6\u56fe\u7684\u4efb\u52a1\u4f9d\u8d56\u6027\u3002", "conclusion": "GeoRank\u4e3a\u591a\u5149\u8c31\u9065\u611f\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u5730\u7406\u5173\u7cfb\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u679c\uff0c\u540c\u65f6\u7cfb\u7edf\u7814\u7a76\u4e3a\u9065\u611f\u9886\u57df\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2601.02309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02309", "abs": "https://arxiv.org/abs/2601.02309", "authors": ["Xiaopeng Guo", "Yinzhe Xu", "Huajian Huang", "Sai-Kit Yeung"], "title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera", "comment": "12 pages. Received by RA-L", "summary": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage", "AI": {"tldr": "360DVO\u662f\u9996\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5355\u76ee\u5168\u666f\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5931\u771f\u611f\u77e5\u7403\u9762\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5168\u666f\u53ef\u5fae\u5206\u675f\u8c03\u6574\u6a21\u5757\uff0c\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5168\u666f\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u624b\u5de5\u7279\u5f81\u6216\u5149\u5ea6\u76ee\u6807\uff0c\u5728\u5267\u70c8\u8fd0\u52a8\u548c\u5149\u7167\u53d8\u5316\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86360DVO\u6846\u67b6\uff0c\u5305\u542b\u5931\u771f\u611f\u77e5\u7403\u9762\u7279\u5f81\u63d0\u53d6\u5668(DAS-Feat)\u81ea\u9002\u5e94\u5b66\u4e60\u6297\u5931\u771f\u7279\u5f81\uff0c\u4ee5\u53ca\u5168\u666f\u53ef\u5fae\u5206\u675f\u8c03\u6574(ODBA)\u6a21\u5757\u8fdb\u884c\u6709\u6548\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u548c\u516c\u5f00\u5408\u6210\u6570\u636e\u96c6(TartanAir V2\u548c360VO)\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c360DVO\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u57fa\u7ebf(\u5305\u62ec360VO\u548cOpenVSLAM)\uff0c\u9c81\u68d2\u6027\u63d0\u534750%\uff0c\u7cbe\u5ea6\u63d0\u534737.5%\u3002", "conclusion": "360DVO\u662f\u9996\u4e2a\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u5168\u666f\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u63d0\u53d6\u548c\u4f18\u5316\u6a21\u5757\uff0c\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u8d21\u732e\u4e86\u65b0\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2601.02315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02315", "abs": "https://arxiv.org/abs/2601.02315", "authors": ["Saurabh Kaushik", "Lalit Maurya", "Beth Tellman"], "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping", "comment": "Accepted at CV4EO Workshop @ WACV 2026", "summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}", "AI": {"tldr": "Prithvi-CAFE\u6a21\u578b\u901a\u8fc7\u878d\u5408\u5730\u7406\u57fa\u7840\u6a21\u578b\u548cCNN\u7279\u5f81\uff0c\u5728\u6d2a\u6c34\u5236\u56fe\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86GFMs\u5728\u6355\u6349\u5c40\u90e8\u7ec6\u8282\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5730\u7406\u57fa\u7840\u6a21\u578b\u5728\u6d2a\u6c34\u5236\u56fe\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u6355\u6349\u5173\u952e\u5c40\u90e8\u7ec6\u8282\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u8d85\u8d8a\u57fa\u7ebfU-Net\u6a21\u578b\u3002", "method": "\u63d0\u51faPrithvi-CAFE\u6a21\u578b\uff0c\u5c06Prithvi GFM\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4e0e\u5e76\u884cCNN\u6b8b\u5dee\u5206\u652f\u7ed3\u5408\uff0c\u4f7f\u7528\u5377\u79ef\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\uff0c\u901a\u8fc7\u9002\u914d\u5668\u5b9e\u73b0\u5feb\u901f\u5fae\u8c03\uff0c\u5e76\u8fdb\u884c\u591a\u5c3a\u5ea6\u3001\u591a\u5c42\u6b21\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728Sen1Flood11\u6d4b\u8bd5\u6570\u636e\u4e0aIoU\u8fbe\u523083.41\uff0c\u4f18\u4e8e\u539f\u59cbPrithvi(82.50)\u548c\u5176\u4ed6GFMs\uff1b\u5728\u4fdd\u7559\u6d4b\u8bd5\u7ad9\u70b9\u4e0aIoU 81.37\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfU-Net(70.57)\uff1b\u5728FloodPlanet\u6570\u636e\u96c6\u4e0aIoU 64.70\u4e5f\u4f18\u4e8e\u6240\u6709\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "Prithvi-CAFE\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6a21\u578b\uff0c\u5728\u9700\u8981\u591a\u901a\u9053\u591a\u6a21\u6001\u6570\u636e\u4e92\u8865\u4fe1\u606f\u4e14\u5c40\u90e8\u7ec6\u8282\u5173\u952e\u7684\u5206\u5272\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2601.02339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02339", "abs": "https://arxiv.org/abs/2601.02339", "authors": ["Jingming He", "Chongyi Li", "Shiqi Wang", "Sam Kwong"], "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding", "comment": "Accepted by ICCV 2025", "summary": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.", "AI": {"tldr": "\u63d0\u51fa\u8054\u5408\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5404\u5411\u5f02\u60273D\u9ad8\u65af\u5207\u6bd4\u96ea\u592b\u63cf\u8ff0\u7b26\u548c\u8bed\u4e49\u5f62\u72b6\u4fe1\u53f7\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u63d0\u53473D\u8bed\u4e49\u9ad8\u65af\u5efa\u6a21\u7684\u8bed\u4e49\u5206\u5272\u548c\u6e32\u67d3\u8d28\u91cf", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u8bed\u4e49\u548c\u6e32\u67d3\u5206\u652f\u5206\u5f00\u5904\u7406\uff0c\u4ec5\u4f9d\u8d562D\u76d1\u7763\u800c\u5ffd\u75653D\u9ad8\u65af\u51e0\u4f55\uff0c\u81ea\u9002\u5e94\u7b56\u7565\u4ec5\u4f9d\u8d56\u6e32\u67d3\u68af\u5ea6\uff0c\u5728\u7ec6\u5fae\u6216\u65e0\u7eb9\u7406\u533a\u57df\u6548\u679c\u4e0d\u4f73", "method": "1) \u5f15\u5165\u5404\u5411\u5f02\u60273D\u9ad8\u65af\u5207\u6bd4\u96ea\u592b\u63cf\u8ff0\u7b26\u6355\u6349\u7ec6\u7c92\u5ea63D\u5f62\u72b6\u7ec6\u8282\uff1b2) \u57fa\u4e8e\u5c40\u90e8\u8bed\u4e49\u548c\u5f62\u72b6\u4fe1\u53f7\u81ea\u9002\u5e94\u8c03\u6574\u9ad8\u65af\u5206\u914d\u548c\u7403\u8c10\u51fd\u6570\uff1b3) \u4f7f\u7528\u8de8\u573a\u666f\u77e5\u8bc6\u8f6c\u79fb\u6a21\u5757\u6301\u7eed\u66f4\u65b0\u5b66\u4e60\u5230\u7684\u5f62\u72b6\u6a21\u5f0f", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4fdd\u6301\u9ad8\u6e32\u67d3\u5e27\u7387\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u589e\u5f3a\u6846\u67b6\u901a\u8fc7\u534f\u540c\u8bed\u4e49\u548c\u6e32\u67d3\u5206\u652f\uff0c\u7ed3\u54083D\u5f62\u72b6\u63cf\u8ff0\u7b26\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u8bed\u4e49\u9ad8\u65af\u5efa\u6a21\u7684\u6027\u80fd"}}
{"id": "2601.02356", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02356", "abs": "https://arxiv.org/abs/2601.02356", "authors": ["Jing Tan", "Zhaoyang Zhang", "Yantao Shen", "Jiarui Cai", "Shuo Yang", "Jiajun Wu", "Wei Xia", "Zhuowen Tu", "Stefano Soatto"], "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes", "comment": "Project page: https://sparkstj.github.io/talk2move", "summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.", "AI": {"tldr": "Talk2Move\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u573a\u666f\u4e2d\u7269\u4f53\u7684\u7a7a\u95f4\u53d8\u6362\uff08\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\uff09\uff0c\u65e0\u9700\u6210\u5bf9\u76d1\u7763\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u8c03\u6574\u5916\u89c2\u6216\u98ce\u683c\uff0c\u96be\u4ee5\u6267\u884c\u5bf9\u8c61\u7ea7\u522b\u7684\u51e0\u4f55\u53d8\u6362\uff08\u5982\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\uff09\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u6210\u5bf9\u76d1\u7763\u6570\u636e\u548c\u50cf\u7d20\u7ea7\u4f18\u5316\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528Group Relative Policy Optimization (GRPO)\u901a\u8fc7\u8f93\u5165\u56fe\u50cf\u548c\u8f7b\u91cf\u7ea7\u6587\u672c\u53d8\u4f53\u751f\u6210\u591a\u6837\u5316\u7684rollouts\u6765\u63a2\u7d22\u51e0\u4f55\u52a8\u4f5c\uff1b\u8bbe\u8ba1\u7a7a\u95f4\u5956\u52b1\u5f15\u5bfc\u6a21\u578b\u5c06\u51e0\u4f55\u53d8\u6362\u4e0e\u8bed\u8a00\u63cf\u8ff0\u5bf9\u9f50\uff1b\u91c7\u7528\u79bb\u7b56\u7565\u6b65\u9aa4\u8bc4\u4f30\u548c\u4e3b\u52a8\u6b65\u9aa4\u91c7\u6837\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\uff1b\u8bbe\u8ba1\u9762\u5411\u5bf9\u8c61\u7684\u7a7a\u95f4\u5956\u52b1\u76f4\u63a5\u8bc4\u4f30\u4f4d\u79fb\u3001\u65cb\u8f6c\u548c\u7f29\u653e\u884c\u4e3a\u3002", "result": "\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTalk2Move\u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u4e00\u81f4\u4e14\u8bed\u4e49\u5fe0\u5b9e\u7684\u5bf9\u8c61\u53d8\u6362\uff0c\u5728\u7a7a\u95f4\u51c6\u786e\u6027\u548c\u573a\u666f\u8fde\u8d2f\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "Talk2Move\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6587\u672c\u6307\u5bfc\u7684\u7a7a\u95f4\u53d8\u6362\u95ee\u9898\uff0c\u65e0\u9700\u6602\u8d35\u7684\u6210\u5bf9\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u4e14\u8fde\u8d2f\u7684\u51e0\u4f55\u53d8\u6362\u3002"}}
{"id": "2601.02359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.02359", "abs": "https://arxiv.org/abs/2601.02359", "authors": ["Kaede Shiohara", "Toshihiko Yamasaki", "Vladislav Golyanik"], "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors", "comment": "17 pages, 8 figures, 11 tables; project page: https://mapooon.github.io/ExposeAnyonePage/", "summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.", "AI": {"tldr": "\u63d0\u51faExposeAnyone\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u4ece\u97f3\u9891\u751f\u6210\u8868\u60c5\u5e8f\u5217\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u5efa\u6a21\u548c\u6269\u6563\u91cd\u5efa\u8bef\u5dee\u5b9e\u73b0\u4eba\u7269\u7279\u5b9a\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u68c0\u6d4bSora2\u751f\u6210\u7684\u89c6\u9891\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4f2a\u9020\u7c7b\u578b\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u8bad\u7ec3\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u7279\u5b9a\u7684\u4f2a\u9020\u6a21\u5f0f\u3002\u81ea\u76d1\u7763\u65b9\u6cd5\u867d\u6709\u6cdb\u5316\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ec5\u4ece\u81ea\u76d1\u7763\u4e2d\u5b66\u4e60\u5230\u6709\u533a\u5206\u5ea6\u7684\u8868\u793a\u3002", "method": "\u63d0\u51faExposeAnyone\u65b9\u6cd5\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u4ece\u97f3\u9891\u751f\u6210\u8868\u60c5\u5e8f\u5217\u3002\u9996\u5148\u5c06\u6a21\u578b\u4e2a\u6027\u5316\u5230\u7279\u5b9a\u4eba\u7269\uff08\u4f7f\u7528\u53c2\u8003\u96c6\uff09\uff0c\u7136\u540e\u901a\u8fc7\u6269\u6563\u91cd\u5efa\u8bef\u5dee\u8ba1\u7b97\u53ef\u7591\u89c6\u9891\u4e0e\u4e2a\u6027\u5316\u4eba\u7269\u4e4b\u95f4\u7684\u8eab\u4efd\u8ddd\u79bb\uff0c\u5b9e\u73b0\u4eba\u7269\u7279\u5b9a\u7684\u4eba\u8138\u4f2a\u9020\u68c0\u6d4b\u3002", "result": "1) \u5728DF-TIMIT\u3001DFDCP\u3001KoDF\u548cIDForge\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747AUC\u6bd4\u4e4b\u524d\u6700\u4f18\u65b9\u6cd5\u63d0\u53474.22\u4e2a\u767e\u5206\u70b9\uff1b2) \u80fd\u591f\u68c0\u6d4bSora2\u751f\u6210\u7684\u89c6\u9891\uff0c\u800c\u4e4b\u524d\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff1b3) \u5bf9\u6a21\u7cca\u548c\u538b\u7f29\u7b49\u635f\u574f\u5177\u6709\u9ad8\u5ea6\u9c81\u68d2\u6027\u3002", "conclusion": "ExposeAnyone\u662f\u4e00\u79cd\u5b8c\u5168\u81ea\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u4e2a\u6027\u5316\u5efa\u6a21\u5b9e\u73b0\u4e86\u5bf9\u672a\u77e5\u6df1\u5ea6\u4f2a\u9020\u7684\u6709\u6548\u68c0\u6d4b\uff0c\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
