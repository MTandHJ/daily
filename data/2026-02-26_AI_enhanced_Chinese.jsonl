{"id": "2602.21221", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.21221", "abs": "https://arxiv.org/abs/2602.21221", "authors": ["Zeju Li", "Yizhou Zhou", "Qiang Xu"], "title": "Latent Context Compilation: Distilling Long Context into Compact Portable Memory", "comment": null, "summary": "Efficient long-context LLM deployment is stalled by a dichotomy between amortized compression, which struggles with out-of-distribution generalization, and Test-Time Training, which incurs prohibitive synthetic data costs and requires modifying model weights, creating stateful parameters that complicate concurrent serving. We propose Latent Context Compilation, a framework that fundamentally shifts context processing from adaptation to compilation. By utilizing a disposable LoRA module as a compiler, we distill long contexts into compact buffer tokens -- stateless, portable memory artifacts that are plug-and-play compatible with frozen base models. Crucially, we introduce a self-aligned optimization strategy that eliminates the need for synthetic context-relevant QA pairs. By regularizing context reconstruction task with context-agnostic random queries, we force compressed tokens to reside within the model's existing instruction-following manifold. Experiments with Llama-3.1-8B demonstrate that Latent Context Compilation preserves fine-grained details and reasoning capabilities where prior methods falter, effectively decoupling memory density from model parameters even at a 16x compression ratio.", "AI": {"tldr": "\u63d0\u51faLatent Context Compilation\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u4e22\u5f03\u7684LoRA\u6a21\u5757\u4f5c\u4e3a\u7f16\u8bd1\u5668\uff0c\u5c06\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u4e3a\u7d27\u51d1\u7684buffer tokens\uff0c\u5b9e\u73b0\u65e0\u72b6\u6001\u3001\u53ef\u79fb\u690d\u7684\u5185\u5b58\u5de5\u4ef6\uff0c\u65e0\u9700\u5408\u6210\u6570\u636e\u4e14\u4fdd\u6301\u6a21\u578b\u6743\u91cd\u4e0d\u53d8\u3002", "motivation": "\u5f53\u524d\u957f\u4e0a\u4e0b\u6587LLM\u90e8\u7f72\u5b58\u5728\u4e24\u96be\uff1a\u644a\u9500\u538b\u7f29\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u5408\u6210\u6570\u636e\u6210\u672c\u4e14\u9700\u8981\u4fee\u6539\u6a21\u578b\u6743\u91cd\uff0c\u4ea7\u751f\u6709\u72b6\u6001\u53c2\u6570\uff0c\u4f7f\u5e76\u53d1\u670d\u52a1\u590d\u6742\u5316\u3002", "method": "\u91c7\u7528Latent Context Compilation\u6846\u67b6\uff0c\u4f7f\u7528\u53ef\u4e22\u5f03\u7684LoRA\u6a21\u5757\u4f5c\u4e3a\u7f16\u8bd1\u5668\uff0c\u5c06\u957f\u4e0a\u4e0b\u6587\u84b8\u998f\u4e3a\u7d27\u51d1\u7684buffer tokens\u3002\u5f15\u5165\u81ea\u5bf9\u9f50\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u968f\u673a\u67e5\u8be2\u6b63\u5219\u5316\u4e0a\u4e0b\u6587\u91cd\u5efa\u4efb\u52a1\uff0c\u5f3a\u5236\u538b\u7f29token\u9a7b\u7559\u5728\u6a21\u578b\u73b0\u6709\u7684\u6307\u4ee4\u9075\u5faa\u6d41\u5f62\u4e2d\u3002", "result": "\u5728Llama-3.1-8B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLatent Context Compilation\u5728\u5148\u524d\u65b9\u6cd5\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4fdd\u7559\u4e86\u7ec6\u7c92\u5ea6\u7ec6\u8282\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5373\u4f7f\u572816\u500d\u538b\u7f29\u6bd4\u4e0b\u4e5f\u80fd\u6709\u6548\u89e3\u8026\u5185\u5b58\u5bc6\u5ea6\u4e0e\u6a21\u578b\u53c2\u6570\u3002", "conclusion": "Latent Context Compilation\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u5904\u7406\u4ece\u9002\u5e94\u8f6c\u53d8\u4e3a\u7f16\u8bd1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u72b6\u6001\u3001\u53ef\u79fb\u690d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u65e0\u9700\u5408\u6210\u6570\u636e\u4e14\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u51bb\u7ed3\u3002"}}
{"id": "2602.21233", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21233", "abs": "https://arxiv.org/abs/2602.21233", "authors": ["Rui Cen", "QiangQiang Hu", "Hong Huang", "Hong Liu", "Song Liu", "Xin Luo", "Lin Niu", "Yifan Tan", "Decheng Wu", "Linchuan Xie", "Rubing Yang", "Guanghua Yu", "Jianchen Zhu"], "title": "AngelSlim: A more accessible, comprehensive, and efficient toolkit for large model compression", "comment": null, "summary": "This technical report introduces AngelSlim, a comprehensive and versatile toolkit for large model compression developed by the Tencent Hunyuan team. By consolidating cutting-edge algorithms, including quantization, speculative decoding, token pruning, and distillation. AngelSlim provides a unified pipeline that streamlines the transition from model compression to industrial-scale deployment. To facilitate efficient acceleration, we integrate state-of-the-art FP8 and INT8 Post-Training Quantization (PTQ) algorithms alongside pioneering research in ultra-low-bit regimes, featuring HY-1.8B-int2 as the first industrially viable 2-bit large model. Beyond quantization, we propose a training-aligned speculative decoding framework compatible with multimodal architectures and modern inference engines, achieving 1.8x to 2.0x throughput gains without compromising output correctness. Furthermore, we develop a training-free sparse attention framework that reduces Time-to-First-Token (TTFT) in long-context scenarios by decoupling sparse kernels from model architectures through a hybrid of static patterns and dynamic token selection. For multimodal models, AngelSlim incorporates specialized pruning strategies, namely IDPruner for optimizing vision tokens via Maximal Marginal Relevance and Samp for adaptive audio token merging and pruning. By integrating these compression strategies from low-level implementations, AngelSlim enables algorithm-focused research and tool-assisted deployment.", "AI": {"tldr": "AngelSlim\u662f\u817e\u8baf\u6df7\u5143\u56e2\u961f\u5f00\u53d1\u7684\u5927\u6a21\u578b\u538b\u7f29\u5de5\u5177\u5305\uff0c\u6574\u5408\u4e86\u91cf\u5316\u3001\u63a8\u6d4b\u89e3\u7801\u3001token\u526a\u679d\u548c\u84b8\u998f\u7b49\u524d\u6cbf\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4ece\u6a21\u578b\u538b\u7f29\u5230\u5de5\u4e1a\u90e8\u7f72\u7684\u7edf\u4e00\u6d41\u7a0b\u3002", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u4e14\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u5957\u5168\u9762\u7684\u538b\u7f29\u5de5\u5177\u6765\u964d\u4f4e\u90e8\u7f72\u95e8\u69db\u5e76\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "method": "1. \u96c6\u6210FP8\u548cINT8\u540e\u8bad\u7ec3\u91cf\u5316\u7b97\u6cd5\uff0c\u5f00\u53d1\u8d85\u4f4e\u4f4d\u91cf\u5316\u6280\u672f\uff08\u59822\u4f4d\u5927\u6a21\u578bHY-1.8B-int2\uff09\n2. \u63d0\u51fa\u8bad\u7ec3\u5bf9\u9f50\u7684\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u517c\u5bb9\u591a\u6a21\u6001\u67b6\u6784\u548c\u73b0\u4ee3\u63a8\u7406\u5f15\u64ce\n3. \u5f00\u53d1\u8bad\u7ec3\u514d\u8d39\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u9759\u6001\u6a21\u5f0f\u548c\u52a8\u6001token\u9009\u62e9\u7684\u6df7\u5408\u65b9\u5f0f\u89e3\u8026\u7a00\u758f\u6838\u4e0e\u6a21\u578b\u67b6\u6784\n4. \u9488\u5bf9\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u4e13\u95e8\u526a\u679d\u7b56\u7565\uff1aIDPruner\u4f18\u5316\u89c6\u89c9token\uff0cSamp\u81ea\u9002\u5e94\u5408\u5e76\u548c\u526a\u679d\u97f3\u9891token", "result": "1. \u5b9e\u73b0\u4e86\u9996\u4e2a\u5de5\u4e1a\u53ef\u884c\u76842\u4f4d\u5927\u6a21\u578bHY-1.8B-int2\n2. \u63a8\u6d4b\u89e3\u7801\u6846\u67b6\u5728\u4e0d\u5f71\u54cd\u8f93\u51fa\u6b63\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b01.8-2.0\u500d\u541e\u5410\u91cf\u63d0\u5347\n3. \u7a00\u758f\u6ce8\u610f\u529b\u6846\u67b6\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u964d\u4f4e\u9996token\u751f\u6210\u65f6\u95f4\n4. \u591a\u6a21\u6001\u526a\u679d\u7b56\u7565\u6709\u6548\u4f18\u5316\u89c6\u89c9\u548c\u97f3\u9891token\u5904\u7406", "conclusion": "AngelSlim\u901a\u8fc7\u6574\u5408\u591a\u79cd\u538b\u7f29\u6280\u672f\uff0c\u4e3a\u7b97\u6cd5\u7814\u7a76\u548c\u5de5\u5177\u8f85\u52a9\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5927\u6a21\u578b\u7684\u90e8\u7f72\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2602.21268", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21268", "abs": "https://arxiv.org/abs/2602.21268", "authors": ["Takaaki Fujita", "Florentin Smarandache"], "title": "A Dynamic Survey of Soft Set Theory and Its Extensions", "comment": "Book.143 pages. Publisher: Neutrosophic Science International Association (NSIA) Publishing House. ISBN: 978-1-59973-859-8", "summary": "Soft set theory provides a direct framework for parameterized decision modeling by assigning to each attribute (parameter) a subset of a given universe, thereby representing uncertainty in a structured way [1, 2]. Over the past decades, the theory has expanded into numerous variants-including hypersoft sets, superhypersoft sets, TreeSoft sets, bipolar soft sets, and dynamic soft sets-and has been connected to diverse areas such as topology and matroid theory. In this book, we present a survey-style overview of soft sets and their major extensions, highlighting core definitions, representative constructions, and key directions of current development.", "AI": {"tldr": "\u672c\u4e66\u5bf9\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u4e3b\u8981\u6269\u5c55\u8fdb\u884c\u4e86\u7efc\u8ff0\u6027\u6982\u8ff0\uff0c\u6db5\u76d6\u6838\u5fc3\u5b9a\u4e49\u3001\u4ee3\u8868\u6027\u6784\u9020\u548c\u5f53\u524d\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u8f6f\u96c6\u7406\u8bba\u4e3a\u53c2\u6570\u5316\u51b3\u7b56\u5efa\u6a21\u63d0\u4f9b\u4e86\u76f4\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u5c5e\u6027\uff08\u53c2\u6570\uff09\u5206\u914d\u7ed9\u7ed9\u5b9a\u5b87\u5b99\u7684\u5b50\u96c6\u6765\u7ed3\u6784\u5316\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u7406\u8bba\u5728\u8fc7\u53bb\u51e0\u5341\u5e74\u5df2\u6269\u5c55\u5230\u591a\u4e2a\u53d8\u4f53\uff0c\u5e76\u4e0e\u62d3\u6251\u5b66\u3001\u62df\u9635\u7406\u8bba\u7b49\u591a\u4e2a\u9886\u57df\u5efa\u7acb\u4e86\u8054\u7cfb\u3002", "method": "\u91c7\u7528\u8c03\u67e5\u5f0f\u6982\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6574\u7406\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u4e3b\u8981\u6269\u5c55\uff0c\u5305\u62ec\u8d85\u8f6f\u96c6\u3001\u8d85\u8d85\u8f6f\u96c6\u3001\u6811\u8f6f\u96c6\u3001\u53cc\u6781\u8f6f\u96c6\u548c\u52a8\u6001\u8f6f\u96c6\u7b49\u53d8\u4f53\uff0c\u7a81\u51fa\u6838\u5fc3\u5b9a\u4e49\u548c\u4ee3\u8868\u6027\u6784\u9020\u3002", "result": "\u63d0\u4f9b\u4e86\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u6269\u5c55\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u5c55\u793a\u4e86\u8be5\u7406\u8bba\u7684\u53d1\u5c55\u8109\u7edc\u3001\u4e3b\u8981\u53d8\u4f53\u4ee5\u53ca\u4e0e\u4e0d\u540c\u6570\u5b66\u9886\u57df\u7684\u8fde\u63a5\u5173\u7cfb\u3002", "conclusion": "\u672c\u4e66\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406\u8f6f\u96c6\u7406\u8bba\u53ca\u5176\u6269\u5c55\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\u6846\u67b6\uff0c\u5e76\u6307\u660e\u4e86\u5f53\u524d\u7684\u53d1\u5c55\u65b9\u5411\u548c\u6f5c\u5728\u7684\u7814\u7a76\u673a\u4f1a\u3002"}}
{"id": "2602.21269", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.21269", "abs": "https://arxiv.org/abs/2602.21269", "authors": ["Wang Zixian"], "title": "Group Orthogonalized Policy Optimization:Group Policy Optimization as Orthogonal Projection in Hilbert Space", "comment": null, "summary": "We present Group Orthogonalized Policy Optimization (GOPO), a new alignment algorithm for large language models derived from the geometry of Hilbert function spaces. Instead of optimizing on the probability simplex and inheriting the exponential curvature of Kullback-Leibler divergence, GOPO lifts alignment into the Hilbert space L2(pi_k) of square-integrable functions with respect to the reference policy. Within this space, the simplex constraint reduces to a linear orthogonality condition <v, 1> = 0, defining a codimension-one subspace H0. Minimizing distance to an unconstrained target u_star yields the work-dissipation functional J(v) = <g, v> - (mu / 2) ||v||^2, whose maximizer follows directly from the Hilbert projection theorem. Enforcing the boundary v >= -1 produces a bounded Hilbert projection that induces exact sparsity, assigning zero probability to catastrophically poor actions through a closed-form threshold. To connect this functional theory with practice, GOPO projects from infinite-dimensional L2(pi_k) to a finite empirical subspace induced by group sampling. Because group-normalized advantages sum to zero, the Lagrange multiplier enforcing probability conservation vanishes exactly, reducing the constrained projection to an unconstrained empirical loss. The resulting objective has constant Hessian curvature mu I, non-saturating linear gradients, and an intrinsic dead-zone mechanism without heuristic clipping. Experiments on mathematical reasoning benchmarks show that GOPO achieves competitive generalization while maintaining stable gradient dynamics and entropy preservation in regimes where clipping-based methods plateau.", "AI": {"tldr": "GOPO\u662f\u4e00\u79cd\u57fa\u4e8e\u5e0c\u5c14\u4f2f\u7279\u51fd\u6570\u7a7a\u95f4\u51e0\u4f55\u7684\u65b0\u5bf9\u9f50\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u4f18\u5316\u95ee\u9898\u4ece\u6982\u7387\u5355\u7eaf\u5f62\u63d0\u5347\u5230L2\u7a7a\u95f4\uff0c\u5229\u7528\u7ebf\u6027\u6b63\u4ea4\u6761\u4ef6\u548c\u5e0c\u5c14\u4f2f\u7279\u6295\u5f71\u5b9a\u7406\uff0c\u5b9e\u73b0\u4e86\u6709\u754c\u6295\u5f71\u548c\u7cbe\u786e\u7a00\u758f\u6027\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u7b97\u6cd5\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u4f18\u5316\uff0c\u7ee7\u627f\u4e86KL\u6563\u5ea6\u7684\u6307\u6570\u66f2\u7387\u95ee\u9898\uff0c\u5bfc\u81f4\u68af\u5ea6\u9971\u548c\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u51fd\u6570\u7a7a\u95f4\u51e0\u4f55\u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u8fd9\u4e9b\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "GOPO\u5c06\u5bf9\u9f50\u95ee\u9898\u63d0\u5347\u5230\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4L2(pi_k)\uff0c\u5c06\u5355\u7eaf\u5f62\u7ea6\u675f\u8f6c\u5316\u4e3a\u7ebf\u6027\u6b63\u4ea4\u6761\u4ef6\u3002\u901a\u8fc7\u5e0c\u5c14\u4f2f\u7279\u6295\u5f71\u5b9a\u7406\u6700\u5927\u5316\u5de5\u4f5c-\u8017\u6563\u6cdb\u51fd\uff0c\u5b9e\u65bd\u6709\u754c\u6295\u5f71\u4ea7\u751f\u7cbe\u786e\u7a00\u758f\u6027\u3002\u901a\u8fc7\u7fa4\u91c7\u6837\u6295\u5f71\u5230\u6709\u9650\u7ecf\u9a8c\u5b50\u7a7a\u95f4\uff0c\u5229\u7528\u7fa4\u5f52\u4e00\u5316\u4f18\u52bf\u7684\u96f6\u548c\u7279\u6027\u6d88\u9664\u62c9\u683c\u6717\u65e5\u4e58\u5b50\uff0c\u5f97\u5230\u5177\u6709\u6052\u5b9aHessian\u66f2\u7387\u548c\u975e\u9971\u548c\u7ebf\u6027\u68af\u5ea6\u7684\u65e0\u7ea6\u675f\u7ecf\u9a8c\u635f\u5931\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGOPO\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u68af\u5ea6\u52a8\u6001\u548c\u71b5\u4fdd\u7559\u3002\u5728\u57fa\u4e8e\u88c1\u526a\u7684\u65b9\u6cd5\u8fbe\u5230\u5e73\u53f0\u671f\u7684\u60c5\u51b5\u4e0b\uff0cGOPO\u4ecd\u80fd\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "GOPO\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u51e0\u4f55\u7684\u5bf9\u9f50\u65b0\u8303\u5f0f\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6307\u6570\u66f2\u7387\u548c\u68af\u5ea6\u9971\u548c\u95ee\u9898\uff0c\u901a\u8fc7\u7cbe\u786e\u7a00\u758f\u6027\u548c\u6052\u5b9a\u66f2\u7387\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u3002"}}
{"id": "2602.21351", "categories": ["cs.AI", "cs.IR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.21351", "abs": "https://arxiv.org/abs/2602.21351", "authors": ["Dmitrii Pantiukhin", "Ivan Kuznetsov", "Boris Shapkin", "Antonia Anna Jost", "Thomas Jung", "Nikolay Koldunov"], "title": "A Hierarchical Multi-Agent System for Autonomous Discovery in Geoscientific Data Archives", "comment": "20 pages, 6 figures, 7 tables, supplementary material included", "summary": "The rapid accumulation of Earth science data has created a significant scalability challenge; while repositories like PANGAEA host vast collections of datasets, citation metrics indicate that a substantial portion remains underutilized, limiting data reusability. Here we present PANGAEA-GPT, a hierarchical multi-agent framework designed for autonomous data discovery and analysis. Unlike standard Large Language Model (LLM) wrappers, our architecture implements a centralized Supervisor-Worker topology with strict data-type-aware routing, sandboxed deterministic code execution, and self-correction via execution feedback, enabling agents to diagnose and resolve runtime errors. Through use-case scenarios spanning physical oceanography and ecology, we demonstrate the system's capacity to execute complex, multi-step workflows with minimal human intervention. This framework provides a methodology for querying and analyzing heterogeneous repository data through coordinated agent workflows.", "AI": {"tldr": "PANGAEA-GPT\uff1a\u4e00\u4e2a\u7528\u4e8e\u5730\u7403\u79d1\u5b66\u6570\u636e\u81ea\u4e3b\u53d1\u73b0\u548c\u5206\u6790\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u76d1\u7763-\u5de5\u4f5c\u8005\u62d3\u6251\u7ed3\u6784\u89e3\u51b3\u6570\u636e\u53ef\u6269\u5c55\u6027\u6311\u6218", "motivation": "\u5730\u7403\u79d1\u5b66\u6570\u636e\u5feb\u901f\u79ef\u7d2f\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0cPANGAEA\u7b49\u5b58\u50a8\u5e93\u4e2d\u5927\u91cf\u6570\u636e\u96c6\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9650\u5236\u4e86\u6570\u636e\u91cd\u7528\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u548c\u5206\u6790\u5f02\u6784\u5b58\u50a8\u5e93\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPANGAEA-GPT\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u91c7\u7528\u96c6\u4e2d\u5f0f\u76d1\u7763-\u5de5\u4f5c\u8005\u62d3\u6251\u7ed3\u6784\uff0c\u5177\u6709\u4e25\u683c\u7684\u6570\u636e\u7c7b\u578b\u611f\u77e5\u8def\u7531\u3001\u6c99\u76d2\u5316\u786e\u5b9a\u6027\u4ee3\u7801\u6267\u884c\u548c\u901a\u8fc7\u6267\u884c\u53cd\u9988\u7684\u81ea\u6211\u6821\u6b63\u673a\u5236\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u8bca\u65ad\u548c\u89e3\u51b3\u8fd0\u884c\u65f6\u9519\u8bef\u3002", "result": "\u901a\u8fc7\u7269\u7406\u6d77\u6d0b\u5b66\u548c\u751f\u6001\u5b66\u7684\u7528\u4f8b\u573a\u666f\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u4eba\u5de5\u5e72\u9884\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u5728\u67e5\u8be2\u548c\u5206\u6790\u5f02\u6784\u5b58\u50a8\u5e93\u6570\u636e\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u534f\u8c03\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7a0b\u67e5\u8be2\u548c\u5206\u6790\u5f02\u6784\u5b58\u50a8\u5e93\u6570\u636e\u7684\u65b9\u6cd5\u8bba\uff0c\u4e3a\u89e3\u51b3\u5730\u7403\u79d1\u5b66\u6570\u636e\u53ef\u6269\u5c55\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21534", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21534", "abs": "https://arxiv.org/abs/2602.21534", "authors": ["Xiaoxuan Wang", "Han Zhang", "Haixin Wang", "Yidan Shi", "Ruoyan Li", "Kaiqiao Han", "Chenyi Tong", "Haoran Deng", "Renliang Sun", "Alexander Taylor", "Yanqiao Zhu", "Jason Cong", "Yizhou Sun", "Wei Wang"], "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning", "comment": null, "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ARLArena\u6846\u67b6\u548cSAMPO\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3Agentic\u5f3a\u5316\u5b66\u4e60\uff08ARL\uff09\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u7a33\u5b9a\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "Agentic\u5f3a\u5316\u5b66\u4e60\uff08ARL\uff09\u5728\u5904\u7406\u590d\u6742\u591a\u6b65\u4ea4\u4e92\u4efb\u52a1\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u6781\u4e0d\u7a33\u5b9a\uff0c\u7ecf\u5e38\u5bfc\u81f4\u8bad\u7ec3\u5d29\u6e83\u3002\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u9650\u5236\u4e86ARL\u5728\u66f4\u5927\u73af\u5883\u548c\u66f4\u957f\u4ea4\u4e92\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e5f\u5236\u7ea6\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u7684\u7cfb\u7edf\u6027\u63a2\u7d22\u3002", "method": "\u9996\u5148\u63d0\u51faARLArena\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u6784\u5efa\u5e72\u51c0\u6807\u51c6\u5316\u7684\u6d4b\u8bd5\u5e73\u53f0\uff1b2\uff09\u5c06\u7b56\u7565\u68af\u5ea6\u5206\u89e3\u4e3a\u56db\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u7ef4\u5ea6\u5e76\u8bc4\u4f30\u6bcf\u4e2a\u7ef4\u5ea6\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002\u57fa\u4e8e\u6b64\u5206\u6790\uff0c\u63d0\u51faSAMPO\u65b9\u6cd5\u2014\u2014\u4e00\u79cd\u7a33\u5b9a\u7684Agentic\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u4e13\u95e8\u8bbe\u8ba1\u6765\u7f13\u89e3ARL\u4e2d\u7684\u4e3b\u8981\u4e0d\u7a33\u5b9a\u56e0\u7d20\u3002", "result": "SAMPO\u5728\u5404\u79cdAgentic\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u5f3a\u5927\u7684\u6027\u80fd\u8868\u73b0\u3002\u8be5\u7814\u7a76\u4e3aARL\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7b56\u7565\u68af\u5ea6\u89c6\u89d2\uff0c\u5e76\u4e3a\u6784\u5efa\u7a33\u5b9a\u53ef\u590d\u73b0\u7684LLM\u667a\u80fd\u4f53\u8bad\u7ec3\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7ARLArena\u6846\u67b6\u548cSAMPO\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86Agentic\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u6790\u548c\u4f18\u5316\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53ef\u91cd\u590d\u6027\u548c\u7a33\u5b9a\u6027\u53d1\u5c55\u3002"}}
{"id": "2602.21319", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.21319", "abs": "https://arxiv.org/abs/2602.21319", "authors": ["Marion Neumeier", "Niklas Ro\u00dfberg", "Michael Botsch", "Wolfgang Utschick"], "title": "Uncertainty-Aware Diffusion Model for Multimodal Highway Trajectory Prediction via DDIM Sampling", "comment": "Accepted as a conference paper in IEEE Intelligent Vehicles Symposium (IV) 2026, Detroit, MI, United States", "summary": "Accurate and uncertainty-aware trajectory prediction remains a core challenge for autonomous driving, driven by complex multi-agent interactions, diverse scene contexts and the inherently stochastic nature of future motion. Diffusion-based generative models have recently shown strong potential for capturing multimodal futures, yet existing approaches such as cVMD suffer from slow sampling, limited exploitation of generative diversity and brittle scenario encodings.\n  This work introduces cVMDx, an enhanced diffusion-based trajectory prediction framework that improves efficiency, robustness and multimodal predictive capability. Through DDIM sampling, cVMDx achieves up to a 100x reduction in inference time, enabling practical multi-sample generation for uncertainty estimation. A fitted Gaussian Mixture Model further provides tractable multimodal predictions from the generated trajectories. In addition, a CVQ-VAE variant is evaluated for scenario encoding. Experiments on the publicly available highD dataset show that cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction.", "AI": {"tldr": "cVMDx\u662f\u4e00\u4e2a\u6539\u8fdb\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7DDIM\u91c7\u6837\u5b9e\u73b0100\u500d\u63a8\u7406\u52a0\u901f\uff0c\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u63d0\u4f9b\u591a\u6a21\u6001\u9884\u6d4b\uff0c\u5728highD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u51c6\u786e\u4e14\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8f68\u8ff9\u9884\u6d4b\u9762\u4e34\u6838\u5fc3\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u3001\u591a\u6837\u5316\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u4ee5\u53ca\u672a\u6765\u8fd0\u52a8\u7684\u56fa\u6709\u968f\u673a\u6027\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u5982cVMD\u5b58\u5728\u91c7\u6837\u901f\u5ea6\u6162\u3001\u751f\u6210\u591a\u6837\u6027\u5229\u7528\u6709\u9650\u548c\u573a\u666f\u7f16\u7801\u8106\u5f31\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51facVMDx\u589e\u5f3a\u6846\u67b6\uff1a1) \u91c7\u7528DDIM\u91c7\u6837\u6280\u672f\u5927\u5e45\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\uff1b2) \u4f7f\u7528\u62df\u5408\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4ece\u751f\u6210\u7684\u8f68\u8ff9\u4e2d\u63d0\u53d6\u53ef\u5904\u7406\u7684\u591a\u6a21\u6001\u9884\u6d4b\uff1b3) \u8bc4\u4f30CVQ-VAE\u53d8\u4f53\u7528\u4e8e\u573a\u666f\u7f16\u7801\u3002", "result": "\u5728\u516c\u5f00\u7684highD\u6570\u636e\u96c6\u4e0a\uff0ccVMDx\u76f8\u6bd4cVMD\u5b9e\u73b0\u4e86\u9ad8\u8fbe100\u500d\u7684\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\uff0c\u540c\u65f6\u8fbe\u5230\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u8fdb\u884c\u5b8c\u5168\u968f\u673a\u7684\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\u3002", "conclusion": "cVMDx\u901a\u8fc7\u6539\u8fdb\u91c7\u6837\u6548\u7387\u548c\u751f\u6210\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21857", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21857", "abs": "https://arxiv.org/abs/2602.21857", "authors": ["Jabez Magomere", "Elena Kochkina", "Samuel Mensah", "Simerjot Kaur", "Fernando Acero", "Arturo Oncevay", "Charese H. Smiley", "Xiaomo Liu", "Manuela Veloso"], "title": "Distill and Align Decomposition for Enhanced Claim Verification", "comment": "EACL Findings 2026", "summary": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5206\u89e3\u8d28\u91cf\u548c\u9a8c\u8bc1\u5668\u5bf9\u9f50\uff0c\u63d0\u5347\u590d\u6742\u58f0\u660e\u9a8c\u8bc1\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5c06\u5206\u89e3\u8d28\u91cf\u4e0e\u9a8c\u8bc1\u6027\u80fd\u5bf9\u9f50\uff0c\u9700\u8981\u66f4\u597d\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5", "method": "\u4f7f\u7528Group Relative Policy Optimization\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u987a\u5e8f\u63a8\u7406\u3001\u6559\u5e08\u84b8\u998f\u793a\u4f8b\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4ee5\u53ca\u5e73\u8861\u683c\u5f0f\u5408\u89c4\u6027\u3001\u9a8c\u8bc1\u5668\u5bf9\u9f50\u548c\u5206\u89e3\u8d28\u91cf\u7684\u591a\u76ee\u6807\u5956\u52b1", "result": "\u5728\u516d\u4e2a\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\uff0c\u8bad\u7ec3\u76848B\u5206\u89e3\u5668\u5c06\u4e0b\u6e38\u9a8c\u8bc1\u6027\u80fd\u63d0\u5347\u81f371.75% macro-F1\uff0c\u4f18\u4e8e\u63d0\u793a\u65b9\u6cd5\uff08+1.99\uff0c+6.24\uff09\u548c\u73b0\u6709RL\u65b9\u6cd5\uff08+5.84\uff09\uff0c\u4eba\u7c7b\u8bc4\u4f30\u786e\u8ba4\u751f\u6210\u5b50\u58f0\u660e\u7684\u9ad8\u8d28\u91cf", "conclusion": "\u8be5\u6846\u67b6\u4f7f\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8054\u5408\u4f18\u5316\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u5206\u89e3\u8d28\u91cf\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u58f0\u660e\u9a8c\u8bc1\u6027\u80fd"}}
{"id": "2602.21858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21858", "abs": "https://arxiv.org/abs/2602.21858", "authors": ["Dezhi Kong", "Zhengzhao Feng", "Qiliang Liang", "Hao Wang", "Haofei Sun", "Changpeng Yang", "Yang Li", "Peng Zhou", "Shuai Nie", "Hongzhen Wang", "Linfeng Zhou", "Hao Jia", "Jiaming Xu", "Runyu Shi", "Ying Huang"], "title": "ProactiveMobile: A Comprehensive Benchmark for Boosting Proactive Intelligence on Mobile Devices", "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in mobile agent development, yet their capabilities are predominantly confined to a reactive paradigm, where they merely execute explicit user commands. The emerging paradigm of proactive intelligence, where agents autonomously anticipate needs and initiate actions, represents the next frontier for mobile agents. However, its development is critically bottlenecked by the lack of benchmarks that can address real-world complexity and enable objective, executable evaluation. To overcome these challenges, we introduce ProactiveMobile, a comprehensive benchmark designed to systematically advance research in this domain. ProactiveMobile formalizes the proactive task as inferring latent user intent across four dimensions of on-device contextual signals and generating an executable function sequence from a comprehensive function pool of 63 APIs. The benchmark features over 3,660 instances of 14 scenarios that embrace real-world complexity through multi-answer annotations. To ensure quality, a team of 30 experts conducts a final audit of the benchmark, verifying factual accuracy, logical consistency, and action feasibility, and correcting any non-compliant entries. Extensive experiments demonstrate that our fine-tuned Qwen2.5-VL-7B-Instruct achieves a success rate of 19.15%, outperforming o1 (15.71%) and GPT-5 (7.39%). This result indicates that proactivity is a critical competency widely lacking in current MLLMs, yet it is learnable, emphasizing the importance of the proposed benchmark for proactivity evaluation.", "AI": {"tldr": "ProactiveMobile\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79fb\u52a8\u667a\u80fd\u4f53\u4e3b\u52a8\u667a\u80fd\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3,660\u4e2a\u5b9e\u4f8b\u300114\u4e2a\u573a\u666f\u548c63\u4e2aAPI\u51fd\u6570\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u9884\u6d4b\u7528\u6237\u9700\u6c42\u65b9\u9762\u7684\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u4e3b\u8981\u5c40\u9650\u4e8e\u88ab\u52a8\u6267\u884c\u7528\u6237\u547d\u4ee4\u7684\u53cd\u5e94\u5f0f\u8303\u5f0f\uff0c\u800c\u4e3b\u52a8\u667a\u80fd\uff08\u667a\u80fd\u4f53\u81ea\u4e3b\u9884\u6d4b\u9700\u6c42\u5e76\u542f\u52a8\u884c\u52a8\uff09\u662f\u79fb\u52a8\u667a\u80fd\u4f53\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\u9886\u57df\uff0c\u4f46\u7f3a\u4e4f\u80fd\u591f\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\u5e76\u652f\u6301\u5ba2\u89c2\u53ef\u6267\u884c\u8bc4\u4f30\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51faProactiveMobile\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c06\u4e3b\u52a8\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u57fa\u4e8e\u8bbe\u5907\u4e0a\u4e0b\u6587\u4fe1\u53f7\u7684\u56db\u4e2a\u7ef4\u5ea6\u63a8\u65ad\u6f5c\u5728\u7528\u6237\u610f\u56fe\uff0c\u5e76\u4ece\u5305\u542b63\u4e2aAPI\u7684\u5168\u9762\u51fd\u6570\u6c60\u4e2d\u751f\u6210\u53ef\u6267\u884c\u51fd\u6570\u5e8f\u5217\u3002\u57fa\u51c6\u5305\u542b3,660\u4e2a\u5b9e\u4f8b\u300114\u4e2a\u573a\u666f\uff0c\u91c7\u7528\u591a\u7b54\u6848\u6807\u6ce8\u6765\u5e94\u5bf9\u73b0\u5b9e\u590d\u6742\u6027\uff0c\u5e76\u753130\u540d\u4e13\u5bb6\u56e2\u961f\u8fdb\u884c\u6700\u7ec8\u5ba1\u6838\uff0c\u786e\u4fdd\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u903b\u8f91\u4e00\u81f4\u6027\u548c\u884c\u52a8\u53ef\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u7684Qwen2.5-VL-7B-Instruct\u6a21\u578b\u5b9e\u73b0\u4e8619.15%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8eo1\uff0815.71%\uff09\u548cGPT-5\uff087.39%\uff09\u3002\u8fd9\u8868\u660e\u4e3b\u52a8\u667a\u80fd\u662f\u5f53\u524dMLLMs\u666e\u904d\u7f3a\u4e4f\u4f46\u53ef\u5b66\u4e60\u7684\u5173\u952e\u80fd\u529b\u3002", "conclusion": "\u4e3b\u52a8\u667a\u80fd\u662f\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u666e\u904d\u7f3a\u4e4f\u4f46\u53ef\u901a\u8fc7\u5b66\u4e60\u83b7\u5f97\u7684\u5173\u952e\u80fd\u529b\uff0cProactiveMobile\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u4e8e\u8bc4\u4f30\u548c\u63a8\u52a8\u4e3b\u52a8\u667a\u80fd\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4e3a\u79fb\u52a8\u667a\u80fd\u4f53\u7684\u4e3b\u52a8\u667a\u80fd\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.22067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22067", "abs": "https://arxiv.org/abs/2602.22067", "authors": ["Giuseppe Canonaco", "Alberto Pozanco", "Daniel Borrajo"], "title": "Semantic Partial Grounding via LLMs", "comment": null, "summary": "Grounding is a critical step in classical planning, yet it often becomes a computational bottleneck due to the exponential growth in grounded actions and atoms as task size increases. Recent advances in partial grounding have addressed this challenge by incrementally grounding only the most promising operators, guided by predictive models. However, these approaches primarily rely on relational features or learned embeddings and do not leverage the textual and structural cues present in PDDL descriptions. We propose SPG-LLM, which uses LLMs to analyze the domain and problem files to heuristically identify potentially irrelevant objects, actions, and predicates prior to grounding, significantly reducing the size of the grounded task. Across seven hard-to-ground benchmarks, SPG-LLM achieves faster grounding-often by orders of magnitude-while delivering comparable or better plan costs in some domains.", "AI": {"tldr": "SPG-LLM\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790PDDL\u63cf\u8ff0\uff0c\u5728\u89c4\u5212\u4efb\u52a1\u63a5\u5730\u524d\u8bc6\u522b\u6f5c\u5728\u65e0\u5173\u7684\u5bf9\u8c61\u3001\u52a8\u4f5c\u548c\u8c13\u8bcd\uff0c\u5927\u5e45\u51cf\u5c11\u63a5\u5730\u4efb\u52a1\u89c4\u6a21\uff0c\u663e\u8457\u63d0\u5347\u63a5\u5730\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u89c4\u5212\u4e2d\u7684\u63a5\u5730\u6b65\u9aa4\u5e38\u56e0\u4efb\u52a1\u89c4\u6a21\u589e\u5927\u5bfc\u81f4\u63a5\u5730\u52a8\u4f5c\u548c\u539f\u5b50\u5448\u6307\u6570\u589e\u957f\u800c\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\u3002\u73b0\u6709\u90e8\u5206\u63a5\u5730\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5173\u7cfb\u7279\u5f81\u6216\u5b66\u4e60\u5d4c\u5165\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528PDDL\u63cf\u8ff0\u4e2d\u7684\u6587\u672c\u548c\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u63d0\u51faSPG-LLM\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u9886\u57df\u548c\u95ee\u9898\u6587\u4ef6\uff0c\u5728\u63a5\u5730\u524d\u542f\u53d1\u5f0f\u8bc6\u522b\u6f5c\u5728\u65e0\u5173\u7684\u5bf9\u8c61\u3001\u52a8\u4f5c\u548c\u8c13\u8bcd\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u63a5\u5730\u4efb\u52a1\u89c4\u6a21\u3002", "result": "\u5728\u4e03\u4e2a\u96be\u4ee5\u63a5\u5730\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPG-LLM\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a5\u5730\u901f\u5ea6\uff08\u901a\u5e38\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff09\uff0c\u5728\u67d0\u4e9b\u9886\u57df\u8fd8\u63d0\u4f9b\u4e86\u76f8\u5f53\u6216\u66f4\u597d\u7684\u89c4\u5212\u6210\u672c\u3002", "conclusion": "SPG-LLM\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790PDDL\u7684\u6587\u672c\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c4\u5212\u63a5\u5730\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a5\u5730\u6548\u7387\u3002"}}
{"id": "2602.22146", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22146", "abs": "https://arxiv.org/abs/2602.22146", "authors": ["Yining Li", "Peizhong Ju", "Ness Shroff"], "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parameterization in practical applications. In this work, we propose a universal primal-dual framework for safe RLHF that unifies a broad class of existing alignment algorithms, including safe-RLHF, one-shot, and multi-shot based methods. Building on this framework, we introduce an optimistic primal-dual (OPD) algorithm that incorporates predictive updates for both primal and dual variables to stabilize saddle-point dynamics. We establish last-iterate convergence guarantees for the proposed method, covering both exact policy optimization in the distributional space and convergence to a neighborhood of the optimal solution whose gap is related to approximation error and bias under parameterized policies. Our analysis reveals that optimism plays a crucial role in mitigating oscillations inherent to constrained alignment objectives, thereby closing a key theoretical gap between constrained RL and practical RLHF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u539f\u59cb-\u5bf9\u5076\u6846\u67b6\u7edf\u4e00\u73b0\u6709\u5bf9\u9f50\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u4e50\u89c2\u539f\u59cb-\u5bf9\u5076\u7b97\u6cd5\u6765\u89e3\u51b3RLHF\u4e2d\u7684\u6536\u655b\u4e0d\u7a33\u5b9a\u95ee\u9898", "motivation": "RLHF\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u6807\u51c6\u539f\u59cb-\u5bf9\u5076\u65b9\u6cd5\u5b58\u5728\u6536\u655b\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7b56\u7565\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\u53ef\u80fd\u53d1\u6563", "method": "\u63d0\u51fa\u901a\u7528\u539f\u59cb-\u5bf9\u5076\u6846\u67b6\u7edf\u4e00\u73b0\u6709\u5bf9\u9f50\u7b97\u6cd5\uff0c\u5f15\u5165\u4e50\u89c2\u539f\u59cb-\u5bf9\u5076\u7b97\u6cd5\uff0c\u5305\u542b\u539f\u59cb\u53d8\u91cf\u548c\u5bf9\u5076\u53d8\u91cf\u7684\u9884\u6d4b\u66f4\u65b0\u4ee5\u7a33\u5b9a\u978d\u70b9\u52a8\u6001", "result": "\u5efa\u7acb\u4e86\u6700\u540e\u8fed\u4ee3\u6536\u655b\u4fdd\u8bc1\uff0c\u6db5\u76d6\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u7cbe\u786e\u7b56\u7565\u4f18\u5316\u4ee5\u53ca\u5728\u53c2\u6570\u5316\u7b56\u7565\u4e0b\u6536\u655b\u5230\u6700\u4f18\u89e3\u90bb\u57df\uff0c\u90bb\u57df\u5927\u5c0f\u4e0e\u8fd1\u4f3c\u8bef\u5dee\u548c\u504f\u5dee\u76f8\u5173", "conclusion": "\u4e50\u89c2\u673a\u5236\u5728\u7f13\u89e3\u7ea6\u675f\u5bf9\u9f50\u76ee\u6807\u56fa\u6709\u632f\u8361\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u586b\u8865\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e0e\u5b9e\u7528RLHF\u4e4b\u95f4\u7684\u7406\u8bba\u7a7a\u767d"}}
{"id": "2602.22190", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.22190", "abs": "https://arxiv.org/abs/2602.22190", "authors": ["Rui Yang", "Qianhui Wu", "Zhaoyang Wang", "Hanyang Chen", "Ke Yang", "Hao Cheng", "Huaxiu Yao", "Baoling Peng", "Huan Zhang", "Jianfeng Gao", "Tong Zhang"], "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL", "comment": "57 pages, 17 figures", "summary": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.", "AI": {"tldr": "GUI-Libra\u63d0\u51fa\u9488\u5bf9GUI\u667a\u80fd\u4f53\u7684\u5b9a\u5236\u5316\u8bad\u7ec3\u65b9\u6848\uff0c\u89e3\u51b3\u5f00\u6e90GUI\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u5bfc\u822a\u4efb\u52a1\u4e2d\u843d\u540e\u4e8e\u95ed\u6e90\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u636e\u6784\u5efa\u3001\u52a8\u4f5c\u611f\u77e5SFT\u548cKL\u6b63\u5219\u5316RL\u7b49\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f00\u6e90\u539f\u751fGUI\u667a\u80fd\u4f53\u5728\u957f\u65f6\u7a0b\u5bfc\u822a\u4efb\u52a1\u4e0a\u4ecd\u843d\u540e\u4e8e\u95ed\u6e90\u7cfb\u7edf\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u52a8\u4f5c\u5bf9\u9f50\u63a8\u7406\u6570\u636e\uff0c\u4ee5\u53ca\u76f4\u63a5\u91c7\u7528\u901a\u7528\u540e\u8bad\u7ec3\u6d41\u7a0b\u800c\u5ffd\u89c6\u4e86GUI\u667a\u80fd\u4f53\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "1) \u6784\u5efa\u548c\u8fc7\u6ee4\u6570\u636e\u7ba1\u9053\uff0c\u53d1\u5e0381K GUI\u63a8\u7406\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u52a8\u4f5c\u611f\u77e5SFT\uff0c\u6df7\u5408\u63a8\u7406-\u52a8\u4f5c\u548c\u76f4\u63a5\u52a8\u4f5c\u6570\u636e\uff0c\u91cd\u65b0\u52a0\u6743token\u4ee5\u5f3a\u8c03\u52a8\u4f5c\u548c\u63a5\u5730\uff1b3) \u5728\u90e8\u5206\u53ef\u9a8c\u8bc1\u6027\u4e0b\u7a33\u5b9aRL\uff0c\u5f3a\u8c03KL\u6b63\u5219\u5316\u7684\u91cd\u8981\u6027\uff0c\u5f15\u5165\u6210\u529f\u81ea\u9002\u5e94\u7f29\u653e\u6765\u964d\u4f4e\u4e0d\u53ef\u9760\u8d1f\u68af\u5ea6\u6743\u91cd\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u7f51\u9875\u548c\u79fb\u52a8\u7aef\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGUI-Libra\u6301\u7eed\u63d0\u5347\u4e86\u6b65\u8fdb\u51c6\u786e\u7387\u548c\u7aef\u5230\u7aef\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u8868\u660e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u540e\u8bad\u7ec3\u548c\u6570\u636e\u7ba1\u7406\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5728\u7ebf\u6570\u636e\u6536\u96c6\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u6570\u636e\u7ba1\u7406\u548c\u8bad\u7ec3\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347GUI\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u4e3a\u63a8\u7406\u80fd\u529b\u5f3a\u7684GUI\u667a\u80fd\u4f53\u7684\u6570\u636e\u9ad8\u6548\u540e\u8bad\u7ec3\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.22130", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.22130", "abs": "https://arxiv.org/abs/2602.22130", "authors": ["Ilias Diakonikolas", "Giannis Iakovidis", "Daniel M. Kane", "Sihan Liu"], "title": "Sample Complexity Bounds for Robust Mean Estimation with Mean-Shift Contamination", "comment": null, "summary": "We study the basic task of mean estimation in the presence of mean-shift contamination. In the mean-shift contamination model, an adversary is allowed to replace a small constant fraction of the clean samples by samples drawn from arbitrarily shifted versions of the base distribution. Prior work characterized the sample complexity of this task for the special cases of the Gaussian and Laplace distributions. Specifically, it was shown that consistent estimation is possible in these cases, a property that is provably impossible in Huber's contamination model. An open question posed in earlier work was to determine the sample complexity of mean estimation in the mean-shift contamination model for general base distributions. In this work, we study and essentially resolve this open question. Specifically, we show that, under mild spectral conditions on the characteristic function of the (potentially multivariate) base distribution, there exists a sample-efficient algorithm that estimates the target mean to any desired accuracy. We complement our upper bound with a qualitatively matching sample complexity lower bound. Our techniques make critical use of Fourier analysis, and in particular introduce the notion of a Fourier witness as an essential ingredient of our upper and lower bounds.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5747\u503c\u504f\u79fb\u6c61\u67d3\u6a21\u578b\u4e0b\u7684\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u6e29\u548c\u7684\u5149\u8c31\u6761\u4ef6\u4e0b\uff0c\u5b58\u5728\u6837\u672c\u9ad8\u6548\u7684\u7b97\u6cd5\u53ef\u4ee5\u4f30\u8ba1\u76ee\u6807\u5747\u503c\uff0c\u5e76\u7ed9\u51fa\u4e86\u5339\u914d\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u754c\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u4ec5\u9488\u5bf9\u9ad8\u65af\u5206\u5e03\u548c\u62c9\u666e\u62c9\u65af\u5206\u5e03\u7b49\u7279\u6b8a\u60c5\u51b5\u5206\u6790\u4e86\u5747\u503c\u504f\u79fb\u6c61\u67d3\u6a21\u578b\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4f46\u5bf9\u4e8e\u4e00\u822c\u57fa\u7840\u5206\u5e03\u7684\u5747\u503c\u4f30\u8ba1\u6837\u672c\u590d\u6742\u5ea6\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5085\u91cc\u53f6\u5206\u6790\u6280\u672f\uff0c\u7279\u522b\u662f\u5f15\u5165\u4e86\u5085\u91cc\u53f6\u89c1\u8bc1\u7684\u6982\u5ff5\uff0c\u4f5c\u4e3a\u4e0a\u4e0b\u754c\u8bc1\u660e\u7684\u5173\u952e\u5de5\u5177\u3002\u5728\u6e29\u548c\u7684\u5149\u8c31\u6761\u4ef6\u4e0b\uff0c\u63d0\u51fa\u4e86\u6837\u672c\u9ad8\u6548\u7684\u5747\u503c\u4f30\u8ba1\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6e29\u548c\u7684\u5149\u8c31\u6761\u4ef6\u4e0b\uff0c\u5b58\u5728\u6837\u672c\u9ad8\u6548\u7684\u7b97\u6cd5\u53ef\u4ee5\u4f30\u8ba1\u76ee\u6807\u5747\u503c\u5230\u4efb\u610f\u6240\u9700\u7cbe\u5ea6\u3002\u540c\u65f6\u7ed9\u51fa\u4e86\u4e0e\u4e0a\u754c\u5728\u8d28\u91cf\u4e0a\u5339\u914d\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u754c\u3002", "conclusion": "\u672c\u6587\u57fa\u672c\u89e3\u51b3\u4e86\u5747\u503c\u504f\u79fb\u6c61\u67d3\u6a21\u578b\u4e2d\u4e00\u822c\u57fa\u7840\u5206\u5e03\u7684\u5747\u503c\u4f30\u8ba1\u6837\u672c\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u8868\u660e\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u5b58\u5728\u6837\u672c\u9ad8\u6548\u7684\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u7684\u4e0a\u4e0b\u754c\u3002"}}
{"id": "2602.22179", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22179", "abs": "https://arxiv.org/abs/2602.22179", "authors": ["Mhd Jawad Al Rahwanji", "Sascha Xu", "Nils Philipp Walter", "Jilles Vreeken"], "title": "Learning and Naming Subgroups with Exceptional Survival Characteristics", "comment": null, "summary": "In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.", "AI": {"tldr": "Sysurv\u662f\u4e00\u79cd\u5b8c\u5168\u53ef\u5fae\u5206\u7684\u975e\u53c2\u6570\u65b9\u6cd5\uff0c\u5229\u7528\u968f\u673a\u751f\u5b58\u68ee\u6797\u5b66\u4e60\u4e2a\u4f53\u751f\u5b58\u66f2\u7ebf\uff0c\u81ea\u52a8\u5b66\u4e60\u6761\u4ef6\u5e76\u5c06\u5176\u7ec4\u5408\u6210\u53ef\u89e3\u91ca\u7684\u89c4\u5219\uff0c\u4ee5\u8bc6\u522b\u5177\u6709\u5f02\u5e38\u751f\u5b58\u7279\u5f81\u7684\u4e9a\u7fa4\u3002", "motivation": "\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u8bc6\u522b\u751f\u5b58\u65f6\u95f4\u66f4\u957f\u6216\u66f4\u77ed\u7684\u4e9a\u7fa4\u975e\u5e38\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9650\u5236\u6027\u5047\u8bbe\uff08\u5982\u6bd4\u4f8b\u98ce\u9669\uff09\u3001\u9884\u79bb\u6563\u5316\u7279\u5f81\uff0c\u5e76\u4e14\u7531\u4e8e\u6bd4\u8f83\u5e73\u5747\u7edf\u8ba1\u91cf\u800c\u5bb9\u6613\u5ffd\u7565\u4e2a\u4f53\u504f\u5dee\u3002", "method": "\u63d0\u51faSysurv\u65b9\u6cd5\uff1a\u5b8c\u5168\u53ef\u5fae\u5206\u3001\u975e\u53c2\u6570\uff0c\u5229\u7528\u968f\u673a\u751f\u5b58\u68ee\u6797\u5b66\u4e60\u4e2a\u4f53\u751f\u5b58\u66f2\u7ebf\uff0c\u81ea\u52a8\u5b66\u4e60\u6761\u4ef6\u5e76\u5c06\u5176\u7ec4\u5408\u6210\u5185\u5728\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u548c\u8bbe\u7f6e\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5305\u62ec\u764c\u75c7\u6570\u636e\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u663e\u793aSysurv\u80fd\u591f\u63ed\u793a\u6709\u6d1e\u5bdf\u529b\u548c\u53ef\u64cd\u4f5c\u7684\u751f\u5b58\u4e9a\u7fa4\u3002", "conclusion": "Sysurv\u662f\u4e00\u79cd\u6709\u6548\u7684\u975e\u53c2\u6570\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u5177\u6709\u5f02\u5e38\u751f\u5b58\u7279\u5f81\u7684\u4e9a\u7fa4\uff0c\u65e0\u9700\u9650\u5236\u6027\u5047\u8bbe\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u3002"}}
{"id": "2602.22124", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22124", "abs": "https://arxiv.org/abs/2602.22124", "authors": ["Patrick Tser Jern Kon", "Archana Pradeep", "Ang Chen", "Alexander P. Ellis", "Warren Hunt", "Zijian Wang", "John Yang", "Samuel Thompson"], "title": "SWE-Prot\u00e9g\u00e9: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents", "comment": null, "summary": "Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Prot\u00e9g\u00e9, a post-training framework that reframes software repair as an expert-prot\u00e9g\u00e9 collaboration problem. In SWE-Prot\u00e9g\u00e9, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).", "AI": {"tldr": "SWE-Prot\u00e9g\u00e9\u662f\u4e00\u4e2a\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6-\u5b66\u5f92\u534f\u4f5c\u65b9\u5f0f\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c06Qwen2.5-Coder-7B-Instruct\u5728SWE-bench Verified\u4e0a\u7684Pass@1\u63d0\u5347\u523042.4%\uff0c\u76f8\u6bd4\u4e4b\u524d\u6700\u4f73SLM\u63d0\u534725.4%", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u9002\u5e94\u6027\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5728SWE-bench\u7b49\u957f\u89c6\u91ce\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u666e\u904d\u7684\u52a8\u4f5c\u5faa\u73af\u548c\u4f4e\u89e3\u51b3\u7387\u95ee\u9898", "method": "\u63d0\u51faSWE-Prot\u00e9g\u00e9\u6846\u67b6\uff0c\u5c06\u8f6f\u4ef6\u4fee\u590d\u91cd\u6784\u4e3a\u4e13\u5bb6-\u5b66\u5f92\u534f\u4f5c\u95ee\u9898\u3002SLM\u4f5c\u4e3a\u552f\u4e00\u51b3\u7b56\u8005\uff0c\u5b66\u4e60\u9009\u62e9\u6027\u5bfb\u6c42\u4e13\u5bb6\u6307\u5bfc\u3001\u8bc6\u522b\u505c\u6ede\u72b6\u6001\u5e76\u6267\u884c\u4e13\u5bb6\u53cd\u9988\u3002\u7ed3\u5408\u4e13\u5bb6\u589e\u5f3a\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\u548c\u663e\u5f0f\u963b\u6b62\u9000\u5316\u5faa\u73af\u53ca\u65e0\u6548\u4e13\u5bb6\u534f\u4f5c\u7684\u667a\u80fd\u5f3a\u5316\u5b66\u4e60", "result": "\u5bf9Qwen2.5-Coder-7B-Instruct\u8fdb\u884c\u8f7b\u91cf\u540e\u8bad\u7ec3\uff0c\u5728SWE-bench Verified\u4e0a\u8fbe\u523042.4% Pass@1\uff0c\u76f8\u6bd4\u4e4b\u524d\u6700\u4f73SLM\u63d0\u534725.4%\uff0c\u540c\u65f6\u7a00\u758f\u4f7f\u7528\u4e13\u5bb6\u534f\u52a9\uff08\u7ea64\u6b21\u8c03\u7528/\u4efb\u52a1\uff0c\u5360\u603btoken\u768411%\uff09", "conclusion": "SWE-Prot\u00e9g\u00e9\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86SLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u52a8\u4f5c\u5faa\u73af\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u7684\u4e13\u5bb6\u534f\u4f5c\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8f6f\u4ef6\u4fee\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
