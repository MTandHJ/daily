<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]
- [cs.LG](#cs.LG) [Total: 191]
- [cs.CY](#cs.CY) [Total: 10]
- [cs.IR](#cs.IR) [Total: 25]
- [cs.AI](#cs.AI) [Total: 79]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 本文开发了一种分层贝叶斯模型来量化鞋印中"偶然特征"的稀有性，通过潜在高斯模型和空间变化系数改进现有方法，提高法医鞋印分析的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 鞋印证据在法医调查中至关重要，但仅匹配鞋的品牌和型号不足以确定嫌疑人的鞋子，因为同一型号可能生产数千双。需要量化鞋底上因磨损形成的"偶然特征"（如划痕、切口）的稀有性，以准确评估证据强度。

Method: 开发了分层贝叶斯模型，采用潜在高斯模型框架，通过集成嵌套拉普拉斯近似实现大规模注释鞋印数据的高效推理，并引入空间变化系数来建模鞋底花纹与偶然特征位置之间的关系。

Result: 在保留数据上表现出优越性能，相比现有方法在准确性和可靠性方面有所提升，增强了法医鞋印分析的能力。

Conclusion: 该方法通过统计建模量化鞋印偶然特征的稀有性，为法医调查提供了更准确、可靠的证据评估工具，有助于提高鞋印证据在法庭上的科学价值。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [2] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于归因的人类先验对齐方法，通过约束模型决策证据与人类预期区域的一致性，提升模型可靠性和决策合理性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习仅提供类别标签，模型可能通过捷径相关性而非预期证据实现高准确率。人类先验可以帮助约束这种行为，但模型学习到的表示往往与人类感知存在差异，对齐模型与人类先验仍然具有挑战性。

Method: 提出基于归因的人类先验对齐方法：将人类先验编码为模型预期依赖的输入区域（如边界框），利用高保真度的基于子集选择的归因方法在训练中暴露模型的决策证据。当归因区域显著偏离先验区域时，惩罚对非先验证据的依赖，促使模型将其归因转向预期区域，通过训练目标施加人类先验诱导的归因约束。

Result: 在图像分类和点击决策任务中验证了该方法，涵盖传统分类和自回归生成设置。人类先验对齐一致提高了任务准确性，同时增强了模型的决策合理性。

Conclusion: 基于归因的人类先验对齐方法能够有效约束模型依赖预期证据进行决策，提高模型可靠性和决策合理性，在多种任务和设置中均表现出良好效果。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

</details>


### [3] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: MAU-Set数据集和MAU-GPT模型用于工业异常检测，通过多领域数据集和新型AMoE-LoRA机制提升模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 工业制造规模扩大需要自动化细粒度产品图像分析，但现有方法受限于数据集覆盖不足和模型泛化能力差，难以处理多样复杂的异常模式

Method: 1) 引入MAU-Set多类型工业异常理解数据集，涵盖多个工业领域和分层任务结构；2) 建立严格评估协议；3) 提出MAU-GPT领域适应多模态大模型，采用新颖的AMoE-LoRA机制统一异常感知和通用专家适应

Result: 大量实验表明MAU-GPT在所有领域均优于先前最先进方法，展示了可扩展自动化工业检测的强大潜力

Conclusion: MAU-Set数据集和MAU-GPT模型为解决工业异常检测中的数据集覆盖和模型泛化问题提供了有效解决方案，推动了工业质量控制的自动化发展

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [4] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: RetSAM是一个通用的视网膜分割和量化框架，用于眼底成像，能够进行多目标分割并提取标准化生物标志物，支持大规模眼科研究和转化应用。


<details>
  <summary>Details</summary>
Motivation: 视网膜成像快速、无创且广泛应用，为眼科和全身健康评估提供了可量化的结构和血管信号。然而，由于公共多标签数据集的有限性以及缺乏统一的分割到量化流程，大规模分析仍然困难。

Method: RetSAM是一个通用的视网膜分割和量化框架，使用超过20万张眼底图像进行训练，支持三类任务，分割五种解剖结构、四种视网膜表型模式和20多种不同的病变类型。采用多阶段训练策略，结合私有和公共眼底数据。

Result: 在17个公共数据集上实现了优异的分割性能，平均DSC比先前最佳方法提高3.9个百分点，在具有挑战性的多任务基准上提升高达15个百分点。能够跨不同人群、成像设备和临床环境良好泛化。

Conclusion: RetSAM将眼底图像转化为标准化、可解释的定量表型，支持跨主要眼科疾病的系统性相关分析，包括糖尿病视网膜病变、年龄相关性黄斑变性、青光眼和病理性近视，从而促进大规模眼科研究和转化应用。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [5] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: CR-VLM提出了一种可配置的视觉语言模型拒绝机制，通过激活引导实现用户自适应的安全对齐，解决现有拒绝策略"一刀切"的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的拒绝机制大多是"一刀切"的，无法适应不同用户需求和上下文约束，导致拒绝不足或过度拒绝的问题。

Method: CR-VLM包含三个核心组件：1) 通过教师强制机制提取可配置拒绝向量以增强拒绝信号；2) 引入门控机制保留范围内查询的接受能力；3) 设计反事实视觉增强模块对齐视觉表示与拒绝需求。

Result: 在多个数据集和各种VLM上的综合实验表明，CR-VLM实现了有效、高效且鲁棒的可配置拒绝，为VLM中的用户自适应安全对齐提供了可扩展路径。

Conclusion: CR-VLM为视觉语言模型提供了一种可配置的拒绝机制，能够更好地平衡安全性和实用性，推动用户自适应的安全对齐发展。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [6] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: Vectra是首个无参考、基于MLLM的电商图像内机器翻译视觉质量评估框架，通过14维可解释指标、大规模数据集和4B参数模型，在人类排名相关性上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前电商图像内机器翻译(IIMT)研究中，现有方法在评估视觉渲染质量方面存在不足：基于参考的方法(如SSIM、FID)缺乏可解释性，而模型即评判方法缺乏领域基础和细粒度奖励信号，特别是在面对上下文密集的产品图像和多模态缺陷时。

Method: Vectra包含三个核心组件：1) Vectra Score - 14维可解释质量度量系统，通过空间感知的缺陷面积比(DAR)量化减少标注歧义；2) Vectra Dataset - 从110万真实产品图像构建，包含2K基准集、30K推理标注和3.5K专家偏好标注；3) Vectra Model - 4B参数MLLM，能生成量化分数和诊断推理。

Result: 实验表明Vectra在人类排名相关性上达到最先进水平，其模型在评分性能上超越了包括GPT-5和Gemini-3在内的领先MLLMs。

Conclusion: Vectra填补了电商IIMT视觉质量评估的空白，提供了首个无参考、可解释的评估框架，其数据集和模型将在论文接受后发布。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [7] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 提出一种用于孟加拉国纸币识别的混合CNN架构，结合MobileNetV3-Large和EfficientNetB0进行特征提取，配合MLP分类器，在资源受限设备上实现高精度纸币识别。


<details>
  <summary>Details</summary>
Motivation: 准确的纸币识别对于辅助技术至关重要，特别是视障人士依赖他人识别纸币容易遭受欺诈和剥削。现有识别模型存在局限性，需要更鲁棒和高效的解决方案。

Method: 1) 构建新的孟加拉国纸币数据集，包含受控和真实场景；2) 整合四个额外数据集增强鲁棒性；3) 提出混合CNN架构，结合MobileNetV3-Large和EfficientNetB0进行特征提取；4) 使用多层感知机(MLP)分类器；5) 采用五折交叉验证和七种评估指标；6) 集成LIME和SHAP等可解释AI方法。

Result: 模型在受控数据集上达到97.95%准确率，复杂背景上达到92.84%准确率，所有数据集综合准确率为94.98%。通过七种评估指标和可解释AI方法验证了模型性能。

Conclusion: 提出的混合CNN架构在孟加拉国纸币识别任务中表现出色，既保持了高精度又适合资源受限设备，同时通过可解释AI方法增强了透明度，有助于视障人士的金融独立和安全。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [8] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 该论文研究了在无监督3D场景重建中应用高斯约束表示的方法，针对多场景图像集合中的视觉模糊性问题，提出了基于LeJEPA启发的管道，通过各向同性高斯约束改进场景分离和相机姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决从非结构化图像集合进行无监督3D场景重建的挑战，特别是在图像来自多个不相关场景且存在显著视觉模糊性的情况下。IMC2025挑战赛突出了这些困难，需要在包含异常值和混合内容的真实世界条件下同时进行场景发现和相机姿态估计。

Method: 提出了三个逐步改进的管道，最终采用LeJEPA启发的各向同性高斯约束方法。该方法在学习的图像嵌入上强制执行高斯约束，而不是引入新的理论保证，主要从经验上评估这些约束如何影响聚类一致性和姿态估计鲁棒性。

Result: 在IMC2025上的实验结果表明，与启发式基线相比，高斯约束嵌入可以改善场景分离和姿态合理性，特别是在视觉模糊性设置中。这些约束有助于提高场景分离能力和相机姿态估计的鲁棒性。

Conclusion: 理论上动机的表征约束为桥接自监督学习原理和实际运动结构管道提供了一个有前景的方向。高斯约束表示在实践中能够有效处理多场景图像集合中的视觉模糊性问题。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [9] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: XAI-CLIP：一种基于多模态视觉语言模型嵌入的ROI引导扰动框架，用于生成更清晰、边界感知的显著性图，同时显著降低计算开销，提高医学图像分割的可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在医学图像分割中表现出色，但其有限的可解释性阻碍了临床信任和部署。现有的可解释人工智能技术通常计算成本高、需要多次前向传播，且经常产生噪声大或解剖学上不相关的解释。

Method: 提出XAI-CLIP框架，利用多模态视觉语言模型嵌入来定位临床相关的解剖区域，并指导解释过程。该方法整合了语言引导的区域定位与医学图像分割，应用有针对性的区域感知扰动。

Result: 在FLARE22和CHAOS数据集上的实验表明，XAI-CLIP相比传统扰动方法实现了：运行时减少60%，Dice分数提高44.6%，基于遮挡的解释的IoU提高96.7%。定性结果也显示更干净、解剖学上一致的归因图。

Conclusion: 将多模态视觉语言表示整合到基于扰动的XAI框架中，显著提高了医学图像分割系统的可解释性和效率，为实现透明且可临床部署的系统提供了可能。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [10] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 研究通过分析视觉语言模型的表征几何结构，揭示了多目标视觉任务中错误模式的内在机制，发现概念向量间的几何重叠与特定错误模式强相关。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多目标视觉任务中表现出令人困惑的失败模式，如幻觉不存在的元素或无法在干扰物中识别最相似的对象。这些错误反映了类似人类认知约束的"绑定问题"，但人工系统中驱动这些错误的内部机制仍不清楚。

Method: 通过分析开放权重视觉语言模型（Qwen、InternVL、Gemma）的表征几何结构，比较提取"概念向量"的方法论——这些向量是编码视觉概念的潜在方向。通过转向干预验证概念向量，在简化和自然主义视觉任务中可靠地操纵模型行为。

Result: 观察到这些概念向量之间的几何重叠与特定错误模式强相关，为理解内部表征如何塑造模型行为和驱动视觉失败提供了基于量化的框架。

Conclusion: 研究提出了一个机制性见解，通过表征几何分析揭示了视觉语言模型在多目标视觉任务中错误模式的内在驱动因素，为理解和诊断模型视觉失败提供了定量框架。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [11] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出ReAlign和ReVision框架，通过精确建模模态间隙的几何形状，利用大规模非配对数据实现模态对齐，为多模态大语言模型提供高效扩展路径。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态对比学习在视觉和语言表示对齐方面取得了成功，但存在模态间隙问题：表达相同语义的不同模态嵌入占据系统偏移区域。现有方法受限于过度简化的各向同性假设，难以应用于大规模场景。

Method: 提出固定框架模态间隙理论，将模态间隙分解为稳定偏差和各向异性残差。基于此提出ReAlign训练无关的模态对齐策略，通过锚点、追踪和质心对齐三步将文本表示对齐到图像表示分布。进一步提出ReVision训练范式，将ReAlign集成到预训练阶段。

Result: ReAlign能够明确修正几何错位，ReVision使模型能够在视觉指令调优前从未配对文本中学习视觉表示分布，无需大规模高质量图像-文本对。统计对齐的非配对数据可有效替代昂贵的图像-文本对。

Conclusion: 该框架为多模态大语言模型的高效扩展提供了稳健路径，通过精确建模模态间隙几何形状并利用大规模非配对数据，解决了模态对齐的挑战。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [12] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: FCL是一种避免熵最小化的测试时自适应框架，通过公平性约束解决共享证据偏差问题，提升CLIP等视觉语言模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的TTA方法依赖熵最小化，但在类别共享视觉特征时可能放大虚假相关性并导致过度自信的错误。需要一种避免熵最小化、能解决共享证据偏差的自适应方法。

Method: 提出公平上下文学习(FCL)：1) 基于增强的探索识别可能的类别候选；2) 公平驱动的校准，通过公平性约束调整文本上下文，使对共同视觉证据的敏感性均等化。

Result: FCL在多种领域偏移和细粒度基准测试中，相对于最先进的TTA方法实现了具有竞争力的自适应性能，并实证验证了理论动机。

Conclusion: FCL通过避免熵最小化和引入公平性约束，有效缓解了部分特征痴迷问题，能够在不需要熵减少的情况下有效校准文本嵌入，提升模型在分布偏移下的鲁棒性。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [13] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: ANFIS增强的CNN在对抗攻击下的鲁棒性表现不一致：ResNet18-ANFIS有所改善，VGG-ANFIS则表现更差，表明神经模糊增强并非普遍有效


<details>
  <summary>Details</summary>
Motivation: 传统CNN缺乏可解释性且易受对抗攻击，神经模糊混合模型如DCNFIS通过ANFIS替代全连接层来提升可解释性，但其鲁棒性尚未充分研究

Method: 比较标准CNN（ConvNet、VGG、ResNet18）与其ANFIS增强版本在MNIST、Fashion-MNIST、CIFAR-10和CIFAR-100数据集上的表现，使用基于梯度的PGD攻击和无梯度的Square攻击进行鲁棒性评估

Result: ANFIS集成并未一致提升干净准确率，且对鲁棒性的影响具有架构依赖性：ResNet18-ANFIS表现出改进的对抗鲁棒性，而VGG-ANFIS通常表现不如其基线模型

Conclusion: 神经模糊增强可以在特定架构中提升鲁棒性，但并非普遍有益，需要针对具体架构进行仔细评估

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [14] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

TL;DR: UNIKIE-BENCH是一个用于评估大型多模态模型在文档关键信息提取任务中性能的统一基准，包含约束类别和开放类别两个评测轨道，揭示了现有模型在多样化模式定义、长尾关键字段和复杂布局下的性能挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界文档的关键信息提取面临布局结构、视觉质量和任务特定信息需求的巨大差异，而现有大型多模态模型在这方面的能力缺乏全面系统的评估。

Method: 提出了UNIKIE-BENCH统一基准，包含两个互补轨道：1)约束类别KIE轨道，基于场景预定义模式反映实际应用需求；2)开放类别KIE轨道，提取文档中明确存在的任何关键信息。

Result: 对15个最先进的大型多模态模型进行评估，发现在多样化模式定义、长尾关键字段和复杂布局下性能显著下降，不同文档类型和场景间存在明显性能差异。

Conclusion: 研究揭示了基于大型多模态模型的关键信息提取在基础准确性和布局感知推理方面仍面临持续挑战，为未来研究提供了评估基准和方向。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [15] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: OMNI-Dent是一个数据高效、可解释的牙科诊断框架，通过将临床推理原则融入视觉语言模型，利用智能手机多视角照片进行牙齿级评估，无需牙科特定微调。


<details>
  <summary>Details</summary>
Motivation: 当前AI牙科诊断方法主要将诊断视为视觉模式识别任务，未能反映牙科专业人员的结构化临床推理，且需要大量专家标注数据，在多样化真实世界成像条件下泛化能力有限。

Method: 提出OMNI-Dent框架，将临床推理启发式方法嵌入视觉语言模型（VLM）流程，利用多视角智能手机照片，引导通用VLM进行牙齿级评估，无需牙科特定微调。

Result: 该框架旨在支持在缺乏临床影像数据的场景中进行诊断评估，作为早期辅助工具帮助用户识别潜在异常并判断是否需要专业评估。

Conclusion: OMNI-Dent为缺乏面对面护理机会的个人提供了实用选择，通过数据高效、可解释的方法将临床推理融入AI诊断流程。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [16] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

TL;DR: COMBOOD是一个用于图像识别中OOD检测的无监督半参数框架，通过结合最近邻和马氏距离两种信号，在近OOD和远OOD场景下都能提供准确的置信度评分。


<details>
  <summary>Details</summary>
Motivation: 在机器学习应用中，推理时识别分布外数据对自动化应用至关重要。现有方法在近OOD场景（实际应用中常见）中表现不佳，需要一种能同时处理近OOD和远OOD场景的鲁棒方法。

Method: 提出COMBOOD框架，结合两种距离度量信号：1）最近邻距离（非参数方法）提供OOD检测；2）马氏距离（参数方法）在远OOD场景中特别有效。在半参数设置下融合这两种信号，为推理点生成置信度评分。

Result: 在OpenOOD（v1和v1.5）基准数据集和文档数据集上，COMBOOD在准确率方面优于最先进的OOD检测方法，对近OOD和远OOD场景都有效。在大多数基准数据集上，准确率提升具有统计显著性，且计算复杂度与嵌入空间大小呈线性关系。

Conclusion: COMBOOD是一个有效的半参数OOD检测框架，通过结合最近邻和马氏距离的优势，在近OOD和远OOD场景下都能提供准确的检测结果，具有线性计算复杂度，适合实际应用。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [17] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: PipeMFL-240K：首个大规模公开管道漏磁检测数据集与基准，包含24万张图像和19万标注，解决现有研究缺乏公平比较和可重复评估的问题。


<details>
  <summary>Details</summary>
Motivation: 管道完整性对工业安全和环境保护至关重要，漏磁检测是主要无损检测技术。尽管深度学习在自动化漏磁解释方面有前景，但由于缺乏大规模公开数据集和基准，可靠模型的进展受到限制，难以进行公平比较和可重复评估。

Method: 构建PipeMFL-240K数据集，包含240,320张图像和191,530个高质量边界框标注，收集自11条总长约1,480公里的管道。数据集反映了真实检测复杂性，具有三个独特挑战：1）12个类别的极端长尾分布；2）大量仅包含少量像素的微小物体；3）显著的类内变异性。

Result: 通过最先进的目标检测器进行广泛实验建立基准。结果显示，现代检测器仍然难以应对漏磁数据的内在特性，表明仍有很大改进空间。PipeMFL-240K为未来研究提供了可靠且具有挑战性的测试平台。

Conclusion: 作为首个公开的管道漏磁检测数据集和基准，PipeMFL-240K为高效管道诊断和维护规划提供了关键基础，有望加速基于漏磁的管道完整性评估的算法创新和可重复研究。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [18] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: VLRS-Bench是首个专门针对遥感复杂推理任务的基准测试，包含2000个问答对，涵盖认知、决策和预测三个维度，旨在推动遥感领域多模态大语言模型的发展。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基准测试主要偏向感知任务（如目标识别和场景分类），这限制了多模态大语言模型在认知要求高的遥感应用中的发展，需要专门的复杂推理基准来推动该领域进步。

Method: 通过专门构建的流程创建VLRS-Bench，该流程整合了遥感特定的先验知识和专家知识，确保地理空间真实性和推理复杂性。基准包含2000个问答对，平均长度71个词，涵盖14个任务和最多八个时间阶段。

Result: 实验结果显示现有最先进的多模态大语言模型在VLRS-Bench上存在显著瓶颈，为遥感社区推进多模态推理提供了关键见解。

Conclusion: VLRS-Bench作为首个专门针对遥感复杂推理的基准测试，填补了现有基准的空白，揭示了当前模型的局限性，为遥感领域多模态推理的发展提供了重要方向。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [19] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: ShapBPT是一种基于分层Shapley值的数据感知可解释AI方法，通过将Shapley系数分配给为图像定制的多尺度层次结构（二进制分区树），使特征归因与图像形态对齐，提高计算效率和语义意义。


<details>
  <summary>Details</summary>
Motivation: 现有的分层Shapley方法未能利用图像数据的多尺度结构，导致收敛缓慢且与真实形态特征对齐不佳。同时，没有先前的Shapley方法在计算机视觉任务中利用数据感知的层次结构，导致结构化视觉数据的模型可解释性存在空白。

Method: 提出ShapBPT方法，基于分层Shapley公式，将Shapley系数分配给专门为图像设计的二进制分区树（BPT）多尺度层次结构。通过这种数据感知的层次分区，确保特征归因与内在图像形态对齐，有效优先考虑相关区域同时减少计算开销。

Result: 实验结果表明ShapBPT的有效性，展示了与图像结构的优越对齐性和相比现有XCV方法的改进效率。20名参与者的用户研究证实人类更偏好ShapBPT的解释。

Conclusion: ShapBPT将分层Shapley方法与图像数据连接起来，为视觉可解释性提供了更高效和语义更有意义的方法，填补了结构化视觉数据模型可解释性的空白。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [20] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

TL;DR: 提出ECHWR训练框架，通过临时辅助分支和双重对比目标提升基于IMU的在线手写识别性能，不增加推理成本，在OnHW-Words500数据集上显著降低字符错误率。


<details>
  <summary>Details</summary>
Motivation: 基于IMU的在线手写识别可实现纸上手写到数字设备的输入，在边缘硬件上运行可提升隐私和降低延迟，但面临内存限制。需要在不增加推理成本的情况下提升识别精度。

Method: 提出ECHWR训练框架：使用临时辅助分支在训练阶段对齐传感器信号与语义文本嵌入；采用双重对比目标：批量内对比损失用于模态对齐，新颖的基于错误的对比损失区分正确信号与合成硬负样本；训练后丢弃辅助分支，保持原始高效架构。

Result: 在OnHW-Words500数据集上，ECHWR显著优于最先进基线：在独立于书写者分割上降低字符错误率7.4%，在依赖于书写者分割上降低10.4%。基于错误的对比损失在处理未见书写风格方面表现出有效性。

Conclusion: ECHWR框架通过临时辅助分支和双重对比目标有效提升了基于IMU的手写识别性能，不增加推理成本，特别适用于边缘设备部署。基于错误的对比损失在处理未见书写风格方面具有优势。

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [21] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

TL;DR: 研究发现现代视频编码器通过分布式而非因子化表示物理变量，存在"物理涌现区"的中间层过渡，其中运动方向以高维环形几何结构编码。


<details>
  <summary>Details</summary>
Motivation: 探索视频模型是否依赖物理变量的因子化表示来做出准确物理预测，还是通过任务特定的分布式方式隐式表示这些变量，并研究大型视频编码器内部的物理表征机制。

Method: 使用分层探测、子空间几何分析、补丁级解码和针对性注意力消融等方法，研究基于编码器的视频变换器中的物理信息可访问性和组织方式。

Result: 发现所有架构中都存在一个尖锐的中间深度过渡区（物理涌现区），物理变量在此变得可访问；速度、加速度等标量从早期层就可获得，而运动方向仅在物理涌现区变得可访问，且通过具有环形几何结构的高维群体结构编码。

Conclusion: 现代视频模型不使用经典物理引擎那样的物理变量因子化表示，而是使用分布式表示，这种表示足以进行物理预测。

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [22] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: Neural Sentinel：基于视觉语言模型的统一车牌识别系统，单次前向传播完成车牌识别、状态分类和车辆属性提取，准确率92.3%，比传统OCR方法提升14.1%，支持零样本泛化到其他车辆检测任务。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR系统采用多阶段流水线（目标检测+OCR模块），存在误差累积、延迟增加和架构复杂的问题。需要一种更统一、高效且能处理多任务的解决方案。

Method: 1. 使用PaliGemma 3B视觉语言模型，通过LoRA微调进行适配；2. 单次前向传播同时回答多个关于车辆图像的视觉问题；3. 引入人机协同持续学习框架，保持70:30的原始训练数据与修正样本比例，防止灾难性遗忘；4. 支持零样本泛化到辅助任务。

Result: 1. 车牌识别准确率92.3%，比EasyOCR提升14.1%，比PaddleOCR提升9.9%；2. 平均推理延迟152ms；3. 预期校准误差0.048，置信度估计良好；4. 零样本泛化能力：车辆颜色检测89%，安全带检测82%，乘员计数78%；5. 在真实收费站图像上验证有效性。

Conclusion: 统一的视觉语言方法代表了ALPR系统的范式转变，提供了优于传统流水线方法的准确性、降低的架构复杂性以及新兴的多任务能力，为智能交通系统提供了更高效的解决方案。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [23] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

TL;DR: 提出并评估了无标记神经导航方法，使用低成本可见光和红外光相机结合面部几何建模，替代传统依赖标记的系统，验证显示精度足够用于经颅磁刺激。


<details>
  <summary>Details</summary>
Motivation: 传统神经导航系统依赖主体安装的标记，需要手动配准，可能在操作过程中移位，并引起不适。这些系统昂贵且复杂，限制了神经导航在临床和研究环境中的可及性。

Method: 引入无标记方法，使用低成本可见光和红外光相机（结合立体视觉和深度感知），通过算法对面部几何进行建模，替代昂贵的硬件和物理标记。

Result: 在50名人类受试者验证中，最佳无标记算法的中位跟踪误差仅为2.32毫米和2.01度，相比传统标记系统具有足够精度用于经颅磁刺激，且显著优于先前无标记结果。

Conclusion: 提出的无标记神经导航方法可以降低设置成本和复杂性，提高患者舒适度，并扩大神经导航在临床和研究环境中的可及性。不同相机传感器的数据融合可进一步提高整体精度。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [24] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

TL;DR: RECITYGEN是一个结合潜在扩散模型和交互式语义分割的工具，允许用户通过文本提示交互式生成城市环境的变体街景图像，用于参与式城市设计。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下的城市设计方法往往忽视公众意见，导致设计愿景与现实之间存在差距。数字工具如城市信息建模和增强现实已经使更多利益相关者参与城市设计成为可能，而深度学习和潜在扩散模型进一步降低了设计生成的门槛，为参与式城市设计提供了更多机会。

Method: 结合最先进的潜在扩散模型与交互式语义分割技术，开发了RECITYGEN工具，允许用户通过文本提示交互式创建城市环境的变体街景图像。

Result: 在北京的一个试点项目中，用户使用RECITYGEN为正在进行的城市更新项目提出改进建议。尽管存在一些局限性，但RECITYGEN显示出与公众偏好高度一致的显著潜力。

Conclusion: RECITYGEN表明城市设计方法正在向更加动态和包容的方向转变，为参与式城市规划提供了新的可能性。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [25] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: FADE是一种用于文本到图像扩散模型的快速两阶段遗忘方法，通过参数定位和自蒸馏实现高效概念擦除，同时保持模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练模型中移除特定数据或概念的影响，以满足数据保护法规和负责任AI实践的需求。当前文本到图像扩散模型的遗忘方法面临高计算成本和难以平衡有效遗忘与保留无关概念的挑战。

Method: FADE采用两阶段方法：第一阶段使用基于梯度的显著性识别与遗忘集最相关的参数，并通过稀疏LoRA适配器约束更新；第二阶段应用自蒸馏目标，用用户定义的替代概念覆盖被遗忘概念，同时保留在保留数据上的行为。

Result: 在UnlearnCanvas基准测试以及Imagenette、Labeled Faces in the Wild、AtharvaTaras Dog Breeds Dataset和SUN Attributes数据集上的消融研究表明，FADE实现了最先进的遗忘性能，在遗忘-保留权衡方面具有细粒度控制。

Conclusion: FADE实现了强大的概念擦除和跨多个领域的高保留性，适配器内存效率高、可逆，可在运行时合并或移除，使其成为扩散基图像生成模型中选择性遗忘的合适解决方案。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [26] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

TL;DR: 开发了一个计算机视觉系统，通过图像分析评估废钢中的非金属夹杂物污染程度，并分类废钢类型，用于钢铁生产中的质量控制


<details>
  <summary>Details</summary>
Motivation: 目前废钢质量主要通过人工目视检查非金属夹杂物污染程度，这种方法主观性强且存在安全隐患（粉尘和移动设备）。需要客观、安全的自动化评估方法

Method: 将污染评估构建为车厢级别的回归任务，采用多实例学习（MIL）处理序列数据，结合多任务学习（MTL）同时进行污染评估和废钢分类。系统包括磁铁/车厢检测、版本化推理服务、操作员审查和主动学习循环

Result: 最佳结果：MIL方法达到MAE 0.27和R² 0.83；MTL设置达到MAE 0.36，废钢分类F1分数0.79。系统已集成到验收工作流程中，实现近实时处理

Conclusion: 该计算机视觉管道减少了主观变异性，提高了人员安全性，能够集成到废钢验收和熔炼计划工作流程中，通过主动学习实现持续改进

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [27] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Bird-SR是一个双向奖励引导的扩散框架，通过奖励反馈学习将超分辨率建模为轨迹级偏好优化，同时利用合成LR-HR对和真实世界LR图像，在保持结构一致性的同时提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的超分辨率方法能合成丰富细节，但在合成配对数据上训练的模型常因分布偏移而在真实世界低分辨率图像上失效。需要一种能同时利用合成数据和真实世界图像的方法，在保持结构保真度的同时提升感知质量。

Method: 提出Bird-SR框架：1）早期扩散步骤直接在合成对上优化以保证结构保真度；2）后期采样步骤对合成和真实图像应用质量引导奖励；3）采用相对优势空间和语义对齐约束防止奖励攻击；4）使用动态保真度-感知权重策略平衡结构保持和感知优化。

Result: 在真实世界超分辨率基准测试中，Bird-SR在感知质量方面持续优于最先进方法，同时保持了结构一致性，验证了其对真实世界超分辨率的有效性。

Conclusion: Bird-SR通过双向奖励引导的扩散框架和轨迹级偏好优化，成功解决了真实世界超分辨率中的分布偏移问题，在结构保真度和感知质量之间取得了良好平衡。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [28] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: MosaicThinker：一种用于设备端具身AI的推理时计算技术，通过整合多帧碎片化空间信息到统一语义地图中，增强小型视觉语言模型在跨帧空间推理任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 随着具身AI从传统物体检测扩展到机器人操作和动作规划等高级任务，需要从视频输入中进行视觉空间推理来感知物体空间关系并指导设备动作。然而，现有视觉语言模型由于缺乏3D空间信息知识，在空间推理方面能力很弱，特别是在涉及跨多帧复杂空间关系的推理任务上。

Method: 提出MosaicThinker技术，将多帧碎片化的空间信息整合到统一的全局语义地图表示中，然后通过视觉提示引导视觉语言模型在语义地图上进行空间推理。

Result: 实验结果表明，该技术能显著提高资源受限的具身AI设备在跨帧空间推理任务上的准确性，适用于多种类型和复杂度的推理任务。

Conclusion: MosaicThinker通过构建统一的语义地图表示和视觉提示机制，有效增强了设备端小型视觉语言模型在复杂跨帧空间推理任务上的能力，为资源受限的具身AI设备提供了实用的空间推理增强方案。

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [29] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: WorldEdit是一个专门为世界知识驱动的图像编辑设计的数据集，通过两阶段训练框架和因果验证奖励机制，显著提升了模型在隐式编辑指令下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型在处理显式指令（如属性操作、风格转换）时表现出色，但在处理隐式编辑指令（描述视觉变化原因而非具体结果）时面临挑战，因为它们缺乏处理复杂世界知识和推理的能力。

Method: 提出了WorldEdit数据集，包含高质量编辑样本和符合现实世界因果逻辑的改写指令；使用两阶段训练框架微调Bagel等模型，并整合因果验证奖励机制。

Result: 该方法显著缩小了与GPT-4o和Nano-Banana的差距，在指令遵循和知识合理性方面都表现出竞争力，而这两个方面通常是开源系统的薄弱环节。

Conclusion: WorldEdit数据集和提出的方法有效解决了图像编辑模型在处理隐式指令时的局限性，通过世界知识驱动的编辑策略提升了模型的因果推理能力。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [30] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

TL;DR: TLC-Plan是一个直接合成矢量平面图的层次生成模型，通过两级VQ-VAE编码全局布局和局部几何，使用CodeTree表示和自回归transformer生成多样且拓扑有效的设计，在RPLAN和LIFULL数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在栅格空间操作并依赖后处理矢量化，导致结构不一致并阻碍端到端学习。受组合空间推理启发，需要开发符合人类建筑工作流程（基于模块化和可重用模式）的直接矢量平面图生成方法。

Method: 提出TLC-Plan层次生成模型：1）使用两级VQ-VAE编码全局布局（语义标记的房间边界框）和局部几何（多边形级编码）；2）统一为CodeTree表示；3）使用自回归transformer根据边界条件采样编码，生成多样且拓扑有效的设计，无需显式房间拓扑或维度先验。

Result: 在RPLAN数据集上达到FID=1.84、MSE=2.06的SOTA性能，在LIFULL数据集上也取得领先结果。框架支持约束感知和可扩展的矢量平面图生成，适用于实际建筑应用。

Conclusion: TLC-Plan通过直接合成矢量平面图，解决了现有栅格方法的结构不一致问题，实现了与人类建筑工作流程一致的可扩展生成，为实际建筑应用提供了先进的约束感知矢量平面图生成框架。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [31] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

TL;DR: 提出基于可重光照3D高斯泼溅的端到端强化学习框架，实现无人机在非结构化室外环境的零样本导航，通过分解场景组件和多样化光照增强训练，达到10m/s高速避障能力。


<details>
  <summary>Details</summary>
Motivation: 无人机在非结构化室外环境使用单目视觉导航面临仿真与现实间的视觉域差距问题，现有3D高斯泼溅方法将静态光照与几何耦合，限制了策略在动态真实光照下的泛化能力。

Method: 提出可重光照3D高斯泼溅技术，分解场景组件实现物理基础的光照编辑；在基于真实数据的高保真仿真中训练端到端强化学习策略，通过多样化合成光照条件增强训练，使策略学习光照不变的视觉特征。

Result: 真实世界实验显示，轻量级四旋翼无人机在复杂森林环境中达到10m/s的鲁棒无碰撞导航，对剧烈光照变化表现出显著适应能力，无需微调。

Conclusion: 该方法通过解耦光照与几何表示，结合多样化光照增强训练，实现了无人机在非结构化室外环境的零样本鲁棒导航，解决了仿真到现实的光照域差距问题。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [32] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: Ex-Omni是一个开源的全模态框架，通过解耦语义推理与时间生成，利用语音单元作为时间支架和统一的token-as-query门控融合机制，为全模态大语言模型添加语音伴随的3D面部动画功能。


<details>
  <summary>Details</summary>
Motivation: 尽管全模态大语言模型旨在统一多模态理解与生成，但将语音与3D面部动画结合的研究仍然很少，而这对于自然交互至关重要。主要挑战在于LLMs的离散、token级语义推理与3D面部运动所需的密集、细粒度时间动态之间的表示不匹配。

Method: 提出Ex-Omni框架，通过解耦语义推理与时间生成来降低学习难度：1) 使用语音单元作为时间支架；2) 采用统一的token-as-query门控融合机制进行受控语义注入；3) 引入InstructEx数据集来支持训练。

Result: 大量实验表明，Ex-Omni在性能上与现有开源全模态大语言模型相当，同时能够稳定生成对齐的语音和面部动画。

Conclusion: Ex-Omni成功解决了将语音与3D面部动画集成到全模态大语言模型中的挑战，通过解耦语义推理与时间生成的策略，实现了稳定对齐的多模态生成能力。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [33] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 研究发现在LAION-400M数据集中存在大量包含敏感个人信息的孕期超声图像，这些图像可能被用于身份识别或冒充，需要改进数据集管理和隐私保护措施。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的发展，大规模互联网数据集的使用日益普遍，但往往缺乏充分的数据筛选。这引发了包含敏感或隐私信息的担忧，特别是孕期超声图像这类包含高度个人隐私信息的内容。

Method: 通过CLIP嵌入相似性对LAION-400M数据集进行系统性检查，检索包含孕期超声的图像，并检测其中的隐私信息实体（如姓名、位置等）。

Result: 研究发现数千个包含隐私信息的实体，多个图像含有高风险信息，可能被用于身份重新识别或冒充。

Conclusion: 需要改进数据集筛选实践，加强数据隐私保护，并建立公共图像数据集的伦理使用规范。

Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [34] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

TL;DR: DuMeta++是一个无需配对纵向数据的双元学习框架，用于解决大脑MRI分割在人类生命周期中的跨年龄泛化问题，通过元特征学习和元初始化学习实现数据高效适应。


<details>
  <summary>Details</summary>
Motivation: 大脑MRI分割在神经科学和临床应用中至关重要，但由于大脑外观和形态随年龄的动态变化，实现跨生命周期的稳定性能具有挑战性。现有方法通常依赖配对纵向数据进行自监督正则化，但这类数据在实践中往往难以获得。

Method: 提出DuMeta++双元学习框架：1) 元特征学习提取年龄无关的时空演化脑结构语义表示；2) 元初始化学习实现分割模型的数据高效适应；3) 基于记忆库的类感知正则化策略，在没有显式纵向监督的情况下强制纵向一致性。理论上证明了算法的收敛性。

Result: 在多个数据集(iSeg-2019, IBIS, OASIS, ADNI)上的少样本实验表明，DuMeta++在跨年龄泛化方面优于现有方法。

Conclusion: DuMeta++成功解决了无需配对纵向数据的大脑MRI跨年龄分割问题，通过双元学习和类感知正则化实现了优异的泛化性能，为实际临床应用提供了可行方案。

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [35] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出使用视角不变语义特征作为条件输入，解决传统全头3D GAN因使用视角角度作为条件导致的生成偏差问题，提升生成质量和全局一致性。


<details>
  <summary>Details</summary>
Motivation: 传统全头3D GAN使用视角角度作为条件输入，导致学习到的3D头部空间沿条件视角方向存在偏差，造成条件视角与非条件视角之间的生成质量和多样性显著差异，不同头部区域缺乏全局一致性。

Method: 提出使用视角不变语义特征作为条件输入，通过FLUX.1 Kontext扩展现有高质量正面人脸数据集到多视角，提取正面视角的图像clip特征作为所有视角的共享语义条件，确保语义对齐并消除方向偏差。

Result: 在全头合成和单视角GAN反转实验中，该方法在保真度、多样性和泛化能力方面均取得显著提升，加速训练并增强生成3D头部的全局一致性。

Conclusion: 使用视角不变语义条件能够解耦3D头部的生成能力与视角方向，有效解决传统方法中的视角偏差问题，促进持续学习和多样化生成。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [36] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

TL;DR: RoadSafe365是一个大规模视觉语言基准，用于细粒度交通安全性分析，包含36,196个标注视频片段和864K候选选项，基于官方安全标准构建分层分类体系。


<details>
  <summary>Details</summary>
Motivation: 现有交通基准缺乏与官方安全标准的系统性对齐评估，主要关注粗略的事故识别，需要更精细的交通安全性分析基准来连接官方标准与数据驱动的交通理解系统。

Method: 构建大规模视觉语言基准，使用分层分类法细化和扩展碰撞、事件、违规的基础定义，从行车记录仪和监控摄像头收集多样化真实世界视频数据，提供丰富的属性标注和多选题问答集。

Result: 创建了包含36,196个标注视频片段的基准，配有864K候选选项、8.4K唯一答案和36K详细场景描述，建立强基线并在微调时观察到一致性能提升，跨域实验验证了其有效性。

Conclusion: RoadSafe365为大规模训练和标准化评估提供了全面基准，能够推进真实世界交通安全性分析的可重复研究，填补了现有基准与官方安全标准之间的空白。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [37] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: AdvSR是一种将对抗性行为嵌入超分辨率模型权重的攻击框架，无需推理时访问输入，能在保持图像质量的同时诱导下游分类器误判。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的超分辨率模型常作为预处理步骤集成到成像管道中，但这些模型引入了一个先前未探索的攻击面。研究者旨在展示对抗性行为可以直接嵌入SR模型权重中，无需在推理时访问输入。

Method: AdvSR框架通过联合优化重建质量和目标对抗结果，在训练期间将对抗行为直接嵌入SR模型权重。该方法完全在模型层面操作，不依赖于输入扰动或后门触发器。评估了三种SR架构（SRCNN、EDSR、SwinIR）与YOLOv11分类器的组合。

Result: AdvSR模型在标准图像质量指标下看起来良性，但能诱导下游误分类，实现高攻击成功率且质量退化最小。这揭示了成像管道中新的模型级威胁。

Conclusion: AdvSR框架展示了将对抗行为嵌入SR模型权重的可行性，这对安全关键应用中如何获取和验证模型具有重要影响，揭示了成像管道中先前未探索的攻击面。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [38] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 3D-TBM是一个基于最优传输的3D医学图像形态分析工具，通过可逆变换将图像嵌入传输域进行分析，并将结果投影回原始图像空间进行空间解释。


<details>
  <summary>Details</summary>
Motivation: 促进基于传输的形态测量学（TBM）在临床影像研究中的更广泛应用，为研究人员提供一个完整的3D医学图像形态分析框架。

Method: 开发3D-TBM工具，包括数据预处理、最优传输嵌入计算、主要传输方向可视化、判别方向识别等分析方法，并提供完整文档和教程。

Result: 提出了一个完整的3D-TBM框架，源代码通过PyTransKit公开，为医学影像研究提供了基于传输的形态分析工具。

Conclusion: 3D-TBM为3D医学图像的形态分析提供了一个有效的基于传输的框架，有助于促进TBM在临床影像研究中的广泛应用。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [39] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

TL;DR: TwistNet-2D是一个轻量级模块，通过局部成对通道乘积和方向性空间位移来同时编码特征共现位置和交互方式，在纹理识别任务中超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在根本性矛盾：双线性池化和Gram矩阵捕捉全局通道相关性但破坏空间结构，而自注意力通过加权聚合建模空间上下文而非显式的成对特征交互。需要一种能同时编码特征共现位置和交互方式的方法。

Method: 提出TwistNet-2D模块，核心是螺旋扭曲通道交互(STCI)：将特征图沿预定方向位移后进行逐通道元素级乘法，捕捉结构化纹理的跨位置共现模式。通过四个方向头聚合，结合学习到的通道重加权，并通过sigmoid门控残差路径注入。

Result: 在ResNet-18基础上仅增加3.5%参数和2%计算量，但在四个纹理和细粒度识别基准上一致超越了参数匹配和更大的基线模型，包括ConvNeXt、Swin Transformer和混合CNN-Transformer架构。

Conclusion: TwistNet-2D通过局部成对通道乘积和方向性空间位移，有效解决了现有方法在纹理识别中的局限性，以极小的计算代价显著提升了性能，为纹理识别提供了新的有效方法。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [40] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

TL;DR: VideoNeuMat：从视频扩散模型中提取可重用神经材质资产的两阶段流程，通过虚拟测角反射仪生成材质样本视频，再用大型重建模型重构为神经材质参数


<details>
  <summary>Details</summary>
Motivation: 为3D渲染创建逼真材质需要高超的艺术技能，而现有生成模型受限于高质量训练数据的缺乏。视频生成模型能产生逼真的材质外观，但这些知识与几何和光照纠缠在一起，难以直接利用

Method: 两阶段流程：1）微调大型视频模型（Wan 2.1 14B）在受控相机和光照轨迹下生成材质样本视频，创建"虚拟测角反射仪"；2）通过基于Wan 1.3B视频骨干微调的大型重建模型（LRM），从17个生成视频帧中单次推理预测神经材质参数

Result: 生成的材质在真实感和多样性上远超有限的合成训练数据，能够泛化到新的视角和光照条件，成功将互联网规模视频模型中的材质知识转移到独立的、可重用的神经3D资产中

Conclusion: VideoNeuMat证明可以从视频扩散模型中成功提取材质知识，创建出逼真且多样化的可重用神经材质资产，为3D渲染材质生成提供了新途径

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [41] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: XVWM通过跨视角预测目标训练世界模型，使智能体能够在不同视角下规划，利用多视角一致性作为几何正则化学习环境3D结构


<details>
  <summary>Details</summary>
Motivation: 现有世界模型通常仅从单一视角（通常是自我中心视角）操作，即使其他视角（如鸟瞰视角）可能使规划更容易。导航等任务尤其受益于多视角规划能力。

Method: 提出跨视角世界模型（XVWM），采用跨视角预测目标：给定一个视角的帧序列，预测执行动作后相同或不同视角的未来状态。使用Aimlabs平台提供的同步多视角游戏数据进行训练，该数据包含精确对齐的多摄像头记录和高频动作标签。

Result: 模型为智能体提供跨视角的并行想象流，使其能够在最适合任务的参考系中规划，同时从自我中心视角执行。多视角一致性为空间基础表示提供了强大的学习信号。

Conclusion: 跨视角世界模型通过强制跨视角一致性作为几何正则化，学习环境3D结构的视角不变表示。从他人视角预测自身行动后果的能力为多智能体环境中的视角采择奠定了基础。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [42] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

TL;DR: 该研究提出了一种基于注意力机制的DeepLab-V3+模型，用于糖尿病视网膜病变（DR）相关病变的像素级分割，在DDR数据集上实现了性能提升。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是一种可能导致视力丧失和失明的眼病，早期检测至关重要。虽然已有许多基于深度学习的自动筛查算法，但在病变分割方面的临床应用仍有限制。需要提供像素级注释来支持眼科医生进行DR筛查。

Method: 在DeepLab-V3+模型中集成注意力机制，用于分割四种DR相关病变：微动脉瘤、软性渗出物、硬性渗出物和出血。使用DDR数据集的757张图像进行训练和评估。

Result: Attention-DeepLab模型相比基线模型，平均精度（mAP）从0.3010提升到0.3326，平均交并比（IoU）从0.1791提升到0.1928。微动脉瘤检测从0.0205显著提升到0.0763，这在临床上具有重要意义，因为微动脉瘤是DR最早可见的症状。

Conclusion: 集成注意力机制的DeepLab-V3+模型能够有效提升DR病变分割性能，特别是在微动脉瘤检测方面取得了临床显著的改进，有助于支持眼科医生进行更准确的DR筛查和早期诊断。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [43] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 该研究开发了一种基于线性遗传编程的过滤和分割算法，用于自动检测增材制造铌基铜合金显微图像中的析出物，替代了传统的手动标注方法。


<details>
  <summary>Details</summary>
Motivation: 当前增材制造铌基铜合金的分析依赖手动标注显微图像中的析出物，但由于图像对比度变化、噪声和伪影等问题，这一过程耗时且限制了合金开发迭代速度。

Method: 采用线性遗传编程优化图像处理算法，使用特定领域语言构建图像过滤块序列，通过遗传算法迭代优化参数，最终生成可解释的MATLAB代码表示的图像处理流程。

Result: 在理想条件下（种群大小60，最大程序长度5个块），系统找到了接近人工精度的解决方案，平均评估误差为1.8%，处理360万像素图像平均仅需2秒。

Conclusion: 该自动化方法显著加快了材料开发迭代周期，促进了材料成分和工艺空间的探索，最终有助于开发用于增材制造聚变反应堆部件的强韧、低活化、沉淀硬化铜合金。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [44] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: LUCID提出了一种统一的视觉-语言稀疏自编码器，通过学习共享的潜在字典来对齐图像块和文本标记表示，实现跨模态的可解释概念发现。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器按模态单独训练，产生的特征字典不可直接理解，且解释无法跨域迁移。需要一种统一的方法来学习可解释的跨模态表示。

Method: LUCID采用统一的视觉-语言稀疏自编码器，学习图像块和文本标记表示的共享潜在字典，同时保留模态特定的私有容量。通过最优传输匹配目标实现特征对齐，无需标注数据。

Result: LUCID产生可解释的共享特征，支持块级定位，建立跨模态神经元对应关系，增强对相似性评估中概念聚类问题的鲁棒性。共享特征捕获了超越对象的多样化语义类别，包括动作、属性和抽象概念。

Conclusion: LUCID提供了一种全面的可解释多模态表示方法，通过统一的稀疏编码实现跨模态概念发现和解释，为理解多模态表示提供了新途径。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [45] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

TL;DR: 提出了一种结合改进U-Net和行列分离注意力模块的低光照图像/视频增强方法，通过全局信息引导局部信息，同时保持时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统U-Net结构在低光照增强中缺乏全局信息指导，导致局部噪声大且细节丢失；注意力机制能更好利用全局信息但参数和计算量大。

Method: 1. 改进U-Net结构；2. 提出行列分离注意力模块(RCSA)，利用特征图行列的均值和最大值作为输入，以较少参数实现全局信息引导局部信息；3. 提出两种时间损失函数用于视频增强的时间一致性。

Result: 在LOL、MIT Adobe FiveK图像数据集和SDSD视频数据集上的大量实验证明了该方法的有效性。

Conclusion: 提出的URCSA方法通过行列分离注意力模块有效解决了低光照增强中全局信息指导不足的问题，同时减少了参数和计算量，在图像和视频增强任务中都表现出色。

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [46] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

TL;DR: 提出了一种透视感知的对数深度融合方法，从单视角相机获取的深度和法线图重建3D表面，通过显式考虑透视投影提高度量精度，并能利用法线信息填补深度缺失区域。


<details>
  <summary>Details</summary>
Motivation: 现有的基于梯度的深度-法线融合方法主要针对正交投影，而实际传感器系统（如结构光扫描、光度立体）通常使用透视相机，需要处理透视投影效应以获得度量准确的3D重建。

Method: 提出透视感知的对数深度融合方法，扩展现有正交梯度基深度-法线融合方法，显式考虑透视投影；利用可用的表面法线信息填补深度测量缺失区域。

Result: 在DiLiGenT-MV数据集上的实验证明了该方法的有效性，并突出了透视感知深度-法线融合的重要性。

Conclusion: 该方法能够从单视角相机获取的深度和法线图实现度量准确的3D重建，通过透视感知融合和缺失数据填补提高了重建质量。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [47] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

TL;DR: PTB-XL-Image-17K是一个包含17,271个高质量12导联心电图图像的合成数据集，为心电图数字化研究提供首个大规模资源，支持完整的处理流程评估。


<details>
  <summary>Details</summary>
Motivation: 心电图数字化（将纸质或扫描的心电图图像转换回时间序列信号）对于利用数十年的临床数据至关重要，但缺乏大规模同时包含心电图图像和对应真实信号的数据集阻碍了研究进展。

Method: 基于PTB-XL信号数据库，开发了一个开源Python框架，生成包含五种互补数据类型的合成心电图图像：真实的心电图图像、像素级分割掩码、真实时间序列信号、YOLO格式的边界框标注以及全面的元数据。

Result: 成功生成了17,271个高质量12导联心电图图像，生成成功率达100%，平均处理时间为每样本1.35秒。数据集提供了可控制参数的自定义生成功能。

Conclusion: PTB-XL-Image-17K填补了心电图数字化研究的关键空白，为首个支持完整处理流程（导联检测、波形分割和信号提取）的大规模资源，为严格评估提供了完整真实数据。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [48] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: SoulX-FlashHead是一个1.3B参数的实时音频驱动肖像生成框架，通过流式感知时空预训练和Oracle引导双向蒸馏技术，在保持高视觉质量的同时实现96FPS的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动肖像生成模型面临计算成本高与视觉质量/时间稳定性之间的权衡问题。大型模型计算成本过高，轻量级模型则牺牲面部表示完整性和时间稳定性，难以实现实时高保真流式视频生成。

Method: 1. 提出1.3B参数的统一框架SoulX-FlashHead；2. 引入流式感知时空预训练，配备时间音频上下文缓存机制，从短音频片段中提取稳健特征；3. 提出Oracle引导双向蒸馏，利用真实运动先验提供精确物理指导，缓解长序列自回归生成的误差累积和身份漂移；4. 构建VividHead数据集（782小时严格对齐素材）支持训练。

Result: 在HDTF和VFHQ基准测试中达到最先进性能；Lite变体在单张NVIDIA RTX 4090上实现96FPS推理速度，支持超快速交互而不牺牲视觉连贯性。

Conclusion: SoulX-FlashHead成功解决了音频驱动肖像生成中高保真视觉质量与低延迟流式传输之间的平衡问题，通过创新的流式感知训练和蒸馏技术，实现了实时、无限长度、高保真的流式视频生成。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [49] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

TL;DR: SpatialReward：通过空间推理解决在线强化学习图像编辑中的奖励信号问题，提升评估准确性和编辑效果


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在复杂图像编辑中很有前景，但面临可靠细粒度奖励信号稀缺的问题。现有评估器存在"注意力崩溃"问题，即模型忽视跨图像比较和细粒度细节，导致感知不准确和分数校准错误。

Method: 提出SpatialReward奖励模型，通过显式空间推理实现精确验证。通过将推理锚定到预测的编辑区域，将语义判断基于像素级证据。在精心策划的260k空间感知数据集上进行训练。

Result: 在MMRB2和EditReward-Bench上达到最先进性能，在提出的MultiEditReward-Bench上优于专有评估器。在在线RL中作为强大信号，将OmniGen2在GEdit-Bench上提升+0.90，超过领先的判别模型，是GPT-4.1增益的两倍。

Conclusion: 空间推理对于解锁图像编辑中的有效对齐至关重要，SpatialReward通过像素级证据和空间感知显著提升了评估准确性和编辑效果。

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [50] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

TL;DR: 本文介绍了GlobalWasteData（GWD）数据集，这是一个包含89,807张图像、涵盖14个主要类别和68个子类的大规模废物分类数据集，旨在解决现有公开废物数据集碎片化、不一致和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 废物增长需要高效的自动分类系统，但现有公开废物数据集存在碎片化、不一致、标注格式不统一、图像条件差异大、类别分布不平衡等问题，导致难以训练出泛化能力强的模型。

Method: 通过整合多个公开数据集创建统一的GWD数据集，进行质量过滤、重复移除和元数据生成等预处理，提供一致的标注、改进的领域多样性和更平衡的类别表示。

Result: 构建了包含89,807张图像、14个主要类别和68个子类的GWD数据集，具有一致的标注、更好的领域多样性和更平衡的类别分布，为废物识别模型开发提供了可靠基础。

Conclusion: GWD数据集为环境监测、回收自动化和废物识别等机器学习应用提供了坚实基础，公开可用以促进未来研究和可重复性，有助于开发鲁棒且泛化能力强的废物识别模型。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [51] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

TL;DR: TOM-GS：首个基于高斯泼溅的SLAM系统，专为热成像相机设计，结合学习式里程计与密集建图，在恶劣条件下实现鲁棒的运动估计和高质量重建。


<details>
  <summary>Details</summary>
Motivation: 热红外传感器能在黑暗、灰尘和烟雾等恶劣条件下稳定成像，但现有热成像里程计和建图方法主要是几何方法，在不同数据集上表现不稳定且无法生成密集地图。受高斯泼溅技术高效高质量重建能力的启发，需要开发专门针对热成像相机的SLAM系统。

Method: 提出TOM-GS方法，将学习式里程计与基于高斯泼溅的密集建图相结合。系统包含专门的热图像增强模块和单目深度集成模块，是首个针对热成像相机的高斯泼溅SLAM系统。

Result: 在运动估计和新视角渲染方面的广泛实验表明，TOM-GS优于现有的学习式方法，验证了学习式流水线在鲁棒热成像里程计和密集重建方面的优势。

Conclusion: TOM-GS成功展示了将学习式里程计与高斯泼溅建图相结合的有效性，为热成像相机在恶劣条件下的SLAM应用提供了鲁棒且高质量的解决方案。

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [52] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的脑信号-图像对齐策略，通过多尺度视觉编码器和对比学习实现脑信号与视觉表征的有效对齐，并引入融合先验增强跨模态分布一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉解码方法大多关注高层语义特征而忽略像素级细节，限制了我们对人类视觉系统的理解。脑信号是否真正编码视觉信息仍不清楚，需要更好的脑-图像对齐策略。

Method: 1) 利用多个具有不同归纳偏好的预训练视觉编码器捕捉层次化、多尺度的视觉表征；2) 采用对比学习目标实现脑信号与视觉嵌入的有效对齐；3) 引入融合先验，在大规模视觉数据上学习稳定映射，然后将脑特征匹配到这个预训练先验中。

Result: 广泛的定量和定性实验表明，该方法在检索准确性和重建保真度之间实现了良好的平衡。

Conclusion: 提出的脑-图像对齐策略能够有效解码脑信号中的视觉信息，通过多尺度表征和融合先验实现了更好的跨模态对齐，为理解人类视觉系统提供了新视角。

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [53] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的隐式运动表示方法，将每帧运动压缩为紧凑的1D运动标记，解决了现有显式方法的空间不匹配问题和隐式方法的身份泄漏问题，实现了更好的角色动画效果。


<details>
  <summary>Details</summary>
Motivation: 现有的角色动画方法存在明显缺陷：显式方法（如骨架、DWPose等）难以处理空间不匹配和身体比例变化；隐式方法虽然能捕捉高级运动语义，但存在身份信息泄漏和运动与外观纠缠的问题。

Method: 1. 提出新颖的隐式运动表示，将每帧运动压缩为紧凑的1D运动标记；2. 设计基于时间一致掩码标记的重定向模块，通过时间训练瓶颈减少源图像运动的干扰；3. 采用三阶段训练策略提高训练效率和保真度。

Result: 大量实验表明，提出的隐式运动表示和IM-Animation生成能力在性能上优于或与最先进方法相当，能够有效解决空间不匹配和身份泄漏问题。

Conclusion: 本文提出的隐式运动表示方法通过1D运动标记和掩码标记重定向模块，成功解决了现有角色动画方法的局限性，实现了高质量、一致性的角色动画生成。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [54] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

TL;DR: 本文提出ZoomDet框架，通过自适应放大无人机图像中的小目标来提升检测性能，采用轻量级偏移预测和非均匀放大方法，在多个无人机数据集上显著提升mAP。


<details>
  <summary>Details</summary>
Motivation: 无人机拍摄的图像中目标通常较小且稀疏，这阻碍了有效目标检测器的优化。为了解决小目标检测的挑战，需要自适应地放大目标区域以更好地捕捉目标特征。

Method: 提出自适应放大框架ZoomDet，包含两个核心设计：1) 轻量级偏移预测方案结合基于框的放大目标，学习对输入图像进行非均匀放大；2) 角点对齐的边界框变换方法，将真实框变换到放大空间进行训练，在推理时将预测框变换回原始空间。

Result: 在VisDrone、UAVDT和SeaDronesSee三个无人机目标检测数据集上进行实验。在SeaDronesSee数据集上，ZoomDet使Faster R-CNN模型的mAP提升超过8.4个绝对百分点，仅增加约3ms延迟。该方法与架构无关，可应用于任意目标检测架构。

Conclusion: ZoomDet是一个简单高效的框架，通过自适应放大无人机图像中的小目标显著提升了检测性能，具有架构无关性和低延迟特点，为解决无人机图像小目标检测问题提供了有效方案。

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [55] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 论文提出了一种新的面向对象中心学习（OCL）模型的评估框架，使用指令调优的视觉语言模型作为评估器，并引入统一的评估任务和指标来同时评估定位能力和表示有用性。


<details>
  <summary>Details</summary>
Motivation: 现有OCL模型的评估存在两个主要问题：1）现有基准测试对OCL模型表示有用性的洞察有限；2）定位能力和表示有用性使用分离的指标进行评估，导致评估不一致。

Method: 1）使用指令调优的视觉语言模型作为评估器，在多样化的VQA数据集上进行可扩展的基准测试；2）引入统一的评估任务和指标，同时评估定位（where）和表示有用性（what）；3）包含一个简单的多特征重建基线作为参考点。

Result: 该方法能够更全面地评估OCL模型的表示有用性，消除分离评估带来的不一致性，并通过多特征重建基线提供参考基准。

Conclusion: 论文提出了一个更全面、统一的OCL模型评估框架，解决了现有评估方法的局限性，为评估OCL模型在复杂推理任务中的表现提供了更好的工具。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [56] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 使用GCViT-Tiny架构在牛津-IIIT宠物数据集子集上实现猫品种分类，通过数据增强提升泛化能力，达到92.00%测试准确率和94.54%验证准确率。


<details>
  <summary>Details</summary>
Motivation: 猫品种识别具有挑战性，因为不同品种在毛色、面部结构和图案上差异细微。需要准确的方法来支持兽医诊断、动物收容所管理和移动端识别系统等应用。

Method: 采用全局上下文视觉变换器（GCViT）Tiny架构进行猫品种分类，使用牛津-IIIT宠物数据集的高分辨率图像子集，通过旋转、水平翻转和亮度调整等数据增强技术提升模型泛化能力。

Result: GCViT-Tiny模型在测试集上达到92.00%准确率，验证集上达到94.54%准确率，证明了变换器架构在细粒度图像分类任务中的有效性。

Conclusion: 基于变换器的架构在猫品种识别任务中表现出色，具有实际应用价值，包括兽医诊断、动物收容所管理和移动端识别系统，并提供了Hugging Face演示平台。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [57] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 提出双时相分析框架，使用统计描述符、影像组学纹理特征和深度特征嵌入来表征缺血组织，通过特征空间分析揭示卒中演变的组织表型


<details>
  <summary>Details</summary>
Motivation: 单时间点分割无法捕捉卒中的生物学异质性和时间演变，需要开发能够表征组织状态转变的分析方法

Method: 使用双时相（入院T1和随访T2）CT灌注和DWI数据，提取统计描述符、GLCM纹理特征和两种深度架构（mJ-Net和nnU-Net）的特征嵌入，构建6个ROI区域，在特征空间中进行聚类分析

Result: 在18名成功再灌注患者中，区域级表征显示出有意义的聚类模式：最终恢复的缺血半暗带区域特征与保留脑组织相似，而梗死区域形成明显分组；深度特征空间（特别是mJ-Net）能显著区分可挽救与不可挽救组织

Conclusion: 编码器衍生的特征流形反映了基础组织表型和状态转变，为基于影像的卒中演变量化提供了新见解

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [58] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: LGDEA提出了一种基于LLM引导的诊断证据对齐方法，通过证据级对齐而非全局或局部对齐，减少对配对数据的依赖，在医学视觉-语言预训练中取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言预训练方法存在两个主要问题：全局对齐容易被非诊断信息主导，而局部对齐无法整合关键诊断证据，导致难以学习可靠的诊断表征，限制了在配对数据有限场景下的应用。

Method: 提出LLM引导的诊断证据对齐方法（LGDEA），利用大语言模型从放射学报告中提取关键诊断证据，构建共享的诊断证据空间，实现证据感知的跨模态对齐，有效利用大量未配对的医学图像和报告。

Result: 实验结果表明，该方法在短语定位、图像-文本检索和零样本分类任务上取得一致且显著的改进，甚至能与依赖大量配对数据的预训练方法相媲美。

Conclusion: LGDEA通过证据级对齐更符合医学诊断过程，显著减轻了对配对数据的依赖，为医学视觉-语言预训练提供了更有效的解决方案。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [59] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: MUFASA是一个轻量级即插即用框架，通过在多层ViT特征上计算slot attention来提升无监督物体分割性能


<details>
  <summary>Details</summary>
Motivation: 当前基于slot attention的无监督物体中心学习方法仅使用预训练ViT的最后一层特征，忽略了其他层中丰富的语义信息

Method: 提出MUFASA框架，在ViT编码器的多个特征层上计算slot attention，并提出融合策略将多层获得的slot聚合成统一的物体中心表示

Result: 将MUFASA集成到现有OCL方法中，在多个数据集上提升了分割结果，达到新的SOTA，同时改善了训练收敛性，仅带来轻微推理开销

Conclusion: 通过充分利用ViT多层语义信息，MUFASA有效提升了无监督物体分割性能，是一个高效且通用的改进框架

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [60] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: FSSDINO：基于冻结DINOv3特征的训练免费少样本语义分割基线，仅使用类别原型和Gram矩阵优化，在多个基准测试中与复杂方法竞争，揭示了"最安全vs最优"的语义选择困境。


<details>
  <summary>Details</summary>
Motivation: 探索冻结DINOv3特征在少样本语义分割中的内在能力，建立无需训练的基线方法，分析特征层选择对性能的影响，揭示基础模型中语义选择的局限性。

Method: 提出FSSDINO方法：使用冻结的DINOv3特征，通过类别特定原型和Gram矩阵优化进行训练免费少样本分割。采用Oracle引导的层分析，比较不同特征层的性能表现。

Result: 在二元、多类别和跨域基准测试中，这种最小化方法在最终骨干层上表现优异，与复杂解码器或测试时适应方法竞争。Oracle分析显示中间层存在显著性能差距，但当前无监督和支持引导的选择指标无法可靠识别这些高性能特征。

Conclusion: "最后一层"是一个具有欺骗性的强基线，揭示了DINOv3中潜在的语义潜力。研究发现了"语义选择鸿沟"，传统启发式方法无法可靠识别高保真特征，提出了基础模型特征选择的新挑战。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [61] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: FlexID是一个无需训练的身份感知文本到图像生成框架，通过意图感知调制正交解耦身份特征，实现身份保真度和文本适应性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法依赖刚性的视觉特征注入，导致身份保真度和文本适应性之间存在冲突，需要一种更灵活的方法来平衡这两者。

Method: 提出FlexID框架，包含：1) 语义身份投影器(SIP)将高层先验注入语言空间；2) 视觉特征锚点(VFA)确保潜在空间中的结构保真度；3) 上下文感知自适应门控(CAG)机制，根据编辑意图和扩散时间步动态调制两个流的权重。

Result: 在IBench上的大量实验表明，FlexID在身份一致性和文本遵循方面实现了最先进的平衡，为复杂叙事生成提供了高效解决方案。

Conclusion: FlexID通过意图感知调制有效解决了身份保真度和文本适应性之间的冲突，实现了无需训练的高质量个性化文本到图像生成。

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [62] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

TL;DR: 该论文分析了2025年远距离人体识别竞赛，使用SUSTech-Competition步态数据集，参赛者在无专用训练数据情况下通过外部数据集训练，最佳方法达到94.2%准确率，创下新纪录。


<details>
  <summary>Details</summary>
Motivation: 远距离人体识别面临传统生物特征难以获取的挑战，步态识别提供了一种实用替代方案。国际远距离人体识别竞赛旨在推动步态识别进展并提供公平评估平台。

Method: 竞赛采用SUSTech-Competition数据集，包含服装、携带物品和视角的显著变化，不提供专用训练数据，参赛者需使用外部数据集训练模型。每年使用不同随机种子生成评估分割，减少过拟合风险并支持跨域泛化公平评估。

Result: 尽管难度增加，参赛者仍取得进一步改进，最佳方法达到94.2%准确率，在该数据集上创下新基准。论文还分析了关键技术趋势。

Conclusion: 2025年竞赛证明算法进步能够超越先前观察到的准确率限制，为步态识别领域设立了新标杆，并指出了未来研究方向。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [63] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 提出基于解耦表示学习的跨摄像头奶牛识别框架，利用子空间可识别性保证理论，通过特征解耦模块分离身份相关特征，显著提升在未见摄像头上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有动物识别方法在受控单摄像头环境下表现良好，但在跨摄像头场景中面临严重泛化挑战。当模型从源摄像头部署到具有不同光照、背景、视角和成像特性的新监控节点时，识别性能急剧下降，限制了非接触技术在动态真实农场环境中的大规模应用。

Method: 提出基于解耦表示学习的跨摄像头奶牛识别框架，利用子空间可识别性保证理论，通过建模底层物理数据生成过程，设计原则驱动的特征解耦模块，将观测图像分解为多个正交潜在子空间，有效分离跨摄像头保持不变的稳定身份相关生物特征。

Result: 构建了涵盖5个不同摄像头节点的高质量数据集，包含异构采集设备和复杂的光照角度变化。在7个跨摄像头任务上的广泛实验表明，该方法平均准确率达到86.0%，显著优于源摄像头基线（51.9%）和最强的跨摄像头基线方法（79.8%）。

Conclusion: 本研究建立了基于子空间理论的特征解耦框架，用于协作式跨摄像头奶牛识别，为不受控智能农场环境中的精确动物监测提供了新范式。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [64] [ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention](https://arxiv.org/abs/2602.07574)
*Wenjie Liu,Hao Wu,Xin Qiu,Yingqi Fan,Yihan Zhang,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CV

TL;DR: ViCA是一种高效的多模态大语言模型架构，通过稀疏跨注意力机制大幅减少视觉处理计算量，在保持98%准确率的同时将视觉侧计算降至4%，实现3.5-10倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM采用统一的self-attention设计，在每个Transformer层处理视觉和文本token，导致大量计算开销。研究发现视觉嵌入已与语言空间对齐，且有效的视觉-语言交互仅发生在少数层中。

Method: 提出ViCA架构，视觉token绕过所有self-attention和feed-forward层，仅通过选定的稀疏跨注意力层与文本交互，实现最小化的视觉处理。

Result: 在3个MLLM主干、9个多模态基准和26个剪枝基线上的评估显示，ViCA保持98%基线准确率，视觉侧计算降至4%，单批次推理加速3.5倍，多批次加速10倍以上。

Conclusion: ViCA通过稀疏跨注意力机制实现了卓越的性能-效率权衡，提供硬件友好的推理流程，视觉定位开销接近零，且可与token剪枝方法正交组合获得进一步效率提升。

Abstract: Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.

</details>


### [65] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出了一个系统化的视频生成模型后训练框架，将监督策略塑造、奖励驱动的强化学习和基于偏好的精炼整合到单一稳定性约束优化堆栈中，旨在提升生成视频的感知保真度、时间连贯性和提示遵循能力。


<details>
  <summary>Details</summary>
Motivation: 后训练是将预训练视频生成器转化为生产导向模型的关键步骤，需要解决高计算成本、时间累积故障模式以及反馈信息异构、不确定且区分度弱等实际问题。

Method: 采用分阶段、诊断驱动的优化方法，将监督策略塑造、奖励驱动的强化学习和基于偏好的精炼整合到单一稳定性约束优化堆栈中，围绕实际视频生成约束设计框架。

Result: 该框架提供了清晰的蓝图，用于构建可扩展的后训练流程，能够在保持初始化时建立的可控性的同时，提升感知保真度、时间连贯性和提示遵循能力。

Conclusion: 通过将后训练视为分阶段、诊断驱动的过程而非孤立技巧的集合，该框架为构建稳定、可扩展且在实际部署中有效的视频生成后训练管道提供了系统化解决方案。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [66] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: Fine-R1是一种专门用于细粒度视觉识别的多模态大语言模型，通过R1式训练框架，仅需4-shot训练就能超越现有通用MLLM和对比CLIP模型，在seen和unseen子类别识别上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在粗粒度视觉任务上表现良好，但在细粒度视觉识别任务上存在不足：需要大量标注数据、容易过拟合seen子类别、对unseen子类别泛化能力差。同时，与专门用于判别任务的对比CLIP模型相比存在性能差距。

Method: 采用R1式训练框架：1) 链式思维监督微调：构建高质量细粒度视觉识别CoT数据集，包含"视觉分析、候选子类别、比较、预测"的推理过程，将模型转变为强大的开放世界分类器；2) 三元组增强策略优化：类内增强混合同一类别内锚点和正样本图像的轨迹以提高对类内变化的鲁棒性，类间增强最大化跨子类别图像条件下的响应差异以增强判别能力。

Result: 仅需4-shot训练，Fine-R1在识别seen和unseen子类别上都超越了现有通用MLLM、推理MLLM甚至对比CLIP模型，在知识密集型领域显示出应用潜力。

Conclusion: Fine-R1通过创新的R1式训练框架有效解决了细粒度视觉识别的关键挑战，在少样本设置下实现了优异性能，为难以获取专家标注的知识密集型领域提供了有前景的解决方案。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [67] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出HistoMet框架，通过两阶段决策感知的多实例学习从原发肿瘤病理图像预测转移风险和转移部位，整合语言定义的概念提升临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 转移是癌症相关死亡的主要原因，但直接从组织病理学预测原发肿瘤是否会转移以及转移部位仍然是一个基本挑战。现有计算方法通常将转移状态或部位预测作为孤立任务处理，没有明确模拟临床顺序决策过程。

Method: 提出决策感知、概念对齐的MIL框架HistoMet，采用两模块预测流程：首先估计原发肿瘤转移进展的可能性，然后对高风险病例进行转移部位的条件预测。通过预训练的病理视觉语言模型整合语言定义和数据自适应的转移概念来指导表示学习。

Result: 在6504名患者的多个机构泛癌队列上评估，在95%灵敏度的高灵敏度筛查设置下，显著减少下游工作量同时保持高转移风险召回率。对于转移病例，获得74.6的宏观F1分数和92.1的宏观一对多AUC。

Conclusion: 明确模拟临床决策结构能够直接从原发肿瘤组织病理学实现稳健且可部署的转移进展和部位倾向性预后预测。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [68] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

TL;DR: 该研究通过创建UMD数据集（490个PET/CT和464个PET/MRI全身扫描）评估3D医学基础模型，发现从结构成像转向功能成像时存在显著性能差距，表明当前模型远未达到真正的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学基础模型的验证主要局限于区域性和结构性成像，存在显著的模态差异未被探索。需要提供严格客观的评估来检验这些模型在真实世界应用中的鲁棒性。

Method: 创建UMD数据集包含490个全身PET/CT和464个全身PET/MRI扫描（约675k 2D图像，约12k 3D器官标注），通过受试者内对照比较配对扫描，将成像模态作为主要自变量，对代表性3D分割基础模型进行全面评估。

Result: 评估揭示了文献报道的基准与真实世界效果之间的显著差异，特别是从结构域转向功能域时。这种系统性失败表明当前3D基础模型远未达到真正的通用目的状态。

Conclusion: 需要向多模态训练和评估的范式转变，以弥合理想化基准测试与全面临床效用之间的差距。该数据集和分析为未来开发真正模态无关的医学基础模型奠定了基石。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [69] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

TL;DR: 该研究评估了医学影像重建流程中的误差，比较了不同分割算法和几何类型在体素和表面精度指标上的表现，发现Otsu方法最适用于各种几何形状，Jaccard指数比Dice更适合薄壁结构的精度评估。


<details>
  <summary>Details</summary>
Motivation: 医学扫描创建3D模型的精度受多种因素影响，包括成像硬件、分割方法和网格处理技术等。几何类型、类别不平衡、体素和点云对齐对精度的影响尚未得到充分探索，需要系统评估重建流程中的误差。

Method: 使用SLA技术打印球体、面罩和AAA（腹主动脉瘤）模型，通过微CT扫描获取数据。采用GMM、Otsu和RG三种分割方法进行处理。将分割模型与参考模型使用KU算法对齐，定量比较Dice、Jaccard分数和精度等指标。表面网格使用ICP配准，评估Chamfer距离和平均Hausdorff距离。

Result: Otsu方法对所有几何形状都最适用。AAA由于壁薄和对齐问题导致重叠分数较低。类别不平衡对AAA的特异性影响最大。表面精度指标与体素指标趋势不同：RG方法对球体表现最好，而GMM和Otsu对AAA表现更好。面罩表面误差最大，可能是ICP配准过程中的对齐问题所致。

Conclusion: 分割精度是重建过程各阶段误差的累积总和。在高类别不平衡和对齐敏感的情况下，高体素精度指标可能具有误导性。Jaccard指数比Dice更严格，更适合薄壁结构的精度评估。为确保重建流程的可靠评估，必须确保体素和点云对齐。

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [70] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 该研究探索了视觉语言模型在自动驾驶安全评估和决策中的应用，通过三个系统级用例展示了如何利用语义表示进行危险筛查、轨迹规划和行为约束。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型能够将视觉观察与自然语言概念对齐，为安全关键自动驾驶中的语义推理提供了新机会。研究旨在探索如何将视觉语言表示集成到感知、预测和规划流程中，以支持驾驶场景安全评估和决策。

Method: 研究采用三个互补的系统级用例：1）基于CLIP图像-文本相似性的轻量级、类别无关的危险筛查方法；2）将场景级视觉语言嵌入集成到基于Transformer的轨迹规划框架中；3）使用自然语言作为运动规划的显式行为约束。

Result: 1）危险筛查方法能够低延迟检测多样化和分布外的道路危险；2）简单地将全局嵌入条件化到规划器中并不能提高轨迹精度；3）基于视觉场景元素的乘客式指令能够抑制罕见但严重的规划失败，在模糊场景中改善安全对齐行为。

Conclusion: 视觉语言表示在表达语义风险、意图和行为约束方面对自动驾驶安全具有重要潜力，但实现这一潜力本质上是一个工程问题，需要精心设计的系统架构和结构化基础，而不是简单的特征注入。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [71] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

TL;DR: PoT Reasoning for Videos：一个通过结构化推理步骤提升视频理解准确性和可解释性的框架


<details>
  <summary>Details</summary>
Motivation: 视频理解需要处理长而嘈杂的观测数据，进行时间锚定的多步推理。现有方法缺乏明确的推理过程，难以验证和诊断。

Method: 提出Process-of-Thought (PoT)框架，将视频推理分解为三个交替步骤：时间证据选择、逐步状态更新、约束答案合成。采用统一表示对齐中间决策与时间片段。

Result: 在标准视频推理任务上的实验表明，PoT能持续提升事实正确性和时间锚定能力，同时提供可解释的推理轨迹用于诊断。

Conclusion: PoT框架通过结构化推理过程，增强了视频理解的鲁棒性、可解释性和准确性，支持模型无关的集成和外部工具增强。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [72] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

TL;DR: 提出Rolling Sink方法，无需额外训练即可解决自回归视频扩散模型在超出训练时长时的视觉退化问题，实现超长视频生成


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在训练时长有限的情况下，测试时遇到超出训练时长的开放时长时会出现视觉退化问题。由于开放测试可以超越任何有限训练窗口，且长视频训练计算成本高，需要寻找无需训练的解决方案

Method: 基于Self Forcing（仅用5秒片段训练），通过系统分析自回归缓存维护机制，提出Rolling Sink方法。该方法在测试时有效扩展自回归视频合成到超长时长（5-30分钟，16 FPS）

Result: Rolling Sink在超长时长视频生成中保持一致的物体、稳定的颜色、连贯的结构和平滑的运动，相比现有最佳基线方法，在长时域视觉保真度和时间一致性方面表现更优

Conclusion: Rolling Sink提供了一种无需训练的解决方案，有效弥合了自回归视频扩散模型在训练时长和开放测试时长之间的差距，实现了高质量的超长视频生成

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [73] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

TL;DR: UCATSC是一种基于模型的交通信号控制系统，通过随机决策过程建模路口信号控制，考虑视觉感知不确定性，使用硬约束确保安全和防止饥饿，提供可解释的控制策略。


<details>
  <summary>Details</summary>
Motivation: 当前自适应交通信号控制在实际部署中受限，主要因为视觉感知的不确定性、隐含的安全性问题以及在模拟环境中学习的非可解释控制策略。需要一种能够处理不确定性、确保安全并提供可解释性的系统。

Method: UCATSC采用基于模型的方法，将交通信号控制建模为具有约束和部分可观测性的随机决策过程。系统在信念空间中进行反事实推演，预测并强制执行与安全和防止饥饿相关的硬约束，而不是通过奖励塑造来学习安全。

Result: 系统设计旨在改善交通延迟和排放，同时防止安全关键错误，并提供基于显式模型的可解释控制策略输出。

Conclusion: UCATSC通过基于模型的方法解决了自适应交通信号控制在实际部署中的关键挑战，包括感知不确定性、安全保证和可解释性，为实际应用提供了更可靠的解决方案。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [74] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 该研究首次对16种最先进的AI生成图像检测方法进行了全面的零样本评估，覆盖23个预训练检测器变体和12个数据集（260万图像样本），揭示了现有检测器在真实部署场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像在数字平台上的激增，可靠的检测方法对于打击错误信息和维护内容真实性变得至关重要。现有基准主要评估微调模型，而忽略了开箱即用性能——这是从业者最常见的部署场景，存在重要的研究空白。

Method: 对16种最先进的检测方法（23个预训练检测器变体）进行零样本评估，覆盖12个多样化数据集，包含260万图像样本，涵盖291个独特的生成器，包括现代扩散模型。使用统计分析方法评估性能差异。

Result: 研究发现：(1)不存在通用优胜者，检测器排名极不稳定；(2)最佳检测器（75.0%平均准确率）与最差检测器（37.5%）存在37个百分点的性能差距；(3)训练数据对齐对泛化能力有重要影响；(4)现代商业生成器（Flux Dev、Firefly v4、Midjourney v7）能击败大多数检测器；(5)识别出三种影响跨数据集泛化的系统性失败模式。

Conclusion: 研究结果挑战了"一刀切"的检测器范式，表明从业者必须根据具体威胁环境仔细选择检测器，而不能依赖已发布的基准性能。这为实际部署提供了可操作的指导方针。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [75] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

TL;DR: 首个大规模跨范式面部年龄估计基准测试显示，零样本视觉语言模型（VLMs）显著优于大多数专用模型，最佳VLM比最佳专用模型MAE降低15%，挑战了任务专用架构必要的假设。


<details>
  <summary>Details</summary>
Motivation: 面部年龄估计对内容审核、年龄验证和深度伪造检测至关重要，但此前没有系统比较现代视觉语言模型与专用年龄估计架构的基准测试。

Method: 提出首个大规模跨范式基准测试，评估34个模型（22个专用架构和12个通用VLMs），在8个标准数据集（总计每个模型1100张测试图像）上进行评估，分析MAE、18岁阈值年龄验证、粗粒度分箱效果和14个年龄组的分层分析。

Result: 零样本VLMs显著优于大多数专用模型，平均MAE为5.65年（专用模型为9.88年）；最佳VLM（Gemini 3 Flash Preview，MAE 4.32）比最佳专用模型（MiVOLO，MAE 5.10）提升15%；VLMs在未成年人年龄验证中假成人率仅为13-25%（专用模型为60-100%）；所有模型在极端年龄（<5岁和65+）表现最差。

Conclusion: 研究挑战了任务专用架构对年龄估计必要的假设，建议领域应转向将VLM能力蒸馏到高效专用模型中。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [76] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: OTA-Det是一个统一框架，将开放词汇航空检测(OVAD)和遥感视觉定位(RSVG)两种范式结合，支持丰富的语义理解和多目标检测，在六个基准测试上达到最先进性能，同时保持34 FPS的实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有OVAD方法仅限于粗粒度的类别级语义理解，而RSVG方法在结构上仅限于单目标定位。这两种范式各自独立运作时都存在固有局限性，无法同时支持丰富的语义理解和多目标检测。

Method: 提出OTA-Det统一框架：1）任务重构策略，统一任务目标和监督机制，实现跨数据集联合训练；2）密集语义对齐策略，建立从整体表达到个体属性的多粒度对应关系；3）基于RT-DETR架构扩展，引入高效模块，从闭集检测扩展到开放文本检测。

Result: 在六个涵盖OVAD和RSVG任务的基准测试上达到最先进性能，同时保持34 FPS的实时推理速度。

Conclusion: OTA-Det成功将OVAD和RSVG两种范式统一到一个框架中，克服了各自局限性，实现了同时支持丰富语义理解和多目标检测的能力，为航空场景理解提供了更全面的解决方案。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [77] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: 该论文提出了SPD-Faith Bench基准，用于评估多模态大语言模型推理过程的忠实性，发现了感知盲点和感知-推理分离两种系统故障模式，并提出了SAGE框架来改善视觉证据校准。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型广泛使用思维链推理来提高可解释性，但生成的推理轨迹的忠实性仍不清楚。先前工作主要关注感知幻觉，而推理层面的不忠实性尚未得到充分探索。

Method: 引入SPD-Faith Bench诊断基准，基于细粒度图像差异推理来强制显式视觉比较，以隔离语言先验对忠实性的影响。通过分析发现故障模式源于衰减的视觉注意力和残差流中的表示偏移，并提出了SAGE框架来改善视觉路由和对齐推理与感知。

Result: 评估最先进的多模态大语言模型揭示了两种系统故障模式：感知盲点和感知-推理分离。这些故障源于衰减的视觉注意力和残差流中的表示偏移。提出的SAGE框架能够改善视觉路由和对齐推理与感知。

Conclusion: 研究强调了超越响应正确性来显式评估忠实性的重要性。提出的基准和SAGE框架为理解和改善多模态大语言模型的推理忠实性提供了工具和方法。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [78] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: VFace是一种无需训练、即插即用的视频人脸交换方法，可与基于扩散模型的图像人脸交换方法无缝集成，通过频率谱注意力插值、目标结构引导和流引导注意力时序平滑技术提升视频人脸交换的时序一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频人脸交换方法通常存在时序不一致问题，需要额外的训练或视频特定的微调。为了解决这些问题，作者提出了一个无需训练、即插即用的解决方案，能够与现有的基于扩散模型的图像人脸交换方法无缝集成。

Method: VFace包含三个核心技术：1) 频率谱注意力插值技术，促进生成并保留关键身份特征；2) 目标结构引导，通过即插即用的注意力注入更好地对齐目标帧的结构特征；3) 流引导注意力时序平滑机制，在不修改底层扩散模型的情况下增强时空一致性，减少逐帧生成中的时序不一致问题。

Result: 广泛的实验表明，该方法显著提升了时序一致性和视觉保真度，为视频人脸交换提供了一个实用且模块化的解决方案。该方法无需额外训练或视频特定的微调。

Conclusion: VFace是一个无需训练、即插即用的视频人脸交换方法，能够与现有的基于扩散模型的图像人脸交换方法无缝集成，通过创新的注意力机制和时序平滑技术有效解决了视频人脸交换中的时序一致性问题，提供了高质量的实用解决方案。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [79] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: ViewRope：一种几何感知的视频Transformer编码方法，通过将相机射线方向直接注入自注意力层，解决了世界模型中长期空间一致性问题，显著减少了几何漂移和幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 当前预测性世界模型缺乏空间持久性，在长轨迹中无法保持稳定的场景结构，当相机重新访问先前观察位置时经常产生幻觉细节。这种几何漂移源于对屏幕空间位置嵌入的依赖，这与3D一致性所需的投影几何相冲突。

Method: 提出了ViewRope几何感知编码，将相机射线方向直接注入视频Transformer自注意力层；通过相对射线几何而非像素局部性参数化注意力；提出几何感知帧稀疏注意力，利用几何线索选择性关注相关历史帧；开发了ViewBench诊断套件测量闭环保真度和几何漂移。

Result: ViewRope显著改善了长期一致性，同时减少了计算成本。在ViewBench评估中表现出更好的闭环保真度和更少的几何漂移。

Conclusion: 通过几何感知的注意力机制，ViewRope为视频Transformer提供了3D一致的归纳偏置，有效解决了预测性世界模型中的空间持久性问题，为实现更可靠的交互式AI系统奠定了基础。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [80] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 提出一种从超高速运动模糊图像中恢复3D形状的新方法，通过快速重心坐标求解器实现高效可微渲染，在平移和旋转两种典型运动类型上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法（如多视角立体视觉）在处理超高速运动模糊图像时失效，而这类场景在体育（如快速移动的球体）和工业（如旋转机械）中普遍存在，需要新的解决方案。

Method: 提出一种新颖的逆渲染方法，包含快速重心坐标求解器，显著减少计算开销（加速达4.57倍），实现高效且可微分的超高速运动模拟，支持从渲染图像到3D形状的梯度传播。

Result: 实验验证了方法在快速平移和旋转两种运动类型上的有效性，能够高效真实地模拟超高速运动物体，并成功从极端平移和旋转运动的2D图像中恢复3D形状。

Conclusion: 该方法突破了基于视觉的3D重建边界，为从超高速运动模糊图像中恢复几何形状提供了有效解决方案，在自然和工业场景中具有应用潜力。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [81] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

TL;DR: SSI-Bench是一个用于评估视觉语言模型空间推理能力的基准测试，专注于受约束流形上的空间推理，包含1000个排序问题，涵盖几何和拓扑推理，需要多种空间操作能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多评估无约束场景，模型可以利用2D捷径，缺乏对真实物理世界中受几何、拓扑和物理约束的空间推理能力的评估。

Method: 通过完全人工中心的流程创建：10名研究人员花费400多小时精心挑选图像、标注结构组件、设计问题以最小化像素级线索，构建包含1000个排序问题的基准测试。

Result: 评估31个广泛使用的VLM显示与人类存在巨大差距：最佳开源模型准确率22.2%，最强闭源模型33.6%，而人类得分91.6%。鼓励模型思考仅带来边际收益，错误分析显示模型在结构基础和约束一致的3D推理方面存在失败。

Conclusion: SSI-Bench揭示了当前VLM在受约束空间推理方面的严重不足，为评估和改进模型在物理世界中的空间智能提供了重要基准。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [82] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

TL;DR: WristMIR：基于区域感知的儿科腕部X光片检索框架，利用密集放射学报告和骨骼特定定位学习细粒度图像表示，无需人工标注，显著提升骨折模式检索和诊断性能。


<details>
  <summary>Details</summary>
Motivation: 腕部X光片中骨折模式的检索具有挑战性，因为临床重要线索细微、高度局部化，常被重叠解剖结构或不同成像视角遮挡。此外，缺乏大规模、高质量标注的医学图像检索数据集也限制了进展。

Method: WristMIR采用区域感知框架：1）使用MedGemma结构化报告挖掘生成全局和区域级描述；2）处理腕部图像和骨骼特定裁剪（远端桡骨、远端尺骨、尺骨茎突）；3）联合训练全局和局部对比编码器；4）执行两阶段检索：粗粒度全局匹配候选检查，然后基于解剖骨骼区域的区域条件重排序。

Result: WristMIR显著提升检索性能：图像到文本Recall@5从0.82%提升至9.35%；嵌入表示在骨折分类上表现更强（AUROC 0.949，AUPRC 0.953）；区域感知评估中，两阶段设计将基于检索的骨折诊断平均F1从0.568提升至0.753；放射科医生评价其检索病例临床相关性更高，平均评分从3.36提升至4.35。

Conclusion: 研究证明了基于解剖引导的检索在增强儿科肌肉骨骼成像诊断推理和临床决策支持方面的潜力。该方法无需人工图像级标注，通过利用密集放射学报告和骨骼特定定位学习临床有意义的细粒度表示。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [83] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: SAGE是一个从原始视频流中可扩展适应几何基础模型的框架，通过层次化挖掘管道将视频转化为训练轨迹，结合稀疏几何锚点和密集可微分一致性监督，显著提升了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 几何基础模型在3D重建方面前景广阔，但其发展受到多样化、大规模3D标注数据稀缺的严重制约。虽然互联网视频提供了几乎无限的原始数据，但由于缺乏真实几何信息和存在观测噪声，将其作为几何学习的扩展源具有挑战性。

Method: SAGE采用层次化挖掘管道将视频转化为训练轨迹，结合三种监督方式：(1)信息丰富的训练轨迹选择；(2)通过SfM点云进行稀疏几何锚点，提供全局结构指导；(3)通过3D高斯渲染实现密集可微分一致性，提供多视角约束。为防止灾难性遗忘，引入了基于锚点数据的正则化策略。

Result: 大量实验表明，SAGE显著增强了零样本泛化能力，在未见过的基准测试（7Scenes、TUM-RGBD、Matterport3D）上，Chamfer距离比最先进的基线方法减少了20-42%。

Conclusion: SAGE开创了通过互联网视频适应几何基础模型的先河，为通用3D学习建立了可扩展的范式，解决了3D标注数据稀缺的核心瓶颈问题。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [84] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

TL;DR: TLQ提出了一种针对视觉语言模型的令牌级重要性感知层量化框架，通过梯度信息指导的令牌级重要性集成机制和多GPU量化暴露层校准方案，显著提升了后训练量化的性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中视觉和文本令牌在激活分布和量化误差敏感性方面存在显著差异，这对后训练量化的有效校准提出了重大挑战。传统校准方法难以同时处理这两种不同类型令牌的特性差异。

Method: 提出了令牌级重要性感知层量化框架：1）基于梯度信息设计令牌级重要性集成机制，构建令牌级校准集；2）引入多GPU、量化暴露层校准方案，保持校准过程与真实量化推理路径一致，并将复杂层校准工作分配到多个RTX3090 GPU上。

Result: 在两个模型、三个模型规模和两种量化设置下进行评估，在所有设置中均实现了性能提升，表明其具有较强的量化稳定性。

Conclusion: TLQ框架通过令牌级重要性感知校准和多GPU层校准方案，有效解决了视觉语言模型后训练量化中的校准挑战，实现了稳定且高效的量化性能。

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [85] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 该研究对开源视觉语言模型在隐私相关属性识别方面进行零样本评估，发现VLM倾向于比人类标注者更频繁地预测隐私属性存在，但在高一致性情况下可以补充人类标注的遗漏。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型常用于图像视觉属性的零样本检测，但它们在隐私相关属性识别方面的表现尚未得到充分评估。研究旨在评估开源VLM在隐私属性识别中的能力，分析其与人类标注的一致性和差异。

Method: 采用零样本评估方法，使用开源视觉语言模型对隐私相关属性进行识别。通过分析VLM之间的标注一致性，并与人类标注进行比较，识别VLM表现良好的属性以及存在分歧的情况。

Result: 研究发现：1) 与人类标注相比，VLM倾向于更频繁地预测隐私属性的存在；2) 在VLM之间具有高标注一致性的情况下，它们能够识别人类标注者忽略的属性，从而补充人类标注；3) 识别出了VLM表现良好的具体属性。

Conclusion: 视觉语言模型在大规模图像数据集的隐私标注方面具有潜力，特别是在高一致性情况下可以作为人类标注的补充工具，识别被人类忽略的隐私属性。

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [86] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于少样本学习的跨监控场景人群计数方法，通过局部和全局密度特征指导模型适应未见过的监控场景。


<details>
  <summary>Details</summary>
Motivation: 不同监控摄像头捕获的人群场景差异很大，现有的人群计数模型对未见过的监控场景泛化能力有限。为了提升模型的泛化能力，研究者将不同监控场景视为不同类别场景，引入少样本学习使模型能够适应属于给定示例类别的未见监控场景。

Method: 提出利用局部和全局密度特征指导模型适应未见监控场景。具体包括：1）多局部密度学习器学习支持场景中代表不同密度分布的多原型；2）编码多个局部密度相似性矩阵，以局部方式指导模型；3）从支持图像中提取全局密度特征，以全局方式指导模型。

Result: 在三个监控数据集上的实验表明，所提方法能够适应未见过的监控场景，并在少样本人群计数任务中优于最近的最先进方法。

Conclusion: 通过结合局部和全局密度特征指导的少样本学习方法，能够有效提升人群计数模型对未见监控场景的适应能力和泛化性能。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [87] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

TL;DR: D-ORCA是一个面向对话的跨模态大语言模型，专注于视频中的说话人识别和语音内容理解，通过强化学习优化音频-视觉字幕生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频中的对话是重要信息源，但现有模型在准确识别"谁在何时说了什么"方面存在不足，缺乏高质量的多方对话视频数据集和针对说话人属性、内容准确性、时间边界对齐的优化方法。

Method: 1) 构建DVD双语大规模对话视频数据集(4万训练+2千评估)；2) 提出D-ORCA对话中心跨模态大模型；3) 采用群体相对策略优化，设计三个新颖奖励函数：说话人属性准确性、全局语音内容准确性、句子级时间边界对齐。

Result: D-ORCA在说话人识别、语音识别和时间定位方面显著优于现有开源模型；仅80亿参数却在多个通用音频-视觉理解基准上与Qwen3-Omni竞争；在双语评估中表现优异。

Conclusion: D-ORCA通过对话中心设计、高质量数据集和创新的强化学习奖励机制，实现了视频对话理解的显著提升，为深度视频理解提供了有效解决方案。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [88] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

TL;DR: EasyTune提出了一种高效的对齐扩散模型的方法，通过分步微调解决现有方法的内存消耗大和优化效率低的问题，并结合自优化偏好学习机制提升运动生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于可微分奖励直接对齐扩散模型偏好的方法存在两个主要问题：(1) 优化效率低且粒度粗糙，(2) 内存消耗大。此外，偏好运动对的稀缺限制了运动奖励模型的训练。

Method: 首先从理论和实证上识别出限制的关键原因：去噪轨迹中不同步骤之间的递归依赖。提出EasyTune方法，在每一步去噪过程中微调扩散模型，而不是在整个轨迹上优化，从而解耦递归依赖。此外，引入自优化偏好学习机制，动态识别偏好对并进行偏好学习。

Result: 实验表明，EasyTune在MM-Dist对齐指标上比DRaFT-50提升8.2%，同时仅需其额外内存开销的31.16%，并实现7.3倍的训练加速。

Conclusion: EasyTune通过分步微调和自优化偏好学习机制，有效解决了现有扩散模型对齐方法的内存消耗大和优化效率低的问题，在运动生成任务上取得了显著改进。

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [89] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: FSP-Diff是一个用于超低剂量能谱CT重建的全谱先验增强双域潜在扩散框架，通过互补特征构建、全谱先验集成和高效潜在扩散合成三大策略，显著提升图像质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 超低剂量条件下，能谱CT中能量特定投影的信噪比急剧下降，导致重建图像出现严重伪影和结构细节丢失，需要开发有效的重建方法来应对这一挑战。

Method: 提出FSP-Diff框架，包含三个核心策略：1)互补特征构建：整合直接图像重建和投影域去噪结果；2)全谱先验集成：融合多能量投影为高信噪比全谱图像作为统一结构参考；3)高效潜在扩散合成：将多路径特征嵌入紧凑潜在空间，在低维流形中进行交互特征融合。

Result: 在模拟和真实数据集上的广泛实验表明，FSP-Diff在图像质量和计算效率方面显著优于现有最先进方法。

Conclusion: FSP-Diff框架为临床可行的超低剂量能谱CT成像提供了有前景的解决方案，通过创新的双域扩散方法有效平衡了细节保真度和噪声抑制。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [90] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出CSDN方法用于超稀疏视角CBCT重建，通过神经先验编码连续三维衰减表示，结合正弦图与数字放射图像双路径扩散策略，有效抑制伪影并恢复细节纹理。


<details>
  <summary>Details</summary>
Motivation: CBCT临床应用面临辐射暴露与图像质量之间的固有权衡。超稀疏角度采样虽然降低剂量，但会导致严重的欠采样伪影和切片间不一致性，影响诊断可靠性。现有重建方法难以平衡角度连续性与空间细节保真度。

Method: 提出连续性驱动的协同扩散与神经先验（CSDN）方法：1）引入神经先验作为结构基础，编码连续三维衰减表示；2）基于神经先验初始化，开发协同扩散策略，包括正弦图细化扩散（Sino-RD）恢复角度连续性，数字放射图像细化扩散（DR-RD）从投影图像角度增强切片间一致性；3）通过双投影重建融合（DPRF）模块自适应融合两个扩散路径输出，实现连贯体积重建。

Result: 大量实验表明，CSDN在超稀疏视角条件下能有效抑制伪影并恢复精细纹理，性能优于现有最先进技术。

Conclusion: CSDN方法通过神经先验和协同扩散策略，成功解决了超稀疏视角CBCT重建中的角度连续性与切片一致性问题，为低剂量高质量CBCT成像提供了有效解决方案。

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [91] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

TL;DR: 最新深度伪造检测技术在应对现代合成方法时表现不佳，包括人类评估者也无法有效识别高质量深度伪造内容，突显检测技术发展滞后于生成技术进步的严峻现实。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的快速发展（包括扩散模型、NeRF和增强的GANs），合成媒体的真实性和可访问性显著提高。同时，检测方法虽有进步，但需要评估当前最先进检测技术能否有效应对现代合成方法生成的深度伪造内容。

Method: 对最先进的深度伪造检测技术进行全面的实证分析，包括针对尖端合成方法的人类评估实验。通过大量实验评估检测模型在应对现代合成技术生成的深度伪造时的性能表现。

Result: 研究发现令人担忧的趋势：许多最先进的检测模型在面对现代合成技术生成的深度伪造时表现明显不佳，人类参与者在面对最高质量深度伪造时也表现不佳。实验证据表明检测模型需要持续改进以跟上深度伪造生成技术的发展步伐。

Conclusion: 当前检测方法与新一代深度伪造生成技术的复杂性之间存在关键差距，迫切需要在这一关键研究领域加强努力，持续改进检测模型以应对不断发展的深度伪造技术。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [92] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 本研究评估了超声特异性自监督预训练（USF-MAE）在早孕期超声图像中检测囊性水囊瘤的性能，结果显示其优于传统的DenseNet-169基线模型。


<details>
  <summary>Details</summary>
Motivation: 囊性水囊瘤是高风险的产前超声发现，与染色体异常、结构畸形和不良妊娠结局相关。自动化检测可以提高可重复性并支持可扩展的早期筛查项目，但监督式深度学习方法受限于小规模标注数据集。

Method: 研究使用基于掩码自编码（MAE）的超声自监督基础模型（USF-MAE），该模型在超过37万张未标注超声图像上预训练，然后针对本研究中的正常对照和囊性水囊瘤病例进行二元分类微调。评估采用与DenseNet-169基线相同的超声数据集、预处理流程和4折交叉验证协议，使用准确率、灵敏度、特异度和ROC-AUC等指标。模型可解释性通过Score-CAM可视化进行定性分析。

Result: USF-MAE在所有评估指标上均优于DenseNet-169基线模型。USF-MAE的平均准确率为0.96、灵敏度0.94、特异度0.98、ROC-AUC为0.98，而DenseNet-169基线分别为0.93、0.92、0.94和0.94。Score-CAM可视化显示模型预测具有临床相关性，能突出显示胎儿颈部的预期区域。Wilcoxon符号秩检验证实USF-MAE的性能提升具有统计学显著性（p=0.0057）。

Conclusion: 超声特异性自监督预训练能够促进准确、稳健的深度学习检测囊性水囊瘤，优于传统监督学习方法，为早期产前筛查提供了有前景的技术支持。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [93] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: FlashVID是一个无需训练的视频大语言模型推理加速框架，通过注意力与多样性令牌选择和树状时空令牌合并技术，在保留99.1%性能的同时将视觉令牌减少90%，实现10倍视频帧输入扩展。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型需要处理大量视觉令牌，计算效率低下。现有加速框架通常独立压缩空间和时间冗余，忽略了时空关系，导致次优压缩效果。视频的动态特性使得视觉特征在时空维度上高度相关且不断变化。

Method: FlashVID采用两种核心技术：1) 注意力与多样性令牌选择(ADTS)：选择最具代表性的令牌作为基础视频表示；2) 树状时空令牌合并(TSTM)：进行细粒度的时空冗余消除。这是一个无需训练、即插即用的推理加速框架。

Result: 在三个代表性VLLM和五个视频理解基准测试上的广泛实验证明了方法的有效性和泛化能力。仅保留10%视觉令牌时，FlashVID能保留LLaVA-OneVision 99.1%的性能。可使Qwen2.5-VL的视频帧输入增加10倍，在相同计算预算下相对提升8.6%。

Conclusion: FlashVID是一个高效、无需训练的视频大语言模型加速框架，通过创新的时空令牌压缩技术显著提升推理效率，同时保持模型性能，为长视频处理提供了实用的解决方案。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [94] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: MIND是首个用于评估世界模型记忆一致性和动作控制能力的开放域闭环重访基准，包含250个高质量视频，设计了评估框架来衡量记忆一致性和动作控制两大核心能力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏统一的基准来评估世界模型在动态视觉环境中的基本能力，包括理解、记忆和预测能力，这阻碍了世界模型的系统化评估和比较。

Method: 创建了包含250个1080p、24FPS高质量视频的MIND基准，包括第一人称和第三人称视角视频，设计了评估框架来测量记忆一致性和动作控制能力，并引入了MIND-World作为基线模型。

Result: 实验验证了MIND基准的完整性，揭示了当前世界模型面临的关键挑战，包括难以保持长期记忆一致性以及在跨动作空间泛化方面的困难。

Conclusion: MIND基准填补了世界模型评估领域的空白，为系统评估世界模型的记忆一致性和动作控制能力提供了标准化工具，有助于推动世界模型研究的进一步发展。

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [95] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

TL;DR: 提出了一种系统性的等变ViT框架，通过使ViT的关键组件（包括patch embedding、自注意力、位置编码和上下采样）等变化，构建具有理论保证等变性的ViT架构，在各种视觉任务中提升性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有等变ViT在平衡性能与等变性方面存在困难，主要挑战在于实现ViT中多样化模块（特别是自注意力机制与patch embedding的协调）的整体等变修改。

Method: 提出一个简单框架，系统性地使ViT的关键组件（patch embedding、自注意力、位置编码、Down/Up-Sampling）等变化，构建具有理论保证等变性的ViT，可作为即插即用替代方案，甚至可扩展到Swin Transformers。

Result: 广泛的实验表明，提出的等变ViT在各种视觉任务中一致地提升了性能和数据效率。

Conclusion: 该框架提供了一个理论上有保证、实践上多功能的等变ViT架构，能够无缝扩展到不同Transformer变体，在多种视觉任务中展现出优越性能。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [96] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: Picasso：一个物理约束的多物体场景重建系统，通过考虑几何、非穿透性和物理约束来生成物理上合理的场景重建，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在遮挡和测量噪声存在的情况下，几何上准确但物理上不合理的场景重建（如物体穿透或不稳定平衡）难以用于数字孪生和接触丰富的行为规划。需要从整体场景角度考虑物体交互和物理合理性。

Method: 提出Picasso物理约束重建管道，使用快速拒绝采样方法考虑多物体交互，利用推断的物体接触图指导采样，确保非穿透性和物理合理性。

Result: 在自建的Picasso数据集和YCB-V数据集上评估，Picasso大幅优于现有技术，提供物理上合理且更符合人类直觉的重建结果。

Conclusion: 物体姿态和形状估计需要从整体场景角度考虑物理约束，Picasso通过物理约束重建管道实现了物理上合理的多物体场景重建，为数字孪生和接触丰富的行为规划提供了可靠基础。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [97] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: DICE是一个无需训练、实时去除艺术家风格的框架，通过对比子空间分解将风格与内容分离，防止未经授权的风格模仿。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的普及使得风格模仿变得容易，引发了版权和知识产权风险。现有对策要么需要昂贵的权重编辑，要么依赖明确指定的编辑风格，限制了部署侧的安全性实用性。

Method: DICE通过构建对比三元组，迫使模型在潜在空间中区分风格和非风格特征，将解耦过程形式化为可解的广义特征值问题，精确定位风格子空间。采用自适应注意力解耦编辑策略，动态评估每个token的风格浓度，对QKV向量进行差异化抑制和内容增强。

Result: DICE在风格去除的彻底性和内容完整性保存之间实现了优越的平衡，仅需额外3秒即可解耦风格，为遏制风格模仿提供了实用高效的技术。

Conclusion: DICE提出了一种无需训练、实时去除艺术家风格的框架，通过对比子空间分解有效解决风格模仿问题，为部署侧安全提供了实用解决方案。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [98] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

TL;DR: ReRoPE是一种即插即用框架，通过将相对相机姿态信息注入预训练视频扩散模型中未充分利用的低频频段，实现可控视角的视频生成，无需大量训练或架构修改。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用相对于固定参考帧（如第一帧）的相机姿态编码，缺乏平移不变性，导致泛化能力差和累积漂移。相对相机姿态嵌入虽然更鲁棒，但难以在不增加大量训练成本或修改架构的情况下集成到预训练视频扩散模型中。

Method: 基于Rotary Positional Embeddings (RoPE)在现有模型中未充分利用其全频谱带宽（特别是低频分量）的洞察，ReRoPE将相对相机姿态信息无缝注入这些未充分利用的频段，实现精确的相机控制同时保留预训练的生成先验。

Result: 在图像到视频（I2V）和视频到视频（V2V）任务上的评估显示，ReRoPE在相机控制精度和视觉保真度方面表现出色，提供了一种训练高效的路径来实现可控的高质量视频生成。

Conclusion: ReRoPE是一个即插即用框架，能够在不损害预训练视频扩散模型生成能力的情况下，将相对相机信息集成到模型中，为可控、高质量的视频生成提供了训练高效的解决方案。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [99] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 该论文提出了一种利用多模态大语言模型进行视频文本嵌入和检索的新方法，无需视觉监督即可实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究将生成式多模态大语言模型适配为嵌入提取器用于视觉任务，但它们在视频上的性能仍不如专门的视频基础模型。本文旨在探索如何更好地利用MLLMs进行视频文本嵌入和检索。

Method: 首先进行系统性的层级分析，发现中间层已编码大量任务相关信息；然后结合中间层嵌入和校准的MLLM头部实现零样本检索；最后引入轻量级文本对齐策略，将密集视频描述映射到简短摘要，实现无需视觉监督的视频文本嵌入学习。

Result: 该方法在常见视频检索基准测试中超越了当前方法，通常具有显著优势，实现了最先进的结果，且无需任何视觉监督或微调。

Conclusion: 研究表明，通过利用MLLMs中间层的丰富信息并结合文本对齐策略，可以在无需视觉监督的情况下实现强大的视频文本检索性能，为视频理解任务提供了新的有效途径。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [100] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: MMLSv2是一个用于火星表面滑坡分割的多模态数据集，包含7个波段图像，共664张图像，并额外提供276张地理隔离测试集以评估空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有火星滑坡研究缺乏专门的多模态分割数据集，需要能够评估模型在碎片化、细长和小尺度滑坡区域性能的数据集，同时需要测试模型在地理隔离区域的空间泛化能力。

Method: 构建包含RGB、数字高程模型、坡度、热惯性和灰度通道的七波段多模态图像数据集，包含664张图像分为训练、验证和测试集，并额外提供276张地理隔离测试集。

Result: 多个分割模型在数据集上能够稳定训练并达到竞争性性能，但在碎片化、细长和小尺度滑坡区域仍面临挑战；在地理隔离测试集上性能明显下降，表明其评估模型鲁棒性和泛化能力的价值。

Conclusion: MMLSv2数据集为火星滑坡分割提供了有价值的基准，特别适合评估模型在复杂滑坡形态和空间泛化场景下的性能，有助于推动相关研究发展。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [101] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

TL;DR: 该研究评估了Vision Transformer模型在xBD数据集上的建筑损伤分类性能，提出了一种针对性的补丁预处理流程和冻结头微调策略，在噪声和类别不平衡数据上取得了与CNN基线相当的竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 快速建筑损伤评估对灾后响应至关重要。卫星影像损伤分类模型提供了可扩展的态势感知手段，但卫星数据中的标签噪声和严重类别不平衡带来了主要挑战。xBD数据集为跨地理区域的建筑级损伤提供了标准化基准。

Method: 评估了DINOv2-small和DeiT模型进行多类损伤分类。提出了针对性的补丁预处理流程来隔离结构特征并最小化训练中的背景噪声。采用冻结头微调策略以保持计算需求可控。通过准确率、精确率、召回率和宏观平均F1分数评估模型性能。

Result: 研究表明，采用新颖训练方法的小型ViT架构在灾害分类方面相对于先前的CNN基线取得了竞争性的宏观平均F1分数。

Conclusion: 小型Vision Transformer架构结合针对性的预处理和训练策略，能够在噪声和类别不平衡的卫星影像数据上有效进行建筑损伤分类，为快速灾后评估提供了可行的解决方案。

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [102] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

TL;DR: Fields of The World (FTW)生态系统提供了全球农田边界数据集、预训练模型和工具，支持农田边界提取、作物分类和森林损失归因等农业应用。


<details>
  <summary>Details</summary>
Motivation: 农田边界地图是农业数据产品的基础，对于作物监测、产量估算和疾病评估至关重要。当前缺乏大规模、标准化的全球农田边界数据集和工具。

Method: 构建了包含160万农田多边形、覆盖24个国家的基准数据集；开发了预训练分割模型和命令行推理工具；使用MOSAIKS随机卷积特征和FTW农田边界进行作物类型分类；提供本地尺度和国家尺度的推理笔记本。

Result: 作物类型分类的宏观F1分数达到0.65-0.75（使用有限标签）；在5个国家（476万平方公里）上展示了预计算预测结果，预测农田中位数面积从0.06公顷（卢旺达）到0.28公顷（瑞士）。

Conclusion: FTW生态系统为农业遥感应用提供了全面的工具和数据集，能够支持从本地到国家尺度的农田边界提取和作物分类任务，为精准农业和粮食安全监测提供技术支撑。

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [103] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉语言模型安全漏洞：虽然VLM在预训练和指令调优中能处理分割图像，但安全对齐通常只在完整图像上进行，导致无法检测跨多个图像片段分布的有害语义。作者开发了SIVA攻击方法，通过渐进式策略和对抗知识蒸馏实现高转移成功率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型虽然通过RLHF等偏好优化技术增强了安全性，但这些安全对齐主要针对完整图像。然而，VLM的预训练和指令调优能够很好地处理分割图像输入，而安全对齐却没有考虑有害语义分布在多个图像片段中的情况，这导致了一个新的安全漏洞。

Method: 提出了分割图像视觉越狱攻击（SIVA），采用渐进式策略：从简单的图像分割开始，发展到自适应白盒攻击，最终形成黑盒转移攻击。最强的攻击策略采用新颖的对抗知识蒸馏（Adv-KD）算法，显著提高了跨模型转移能力。

Result: 在三个最先进的现代VLM和三个越狱数据集上的评估显示，作者的最强攻击比现有基线实现了高达60%的转移成功率提升，证明了分割图像攻击的有效性和严重性。

Conclusion: 该研究揭示了当前VLM安全对齐中的一个关键漏洞，并提出了有效的攻击方法。最后，作者还提出了解决这一漏洞的高效方法，强调了需要改进VLM安全对齐以处理分割图像输入的重要性。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [104] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus是一个能够生成个性化3D形状的新框架，通过几何和外观层面的概念学习，从参考形状中提取可重用的属性并与文本结合生成新形状。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状生成方法在个性化控制方面存在局限，难以从参考形状中提取可重用的几何和外观属性，并灵活组合这些属性来生成多样化的个性化形状。

Method: 1) 将3D形状个性化定义为提取类别无关的几何和外观属性；2) 设计渐进优化策略，在几何和外观层面解耦学习形状概念；3) 扩展到区域级概念学习，使用上下文感知和无上下文损失。

Result: PEGAsus能够从广泛的参考形状中有效提取属性，并与文本灵活组合生成新形状，实现细粒度控制，支持创建多样化个性化结果，在跨类别场景中表现优异，超越现有最先进方法。

Conclusion: PEGAsus框架通过几何和外观层面的概念学习，实现了有效的3D形状个性化生成，支持细粒度控制和跨类别应用，为个性化3D内容创作提供了新的解决方案。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [105] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

TL;DR: 本文提出MCSDR模型，使用基于分数的扩散模型来估计超声心动图中的左心室射血分数，解决了传统回归方法在病理多模态分布下的局限性。


<details>
  <summary>Details</summary>
Motivation: 从超声心动图估计左心室射血分数是一个病态逆问题，存在噪声、伪影和有限视角等挑战。传统深度学习方法使用均方误差回归，强制模型学习条件期望，但当后验分布是多模态或重尾分布时（这在病理场景中常见），会产生误导性预测。

Method: 提出多模态条件基于分数的扩散回归模型（MCSDR），这是一个概率框架，旨在建模以超声心动图视频和患者人口统计学属性先验为条件的LVEF连续后验分布。

Result: 在EchoNet-Dynamic、EchoNet-Pediatric和CAMUS数据集上的广泛实验表明，MCSDR实现了最先进的性能。定性分析显示，在高噪声或显著生理变异性的病例中，模型的生成轨迹表现出独特行为，为AI辅助诊断提供了新的可解释性。

Conclusion: 该研究探索了从确定性回归向生成回归的范式转变，提出的MCSDR框架能够更好地处理超声心动图LVEF估计中的不确定性，为医疗AI诊断提供了更可靠和可解释的解决方案。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [106] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

TL;DR: 提出GR-CoT框架，通过地理空间推理链增强MLLMs的场景理解能力，解决遥感开放词汇分割中的语义歧义问题


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇分割方法主要依赖视觉特征和文本嵌入的被动映射，缺乏地理空间上下文感知，导致相似光谱特征但不同语义属性的地物类别出现严重语义歧义和误分类

Method: 提出Geospatial Reasoning Chain-of-Thought (GR-CoT)框架，包含离线知识蒸馏流和在线实例推理流。离线流建立细粒度类别解释标准；在线流执行宏观场景锚定、视觉特征解耦和知识驱动决策合成的顺序推理过程，生成图像自适应词汇表

Result: 在LoveDA和GID5基准测试上的广泛实验证明了该方法的优越性

Conclusion: GR-CoT框架通过增强MLLMs的地理空间推理能力，有效解决了遥感开放词汇分割中的语义歧义问题，提高了分割精度

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [107] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

TL;DR: 本文提出了一种名为Chain-of-Caption的训练免费框架，通过结合多种视觉和文本上下文来提升多模态大语言模型在指代表达理解任务上的性能，在多个数据集上实现了5%到30%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 指代表达理解任务需要根据文本描述定位图像中的目标对象。虽然多模态大语言模型通过扩大模型规模和训练数据在REC基准上取得了高准确率，但通过工具使用等技术提供额外视觉或文本上下文可以进一步提升性能。本文旨在分析不同上下文提供技术对MLLM在REC任务上的影响。

Method: 提出了名为Chain-of-Caption的训练免费框架，通过工具使用为MLLM提供额外的视觉和文本上下文。分析不同上下文提供技术的影响，并设计方法结合多种上下文来提升REC性能。

Result: 在RefCOCO、RefCOCOg、RefCOCO+和Ref-L4数据集上的实验表明，单独的文本或视觉上下文都能在不进行微调的情况下提升REC性能。通过结合多种上下文，训练免费框架在不同IoU阈值下的准确率比基线模型提升了5%到30%。

Conclusion: 通过工具使用提供额外的视觉和文本上下文可以有效提升多模态大语言模型在指代表达理解任务上的性能。提出的Chain-of-Caption框架无需训练即可实现显著性能提升，为REC任务提供了一种有效的解决方案。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [108] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 该研究分析了视觉空间推理中测试时视觉想象的作用，提出了自适应框架AVIC，通过选择性调用世界模型来优化想象的使用，在多个基准测试中实现了效率与准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉空间推理中存在局限性，特别是在需要从不同视角理解场景时。虽然已有工作使用世界模型进行视觉想象来增强推理，但何时需要想象、需要多少想象、以及何时想象反而有害等问题仍未得到充分理解。不加区分的想象会增加计算成本，甚至可能因引入误导性证据而降低性能。

Method: 提出了AVIC自适应测试时框架，该框架包含世界模型，能够明确推理当前视觉证据的充分性，然后选择性地调用和扩展视觉想象。框架首先评估静态视觉证据是否足够，仅在必要时才激活世界模型进行想象，并控制想象的规模。

Result: 在空间推理基准测试（SAT、MMSI）和具身导航基准测试（R2R）上的结果表明：1）明确了想象在哪些场景中至关重要、边际有效或有害；2）选择性控制策略能够匹配或超越固定想象策略，同时显著减少世界模型调用次数和语言标记数量；3）揭示了视觉想象作为可控资源在空间推理中的优化潜力。

Conclusion: 研究强调了分析和控制测试时想象对于实现高效可靠空间推理的重要性。自适应选择性想象策略能够在保持或提升准确性的同时，显著提高计算效率，为多模态大语言模型的视觉空间推理能力优化提供了新思路。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [109] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

TL;DR: 本文提出DeCI框架，通过周期-漂移分解和通道独立性建模原始BOLD信号，在fMRI脑疾病分类中超越传统功能连接方法


<details>
  <summary>Details</summary>
Motivation: 现有fMRI分析方法主要依赖基于皮尔逊相关的功能连接，将4D BOLD信号简化为静态2D矩阵，丢失了时间动态信息且只能捕捉线性区域间关系。需要更直接地建模时间信息来更好地捕捉复杂脑动力学。

Method: 提出DeCI框架，包含两个核心原则：(1) 周期与漂移分解：在每个感兴趣区域中分离周期性的振荡波动和漂移性的慢基线趋势；(2) 通道独立性：分别建模每个ROI，提高鲁棒性并减少过拟合。

Result: 在五个公共数据集上的实验表明，DeCI在分类准确率和泛化能力上均优于传统FC方法和现有时间序列模型基准，验证了直接建模时间信息的有效性。

Conclusion: 研究结果支持在fMRI分析中转向端到端的时间建模方法，以更好地捕捉复杂的脑动力学。DeCI框架简单有效，为fMRI分析提供了新思路。

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [110] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: PISCO是一个视频扩散模型，用于在现有视频中精确插入特定实例，支持任意稀疏关键帧控制，解决了传统视频编辑难以保持场景完整性和物理一致性的问题。


<details>
  <summary>Details</summary>
Motivation: AI视频生成正从依赖大量提示工程和"筛选"的通用生成，转向细粒度可控生成和高保真后处理。专业AI辅助电影制作需要精确的目标修改，视频实例插入是这一转变的关键，要求保持场景完整性、物理一致性和原始动态。

Method: 提出PISCO视频扩散模型，支持单关键帧、起止关键帧或任意时间戳稀疏关键帧控制。引入变量信息引导处理稀疏条件引起的分布偏移，分布保持时间掩码稳定时间生成，以及几何感知条件实现真实场景适应。

Result: 构建了PISCO-Bench基准测试集，包含验证过的实例标注和配对干净背景视频。实验表明PISCO在稀疏控制下持续优于强基线方法，随着控制信号增加呈现清晰单调的性能提升。

Conclusion: PISCO实现了精确的视频实例插入，支持灵活的稀疏关键帧控制，为专业AI辅助电影制作提供了有效的解决方案，推动了AI视频生成向可控、高保真方向的发展。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [111] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 提出语言引导的运动离散化方法LG-Tok，通过Transformer架构实现语言与运动的对齐，在减少token数量的同时保持高质量重建，显著提升运动生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统运动离散化方法增加token数量以提高重建质量，但这会增加生成模型的学习难度。需要一种既能保持高质量重建又能降低生成复杂度的解决方案。

Method: 提出语言引导的离散化方法LG-Tok，使用Transformer架构实现语言与运动的对齐，设计语言丢弃方案使解离散器支持无语言指导的生成。

Result: 在HumanML3D和Motion-X基准测试中，LG-Tok获得Top-1分数0.542和0.582，优于MARDM的0.500和0.528；FID分数分别为0.057和0.088，优于0.114和0.147。LG-Tok-mini仅用一半token仍保持竞争力。

Conclusion: 语言引导的运动离散化方法能够产生紧凑的高层语义表示，在保持高质量重建的同时降低生成复杂度，为运动生成任务提供了高效解决方案。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [112] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

TL;DR: 该研究针对"图像思维"范式下的过程奖励模型（PRMs）评估问题，首次构建了专门的综合基准，包含1,206条人工标注的推理轨迹，定义了7种细粒度错误类型，并发现当前大型视觉语言模型作为PRMs存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型的发展，"图像思维"范式使模型能够动态编辑和重新编码视觉信息，但这一范式在推理过程中可能产生多种错误。虽然需要过程奖励模型来区分正负推理步骤，但现有的PRMs基准主要是文本中心化的，缺乏对该范式的全面评估。

Method: 通过广泛分析推理轨迹和PRMs引导搜索实验，定义了7种细粒度错误类型；构建了包含1,206条人工标注的"图像思维"推理轨迹的综合基准，涵盖4个类别和16个子类别；对当前大型视觉语言模型作为PRMs的能力进行了实验分析。

Result: 研究发现当前大型视觉语言模型作为PRMs表现不足：在视觉推理过程评估方面能力有限，在不同错误类型间存在显著性能差异，表现出正向评估偏见，并对推理步骤位置敏感。这些发现证明了基准的有效性。

Conclusion: 该研究为"图像思维"范式下的过程奖励模型评估提供了首个综合基准，揭示了当前大型视觉语言模型作为PRMs的局限性，为推进LVLMs中PRMs的发展奠定了重要基础。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [113] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: GeoEdit是一个基于扩散模型的图像编辑框架，通过扩散transformer模块实现精确的几何变换编辑，并引入效果敏感注意力机制来增强光影效果，在公开基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像编辑中面临两个主要挑战：1）难以实现准确的几何变换编辑（平移、旋转、缩放）；2）对复杂光影效果建模不足，导致结果不真实。这些问题在复杂场景中尤为突出。

Method: 提出GeoEdit框架，包含两个核心组件：1）通过扩散transformer模块实现上下文生成，集成几何变换进行精确对象编辑；2）引入效果敏感注意力机制，增强复杂光影和阴影效果的建模。此外，构建了包含12万高质量图像对的RS-Objects数据集用于训练。

Result: 在公开基准测试上的大量实验表明，GeoEdit在视觉质量、几何精度和真实感方面持续优于最先进的方法。

Conclusion: GeoEdit通过创新的扩散transformer架构和效果敏感注意力机制，成功解决了扩散模型在几何变换编辑和光影效果建模方面的挑战，显著提升了图像编辑的质量和真实感。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [114] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: 提出D²-VR框架，通过降噪鲁棒流对齐模块和对抗蒸馏技术，在保持视频修复感知质量的同时，将采样速度提升12倍并增强时间稳定性


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散先验和时间对齐的视频修复方法虽然能提供优秀的感知质量，但在面对复杂真实世界退化时存在推理延迟过高和时间不稳定的问题，限制了实际部署

Method: 1. 设计降噪鲁棒流对齐模块，利用置信度感知注意力过滤不可靠运动线索；2. 采用对抗蒸馏范式将扩散采样轨迹压缩到快速少步推理；3. 设计协同优化策略平衡感知质量和时间一致性

Result: D²-VR在保持最先进性能的同时，将采样过程加速12倍，显著提升了推理效率和时间稳定性

Conclusion: 提出的D²-VR框架成功解决了基于扩散的视频修复方法在实际部署中的延迟和不稳定问题，为高质量视频修复的实际应用提供了可行方案

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [115] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

TL;DR: RealSynCol是一个高度逼真的合成结肠镜数据集，用于解决结肠镜3D重建中真实标注数据稀缺的问题，显著提升了深度学习算法在临床图像上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习可以改善结肠镜检查，通过3D重建提供全面的黏膜表面和病变视图，并帮助识别未探索区域。但现有方法的发展受到大规模真实标注数据稀缺的限制。

Method: 从10个CT扫描中提取结肠几何结构，导入到模拟术中条件的虚拟环境中，使用逼真的血管纹理进行渲染。生成的数据集包含28,130帧图像，配有深度图、光流、3D网格和相机轨迹等真实标注。

Result: 基准研究表明，RealSynCol的高真实感和多样性显著提升了在临床图像上的泛化性能，证明它是开发支持内镜诊断的深度学习算法的强大工具。

Conclusion: RealSynCol是一个高度逼真的合成结肠镜数据集，解决了真实标注数据稀缺的问题，能够有效支持结肠镜3D重建和病变检测等深度学习算法的开发。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [116] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

TL;DR: 重新审视基于注意力的稀疏图像匹配模型训练，发现关键设计选择对LightGlue性能影响显著，检测器比描述符对性能差异影响更大，提出使用多样化检测器关键点微调现有模型的方法，得到通用检测器无关模型


<details>
  <summary>Details</summary>
Motivation: 重新审视基于注意力的稀疏图像匹配模型训练问题，识别先前被忽视的关键设计选择对LightGlue模型性能的显著影响，并探究检测器和描述符在基于transformer的匹配框架中的作用

Method: 首先识别影响LightGlue模型性能的关键设计选择，然后研究检测器和描述符在transformer匹配框架中的作用，最后提出使用来自多样化检测器集合的关键点微调现有图像匹配模型的方法，得到通用检测器无关模型

Result: 发现检测器（而非描述符）通常是性能差异的主要原因，提出的通用检测器无关模型在作为新检测器的零样本匹配器时，达到或超过了专门为这些特征训练的模型的准确性

Conclusion: 研究结果为基于transformer的匹配模型的部署和局部特征的未来设计提供了有价值的见解，提出的通用检测器无关模型能够有效适应不同的检测器

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [117] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 该论文提出了Demo驱动的视频上下文学习新任务，并构建了Demo-ICL-Bench基准来评估模型从少量示例中学习和适应动态新情境的能力，同时开发了Demo-ICL模型来应对这一挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要评估模型基于静态内部知识的理解能力，而非从动态新颖情境中通过少量示例学习和适应的能力。为了填补这一空白，需要开发能够评估和提升视频上下文学习能力的新任务和基准。

Method: 提出了Demo驱动的视频上下文学习任务；构建了Demo-ICL-Bench基准，包含1200个教学YouTube视频及相关问题，提供文本和视频两种演示类型；开发了Demo-ICL模型，采用两阶段训练策略：视频监督微调和信息辅助直接偏好优化。

Result: 对最先进MLLM的广泛实验证实了Demo-ICL-Bench的难度，证明了Demo-ICL模型的有效性，并揭示了未来研究方向。

Conclusion: 该研究填补了视频上下文学习评估的空白，提出的新任务、基准和模型为视频理解领域提供了重要的研究方向和工具，展示了从动态情境中学习的重要性。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [118] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Vista是一个用于流式视频问答的新型框架，通过场景感知的分割、压缩和召回机制，解决了MLLMs在处理连续视频流时的内存和效率问题。


<details>
  <summary>Details</summary>
Motivation: 流式视频问答对多模态大语言模型提出了独特挑战，因为视频帧是顺序到达的，用户查询可以在任意时间点发出。现有基于固定大小内存或简单压缩的解决方案往往存在上下文丢失或内存溢出的问题，限制了它们在长格式、实时场景中的有效性。

Method: Vista的创新包括三个方面：1) 场景感知分割 - 动态将传入帧聚类为时间和视觉上连贯的场景单元；2) 场景感知压缩 - 将每个场景压缩为紧凑的token表示存储在GPU内存中，同时将全分辨率帧卸载到CPU内存；3) 场景感知召回 - 在接收到查询时选择性召回相关场景并重新整合到模型输入中。

Result: 在StreamingBench上的广泛实验表明，Vista实现了最先进的性能，为现实世界的流式视频理解建立了强大的基准。

Conclusion: Vista是一个模型无关的框架，可以与各种视觉语言骨干无缝集成，在不影响延迟或内存效率的情况下实现长上下文推理，为流式视频问答提供了高效且可扩展的解决方案。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [119] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: TriC-Motion提出了一种基于扩散模型的三域因果文本到运动生成框架，通过时空频三域联合建模与因果干预，解决了现有方法在跨域联合优化和噪声解耦方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动生成方法主要关注时空建模或独立频域分析，缺乏跨时空频三域的统一联合优化框架，导致无法同时利用所有域的信息，生成质量受限。此外，运动生成框架中噪声引起的运动无关线索与有益特征纠缠，导致运动失真。

Method: 提出TriC-Motion框架，包含三个核心建模模块：时间运动编码、空间拓扑建模和混合频率分析。通过评分引导的三域融合模块整合三域有价值信息，确保时间一致性、空间拓扑、运动趋势和动力学。此外，设计了基于因果的反事实运动解耦器，暴露运动无关线索以消除噪声，解耦各域的真实建模贡献。

Result: 在HumanML3D数据集上取得了出色的性能，R@1达到0.612，优于现有最先进方法。实验结果表明该框架能够生成高保真、连贯、多样且与文本对齐的运动序列。

Conclusion: TriC-Motion通过时空频三域联合建模与因果干预，有效解决了现有文本到运动生成方法的局限性，实现了高质量的运动生成，为多域联合优化和噪声解耦提供了新思路。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [120] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 使用2D姿态估计对真实世界交通视频中的行人手势进行分类，将手势分为停止、通行、感谢问候和无手势四类，通过提取76个静态和动态特征，实现了87%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 手势是交通中非语言交流的关键组成部分，有助于行人-驾驶员互动，但自动驾驶车辆难以解释这些手势。当正式交通规则不足时，这个问题更加明显。

Method: 使用2D姿态估计框架处理WIVW数据集的真实世界视频序列，将手势分为四类（停止、通行、感谢问候、无手势），从归一化的关键点中提取76个静态和动态特征。

Result: 分析表明手部位置和运动速度在区分手势类别时特别具有判别力，实现了87%的分类准确率。

Conclusion: 这些发现不仅提高了自动驾驶系统的感知能力，还有助于更广泛地理解交通环境中的行人行为。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [121] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 该研究探讨了光照变化对多类别食品识别系统的影响，通过合成光照增强数据集和跨数据集评估，提出了提升光照鲁棒性的方法。


<details>
  <summary>Details</summary>
Motivation: 现实环境中的视觉食品识别系统（如自动传送带检测）对光照变化引起的领域偏移高度敏感。现有研究通常局限于单一食品类别或受控环境，且大多数公共食品数据集缺乏明确的光照标注，需要研究光照鲁棒性以提升实际部署的可靠性。

Method: 使用Food-101和Fruits-360两个广泛采用的数据集，通过系统变化光温和强度构建合成光照增强数据集，进行跨数据集评估和领域泛化分析，特别关注苹果类等光照敏感目标类别。

Result: 实验结果显示光照变化导致显著的准确率下降，而光照感知增强方法显著提升了在领域偏移下的识别鲁棒性，同时保持了实时性能。

Conclusion: 研究强调了光照鲁棒性在食品识别系统中的重要性，为在现实世界检测场景中部署可靠的食品识别系统提供了实用见解，光照感知增强是提升系统稳健性的有效方法。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [122] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: Octopus框架通过重组现有rollouts合成密集的自校正示例，解决了VLMs中自校正学习信号稀疏的问题，显著提升了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在视觉语言模型中学习自校正行为时面临挑战，因为有效的自校正行为很少出现，导致学习信号极其稀疏。

Method: 提出了correction-specific rollouts（Octopus）框架：1）通过重组现有rollouts合成密集的自校正示例；2）引入响应掩码策略，将自校正与直接推理解耦；3）构建了具有可控自校正能力的Octopus-8B推理VLM。

Result: 在7个基准测试中，Octopus-8B在开源VLMs中达到最先进性能，比最佳RLVR基线高出1.0分，同时每个训练步骤仅需0.72倍的时间。

Conclusion: Octopus框架通过rollout重组和响应掩码策略，有效解决了VLMs中自校正学习的稀疏信号问题，显著提升了训练效率和模型性能。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [123] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: GeoFocus是一个用于解决几何问题的多模态模型框架，通过关键局部感知器和VertexLang拓扑语言，在多个几何数据集上实现了4.7%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 几何问题解决对大型多模态模型具有挑战性，需要同时关注全局形状识别和基于几何理论的局部关系。现有方法在关键局部特征覆盖和拓扑表示效率方面存在不足。

Method: 提出GeoFocus框架，包含两个核心模块：1) 关键局部感知器，通过13个基于理论的感知模板自动识别和强调关键局部结构；2) VertexLang，一种紧凑的拓扑形式语言，通过顶点坐标和连接关系编码全局图形。

Result: 在Geo3K、GeoQA和FormalGeo7K数据集上，GeoFocus比领先的专业模型准确率提升4.7%；关键局部特征覆盖提升61%；全局感知训练时间减少20%；在MATHVERSE中表现出更好的视觉条件鲁棒性。

Conclusion: GeoFocus通过结合理论驱动的局部感知和高效的拓扑表示，显著提升了多模态模型在几何问题解决方面的性能，为解决复杂几何推理问题提供了有效框架。

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [124] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

TL;DR: 提出了一种基于双网格离散化的自动正则化参数选择方法，通过反馈控制算法动态调整正则化强度，使迭代重建达到两个网格上重建结果足够相似的最小参数值。


<details>
  <summary>Details</summary>
Motivation: X射线断层扫描中的图像重建是一个不适定逆问题，特别是在数据有限的情况下。正则化是必要的，但其效果取决于正则化参数的选择，该参数需要在数据保真度和先验信息之间取得平衡。

Method: 提出了一种基于两个不同计算离散化的自动参数选择方法。使用反馈控制算法动态调整正则化强度，驱动迭代重建朝着使两个网格上的重建结果达到足够相似的最小参数值发展。

Result: 该方法在实际断层扫描数据上证明了其有效性。

Conclusion: 提出的双网格离散化方法能够自动选择正则化参数，解决了传统方法中参数选择困难的问题，在实际应用中表现出良好的效果。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [125] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

TL;DR: 提出基于稀疏单目图的热成像SLAM系统，利用可见光数据训练的通用学习特征（SuperPoint检测器和LightGlue匹配器），通过预处理增强热图像适应性，改进SLAM模块处理稀疏和异常匹配，并引入置信度加权因子图提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 热成像在视觉退化环境（如低光照、烟雾、恶劣天气）中具有实用价值，但热图像通常存在纹理少、对比度低、噪声高等问题，使得基于特征的SLAM变得复杂。现有方法需要针对特定热数据集进行训练，而高质量热数据稀缺。

Method: 1. 使用在可见光谱大规模数据上训练的SuperPoint检测器和LightGlue匹配器，实现跨域泛化；2. 引入预处理流程增强热图像输入适应性；3. 修改核心SLAM模块以处理稀疏和异常特征匹配；4. 将SuperPoint的关键点置信度分数整合到置信度加权因子图中。

Result: 在公开热数据集上的评估表明，所提系统实现了可靠的性能，无需针对特定数据集进行训练或微调特征检测器，解决了热数据稀缺的问题。

Conclusion: 该系统为热成像SLAM提供了一种实用解决方案，通过利用可见光训练的通用学习特征和适应性改进，在视觉退化环境中实现了可靠的定位与建图，代码将在发表后公开。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [126] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 提出TIBR4D框架，通过两阶段迭代边界精炼实现动态4D高斯场景中的高效无学习对象分割，提升边界清晰度和处理遮挡能力。


<details>
  <summary>Details</summary>
Motivation: 动态4D高斯场景中的对象级分割面临复杂运动、遮挡和模糊边界的挑战，现有基于阈值的一次性方法难以处理这些问题。

Method: 提出TIBR4D框架：1) 迭代高斯实例追踪(IGIT)在时间片段级逐步精炼高斯到实例的概率；2) 帧级高斯渲染范围控制(RCC)抑制边界附近不确定高斯；3) 时间分割合并策略平衡身份一致性和动态感知。

Result: 在HyperNeRF和Neu3D数据集上实验表明，相比SOTA方法，本方法能生成边界更清晰、更准确的对象高斯点云，且效率更高。

Conclusion: TIBR4D框架通过两阶段迭代边界精炼和自适应时间分割策略，有效解决了动态4D高斯场景中的对象分割问题，在准确性和效率上均优于现有方法。

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [127] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: GOT-Edit：一种在线跨模态模型编辑方法，通过将几何感知线索集成到通用目标跟踪器中，结合2D语义与3D几何推理提升跟踪性能


<details>
  <summary>Details</summary>
Motivation: 人类感知在2D视频流中进行有效目标跟踪时，隐式地结合了先验3D知识和语义推理。相比之下，大多数通用目标跟踪方法主要依赖目标的2D特征及其周围环境，忽略了3D几何线索，这使得它们容易受到部分遮挡、干扰物以及几何和外观变化的影响。

Method: GOT-Edit采用在线跨模态模型编辑方法，从预训练的视觉几何基础Transformer中提取特征，仅从少量2D图像推断几何线索。通过零空间约束更新进行在线模型编辑，无缝结合几何和语义信息，在保持语义判别能力的同时融入几何信息。

Result: 在多个通用目标跟踪基准测试上的广泛实验表明，GOT-Edit实现了卓越的鲁棒性和准确性，特别是在遮挡和杂乱场景下，为结合2D语义与3D几何推理进行通用目标跟踪建立了新范式。

Conclusion: GOT-Edit通过在线模型编辑将几何感知线索集成到通用目标跟踪器中，有效解决了传统方法忽略3D几何线索的问题，在遮挡和杂乱场景下表现出色，为2D语义与3D几何推理的结合提供了新思路。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [128] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

TL;DR: SemiNFT是一个基于扩散变换器的色彩润色框架，通过模仿人类艺术训练轨迹（从刚性模仿到直觉创作）实现参考图像色彩预设的智能迁移。


<details>
  <summary>Details</summary>
Motivation: 传统参考式色彩润色方法仅依赖像素级统计进行全局色彩映射，缺乏对语义上下文和人类美学的真正理解，无法实现智能化的色彩调整。

Method: 采用两阶段训练：1) 使用配对三元组学习基本结构保留和色彩映射技能；2) 在无配对数据上进行强化学习以培养细腻的美学感知，并设计混合在线-离线奖励机制防止灾难性遗忘。

Result: 在标准预设迁移基准测试中优于现有最先进方法，在黑白照片着色和跨域（动漫到照片）预设迁移等零样本任务中表现出色，证实了其超越简单统计匹配的美学理解能力。

Conclusion: SemiNFT通过模仿人类艺术学习过程，实现了从机械模仿到直觉创作的色彩润色能力，达到了超越传统统计匹配方法的复杂美学理解水平。

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [129] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

TL;DR: 本文回顾了中国音视频编码标准工作组（AVS）制定的第一代点云压缩标准AVS PCC，从技术特点和性能比较两个角度进行分析。


<details>
  <summary>Details</summary>
Motivation: 点云作为重要的3D数据表示格式，在沉浸式媒体、自动驾驶、数字遗产保护等领域有广泛应用价值，但其大数据量给传输和存储带来挑战。虽然MPEG已制定了G-PCC和V-PCC两个标准，但中国AVS工作组也开发了自己的点云压缩标准AVS PCC，该标准采用了不同于其他标准的新编码工具和技术。

Method: 从两个视角回顾AVS PCC标准：1）相关技术分析，介绍AVS PCC采用的新编码工具和技术；2）性能比较，将AVS PCC与其他点云压缩标准进行对比评估。

Result: AVS PCC作为中国自主制定的第一代点云压缩标准，采用了与MPEG标准不同的新编码工具和技术，在点云压缩领域提供了另一种标准化解决方案。

Conclusion: AVS PCC标准的制定为中国在点云压缩领域提供了自主的技术标准，该标准采用了创新的编码工具，与其他国际标准形成差异化，有助于推动点云压缩技术的多样化发展和实际应用部署。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [130] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

TL;DR: Inspiration Seeds：一个无需文本提示的生成框架，通过两张输入图像生成多样化视觉组合，支持创意探索阶段的可视化构思


<details>
  <summary>Details</summary>
Motivation: 现有生成模型主要针对精心设计的文本提示进行优化，不支持创意形成前的开放式视觉探索。设计师通常从松散连接的视觉参考中寻找灵感，需要能够揭示输入图像间潜在关系的方法。

Method: 使用CLIP稀疏自编码器提取CLIP潜在空间中的编辑方向，通过纯视觉手段分解视觉方面，构建合成三元组进行前馈训练。给定两张输入图像，模型生成多样化且视觉连贯的组合，无需用户指定文本提示。

Result: 模型能够生成揭示输入图像间潜在关系的多样化视觉组合，支持快速直观的视觉重组，适用于创意工作的早期和模糊阶段。

Conclusion: Inspiration Seeds框架将图像生成从最终执行转向探索性构思，通过消除对语言的依赖，支持创意过程中的视觉构思，特别适合早期创意探索阶段。

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [131] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: LV-RAE是一种表示自编码器，通过增强语义特征的低级信息来提升潜在扩散模型的重建保真度，同时保持语义分布对齐，并通过解码器鲁棒性微调和潜在平滑来改善生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用视觉基础模型作为图像编码器来提升潜在扩散模型的生成性能，但这些语义特征缺乏低级信息（如颜色和纹理），导致重建保真度下降，成为进一步扩展LDMs的主要瓶颈。

Method: 提出LV-RAE表示自编码器，增强语义特征中缺失的低级信息；通过分析发现信息丰富的潜在表示使解码器对潜在扰动敏感，因此微调解码器以提高鲁棒性，并通过受控噪声注入平滑生成的潜在表示。

Result: 实验表明LV-RAE显著提高了重建保真度，同时保持了语义抽象能力，并实现了强大的生成质量。

Conclusion: LV-RAE通过增强语义特征的低级信息解决了潜在扩散模型重建保真度不足的问题，并通过提高解码器鲁棒性和潜在平滑技术进一步提升了生成质量，为扩展LDMs提供了有效解决方案。

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [132] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

TL;DR: 该论文研究Vision Transformers中全局（CLS token）和局部（patch token）特征学习的冲突问题，提出通过专门化处理路径分离两类token的计算流程，显著提升密集预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers通常将可学习的[CLS]类token与patch token一起处理，尽管它们性质不同。作者发现标准归一化层在这两类token之间引入了隐式区分，导致全局和局部特征学习存在摩擦，需要专门化处理来改善密集预测任务的表示质量。

Method: 通过分析类token和patch token之间的交互，提出专门化处理路径，特别是在归一化层和早期query-key-value投影中，有选择地解耦两类token的计算流程。这种方法针对性地分离了全局和局部特征的处理。

Result: 实验显示在标准基准测试中分割性能提升超过2 mIoU点，同时保持强大的分类准确性。提出的修改仅增加8%的参数，没有额外的计算开销。通过全面的消融研究，揭示了哪些架构组件从专门化中获益最多，以及该方法如何在不同模型规模和学习框架中泛化。

Conclusion: 通过专门化处理路径分离Vision Transformers中类token和patch token的计算流程，可以有效改善密集预测任务的表示质量，在分割性能上获得显著提升，同时保持分类准确性，且计算开销很小。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [133] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 提出基于深度学习的模型，使用低分辨率预扫描缩略图预测组织固定类型，实现快速高通量的质量控制，比现有方法快400倍。


<details>
  <summary>Details</summary>
Motivation: 病理实验室中组织固定类型的手动标注容易出错，影响下游分析和诊断准确性。现有方法需要全分辨率全切片图像，限制了高通量质量控制的扩展性。

Method: 开发深度学习模型，利用低分辨率预扫描缩略图预测福尔马林固定石蜡包埋（FFPE）和冰冻切片（FS）的固定类型。模型在TUM病理研究所的1200张WSI上训练，并在TCGA、奥格斯堡和雷根斯堡的数据集上评估。

Result: 模型在TCGA数据集上AUROC达到0.88，比可比预扫描方法提高4.8%。在雷根斯堡和奥格斯堡数据集上AUROC为0.72，揭示了扫描仪引起的域偏移挑战。处理每张切片仅需21毫秒，比现有高倍率全分辨率方法快400倍。

Conclusion: 该方法为检测标注错误提供了高效解决方案，无需依赖高倍率扫描，是病理高通量工作流程中有价值的质量控制工具。未来工作将改进模型对更多扫描仪类型的泛化能力。

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [134] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

TL;DR: WiFlow是一个基于WiFi信号的连续人体姿态估计框架，采用编码器-解码器架构，在自收集数据集上达到97.00% PCK@20和99.48% PCK@50的精度，模型参数仅4.82M，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 人体姿态估计是物联网智能感知的基础，现有WiFi方法在连续运动处理和高计算开销方面存在不足。WiFlow旨在解决这些问题，为实际WiFi姿态估计建立新的性能基准。

Method: 采用编码器-解码器架构：编码器使用时序和非对称卷积捕捉CSI的时空特征，保持信号原始序列结构；通过轴向注意力精炼人体关键点特征并捕捉结构依赖；解码器将编码的高维特征映射为关键点坐标。

Result: 在5名受试者执行8种日常活动的36万个同步CSI-姿态样本数据集上，WiFlow达到PCK@20为97.00%，PCK@50为99.48%，平均每关节位置误差0.008米，模型参数仅4.82M，显著降低计算复杂度。

Conclusion: WiFlow为实际WiFi人体姿态估计建立了新的性能基准，在保持高精度的同时显著降低了模型复杂度和计算成本，代码和数据集已开源。

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [135] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: ALIVE是一个音频-视频生成模型，通过改进MMDiT架构和构建高质量数据集，将预训练的文本到视频模型升级为类似Sora的音频-视频生成和动画能力。


<details>
  <summary>Details</summary>
Motivation: 视频生成正朝着统一的音频-视频生成发展，现有文本到视频模型缺乏音频生成和动画能力，需要开发能够同时生成同步音频和视频内容的模型。

Method: 1) 扩展MMDiT架构，增加联合音频-视频分支，包含TA-CrossAttn用于时序对齐的跨模态融合和UniTemp-RoPE用于精确的音频-视觉对齐；2) 设计全面的数据管道，包括音频-视频字幕生成和质量控制，收集百万级高质量微调数据；3) 引入新的基准测试进行综合评估。

Result: ALIVE在百万级高质量数据上持续预训练和微调后，表现出色，持续优于开源模型，匹配或超越最先进的商业解决方案。

Conclusion: ALIVE成功将文本到视频模型升级为音频-视频生成和动画模型，通过详细的实现方案和基准测试，为社区开发音频-视频生成模型提供了高效支持。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [136] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

TL;DR: 论文提出视觉理解的核心是压缩问题，通过编码器架构与视频信息论原理对齐，采用稀疏计算聚焦信号熵区域，实现效率与准确性的正相关


<details>
  <summary>Details</summary>
Motivation: 现代视觉架构偏离了信息论基本原则，视觉信号高度冗余而判别信息稀疏，当前模型均匀处理密集像素网格，浪费大量计算在静态背景而非预测残差上

Method: 采用Codec Patchification技术，仅处理3.1%-25%富含信号熵的区域；使用共享3D RoPE统一时空推理；通过大规模聚类判别目标训练，捕获对象持久性和运动动态

Result: 在16个图像、视频和文档理解基准测试中，OV-Encoder使用更少视觉token和预训练数据，仍优于Qwen3-ViT和SigLIP2；视频理解任务平均提升4.1%

Conclusion: 效率与准确性呈正相关，Codec对齐的补丁级稀疏性是基础原则，使OV-Encoder成为下一代视觉通用模型的可扩展引擎

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [137] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TL;DR: 提出Omni Dense Captioning任务，构建高质量基准数据集OmniDCBench和训练数据集TimeChatCap-42K，开发TimeChat-Captioner-7B模型，在密集描述生成和下游任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述任务通常生成简短、概括性的描述，缺乏连续、细粒度和结构化的音频-视觉叙事。需要一种能够生成类似电影剧本的详细时间戳描述的方法，使读者能够逐场景生动想象视频内容。

Method: 1) 提出六维结构模式创建"脚本式"描述；2) 构建高质量人工标注基准OmniDCBench；3) 提出SodaM评估指标，评估时间感知的详细描述并缓解场景边界模糊；4) 构建训练数据集TimeChatCap-42K；5) 开发TimeChat-Captioner-7B模型，通过SFT和GRPO与任务特定奖励进行训练。

Result: TimeChat-Captioner-7B在密集描述生成上超越Gemini-2.5-Pro，达到SOTA性能。生成的密集描述显著提升下游任务能力：音频-视觉推理（DailyOmni和WorldSense）和时间定位（Charades-STA）。

Conclusion: 提出的Omni Dense Captioning任务、基准数据集、评估指标和模型为生成连续、细粒度、结构化的音频-视觉叙事提供了有效解决方案，在密集描述生成和下游任务上均表现出色，所有资源将公开可用。

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [138] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 该论文首次对视觉语言模型（VLM）的适应过程进行机制性分析，通过阶段式模型差异技术揭示语言模型如何学习"看"的能力，识别出在微调过程中出现或重新定向的视觉偏好特征，并追踪这些特征到特定注意力头。


<details>
  <summary>Details</summary>
Motivation: 尽管当代视觉语言模型在各种任务上表现出色，但语言主干表示在多模态训练中如何适应以及视觉特定能力何时出现仍不清楚。需要理解预训练语言模型如何获得视觉基础能力。

Method: 使用阶段式模型差异技术，隔离多模态微调期间引入的表征变化。识别视觉偏好特征，通过空间提示的受控偏移揭示这些特征如何编码空间关系，并追踪这些特征的因果激活到特定的注意力头。

Result: 发现阶段式模型差异技术能够揭示时空基础的多模态特征何时何地出现，显示视觉基础如何重塑先前仅基于文本的特征，识别出可靠编码空间关系的视觉偏好特征子集。

Conclusion: 该方法增强了多模态训练的可解释性，为理解和改进预训练语言模型如何获得视觉基础能力提供了基础，揭示了语言模型学习"看"的机制。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [139] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 本研究探索了在无监督、零样本条件下，利用预训练基础模型进行CT和MR图像解剖区域自动识别的方法，提出了三种无需训练的流程，其中基于分割的规则系统表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前解剖区域识别主要依赖不可靠的DICOM元数据或监督学习，限制了在真实场景中的应用。本研究旨在探索是否可以利用预训练基础模型的知识，实现完全零样本的解剖区域检测。

Method: 提出了三种无需训练的流程：1）基于预训练多器官分割模型的规则系统；2）基于放射科医生定义规则的多模态大语言模型；3）结合视觉输入和解剖证据的分割感知多模态大语言模型。

Result: 在887个CT和MR扫描数据上评估，基于分割的规则方法表现最佳，加权F1分数分别为0.947（CT）和0.914（MR），在多模态和非典型扫描覆盖下均表现稳健。多模态大语言模型在视觉特征明显区域表现良好，但分割感知多模态大语言模型存在根本性限制。

Conclusion: 基于预训练分割模型的规则系统能够在零样本条件下实现可靠的解剖区域识别，为医疗影像工作流提供了不依赖DICOM元数据或监督学习的实用解决方案。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [140] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

TL;DR: FusionEdit是一个无需训练的文本引导图像编辑框架，通过语义差异自动识别编辑区域，使用距离感知潜在融合和统计注意力融合实现精确可控的编辑，避免硬掩码边界带来的伪影。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用显式二进制掩码约束编辑，但硬掩码边界会引入伪影并降低可编辑性，需要解决这些问题以实现更精确可控的图像编辑。

Method: 1) 通过测量源提示词和目标提示词语义差异自动识别编辑和保留区域；2) 使用距离感知潜在融合生成软掩码，结合总变差损失确保平滑过渡；3) 在DiT注意力层中使用AdaIN调制进行统计注意力融合，增强可编辑性同时保持全局一致性。

Result: 大量实验表明，FusionEdit在文本引导图像编辑任务上显著优于现有最先进方法，能够实现更自然、精确的编辑效果。

Conclusion: FusionEdit通过软掩码生成和统计注意力融合，有效解决了硬掩码边界带来的伪影问题，实现了无需训练的高质量图像编辑，在精确性和可控性方面表现出色。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [141] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 提出了一种结合2D和3D模型优势的混合深度学习框架，用于从欠采样CT体积中去除伪影，在保持计算效率的同时提高图像质量和层间一致性。


<details>
  <summary>Details</summary>
Motivation: 欠采样CT扫描可以减少采集时间和辐射暴露，但会引入伪影，降低图像质量和诊断价值。需要一种既能有效去除伪影又计算高效的方法。

Method: 采用两阶段混合框架：首先使用2D U-Net处理单个CT切片提取特征图，然后将这些切片特征图堆叠成体积，输入到3D解码器中，利用跨切片上下文信息预测无伪影的3D CT体积。

Result: 该方法在冠状面和矢状面方向上显著提高了层间一致性，同时保持了较低的计算开销，为高质量3D CT图像后处理提供了稳健高效的解决方案。

Conclusion: 提出的混合框架平衡了2D处理的计算效率和3D建模的体积一致性，是高质量3D CT图像后处理的稳健高效解决方案，代码已在GitHub开源。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [142] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

TL;DR: 提出CLIP引导对齐(CGA)框架，通过检测、建模和缓解类别混淆来解决无源域自适应中的伪标签噪声问题，特别适用于细粒度场景


<details>
  <summary>Details</summary>
Motivation: 现有无源域自适应方法在细粒度场景中因类别间相似性导致伪标签噪声严重，特别是存在不对称和动态的类别混淆问题，现有方法忽视这些混淆模式

Method: CGA包含三个部分：MCA检测目标域中的方向性混淆对；MCC利用CLIP构建混淆感知的文本提示；FAM建立混淆引导的特征库并通过对比学习对齐CLIP和源模型的特征空间

Result: 在多个数据集上的实验表明，CGA在无源域自适应方法中表现优异，特别是在易混淆和细粒度场景中取得显著提升

Conclusion: 明确建模类别间混淆对于有效的无源域自适应至关重要，CGA框架通过检测和缓解混淆显著提升了目标域的分类性能

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [143] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

TL;DR: HATCH训练框架通过补丁级空间对齐和动作-答案推理，提升多模态大语言模型在多图像空间推理任务中的表现，超越同规模基线模型并接近更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在单图像空间推理方面取得进展，但在需要整合多个视角信息的多图像空间推理方面仍面临挑战。人类通过跨视角对应和逐步视角变换两种机制解决此类任务，但现有研究仅部分且隐式地结合这些机制，缺乏对两者的显式监督。

Method: 提出HATCH训练框架，包含两个互补目标：1) 补丁级空间对齐，鼓励不同视角中空间对应区域的补丁表示对齐；2) 动作-答案推理，要求模型在预测最终答案前生成显式的视角转换动作。

Result: 在三个基准测试上的实验表明，HATCH始终以明显优势超越同规模基线模型，并与更大模型取得竞争性结果，同时保持单图像推理能力。

Conclusion: HATCH通过显式监督跨视角对应和逐步视角变换，有效提升了多模态大语言模型在多图像空间推理任务中的性能，为复杂空间推理问题提供了有效解决方案。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [144] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Instance-Disentangled Attention的新机制，用于解决现有流匹配模型在多实例编辑场景中的局限性，能够实现单次传递的实例级图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的图像编辑器主要支持全局或单指令编辑，在多实例场景中表现不佳，即当需要同时编辑参考输入的多个部分时，会产生语义干扰。这种局限性源于全局条件化的速度场和联合注意力机制，导致并发编辑相互纠缠。

Method: 提出了Instance-Disentangled Attention机制，该机制通过划分联合注意力操作，在速度场估计期间强制绑定实例特定的文本指令和空间区域，从而实现编辑的解耦和局部化。

Result: 实验结果表明，该方法在自然图像编辑和新引入的文本密集信息图基准测试中均表现出色，能够促进编辑解耦和局部性，同时保持全局输出的一致性。

Conclusion: 提出的Instance-Disentangled Attention机制有效解决了多实例编辑中的语义干扰问题，实现了单次传递的实例级编辑，为基于流的图像编辑提供了更强大的多实例处理能力。

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [145] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: MVAnimate是一个利用多视角先验信息生成高质量2D和3D角色动画的新框架，解决了现有动画生成算法输出质量低和训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前角色动画生成需求激增，但现有基于2D或3D人体姿态建模的算法面临输出质量低和训练数据不足的问题，无法生成高质量动画视频。

Method: 提出MVAnimate框架，基于多视角先验信息合成动态角色的2D和3D信息，利用多视角先验生成时间一致和空间连贯的动画输出，并优化目标角色的多视角视频质量。

Result: 实验结果表明该方法在多样化数据集上表现出色，能够处理各种运动模式和外观，相比现有动画方法有显著改进。

Conclusion: MVAnimate通过整合多视角信息有效提升了角色动画生成质量，为高质量动画视频生成提供了新解决方案。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [146] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

TL;DR: 提出了一种基于符号吠陀计算的确定性CPU导向说话头生成框架，用于教育资源受限环境下的实时语音驱动头像合成


<details>
  <summary>Details</summary>
Motivation: 当前说话头生成方法依赖GPU渲染、大训练集或高容量扩散模型，难以在离线或资源受限的教育环境中部署，需要轻量级CPU可行方案

Method: 使用符号吠陀计算：语音转时间对齐音素流，映射到紧凑视素库，通过受吠陀经Urdhva Tiryakbhyam启发的符号协同发音生成平滑视素轨迹，轻量2D渲染器进行ROI扭曲和嘴部合成

Result: 在仅CPU执行下实现了可接受的唇同步质量，显著降低计算负载和延迟，支持在低端硬件上运行实用教育头像

Conclusion: 提出的符号吠陀计算框架为资源受限教育环境提供了可行的说话头生成方案，平衡了质量与计算效率

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [147] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 提出了一种用于检测受电弓-接触网界面电弧的多模态框架，结合高分辨率图像和力测量数据，通过MultiDeepSAD算法和特定伪异常生成技术，显著提高了电弧检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受电弓-接触网界面的电弧现象对电气化铁路系统构成严重风险，包括加速接触部件磨损、系统性能下降和潜在服务中断。然而，电弧检测面临瞬态特性、噪声环境、数据稀缺以及与其他瞬态现象难以区分等挑战。

Method: 1. 构建了两个包含同步视觉和力测量的电弧检测数据集：一个来自瑞士联邦铁路公司(SBB)的真实数据，另一个来自公开视频和模拟力数据的合成数据集；2. 提出了MultiDeepSAD算法，扩展DeepSAD算法以处理多模态数据并采用新的损失函数；3. 针对每种数据类型设计了特定的伪异常生成技术，如图像中的合成电弧伪影和模拟力不规则性，以增强训练数据。

Result: 通过大量实验和消融研究，证明该框架显著优于基线方法，即使在领域偏移和真实电弧观测数据有限的情况下，对真实电弧事件也表现出增强的敏感性。

Conclusion: 提出的多模态框架通过结合视觉和力测量数据，以及创新的伪异常生成技术，有效解决了受电弓-接触网界面电弧检测的挑战，为电气化铁路系统的可靠运行提供了更准确和鲁棒的电弧检测解决方案。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [148] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

TL;DR: MOVA是一个开源的音频-视频联合生成模型，采用混合专家架构，能够生成高质量、同步的视听内容，包括唇语同步的语音、环境感知音效和内容对齐的音乐。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型大多忽视音频组件，现有方法依赖级联管道导致成本增加、误差累积和质量下降。虽然Veo 3和Sora 2等系统强调同时生成的价值，但联合多模态建模面临架构、数据和训练挑战，且现有系统闭源限制了领域进展。

Method: 采用混合专家架构，总参数量320亿，推理时激活180亿参数。支持图像-文本到视频-音频生成任务，提供高效推理、LoRA微调和提示增强的完整代码库。

Result: 开发出能够生成高质量同步视听内容的开源模型，包括逼真的唇语同步语音、环境感知音效和内容对齐音乐。

Conclusion: 通过开源模型权重和代码，MOVA旨在推动音频-视频联合生成研究，促进创作者社区发展，解决当前闭源系统的局限性。

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [149] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 提出了一种半监督师生框架，结合不确定性感知伪标签教师和基于置信度的渐进课程学习，用于MRI脑肿瘤分割，在有限标注下实现高效学习。


<details>
  <summary>Details</summary>
Motivation: MRI脑肿瘤分割面临标注成本高和数据异质性（不同扫描仪和站点）的挑战，需要开发在有限监督下鲁棒的半监督学习方法。

Method: 采用师生框架：教师模型生成概率掩码和逐像素不确定性；学生模型通过基于置信度的渐进课程学习，按图像级置信度分阶段引入未标注数据，使用双损失目标从高置信区域学习并遗忘低置信区域，通过一致性优化改进伪标签质量。

Result: 在BraTS 2021上，验证DSC从0.393（10%数据）提升到0.872（100%），早期阶段提升最大；教师模型DSC达0.922，学生在肿瘤亚区（如NCR/NET 0.797和Edema 0.980）超越教师，特别是在教师失败的增强类别上恢复DSC 0.620。

Conclusion: 置信度驱动的课程学习和选择性遗忘在有限监督和噪声伪标签下提供了鲁棒的分割性能，展示了数据效率和模型改进潜力。

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [150] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video 2是一个可扩展且计算高效的视频生成与编辑模型，通过连接预训练的多模态大语言模型和视频扩散模型，利用MLLM的理解推理能力生成明确的目标描述来指导生成过程，实现高质量的视频生成和复杂编辑任务。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成和编辑中复杂组合指令的遵循问题，通过利用多模态大语言模型的理解推理能力来更好地解释用户指令，从而提升复杂组合编辑的性能。

Method: 1. 利用预训练MLLM生成明确的目标描述来解释用户指令；2. 开发轻量级适配器将多模态条件标记注入预训练的文本到视频扩散模型；3. 在精心策划的训练数据上扩展到140亿参数的视频扩散模型。

Result: 在FiVE基准测试中展现出遵循复杂组合指令的卓越能力，在VBench基准测试中实现竞争性或更优的视频生成质量，支持高质量文本到视频生成和各种视频编辑任务。

Conclusion: Omni-Video 2通过连接MLLM和视频扩散模型，以参数高效的方式实现了高质量的视频生成和复杂编辑，在遵循复杂指令方面表现出色，为统一的视频生成和编辑提供了有效解决方案。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [151] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

TL;DR: 开发了一个统一的基础模型，通过对比视觉表示学习和视觉语言对齐，实现任意到所有MRI合成，提高鼻咽癌放疗规划准确性。


<details>
  <summary>Details</summary>
Motivation: MRI对鼻咽癌放疗至关重要，但实际临床中常因患者不适、扫描时间长、成本高等因素导致模态不完整，影响放疗规划准确性。传统MRI合成方法存在模态特定性、解剖适应性有限、缺乏临床可解释性等问题。

Method: 开发统一基础模型，整合对比视觉表示学习和视觉语言对齐。使用对比编码器获取模态不变表示，基于CLIP的文本信息解码器进行语义一致合成，支持通过单一统一模型实现任意到所有MRI合成。

Result: 在13个机构的40,825张图像上训练，在26个内部/外部验证站点（15,748张图像）上实现一致高性能（平均SSIM 0.90，PSNR 27），具有优异的合成保真度和对噪声及域偏移的鲁棒性。统一表示还增强了放疗相关下游任务（如分割）的性能。

Conclusion: 这项工作通过利用基础模型桥接技术合成和临床实用性，推进了鼻咽癌护理的数字医学解决方案。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [152] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: VideoVeritas框架通过联合偏好对齐和感知预文本强化学习，结合细粒度感知与事实推理，提升视频生成内容的检测能力，并在新数据集MintVid上验证了其平衡性能。


<details>
  <summary>Details</summary>
Motivation: 视频生成能力的提升带来了安全风险，需要可靠的检测方法。当前多模态大语言模型虽然推理能力强，但细粒度感知能力有限，需要改进。

Method: 提出VideoVeritas框架，整合细粒度感知和事实推理。引入联合偏好对齐和感知预文本强化学习（PPRL），通过通用时空定位和自监督物体计数等感知预任务来增强检测性能。

Result: 实验结果表明，现有方法倾向于偏向表面推理或机械分析，而VideoVeritas在多样化基准测试中实现了更平衡的性能表现。

Conclusion: VideoVeritas框架通过增强感知能力与推理能力的平衡，为视频生成内容检测提供了更有效的解决方案，并通过MintVid数据集促进了该领域的稳健评估。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [153] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

TL;DR: TiFRe是一个文本引导的视频帧缩减框架，通过智能选择关键帧并合并非关键帧信息来降低计算成本，同时保持视频语义完整性。


<details>
  <summary>Details</summary>
Motivation: 视频多模态大语言模型在处理大量视频帧时面临高计算成本问题，传统固定帧率的关键帧选择方法会丢失非关键帧的重要信息，导致性能下降。

Method: 提出TiFRe框架：1) 文本引导帧采样(TFS)：使用LLM根据用户输入生成CLIP风格提示，通过CLIP编码器计算提示与每帧的语义相似度来选择关键帧；2) 帧匹配与合并(FMM)：将非关键帧信息整合到选定的关键帧中，最小化信息损失。

Result: 实验表明TiFRe能有效降低计算成本，同时在视频语言任务上提高性能。

Conclusion: TiFRe通过文本引导的智能帧选择和帧合并机制，在减少计算开销的同时保持了视频语义完整性，为视频MLLMs的高效处理提供了有效解决方案。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [154] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 该论文研究了3D高斯溅射优化中出现的结构模式，揭示了渲染最优参考的统计特性，并分析了密度分层对参数可学习性的影响。


<details>
  <summary>Details</summary>
Motivation: 研究3D高斯溅射（3DGS）在多视角优化中形成的结构模式，理解这些渲染最优参考（RORs）的统计特性，探索决定这些参数的因素，特别是密度分层如何影响参数的可学习性。

Method: 通过分析RORs的统计特性，使用可学习性探针训练预测器从点云重建RORs，应用方差分解分析密度分层对参数耦合的影响，并提出密度感知策略。

Result: 发现RORs具有稳定的统计模式：混合结构的尺度和双峰辐射分布；密度分层导致参数可学习性差异：密集区域参数与几何相关可预测，稀疏区域参数受可见性异质性影响难以预测；方差分解显示稀疏区域几何与外观参数存在协方差主导的耦合。

Conclusion: RORs具有双重特性：在密集区域表现为几何基元（点云足够），在稀疏区域表现为视图合成基元（需要多视角约束）；密度感知策略能提高训练鲁棒性，为自适应平衡前馈预测和渲染优化的系统设计提供架构启示。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [155] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 提出了一种用于植物生长建模的3D高斯流场表示方法，通过时间变化的高斯参数导数来模拟非线性连续生长动态，相比现有方法在图像质量和几何精度上表现更优。


<details>
  <summary>Details</summary>
Motivation: 植物生长建模面临独特挑战：植物会随时间生成新几何结构（扩张、分枝、分化），而现有的运动建模技术（如变形场无法引入新几何，4D高斯溅射限制线性轨迹）不适用于这种问题设置。

Method: 引入3D高斯流场表示，将植物生长建模为高斯参数（位置、尺度、方向、颜色、不透明度）的时间变化导数。通过重建成熟植物并学习反向生长过程来初始化足够的高斯基元，模拟植物的发育历史。

Result: 在多视角植物生长时序数据集上，该方法在图像质量和几何精度方面优于先前方法，为生长中3D结构的外观建模提供了新方法。

Conclusion: 提出的3D高斯流场表示能够有效建模植物生长的非线性连续动态，通过反向生长初始化策略解决了新几何生成问题，为动态植物场景建模提供了创新解决方案。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [156] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: MotionCrafter是一个基于视频扩散的框架，能够从单目视频中联合重建4D几何并估计密集运动，通过新的联合表示和4D VAE实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法强制3D值和潜在变量与RGB VAE潜在变量严格对齐，尽管它们的分布本质不同，这导致次优性能。需要一种更好的方法来学习4D几何和运动的联合表示。

Method: 提出了一种新颖的联合表示方法，将密集3D点图和3D场景流在共享坐标系中表示，并设计了新的4D VAE来有效学习这种表示。采用新的数据归一化和VAE训练策略，更好地传递扩散先验。

Result: 在多个数据集上的实验表明，MotionCrafter在几何重建和密集场景流估计方面都达到了最先进的性能，几何重建提升38.64%，运动重建提升25.0%，且无需任何后优化。

Conclusion: MotionCrafter通过创新的联合表示和4D VAE设计，成功解决了先前方法中分布对齐不当的问题，为单目视频的4D重建和运动估计提供了高效解决方案。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [157] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

TL;DR: WorldArena是一个统一的基准测试，用于系统评估具身世界模型在感知和功能两个维度上的表现，揭示了感知质量与下游任务功能之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前对具身世界模型的评估主要关注感知保真度（如视频生成质量），而忽视了这些模型在下游决策任务中的功能效用，缺乏统一的评估框架。

Method: 提出WorldArena基准测试，通过三个维度评估模型：1) 视频感知质量（16个指标覆盖6个子维度）；2) 具身任务功能（作为数据引擎、策略评估器和动作规划器）；3) 结合主观人类评估。同时提出EWMScore综合指标。

Result: 通过对14个代表性模型的广泛实验，发现感知与功能之间存在显著差距，高视觉质量不一定转化为强大的具身任务能力。

Conclusion: WorldArena为追踪具身AI中真正功能性世界模型的进展提供了一个框架，揭示了当前评估方法的局限性，并强调了需要同时考虑感知和功能维度的评估。

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [158] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: ArcFlow提出了一种新的扩散模型蒸馏框架，通过非线性流轨迹近似教师轨迹，实现2步推理的40倍加速，仅需微调不到5%的参数。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型蒸馏方法使用线性捷径近似教师轨迹，难以匹配随时间步变化的切线方向，导致质量下降。需要一种能捕捉速度演化的非线性轨迹近似方法。

Method: ArcFlow将推理轨迹的底层速度场参数化为连续动量过程的混合，捕捉速度演化并外推形成连续非线性轨迹。该参数化允许对非线性轨迹进行解析积分，避免数值离散误差。通过轻量级适配器在预训练教师模型上进行轨迹蒸馏训练。

Result: 基于Qwen-Image-20B和FLUX.1-dev等大规模模型，ArcFlow仅微调不到5%的原始参数，在2步推理下实现40倍加速，且无明显质量下降。基准测试显示ArcFlow在定性和定量上均有效。

Conclusion: ArcFlow通过非线性流轨迹近似教师轨迹，成功解决了现有蒸馏方法难以匹配速度演化的问题，实现了高质量、高效率的少步扩散模型推理。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [159] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: Raster2Seq将楼层平面图重建视为序列到序列任务，通过自回归解码器将楼层平面元素表示为带标签的多边形序列，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有技术在处理复杂楼层平面图时难以准确生成结构和语义信息，这些平面图通常包含大量房间和不同数量的多边形角落，需要更有效的重建方法。

Method: 将楼层平面图重建框架化为序列到序列任务，使用自回归解码器预测下一个角落点，通过可学习锚点引导注意力机制聚焦信息丰富的图像区域，将房间、门窗等元素表示为带标签的多边形序列。

Result: 在Structure3D、CubiCasa5K和Raster2Graph等标准基准测试中达到最先进性能，同时在包含多样化房间结构和复杂几何变化的WAFFLE数据集上表现出强大的泛化能力。

Conclusion: Raster2Seq方法通过序列到序列框架和自回归机制，能够有效处理复杂楼层平面图的重建任务，为自动化理解和CAD工作流程提供了可靠的结构化矢量图形表示。

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [160] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

TL;DR: WorldCompass是一个用于长时程交互式视频世界模型的RL后训练框架，通过三种创新方法提升世界模型的探索准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的长时程交互式视频世界模型在探索世界时缺乏准确性和一致性，需要一种有效的后训练框架来基于交互信号"引导"模型探索。

Method: 提出了三种核心创新：1）片段级展开策略，在单个目标片段生成和评估多个样本；2）互补奖励函数，设计交互准确性和视觉质量的奖励函数；3）高效RL算法，采用负感知微调策略和各种效率优化。

Result: 在SoTA开源世界模型WorldPlay上的评估表明，WorldCompass在各种场景下显著提高了交互准确性和视觉保真度。

Conclusion: WorldCompass是一个有效的RL后训练框架，能够显著提升长时程交互式视频世界模型的探索能力和生成质量。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [161] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

TL;DR: 本文挑战了视觉生成中连续管道的统治地位，通过研究发现离散与连续方法的性能差距主要源于潜在空间中分配的比特总数，而非离散标记器本身的内在劣势。通过扩大码本规模可以有效弥合这一差距，并提出BAR框架支持任意码本大小，在ImageNet-256上实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 挑战视觉生成领域中连续管道的主导地位，系统研究离散与连续方法之间的性能差距。针对普遍认为离散标记器本质上劣于连续方法的观点，提出新的研究视角。

Method: 提出掩码比特自回归建模（BAR）框架，通过为自回归变换器配备掩码比特建模头，支持任意码本大小。BAR通过逐步生成构成离散标记的比特来预测离散标记，解决了现有离散方法在扩大码本时遇到的性能下降或训练成本过高问题。

Result: BAR在ImageNet-256上实现了新的最先进gFID为0.99，超越了连续和离散范式中的领先方法。同时显著降低了采样成本，收敛速度也比之前的连续方法更快。

Conclusion: 离散与连续方法的性能差距主要源于潜在空间中的比特分配数量（压缩比），而非离散标记器的内在劣势。通过扩大码本规模可以有效弥合这一差距，BAR框架为此提供了可扩展的解决方案，在多个指标上实现了SOTA性能。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [162] [Attractor Patch Networks: Reducing Catastrophic Forgetting with Routed Low-Rank Patch Experts](https://arxiv.org/abs/2602.06993)
*Shashank*

Main category: cs.LG

TL;DR: APN（Attractor Patch Networks）作为Transformer FFN的替代方案，通过基于相似性的路由选择少量专家补丁，实现条件化、上下文特化的非线性变换，在保持竞争力的语言建模性能的同时，显著提升了持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的FFN存在两个实际问题：1）对所有token使用相同计算量，无法根据上下文结构灵活分配容量；2）持续学习时广泛共享的权重更新会导致干扰问题。

Method: 提出APN作为Transformer FFN的即插即用替代方案。它包含一组补丁专家，通过相似性路由器为每个token选择top-k个补丁，每个选中的补丁基于紧凑代码生成低秩残差更新，实现条件化、上下文特化的非线性变换。

Result: 在字符级语言建模任务中，APN达到有竞争力的困惑度（4.57 vs 4.32 PPL）。在持续适应实验中，当适应到偏移领域时，APN相比密集FFN基线的全局微调，在原始领域上获得2.6倍更好的保留能力（11.1 vs 29.4 PPL），在新领域上获得2.8倍更好的适应能力（6.4 vs 17.8 PPL）。

Conclusion: APN作为一种架构原语，不仅保持了Transformer的标准接口，还通过条件化、上下文特化的变换机制，在保持语言建模性能的同时，显著提升了模型的持续学习能力和适应性，为解决FFN的密集性和全局共享问题提供了有效方案。

Abstract: Transformers achieve strong language modeling accuracy, yet their position-wise feed-forward networks (FFNs) are dense, globally shared, and typically updated end to end. These properties create two practical tensions. First, dense FFNs spend the same compute on every token regardless of context, and they allocate capacity uniformly even when language exhibits highly clustered context structure. Second, continual learning, in the sense of updating the model while serving a data stream, often produces interference because a small update touches broadly shared weights.
  We propose Attractor Patch Networks (APN), a plug-compatible replacement for the Transformer FFN. APN is a bank of patch experts. A similarity router selects a small top-k set of patches for each token by matching the token representation to learned prototypes. Each selected patch emits a low-rank residual update conditioned on a compact code. The architecture yields conditional, context-specialized nonlinear transformations while preserving the standard Transformer interface.
  This paper focuses on APN as an architectural primitive. We formalize APN, analyze its expressivity as a piecewise low-rank residual function class, and derive simple interference and stability arguments that make APN naturally compatible with continual learning. In experiments on character-level language modeling, APN achieves competitive perplexity (4.57 vs 4.32 PPL) while enabling dramatically better continual adaptation: when adapting to a shifted domain, APN achieves 2.6 times better retention (11.1 vs 29.4 PPL on the original domain) and 2.8 times better adaptation (6.4 vs 17.8 PPL on the new domain) compared to global fine-tuning of a dense FFN baseline.

</details>


### [163] [Neural Sabermetrics with World Model: Play-by-play Predictive Modeling with Large Language Model](https://arxiv.org/abs/2602.07030)
*Young Jin Ahn,Yiyang Du,Zheyuan Zhang,Haisen Kang*

Main category: cs.LG

TL;DR: 该研究提出Neural Sabermetrics with World Model，一个基于大语言模型的棒球逐球世界模型，能够预测比赛的多方面演化


<details>
  <summary>Details</summary>
Motivation: 传统棒球统计方法虽然能总结长期比赛历史，但无法建立逐球生成的比赛模型，现有方法大多局限于单步预测或事后分析

Method: 将棒球比赛建模为事件的长自回归序列，在超过10年的MLB追踪数据上持续预训练单个LLM，包含700万次投球序列和约30亿个token

Result: 模型在分布内常规赛和分布外季后赛数据上均优于现有基线：正确预测约64%的下一次投球和78%的击球员挥棒决策

Conclusion: LLM可以作为有效的体育世界模型，为棒球比赛提供统一的生成式建模框架

Abstract: Classical sabermetrics has profoundly shaped baseball analytics by summarizing long histories of play into compact statistics. While these metrics are invaluable for valuation and retrospective analysis, they do not define a generative model of how baseball games unfold pitch by pitch, leaving most existing approaches limited to single-step prediction or post-hoc analysis. In this work, we present Neural Sabermetrics with World Model, a Large Language Model (LLM) based play-by-play world model for baseball. We cast baseball games as long auto-regressive sequences of events and continuously pretrain a single LLM on more than ten years of Major League Baseball (MLB) tracking data, comprising over seven million pitch sequences and approximately three billion tokens. The resulting model is capable of predicting multiple aspects of game evolution within a unified framework. We evaluate our model on both in-distribution regular-season data and out-of-distribution postseason games and compare against strong neural baselines from prior work. Despite using a single backbone model, our approach outperforms the performance of existing baselines, (1) correctly predicting approximately 64% of next pitches within a plate appearance and (2) 78% of batter swing decisions, suggesting that LLMs can serve as effective world models for sports.

</details>


### [164] [Escaping Spectral Bias without Backpropagation: Fast Implicit Neural Representations with Extreme Learning Machines](https://arxiv.org/abs/2602.07603)
*Woojin Cho,Junghwan Park*

Main category: cs.LG

TL;DR: ELM-INR：一种免反向传播的隐式神经表示方法，通过局部ELM闭式解和分区统一实现快速稳健重建，并提出BEAM自适应网格细化策略平衡频谱复杂度


<details>
  <summary>Details</summary>
Motivation: 传统INR训练依赖迭代反向传播，在处理非均匀频谱内容时受到频谱偏差限制，需要更快速、稳健的重建方法

Method: 1. 将域分解为重叠子域；2. 每个子域使用极限学习机（ELM）进行闭式拟合；3. 通过分区统一组合局部预测器；4. 提出BEAM自适应网格细化策略平衡频谱复杂度

Result: ELM-INR实现了快速、数值稳健的重建，避免了迭代优化；BEAM策略在容量受限情况下改善了重建质量

Conclusion: ELM-INR提供了一种免反向传播的INR训练方法，结合自适应网格细化策略，能够有效处理非均匀频谱内容，实现快速稳健的重建

Abstract: Training implicit neural representations (INRs) to capture fine-scale details typically relies on iterative backpropagation and is often hindered by spectral bias when the target exhibits highly non-uniform frequency content. We propose ELM-INR, a backpropagation-free INR that decomposes the domain into overlapping subdomains and fits each local problem using an Extreme Learning Machine (ELM) in closed form, replacing iterative optimization with stable linear least-squares solutions. This design yields fast and numerically robust reconstruction by combining local predictors through a partition of unity. To understand where approximation becomes difficult under fixed local capacity, we analyze the method from a spectral Barron norm perspective, which reveals that global reconstruction error is dominated by regions with high spectral complexity. Building on this insight, we introduce BEAM, an adaptive mesh refinement strategy that balances spectral complexity across subdomains to improve reconstruction quality in capacity-constrained regimes.

</details>


### [165] [TransConv-DDPM: Enhanced Diffusion Model for Generating Time-Series Data in Healthcare](https://arxiv.org/abs/2602.07033)
*Md Shahriar Kabir,Sana Alamgeer,Minakshi Debnath,Anne H. H. Ngu*

Main category: cs.LG

TL;DR: TransConv-DDPM是一种改进的生成式AI方法，专门用于生成生物力学和生理时间序列数据，通过结合DDPM、U-Net、多尺度卷积模块和Transformer层来捕捉全局和局部时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 临床领域缺乏真实世界数据阻碍了医学诊断和预防工具中有效AI模型的训练。生理时间序列数据生成面临独特挑战，因其固有的复杂性和变异性。

Method: 提出TransConv-DDPM模型，采用去噪扩散概率模型(DDPM)架构，结合U-Net、多尺度卷积模块和Transformer层，以捕捉时间序列数据的全局和局部时间依赖性。

Result: 在三个不同数据集上评估，与TimeGAN和Diffusion-TS等最先进方法相比表现出色，特别是在SmartFallMM和EEG数据集上能有效捕捉数据点间渐变的时间模式。在SmartFallMM数据集上，添加TransConv-DDPM生成的合成跌倒数据使预测模型F1分数提高13.64%，整体准确率提高14.93%。

Conclusion: TransConv-DDPM能够生成高质量的合成生理时间序列数据，具有实际应用潜力，可解决医学AI应用中数据稀缺问题。

Abstract: The lack of real-world data in clinical fields poses a major obstacle in training effective AI models for diagnostic and preventive tools in medicine. Generative AI has shown promise in increasing data volume and enhancing model training, particularly in computer vision and natural language processing (NLP) domains. However, generating physiological time-series data, a common type in medical AI applications, presents unique challenges due to its inherent complexity and variability. This paper introduces TransConv-DDPM, an enhanced generative AI method for biomechanical and physiological time-series data generation. The model employs a denoising diffusion probabilistic model (DDPM) with U-Net, multi-scale convolution modules, and a transformer layer to capture both global and local temporal dependencies. We evaluated TransConv-DDPM on three diverse datasets, generating both long and short-sequence time-series data. Quantitative comparisons against state-of-the-art methods, TimeGAN and Diffusion-TS, using four performance metrics, demonstrated promising results, particularly on the SmartFallMM and EEG datasets, where it effectively captured the more gradual temporal change patterns between data points. Additionally, a utility test on the SmartFallMM dataset revealed that adding synthetic fall data generated by TransConv-DDPM improved predictive model performance, showing a 13.64% improvement in F1-score and a 14.93% increase in overall accuracy compared to the baseline model trained solely on fall data from the SmartFallMM dataset. These findings highlight the potential of TransConv-DDPM to generate high-quality synthetic data for real-world applications.

</details>


### [166] [AVERE: Improving Audiovisual Emotion Reasoning with Preference Optimization](https://arxiv.org/abs/2602.07054)
*Ashutosh Chaubey,Jiacheng Pang,Maksim Siniukov,Mohammad Soleymani*

Main category: cs.LG

TL;DR: 该论文提出了EmoReAlM基准测试和AVEm-DPO优化方法，用于解决多模态大语言模型在情感理解中的虚假关联和幻觉问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在情感理解任务中存在两个关键挑战：1）情感与无关视听线索之间的虚假关联；2）语言模型主干中的文本先验驱动的视听线索幻觉。需要量化并解决这些问题。

Method: 1）引入EmoReAlM基准测试，用于评估MLLMs的线索-情感关联、幻觉和模态一致性；2）提出AVEm-DPO偏好优化技术，通过构建对虚假关联或幻觉响应的偏好，以及基于文本提示的视听输入对来对齐模型响应；3）包含正则化项惩罚对文本先验的依赖，减轻模态特定线索幻觉。

Result: 在DFEW、RAVDESS和EMER数据集上的实验结果表明，该方法显著提升了参考基线模型的性能，在零样本设置中获得了6-19%的相对性能提升。

Conclusion: 通过提供严格的基准测试和稳健的优化框架，这项工作为情感理解和社会AI领域的MLLMs评估和改进提供了原则性方法。代码、模型和基准测试将在指定网站发布。

Abstract: Emotion understanding is essential for building socially intelligent agents. Although recent multimodal large language models have shown strong performance on this task, two key challenges remain - spurious associations between emotions and irrelevant audiovisual cues, and hallucinations of audiovisual cues driven by text priors in the language model backbone. To quantify and understand these issues, we introduce EmoReAlM, a benchmark designed to evaluate MLLMs for cue-emotion associations, hallucinations and modality agreement. We then propose AVEm-DPO, a preference optimization technique that aligns model responses with both audiovisual inputs and emotion-centric queries. Specifically, we construct preferences over responses exhibiting spurious associations or hallucinations, and audiovisual input pairs guided by textual prompts. We also include a regularization term that penalizes reliance on text priors, thereby mitigating modality-specific cue hallucinations. Experimental results on DFEW, RAVDESS and EMER demonstrate that our method significantly improves the performance of the reference baseline models with 6-19% of relative performance gains in zero-shot settings. By providing both a rigorous benchmark and a robust optimization framework, this work enables principled evaluation and improvement of MLLMs for emotion understanding and social AI. Code, models and benchmark will be released at https://avere-iclr.github.io.

</details>


### [167] [V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning](https://arxiv.org/abs/2602.08043)
*Yiheng Gao,Qin Hua,Zizhong Chen*

Main category: cs.LG

TL;DR: V-ABFT是一种基于方差的自适应阈值算法，用于检测矩阵乘法中的静默数据损坏，相比现有方法显著提高了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的ABFT阈值确定方法存在严重问题：分析边界过于保守，而概率方法如A-ABFT的阈值比实际舍入误差大160-4200倍，导致检测精度不足。

Method: 提出V-ABFT算法，通过直接建模验证差异并利用统计方差估计来获得更紧密的误差边界。该方法仅需O(n)复杂度，使用最大/最小/均值统计量，而A-ABFT需要O(pn)复杂度来寻找p个最大值。

Result: V-ABFT将阈值与实际误差比降低到FP32/FP64的7-20倍和BF16的48-158倍，相比A-ABFT有6-48倍的改进。对于融合核ABFT实现，低精度GEMM可以使用FP32级阈值，实现约1000倍更精细的检测粒度。

Conclusion: V-ABFT在BF16、FP16、FP32和FP64精度下均保持零假阳性率，平台无关，已集成到NPU和GPU的容错GEMM实现中，在各种分布下都表现出有效性。

Abstract: Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\times$ for FP32/FP64 and $48$--$158\times$ for BF16, representing a \textbf{6--48$\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\max} \approx 10^{-6}$), enabling \textbf{$\sim$1000$\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\max} \approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.

</details>


### [168] [Time-Delayed Transformers for Data-Driven Modeling of Low-Dimensional Dynamics](https://arxiv.org/abs/2602.08478)
*Albert Alcalde,Markus Widhalm,Emre Yılmaz*

Main category: cs.LG

TL;DR: 提出时间延迟变换器(TD-TF)，一种简化的Transformer架构，用于非稳态时空动力学的数据驱动建模，将线性算子方法与深度序列模型联系起来。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模复杂非线性时空动力学时存在局限性，需要一种既能保持线性模型可解释性和效率，又能提供更强表达能力的架构。

Method: 设计极简的单层单头Transformer架构，每预测一个查询，包含一个自注意力层和一个前馈层，计算复杂度与序列长度呈线性关系，参数数量少。

Result: 在近线性系统中性能与强线性基线相当，在非线性和混沌系统中显著优于线性方法，能准确捕捉长期动力学行为，同时保持可解释性和计算效率。

Conclusion: TD-TF成功桥接了线性算子方法和深度序列模型，在线性模型的可解释性和效率基础上，为复杂动力学提供了显著增强的表达能力。

Abstract: We propose the time-delayed transformer (TD-TF), a simplified transformer architecture for data-driven modeling of unsteady spatio-temporal dynamics. TD-TF bridges linear operator-based methods and deep sequence models by showing that a single-layer, single-head transformer can be interpreted as a nonlinear generalization of time-delayed dynamic mode decomposition (TD-DMD). The architecture is deliberately minimal, consisting of one self-attention layer with a single query per prediction and one feedforward layer, resulting in linear computational complexity in sequence length and a small parameter count. Numerical experiments demonstrate that TD-TF matches the performance of strong linear baselines on near-linear systems, while significantly outperforming them in nonlinear and chaotic regimes, where it accurately captures long-term dynamics. Validation studies on synthetic signals, unsteady aerodynamics, the Lorenz '63 system, and a reaction-diffusion model show that TD-TF preserves the interpretability and efficiency of linear models while providing substantially enhanced expressive power for complex dynamics.

</details>


### [169] [The Optimal Token Baseline: Variance Reduction for Long-Horizon LLM-RL](https://arxiv.org/abs/2602.07078)
*Yingru Li,Jiawei Xu,Ziniu Li,Jiacai Liu,Wei Liu,Yuxuan Tong,Longtao Zheng,Zhenghai Xue,Yaxiang Zhang,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出Optimal Token Baseline方法解决LLM强化学习中的梯度方差爆炸问题，通过基于梯度范数的逆加权更新实现训练稳定性，大幅减少token消耗


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习在长时程任务中常因梯度方差爆炸导致训练崩溃。传统基线方法存在优化困难、忽略序列异质性等问题，需要更有效的方差减少方案

Method: 从第一性原理推导出Optimal Token Baseline，证明梯度更新应按其累积梯度范数的倒数加权。提出Logit-Gradient Proxy仅使用前向传播概率近似梯度范数，确保计算效率

Result: 方法实现了训练稳定性，仅用N=4就能达到N=32大组规模的性能，在单轮和工具集成推理任务中减少超过65%的token消耗

Conclusion: OTB方法有效解决了LLM强化学习中的梯度方差问题，通过理论推导和高效实现，在保持性能的同时显著降低了计算成本

Abstract: Reinforcement Learning (RL) for Large Language Models (LLMs) often suffers from training collapse in long-horizon tasks due to exploding gradient variance. To mitigate this, a baseline is commonly introduced for advantage computation; however, traditional value models remain difficult to optimize, and standard group-based baselines overlook sequence heterogeneity. Although classic optimal baseline theory can achieve global variance reduction, it neglects token heterogeneity and requires prohibitive gradient-based computation. In this work, we derive the Optimal Token Baseline (OTB) from first principles, proving that gradient updates should be weighted inversely to their cumulative gradient norm. To ensure efficiency, we propose the Logit-Gradient Proxy that approximates the gradient norm using only forward-pass probabilities. Our method achieves training stability and matches the performance of large group sizes ($N=32$) with only $N=4$, reducing token consumption by over 65% across single-turn and tool-integrated reasoning tasks.

</details>


### [170] [BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability](https://arxiv.org/abs/2602.07144)
*Samuel Daulton,David Eriksson,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: BONSAI是一种默认感知的贝叶斯优化策略，在保持优化性能的同时减少对默认配置的偏离，特别适用于需要最小化参数变化的实际应用场景。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化在处理带有精心设计的默认配置的参数时存在不足，它会将弱相关参数推到搜索空间边界，难以区分重要和虚假的变化，增加了验证推荐配置的负担。

Method: BONSAI是一种默认感知的贝叶斯优化策略，通过剪枝对默认配置的低影响偏离，同时明确控制获取函数的损失。该方法兼容多种获取函数，包括期望改进和上置信界。

Result: BONSAI在理论上具有与标准GP-UCB相同的无遗憾性质。在实际应用中，它能显著减少推荐配置中的非默认参数数量，同时保持竞争力的优化性能，对运行时间影响很小。

Conclusion: BONSAI提供了一种实用的贝叶斯优化方法，能够在保持优化效果的同时最小化对默认配置的偏离，特别适用于需要平衡优化性能和参数变化最小化的实际应用场景。

Abstract: Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.

</details>


### [171] [Convex Dominance in Deep Learning I: A Scaling Law of Loss and Learning Rate](https://arxiv.org/abs/2602.07145)
*Zhiqi Bu,Shiyun Xu,Jialin Mao*

Main category: cs.LG

TL;DR: 该研究探讨深度学习优化中的凸性现象，发现深度学习训练后很快变得弱凸，并基于此建立了学习率和损失的缩放规律


<details>
  <summary>Details</summary>
Motivation: 深度学习具有非凸损失景观，优化动态难以分析或控制，但经验上表现出类似凸性的行为。研究旨在探索凸性和Lipschitz连续性在深度学习中的适用性，以通过学习率调度精确控制损失动态

Method: 通过分析深度学习优化过程中的凸性特征，发现训练后很快变得弱凸，损失可通过最后迭代的上界预测，从而推导最优学习率的缩放规律。基于凸性视角建立学习率和损失的缩放定律

Result: 深度学习在短期训练后变得弱凸，损失可通过最后迭代上界预测，这进一步指导了最优学习率的缩放。建立了学习率和损失的缩放定律，可在训练时长上外推80倍，模型大小上外推70倍

Conclusion: 深度学习优化动态表现出弱凸特性，基于凸性视角建立的缩放定律能够有效预测和控制学习过程，为深度学习优化提供了新的理论框架和实用工具

Abstract: Deep learning has non-convex loss landscape and its optimization dynamics is hard to analyze or control. Nevertheless, the dynamics can be empirically convex-like across various tasks, models, optimizers, hyperparameters, etc. In this work, we examine the applicability of convexity and Lipschitz continuity in deep learning, in order to precisely control the loss dynamics via the learning rate schedules. We illustrate that deep learning quickly becomes weakly convex after a short period of training, and the loss is predicable by an upper bound on the last iterate, which further informs the scaling of optimal learning rate. Through the lens of convexity, we build scaling laws of learning rates and losses that extrapolate as much as 80X across training horizons and 70X across model sizes.

</details>


### [172] [On Randomness in Agentic Evals](https://arxiv.org/abs/2602.07150)
*Bjarni Haukur Bjarnason,André Silva,Martin Monperrus*

Main category: cs.LG

TL;DR: 该研究发现智能体系统评估中单次运行pass@1分数存在显著方差，2-3个百分点的改进可能只是统计噪声而非真实算法进步，建议采用多次运行、统计功效分析和pass@k等更可靠的评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统评估通常基于每个任务单次运行的pass@1分数，假设这能提供可靠的性能估计。本文旨在验证这一假设，探究单次运行评估的可靠性问题。

Method: 在SWE-Bench-Verified基准上收集60,000个智能体轨迹，涵盖三个模型和两种框架。通过token级分析追踪轨迹分歧点，并分析评估方差对性能比较的影响。

Result: 发现显著方差：单次运行pass@1估计根据所选运行不同变化2.2-6.0个百分点，即使在温度0时标准差也超过1.5个百分点。轨迹在早期（前几个百分点的token内）就发生分歧，微小差异会级联成不同的解决策略。

Conclusion: 建议三个具体实践：(1) 基于每个任务多次独立运行估计pass@1，(2) 使用统计功效分析确定检测预期效应大小所需的运行次数，(3) 考虑k>1的pass@k（乐观界限）和pass^k（悲观界限）指标来更好表征完整性能范围。这些实践对于区分真实科学进步与统计噪声至关重要。

Abstract: Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.

</details>


### [173] [Beyond Pooling: Matching for Robust Generalization under Data Heterogeneity](https://arxiv.org/abs/2602.07154)
*Ayush Roy,Rudrasis Chakraborty,Lav Varshney,Vishnu Suresh Lokhande*

Main category: cs.LG

TL;DR: 提出匹配框架处理异构数据集池化中的分布不对称问题，通过自适应质心选择和迭代优化提高零样本泛化能力


<details>
  <summary>Details</summary>
Motivation: 异构数据集池化会放大分布不对称性并产生有偏估计，特别是在需要零样本泛化的场景中，传统方法如朴素池化和均匀子采样无法有效处理这种异质性

Method: 提出匹配框架，基于自适应质心选择样本，通过迭代优化表示分布，结合双重鲁棒性和倾向得分匹配来过滤混淆域（异质性的主要来源）

Result: 理论和实证分析表明，匹配方法在不对称元分布下优于朴素池化和均匀子采样，这些改进扩展到非高斯和多模态现实场景，并在零样本医学异常检测中取得显著效果

Conclusion: 匹配框架能有效处理异构数据集池化中的分布不对称问题，提高零样本泛化能力，特别是在医学异常检测等极端异质性场景中具有重要应用价值

Abstract: Pooling heterogeneous datasets across domains is a common strategy in representation learning, but naive pooling can amplify distributional asymmetries and yield biased estimators, especially in settings where zero-shot generalization is required. We propose a matching framework that selects samples relative to an adaptive centroid and iteratively refines the representation distribution. The double robustness and the propensity score matching for the inclusion of data domains make matching more robust than naive pooling and uniform subsampling by filtering out the confounding domains (the main cause of heterogeneity). Theoretical and empirical analyses show that, unlike naive pooling or uniform subsampling, matching achieves better results under asymmetric meta-distributions, which are also extended to non-Gaussian and multimodal real-world settings. Most importantly, we show that these improvements translate to zero-shot medical anomaly detection, one of the extreme forms of data heterogeneity and asymmetry. The code is available on https://github.com/AyushRoy2001/Beyond-Pooling.

</details>


### [174] [Learning Nonlinear Systems In-Context: From Synthetic Data to Real-World Motor Control](https://arxiv.org/abs/2602.07173)
*Tong Jian,Tianyu Dai,Tao Yu*

Main category: cs.LG

TL;DR: 首次将大语言模型的上下文学习能力应用于电机前馈控制，通过Transformer架构分离信号表示与系统行为，实现少样本微调和单样本上下文学习，在多种电机负载配置上超越传统PI控制器和基于物理的前馈方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现了强大的上下文学习能力，但尚未扩展到信号处理系统。传统PI控制器和基于物理的方法在处理非线性和复杂负载条件时存在困难，需要一种能够适应未见系统动态的数据高效控制方法。

Method: 提出基于Transformer的模型架构，分离信号表示与系统行为，支持少样本微调和单样本上下文学习。在大规模合成线性和非线性系统上进行预训练，使模型能够仅通过少量示例泛化到真实世界电机的未见系统动态。

Result: 模型在多种电机负载配置上表现出良好的泛化能力，能够将未调优的示例转化为准确的前馈预测，性能优于PI控制器和基于物理的前馈基准方法。

Conclusion: 上下文学习能够桥接合成预训练和真实世界适应性，为物理系统的数据高效控制开辟了新方向，展示了将大语言模型能力扩展到信号处理和控制系统的潜力。

Abstract: LLMs have shown strong in-context learning (ICL) abilities, but have not yet been extended to signal processing systems. Inspired by their design, we have proposed for the first time ICL using transformer models applicable to motor feedforward control, a critical task where classical PI and physics-based methods struggle with nonlinearities and complex load conditions. We propose a transformer based model architecture that separates signal representation from system behavior, enabling both few-shot finetuning and one-shot ICL. Pretrained on a large corpus of synthetic linear and nonlinear systems, the model learns to generalize to unseen system dynamics of real-world motors only with a handful of examples. In experiments, our approach generalizes across multiple motor load configurations, transforms untuned examples into accurate feedforward predictions, and outperforms PI controllers and physics-based feedforward baselines. These results demonstrate that ICL can bridge synthetic pretraining and real-world adaptability, opening new directions for data efficient control of physical systems.

</details>


### [175] [Risk-Sensitive Exponential Actor Critic](https://arxiv.org/abs/2602.07202)
*Alonso Granados,Jason Pacheco*

Main category: cs.LG

TL;DR: 提出rsEAC方法解决风险敏感强化学习中的数值不稳定问题，在连续控制任务中可靠学习风险敏感策略


<details>
  <summary>Details</summary>
Motivation: 无模型深度强化学习在实际应用中存在安全隐患，需要风险感知智能体。现有基于熵风险度量的策略梯度方法存在高方差和数值不稳定问题，限制了其在复杂任务中的应用。

Method: 提出风险敏感指数演员-评论家（rsEAC）方法，这是一种离策略无模型方法，包含避免显式表示指数价值函数及其梯度的新程序，并针对熵风险度量优化策略。

Result: rsEAC相比现有方法产生更数值稳定的更新，在MuJoCo连续任务的危险变体中可靠地学习风险敏感策略。

Conclusion: 通过理论分析和实验验证，rsEAC方法有效解决了风险敏感强化学习中的数值不稳定问题，为实际应用中的风险感知智能体提供了可行的解决方案。

Abstract: Model-free deep reinforcement learning (RL) algorithms have achieved tremendous success on a range of challenging tasks. However, safety concerns remain when these methods are deployed on real-world applications, necessitating risk-aware agents. A common utility for learning such risk-aware agents is the entropic risk measure, but current policy gradient methods optimizing this measure must perform high-variance and numerically unstable updates. As a result, existing risk-sensitive model-free approaches are limited to simple tasks and tabular settings. In this paper, we provide a comprehensive theoretical justification for policy gradient methods on the entropic risk measure, including on- and off-policy gradient theorems for the stochastic and deterministic policy settings. Motivated by theory, we propose risk-sensitive exponential actor-critic (rsEAC), an off-policy model-free approach that incorporates novel procedures to avoid the explicit representation of exponential value functions and their gradients, and optimizes its policy w.r.t the entropic risk measure. We show that rsEAC produces more numerically stable updates compared to existing approaches and reliably learns risk-sensitive policies in challenging risky variants of continuous tasks in MuJoCo.

</details>


### [176] [Exactly Computing do-Shapley Values](https://arxiv.org/abs/2602.07203)
*R. Teal Witter,Álvaro Parafita,Tomas Garriga,Maximilian Muschalik,Fabian Fumagalli,Axel Brando,Lucas Rosenblatt*

Main category: cs.LG

TL;DR: 本文提出了一种高效计算do-Shapley值的新方法，通过将do-Shapley值重新表述为结构因果模型中不可约集的形式，实现了从线性到指数级的计算加速，并降低了识别负担。


<details>
  <summary>Details</summary>
Motivation: 结构因果模型（SCM）是描述自然科学中复杂动态的强大框架，do-Shapley是一种基于博弈论的方法，用于量化d个变量在指数级干预下的平均效应。然而，与Shapley值类似，计算do-Shapley值通常需要评估指数级数量的项，这在实际应用中存在计算瓶颈。

Method: 1. 将do-Shapley值重新表述为底层SCM的不可约集形式；2. 基于此洞察，开发了精确算法，计算时间与不可约集数量r呈线性关系（r范围从d到2^d）；3. 提出估计器，可在任意查询预算下运行，随着预算接近r，估计精度显著提高；4. 证明非参数识别do-Shapley值仅需识别d个单元素联盟的干预效应，而非所有类别。

Result: 1. 当查询预算接近r时，新估计器比先前方法精度提高数个数量级；2. 当预算达到r时，可返回机器精度的Shapley值；3. 计算速度显著提升，从指数级降低到线性；4. 识别负担大幅降低，仅需识别d个单元素联盟而非所有类别。

Conclusion: 本文通过将do-Shapley值重新表述为不可约集形式，实现了计算效率的显著提升和识别负担的大幅降低。新方法在计算速度和估计精度方面均优于现有方法，为结构因果模型中的因果效应量化提供了更实用的工具。

Abstract: Structural Causal Models (SCM) are a powerful framework for describing complicated dynamics across the natural sciences. A particularly elegant way of interpreting SCMs is do-Shapley, a game-theoretic method of quantifying the average effect of $d$ variables across exponentially many interventions. Like Shapley values, computing do-Shapley values generally requires evaluating exponentially many terms. The foundation of our work is a reformulation of do-Shapley values in terms of the irreducible sets of the underlying SCM. Leveraging this insight, we can exactly compute do-Shapley values in time linear in the number of irreducible sets $r$, which itself can range from $d$ to $2^d$ depending on the graph structure of the SCM. Since $r$ is unknown a priori, we complement the exact algorithm with an estimator that, like general Shapley value estimators, can be run with any query budget. As the query budget approaches $r$, our estimators can produce more accurate estimates than prior methods by several orders of magnitude, and, when the budget reaches $r$, return the Shapley values up to machine precision. Beyond computational speed, we also reduce the identification burden: we prove that non-parametric identifiability of do-Shapley values requires only the identification of interventional effects for the $d$ singleton coalitions, rather than all classes.

</details>


### [177] [Permissive-Washing in the Open AI Supply Chain: A Large-Scale Audit of License Integrity](https://arxiv.org/abs/2602.08816)
*James Jewitt,Gopi Krishnan Rajbahadur,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.LG

TL;DR: 研究发现开源AI领域存在普遍的"许可清洗"现象：96.5%的数据集和95.8%的模型缺乏必要的许可文本，只有极少数满足完整的许可要求，即使上游提供了合规许可，下游也很少正确传播。


<details>
  <summary>Details</summary>
Motivation: 开源AI项目虽然使用MIT、Apache-2.0等宽松许可证，但这些许可证包含必须满足的法律要求（完整许可文本、版权声明、上游归属）。目前缺乏对这些要求的大规模验证，可能导致实际使用超出许可范围，使下游用户面临法律风险。

Method: 对124,278条数据集→模型→应用供应链进行实证审计，涵盖Hugging Face和GitHub上的3,338个数据集、6,664个模型和28,516个应用。检查是否包含完整许可文本、版权声明，以及上游归属是否在下游正确传播。

Result: 惊人的96.5%数据集和95.8%模型缺乏必要许可文本；仅2.3%数据集和3.2%模型同时满足许可文本和版权要求；即使上游提供完整许可证据，下游传播率极低：仅27.59%模型保留合规数据集声明，仅5.75%应用保留合规模型声明。

Conclusion: 从业者不能假设宽松许可证标签能提供其声称的权利：许可文件和声明（而非元数据）才是法律真实性的来源。开源AI存在普遍的"许可清洗"问题，需要更严格的合规实践。

Abstract: Permissive licenses like MIT, Apache-2.0, and BSD-3-Clause dominate open-source AI, signaling that artifacts like models, datasets, and code can be freely used, modified, and redistributed. However, these licenses carry mandatory requirements: include the full license text, provide a copyright notice, and preserve upstream attribution, that remain unverified at scale. Failure to meet these conditions can place reuse outside the scope of the license, effectively leaving AI artifacts under default copyright for those uses and exposing downstream users to litigation. We call this phenomenon ``permissive washing'': labeling AI artifacts as free to use, while omitting the legal documentation required to make that label actionable. To assess how widespread permissive washing is in the AI supply chain, we empirically audit 124,278 dataset $\rightarrow$ model $\rightarrow$ application supply chains, spanning 3,338 datasets, 6,664 models, and 28,516 applications across Hugging Face and GitHub. We find that an astonishing 96.5\% of datasets and 95.8\% of models lack the required license text, only 2.3\% of datasets and 3.2\% of models satisfy both license text and copyright requirements, and even when upstream artifacts provide complete licensing evidence, attribution rarely propagates downstream: only 27.59\% of models preserve compliant dataset notices and only 5.75\% of applications preserve compliant model notices (with just 6.38\% preserving any linked upstream notice). Practitioners cannot assume permissive labels confer the rights they claim: license files and notices, not metadata, are the source of legal truth. To support future research, we release our full audit dataset and reproducible pipeline.

</details>


### [178] [Online Learning for Uninformed Markov Games: Empirical Nash-Value Regret and Non-Stationarity Adaptation](https://arxiv.org/abs/2602.07205)
*Junyan Liu,Haipeng Luo,Zihan Zhang,Lillian J. Ratliff*

Main category: cs.LG

TL;DR: 本文针对双人无信息马尔可夫博弈中的在线学习问题，提出了新的经验纳什值遗憾度量，并设计了自适应算法，在对手策略变化程度不同的情况下都能获得最优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有研究在双人无信息马尔可夫博弈中面临两个主要问题：1) 无法实现无外部遗憾；2) 现有纳什值遗憾算法无法适应问题难度，即使在对手策略固定的简单情况下也只能获得较差的遗憾界。本文旨在同时解决这两个限制。

Method: 首先提出经验纳什值遗憾这一新的遗憾度量，它比纳什值遗憾更强，且在对手固定时自然退化为外部遗憾。然后提出参数自由算法，通过分析基于周期的V-learning算法，建立遗憾界，并设计自适应重启机制来应对对手的非平稳性。

Result: 算法获得了O(min{√K + (CK)^{1/3}, √LK})的遗憾界，其中C量化对手策略的方差，L表示策略切换次数。该结果不仅恢复了两个极端情况（对手固定时的O(√K)外部遗憾和最坏情况下的O(K^{2/3})纳什值遗憾），还能通过自适应对手非平稳性在这些极端之间平滑插值。

Conclusion: 本文完全解决了双人无信息马尔可夫博弈在线学习中的两个关键限制，提出了更强的遗憾度量和自适应算法，能够在不同对手行为模式下自动获得最优遗憾性能。

Abstract: We study online learning in two-player uninformed Markov games, where the opponent's actions and policies are unobserved. In this setting, Tian et al. (2021) show that achieving no-external-regret is impossible without incurring an exponential dependence on the episode length $H$. They then turn to the weaker notion of Nash-value regret and propose a V-learning algorithm with regret $O(K^{2/3})$ after $K$ episodes. However, their algorithm and guarantee do not adapt to the difficulty of the problem: even in the case where the opponent follows a fixed policy and thus $O(\sqrt{K})$ external regret is well-known to be achievable, their result is still the worse rate $O(K^{2/3})$ on a weaker metric.
  In this work, we fully address both limitations. First, we introduce empirical Nash-value regret, a new regret notion that is strictly stronger than Nash-value regret and naturally reduces to external regret when the opponent follows a fixed policy. Moreover, under this new metric, we propose a parameter-free algorithm that achieves an $O(\min \{\sqrt{K} + (CK)^{1/3},\sqrt{LK}\})$ regret bound, where $C$ quantifies the variance of the opponent's policies and $L$ denotes the number of policy switches (both at most $O(K)$). Therefore, our results not only recover the two extremes -- $O(\sqrt{K})$ external regret when the opponent is fixed and $O(K^{2/3})$ Nash-value regret in the worst case -- but also smoothly interpolate between these extremes by automatically adapting to the opponent's non-stationarity. We achieve so by first providing a new analysis of the epoch-based V-learning algorithm by Mao et al. (2022), establishing an $O(ηC + \sqrt{K/η})$ regret bound, where $η$ is the epoch incremental factor. Next, we show how to adaptively restart this algorithm with an appropriate $η$ in response to the potential non-stationarity of the opponent, eventually achieving our final results.

</details>


### [179] [A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents](https://arxiv.org/abs/2602.08964)
*Raghu Arghal,Fade Chen,Niall Dalton,Evgenii Kortukov,Calum McNamara,Angelos Nalmpantis,Moksh Nirvaan,Gabriele Sarti,Mario Giulianelli*

Main category: cs.LG

TL;DR: 该研究提出了一个评估智能体目标导向性的框架，结合行为评估和模型内部表示的可解释性分析，通过LLM智能体在2D网格世界导航的案例研究，发现智能体内部非线性编码环境空间地图，其行动与内部表示一致，推理过程会重组这些表示。


<details>
  <summary>Details</summary>
Motivation: 理解智能体的目标有助于解释和预测其行为，但目前缺乏可靠地将目标归因于智能体系统的成熟方法。需要建立综合评估框架来更好地理解智能体如何表示和追求目标。

Method: 提出了一个评估目标导向性的框架，整合行为评估和基于可解释性的内部表示分析。以LLM智能体在2D网格世界导航为案例研究：1）行为评估：在不同网格大小、障碍物密度和目标结构下评估智能体相对于最优策略的表现；2）可解释性分析：使用探测方法解码智能体对环境状态和多步行动计划的内部表示。

Result: 1）行为表现：智能体性能随任务难度增加而提升，同时对难度保持不变的变换和复杂目标结构保持鲁棒性；2）内部表示：LLM智能体非线性编码环境的粗略空间地图，保留关于自身位置和目标位置的任务相关线索；3）行动一致性：智能体行动与这些内部表示基本一致；4）推理重组：推理过程会重组内部表示，从更广泛的环境结构线索转向支持即时行动选择的信息。

Conclusion: 仅靠行为评估不足以充分理解智能体如何表示和追求目标，需要结合内省式检查（可解释性分析）来全面表征智能体的目标导向性。该框架为评估智能体目标提供了更全面的方法。

Abstract: Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.

</details>


### [180] [Probing Neural TSP Representations for Prescriptive Decision Support](https://arxiv.org/abs/2602.07216)
*Reuben Narad,Léonard Boussioux,Michael Wagner*

Main category: cs.LG

TL;DR: 该研究探索了神经组合优化（NCO）模型在旅行商问题（TSP）训练后，其内部表征是否能够迁移到其他优化相关任务，如节点移除敏感性和边禁止敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索训练好的TSP求解器是否能够学习到可迁移的内部表征，用于其他与优化相关的下游任务，类似于其他领域的迁移学习。这有助于评估NCO模型在解决实际物流场景中的决策支持问题时的通用性。

Method: 方法包括：1）训练多个基于注意力的TSP策略模型；2）收集这些模型的内部激活状态；3）在节点/边嵌入上训练探针，用于两个NP-hard的下游任务：节点移除敏感性（识别最具影响力的节点）和边禁止敏感性（识别最关键的边）。

Result: 在欧几里得TSP100训练的模型上，两个任务的探针性能与现有基线相当。将探针信号与几何特征集成后，性能超过最强基线：最佳节点移除任务达到65%的top-1准确率（基线为58%），最差边识别任务达到73%的top-1准确率（基线为67%）。

Conclusion: 该研究首次将神经TSP求解器作为可迁移编码器，用于超越路径构建的预测性决策支持任务。研究还发现迁移准确率随求解器质量和模型规模提升而增加，表明训练更强的NCO求解器也能产生更有用的下游编码器。

Abstract: The field of neural combinatorial optimization (NCO) trains neural policies to solve NP-hard problems such as the traveling salesperson problem (TSP). We ask whether, beyond producing good tours, a trained TSP solver learns internal representations that transfer to other optimization-relevant objectives, in the spirit of transfer learning from other domains. We train several attention-based TSP policies, collect their internal activations, and train probes on node/edge embeddings for two NP-hard prescriptive downstream tasks inspired by real-world logistics scenarios: node-removal sensitivity (identifying the most impactful node to remove) and edge-forbid sensitivity (identifying the most critical edge to retain). On a Euclidean TSP100-trained model, probes for both tasks are competitive with existing baselines. Ensembling probe signals with geometric features outperforms the strongest baselines: 65\% top-1 accuracy (vs. 58\% baseline) for the best-node-removal task, and 73\% top-1 accuracy (vs. 67\% baseline) for the worst-edge identification task. To our knowledge, we are the first to study neural TSP solvers as transferable encoders for prescriptive what-if decision-support objectives beyond tour construction. Finally, we show that transfer accuracy increases with solver quality across training and model scale, suggesting that training stronger NCO solvers also yields more useful encoders for downstream objectives. Our code is available at: github.com/ReubenNarad/tsp_prescriptive_probe

</details>


### [181] [Collaborative and Efficient Fine-tuning: Leveraging Task Similarity](https://arxiv.org/abs/2602.07218)
*Gagik Magakyan,Amirhossein Reisizadeh,Chanwoo Park,Pablo A. Parrilo,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: CoLoRA：一种利用任务相似性进行协作式低秩适应的参数高效微调方法，通过共享适配器和个人适配器提升基础模型在数据稀缺场景下的性能


<details>
  <summary>Details</summary>
Motivation: 解决基础模型微调中高质量标注数据稀缺的问题，通过利用多个下游用户之间的任务相似性来提升有效微调数据规模

Method: 提出协作式低秩适应（CoLoRA），包含一个共享适配器捕捉所有任务的底层相似性，以及针对用户特定任务的个性化适配器

Result: 在异构线性回归上提供了理论保证，并在多个自然语言实验中证明，当与相似任务一起训练时，个体性能显著提升

Conclusion: CoLoRA通过利用任务相似性进行协作式微调，有效缓解了数据稀缺问题，显著提升了基础模型在下游任务中的适应性能

Abstract: Adaptability has been regarded as a central feature in the foundation models, enabling them to effectively acclimate to unseen downstream tasks. Parameter-efficient fine-tuning methods such as celebrated LoRA facilitate efficient adaptation of large foundation models using labeled, high-quality and generally scarce task data. To mitigate data scarcity in fine-tuning of foundation models, we propose to leverage task similarity across multiple downstream users. Intuitively, users with similar tasks must be able to assist each other in boosting the effective fine-tuning data size. We propose Collaborative Low-Rank Adaptation, or CoLoRA, which exploits task similarity to collaboratively and efficiently fine-tune personalized foundation models. The main idea in CoLoRA is to train one shared adapter capturing underlying task similarities across all tasks, and personalized adapters tailored to user-specific tasks. We theoretically study CoLoRA on heterogeneous linear regression and provide provable guarantees for ground truth recovery. We also conduct several natural language experiments with varying task similarity, which further demonstrate that when trained together with similar tasks, individual performances are significantly boosted.

</details>


### [182] [SpecAttn: Co-Designing Sparse Attention with Self-Speculative Decoding](https://arxiv.org/abs/2602.07223)
*Yikang Yue,Yuqi Xue,Jian Huang*

Main category: cs.LG

TL;DR: SpecAttn是一种基于验证引导稀疏注意力的自推测解码方法，通过验证过程识别关键KV条目，在后续令牌生成时仅加载这些条目，提高解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型推理面临KV缓存内存需求增长的瓶颈。现有自推测解码方法依赖独立的KV选择算法，忽略了验证过程中已计算出的KV条目重要性信息。

Method: 提出SpecAttn方法，将验证过程作为副产品来识别关键KV条目，在后续令牌生成时仅加载这些关键条目，实现验证引导的稀疏注意力机制。

Result: SpecAttn相比传统自回归解码实现2.81倍吞吐量提升，相比最先进的基于稀疏性的自推测解码方法提升1.29倍，同时提高了草稿令牌接受率。

Conclusion: 通过利用验证过程中自然计算出的KV条目重要性信息，SpecAttn有效解决了KV缓存内存瓶颈，实现了高效的自推测解码。

Abstract: Long-context large language model (LLM) inference has become the norm for today's AI applications. However, it is severely bottlenecked by the increasing memory demands of its KV cache. Previous works have shown that self-speculative decoding with sparse attention, where tokens are drafted using a subset of the KV cache and verified in parallel with full KV cache, speeds up inference in a lossless way. However, this approach relies on standalone KV selection algorithms to select the KV entries used for drafting and overlooks that the criticality of each KV entry is inherently computed during verification. In this paper, we propose SpecAttn, a self-speculative decoding method with verification-guided sparse attention. SpecAttn identifies critical KV entries as a byproduct of verification and only loads these entries when drafting subsequent tokens. This not only improves draft token acceptance rate but also incurs low KV selection overhead, thereby improving decoding throughput. SpecAttn achieves 2.81$\times$ higher throughput over vanilla auto-regressive decoding and 1.29$\times$ improvement over state-of-the-art sparsity-based self-speculative decoding methods.

</details>


### [183] [Fault-Tolerant Evaluation for Sample-Efficient Model Performance Estimators](https://arxiv.org/abs/2602.07226)
*Zihan Zhu,Yanqiu Wu,Qiongkai Xu*

Main category: cs.LG

TL;DR: 提出一个容错评估框架，用于在模型即服务时代评估样本高效的性能估计器，通过可调节容错水平ε平衡偏差和方差，避免现有方法在低方差场景下的问题。


<details>
  <summary>Details</summary>
Motivation: 在模型即服务时代，组织依赖第三方AI模型快速部署，但新兴AI应用的动态性、新数据集的持续引入以及声称优越性能的模型数量增加，使得模型服务的有效可靠验证变得日益困难。现有评估方法在低方差设置下存在问题：RMSE混淆偏差和方差，当方差小时掩盖持续偏差；而基于p值的测试变得过于敏感，因微小偏差拒绝足够好的估计器。

Method: 提出一个容错评估框架，将偏差和方差考虑整合到可调节容错水平ε中，允许在实践可接受的误差范围内评估性能估计器。理论上证明适当校准ε可确保在不同方差机制下的可靠评估，并进一步提出自动优化和选择ε的算法。

Result: 在真实世界数据集上的实验表明，该框架提供了对估计器行为的全面且可操作的洞察。

Conclusion: 提出的容错评估框架解决了现有方法在低方差场景下的局限性，通过整合偏差和方差考虑并引入可调节容错水平，为样本高效性能估计器的评估提供了更可靠和实用的方法。

Abstract: In the era of Model-as-a-Service, organizations increasingly rely on third-party AI models for rapid deployment. However, the dynamic nature of emerging AI applications, the continual introduction of new datasets, and the growing number of models claiming superior performance make efficient and reliable validation of model services increasingly challenging. This motivates the development of sample-efficient performance estimators, which aim to estimate model performance by strategically selecting instances for labeling, thereby reducing annotation cost. Yet existing evaluation approaches often fail in low-variance settings: RMSE conflates bias and variance, masking persistent bias when variance is small, while p-value based tests become hypersensitive, rejecting adequate estimators for negligible deviations. To address this, we propose a fault-tolerant evaluation framework that integrates bias and variance considerations within an adjustable tolerance level ${\varepsilon}$, enabling the evaluation of performance estimators within practically acceptable error margins. We theoretically show that proper calibration of ${\varepsilon}$ ensures reliable evaluation across different variance regimes, and we further propose an algorithm that automatically optimizes and selects ${\varepsilon}$. Experiments on real-world datasets demonstrate that our framework provides comprehensive and actionable insights into estimator behavior.

</details>


### [184] [Cerebellar-Inspired Residual Control for Fault Recovery: From Inference-Time Adaptation to Structural Consolidation](https://arxiv.org/abs/2602.07227)
*Nethmi Jayasinghe,Diana Gontero,Spencer T. Brown,Vinod K. Sangwan,Mark C. Hersam,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 提出一种受小脑启发的推理时残差控制框架，在冻结的强化学习策略基础上添加在线校正动作，实现故障恢复而无需修改基础策略参数。


<details>
  <summary>Details</summary>
Motivation: 机器人策略在真实世界部署中常遇到训练后故障，而重新训练、探索或系统辨识往往不切实际，需要一种无需修改基础策略参数的在线故障恢复方法。

Method: 采用小脑启发的残差控制框架：1) 通过固定特征扩展实现高维模式分离；2) 并行微区式残差通路；3) 具有兴奋性和抑制性资格迹的局部误差驱动可塑性，在不同时间尺度上运行；4) 性能驱动的元适应机制调节残差权限和可塑性。

Result: 在MuJoCo基准测试中，在驱动器、动态和环境扰动下，HalfCheetah-v5性能提升达+66%，Humanoid-v5提升达+53%；在严重扰动下性能优雅下降，并能将持久残差校正整合到策略参数中增强鲁棒性。

Conclusion: 该小脑启发的推理时残差控制框架能够有效处理训练后故障，实现快速局部校正，避免破坏性全局策略更新，同时保持名义行为并抑制不必要的干预。

Abstract: Robotic policies deployed in real-world environments often encounter post-training faults, where retraining, exploration, or system identification are impractical. We introduce an inference-time, cerebellar-inspired residual control framework that augments a frozen reinforcement learning policy with online corrective actions, enabling fault recovery without modifying base policy parameters. The framework instantiates core cerebellar principles, including high-dimensional pattern separation via fixed feature expansion, parallel microzone-style residual pathways, and local error-driven plasticity with excitatory and inhibitory eligibility traces operating at distinct time scales. These mechanisms enable fast, localized correction under post-training disturbances while avoiding destabilizing global policy updates. A conservative, performance-driven meta-adaptation regulates residual authority and plasticity, preserving nominal behavior and suppressing unnecessary intervention. Experiments on MuJoCo benchmarks under actuator, dynamic, and environmental perturbations show improvements of up to $+66\%$ on \texttt{HalfCheetah-v5} and $+53\%$ on \texttt{Humanoid-v5} under moderate faults, with graceful degradation under severe shifts and complementary robustness from consolidating persistent residual corrections into policy parameters.

</details>


### [185] [ArcMark: Multi-bit LLM Watermark via Optimal Transport](https://arxiv.org/abs/2602.07235)
*Atefeh Gilani,Carol Xuan Long,Sajani Vithana,Oliver Kosut,Lalitha Sankar,Flavio P. Calmon*

Main category: cs.LG

TL;DR: 该论文首次推导出多比特水印的信息论容量，并基于编码理论设计了达到该容量的ArcMark水印方案，在比特率和检测准确率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型水印方法主要从零比特水印扩展而来，每令牌仅编码单个比特，而多比特水印的信息论容量（在不改变平均下一个令牌预测的情况下每令牌可插入和检测的最大比特数）一直未知。

Method: 通过信息论分析推导多比特水印的容量特性，基于编码理论原理设计ArcMark水印方案，该方案在特定假设下能够达到多比特水印信道的容量。

Result: ArcMark在实际应用中在每令牌比特率和检测准确率方面优于竞争的多比特水印方法，证明了语言模型水印本质上是信道编码问题。

Conclusion: 该工作首次表征了多比特水印的容量，为基于编码理论原理的水印设计开辟了新途径，将语言模型水印视为信道编码问题。

Abstract: Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.

</details>


### [186] [Graph homophily booster: Reimagining the role of discrete features in heterophilic graph learning](https://arxiv.org/abs/2602.07256)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: GRAPHITE通过创建特征节点直接提升图同质性，解决异质图上的GNN性能问题，在异质图上显著优于现有方法，在同质图上达到可比性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在异质图（连接节点特征或标签不相似）上表现不佳，甚至不如简单的MLP。现有方法主要关注架构设计，而没有直接针对异质性的根本原因。需要一种创新方法来直接解决图异质性问题。

Method: 提出GRAPHITE框架，通过精心设计的图变换直接提高图同质性。基于同质性的精确定义，创建特征节点来促进具有相似特征的节点之间的同质性消息传递。该方法在理论上和经验上都证明能显著提高原始异质图的同质性，同时只轻微增加图的大小。

Result: 在具有挑战性的数据集上的大量实验表明，GRAPHITE在异质图上显著优于最先进的方法，同时在同质图上达到与最先进方法相当的准确率。例如，在Actor数据集上，21个最新的GNN仍然落后于MLP，而GRAPHITE解决了这一问题。

Conclusion: GRAPHITE提出了一种新的、未探索的范式：通过图变换直接提高图同质性，有效解决了GNN在异质图上的性能问题。这是第一个明确通过图变换直接改善图同质性的方法，为处理图异质性提供了创新思路。

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling graph-structured data. However, existing GNNs often struggle with heterophilic graphs, where connected nodes tend to have dissimilar features or labels. While numerous methods have been proposed to address this challenge, they primarily focus on architectural designs without directly targeting the root cause of the heterophily problem. These approaches still perform even worse than the simplest MLPs on challenging heterophilic datasets. For instance, our experiments show that 21 latest GNNs still fall behind the MLP on the Actor dataset. This critical challenge calls for an innovative approach to addressing graph heterophily beyond architectural designs. To bridge this gap, we propose and study a new and unexplored paradigm: directly increasing the graph homophily via a carefully designed graph transformation. In this work, we present a simple yet effective framework called GRAPHITE to address graph heterophily. To the best of our knowledge, this work is the first method that explicitly transforms the graph to directly improve the graph homophily. Stemmed from the exact definition of homophily, our proposed GRAPHITE creates feature nodes to facilitate homophilic message passing between nodes that share similar features. Furthermore, we both theoretically and empirically show that our proposed GRAPHITE significantly increases the homophily of originally heterophilic graphs, with only a slight increase in the graph size. Extensive experiments on challenging datasets demonstrate that our proposed GRAPHITE significantly outperforms state-of-the-art methods on heterophilic graphs while achieving comparable accuracy with state-of-the-art methods on homophilic graphs.

</details>


### [187] [XShare: Collaborative in-Batch Expert Sharing for Faster MoE Inference](https://arxiv.org/abs/2602.07265)
*Daniil Vankov,Nikita Ivkin,Kyle Ulrich,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

TL;DR: XShare通过批感知专家选择优化MoE架构，无需重新训练即可动态适应每个批次，减少专家激活30%，降低GPU峰值负载3倍，在推测解码中提升吞吐量14%


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能高效扩展大语言模型，但在生产推理中，请求批处理和推测解码会显著增加专家激活，削弱效率优势。需要解决批处理环境下的专家选择优化问题。

Method: 将批感知专家选择建模为模块化优化问题，设计针对不同部署场景的高效贪心算法。XShare方法无需重新训练，动态适应每个批次，通过最大化选定专家的总门控分数来优化专家选择。

Result: 在标准批处理下减少专家激活达30%；在专家并行部署中降低GPU峰值负载达3倍；在推测解码中通过分层、相关性感知的专家选择实现吞吐量提升达14%，即使批次中的请求来自异构数据集。

Conclusion: XShare通过优化批感知专家选择，有效解决了MoE架构在生产推理中的效率问题，显著提升了系统性能，适用于各种部署场景。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly used to efficiently scale large language models. However, in production inference, request batching and speculative decoding significantly amplify expert activation, eroding these efficiency benefits. We address this issue by modeling batch-aware expert selection as a modular optimization problem and designing efficient greedy algorithms for different deployment settings. The proposed method, namely XShare, requires no retraining and dynamically adapts to each batch by maximizing the total gating score of selected experts. It reduces expert activation by up to 30% under standard batching, cuts peak GPU load by up to 3x in expert-parallel deployments, and achieves up to 14% throughput gains in speculative decoding via hierarchical, correlation-aware expert selection even if requests in a batch drawn from heterogeneous datasets.

</details>


### [188] [Hybrid Feedback-Guided Optimal Learning for Wireless Interactive Panoramic Scene Delivery](https://arxiv.org/abs/2602.07273)
*Xiaoyi Wu,Juaren Steiger,Bin Li,R. Srikant*

Main category: cs.LG

TL;DR: 该论文提出了一种混合反馈模型和AdaPort算法，用于优化沉浸式应用中视口预测和传输的在线学习问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实和增强现实应用对帧率、延迟和物理虚拟环境同步有严格要求。现有方法将视口选择建模为多臂老虎机问题，但未能充分利用预测反馈可以在观察到用户头部姿态后为所有候选视口计算这一事实，导致学习效率不高。

Method: 提出了一种结合完全信息和老虎机反馈的两级混合反馈模型，并设计了AdaPort算法。该算法利用预测反馈的完全信息特性和传输反馈的老虎机特性，通过在线学习优化视口选择。

Result: 推导了混合反馈模型的实例相关遗憾下界，并证明了AdaPort算法的遗憾上界与下界渐近匹配。通过真实世界轨迹驱动的仿真实验，AdaPort始终优于最先进的基线方法。

Conclusion: 通过将预测反馈识别为完全信息反馈而非老虎机反馈，提出的混合反馈模型和AdaPort算法显著提高了沉浸式应用中视口选择的学习效率，为边缘服务器渲染和传输优化提供了有效解决方案。

Abstract: Immersive applications such as virtual and augmented reality impose stringent requirements on frame rate, latency, and synchronization between physical and virtual environments. To meet these requirements, an edge server must render panoramic content, predict user head motion, and transmit a portion of the scene that is large enough to cover the user viewport while remaining within wireless bandwidth constraints. Each portion produces two feedback signals: prediction feedback, indicating whether the selected portion covers the actual viewport, and transmission feedback, indicating whether the corresponding packets are successfully delivered. Prior work models this problem as a multi-armed bandit with two-level bandit feedback, but fails to exploit the fact that prediction feedback can be retrospectively computed for all candidate portions once the user head pose is observed. As a result, prediction feedback constitutes full-information feedback rather than bandit feedback. Motivated by this observation, we introduce a two-level hybrid feedback model that combines full-information and bandit feedback, and formulate the portion selection problem as an online learning task under this setting. We derive an instance-dependent regret lower bound for the hybrid feedback model and propose AdaPort, a hybrid learning algorithm that leverages both feedback types to improve learning efficiency. We further establish an instance-dependent regret upper bound that matches the lower bound asymptotically, and demonstrate through real-world trace driven simulations that AdaPort consistently outperforms state-of-the-art baseline methods.

</details>


### [189] [Fair Decisions from Calibrated Scores: Achieving Optimal Classification While Satisfying Sufficiency](https://arxiv.org/abs/2602.07285)
*Etam Benger,Katrina Ligett*

Main category: cs.LG

TL;DR: 本文提出了一种在充分性约束下优化二元分类的方法，解决了即使完美组校准的分数在阈值化后也会违反预测公平性的问题。


<details>
  <summary>Details</summary>
Motivation: 基于预测概率的二元分类是监督机器学习的基本任务。虽然阈值化在无约束情况下是贝叶斯最优的，但使用单一阈值通常会违反统计群体公平约束。在充分性约束下，即使完美组校准的分数（包括真实类别概率）在阈值化后也会违反预测公平性，需要新的解决方案。

Method: 提出了一种精确解决方案，用于在充分性约束下优化二元（随机化）分类，假设有限组校准分数集。提供了可实现的正预测值（PPV）和假遗漏率（FOR）对的可达几何特征化，并基于此推导出简单的后处理算法，仅使用组校准分数和组成员身份即可获得最优分类器。

Result: 算法能够获得在充分性约束下的最优分类器。由于充分性和分离性通常不兼容，还识别了在满足充分性条件下最小化与分离性偏差的分类器，并证明该分类器也可以通过算法获得，通常能达到与最优性能相当的表现。

Conclusion: 本文为在充分性公平约束下的二元分类问题提供了理论框架和实用算法，解决了即使完美校准分数也无法通过简单阈值化满足预测公平性的挑战，并提供了处理充分性与分离性冲突的解决方案。

Abstract: Binary classification based on predicted probabilities (scores) is a fundamental task in supervised machine learning. While thresholding scores is Bayes-optimal in the unconstrained setting, using a single threshold generally violates statistical group fairness constraints. Under independence (statistical parity) and separation (equalized odds), such thresholding suffices when the scores already satisfy the corresponding criterion. However, this does not extend to sufficiency: even perfectly group-calibrated scores -- including true class probabilities -- violate predictive parity after thresholding. In this work, we present an exact solution for optimal binary (randomized) classification under sufficiency, assuming finite sets of group-calibrated scores. We provide a geometric characterization of the feasible pairs of positive predictive value (PPV) and false omission rate (FOR) achievable by such classifiers, and use it to derive a simple post-processing algorithm that attains the optimal classifier using only group-calibrated scores and group membership. Finally, since sufficiency and separation are generally incompatible, we identify the classifier that minimizes deviation from separation subject to sufficiency, and show that it can also be obtained by our algorithm, often achieving performance comparable to the optimum.

</details>


### [190] [Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions](https://arxiv.org/abs/2602.07341)
*Yicheng Yang,Ruijiao Li,Lifeng Wang,Shuai Zheng,Shunzheng Ma,Keyu Zhang,Tuoyu Sun,Chenyun Dai,Jie Ding,Zhuo Zou*

Main category: cs.LG

TL;DR: 提出一个用于灵巧机械臂-手系统操作任务的可扩展机器人学习框架，结合AR远程人机交互收集专家数据，采用行为克隆预训练和对比学习增强的强化学习两阶段方法，提升策略效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决灵巧机械臂-手系统在复杂操作任务中学习效率低、数据收集困难的问题，通过AR远程人机交互系统收集专家演示数据，提高学习效率和安全性。

Method: 提出两阶段统一框架：第一阶段通过AR远程人机交互系统收集专家数据，采用行为克隆方式进行预训练；第二阶段开发对比学习增强的强化学习方法，设计投影头加速学习进程，采用事件驱动增强奖励机制提升安全性。

Result: 在PyBullet物理仿真和真实世界实验中验证，相比经典PPO和SAC策略，该方法不仅显著加快推理速度，而且在完成操作任务的成功率方面表现更好。消融研究证实对比学习强化学习避免了策略崩溃。

Conclusion: 提出的AR远程人机交互数据收集与对比学习增强强化学习相结合的方法，为灵巧机械臂-手系统操作任务提供了一个高效、可扩展的学习框架，在仿真和真实环境中均表现出优越性能。

Abstract: This paper focuses on the scalable robot learning for manipulation in the dexterous robot arm-hand systems, where the remote human-robot interactions via augmented reality (AR) are established to collect the expert demonstration data for improving efficiency. In such a system, we present a unified framework to address the general manipulation task problem. Specifically, the proposed method consists of two phases: i) In the first phase for pretraining, the policy is created in a behavior cloning (BC) manner, through leveraging the learning data from our AR-based remote human-robot interaction system; ii) In the second phase, a contrastive learning empowered reinforcement learning (RL) method is developed to obtain more efficient and robust policy than the BC, and thus a projection head is designed to accelerate the learning progress. An event-driven augmented reward is adopted for enhancing the safety. To validate the proposed method, both the physics simulations via PyBullet and real-world experiments are carried out. The results demonstrate that compared to the classic proximal policy optimization and soft actor-critic policies, our method not only significantly speeds up the inference, but also achieves much better performance in terms of the success rate for fulfilling the manipulation tasks. By conducting the ablation study, it is confirmed that the proposed RL with contrastive learning overcomes policy collapse. Supplementary demonstrations are available at https://cyberyyc.github.io/.

</details>


### [191] [Controllable Value Alignment in Large Language Models through Neuron-Level Editing](https://arxiv.org/abs/2602.07356)
*Yonghui Yang,Junwei Li,Jilong Liu,Yicheng He,Fengbin Zhu,Weibiao Huang,Le Wu,Richang Hong,Tat-Seng Chua*

Main category: cs.LG

TL;DR: NeVA框架通过神经元级编辑实现可控的价值对齐，显著减少价值泄漏问题


<details>
  <summary>Details</summary>
Motivation: 现有基于引导的价值对齐方法存在有限的可控性：引导目标价值时常常无意中激活其他非目标价值，这被称为价值泄漏问题

Method: 提出NeVA框架，识别稀疏的价值相关神经元，并在推理时进行激活编辑，实现细粒度控制而无需参数更新或重新训练

Result: NeVA实现了更强的目标价值对齐，同时在大模型通用能力上造成更小的性能下降，显著降低了平均泄漏率，残余效应主要局限于语义相关的价值类别

Conclusion: NeVA为价值对齐提供了更可控和可解释的机制

Abstract: Aligning large language models (LLMs) with human values has become increasingly important as their influence on human behavior and decision-making expands. However, existing steering-based alignment methods suffer from limited controllability: steering a target value often unintentionally activates other, non-target values. To characterize this limitation, we introduce value leakage, a diagnostic notion that captures the unintended activation of non-target values during value steering, along with a normalized leakage metric grounded in Schwartz's value theory. In light of this analysis, we propose NeVA, a neuron-level editing framework for controllable value alignment in LLMs. NeVA identifies sparse, value-relevant neurons and performs inference-time activation editing, enabling fine-grained control without parameter updates or retraining. Experiments show that NeVA achieves stronger target value alignment while incurring smaller performance degradation on general capability. Moreover, NeVA significantly reduces the average leakage, with residual effects largely confined to semantically related value classes. Overall, NeVA offers a more controllable and interpretable mechanism for value alignment.

</details>


### [192] [UTOPIA: Unlearnable Tabular Data via Decoupled Shortcut Embedding](https://arxiv.org/abs/2602.07358)
*Jiaming He,Fuming Luo,Hongwei Li,Wenbo Jiang,Wenshu Fan,Zhenbo Shi,Xudong Jiang,Yi Yu*

Main category: cs.LG

TL;DR: UTOPIA方法为表格数据提供认证的不可学习性保护，通过解耦优化在高显著性特征上混淆语义，在低显著性冗余特征上嵌入超相关捷径，防止未经授权的模型训练。


<details>
  <summary>Details</summary>
Motivation: 金融和医疗领域的表格数据高度敏感，现有不可学习示例方法在表格数据上效果不佳，因为表格特征混合数值和类别约束，且存在显著性稀疏性（学习主要集中于少数维度）。

Method: 提出UTOPIA方法，利用特征冗余将优化解耦为两个通道：高显著性特征用于语义混淆，低显著性冗余特征用于嵌入超相关捷径，在满足表格数据约束的同时创建主导性捷径。

Result: 在多个表格数据集和模型上的实验表明，UTOPIA能够将未经授权的训练推向接近随机性能，优于现有不可学习示例基线方法，且在不同架构间具有良好的迁移性。

Conclusion: 在满足谱主导条件下，表格数据的认证不可学习性是可行的。UTOPIA通过解耦优化策略有效保护敏感表格数据，防止未经授权的模型训练，为表格数据隐私保护提供了新方法。

Abstract: Unlearnable examples (UE) have emerged as a practical mechanism to prevent unauthorized model training on private vision data, while extending this protection to tabular data is nontrivial. Tabular data in finance and healthcare is highly sensitive, yet existing UE methods transfer poorly because tabular features mix numerical and categorical constraints and exhibit saliency sparsity, with learning dominated by a few dimensions. Under a Spectral Dominance condition, we show certified unlearnability is feasible when the poison spectrum overwhelms the clean semantic spectrum. Guided by this, we propose Unlearnable Tabular Data via DecOuPled Shortcut EmbeddIng (UTOPIA), which exploits feature redundancy to decouple optimization into two channels: high saliency features for semantic obfuscation and low saliency redundant features for embedding a hyper correlated shortcut, yielding constraint-aware dominant shortcuts while preserving tabular validity. Extensive experiments across tabular datasets and models show UTOPIA drives unauthorized training toward near random performance, outperforming strong UE baselines and transferring well across architectures.

</details>


### [193] [Privately Learning Decision Lists and a Differentially Private Winnow](https://arxiv.org/abs/2602.07370)
*Mark Bun,William Fang*

Main category: cs.LG

TL;DR: 本文提出了新的差分隐私算法，用于在PAC和在线模型中学习决策列表和大间隔半空间，在隐私保护下匹配或接近非隐私算法的最优性能。


<details>
  <summary>Details</summary>
Motivation: 差分隐私机器学习需要在不泄露个体信息的前提下实现有效学习。现有隐私算法在决策列表和半空间学习等经典问题上通常存在性能损失，需要开发能够在隐私保护下接近非隐私算法性能的新方法。

Method: 1. 在PAC模型中：开发了计算高效的差分隐私算法学习决策列表，样本复杂度接近最优非隐私算法
2. 在在线模型中：提出了Winnow算法的隐私版本，用于学习大间隔半空间，错误界限在维度上为多对数级，与间隔成反多项式关系
3. 应用：将在线模型方法扩展到决策列表学习，实现与最先进非隐私算法相当的性能

Result: 1. PAC模型中：决策列表学习的样本复杂度最小化，仅比最优非隐私算法有少量额外开销
2. 在线模型中：半空间学习的错误界限为polylog(dimension) × poly(1/margin)，其中维度为d，间隔为γ
3. 在线决策列表学习：实现了与最先进非隐私算法相当的定性性能保证

Conclusion: 本文证明了在差分隐私约束下，经典学习问题（决策列表和半空间）可以达到接近非隐私算法的性能，为隐私保护机器学习提供了新的理论保证和实用算法。

Abstract: We give new differentially private algorithms for the classic problems of learning decision lists and large-margin halfspaces in the PAC and online models. In the PAC model, we give a computationally efficient algorithm for learning decision lists with minimal sample overhead over the best non-private algorithms. In the online model, we give a private analog of the influential Winnow algorithm for learning halfspaces with mistake bound polylogarithmic in the dimension and inverse polynomial in the margin. As an application, we describe how to privately learn decision lists in the online model, qualitatively matching state-of-the art non-private guarantees.

</details>


### [194] [Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference](https://arxiv.org/abs/2602.07397)
*Hoang Anh Duy Le,Sahil Joshi,Zeyu Yang,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: Sketch&Walk Attention是一种无需训练的稀疏注意力方法，通过轻量级草图技术和确定性游走机制动态选择top-k注意力块，在保持接近无损精度的同时实现高达6倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制在长上下文LLM推理中（包括预填充和解码阶段）占据了主要的计算和内存成本，需要一种高效的稀疏注意力解决方案来降低这些开销。

Method: 使用Hadamard草图技术廉价地近似注意力分数，然后通过游走机制在层间聚合这些估计值，捕捉超越token间直接交互的注意力影响，最终基于累积的游走分数动态选择top-k注意力块。

Result: 在多种模型和任务上，Sketch&Walk在20%注意力密度下保持接近无损的精度，在某些设置中甚至略微优于密集注意力，同时实现高达6倍的推理加速。

Conclusion: Sketch&Walk Attention是一种统一适用于预填充和解码阶段的训练免费稀疏注意力方法，能够显著降低长上下文LLM推理的计算和内存成本，同时保持模型性能。

Abstract: Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.

</details>


### [195] [Nonparametric Bayesian Optimization for General Rewards](https://arxiv.org/abs/2602.07411)
*Zishi Zhang,Tao Ren,Yijie Peng*

Main category: cs.LG

TL;DR: 本文提出了一种在奖励模型不确定性下的贝叶斯优化算法，使用无限高斯过程作为替代模型，结合汤普森采样实现无遗憾保证，计算可扩展且性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化通常假设高斯过程作为替代模型，但在实际应用中奖励模型可能存在不确定性、非平稳性、重尾分布等复杂情况，需要更通用的方法。

Method: 提出无限高斯过程作为贝叶斯非参数模型，对奖励分布空间设置先验，结合汤普森采样进行探索与利用，采用截断吉布斯采样实现计算可扩展性。

Result: 算法在一般奖励设置下实现了无遗憾保证，仅需目标函数的Lipschitz连续性，计算复杂度与经典高斯过程相当，在非平稳、重尾等复杂奖励场景中表现优异。

Conclusion: 无限高斯过程结合汤普森采样为贝叶斯优化提供了一种通用且高效的框架，能够处理广泛的奖励模型不确定性，具有理论保证和实际应用价值。

Abstract: This work focuses on Bayesian optimization (BO) under reward model uncertainty. We propose the first BO algorithm that achieves no-regret guarantee in a general reward setting, requiring only Lipschitz continuity of the objective function and accommodating a broad class of measurement noise. The core of our approach is a novel surrogate model, termed as infinite Gaussian process ($\infty$-GP). It is a Bayesian nonparametric model that places a prior on the space of reward distributions, enabling it to represent a substantially broader class of reward models than classical Gaussian process (GP). The $\infty$-GP is used in combination with Thompson Sampling (TS) to enable effective exploration and exploitation. Correspondingly, we develop a new TS regret analysis framework for general rewards, which relates the regret to the total variation distance between the surrogate model and the true reward distribution. Furthermore, with a truncated Gibbs sampling procedure, our method is computationally scalable, incurring minimal additional memory and computational complexities compared to classical GP. Empirical results demonstrate state-of-the-art performance, particularly in settings with non-stationary, heavy-tailed, or other ill-conditioned rewards.

</details>


### [196] [Learning Molecular Chirality via Chiral Determinant Kernels](https://arxiv.org/abs/2602.07415)
*Runhan Shi,Zhicheng Zhang,Letian Chen,Gufeng Yu,Yang Yang*

Main category: cs.LG

TL;DR: ChiDeK是一个将立体化学信息整合到分子表示学习中的框架，通过手性行列式核编码SE(3)不变的手性矩阵，能够同时处理中心手性和轴向手性，在多个手性相关任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手性是分子在化学和生物学中控制立体特异性行为的基本属性。现有的机器学习模型在捕捉手性方面面临挑战，因为传统分子表示通常缺乏明确的立体化学编码，且现有方法主要关注中心手性，无法泛化到更复杂的轴向手性等形式。

Method: 提出ChiDeK框架，引入手性行列式核来编码SE(3)不变的手性矩阵，并使用交叉注意力机制将局部手性中心的立体化学信息整合到全局分子表示中。该设计能够在统一架构中显式建模手性相关特征，同时编码中心手性和轴向手性。

Result: 在四个任务（R/S构型分类、对映体排序、ECD光谱预测和OR预测）上，ChiDeK相比最先进的基线方法取得了显著改进，特别是在轴向手性任务上平均准确率提高了7%以上。为评估轴向手性，还构建了新的ECD和OR预测基准。

Conclusion: ChiDeK通过系统地将立体源信息整合到分子表示学习中，有效解决了手性建模的挑战，能够同时处理中心手性和轴向手性，在手性相关任务上表现出优越性能，为手性分子表示学习提供了新框架。

Abstract: Chirality is a fundamental molecular property that governs stereospecific behavior in chemistry and biology. Capturing chirality in machine learning models remains challenging due to the geometric complexity of stereochemical relationships and the limitations of traditional molecular representations that often lack explicit stereochemical encoding. Existing approaches to chiral molecular representation primarily focus on central chirality, relying on handcrafted stereochemical tags or limited 3D encodings, and thus fail to generalize to more complex forms such as axial chirality. In this work, we introduce ChiDeK (Chiral Determinant Kernels), a framework that systematically integrates stereogenic information into molecular representation learning. We propose the chiral determinant kernel to encode the SE(3)-invariant chirality matrix and employ cross-attention to integrate stereochemical information from local chiral centers into the global molecular representation. This design enables explicit modeling of chiral-related features within a unified architecture, capable of jointly encoding central and axial chirality. To support the evaluation of axial chirality, we construct a new benchmark for electronic circular dichroism (ECD) and optical rotation (OR) prediction. Across four tasks, including R/S configuration classification, enantiomer ranking, ECD spectrum prediction, and OR prediction, ChiDeK achieves substantial improvements over state-of-the-art baselines, most notably yielding over 7% higher accuracy on axially chiral tasks on average.

</details>


### [197] [Achieving Optimal Static and Dynamic Regret Simultaneously in Bandits with Deterministic Losses](https://arxiv.org/abs/2602.07418)
*Jian Qian,Chen-Yu Wei*

Main category: cs.LG

TL;DR: 该研究在对抗性多臂老虎机中，首次针对确定性损失和遗忘性对手实现了静态遗憾和动态遗憾的同时最优性，揭示了自适应对手与遗忘性对手之间的根本差异。


<details>
  <summary>Details</summary>
Motivation: 在对抗性多臂老虎机中，静态遗憾和动态遗憾是两个常用的性能度量。虽然已有针对单个度量的最优算法，但尚无算法能同时实现两者的最优界限。Marinov和Zimmert[2021]首次证明针对自适应对手无法实现同时最优性，本研究旨在探索在遗忘性对手和确定性损失条件下的可能性。

Method: 首先将Marinov和Zimmert[2021]的不可能性结果扩展到确定性损失情况。然后提出一种新算法，利用负静态遗憾来补偿控制动态遗憾时的探索开销，并借助Blackwell可接近性来联合控制两种遗憾，从而实现同时最优性。

Result: 成功证明了在确定性损失和遗忘性对手条件下，可以同时实现最优的静态遗憾和动态遗憾。这揭示了当同时考虑多个遗憾基准时，自适应对手与遗忘性对手之间的根本分离，并为同时实现不同切换次数基准的最优遗憾这一长期开放问题提供了新见解。

Conclusion: 该研究首次展示了在对抗性多臂老虎机中，针对遗忘性对手和确定性损失，可以同时实现静态遗憾和动态遗憾的最优性。提出的算法利用负静态遗憾补偿探索开销，通过Blackwell可接近性联合控制两种遗憾，为老虎机模型选择提供了新方法，具有独立的研究价值。

Abstract: In adversarial multi-armed bandits, two performance measures are commonly used: static regret, which compares the learner to the best fixed arm, and dynamic regret, which compares it to the best sequence of arms. While optimal algorithms are known for each measure individually, there is no known algorithm achieving optimal bounds for both simultaneously. Marinov and Zimmert [2021] first showed that such simultaneous optimality is impossible against an adaptive adversary. Our work takes a first step to demonstrate its possibility against an oblivious adversary when losses are deterministic. First, we extend the impossibility result of Marinov and Zimmert [2021] to the case of deterministic losses. Then, we present an algorithm achieving optimal static and dynamic regret simultaneously against an oblivious adversary. Together, they reveal a fundamental separation between adaptive and oblivious adversaries when multiple regret benchmarks are considered simultaneously. It also provides new insight into the long open problem of simultaneously achieving optimal regret against switching benchmarks of different numbers of switches.
  Our algorithm uses negative static regret to compensate for the exploration overhead incurred when controlling dynamic regret, and leverages Blackwell approachability to jointly control both regrets. This yields a new model selection procedure for bandits that may be of independent interest.

</details>


### [198] [Sign-Based Optimizers Are Effective Under Heavy-Tailed Noise](https://arxiv.org/abs/2602.07425)
*Dingzhi Yu,Hongyi Tao,Yuanyu Wan,Luo Luo,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文从重尾梯度噪声角度解释了符号优化算法在训练大语言模型时优于自适应梯度方法的原因，提出了新的广义重尾噪声模型，为SignSGD、Lion、Muon等算法提供了理论分析


<details>
  <summary>Details</summary>
Motivation: 尽管符号优化算法（如Lion和Muon）在训练大语言模型时表现出优于AdamW的实证性能，但其理论原因尚不清楚。本文旨在通过重尾梯度噪声这一在语言建模任务中常见的现象，来弥合理论与实践的差距。

Method: 提出了一个新颖的广义重尾噪声条件，比标准有限方差假设更准确地捕捉大语言模型的行为。在此噪声模型下，为广义光滑函数类建立了SignSGD和Lion的尖锐收敛率。还将分析扩展到Muon和Muonlight，首次提供了重尾随机性下矩阵优化的严格分析。

Result: 理论分析表明，符号优化算法在重尾噪声条件下具有优越的收敛性能，匹配或超越了先前已知的最佳边界。大语言模型预训练实验验证了理论见解，并确认提出的噪声模型与实践相符。

Conclusion: 符号优化算法天然适合处理与重尾相关的噪声梯度，这为它们在训练大语言模型时的实证优越性提供了强有力的理论依据。研究结果解释了为什么基于符号的更新方法优于方差自适应方法。

Abstract: While adaptive gradient methods are the workhorse of modern machine learning, sign-based optimization algorithms such as Lion and Muon have recently demonstrated superior empirical performance over AdamW in training large language models (LLM). However, a theoretical understanding of why sign-based updates outperform variance-adapted methods remains elusive. In this paper, we aim to bridge the gap between theory and practice through the lens of heavy-tailed gradient noise, a phenomenon frequently observed in language modeling tasks. Theoretically, we introduce a novel generalized heavy-tailed noise condition that captures the behavior of LLMs more accurately than standard finite variance assumptions. Under this noise model, we establish sharp convergence rates of SignSGD and Lion for generalized smooth function classes, matching or surpassing previous best-known bounds. Furthermore, we extend our analysis to Muon and Muonlight, providing what is, to our knowledge, the first rigorous analysis of matrix optimization under heavy-tailed stochasticity. These results offer a strong theoretical justification for the empirical superiority of sign-based optimizers, showcasing that they are naturally suited to handle the noisy gradients associated with heavy tails. Empirically, LLM pretraining experiments validate our theoretical insights and confirm that our proposed noise models are well-aligned with practice.

</details>


### [199] [Brep2Shape: Boundary and Shape Representation Alignment via Self-Supervised Transformers](https://arxiv.org/abs/2602.07429)
*Yuanxu Sun,Yuezhou Ma,Haixu Wu,Guanyang Zeng,Muye Chen,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Brep2Shape：一种新颖的自监督预训练方法，通过几何感知任务和对偶Transformer架构，弥合B-rep表示中连续方法与离散方法之间的差距，实现抽象边界表示与直观形状表示的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法处理B-rep模型存在表示差距：连续方法提供分析精度但视觉抽象，离散方法提供直观清晰度但牺牲几何精度。需要弥合这一差距以更好地处理CAD模型。

Method: 提出Brep2Shape自监督预训练方法，采用几何感知任务让模型从参数化Bézier控制点预测密集空间点；使用对偶Transformer主干并行编码曲面和曲线token以捕捉不同几何特性；集成拓扑注意力建模曲面与曲线间的相互依赖关系。

Result: 实验结果表明Brep2Shape具有显著可扩展性，在各种下游任务中实现了最先进的精度和更快的收敛速度。

Conclusion: Brep2Shape成功弥合了B-rep表示中的表示差距，通过自监督预训练实现了抽象边界表示与直观形状表示的有效对齐，为CAD模型的深度学习处理提供了更好的解决方案。

Abstract: Boundary representation (B-rep) is the industry standard for computer-aided design (CAD). While deep learning shows promise in processing B-rep models, existing methods suffer from a representation gap: continuous approaches offer analytical precision but are visually abstract, whereas discrete methods provide intuitive clarity at the expense of geometric precision. To bridge this gap, we introduce Brep2Shape, a novel self-supervised pre-training method designed to align abstract boundary representations with intuitive shape representations. Our method employs a geometry-aware task where the model learns to predict dense spatial points from parametric Bézier control points, enabling the network to better understand physical manifolds derived from abstract coefficients. To enhance this alignment, we propose a Dual Transformer backbone with parallel streams that independently encode surface and curve tokens to capture their distinct geometric properties. Moreover, the topology attention is integrated to model the interdependencies between surfaces and curves, thereby maintaining topological consistency. Experimental results demonstrate that Brep2Shape offers significant scalability, achieving state-of-the-art accuracy and faster convergence across various downstream tasks.

</details>


### [200] [Active Learning Using Aggregated Acquisition Functions: Accuracy and Sustainability Analysis](https://arxiv.org/abs/2602.07440)
*Cédric Jung,Shirin Salehi,Anke Schmeink*

Main category: cs.LG

TL;DR: 本文提出多种聚合结构来解决主动学习中的探索-利用困境，通过组合不同获取函数来平衡准确性和计算成本，实现更节能的AI训练。


<details>
  <summary>Details</summary>
Motivation: 主动学习虽然能减少标注成本和提高数据效率，但传统获取函数存在探索与利用的权衡问题，以及批处理模式效率低下和冷启动问题。需要设计更有效的聚合策略来平衡准确性和能耗。

Method: 提出了六种聚合结构：串联、并联、混合、自适应反馈、随机探索和退火探索。这些结构通过组合基于代表性的获取函数（如K-Centers）和基于不确定性的获取函数（如BALD、BADGE）来解决探索-利用困境。

Result: 聚合获取函数有效缓解了批处理模式效率低下和冷启动问题。交替使用BALD和BADGE等方法显示出稳健结果，K-Centers后接BALD的串联结构能以少12%的样本达到相同性能目标，同时将获取成本降低近一半。

Conclusion: 提出的聚合结构能够在保持或提高准确性的同时显著降低计算成本，为开发更可持续、能源感知的人工智能提供了有效解决方案。

Abstract: Active learning (AL) is a machine learning (ML) approach that strategically selects the most informative samples for annotation during training, aiming to minimize annotation costs. This strategy not only reduces labeling expenses but also results in energy savings during neural network training, thereby enhancing both data and energy efficiency. In this paper, we implement and evaluate various state-of-the-art acquisition functions, analyzing their accuracy and computational costs, while discussing the advantages and disadvantages of each method. Our findings reveal that representativity-based acquisition functions effectively explore the dataset but do not prioritize boundary decisions, whereas uncertainty-based acquisition functions focus on refining boundary decisions already identified by the neural network. This trade-off is known as the exploration-exploitation dilemma. To address this dilemma, we introduce six aggregation structures: series, parallel, hybrid, adaptive feedback, random exploration, and annealing exploration. Our aggregated acquisition functions alleviate common AL pathologies such as batch mode inefficiency and the cold start problem. Additionally, we focus on balancing accuracy and energy consumption, contributing to the development of more sustainable, energy-aware artificial intelligence (AI). We evaluate our proposed structures on various models and datasets. Our results demonstrate the potential of these structures to reduce computational costs while maintaining or even improving accuracy. Innovative aggregation approaches, such as alternating between acquisition functions such as BALD and BADGE, have shown robust results. Sequentially running functions like $K$-Centers followed by BALD has achieved the same performance goals with up to 12\% fewer samples, while reducing the acquisition cost by almost half.

</details>


### [201] [Proximal Action Replacement for Behavior Cloning Actor-Critic in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07441)
*Jinzong Dong,Wei Huang,Jianshu Zhang,Zhuo Chen,Xinzhe Yuan,Qinying Gu,Zhaohui Jiang,Nanyang Ye*

Main category: cs.LG

TL;DR: 离线强化学习中，行为克隆正则化的演员-评论家方法存在性能天花板问题，本文提出近端动作替换方法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，行为克隆正则化的演员-评论家方法虽然能产生现实策略并缓解分布外动作的偏差，但当数据集动作次优时，不加区分的模仿会限制演员充分利用评论家建议的高价值区域，形成性能天花板。

Method: 提出近端动作替换（PAR），这是一种即插即用的训练样本替换器，逐步用稳定演员生成的高价值动作替换低价值动作，扩大动作探索空间同时减少低价值数据的影响。PAR兼容多种行为克隆正则化范式。

Result: 在受控连续赌博机任务上验证了性能天花板问题，在多个离线强化学习基准测试中，PAR一致提升性能，与基础TD3+BC结合时达到最先进水平。

Conclusion: PAR方法有效突破了行为克隆正则化演员-评论家方法的性能天花板，通过智能替换训练样本中的动作，使离线强化学习算法能更好地利用高价值区域，提升整体性能。

Abstract: Offline reinforcement learning (RL) optimizes policies from a previously collected static dataset and is an important branch of RL. A popular and promising approach is to regularize actor-critic methods with behavior cloning (BC), which yields realistic policies and mitigates bias from out-of-distribution actions, but can impose an often-overlooked performance ceiling: when dataset actions are suboptimal, indiscriminate imitation structurally prevents the actor from fully exploiting high-value regions suggested by the critic, especially in later training when imitation is already dominant. We formally analyzed this limitation by investigating convergence properties of BC-regularized actor-critic optimization and verified it on a controlled continuous bandit task. To break this ceiling, we propose proximal action replacement (PAR), a plug-and-play training sample replacer that progressively replaces low-value actions with high-value actions generated by a stable actor, broadening the action exploration space while reducing the impact of low-value data. PAR is compatible with multiple BC regularization paradigms. Extensive experiments across offline RL benchmarks show that PAR consistently improves performance and approaches state-of-the-art when combined with the basic TD3+BC.

</details>


### [202] [On the Importance of a Multi-Scale Calibration for Quantization](https://arxiv.org/abs/2602.07465)
*Seungwoo Son,Ingyu Seong,Junhan Kim,Hyemi Jang,Yongkweon Jeon*

Main category: cs.LG

TL;DR: MaCa方法通过多尺度序列长度感知的Hessian估计，解决了传统PTQ中固定长度校准集无法适应LLM变长输入的问题，显著提升了低比特量化的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法使用固定长度的随机序列作为校准集，忽略了LLM输入的变长特性。输入长度直接影响激活分布和Hessian矩阵捕获的权重重要性，导致基于固定长度校准的Hessian估计无法准确反映不同输入场景下的真实权重重要性。

Method: 提出MaCa方法：1) 将多尺度序列长度信息融入Hessian估计；2) 将每个序列作为独立样本进行正则化，从而获得更稳定、更有效的Hessian矩阵用于精确量化。

Result: 在Qwen3、Gemma3、LLaMA3等先进LLM上的实验表明，MaCa在低比特量化下能持续提升准确性，提供了一种轻量级增强，与现有PTQ框架兼容。

Conclusion: MaCa是首个系统性地强调多尺度校准在LLM量化中作用的工作，通过长度感知的Hessian构建方法，有效解决了传统固定长度校准的局限性，为LLM高效部署提供了更优的量化方案。

Abstract: Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.

</details>


### [203] [Bandit Allocational Instability](https://arxiv.org/abs/2602.07472)
*Yilun Chen,Jiaqi Lu*

Main category: cs.LG

TL;DR: 多臂老虎机算法在分配拉取次数时存在巨大方差，这对平台运营和统计推断有害。本文引入分配变异性作为新指标，证明了其与遗憾之间的基本权衡关系：R_T·S_T=Ω(T^{3/2})，并提出了可调算法UCB-f来实现帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机算法在分配拉取次数时会产生巨大方差，这对现代应用如学习增强的平台运营和后老虎机统计推断特别有害。因此需要引入新的性能指标来量化这种分配变异性。

Method: 引入分配变异性作为新性能指标，定义为各臂拉取次数标准差的最大值。建立了分配变异性与遗憾之间的基本权衡关系，并提出了UCB-f算法（经典UCB1的推广），通过可调参数实现帕累托前沿上的任意权衡点。

Result: 证明了任何算法的遗憾R_T和分配变异性S_T必须满足R_T·S_T=Ω(T^{3/2})，只要R_T=o(T)。这表明任何极小极大遗憾最优算法必须承受Θ(T)的最坏情况分配变异性；而任何具有次线性最坏情况遗憾的算法必须承受S_T=ω(√T)。UCB-f算法可以实现帕累托前沿R_T·S_T=Θ̃(T^{3/2})上的任意点。

Conclusion: 多臂老虎机算法在分配变异性与遗憾之间存在基本权衡，这种权衡对平台运营和统计推断有重要影响。提出的UCB-f算法能够灵活调节这种权衡，同时解决了Praharaj和Khamaru（2025）的开放性问题。

Abstract: When multi-armed bandit (MAB) algorithms allocate pulls among competing arms, the resulting allocation can exhibit huge variation. This is particularly harmful in modern applications such as learning-enhanced platform operations and post-bandit statistical inference. Thus motivated, we introduce a new performance metric of MAB algorithms termed allocation variability, which is the largest (over arms) standard deviation of an arm's number of pulls. We establish a fundamental trade-off between allocation variability and regret, the canonical performance metric of reward maximization. In particular, for any algorithm, the worst-case regret $R_T$ and worst-case allocation variability $S_T$ must satisfy $R_T \cdot S_T=Ω(T^{\frac{3}{2}})$ as $T\rightarrow\infty$, as long as $R_T=o(T)$. This indicates that any minimax regret-optimal algorithm must incur worst-case allocation variability $Θ(T)$, the largest possible scale; while any algorithm with sublinear worst-case regret must necessarily incur ${S}_T= ω(\sqrt{T})$. We further show that this lower bound is essentially tight, and that any point on the Pareto frontier $R_T \cdot S_T=\tildeΘ(T^{3/2})$ can be achieved by a simple tunable algorithm UCB-f, a generalization of the classic UCB1. Finally, we discuss implications for platform operations and for statistical inference, when bandit algorithms are used. As a byproduct of our result, we resolve an open question of Praharaj and Khamaru (2025).

</details>


### [204] [Bipartite Graph Attention-based Clustering for Large-scale scRNA-seq Data](https://arxiv.org/abs/2602.07475)
*Zhuomin Liang,Liang Bai,Xian Yang*

Main category: cs.LG

TL;DR: BGFormer：基于二分图Transformer的单细胞RNA测序聚类模型，通过引入可学习的锚点标记实现线性计算复杂度，解决了传统Transformer方法O(n²)复杂度在大规模数据集上的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的单细胞RNA测序聚类方法将每个细胞视为序列中的标记，其计算和空间复杂度为O(n²)，限制了在大规模数据集上的应用。需要开发更高效、可扩展的聚类方法。

Method: 提出BGFormer（二分图Transformer聚类模型），引入一组可学习的锚点标记作为共享参考点表示整个数据集，采用二分图注意力机制学习细胞与锚点标记之间的相似性，使同一类别的细胞在嵌入空间中更接近。

Result: BGFormer实现了相对于细胞数量的线性计算复杂度，使其能够扩展到大规模数据集。在多个大规模单细胞RNA测序数据集上的实验证明了该方法的有效性和可扩展性。

Conclusion: BGFormer通过二分图注意力机制和锚点标记设计，成功解决了传统Transformer方法在大规模单细胞RNA测序聚类中的计算复杂度问题，为大规模单细胞数据分析提供了高效可扩展的解决方案。

Abstract: scRNA-seq clustering is a critical task for analyzing single-cell RNA sequencing (scRNA-seq) data, as it groups cells with similar gene expression profiles. Transformers, as powerful foundational models, have been applied to scRNA-seq clustering. Their self-attention mechanism automatically assigns higher attention weights to cells within the same cluster, enhancing the distinction between clusters. Existing methods for scRNA-seq clustering, such as graph transformer-based models, treat each cell as a token in a sequence. Their computational and space complexities are $\mathcal{O}(n^2)$ with respect to the number of cells, limiting their applicability to large-scale scRNA-seq datasets.To address this challenge, we propose a Bipartite Graph Transformer-based clustering model (BGFormer) for scRNA-seq data. We introduce a set of learnable anchor tokens as shared reference points to represent the entire dataset. A bipartite graph attention mechanism is introduced to learn the similarity between cells and anchor tokens, bringing cells of the same class closer together in the embedding space. BGFormer achieves linear computational complexity with respect to the number of cells, making it scalable to large datasets. Experimental results on multiple large-scale scRNA-seq datasets demonstrate the effectiveness and scalability of BGFormer.

</details>


### [205] [Deriving Neural Scaling Laws from the statistics of natural language](https://arxiv.org/abs/2602.07488)
*Francesco Cagnetta,Allan Raventós,Surya Ganguli,Matthieu Wyart*

Main category: cs.LG

TL;DR: 该论文提出了首个能够定量预测现代LLM在自然语言数据集上神经缩放定律指数的理论，特别针对数据受限的缩放定律。理论基于两个关键的语言统计特性，无需自由参数或合成数据模型就能预测缩放指数。


<details>
  <summary>Details</summary>
Motivation: 尽管实验神经缩放定律在很大程度上指导了大规模机器学习的实证进展，但现有理论无法定量预测任何现代LLM在任何自然语言数据集上的这些重要定律的指数。本文旨在填补这一理论空白。

Method: 通过分离两个关键的语言统计特性：(1) 配对token相关性随时间间隔的衰减，(2) 下一token条件熵随上下文长度的衰减。基于这些统计特性推导出一个简单公式，从第一原理预测数据受限的神经缩放指数，无需自由参数或合成数据模型。

Result: 理论预测与实验测量的神经缩放定律表现出显著匹配，包括在GPT-2和LLaMA风格模型上，在两个性质不同的基准测试（TinyStories和WikiText）上进行从头训练的结果。

Conclusion: 该研究首次提供了能够定量预测现代LLM在自然语言数据集上神经缩放定律指数的理论框架，揭示了语言统计特性与缩放指数之间的基本关系，为理解大规模语言模型的缩放行为提供了理论基础。

Abstract: Despite the fact that experimental neural scaling laws have substantially guided empirical progress in large-scale machine learning, no existing theory can quantitatively predict the exponents of these important laws for any modern LLM trained on any natural language dataset. We provide the first such theory in the case of data-limited scaling laws. We isolate two key statistical properties of language that alone can predict neural scaling exponents: (i) the decay of pairwise token correlations with time separation between token pairs, and (ii) the decay of the next-token conditional entropy with the length of the conditioning context. We further derive a simple formula in terms of these statistics that predicts data-limited neural scaling exponents from first principles without any free parameters or synthetic data models. Our theory exhibits a remarkable match with experimentally measured neural scaling laws obtained from training GPT-2 and LLaMA style models from scratch on two qualitatively different benchmarks, TinyStories and WikiText.

</details>


### [206] [CoMI-IRL: Contrastive Multi-Intention Inverse Reinforcement Learning](https://arxiv.org/abs/2602.07496)
*Antonio Mone,Frans A. Oliehoek,Luciano Cavalcante Siebert*

Main category: cs.LG

TL;DR: CoMI-IRL是一个基于transformer的无监督框架，用于多意图逆强化学习，它解耦了行为表示/聚类与下游奖励学习，无需先验知识就能自动发现行为模式。


<details>
  <summary>Details</summary>
Motivation: 现有的深度生成MI-IRL方法需要知道真实行为模式数量K*的先验知识，这限制了它们对新行为的适应性，并且只能分析与学习奖励相关的内容，而不能跨训练时使用的行为模式进行分析。

Method: 提出基于transformer的无监督框架CoMI-IRL，将行为表示和聚类与下游奖励学习解耦。使用对比学习方法学习行为表示，然后进行聚类，最后进行奖励学习。

Result: CoMI-IRL在无需K*先验知识或标签的情况下优于现有方法，同时允许对行为关系进行可视化解释，并且无需完全重新训练就能适应未见过的行为。

Conclusion: CoMI-IRL通过解耦行为表示/聚类与奖励学习，解决了传统MI-IRL方法对先验知识的依赖问题，提高了适应性和可解释性，为多意图逆强化学习提供了更灵活的框架。

Abstract: Inverse Reinforcement Learning (IRL) seeks to infer reward functions from expert demonstrations. When demonstrations originate from multiple experts with different intentions, the problem is known as Multi-Intention IRL (MI-IRL). Recent deep generative MI-IRL approaches couple behavior clustering and reward learning, but typically require prior knowledge of the number of true behavioral modes $K^*$. This reliance on expert knowledge limits their adaptability to new behaviors, and only enables analysis related to the learned rewards, and not across the behavior modes used to train them. We propose Contrastive Multi-Intention IRL (CoMI-IRL), a transformer-based unsupervised framework that decouples behavior representation and clustering from downstream reward learning. Our experiments show that CoMI-IRL outperforms existing approaches without a priori knowledge of $K^*$ or labels, while allowing for visual interpretation of behavior relationships and adaptation to unseen behavior without full retraining.

</details>


### [207] [PALMS: Pavlovian Associative Learning Models Simulator](https://arxiv.org/abs/2602.07519)
*Martin Fixman,Alessandro Abati,Julián Jiménez Nimmo,Sean Lim,Esther Mondragón*

Main category: cs.LG

TL;DR: PALMS是一个Python模拟器，用于模拟巴甫洛夫条件反射实验，整合了多种注意力学习模型，支持大规模刺激模拟和配置线索计算，提供图形界面和快速结果可视化。


<details>
  <summary>Details</summary>
Motivation: 模拟在理论发展和完善过程中不可或缺，帮助研究人员制定精确定义、生成模型并做出准确预测。需要一种工具来模拟巴甫洛夫条件反射实验，整合多种学习模型并支持复杂实验设计。

Method: 开发了PALMS模拟器，包含经典Rescorla-Wagner模型和多种注意力学习方法（Pearce-Kaye-Hall、Mackintosh Extended、Le Pelley's Hybrid等），以及一个具有统一可变学习率的新扩展模型。提供图形界面支持字母数字格式的实验设计输入，支持数百个刺激的模拟和配置线索计算。

Result: PALMS能够高效运行，即时可视化结果，支持在单一架构和环境中快速精确比较各种模型的预测。图形显示可轻松保存，模拟数据可导出到电子表格。通过复现已发表的关联学习实验验证了其功能和能力。

Conclusion: PALMS作为一个开源模拟器，显著扩展了关联学习模型的预测能力，为研究人员提供了一个强大的工具来模拟和比较不同学习理论，促进理论发展和完善。

Abstract: Simulations are an indispensable step in the cycle of theory development and refinement, helping researchers formulate precise definitions, generate models, and make accurate predictions. This paper introduces the Pavlovian Associative Learning Models Simulator (PALMS), a Python environment to simulate Pavlovian conditioning experiments. In addition to the canonical Rescorla-Wagner model, PALMS incorporates several attentional learning approaches, including Pearce-Kaye-Hall, Mackintosh Extended, Le Pelley's Hybrid, and a novel extension of the Rescorla-Wagner model with a unified variable learning rate that integrates Mackintosh's and Pearce and Hall's opposing conceptualisations. The simulator's graphical interface allows for the input of entire experimental designs in an alphanumeric format, akin to that used by experimental neuroscientists. Moreover, it uniquely enables the simulation of experiments involving hundreds of stimuli, as well as the computation of configural cues and configural-cue compounds across all models, thereby considerably expanding their predictive capabilities. PALMS operates efficiently, providing instant visualisation of results, supporting rapid, precise comparisons of various models' predictions within a single architecture and environment. Furthermore, graphic displays can be easily saved, and simulated data can be exported to spreadsheets. To illustrate the simulator's capabilities and functionalities, we provide a detailed description of the software and examples of use, reproducing published experiments in the associative learning literature. PALMS is licensed under the open-source GNU Lesser General Public License 3.0. The simulator source code and the latest multiplatform release build are accessible as a GitHub repository at https://github.com/cal-r/PALMS-Simulator

</details>


### [208] [MedVerse: Efficient and Reliable Medical Reasoning via DAG-Structured Parallel Execution](https://arxiv.org/abs/2602.07529)
*Jianwen Chen,Xinyu Yang,Peng Xia,Arian Azarang,Yueh Z Lee,Gang Li,Hongtu Zhu,Yun Li,Beidi Chen,Huaxiu Yao*

Main category: cs.LG

TL;DR: MedVerse是一个基于Petri网理论的医疗推理框架，将顺序推理转化为可并行的有向无环图过程，提升复杂医疗推理的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗推理任务中表现出色，但其顺序自回归解码方式将本应并行的临床推理（如鉴别诊断）强制为单一线性推理路径，限制了复杂医疗问题的效率和可靠性。

Method: 1. 数据层面：MedVerse Curator自动合成知识基础的医疗推理路径并转化为Petri网结构表示；2. 架构层面：提出具有自适应位置索引的拓扑感知注意力机制，支持并行推理同时保持逻辑一致性；3. 系统层面：开发支持并行执行的自定义推理引擎。

Result: MedVerse将通用LLM性能提升高达8.9%；与专业医疗LLM相比，达到相当性能的同时，推理延迟降低1.3倍，生成吞吐量提高1.7倍。

Conclusion: MedVerse通过将医疗推理重构为可并行的DAG过程，有效解决了顺序解码在复杂医疗推理中的局限性，在保持准确性的同时显著提升了推理效率。

Abstract: Large language models (LLMs) have demonstrated strong performance and rapid progress in a wide range of medical reasoning tasks. However, their sequential autoregressive decoding forces inherently parallel clinical reasoning, such as differential diagnosis, into a single linear reasoning path, limiting both efficiency and reliability for complex medical problems. To address this, we propose MedVerse, a reasoning framework for complex medical inference that reformulates medical reasoning as a parallelizable directed acyclic graph (DAG) process based on Petri net theory. The framework adopts a full-stack design across data, model architecture, and system execution. For data creation, we introduce the MedVerse Curator, an automated pipeline that synthesizes knowledge-grounded medical reasoning paths and transforms them into Petri net-structured representations. At the architectural level, we propose a topology-aware attention mechanism with adaptive position indices that supports parallel reasoning while preserving logical consistency. Systematically, we develop a customized inference engine that supports parallel execution without additional overhead. Empirical evaluations show that MedVerse improves strong general-purpose LLMs by up to 8.9%. Compared to specialized medical LLMs, MedVerse achieves comparable performance while delivering a 1.3x reduction in inference latency and a 1.7x increase in generation throughput, enabled by its parallel decoding capability.

</details>


### [209] [Compact Conformal Subgraphs](https://arxiv.org/abs/2602.07530)
*Sreenivas Gollapudi,Kostas Kollias,Kamesh Munagala,Aravindan Vijayaraghavan*

Main category: cs.LG

TL;DR: 提出图基保形压缩框架，通过选择最小子图来捕获规定概率质量，在保持统计有效性的同时减少结构化预测集的大小。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在结构化领域（如路由、规划、序列推荐）会产生过大的预测集，需要一种既能保持统计有效性又能压缩结构复杂度的新方法。

Method: 将压缩问题形式化为选择能捕获规定概率质量的最小子图，转化为超图中加权最密k子图问题，设计高效近似算法，利用参数最小割的单调性保证嵌套性。

Result: 算法实现了常数倍的覆盖率和大小权衡，证明了在特定算法机制下问题可高效近似，通过行程规划和导航模拟验证了方法的有效性。

Conclusion: 该框架将高效保形预测与组合图压缩通过单调性连接起来，在保持统计有效性的同时实现压缩，为结构化预测集提供了新的解决方案。

Abstract: Conformal prediction provides rigorous, distribution-free uncertainty guarantees, but often yields prohibitively large prediction sets in structured domains such as routing, planning, or sequential recommendation. We introduce "graph-based conformal compression", a framework for constructing compact subgraphs that preserve statistical validity while reducing structural complexity. We formulate compression as selecting a smallest subgraph capturing a prescribed fraction of the probability mass, and reduce to a weighted version of densest $k$-subgraphs in hypergraphs, in the regime where the subgraph has a large fraction of edges. We design efficient approximation algorithms that achieve constant factor coverage and size trade-offs. Crucially, we prove that our relaxation satisfies a monotonicity property, derived from a connection to parametric minimum cuts, which guarantees the nestedness required for valid conformal guarantees. Our results on the one hand bridge efficient conformal prediction with combinatorial graph compression via monotonicity, to provide rigorous guarantees on both statistical validity, and compression or size. On the other hand, they also highlight an algorithmic regime, distinct from classical densest-$k$-subgraph hardness settings, where the problem can be approximated efficiently. We finally validate our algorithmic approach via simulations for trip planning and navigation, and compare to natural baselines.

</details>


### [210] [Gaussian Match-and-Copy: A Minimalist Benchmark for Studying Transformer Induction](https://arxiv.org/abs/2602.07562)
*Antoine Gonon,Alexandre Cordonnier,Nicolas Boumal*

Main category: cs.LG

TL;DR: 论文提出了Gaussian Match-and-Copy基准，用于分离检索和记忆，研究Transformer如何发展匹配-复制电路，并分析了梯度下降的隐式偏差导致硬匹配选择


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型中匹配-复制检索原语如何在自然数据中涌现具有挑战性，因为检索和记忆是纠缠在一起的。需要分离这两种机制来研究纯粹的检索能力。

Method: 引入Gaussian Match-and-Copy基准，通过纯二阶相关信号隔离长程检索。在简化注意力设置中分析优化动态，研究梯度下降如何驱动参数对齐最大间隔分离器。

Result: GMC基准保留了Transformer在实践中发展匹配-复制电路的关键定性方面，并能区分不同架构的检索能力。在特定条件下，梯度下降会使参数发散同时方向对齐最大间隔分离器，产生硬匹配选择。

Conclusion: GMC基准为研究检索机制提供了可控环境，揭示了梯度下降的隐式偏差如何驱动模型学习硬匹配选择，这有助于理解Transformer中匹配-复制电路的涌现机制。

Abstract: Match-and-copy is a core retrieval primitive used at inference time by large language models to retrieve a matching token from the context then copy its successor. Yet, understanding how this behavior emerges on natural data is challenging because retrieval and memorization are entangled. To disentangle the two, we introduce Gaussian Match-and-Copy (GMC), a minimalist benchmark that isolates long-range retrieval through pure second-order correlation signals. Numerical investigations show that this task retains key qualitative aspects of how Transformers develop match-and-copy circuits in practice, and separates architectures by their retrieval capabilities. We also analyze the optimization dynamics in a simplified attention setting. Although many solutions are a priori possible under a regression objective, including ones that do not implement retrieval, we identify an implicit-bias regime in which gradient descent drives the parameters to diverge while their direction aligns with the max-margin separator, yielding hard match selection. We prove this max-margin alignment for GD trajectories that reach vanishing empirical loss under explicit technical conditions.

</details>


### [211] [Unified Biomolecular Trajectory Generation via Pretrained Variational Bridge](https://arxiv.org/abs/2602.07588)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: cs.LG

TL;DR: PVB是一个预训练变分桥模型，通过编码器-解码器架构将初始结构映射到噪声潜在空间，并通过增强桥匹配将其传输到阶段特定目标，统一了单结构和配对轨迹数据的训练，能够高效生成分子动力学轨迹。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟虽然能提供全原子分辨率的分子行为表征，但计算成本高昂限制了其应用。现有的深度生成模型要么在不同系统间泛化能力差，要么由于轨迹数据分子多样性有限而无法充分利用结构信息来提高生成保真度。

Method: 提出预训练变分桥（PVB）模型，采用编码器-解码器架构：1）将初始结构映射到噪声潜在空间；2）通过增强桥匹配将其传输到阶段特定目标；3）统一单结构和配对轨迹数据的训练；4）对于蛋白质-配体复合物，引入基于强化学习的伴随匹配优化，加速向holo状态的进展。

Result: 在蛋白质和蛋白质-配体复合物上的实验表明，PVB能够忠实地再现分子动力学的热力学和动力学观测值，同时提供稳定高效的生成动力学，支持对接构象的高效后优化。

Conclusion: PVB通过统一跨训练阶段的跨域结构知识使用，成功解决了现有深度生成模型在分子动力学轨迹生成中的泛化能力和保真度问题，为高效分子模拟提供了新方法。

Abstract: Molecular Dynamics (MD) simulations provide a fundamental tool for characterizing molecular behavior at full atomic resolution, but their applicability is severely constrained by the computational cost. To address this, a surge of deep generative models has recently emerged to learn dynamics at coarsened timesteps for efficient trajectory generation, yet they either generalize poorly across systems or, due to limited molecular diversity of trajectory data, fail to fully exploit structural information to improve generative fidelity. Here, we present the Pretrained Variational Bridge (PVB) in an encoder-decoder fashion, which maps the initial structure into a noised latent space and transports it toward stage-specific targets through augmented bridge matching. This unifies training on both single-structure and paired trajectory data, enabling consistent use of cross-domain structural knowledge across training stages. Moreover, for protein-ligand complexes, we further introduce a reinforcement learning-based optimization via adjoint matching that speeds progression toward the holo state, which supports efficient post-optimization of docking poses. Experiments on proteins and protein-ligand complexes demonstrate that PVB faithfully reproduces thermodynamic and kinetic observables from MD while delivering stable and efficient generative dynamics.

</details>


### [212] [Beyond Arrow: From Impossibility to Possibilities in Multi-Criteria Benchmarking](https://arxiv.org/abs/2602.07593)
*Polina Gordienko,Christoph Jansen,Julian Rodemann,Georg Schollmeyer*

Main category: cs.LG

TL;DR: 论文将多指标基准测试形式化为社会选择问题，证明了在特定偏好结构条件下（单峰、群可分、距离受限），可以构建稳定的模型排名，解决了传统聚合方法的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现代基准测试（如HELM MMLU）包含准确性、鲁棒性、效率等多个指标，将这些指标聚合成单一排名时，传统方法会出现不一致或不稳定的问题。论文旨在解决多指标基准测试中的聚合难题。

Method: 将基准测试形式化为社会选择问题：每个指标在数据集上诱导出模型偏好排名，基准算子聚合这些"投票"。研究在三种偏好结构限制下（单峰偏好、群可分偏好、距离受限偏好）的聚合可能性。

Result: 证明了在这三种偏好结构条件下，基准算子能够构建行为良好的模型排名。实证研究了HELM MMLU等现代基准套件，验证了哪些结构条件在哪些基准问题上得到满足。

Conclusion: 虽然Arrow不可能定理表明一般情况下的完美聚合不可能，但论文证明在现实基准测试中常见的偏好结构条件下，有意义的多标准基准测试是可行的，为构建稳定的模型排名提供了理论基础。

Abstract: Modern benchmarks such as HELM MMLU account for multiple metrics like accuracy, robustness and efficiency. When trying to turn these metrics into a single ranking, natural aggregation procedures can become incoherent or unstable to changes in the model set. We formalize this aggregation as a social choice problem where each metric induces a preference ranking over models on each dataset, and a benchmark operator aggregates these votes across metrics. While prior work has focused on Arrow's impossibility result, we argue that the impossibility often originates from pathological examples and identify sufficient conditions under which these disappear, and meaningful multi-criteria benchmarking becomes possible. In particular, we deal with three restrictions on the combinations of rankings and prove that on single-peaked, group-separable and distance-restricted preferences, the benchmark operator allows for the construction of well-behaved rankings of the involved models. Empirically, we investigate several modern benchmark suites like HELM MMLU and verify which structural conditions are fulfilled on which benchmark problems.

</details>


### [213] [Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization](https://arxiv.org/abs/2602.07596)
*Xi Chen,Ming Li,Junxi Li,Changsheng Li,Peisong Wang,Lizhong Ding,Ye Yuan,Guoren Wang*

Main category: cs.LG

TL;DR: Astro是一个激活引导的结构化正则化框架，通过重构内在鲁棒的权重来抑制权重和激活异常值对量化精度的影响，实现零推理延迟且与主流量化方法正交。


<details>
  <summary>Details</summary>
Motivation: 仅权重的训练后量化对于大型语言模型的高效部署至关重要，但受到权重和激活异常值导致的精度下降问题困扰。现有缓解策略要么抑制异常值不足，要么带来显著的部署效率问题（如推理延迟、繁重预处理或复杂的算子融合）。

Method: 基于过参数化LLM通常收敛到平坦最小值的洞察，提出Astro框架：利用激活引导的正则化目标主动重构内在鲁棒的权重，积极抑制与高幅度激活对应的权重异常值，同时保持模型精度。该方法引入零推理延迟且与GPTQ等主流量化方法正交。

Result: 在LLaMA-2-7B上，Astro实现了比复杂的基于学习的旋转方法更好的性能，且量化时间仅为后者的约1/3。广泛的实验表明Astro具有高度竞争力的性能。

Conclusion: Astro通过激活引导的结构化正则化有效解决了权重和激活异常值对量化精度的影响，提供了一种硬件友好且高效的解决方案，在保持模型精度的同时实现了零推理延迟，为LLM的高效部署提供了实用方法。

Abstract: Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.

</details>


### [214] [Rational Transductors](https://arxiv.org/abs/2602.07599)
*Mehryar Mohri*

Main category: cs.LG

TL;DR: Rational Transductors：一种双流架构，通过加权有限自动机的矩阵递归增强Transformer，使其能够处理正则语言和NC¹完全问题，解决标准Transformer在序列逻辑和长度泛化上的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer在语义建模方面表现出色，但在刚性序列逻辑和状态跟踪方面存在困难。理论研究表明自注意力机制在硬注意力下限于AC⁰类，软注意力下限于TC⁰类，这些复杂度类通常无法在没有中间思维链的情况下支持序列问题的稳健长度泛化。

Method: 提出Rational Transductors双流架构，通过从加权有限自动机（WFA）派生的矩阵递归增强Transformer。通过深度理性注入方案将理性状态信息注入注意力机制，严格推广了Transformer的表达能力。

Result: 该框架能够捕获所有正则语言、NC¹完全问题（如布尔公式求值）以及基本分离问题（如奇偶性和模计数），同时保持O(L + log T)的并行时间复杂度。理论分析和实证结果表明，Rational Transductors解决了"正则差距"，在标准Transformer失败的算法任务上实现了稳健的长度泛化。

Conclusion: Rational Transductors通过结合Transformer的语义建模能力和WFA的序列处理能力，填补了标准Transformer在序列逻辑处理上的理论空白，同时避免了传统RNN的顺序计算瓶颈，为序列建模提供了更强大的理论基础和实际解决方案。

Abstract: Standard Transformers excel at semantic modeling but struggle with
  rigid sequential logic and state tracking. Theoretical work
  establishes that self-attention is limited to $\AC^0$ (under hard
  attention) or $\TC^0$ (under soft attention), complexity classes
  that often fail to support robust length generalization on
  sequential problems without intermediate chain-of-thought. In this
  work, we introduce \emph{Rational Transductors}, a dual-stream
  architecture that augments the Transformer with a matrix-valued
  recurrence derived from Weighted Finite Automata (WFA). By
  injecting rational state information into the attention mechanism
  via a \emph{Deep Rational Injection} scheme, our framework strictly
  generalizes the expressive power of Transformers to capture all
  Regular Languages, $\NC^1$-complete problems (such as Boolean
  Formula Evaluation), and fundamental separations like Parity and
  Modular Counting, while preserving $O(L + \log T)$ parallel time
  complexity. We ground the architecture in a rigorous learning
  theory: we prove that \emph{Random Rational Features} act as a
  universal basis for sequential dependencies, justifying our
  initialization strategy, while establishing that the
  \emph{Differentiable Rational Feature} regime is necessary to close
  the representational compactness gap. Theoretical analysis and
  empirical results demonstrate that Rational Transductors solve the
  "Regular Gap," enabling robust length generalization on algorithmic
  tasks where standard Transformers fail, without the sequential
  computational bottlenecks of traditional RNNs.

</details>


### [215] [SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models](https://arxiv.org/abs/2602.07616)
*Juntong Wu,Jialiang Cheng,Fuyu Lv,Ou Dan,Li Yuan*

Main category: cs.LG

TL;DR: SERE是一种基于相似性的专家重路由方法，用于提升MoE模型批量解码效率，通过动态减少活跃专家数量实现最高2倍加速，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: MoE模型在生产环境中需要批量推理以优化硬件效率，但这可能导致专家过度激活，从而减慢内存受限的解码阶段。需要解决批量解码与专家稀疏性之间的根本矛盾。

Method: SERE通过相似性分析动态减少活跃专家数量，将次要专家的token重路由到最相似的主要专家，同时利用相似性模式识别并保留关键专家，避免静态专家剪枝或合并，实现基于批量级专家冗余的动态专家跳过。

Result: 在各种复杂推理基准测试中，SERE实现了最高2.0倍的加速，同时质量损失最小，为大规模MoE部署提供了实用的成本效益和延迟敏感解决方案。

Conclusion: SERE通过动态专家重路由有效解决了MoE模型批量解码的效率问题，提供了即插即用的解决方案，支持vLLM中的单行代码更改，具有实际部署价值。

Abstract: Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.

</details>


### [216] [TASTE: Task-Aware Out-of-Distribution Detection via Stein Operators](https://arxiv.org/abs/2602.07640)
*Michał Kozyra,Gesine Reinert*

Main category: cs.LG

TL;DR: 提出TASTE框架，基于Stein算子将分布偏移与模型输入敏感性联系起来，实现任务感知的OOD检测，并提供可解释的像素级诊断


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法要么是数据中心的（仅检测训练输入分布的偏差），要么是模型中心的（依赖分类器输出而不考虑数据几何结构），缺乏将分布偏移与模型任务性能联系起来的任务感知方法

Method: 提出TASTE框架，基于Stein算子将分布偏移与模型输入敏感性联系起来，该算子可解释为分布偏移在模型敏感性场上的投影，支持坐标分解实现偏移定位，对图像数据提供像素级诊断

Result: 在受控高斯偏移、MNIST几何扰动和CIFAR-10扰动基准测试中，该方法与任务退化紧密对齐，且优于现有基线方法

Conclusion: TASTE框架通过Stein算子将分布偏移与模型敏感性联系起来，不仅检测偏移存在，还能定位偏移并提供可解释诊断，在任务感知的OOD检测方面表现优异

Abstract: Out-of-distribution detection methods are often either data-centric, detecting deviations from the training input distribution irrespective of their effect on a trained model, or model-centric, relying on classifier outputs without explicit reference to data geometry. We propose TASTE (Task-Aware STEin operators): a task-aware framework based on so-called Stein operators, which allows us to link distribution shift to the input sensitivity of the model. We show that the resulting operator admits a clear geometric interpretation as a projection of distribution shift onto the sensitivity field of the model, yielding theoretical guarantees. Beyond detecting the presence of a shift, the same construction enables its localisation through a coordinate-wise decomposition, and for image data-provides interpretable per-pixel diagnostics. Experiments on controlled Gaussian shifts, MNIST under geometric perturbations, and CIFAR-10 perturbed benchmarks demonstrate that the proposed method aligns closely with task degradation while outperforming established baselines.

</details>


### [217] [Continuous Program Search](https://arxiv.org/abs/2602.07659)
*Matthew Siper,Muhammad Umair Nasir,Ahmed Khalifa,Lisa Soros,Jay Azhang,Julian Togelius*

Main category: cs.LG

TL;DR: 论文提出一种学习连续程序空间的方法，通过几何编译变异算子提升遗传编程的搜索效率，在交易策略优化中实现数量级评估次数减少和更高样本外夏普比率。


<details>
  <summary>Details</summary>
Motivation: 遗传编程虽然能产生可解释程序，但小的语法变异可能导致大的、不可预测的行为变化，降低局部性和样本效率。这被框定为算子设计问题：学习一个连续程序空间，其中潜在距离具有行为意义，然后设计利用这种结构而不改变进化优化器的变异算子。

Method: 通过跟踪受控潜在扰动下的动作级分歧来测量局部性，确定行为局部连续变化的经验信任区域。使用包含四个语义组件（多头/空头入场和出场）的紧凑交易策略DSL，学习匹配的块因子化嵌入，并比较全潜在空间上的各向同性高斯变异与几何编译变异（后者将更新限制在语义配对的入场-出场子空间，并使用基于学习流模型的方向建议）。

Result: 在五个资产上使用相同的(μ+λ)进化策略和固定评估预算下，学习到的变异算子使用数量级更少的评估发现强策略，并实现最高的中位数样本外夏普比率。虽然各向同性变异偶尔能达到更高的峰值性能，但几何编译变异产生更快、更可靠的进展。

Conclusion: 语义对齐的变异可以显著提高搜索效率，而无需修改底层进化算法，证明了通过精心设计的变异算子利用程序语义结构的重要性。

Abstract: Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.
  We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.
  Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.

</details>


### [218] [Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation](https://arxiv.org/abs/2602.07670)
*Jarrod Barnes*

Main category: cs.LG

TL;DR: 研究发现，在可验证执行基础任务中，搜索策略优于测试时训练，最佳N采样达到90%任务成功率，而梯度适应仅30.6%。关键发现是惊奇引导选择策略能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究测试时训练是否是可验证执行基础任务的最佳策略，特别是在GPU内核优化等具有密集连续奖励信号的领域，探索计算最优的测试时策略。

Method: 使用KernelBench作为测试平台和120B参数模型，比较测试时训练与搜索策略。提出惊奇引导选择策略：选择最高惊奇值（最低置信度）的正确样本，并扩展到惊奇引导前3选择。

Result: 最佳N采样在K=64时达到90%任务成功率，而测试时训练仅30.6%。惊奇引导选择比最置信选择提升30%成功率，惊奇引导前3选择达到100%成功率，匹配oracle性能。

Conclusion: 对于密集奖励的可验证执行基础任务，计算资源应分配给样本多样性和智能选择而非梯度适应。惊奇引导选择原则可能推广到其他执行基础领域，其中最优解位于分布尾部。

Abstract: Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's "equivalent K" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.

</details>


### [219] [Federated Learning with Profile Mapping under Distribution Shifts and Drifts](https://arxiv.org/abs/2602.07671)
*Mohan Li,Dario Fenoglio,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: Feroma是一个新颖的联邦学习框架，通过客户端分布配置文件处理数据异构性中的分布偏移和漂移，无需客户端身份或集群信息，实现动态模型聚合和测试时模型分配。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在真实世界数据异构性下性能下降，无法同时处理跨客户端的分布偏移和随时间变化的分布漂移，且依赖不现实的假设（如已知客户端集群数量和数据异构类型），限制了泛化能力。

Method: Feroma基于客户端分布配置文件（紧凑、隐私保护的本地数据表示），通过自适应相似性加权指导模型聚合和测试时模型分配。该设计允许动态选择聚合策略（从集群化到个性化），并为未见、未标记的测试客户端部署合适模型，无需重新训练、在线适应或先验知识。

Result: 与10个最先进方法相比，Feroma在动态数据异构条件下显著提升性能和稳定性：在6个基准测试中平均准确率比最佳基线提高最多12个百分点，同时保持与FedAvg相当的计算和通信开销。

Conclusion: 基于分布配置文件的聚合为在数据分布偏移和漂移下实现鲁棒联邦学习提供了实用路径，Feroma框架在无需客户端身份或集群信息的情况下有效处理动态数据异构性。

Abstract: Federated Learning (FL) enables decentralized model training across clients without sharing raw data, but its performance degrades under real-world data heterogeneity. Existing methods often fail to address distribution shift across clients and distribution drift over time, or they rely on unrealistic assumptions such as known number of client clusters and data heterogeneity types, which limits their generalizability. We introduce Feroma, a novel FL framework that explicitly handles both distribution shift and drift without relying on client or cluster identity. Feroma builds on client distribution profiles-compact, privacy-preserving representations of local data-that guide model aggregation and test-time model assignment through adaptive similarity-based weighting. This design allows Feroma to dynamically select aggregation strategies during training, ranging from clustered to personalized, and deploy suitable models to unseen, and unlabeled test clients without retraining, online adaptation, or prior knowledge on clients' data. Extensive experiments show that compared to 10 state-of-the-art methods, Feroma improves performance and stability under dynamic data heterogeneity conditions-an average accuracy gain of up to 12 percentage points over the best baselines across 6 benchmarks-while maintaining computational and communication overhead comparable to FedAvg. These results highlight that distribution-profile-based aggregation offers a practical path toward robust FL under both data distribution shifts and drifts.

</details>


### [220] [ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets](https://arxiv.org/abs/2602.07674)
*Bohdan Turbal,Iryna Voitsitska,Lesia Semenova*

Main category: cs.LG

TL;DR: ElliCE是一个新颖的鲁棒算法追索框架，通过优化Rashomon集合的椭球近似上的反事实解释，确保在模型不确定性下的可靠追索建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型直接影响人们的生活决策，需要理解如何通过改变输入获得更好结果。当Rashomon集合（近最优模型集合）很大时，标准的反事实解释可能不可靠，因为对一个模型有效的追索行动可能对另一个模型无效。

Method: 引入ElliCE框架，通过优化Rashomon集合的椭球近似上的反事实解释，确保追索行动在该椭球范围内有效。该方法提供理论保证，包括唯一性、稳定性和与关键特征方向的对齐。

Result: ElliCE生成的反事实解释不仅更鲁棒，而且更灵活，能够适应用户指定的特征约束，同时在计算速度上显著优于现有基线方法。

Conclusion: ElliCE为模型不确定性下的可靠追索提供了原则性和实用的解决方案，确保即使用户模型演化，也能提供稳定的推荐建议。

Abstract: Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified feature constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve.

</details>


### [221] [Dense Feature Learning via Linear Structure Preservation in Medical Data](https://arxiv.org/abs/2602.07706)
*Yuanyun Zhang,Mingxuan Zhang,Siyuan Li,Zihan Wang,Haoran Chen,Wenbo Zhou,Shi Li*

Main category: cs.LG

TL;DR: 该论文提出了一种名为"密集特征学习"的表示学习框架，通过直接优化嵌入矩阵的线性代数特性（谱平衡、子空间一致性、特征正交性），来改善医学数据的表示质量，而不依赖标签或生成重建。


<details>
  <summary>Details</summary>
Motivation: 传统医学深度学习模型通常使用任务特定目标进行训练，这会导致表示坍缩到少量判别方向上。这种方法未能充分利用临床数据的丰富结构，限制了学习特征的迁移性、稳定性和可解释性。

Method: 提出了密集特征学习框架，直接操作嵌入矩阵，通过定义在纯线性代数特性上的目标函数来鼓励谱平衡、子空间一致性和特征正交性。该方法不依赖标签或生成重建。

Result: 在纵向电子健康记录数据、临床文本和多模态患者表示上的实证评估表明，相比监督和自监督基线方法，该方法在下游线性性能、鲁棒性和子空间对齐方面都有持续改进。

Conclusion: 学习覆盖临床变异可能与学习预测临床结果同等重要，应将表示几何作为医学AI的一等目标。密集特征学习为改善医学表示的质量提供了有前景的方向。

Abstract: Deep learning models for medical data are typically trained using task specific objectives that encourage representations to collapse onto a small number of discriminative directions. While effective for individual prediction problems, this paradigm underutilizes the rich structure of clinical data and limits the transferability, stability, and interpretability of learned features. In this work, we propose dense feature learning, a representation centric framework that explicitly shapes the linear structure of medical embeddings. Our approach operates directly on embedding matrices, encouraging spectral balance, subspace consistency, and feature orthogonality through objectives defined entirely in terms of linear algebraic properties. Without relying on labels or generative reconstruction, dense feature learning produces representations with higher effective rank, improved conditioning, and greater stability across time. Empirical evaluations across longitudinal EHR data, clinical text, and multimodal patient representations demonstrate consistent improvements in downstream linear performance, robustness, and subspace alignment compared to supervised and self supervised baselines. These results suggest that learning to span clinical variation may be as important as learning to predict clinical outcomes, and position representation geometry as a first class objective in medical AI.

</details>


### [222] [Analyzing and Guiding Zero-Shot Posterior Sampling in Diffusion Models](https://arxiv.org/abs/2602.07715)
*Roi Benita,Michael Elad,Joseph Keshet*

Main category: cs.LG

TL;DR: 论文提出了一种基于高斯先验假设的零样本扩散逆问题求解器的严格分析方法，建立了闭式表达，并引入了一个原则性的参数设计框架来替代启发式选择策略。


<details>
  <summary>Details</summary>
Motivation: 现有零样本扩散方法在逆问题求解中依赖手动调整和启发式策略，缺乏理论分析基础，需要建立更严格的数学框架来指导算法设计和参数选择。

Method: 在假设先验为高斯分布的条件下，推导出理想后验采样器和扩散重建算法的闭式表达，在谱域进行系统分析，并构建一个与算法无关的原则性参数设计框架。

Result: 提出的谱域分析方法揭示了参数选择与标准启发式策略的结构性差异，表明参数应随扩散步长变化，能够在感知质量和信号保真度之间取得一致平衡。

Conclusion: 该研究为零样本扩散逆问题求解提供了理论基础和系统设计方法，通过谱域分析和原则性参数框架实现了更优的重建性能，为后续研究提供了新方向。

Abstract: Recovering a signal from its degraded measurements is a long standing challenge in science and engineering. Recently, zero-shot diffusion based methods have been proposed for such inverse problems, offering a posterior sampling based solution that leverages prior knowledge. Such algorithms incorporate the observations through inference, often leaning on manual tuning and heuristics. In this work we propose a rigorous analysis of such approximate posterior-samplers, relying on a Gaussianity assumption of the prior. Under this regime, we show that both the ideal posterior sampler and diffusion-based reconstruction algorithms can be expressed in closed-form, enabling their thorough analysis and comparisons in the spectral domain. Building on these representations, we also introduce a principled framework for parameter design, replacing heuristic selection strategies used to date. The proposed approach is method-agnostic and yields tailored parameter choices for each algorithm, jointly accounting for the characteristics of the prior, the degraded signal, and the diffusion dynamics. We show that our spectral recommendations differ structurally from standard heuristics and vary with the diffusion step size, resulting in a consistent balance between perceptual quality and signal fidelity.

</details>


### [223] [Efficient Planning in Reinforcement Learning via Model Introspection](https://arxiv.org/abs/2602.07719)
*Gabriel Stella*

Main category: cs.LG

TL;DR: 该论文提出将强化学习中的内省视为程序分析，建立强化学习与经典规划之间的新联系


<details>
  <summary>Details</summary>
Motivation: 人类在面对任务时，无论任务如何指定，都能通过内省推理内部模型来合成额外任务相关信息。强化学习和经典规划通常被视为两个不同问题，需要不同解决方案，但人类的内省能力表明两者可以建立联系

Method: 将内省视为程序分析，应用于强化学习中使用的各类模型。提出一种算法，能够在关系强化学习模型类上实现高效的目标导向规划

Result: 展示了如何通过程序分析方法在强化学习模型中实现内省，建立了强化学习与经典规划之间的新颖联系

Conclusion: 通过将内省视为程序分析，可以在强化学习模型中合成任务相关信息，从而在关系强化学习模型上实现高效的目标导向规划，为强化学习与经典规划的融合提供了新思路

Abstract: Reinforcement learning and classical planning are typically seen as two distinct problems, with differing formulations necessitating different solutions. Yet, when humans are given a task, regardless of the way it is specified, they can often derive the additional information needed to solve the problem efficiently. The key to this ability is introspection: by reasoning about their internal models of the problem, humans directly synthesize additional task-relevant information. In this paper, we propose that this introspection can be thought of as program analysis. We discuss examples of how this approach can be applied to various kinds of models used in reinforcement learning. We then describe an algorithm that enables efficient goal-oriented planning over the class of models used in relational reinforcement learning, demonstrating a novel link between reinforcement learning and classical planning.

</details>


### [224] [ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs](https://arxiv.org/abs/2602.07721)
*Yanlin Qi,Xinhang Chen,Huiqiang Jiang,Qitong Wang,Botao Peng,Themis Palpanas*

Main category: cs.LG

TL;DR: ParisKV是一个用于长上下文LLM推理的KV-cache检索框架，通过碰撞候选选择和量化内积重排序解决分布漂移和高延迟问题，支持百万token上下文，在解码效率和吞吐量上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有KV-cache检索方法在处理长上下文LLM推理时面临分布漂移和高延迟问题，特别是在大规模场景下表现不佳，需要更高效、更稳健的解决方案。

Method: 基于碰撞候选选择机制，结合量化内积重排序估计器；支持通过统一虚拟寻址实现CPU卸载的KV-cache，实现按需top-k获取；GPU原生设计优化性能。

Result: 在长输入和长生成基准测试中达到或超过全注意力质量；在长上下文解码效率上达到SOTA：在批大小为1时匹配或超过全注意力速度，在可运行范围内提供最高2.8倍吞吐量，支持百万token上下文；相比MagicPIG和PQCache分别减少17倍和44倍解码延迟。

Conclusion: ParisKV是一个高效、稳健的KV-cache检索框架，解决了长上下文LLM推理中的分布漂移和延迟问题，在保持质量的同时显著提升解码效率，支持扩展到百万token规模。

Abstract: KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\times$ and 44$\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.

</details>


### [225] [Do We Need Adam? Surprisingly Strong and Sparse Reinforcement Learning with SGD in LLMs](https://arxiv.org/abs/2602.07729)
*Sagnik Mukherjee,Lifan Yuan,Pavan Jayasinha,Dilek Hakkani-Tür,Hao Peng*

Main category: cs.LG

TL;DR: 研究表明，在大型语言模型的强化学习阶段，简单的SGD优化器比广泛使用的AdamW表现更好，且参数更新稀疏度极高（仅更新不到0.02%的参数），内存效率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练中，强化学习阶段（特别是RLVR）通常沿用预训练和监督微调阶段的优化器实践（如AdamW），但RL与这些阶段存在根本差异。AdamW内存开销大，且其动量机制和自适应学习率在RL中的效果可能不如在监督学习中显著。

Method: 通过分析AdamW在RL和SFT中的不同影响，提出假设：RL从Adam风格的自适应学习率和动量中获益较少。实验验证使用内存效率更高的SGD优化器在LLM的RL训练中表现，并与AdamW进行对比。

Result: SGD在LLM的RL训练中匹配甚至优于AdamW，且参数更新极其稀疏——SGD全参数微调仅更新不到0.02%的参数，比AdamW少1000倍以上，同时不依赖任何稀疏化正则化。

Conclusion: RL阶段的优化动态与监督学习阶段存在本质差异，SGD在RL中表现出色且参数效率极高，这为LLM的RL训练提供了新的优化视角，表明RL可以比以往认知的更加参数高效。

Abstract: Reinforcement learning (RL), particularly RL from verifiable reward (RLVR), has become a crucial phase of training large language models (LLMs) and a key focus of current scaling efforts. However, optimization practices in RL largely follow those of next-token prediction stages (e.g., pretraining and supervised fine-tuning), despite fundamental differences between RL and these stages highlighted by recent work. One such practice is the use of the AdamW optimizer, which is widely adopted for training large-scale transformers despite its high memory overhead. Our analysis shows that both momentum and adaptive learning rates in AdamW are less influential in RL than in SFT, leading us to hypothesize that RL benefits less from Adam-style per-parameter adaptive learning rates and momentum. Confirming this hypothesis, our experiments demonstrate that the substantially more memory-efficient SGD, which is known to perform poorly in supervised learning of large-scale transformers, matches or even outperforms AdamW in RL for LLMs. Remarkably, full fine-tuning with SGD updates fewer than 0.02% of model parameters without any sparsity-promoting regularization, more than 1000 times fewer than AdamW. Our analysis offers potential reasons for this update sparsity. These findings provide new insights into the optimization dynamics of RL in LLMs and show that RL can be substantially more parameter-efficient than previously recognized.

</details>


### [226] [The Laplacian Keyboard: Beyond the Linear Span](https://arxiv.org/abs/2602.07730)
*Siddarth Chandrasekar,Marlos C. Machado*

Main category: cs.LG

TL;DR: Laplacian Keyboard (LK) 是一个分层框架，利用拉普拉斯特征向量构建任务无关的选项库，通过元策略动态组合这些选项，从而超越线性约束学习更复杂的策略。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，拉普拉斯特征向量通常只用于线性近似奖励函数，这限制了在复杂环境中的表达能力。需要一种方法能够超越线性约束，利用这些特征向量构建更灵活的策略表示。

Method: 提出Laplacian Keyboard (LK)分层框架：1) 从拉普拉斯特征向量构建任务无关的选项库，形成行为基；2) 训练元策略动态组合这些选项；3) 该框架保证对线性跨度内的任何奖励函数都能包含最优策略。

Result: 理论分析建立了零样本近似误差的界限。实证结果表明，LK超越了零样本解决方案，同时相比标准强化学习方法实现了更好的样本效率。

Conclusion: Laplacian Keyboard 框架成功地将拉普拉斯特征向量的应用从线性近似扩展到更复杂的策略学习，通过分层选项组合实现了更好的表达能力和学习效率。

Abstract: Across scientific disciplines, Laplacian eigenvectors serve as a fundamental basis for simplifying complex systems, from signal processing to quantum mechanics. In reinforcement learning (RL), these eigenvectors provide a natural basis for approximating reward functions; however, their use is typically limited to their linear span, which restricts expressivity in complex environments. We introduce the Laplacian Keyboard (LK), a hierarchical framework that goes beyond the linear span. LK constructs a task-agnostic library of options from these eigenvectors, forming a behavior basis guaranteed to contain the optimal policy for any reward within the linear span. A meta-policy learns to stitch these options dynamically, enabling efficient learning of policies outside the original linear constraints. We establish theoretical bounds on zero-shot approximation error and demonstrate empirically that LK surpasses zero-shot solutions while achieving improved sample efficiency compared to standard RL methods.

</details>


### [227] [Efficient Adaptive Data Analysis over Dense Distributions](https://arxiv.org/abs/2602.07732)
*Joon Suk Huh*

Main category: cs.LG

TL;DR: 本文提出了一种计算高效的适应性数据分析机制，在数据分布相对于已知先验密集的条件下，实现了最优的O(log T)样本复杂度，解决了计算效率与统计最优性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据工作流本质上是适应性的，反复查询相同数据集以优化和验证顺序决策，但这种适应性可能导致过拟合和无效统计推断。适应性数据分析机制面临计算效率与样本复杂度之间的根本性权衡：计算高效算法通常有次优的O(√T)样本复杂度，而统计最优的O(log T)算法在标准密码学假设下计算不可行。

Method: 提出一种计算高效的ADA机制，当数据分布相对于已知先验密集时，该机制能够达到最优的O(log T)样本复杂度。这种设置特别包括分布特定学习中出现的特征-标签数据分布。该机制还产生分布特定设置中的样本高效统计查询预言机。

Result: 该算法在数据分布相对于已知先验密集的条件下实现了计算效率和最优样本复杂度的双重目标。虽然算法不基于差分隐私，但满足谓词单挑出安全这一松弛的隐私概念，揭示了适应性数据分析与隐私之间的内在联系。

Conclusion: 本文确定了自然的数据分布类别，使得计算效率和最优样本复杂度同时可达成。研究结果揭示了适应性数据分析与差分隐私之外的其他隐私概念之间的内在联系，为实际应用中的适应性数据分析提供了理论保证。

Abstract: Modern data workflows are inherently adaptive, repeatedly querying the same dataset to refine and validate sequential decisions, but such adaptivity can lead to overfitting and invalid statistical inference. Adaptive Data Analysis (ADA) mechanisms address this challenge; however, there is a fundamental tension between computational efficiency and sample complexity. For $T$ rounds of adaptive analysis, computationally efficient algorithms typically incur suboptimal $O(\sqrt{T})$ sample complexity, whereas statistically optimal $O(\log T)$ algorithms are computationally intractable under standard cryptographic assumptions. In this work, we shed light on this trade-off by identifying a natural class of data distributions under which both computational efficiency and optimal sample complexity are achievable. We propose a computationally efficient ADA mechanism that attains optimal $O(\log T)$ sample complexity when the data distribution is dense with respect to a known prior. This setting includes, in particular, feature--label data distributions arising in distribution-specific learning. As a consequence, our mechanism also yields a sample-efficient (i.e., $O(\log T)$ samples) statistical query oracle in the distribution-specific setting. Moreover, although our algorithm is not based on differential privacy, it satisfies a relaxed privacy notion known as Predicate Singling Out (PSO) security (Cohen and Nissim, 2020). Our results thus reveal an inherent connection between adaptive data analysis and privacy beyond differential privacy.

</details>


### [228] [Learnable Chernoff Baselines for Inference-Time Alignment](https://arxiv.org/abs/2602.07738)
*Sunil Madhow,Yuchen Liang,Ness Shroff,Yingbin Liang,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出LCB方法，用于在推理时通过可学习的切尔诺夫基线高效近似采样，实现奖励引导的生成模型对齐，减少对预训练模型的查询次数。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于特定架构调整或计算成本高昂的推理过程，需要一种更高效的推理时奖励引导对齐方法。

Method: 引入可学习的切尔诺夫基线(LCBs)，仅使用预训练模型的黑盒采样访问，通过自适应选择接受概率的拒绝采样实现KL正则化奖励对齐。

Result: 建立了与理想对齐模型的总变差保证，在连续和离散扩散设置中，LCB采样与理想拒绝采样匹配良好，同时显著减少对预训练模型的查询次数。

Conclusion: LCB方法提供了一种高效、可扩展的推理时奖励引导对齐方案，通过自适应拒绝采样实现对推理计算规模的精细控制。

Abstract: We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.

</details>


### [229] [MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training](https://arxiv.org/abs/2602.07790)
*Wanyun Xie,Francesco Tonin,Volkan Cevher*

Main category: cs.LG

TL;DR: MaD-Mix是一个为视觉语言模型训练设计多模态数据混合的框架，通过模态感知的域对齐最大化方法，自动优化数据混合比例，减少人工调参成本。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型训练依赖昂贵的人工调参来确定多模态数据混合比例，特别是在处理复杂多模态场景（如图像-文本-视频）时，手动调优变得不切实际。

Method: MaD-Mix将数据混合问题形式化为模态感知的域对齐最大化，通过Fenchel对偶和跨模态耦合变量获得闭式多模态对齐分数，系统处理缺失模态的域，支持纯语言域的集成。

Result: 在0.5B和7B模型上的实验表明，MaD-Mix能加速VLM训练，在图像-文本指令调优中比人工调优节省22%训练步数，在复杂三模态场景中显著提升平均准确率，混合计算开销极小（<1 GPU小时）。

Conclusion: MaD-Mix提供了一个可扩展、计算高效的多模态数据混合设计框架，能够替代昂贵的人工调优，为现代VLM训练流程提供可扩展的解决方案。

Abstract: Vision-Language Models (VLMs) are typically trained on a diverse set of multi-modal domains, yet current practices rely on costly manual tuning. We propose MaD-Mix, a principled and computationally efficient framework that derives multi-modal data mixtures for VLM training. MaD-Mix formulates data mixing as modality-aware domain alignment maximization and obtains closed-form multi-modal alignment scores from the Fenchel dual through inter-modal coupling variables. MaD-Mix systematically handles domains with missing modalities, allowing for the integration of language-only domains. Empirical evaluations across 0.5B and 7B models demonstrate that MaD-Mix accelerates VLM training across diverse benchmarks. MaD-Mix matches human-tuned data mixtures using 22% fewer training steps in image-text instruction tuning. In complex tri-modal video-image-text scenarios, where manual tuning becomes impractical, MaD-Mix boosts average accuracy over uniform weights, with negligible mixture computation overhead (< 1 GPU-hour), enabling scalable mixture design for modern VLM pipelines.

</details>


### [230] [CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection](https://arxiv.org/abs/2602.07798)
*Ruiqi Wang,Ruikang Liu,Runyu Chen,Haoxiang Suo,Zhiyi Peng,Zhuo Tang,Changjian Chen*

Main category: cs.LG

TL;DR: CausalTaD是一种将因果知识注入LLMs进行表格异常检测的方法，通过识别列间因果关系并重新排序，再对不同列进行加权，在30多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测方法将表格数据转换为文本时随机排列列顺序，忽略了列间的因果关系，而这对准确检测异常至关重要。

Method: 首先识别列间的因果关系并重新排序以对齐这些关系（建模为线性排序问题），然后提出重加权策略为不同列分配不同权重以增强效果。

Result: 在30多个数据集上的实验表明，该方法始终优于当前最先进的方法。

Conclusion: 通过将因果知识注入LLMs进行表格异常检测，CausalTaD方法显著提升了检测性能，证明了考虑列间因果关系的重要性。

Abstract: Detecting anomalies in tabular data is critical for many real-world applications, such as credit card fraud detection. With the rapid advancements in large language models (LLMs), state-of-the-art performance in tabular anomaly detection has been achieved by converting tabular data into text and fine-tuning LLMs. However, these methods randomly order columns during conversion, without considering the causal relationships between them, which is crucial for accurately detecting anomalies. In this paper, we present CausalTaD, a method that injects causal knowledge into LLMs for tabular anomaly detection. We first identify the causal relationships between columns and reorder them to align with these causal relationships. This reordering can be modeled as a linear ordering problem. Since each column contributes differently to the causal relationships, we further propose a reweighting strategy to assign different weights to different columns to enhance this effect. Experiments across more than 30 datasets demonstrate that our method consistently outperforms the current state-of-the-art methods. The code for CausalTAD is available at https://github.com/350234/CausalTAD.

</details>


### [231] [Fairness Aware Reward Optimization](https://arxiv.org/abs/2602.07799)
*Ching Lam Choi,Vighnesh Subramaniam,Phillip Isola,Antonio Torralba,Stefanie Jegelka*

Main category: cs.LG

TL;DR: Faro框架通过训练满足公平性约束的奖励模型来解决LLM对齐中的系统性不公平问题，提供理论保证并显著减少偏见


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据中的人口统计学偏差会通过奖励模型传播到对齐的LLM中，导致系统性不公平，需要开发能够同时保证排序正确性、校准性和公平性的奖励模型训练方法

Method: 提出Fairness Aware Reward Optimization (Faro)框架，在训练奖励模型时施加人口统计学平等、机会均等或反事实公平性约束，提供理论分析包括公平性证书、准确性与公平性权衡的形式化特征以及帕累托前沿的存在性证明

Result: 在多个LLM和基准测试中，Faro显著减少了偏见和有害生成，同时保持或提高了模型质量，证明了奖励模型可以同时具备排序正确性、校准性和公平性

Conclusion: Faro框架为LLM对齐中的公平性提供了有效的解决方案，通过理论保证和实证验证，展示了在奖励模型层面解决公平性问题的可行性，优于预处理和后处理方法

Abstract: Demographic skews in human preference data propagate systematic unfairness through reward models into aligned LLMs. We introduce Fairness Aware Reward Optimization (Faro), an in-processing framework that trains reward models under demographic parity, equalized odds, or counterfactual fairness constraints. We provide the first theoretical analysis of reward-level fairness in LLM alignment, establishing: (i) provable fairness certificates for Faro-trained rewards with controllable slack; a (ii) formal characterization of the accuracy-fairness trade-off induced by KL-regularized fine-tuning, proving fairness transfers from reward to policy; and the (iii) existence of a non-empty Pareto frontier. Unlike pre- and post-processing methods, Faro ensures reward models are simultaneously ordinal (ranking correctly), cardinal (calibrated), and fair. Across multiple LLMs and benchmarks, Faro significantly reduces bias and harmful generations while maintaining or improving model quality.

</details>


### [232] [Efficient Representations are Controllable Representations](https://arxiv.org/abs/2602.07828)
*Charles Ye,Jasmine Cui*

Main category: cs.LG

TL;DR: 通过简单辅助损失微调LLM，在3072个残差流维度中训练16个维度作为惰性可解释性标志，模型围绕这些标志重组并依赖它们进行生成，从而创建可解释的控制开关


<details>
  <summary>Details</summary>
Motivation: 传统方法需要先识别模型内部特征几何结构再进行干预，过程复杂。本文旨在寻找一种更直接、更暴力的方式在模型激活中安装可解释、可控制的特征

Method: 使用简单的辅助损失微调LLM，在3072个残差流维度中专门训练16个维度作为惰性可解释性标志，这些标志指示生成所需的概念。模型在训练过程中围绕这些标志重组，学习在实际生成任务中依赖这些标志

Result: 这些惰性标志成为真正的内部特征：可解释的控制开关，允许在推理时引导生成。当特征在固定位置可靠提供时，梯度下降逐渐消除其他地方的冗余编码，模型侵蚀自身的替代表示

Conclusion: 模型的效率压力是一个可利用的杠杆，可以用来诱导可解释、可控制的表示。通过提供可靠的特征位置，可以迫使模型重组其表示结构，实现直接的特征控制

Abstract: What is the most brute-force way to install interpretable, controllable features into a model's activations? Controlling how LLMs internally represent concepts typically requires sophisticated methods to first identify, then intervene on the model's existing feature geometry. We bypass all of this.
  We finetune an LLM with a simple auxiliary loss, training 16 of its 3072 residual stream dimensions to be inert interpretability flags that simply indicate what concepts are required for generation. The model reorganizes around them anyway, learning to rely on these flags during actual generation tasks. As a result, these inert flags become genuine internal features: interpretable control switches that allow us to steer generation at inference time. Why does this work? When a feature is reliably supplied at a fixed location, gradient descent gradually eliminates redundant encodings elsewhere, and the model erodes its own alternative representations. A model's efficiency pressure is a lever - exploitable to induce interpretable, controllable representations.

</details>


### [233] [rePIRL: Learn PRM with Inverse RL for LLM Reasoning](https://arxiv.org/abs/2602.07832)
*Xian Wu,Kaijie Zhu,Ying Zhang,Lun Wang,Wenbo Guo*

Main category: cs.LG

TL;DR: rePIRL是一个受逆强化学习启发的框架，用于学习有效的过程奖励模型，对专家策略的假设要求最低，在数学和编码推理任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型学习方法要么依赖对专家策略的强假设（如需要其奖励函数），要么存在内在局限性（如熵崩溃），导致模型效果弱或泛化能力有限。

Method: 提出rePIRL框架，设计双重学习过程交替更新策略和过程奖励模型，采用定制化技术解决传统逆强化学习扩展到LLM的挑战。

Result: 在标准化数学和编码推理数据集上的实证评估显示rePIRL优于现有方法，训练出的PRM可用于测试时训练、测试时扩展和为困难问题训练提供早期信号。

Conclusion: rePIRL能够以最小假设学习有效的过程奖励模型，统一了在线和离线PRM学习方法，并通过消融研究验证了训练方案和关键设计选择。

Abstract: Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.

</details>


### [234] [MARTI-MARS$^2$: Scaling Multi-Agent Self-Search via Reinforcement Learning for Code Generation](https://arxiv.org/abs/2602.07848)
*Shijie Wang,Pengfei Li,Yikun Fu,Kaifeng Liu,Fangyuan Li,Yang Liu,Xiaowei Sun,Zonglin Li,Siyao Zhao,Jian Zhao,Kai Tian,Dong Li,Junqi Gao,Yutong Zhang,Yiqun Chen,Yuqiang Li,Zoe Li,Weinan Zhang,Peng Ye,Shuyue Hu,Lei Bai,Bowen Zhou,Kaiyan Zhang,Biqing Qi*

Main category: cs.LG

TL;DR: MARTI-MARS2是一个多智能体强化训练与推理框架，通过将多智能体协作探索过程建模为动态可学习环境，结合策略学习和多智能体树搜索，突破单智能体能力限制，在代码生成任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 单智能体系统在复杂任务（如代码生成）中存在固有的性能瓶颈，而现有的多智能体协作框架通常依赖基于提示的测试时交互或使用同质参数训练的多角色配置，限制了错误纠正能力和策略多样性。

Method: 提出MARTI-MARS2框架，将多智能体协作探索过程建模为动态可学习环境，结合策略学习和多智能体树搜索，使智能体能够在环境中迭代探索和优化。同时提出高效的推理策略MARTI-MARS2-T+，在测试时充分利用多智能体协作的扩展潜力。

Result: 在不同模型规模（8B、14B、32B）的代码生成基准测试中，使用两个协作的32B模型，MARTI-MARS2达到77.7%的性能，优于GPT-5.1等强基线。框架揭示了新的扩展规律：从单智能体到同质多角色再到异质多智能体范式逐步提高RL性能上限、鲁棒的TTS能力和更大的策略多样性。

Conclusion: MARTI-MARS2通过整合策略学习和多智能体树搜索，实现了从参数共享的同质多角色训练到异质多智能体训练的演进，突破了单智能体能力限制。研究表明策略多样性对于通过多智能体强化学习扩展智能至关重要。

Abstract: While the complex reasoning capability of Large Language Models (LLMs) has attracted significant attention, single-agent systems often encounter inherent performance ceilings in complex tasks such as code generation. Multi-agent collaboration offers a promising avenue to transcend these boundaries. However, existing frameworks typically rely on prompt-based test-time interactions or multi-role configurations trained with homogeneous parameters, limiting error correction capabilities and strategic diversity. In this paper, we propose a Multi-Agent Reinforced Training and Inference Framework with Self-Search Scaling (MARTI-MARS2), which integrates policy learning with multi-agent tree search by formulating the multi-agent collaborative exploration process as a dynamic and learnable environment. By allowing agents to iteratively explore and refine within the environment, the framework facilitates evolution from parameter-sharing homogeneous multi-role training to heterogeneous multi-agent training, breaking through single-agent capability limits. We also introduce an efficient inference strategy MARTI-MARS2-T+ to fully exploit the scaling potential of multi-agent collaboration at test time. We conduct extensive experiments across varied model scales (8B, 14B, and 32B) on challenging code generation benchmarks. Utilizing two collaborating 32B models, MARTI-MARS2 achieves 77.7%, outperforming strong baselines like GPT-5.1. Furthermore, MARTI-MARS2 reveals a novel scaling law: shifting from single-agent to homogeneous multi-role and ultimately to heterogeneous multi-agent paradigms progressively yields higher RL performance ceilings, robust TTS capabilities, and greater policy diversity, suggesting that policy diversity is critical for scaling intelligence via multi-agent reinforcement learning.

</details>


### [235] [Dynamic Load Model for Data Centers with Pattern-Consistent Calibration](https://arxiv.org/abs/2602.07859)
*Siyu Lu,Chenhan Xiao,Yang Weng*

Main category: cs.LG

TL;DR: 提出结合物理模型和数据驱动方法的框架，用于大型电子负载建模，通过时间对比学习校准参数，在真实数据中心数据上验证，发现未校准模型无法捕捉负载间的复杂交互效应。


<details>
  <summary>Details</summary>
Motivation: 数据中心的快速增长使大型电子负载建模对电力系统分析日益重要。传统负载模型无法捕捉这类负载的快速工作负载驱动变化以及保护驱动的断开/重连行为。现有物理模型通常未针对设施级运行校准，而数据驱动方法容易过拟合且产生不现实的动态行为。

Method: 设计结合物理模型结构和数据驱动适应性的框架。物理模型参数化以支持从真实运行数据进行数据驱动的模式一致性校准。采用时间对比学习对齐时间和统计模式，而非轨迹级对齐。校准在本地设施进行，仅共享校准参数以保护数据隐私。

Result: 使用MIT Supercloud、ASU Sol、Blue Waters和ASHRAE数据集的真实运行负载数据校准模型，并在ANDES平台上集成，在IEEE 39总线、NPCC 140总线和WECC 179总线系统上评估。发现大型电子负载间的交互会根本改变扰动后恢复行为，产生复合的断开-重连动态和延迟稳定，这些是未校准负载模型无法捕捉的。

Conclusion: 提出的框架成功结合了物理模型的可解释性和数据驱动的适应性，通过模式一致性校准解决了现有方法的局限性。研究揭示了大型电子负载间交互对电力系统动态行为的重要影响，为设施级电网规划提供了更准确的建模工具。

Abstract: The rapid growth of data centers has made large electronic load (LEL) modeling increasingly important for power system analysis. Such loads are characterized by fast workload-driven variability and protection-driven disconnection and reconnection behavior that are not captured by conventional load models. Existing data center load modeling includes physics-based approaches, which provide interpretable structure for grid simulation, and data-driven approaches, which capture empirical workload variability from data. However, physics-based models are typically uncalibrated to facility-level operation, while trajectory alignment in data-driven methods often leads to overfitting and unrealistic dynamic behavior. To resolve these limitations, we design the framework to leverage both physics-based structure and data-driven adaptability. The physics-based structure is parameterized to enable data-driven pattern-consistent calibration from real operational data, supporting facility-level grid planning. We further show that trajectory-level alignment is limited for inherently stochastic data center loads. Therefore, we design the calibration to align temporal and statistical patterns using temporal contrastive learning (TCL). This calibration is performed locally at the facility, and only calibrated parameters are shared with utilities, preserving data privacy. The proposed load model is calibrated by real-world operational load data from the MIT Supercloud, ASU Sol, Blue Waters, and ASHRAE datasets. Then it is integrated into the ANDES platform and evaluated on the IEEE 39-bus, NPCC 140-bus, and WECC 179-bus systems. We find that interactions among LELs can fundamentally alter post-disturbance recovery behavior, producing compound disconnection-reconnection dynamics and delayed stabilization that are not captured by uncalibrated load models.

</details>


### [236] [Direct Soft-Policy Sampling via Langevin Dynamics](https://arxiv.org/abs/2602.07873)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 提出NC-LQL方法，通过噪声条件化Langevin动力学实现软策略采样，解决传统方法在表达性和熵估计方面的限制，在MuJoCo基准测试中达到与扩散方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 现有软策略实现方法存在局限性：参数化策略表达能力有限，扩散策略的不可处理似然阻碍了软策略目标中的可靠熵估计。需要一种既能实现软策略采样又避免这些限制的方法。

Method: 提出噪声条件化Langevin Q学习(NC-LQL)：1) 使用Langevin动力学直接采样目标Boltzmann分布，无需显式参数化策略；2) 引入多尺度噪声扰动到值函数中，学习噪声条件化Q函数；3) 通过渐进平滑的值函数景观实现从全局探索到精确模式细化的采样过程。

Result: 在OpenAI Gym MuJoCo基准测试中，NC-LQL实现了与最先进的扩散方法竞争的性能，为在线强化学习提供了一个简单而强大的解决方案。

Conclusion: NC-LQL通过噪声条件化Langevin动力学有效解决了软策略实现中的挑战，提供了一种无需显式策略参数化的采样方法，在保持简单性的同时实现了高性能。

Abstract: Soft policies in reinforcement learning define policies as Boltzmann distributions over state-action value functions, providing a principled mechanism for balancing exploration and exploitation. However, realizing such soft policies in practice remains challenging. Existing approaches either depend on parametric policies with limited expressivity or employ diffusion-based policies whose intractable likelihoods hinder reliable entropy estimation in soft policy objectives. We address this challenge by directly realizing soft-policy sampling via Langevin dynamics driven by the action gradient of the Q-function. This perspective leads to Langevin Q-Learning (LQL), which samples actions from the target Boltzmann distribution without explicitly parameterizing the policy. However, directly applying Langevin dynamics suffers from slow mixing in high-dimensional and non-convex Q-landscapes, limiting its practical effectiveness. To overcome this, we propose Noise-Conditioned Langevin Q-Learning (NC-LQL), which integrates multi-scale noise perturbations into the value function. NC-LQL learns a noise-conditioned Q-function that induces a sequence of progressively smoothed value landscapes, enabling sampling to transition from global exploration to precise mode refinement. On OpenAI Gym MuJoCo benchmarks, NC-LQL achieves competitive performance compared to state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.

</details>


### [237] [Harpoon: Generalised Manifold Guidance for Conditional Tabular Diffusion](https://arxiv.org/abs/2602.07875)
*Aditya Shankar,Yuandou Wang,Rihan Hai,Lydia Y. Chen*

Main category: cs.LG

TL;DR: HARPOON是一种基于流形理论的表格数据扩散方法，能够在推理时通过流形几何引导无约束样本来满足多样的表格条件，解决了现有方法无法泛化到未见约束和仅限于连续域的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成表格数据的方法存在两个主要问题：1）训练时策略无法泛化到推理时未见约束；2）只能处理表格插补等有限任务。流形理论虽然提供了原则性指导，但现有公式局限于特定推理目标且仅限于连续域。

Method: 将流形理论扩展到表格数据，扩大其处理多样推理目标的能力。基于此提出HARPOON方法，这是一种表格扩散方法，通过流形几何引导无约束样本来满足推理时的各种表格条件。

Result: 在插补和执行不等式约束等任务上验证了理论贡献，HARPOON在不同数据集上表现出色，证明了流形感知引导对表格数据的实际益处。

Conclusion: HARPOON通过扩展流形理论到表格数据，提供了一种能够处理多样推理目标、泛化到未见约束的表格数据生成方法，在多种任务上表现出优越性能。

Abstract: Generating tabular data under conditions is critical to applications requiring precise control over the generative process. Existing methods rely on training-time strategies that do not generalise to unseen constraints during inference, and struggle to handle conditional tasks beyond tabular imputation. While manifold theory offers a principled way to guide generation, current formulations are tied to specific inference-time objectives and are limited to continuous domains. We extend manifold theory to tabular data and expand its scope to handle diverse inference-time objectives. On this foundation, we introduce HARPOON, a tabular diffusion method that guides unconstrained samples along the manifold geometry to satisfy diverse tabular conditions at inference. We validate our theoretical contributions empirically on tasks such as imputation and enforcing inequality constraints, demonstrating HARPOON'S strong performance across diverse datasets and the practical benefits of manifold-aware guidance for tabular data. Code URL: https://github.com/adis98/Harpoon

</details>


### [238] [Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2602.07889)
*Long Chen,Yinkui Liu,Shen Li,Bo Tang,Xuemin Hu*

Main category: cs.LG

TL;DR: 提出基于VQVAE和模糊聚类的离线RL反探索方法，解决现有方法在连续状态-动作对离散化中的维度灾难和信息损失问题，在D4RL基准测试中表现优于现有方法且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习中的反探索方法通过离散化连续状态-动作对进行计数，但存在维度灾难和信息损失问题，导致学习效率降低甚至策略学习失败。

Method: 1. 提出基于多码本VQVAE的高效伪计数方法离散化状态-动作对；2. 设计基于该伪计数方法的离线RL反探索方法；3. 开发基于模糊C均值聚类的码本更新机制提高码本向量利用率。

Result: 在D4RL基准测试中，该方法在多个复杂任务上表现优于现有最先进方法，且需要更少的计算成本。

Conclusion: 提出的基于VQVAE和模糊聚类的反探索方法有效解决了连续状态-动作对离散化中的维度灾难和信息损失问题，提高了离线强化学习的效率和性能。

Abstract: Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.

</details>


### [239] [Safety Alignment as Continual Learning: Mitigating the Alignment Tax via Orthogonal Gradient Projection](https://arxiv.org/abs/2602.07892)
*Guanglong Sun,Siyuan Zhang,Liyuan Wang,Jun Zhu,Hang Su,Yi Zhong*

Main category: cs.LG

TL;DR: OGPSA是一种轻量级方法，通过正交梯度投影解决大语言模型安全对齐中的"对齐税"问题，在保持通用能力的同时提升安全性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在进行安全对齐训练时往往会出现"对齐税"问题，即安全性提升会降低模型的通用能力（如推理和编码）。作者认为这主要是由于连续学习中的遗忘现象造成的，其中分布偏移和冲突目标导致安全更新覆盖了预训练获得的能力。

Method: 将安全对齐视为连续学习问题，提出正交梯度投影安全对齐（OGPSA）方法。该方法通过估计一个低秩能力子空间（从小型参考集的梯度中学习），然后将安全梯度投影到该子空间的正交补空间上进行更新，从而最小化对先前知识的干扰。

Result: 在监督微调（SFT）、直接偏好优化（DPO）以及顺序SFT→DPO设置中，OGPSA始终优于标准基线方法，改善了安全性与通用能力的帕累托前沿。例如，在Qwen2.5-7B-Instruct模型上，OGPSA在保持强大安全性的同时恢复了通用能力，将SimpleQA从0.53%提升到3.03%，IFEval从51.94%提升到63.96%。

Conclusion: OGPSA是一种即插即用的轻量级方法，无需大规模重放、辅助目标或重新训练，能够有效缓解安全对齐中的"对齐税"问题，在保持模型通用能力的同时提升安全性。

Abstract: Large Language Models (LLMs) often incur an alignment tax: safety post-training can reduce general utility (e.g., reasoning and coding). We argue that this tax primarily arises from continual-learning-style forgetting in sequential alignment, where distribution shift and conflicting objectives cause safety updates to overwrite pre-trained competencies. Accordingly, we cast safety alignment as a continual learning (CL) problem that must balance plasticity (acquiring safety constraints) and stability (preserving general abilities). We propose Orthogonal Gradient Projection for Safety Alignment (OGPSA), a lightweight method that mitigates interference by constraining each safety update to be orthogonal (in a first-order sense) to a learned subspace capturing general capabilities. Specifically, OGPSA estimates a low-rank capability subspace from gradients on a small reference set and projects the safety gradient onto its orthogonal complement before updating. This produces safety-directed updates that minimally perturb prior knowledge while retaining capacity for alignment. OGPSA is plug-and-play and integrates into standard post-training pipelines without large-scale replay, auxiliary objectives, or retraining. Across Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and sequential SFT$\rightarrow$DPO settings, OGPSA consistently improves the safety--utility Pareto frontier over standard baselines. For instance, on Qwen2.5-7B-Instruct under SFT$\rightarrow$DPO, OGPSA preserves strong safety while recovering general capability, improving SimpleQA from 0.53\% to 3.03\% and IFEval from 51.94\% to 63.96\%. Our source code is available at \href{https://github.com/SunGL001/OGPSA}{OGPSA}

</details>


### [240] [CausalCompass: Evaluating the Robustness of Time-Series Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2602.07915)
*Huiyang Yi,Xiaojian Shen,Yonggang Wu,Duxin Chen,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: CausalCompass是一个用于评估时间序列因果发现方法在建模假设违反情况下的鲁棒性基准测试套件，实验显示深度学习方法在多种场景下表现最佳，但没有任何方法在所有设置中都最优。


<details>
  <summary>Details</summary>
Motivation: 时间序列因果发现的广泛应用受到两个主要限制：1）对不可测试的因果假设的依赖；2）现有基准测试缺乏面向鲁棒性的评估。为了解决这些问题，作者提出了CausalCompass基准测试套件。

Method: 提出了CausalCompass，一个灵活可扩展的基准测试套件，专门设计用于评估时间序列因果发现方法在建模假设违反情况下的鲁棒性。通过八个假设违反场景对代表性算法进行了广泛基准测试。

Result: 实验结果表明：1）没有任何单一方法在所有设置中都达到最优性能；2）在不同场景下表现出最佳整体性能的方法几乎都是基于深度学习的方法；3）NTS-NOTEARS严重依赖标准化预处理，在原始设置中表现差但在标准化后表现强；4）提供了超参数敏感性分析以深入理解这些发现。

Conclusion: CausalCompass提供了对时间序列因果发现方法在假设违反情况下的全面系统评估，旨在促进这些方法在现实世界应用中的更广泛采用。代码和数据集已开源。

Abstract: Causal discovery from time series is a fundamental task in machine learning. However, its widespread adoption is hindered by a reliance on untestable causal assumptions and by the lack of robustness-oriented evaluation in existing benchmarks. To address these challenges, we propose CausalCompass, a flexible and extensible benchmark suite designed to assess the robustness of time-series causal discovery (TSCD) methods under violations of modeling assumptions. To demonstrate the practical utility of CausalCompass, we conduct extensive benchmarking of representative TSCD algorithms across eight assumption-violation scenarios. Our experimental results indicate that no single method consistently attains optimal performance across all settings. Nevertheless, the methods exhibiting superior overall performance across diverse scenarios are almost invariably deep learning-based approaches. We further provide hyperparameter sensitivity analyses to deepen the understanding of these findings. We also find, somewhat surprisingly, that NTS-NOTEARS relies heavily on standardized preprocessing in practice, performing poorly in the vanilla setting but exhibiting strong performance after standardization. Finally, our work aims to provide a comprehensive and systematic evaluation of TSCD methods under assumption violations, thereby facilitating their broader adoption in real-world applications. The code and datasets are available at https://github.com/huiyang-yi/CausalCompass.

</details>


### [241] [A Kinetic-Energy Perspective of Flow Matching](https://arxiv.org/abs/2602.07928)
*Ziyun Li,Huancheng Hu,Soon Hoe Lim,Xuyu Li,Fei Gao,Enmao Diao,Zezhen Ding,Michalis Vazirgiannis,Henrik Bostrom*

Main category: cs.LG

TL;DR: 论文提出Kinetic Path Energy (KPE)作为流生成模型的诊断工具，发现高KPE对应强语义保真度但可能导致记忆化，进而提出Kinetic Trajectory Shaping (KTS)优化生成质量。


<details>
  <summary>Details</summary>
Motivation: 从物理学视角分析流生成模型，将采样过程视为粒子从噪声到数据的轨迹运动，受经典力学启发，需要量化轨迹的动态努力来理解生成过程。

Method: 引入Kinetic Path Energy (KPE)作为类作用量的每样本诊断指标，测量ODE轨迹的累积动能努力；提出Kinetic Trajectory Shaping (KTS)训练自由的两阶段推理策略，增强早期运动并强制后期软着陆。

Result: KPE表现出两个稳健对应关系：1) 高KPE预测强语义保真度；2) 高KPE轨迹终止于低密度流形边界。理论保证将轨迹能量与数据密度联系起来，但相关性非单调，极高能量会导致记忆化。

Conclusion: 轨迹能量存在Goldilocks原则（适度原则），KTS通过优化轨迹动力学减少记忆化，在基准任务中提高生成质量，为流生成模型提供了新的诊断和优化框架。

Abstract: Flow-based generative models can be viewed through a physics lens: sampling transports a particle from noise to data by integrating a time-varying velocity field, and each sample corresponds to a trajectory with its own dynamical effort. Motivated by classical mechanics, we introduce Kinetic Path Energy (KPE), an action-like, per-sample diagnostic that measures the accumulated kinetic effort along an Ordinary Differential Equation (ODE) trajectory. Empirically, KPE exhibits two robust correspondences: (i) higher KPE predicts stronger semantic fidelity; (ii) high-KPE trajectories terminate on low-density manifold frontiers. We further provide theoretical guarantees linking trajectory energy to data density. Paradoxically, this correlation is non-monotonic. At sufficiently high energy, generation can degenerate into memorization. Leveraging the closed-form of empirical flow matching, we show that extreme energies drive trajectories toward near-copies of training examples. This yields a Goldilocks principle and motivates Kinetic Trajectory Shaping (KTS), a training-free two-phase inference strategy that boosts early motion and enforces a late-time soft landing, reducing memorization and improving generation quality across benchmark tasks.

</details>


### [242] [Attention-Based Deep Learning for Early Parkinson's Disease Detection with Tabular Biomedical Data](https://arxiv.org/abs/2602.07933)
*Olamide Samuel Oseni,Ibraheem Omotolani Obanla,Toheeb Aduramomi Jimoh*

Main category: cs.LG

TL;DR: SAINT注意力模型在帕金森病早期检测中表现最佳，优于MLP、梯度提升和TabNet等基线模型，展示了注意力机制在医疗诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期检测面临挑战，传统机器学习模型依赖特征工程且难以捕捉复杂特征交互，需要更有效的诊断方法。

Method: 使用UCI机器学习库的帕金森病语音测量数据集，比较四种分类模型：多层感知器、梯度提升、TabNet和SAINT，评估其在早期帕金森病检测中的性能。

Result: SAINT在所有评估指标上表现最佳：加权精度0.98、加权召回率0.97、加权F1分数0.97、马修斯相关系数0.9990和最高AUC-ROC。TabNet和MLP表现竞争性，梯度提升表现最差。

Conclusion: 注意力深度学习架构在帕金森病早期检测中具有诊断潜力，双注意力机制能有效建模特征交互，动态特征表示在临床预测任务中很重要。

Abstract: Early and accurate detection of Parkinson's disease (PD) remains a critical challenge in medical diagnostics due to the subtlety of early-stage symptoms and the complex, non-linear relationships inherent in biomedical data. Traditional machine learning (ML) models, though widely applied to PD detection, often rely on extensive feature engineering and struggle to capture complex feature interactions. This study investigates the effectiveness of attention-based deep learning models for early PD detection using tabular biomedical data. We present a comparative evaluation of four classification models: Multi-Layer Perceptron (MLP), Gradient Boosting, TabNet, and SAINT, using a benchmark dataset from the UCI Machine Learning Repository consisting of biomedical voice measurements from PD patients and healthy controls.
  Experimental results show that SAINT consistently outperformed all baseline models across multiple evaluation metrics, achieving a weighted precision of 0.98, weighted recall of 0.97, weighted F1-score of 0.97, a Matthews Correlation Coefficient (MCC) of 0.9990, and the highest Area Under the ROC Curve (AUC-ROC). TabNet and MLP demonstrated competitive performance, while Gradient Boosting yielded the lowest overall scores. The superior performance of SAINT is attributed to its dual attention mechanism, which effectively models feature interactions within and across samples.
  These findings demonstrate the diagnostic potential of attention-based deep learning architectures for early Parkinson's disease detection and highlight the importance of dynamic feature representation in clinical prediction tasks.

</details>


### [243] [A Thermodynamic Theory of Learning Part II: Critical Period Closure and Continual Learning Failure](https://arxiv.org/abs/2602.07950)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 该论文（系列第二部分）从轨迹层面研究有限时间学习中的不可逆性，揭示了有限耗散如何约束学习路径的可达性，导致"关键期关闭"现象，从而将灾难性遗忘重新定义为有限时间耗散施加的动态约束而非直接任务干扰。


<details>
  <summary>Details</summary>
Motivation: 研究有限时间学习中的不可逆性如何影响持续学习，特别是从轨迹层面理解学习路径的动态可达性约束。

Method: 将学习建模为参数分布空间中的传输过程，从轨迹层面分析有限耗散对学习路径的约束，提出"关键期关闭"概念来描述表示自由度的渐进消除。

Result: 有限时间学习不可逆地选择任务等效实现，通过渐进消除能够实现结构重构的自由度，导致关键期关闭现象，使得兼容表示之间的转换在有限耗散预算下变得动态不可达。

Conclusion: 持续学习失败不是由于缺乏满足多个任务的解决方案，而是由先前学习引起的表示自由度的不可逆损失所致，从而将灾难性遗忘重新定义为有限时间耗散施加的动态约束而非直接任务干扰。

Abstract: Learning performed over finite time is necessarily irreversible. In Part~I of this series, we modeled learning as a transport process in the space of parameter distributions and derived the Epistemic Speed Limit, which lower-bounds entropy production under finite-time learning.
  In this work (Part~II), we study the consequences of this irreversibility for continual learning from a trajectory-level perspective. We show that finite dissipation constrains not only which solutions are reachable, but which learning paths remain dynamically accessible.
  Although a continuum of task-equivalent realizations can achieve identical task performance, finite-time learning irreversibly selects among these realizations. This selection occurs through the progressive elimination of degrees of freedom that would otherwise enable structural reconfiguration. We refer to this phenomenon as \emph{critical period closure}: beyond a certain stage of learning, transitions between compatible representations become dynamically inaccessible under any finite dissipation budget.
  As a result, continual learning failure arises not from the absence of solutions satisfying multiple tasks, but from an irreversible loss of representational freedom induced by prior learning. This reframes catastrophic forgetting as a dynamical constraint imposed by finite-time dissipation, rather than direct task interference.

</details>


### [244] [An Explainable Multi-Task Similarity Measure: Integrating Accumulated Local Effects and Weighted Fréchet Distance](https://arxiv.org/abs/2602.07966)
*Pablo Hidalgo,Daniel Rodriguez*

Main category: cs.LG

TL;DR: 本文提出了一种基于可解释人工智能（XAI）技术的多任务相似性度量方法，使用累积局部效应（ALE）曲线和Fréchet距离来衡量任务间的相似性，该方法模型无关且适用于表格和非表格数据。


<details>
  <summary>Details</summary>
Motivation: 在多任务学习（MTL）中，任务通常被视为相互关联的组件，目标是利用任务间的知识转移。因此需要解决关键问题：哪些任务是相似的？它们如何以及为什么表现出相似性？现有的相似性度量方法存在局限性，需要一种更有效的度量工具。

Method: 提出基于可解释人工智能（XAI）技术的多任务相似性度量方法：1）使用累积局部效应（ALE）曲线分析特征对预测的影响；2）通过加权Fréchet距离比较ALE曲线；3）引入缩放因子考虑不同任务的预测性能差异；4）该方法模型无关，可应用于单任务和多任务学习场景。

Result: 在四个数据集上验证了该度量方法的有效性：一个合成数据集和三个真实世界数据集（Parkinson数据集、自行车共享使用数据集、CelebA数据集）。结果表明，该度量方法在表格和非表格数据上都与直观的任务相似性预期一致，能够有效探索任务间的关系。

Conclusion: 提出的基于ALE曲线的多任务相似性度量方法是一个有价值的工具，能够支持任务关系的探索和知情决策制定。该方法模型无关，适用于各种机器学习模型和数据类型，为多任务学习中的任务相似性分析提供了有效解决方案。

Abstract: In many machine learning contexts, tasks are often treated as interconnected components with the goal of leveraging knowledge transfer between them, which is the central aim of Multi-Task Learning (MTL). Consequently, this multi-task scenario requires addressing critical questions: which tasks are similar, and how and why do they exhibit similarity? In this work, we propose a multi-task similarity measure based on Explainable Artificial Intelligence (XAI) techniques, specifically Accumulated Local Effects (ALE) curves.
  ALE curves are compared using the Fréchet distance, weighted by the data distribution, and the resulting similarity measure incorporates the importance of each feature. The measure is applicable in both single-task learning scenarios, where each task is trained separately, and multi-task learning scenarios, where all tasks are learned simultaneously. The measure is model-agnostic, allowing the use of different machine learning models across tasks. A scaling factor is introduced to account for differences in predictive performance across tasks, and several recommendations are provided for applying the measure in complex scenarios.
  We validate this measure using four datasets, one synthetic dataset and three real-world datasets. The real-world datasets include a well-known Parkinson's dataset and a bike-sharing usage dataset -- both structured in tabular format -- as well as the CelebA dataset, which is used to evaluate the application of concept bottleneck encoders in a multitask learning setting. The results demonstrate that the measure aligns with intuitive expectations of task similarity across both tabular and non-tabular data, making it a valuable tool for exploring relationships between tasks and supporting informed decision-making.

</details>


### [245] [On Improving Neurosymbolic Learning by Exploiting the Representation Space](https://arxiv.org/abs/2602.07973)
*Aaditya Naik,Efthymia Tsamoura,Shibo Jin,Mayur Naik,Dan Roth*

Main category: cs.LG

TL;DR: CLIPPER是一种神经符号学习方法，通过整数线性规划剪枝标签组合空间，提升神经符号系统的性能


<details>
  <summary>Details</summary>
Motivation: 在神经符号学习环境中，当隐藏的真实标签必须满足逻辑公式时，可能的标签组合空间会指数级增长，导致学习困难

Method: 提出CLIPPER方法，利用相似潜在表示实例可能共享相同标签的直觉，通过整数线性规划剪枝不一致的标签组合，同时尊重逻辑结构

Result: 在16个复杂神经符号任务基准测试中，CLIPPER将Scallop、Dolphin和ISED等最先进神经符号引擎的性能分别提升48%、53%和8%，达到最先进准确率

Conclusion: CLIPPER是一种正交于现有训练算法的剪枝技术，可无缝集成到神经符号学习系统中，有效解决标签组合空间爆炸问题

Abstract: We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.

</details>


### [246] [Beyond Optimization: Intelligence as Metric-Topology Factorization under Geometric Incompleteness](https://arxiv.org/abs/2602.07974)
*Xin Li*

Main category: cs.LG

TL;DR: 论文提出度量-拓扑分解（MTF）作为智能的几何原理：智能不是固定几何中的优化，而是重塑表示几何以使期望行为成为稳定吸引子的能力。通过将稳定拓扑与可塑度量变形分离，解决了传统ML在分布偏移和持续学习中的稳定性-可塑性困境。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习将智能等同于在固定表示几何中的优化，这在静态环境中有效，但在分布偏移、任务置换和持续学习中会失效。即使轻微的拓扑变化也会使学习到的解决方案失效并引发灾难性遗忘。需要新的几何原理来解决这一根本问题。

Method: 提出度量-拓扑分解（MTF）作为统一几何原理：将表示几何分解为稳定的拓扑结构和可塑的度量变形。学习对应度量收缩（黎曼结构的受控变形），而任务身份和环境变化被编码为拓扑特征并存储在记忆中。进一步提出拓扑Urysohn机（TUM），通过记忆摊销度量推断（MAMI）实现MTF：谱任务签名索引摊销的度量变换，使单个学习到的几何可以在置换、反射或奇偶性改变的环境中重用。

Result: MTF解决了任何固定度量表示的几何不完整性：对于任何局部度量表示，某些拓扑变换会使其奇异或不连贯，这意味着基于权重的系统存在不可避免的稳定性-可塑性权衡。MTF通过分解稳定拓扑和可塑度量变形解决了这一问题，实现了通过几何切换而非重新优化的快速适应。

Conclusion: MTF为理解智能提供了新的几何框架，将智能重新定义为重塑表示几何的能力而非固定几何中的优化。这解释了任务重排的鲁棒性、对灾难性遗忘的抵抗性，以及跨变换的泛化能力，这些能力超越了传统的持续学习方法（如EWC）。

Abstract: Contemporary ML often equates intelligence with optimization: searching for solutions within a fixed representational geometry. This works in static regimes but breaks under distributional shift, task permutation, and continual learning, where even mild topological changes can invalidate learned solutions and trigger catastrophic forgetting. We propose Metric-Topology Factorization (MTF) as a unifying geometric principle: intelligence is not navigation through a fixed maze, but the ability to reshape representational geometry so desired behaviors become stable attractors. Learning corresponds to metric contraction (a controlled deformation of Riemannian structure), while task identity and environmental variation are encoded topologically and stored separately in memory. We show any fixed metric is geometrically incomplete: for any local metric representation, some topological transformations make it singular or incoherent, implying an unavoidable stability-plasticity tradeoff for weight-based systems. MTF resolves this by factorizing stable topology from plastic metric warps, enabling rapid adaptation via geometric switching rather than re-optimization. Building on this, we introduce the Topological Urysohn Machine (TUM), implementing MTF through memory-amortized metric inference (MAMI): spectral task signatures index amortized metric transformations, letting a single learned geometry be reused across permuted, reflected, or parity-altered environments. This explains robustness to task reordering, resistance to catastrophic forgetting, and generalization across transformations that defeat conventional continual learning methods (e.g., EWC).

</details>


### [247] [When Is Compositional Reasoning Learnable from Verifiable Rewards?](https://arxiv.org/abs/2602.07992)
*Daniel Barzilai,Yotam Wolf,Ronen Basri*

Main category: cs.LG

TL;DR: 该论文理论研究在可验证奖励强化学习(RLVR)下自回归模型组合问题的可学习性，提出了任务优势比的概念来刻画哪些任务能从结果级反馈中学习


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR在大型语言模型的组合推理方面取得了经验成功，但尚不清楚哪些组合问题仅通过结果级反馈就能学习，需要理论分析其可学习性

Method: 理论研究自回归模型在RLVR训练下的组合问题可学习性，提出任务优势比作为关键指标，分析不同问题中优势如何自然产生

Result: 当正确中间步骤提供明显优势时，组合问题可通过RLVR高效学习；当结构优势不存在时，RLVR可能收敛到次优组合；基础模型质量影响优势存在与否

Conclusion: 任务优势比是决定RLVR成功与否的关键因素，该分析为理解RLVR何时成功、何时失败提供了原则性理论框架

Abstract: The emergence of compositional reasoning in large language models through reinforcement learning with verifiable rewards (RLVR) has been a key driver of recent empirical successes. Despite this progress, it remains unclear which compositional problems are learnable in this setting using outcome-level feedback alone. In this work, we theoretically study the learnability of compositional problems in autoregressive models under RLVR training. We identify a quantity that we call the task-advantage ratio, a joint property of the compositional problem and the base model, that characterizes which tasks and compositions are learnable from outcome-level feedback. On the positive side, using this characterization, we show that compositional problems where correct intermediate steps provide a clear advantage are efficiently learnable with RLVR. We also analyze how such an advantage naturally arises in different problems. On the negative side, when the structural advantage is not present, RLVR may converge to suboptimal compositions. We prove that, in some cases, the quality of the base model determines if such an advantage exists and whether RLVR will converge to a suboptimal solution. We hope our analysis can provide a principled theoretical understanding of when and why RLVR succeeds and when it does not.

</details>


### [248] [Regret Analysis of Unichain Average Reward Constrained MDPs with General Parameterization](https://arxiv.org/abs/2602.08000)
*Anirudh Satheesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一种针对单链约束马尔可夫决策过程的原始-对偶自然行动者-评论家算法，使用多级蒙特卡洛估计器和显式预热机制，无需混合时间预言机，获得√T量级的遗憾和约束违反边界。


<details>
  <summary>Details</summary>
Motivation: 现有约束强化学习的遗憾分析主要依赖于遍历性或强混合时间假设，这些假设在存在瞬态状态时失效。需要一种能够处理单链动态且无需混合时间预言机的方法。

Method: 提出原始-对偶自然行动者-评论家算法，结合多级蒙特卡洛估计器和显式预热机制来处理单链动态，无需混合时间假设。

Result: 建立了有限时间遗憾和累积约束违反边界，尺度为Õ(√T)，受限于策略和评论家参数化产生的近似误差，将最优阶保证扩展到更广泛的CMDP类。

Conclusion: 该方法成功将最优阶遗憾保证扩展到单链约束马尔可夫决策过程，无需遍历性或混合时间假设，为更广泛的约束强化学习问题提供了理论保证。

Abstract: We study infinite-horizon average-reward constrained Markov decision processes (CMDPs) under the unichain assumption and general policy parameterizations. Existing regret analyses for constrained reinforcement learning largely rely on ergodicity or strong mixing-time assumptions, which fail to hold in the presence of transient states. We propose a primal--dual natural actor--critic algorithm that leverages multi-level Monte Carlo (MLMC) estimators and an explicit burn-in mechanism to handle unichain dynamics without requiring mixing-time oracles. Our analysis establishes finite-time regret and cumulative constraint violation bounds that scale as $\tilde{O}(\sqrt{T})$, up to approximation errors arising from policy and critic parameterization, thereby extending order-optimal guarantees to a significantly broader class of CMDPs.

</details>


### [249] [Don't Always Pick the Highest-Performing Model: An Information Theoretic View of LLM Ensemble Selection](https://arxiv.org/abs/2602.08003)
*Yigit Turkmen,Baturalp Buyukates,Melih Bastopcu*

Main category: cs.LG

TL;DR: 本文提出一种基于互信息最大化的预算约束下大语言模型集成选择方法，通过高斯-连接函数建模模型间相关误差，并设计贪心算法在查询预算内构建最优集成。


<details>
  <summary>Details</summary>
Motivation: 大语言模型集成时模型间存在强相关性，这引发了一个基本问题：在形成LLM集成时应选择哪些模型？同时需要解释为何即使使用多个模型性能也会饱和。

Method: 将预算约束下的集成选择问题形式化为最大化真实标签与所选模型预测之间的互信息。使用高斯-连接函数建模模型间的相关误差，提出贪心互信息选择算法，直接从数据中估计所需信息项，在查询预算内迭代构建集成。

Result: 在MEDMCQA、MMLU和IMDB电影评论三个数据集上进行测试，结果表明该方法在相同查询预算下始终优于强基线方法。

Conclusion: 通过信息论框架分析LLM集成性能，提出的贪心互信息选择算法能有效在预算约束下构建高性能模型集成，为解决模型间强相关性问题提供了有效方案。

Abstract: Large language models (LLMs) are often ensembled together to improve overall reliability and robustness, but in practice models are strongly correlated. This raises a fundamental question: which models should be selected when forming an LLM ensemble? We formulate budgeted ensemble selection as maximizing the mutual information between the true label and predictions of the selected models. Furthermore, to explain why performance can saturate even with many models, we model the correlated errors of the models using Gaussian-copula and show an information-theoretic error floor for the performance of the ensemble. Motivated by these, we propose a simple greedy mutual-information selection algorithm that estimates the required information terms directly from data and iteratively builds an ensemble under a query budget. We test our approach in two question answering datasets and one binary sentiment classification dataset: MEDMCQA, MMLU, and IMDB movie reviews. Across all datasets, we observe that our method consistently outperforms strong baselines under the same query budget.

</details>


### [250] [From $O(mn)$ to $O(r^2)$: Two-Sided Low-Rank Communication for Adam in Distributed Training with Memory Efficiency](https://arxiv.org/abs/2602.08007)
*Sizhe Dang,Jiaqi Shao,Xiaodong Zheng,Guang Dai,Yan Song,Haishan Ye*

Main category: cs.LG

TL;DR: TSR-Adam是一种针对带宽受限分布式训练的低秩通信优化方法，通过同步紧凑的核心矩阵将每步通信负载从O(mn)降低到O(r²)，显著减少通信开销


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模扩大，数据并行分布式预训练中带宽受限的梯度同步成为关键瓶颈。现有的基于投影的低秩优化器主要为内存效率设计，在通信受限的训练中仍不理想，因为单边同步仍需传输O(rn)对象，且刷新步骤可能主导峰值通信字节

Method: 提出TSR-Adam方法：1）通过同步紧凑核心U⊤GV∈ℝ^{r×r}实现Adam族更新的双边低秩通信，将主要每步负载从O(mn)减少到O(r²)；2）采用基于随机SVD的刷新避免全梯度同步；3）将低秩通信扩展到嵌入梯度，使用嵌入特定秩和刷新计划

Result: 在从6000万到10亿参数的模型规模预训练中，TSR-Adam将平均每步通信字节减少13倍；在GLUE微调中减少通信25倍，同时保持可比较的性能表现

Conclusion: TSR-Adam通过双边低秩通信和随机SVD刷新有效解决了大规模分布式训练中的通信瓶颈问题，显著减少了通信开销而不牺牲模型性能，为通信受限的训练提供了高效解决方案

Abstract: As foundation models continue to scale, pretraining increasingly relies on data-parallel distributed optimization, making bandwidth-limited gradient synchronization a key bottleneck. Orthogonally, projection-based low-rank optimizers were mainly designed for memory efficiency, but remain suboptimal for communication-limited training: one-sided synchronization still transmits an $O(rn)$ object for an $m\times n$ matrix gradient and refresh steps can dominate peak communicated bytes. We propose TSR, which brings two-sided low-rank communication to Adam-family updates (TSR-Adam) by synchronizing a compact core $U^\top G V\in\mathbb{R}^{r\times r}$, reducing the dominant per-step payload from $O(mn)$ to $O(r^2)$ while keeping moment states in low-dimensional cores. To further reduce the peak communication from subspace refresh, TSR-Adam adopts a randomized SVD-based refresh that avoids full-gradient synchronization. We additionally extend low-rank communication to embedding gradients with embedding-specific ranks and refresh schedules, yielding additional communication and memory savings over keeping embeddings dense. Across pretraining from 60M to 1B model scales, TSR-Adam reduces average communicated bytes per step by $13\times$, and on GLUE fine-tuning it reduces communication by $25\times$, while achieving comparable performance; we further provide a theoretical stationarity analysis for the proposed update. Code is available at https://github.com/DKmiyan/TSR-Adam.

</details>


### [251] [A Unified Density Operator View of Flow Control and Merging](https://arxiv.org/abs/2602.08012)
*Riccardo De Santi,Malte Franke,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 提出统一概率空间框架，将流模型的奖励适应与模型合并统一处理，支持奖励引导的流合并，实现任务感知的多预训练流组合


<details>
  <summary>Details</summary>
Motivation: 大规模流和扩散模型的发展带来了两个基本算法挑战：基于控制的预训练流奖励适应和多个模型的集成（流合并）。现有方法分别处理这两个问题，需要统一的框架来同时解决这两个挑战

Method: 引入统一的概率空间框架，将奖励适应和流合并作为极限情况包含其中；提出奖励引导流合并（RFM），使用镜像下降方案将奖励引导流合并转化为一系列标准微调问题

Result: 为奖励引导和纯流合并提供了首个理论保证；在分子设计和低能构象生成等高维任务中展示了方法的有效性

Conclusion: 提出的统一框架能够表达丰富的生成模型密度算子，包括交集、并集、插值及其奖励引导版本，通过生成电路支持复杂逻辑表达式，为生成模型的组合操作提供了理论基础和实用方法

Abstract: Recent progress in large-scale flow and diffusion models raised two fundamental algorithmic challenges: (i) control-based reward adaptation of pre-trained flows, and (ii) integration of multiple models, i.e., flow merging. While current approaches address them separately, we introduce a unifying probability-space framework that subsumes both as limit cases, and enables reward-guided flow merging, allowing principled, task-aware combination of multiple pre-trained flows (e.g., merging priors while maximizing drug-discovery utilities). Our formulation renders possible to express a rich family of operators over generative models densities, including intersection (e.g., to enforce safety), union (e.g., to compose diverse models), interpolation (e.g., for discovery), their reward-guided counterparts, as well as complex logical expressions via generative circuits. Next, we introduce Reward-Guided Flow Merging (RFM), a mirror-descent scheme that reduces reward-guided flow merging to a sequence of standard fine-tuning problems. Then, we provide first-of-their-kind theoretical guarantees for reward-guided and pure flow merging via RFM. Ultimately, we showcase the capabilities of the proposed method on illustrative settings providing visually interpretable insights, and apply our method to high-dimensional de-novo molecular design and low-energy conformer generation.

</details>


### [252] [Sharp analysis of linear ensemble sampling](https://arxiv.org/abs/2602.08026)
*Arya Akhavan,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 本文分析了线性集成采样(ES)在高斯扰动下的随机线性赌博机问题，证明了当集成规模为m=Θ(d log n)时，ES能达到$\tilde O(d^{3/2}\sqrt n)$的高概率遗憾界，填补了与Thompson采样基准的差距，同时保持了可比较的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究线性集成采样在随机线性赌博机中的性能，旨在填补其与Thompson采样基准之间的理论差距，同时保持计算效率。探索随机化探索策略在连续时间视角下的理论分析框架。

Method: 采用线性集成采样(ES)结合标准高斯扰动，通过将离散时间问题转化为m个独立布朗运动的时齐超越问题，利用连续时间视角进行分析。集成规模设置为m=Θ(d log n)。

Result: 当集成规模m=Θ(d log n)时，线性集成采样能够达到$\tilde O(d^{3/2}\sqrt n)$的高概率遗憾界，这一结果与Thompson采样基准相匹配，同时计算复杂度保持可比。

Conclusion: 线性集成采样在适当的集成规模下能够达到与Thompson采样相当的理论性能，连续时间视角为分析随机化探索策略提供了自然且可能必要的框架，离散时间问题似乎需要连续时间解决方案才能获得尖锐的边界。

Abstract: We analyse linear ensemble sampling (ES) with standard Gaussian perturbations in stochastic linear bandits. We show that for ensemble size $m=Θ(d\log n)$, ES attains $\tilde O(d^{3/2}\sqrt n)$ high-probability regret, closing the gap to the Thompson sampling benchmark while keeping computation comparable. The proof brings a new perspective on randomized exploration in linear bandits by reducing the analysis to a time-uniform exceedance problem for $m$ independent Brownian motions. Intriguingly, this continuous-time lens is not forced; it appears natural--and perhaps necessary: the discrete-time problem seems to be asking for a continuous-time solution, and we know of no other way to obtain a sharp ES bound.

</details>


### [253] [Horizon Imagination: Efficient On-Policy Training in Diffusion World Models](https://arxiv.org/abs/2602.08032)
*Lior Cohen,Ofir Nabati,Kaixin Wang,Navdeep Kumar,Shie Mannor*

Main category: cs.LG

TL;DR: 提出Horizon Imagination（HI）方法，通过并行去噪多个未来观测来提升扩散世界模型在强化学习中的效率，减少计算成本同时保持控制性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的世界模型在强化学习中虽然生成质量高，但面临严重的效率挑战：要么需要在推理时使用重型模型，要么依赖高度序列化的想象过程，两者都带来过高的计算成本。

Method: 提出Horizon Imagination（HI）方法，这是一种用于离散随机策略的在线策略想象过程，能够并行去噪多个未来观测。HI包含稳定机制和新的采样调度策略，将去噪预算与有效去噪范围解耦，同时支持子帧预算。

Result: 在Atari 100K和Craftium上的实验表明，该方法在使用一半去噪步数的子帧预算下仍能保持控制性能，并在不同调度策略下实现更优的生成质量。

Conclusion: Horizon Imagination方法有效解决了扩散世界模型在强化学习中的效率问题，通过并行去噪和创新的调度策略，在保持控制性能的同时显著降低了计算成本。

Abstract: We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.

</details>


### [254] [The Benefits of Diversity: Combining Comparisons and Ratings for Efficient Scoring](https://arxiv.org/abs/2602.08033)
*Julien Fageot,Matthias Grossglauser,Lê-Nguyên Hoang,Matteo Tacchi-Bénard,Oscar Villemaud*

Main category: cs.LG

TL;DR: SCoRa模型通过结合个体评分和成对比较两种偏好获取方式，在需要准确排序顶级实体的场景中优于单一方法


<details>
  <summary>Details</summary>
Motivation: 长期以来存在关于人类应该单独评估实体还是进行比较评估的争论。本文旨在探索结合两种偏好获取形式是否能超越单一方法，特别是在需要准确排序顶级实体的关键场景中。

Method: 提出了SCoRa（从比较和评分中评分），这是一个统一的概率模型，能够同时从个体评分和成对比较两种信号中学习。证明了SCoRa的MAP估计器具有良好的性质，满足单调性和鲁棒性保证。

Result: 实证研究表明，即使在模型不匹配的情况下，SCoRa也能恢复准确的评分。更重要的是，识别出了一个现实场景：当需要准确排序顶级实体时，结合比较和评分的方法优于单独使用任何一种方法。

Conclusion: 考虑到实际中多种形式信号的可用性，SCoRa为偏好学习提供了一个多功能的基础框架，证明了结合个体评分和成对比较两种偏好获取方式的价值。

Abstract: Should humans be asked to evaluate entities individually or comparatively? This question has been the subject of long debates. In this work, we show that, interestingly, combining both forms of preference elicitation can outperform the focus on a single kind. More specifically, we introduce SCoRa (Scoring from Comparisons and Ratings), a unified probabilistic model that allows to learn from both signals. We prove that the MAP estimator of SCoRa is well-behaved. It verifies monotonicity and robustness guarantees. We then empirically show that SCoRa recovers accurate scores, even under model mismatch. Most interestingly, we identify a realistic setting where combining comparisons and ratings outperforms using either one alone, and when the accurate ordering of top entities is critical. Given the de facto availability of signals of multiple forms, SCoRa additionally offers a versatile foundation for preference learning.

</details>


### [255] [Implicit Strategic Optimization: Rethinking Long-Horizon Decision-Making in Adversarial Poker Environments](https://arxiv.org/abs/2602.08041)
*Boyang Xia,Weiyou Tian,Qingnan Ren,Jiaqi Huang,Jie Xiao,Shuo Lu,Kai Wang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: 论文提出ISO框架，通过预测战略背景来优化LLM智能体在长期对抗游戏中的表现，解决了传统短期优化方法在动态战略环境中失效的问题。


<details>
  <summary>Details</summary>
Motivation: 在长期对抗游戏中，奖励受到随时间演变的潜在战略外部性影响，传统的短期优化和基于变异的遗憾分析在可预测的动态环境中可能变得无效，需要新的框架来处理这种复杂性。

Method: 提出隐式战略优化(ISO)框架，包含战略奖励模型(SRM)来估计行动的长期战略价值，以及iso-grpo（基于情境的乐观学习规则），让智能体预测当前战略背景并在线更新策略。

Result: 理论证明显示次线性情境遗憾和均衡收敛保证，主要项与情境预测错误数量相关；在预测误差有界时，恢复静态游戏的收敛率。在6人无限注德州扑克和竞争性宝可梦实验中，相比强LLM和RL基线，长期回报持续改进，且在受控预测噪声下表现稳健。

Conclusion: ISO框架通过预测战略背景有效解决了长期对抗游戏中的战略优化问题，在理论和实验上都证明了其优于传统方法，为LLM智能体在动态战略环境中的学习提供了有效解决方案。

Abstract: Training large language model (LLM) agents for adversarial games is often driven by episodic objectives such as win rate. In long-horizon settings, however, payoffs are shaped by latent strategic externalities that evolve over time, so myopic optimization and variation-based regret analyses can become vacuous even when the dynamics are predictable. To solve this problem, we introduce Implicit Strategic Optimization (ISO), a prediction-aware framework in which each agent forecasts the current strategic context and uses it to update its policy online. ISO combines a Strategic Reward Model (SRM) that estimates the long-run strategic value of actions with iso-grpo, a context-conditioned optimistic learning rule. We prove sublinear contextual regret and equilibrium convergence guarantees whose dominant terms scale with the number of context mispredictions; when prediction errors are bounded, our bounds recover the static-game rates obtained when strategic externalities are known. Experiments in 6-player No-Limit Texas Hold'em and competitive Pokemon show consistent improvements in long-term return over strong LLM and RL baselines, and graceful degradation under controlled prediction noise.

</details>


### [256] [Interpretable Fuzzy Systems For Forward Osmosis Desalination](https://arxiv.org/abs/2602.08050)
*Qusai Khaled,Uzay Kaymak,Laura Genga*

Main category: cs.LG

TL;DR: 提出了一种人机协同方法开发可解释模糊规则系统，用于预测正渗透海水淡化生产力，在保持语义可解释性的同时达到与聚类方法相当的预测性能


<details>
  <summary>Details</summary>
Motivation: 在水处理应用中，模糊规则系统的可解释性至关重要，因为决策影响公共健康。虽然结构可解释性已通过多目标算法解决，但语义可解释性常因模糊集区分度低而受损

Method: 集成专家驱动的网格划分生成可区分的隶属函数，领域知识引导的特征工程减少冗余，基于触发强度的规则剪枝

Result: 该方法在保持语义可解释性和满足结构复杂度约束的同时，达到了与基于聚类的模糊规则系统相当的预测性能

Conclusion: 为水处理应用提供了一种可解释的解决方案，通过人机协同方法平衡了预测性能和语义可解释性

Abstract: Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.

</details>


### [257] [Epigraph-Guided Flow Matching for Safe and Performant Offline Reinforcement Learning](https://arxiv.org/abs/2602.08054)
*Manan Tayal,Mumuksh Tayal*

Main category: cs.LG

TL;DR: EpiFlow是一种安全的离线强化学习框架，通过状态约束最优控制问题联合优化安全性和性能，使用epigraph值函数指导策略合成，在安全关键任务中实现竞争性回报且几乎零安全违规。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在安全关键领域具有吸引力，但现有方法难以同时实现强安全性和高性能。现有安全离线RL方法要么依赖软约束允许违规，要么过于保守，要么难以平衡安全性、奖励优化和数据分布遵循。

Method: 提出Epigraph-Guided Flow Matching (EpiFlow)框架，将安全离线RL表述为状态约束最优控制问题。学习从最优控制问题的epigraph重构推导出的可行性值函数，避免先前工作中的解耦目标或后处理过滤。策略通过基于epigraph值函数重新加权行为分布，并通过流匹配拟合生成策略来合成。

Result: 在包括Safety-Gymnasium基准测试在内的各种安全关键任务中，EpiFlow实现了竞争性回报且几乎零经验安全违规，证明了epigraph引导策略合成的有效性。

Conclusion: EpiFlow通过epigraph引导的流匹配框架成功解决了安全离线RL中平衡安全性、性能和分布一致性的挑战，为安全关键领域的自主系统训练提供了有效解决方案。

Abstract: Offline reinforcement learning (RL) provides a compelling paradigm for training autonomous systems without the risks of online exploration, particularly in safety-critical domains. However, jointly achieving strong safety and performance from fixed datasets remains challenging. Existing safe offline RL methods often rely on soft constraints that allow violations, introduce excessive conservatism, or struggle to balance safety, reward optimization, and adherence to the data distribution. To address this, we propose Epigraph-Guided Flow Matching (EpiFlow), a framework that formulates safe offline RL as a state-constrained optimal control problem to co-optimize safety and performance. We learn a feasibility value function derived from an epigraph reformulation of the optimal control problem, thereby avoiding the decoupled objectives or post-hoc filtering common in prior work. Policies are synthesized by reweighting the behavior distribution based on this epigraph value function and fitting a generative policy via flow matching, enabling efficient, distribution-consistent sampling. Across various safety-critical tasks, including Safety-Gymnasium benchmarks, EpiFlow achieves competitive returns with near-zero empirical safety violations, demonstrating the effectiveness of epigraph-guided policy synthesis.

</details>


### [258] [Compiler-Assisted Speculative Sampling for Accelerated LLM Inference on Heterogeneous Edge Devices](https://arxiv.org/abs/2602.08060)
*Alejandro Ruiz y Mesa,Guilherme Korol,Moritz Riesteter,João Paulo Cardoso de Lima,Jeronimo Castrillon*

Main category: cs.LG

TL;DR: 该论文提出了一种基于分析成本模型的粗粒度分区方法，用于在资源受限的边缘设备上优化推测解码（Speculative Decoding）性能，解决编译器集成和异构计算资源利用的挑战。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署LLM面临严重的延迟约束，特别是在实时应用中。推测解码虽然是有前景的技术，但在边缘设备上面临两个主要挑战：1）在不牺牲性能或可编程性的情况下将SD集成到基于编译器的工作流中；2）通过精心设计的分区策略利用现代SoC的异构计算资源。

Method: 使用分析成本模型探索异构硬件配置，指导LLM子图的粗粒度分区，特别针对边缘设备典型的短输入序列长度。该模型预测推测采样和异构执行何时共同有益。

Result: 在配备六核Cortex-A CPU和Mali GPU的边缘设备上进行验证，翻译任务实现了最高1.68倍的加速，与分析预期密切匹配。

Conclusion: 提出的分析成本模型能够有效指导边缘设备上推测解码的异构计算资源分区，显著提升LLM在资源受限环境下的推理性能，特别是在短序列场景中。

Abstract: LLM deployment on resource-constrained edge devices faces severe latency constraints, particularly in real-time applications where delayed responses can compromise safety or usability. Among many approaches to mitigate the inefficiencies of sequential token-by-token generation, Speculative Decoding (SD) has emerged as a promising technique. However, SD at the edge is hindered by two major challenges: (1) integrating SD into a compiler-based workflow without sacrificing performance or programmability, and (2) exploiting the heterogeneous compute resources of modern SoCs through carefully designed partitioning strategies. This work addresses these challenges by using an analytical cost model that explores heterogeneous hardware configurations and guides coarse-grained partitioning of LLM subgraphs, particularly with edge-typical short input sequence lengths. The cost model predicts when speculative sampling and heterogeneous execution are jointly beneficial and is validated on an edge device featuring a hexacore Cortex-A CPU and a Mali GPU, revealing up to 1.68$\times$ speedup for translation tasks, closely matching analytic expectations.

</details>


### [259] [Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation](https://arxiv.org/abs/2602.08062)
*Shayan Ali Hassan,Tao Ni,Zafar Ayyub Qazi,Marco Canini*

Main category: cs.LG

TL;DR: BAGEL是一个轻量级、模块化的恶意提示检测框架，通过集成多个专门化的小型模型，在保持高性能的同时显著降低计算成本，并支持增量更新以适应新攻击。


<details>
  <summary>Details</summary>
Motivation: 当前LLM防御系统面临根本性限制：黑箱审核API透明度有限且适应性差，白箱方法计算成本高昂且需要昂贵重训练。现有系统需要在性能、效率和适应性之间做出妥协。

Method: BAGEL采用基于引导聚合和专家混合思想的集成方法，包含多个在不同攻击数据集上微调的专门化模型。推理时使用随机森林路由器选择最合适的集成成员，并通过随机抽样聚合多个成员的预测。支持增量更新，只需微调小型提示安全分类器并加入集成。

Result: BAGEL仅选择5个集成成员（430M参数）就实现了0.92的F1分数，优于需要数十亿参数的OpenAI Moderation API和ShieldGemma。经过9次增量更新后性能保持稳健，并通过路由器结构特征提供可解释性。

Conclusion: 小型微调分类器的集成能够匹配或超越数十亿参数的防护系统，同时提供生产系统所需的适应性和效率，为解决LLM恶意提示检测问题提供了实用解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability.
  To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

</details>


### [260] [Efficient Distribution Learning with Error Bounds in Wasserstein Distance](https://arxiv.org/abs/2602.08063)
*Eduardo Figueiredo,Steven Adams,Luca Laurenti*

Main category: cs.LG

TL;DR: 提出一种基于最优传输和整数线性规划的新框架，从有限样本中近似未知概率分布，并提供Wasserstein距离的非渐近易计算误差界。


<details>
  <summary>Details</summary>
Motivation: Wasserstein距离已成为量化概率分布间距离的关键度量，在机器学习、控制理论、决策理论和生物系统等领域有广泛应用。从有限样本中学习未知分布并给出非渐近且易于计算的Wasserstein距离误差界，已成为许多领域的基本问题。

Method: 开发了一个结合最优传输、非线性优化和集中不等式的算法理论框架。通过求解一个规模仅依赖于近似分布支撑集大小的混合整数线性规划问题，即使真实分布未知，也能高效地以高置信度界定Wasserstein距离误差。利用智能聚类算法优化寻找近似分布的支撑集，同时最小化Wasserstein距离误差。

Result: 在基准测试中，该方法优于现有可比方法，通常返回具有显著更小支撑集和更紧误差界的近似分布。

Conclusion: 该框架为从有限样本中近似未知概率分布提供了一种有效方法，能够计算Wasserstein距离的非渐近误差界，并通过优化支撑集选择实现更紧凑的近似表示。

Abstract: The Wasserstein distance has emerged as a key metric to quantify distances between probability distributions, with applications in various fields, including machine learning, control theory, decision theory, and biological systems. Consequently, learning an unknown distribution with non-asymptotic and easy-to-compute error bounds in Wasserstein distance has become a fundamental problem in many fields. In this paper, we devise a novel algorithmic and theoretical framework to approximate an unknown probability distribution $\mathbb{P}$ from a finite set of samples by an approximate discrete distribution $\widehat{\mathbb{P}}$ while bounding the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$. Our framework leverages optimal transport, nonlinear optimization, and concentration inequalities. In particular, we show that, even if $\mathbb{P}$ is unknown, the Wasserstein distance between $\mathbb{P}$ and $\widehat{\mathbb{P}}$ can be efficiently bounded with high confidence by solving a tractable optimization problem (a mixed integer linear program) of a size that only depends on the size of the support of $\widehat{\mathbb{P}}$. This enables us to develop intelligent clustering algorithms to optimally find the support of $\widehat{\mathbb{P}}$ while minimizing the Wasserstein distance error. On a set of benchmarks, we demonstrate that our approach outperforms state-of-the-art comparable methods by generally returning approximating distributions with substantially smaller support and tighter error bounds.

</details>


### [261] [SiameseNorm: Breaking the Barrier to Reconciling Pre/Post-Norm](https://arxiv.org/abs/2602.08064)
*Tianyu Li,Dongchen Han,Zixuan Cao,Haofeng Huang,Mengyu Zhou,Ming Chen,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang,Gao Huang*

Main category: cs.LG

TL;DR: SiameseNorm提出了一种双流架构，将Pre-Norm和Post-Norm的优势结合，解决了传统单流设计中稳定性与性能的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer主要采用Pre-Norm范式以获得优化稳定性，但放弃了Post-Norm架构的优越潜力。先前尝试结合两者优势通常导致稳定性与性能的权衡，作者认为这是由于单流设计中的结构不兼容性造成的。

Method: 提出SiameseNorm双流架构，耦合Pre-Norm-like和Post-Norm-like流并共享参数。这种设计解耦了两个流的优化动态，使所有残差块都能接收来自两种范式的组合梯度，其中一个流确保稳定性，另一个增强表达能力。

Result: 在13亿参数模型上的大规模预训练实验表明，SiameseNorm表现出优异的优化鲁棒性，并持续优于强基线模型。

Conclusion: SiameseNorm通过双流架构成功调和了Pre-Norm和Post-Norm范式，在保持优化稳定性的同时提升了模型表达能力，为Transformer架构设计提供了新思路。

Abstract: Modern Transformers predominantly adopt the Pre-Norm paradigm for its optimization stability, foregoing the superior potential of the unstable Post-Norm architecture. Prior attempts to combine their strengths typically lead to a stability-performance trade-off. We attribute this phenomenon to a structural incompatibility within a single-stream design: Any application of the Post-Norm operation inevitably obstructs the clean identity gradient preserved by Pre-Norm. To fundamentally reconcile these paradigms, we propose SiameseNorm, a two-stream architecture that couples Pre-Norm-like and Post-Norm-like streams with shared parameters. This design decouples the optimization dynamics of the two streams, retaining the distinct characteristics of both Pre-Norm and Post-Norm by enabling all residual blocks to receive combined gradients inherited from both paradigms, where one stream secures stability while the other enhances expressivity. Extensive pre-training experiments on 1.3B-parameter models demonstrate that SiameseNorm exhibits exceptional optimization robustness and consistently outperforms strong baselines. Code is available at https://github.com/Qwen-Applications/SiameseNorm.

</details>


### [262] [Multimodal normative modeling in Alzheimers Disease with introspective variational autoencoders](https://arxiv.org/abs/2602.08077)
*Sayantan Kumar,Peijie Qiu,Aristeidis Sotiras*

Main category: cs.LG

TL;DR: 提出mmSIVAE模型，结合软自省变分自编码器和混合专家乘积聚合，改进多模态神经影像的规范建模，提升健康参考分布拟合和多模态融合效果


<details>
  <summary>Details</summary>
Motivation: 针对阿尔茨海默病多模态神经影像分析中，现有VAE规范模型存在两个问题：1) 健康参考分布拟合不完美导致假阳性增加；2) 后验聚合方法（如PoE/MoE）在共享潜在空间中多模态融合效果弱

Method: 提出mmSIVAE（多模态软自省变分自编码器），结合Mixture-of-Product-of-Experts（MOPOE）聚合方法，在潜在空间和特征空间计算与学习到的健康分布的距离作为偏差分数，并将统计显著的潜在偏差映射到区域异常以增强可解释性

Result: 在ADNI的MRI区域体积和淀粉样蛋白PET SUVR数据上，mmSIVAE在保留对照组上改善了重建效果，相比VAE基线产生更具区分度的偏差分数用于异常检测，具有更高的似然比和更清晰的对照组与AD谱系队列分离。偏差图突出了与已知AD相关变化一致的区域级模式

Conclusion: 研究结果强调了训练目标中优先考虑参考分布保真度和稳健的多模态后验聚合对于规范建模的重要性，对跨多模态临床数据的偏差分析具有广泛意义

Abstract: Normative modeling learns a healthy reference distribution and quantifies subject-specific deviations to capture heterogeneous disease effects. In Alzheimers disease (AD), multimodal neuroimaging offers complementary signals but VAE-based normative models often (i) fit the healthy reference distribution imperfectly, inflating false positives, and (ii) use posterior aggregation (e.g., PoE/MoE) that can yield weak multimodal fusion in the shared latent space. We propose mmSIVAE, a multimodal soft-introspective variational autoencoder combined with Mixture-of-Product-of-Experts (MOPOE) aggregation to improve reference fidelity and multimodal integration. We compute deviation scores in latent space and feature space as distances from the learned healthy distributions, and map statistically significant latent deviations to regional abnormalities for interpretability. On ADNI MRI regional volumes and amyloid PET SUVR, mmSIVAE improves reconstruction on held-out controls and produces more discriminative deviation scores for outlier detection than VAE baselines, with higher likelihood ratios and clearer separation between control and AD-spectrum cohorts. Deviation maps highlight region-level patterns aligned with established AD-related changes. More broadly, our results highlight the importance of training objectives that prioritize reference-distribution fidelity and robust multimodal posterior aggregation for normative modeling, with implications for deviation-based analysis across multimodal clinical data.

</details>


### [263] [Probability Hacking and the Design of Trustworthy ML for Signal Processing in C-UAS: A Scenario Based Method](https://arxiv.org/abs/2602.08086)
*Liisa Janssens,Laura Middeldorp*

Main category: cs.LG

TL;DR: 该论文提出使用基于场景的方法来分析增强机器学习的反无人机系统，识别概率黑客攻击的挑战，并提出加强系统可信度的法律机制要求。


<details>
  <summary>Details</summary>
Motivation: 为了有效应对无人机系统带来的各种威胁，需要专门的反无人机系统。通过人工智能等新兴颠覆性技术增强反无人机系统可以产生更有效的对抗措施。然而，机器学习增强的反无人机系统面临概率黑客攻击的挑战，需要确保其可信度以建立人机协作中的合理信任。

Method: 采用基于场景的方法来分析增强机器学习的反无人机系统。该方法用于构建概率黑客攻击作为挑战的框架，并识别可以在现有法治机制中实施的要求，以防止概率黑客攻击。

Result: 通过基于场景的方法，论文识别了防止概率黑客攻击的具体要求，这些要求可以集成到现有法治机制中。这些要求增强了反无人机系统的可信度，为在民用和军事环境中建立成功的人机协作所需的合理信任奠定了基础。

Conclusion: 增强机器学习的反无人机系统需要解决概率黑客攻击的挑战以确保可信度。通过基于场景的方法识别并实施到法治机制中的要求，可以加强系统可信度，从而建立合理信任，这对于成功的人机协作至关重要。

Abstract: In order to counter the various threats manifested by Unmanned Aircraft Systems (UAS) adequately, specialized Counter Unmanned Aircraft Systems (C-UAS) are required. Enhancing C-UAS with Emerging and Disruptive Technologies (EDTs) such as Artificial Intelligence (AI) can lead to more effective countermeasures. In this paper a scenario-based method is applied to C-UAS augmented with Machine Learning (ML), a subset of AI, that can enhance signal processing capabilities. Via the scenarios-based method we frame in this paper probability hacking as a challenge and identify requirements which can be implemented in existing Rule of Law mechanisms to prevent probability hacking. These requirements strengthen the trustworthiness of the C-UAS, which feed into justified trust - a key to successful Human-Autonomy Teaming, in civil and military contexts. Index Terms: C-UAS, Scenario-based method, Emerging and Disruptive Technologies, Probability hacking, Trustworthiness.

</details>


### [264] [Online Domain-aware LLM Decoding for Continual Domain Evolution](https://arxiv.org/abs/2602.08088)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: ODD框架通过在线概率融合和自适应置信度调制，使LLM能够实时适应动态变化的领域知识，无需昂贵重训练


<details>
  <summary>Details</summary>
Motivation: 传统LLM离线微调假设静态领域，但实际中领域知识持续演化（新法规、产品、服务等），重训练成本过高，且现实环境存在数据分布漂移（概念漂移），需要高效实时适应方案

Method: 提出在线领域感知解码框架（ODD），在基础LLM和前缀树先验之间进行概率级融合，使用分歧和连续性信号指导自适应置信度调制

Result: 在多种漂移场景下，ODD在所有句法和语义NLG指标上均优于LLM-Greedy和LLM-Temp Scaled，获得0.065的绝对ROUGE-L增益和13.6%的余弦相似度相对提升

Conclusion: ODD对演化的词汇和上下文模式具有鲁棒性，适合动态LLM应用，解决了领域演化与静态适应管道之间的不匹配问题

Abstract: LLMs are typically fine-tuned offline on domain-specific data, assuming a static domain. In practice, domain knowledge evolves continuously through new regulations, products, services, and interaction patterns. Retraining or fine-tuning LLMs for every new instance is computationally infeasible. Additionally, real-world environments also exhibit temporal dynamics with shifting data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. This mismatch between evolving domains and static adaptation pipelines highlights the need for efficient, real-time adaptation without costly retraining. In response, we introduce Online Domain-aware Decoding framework (ODD). ODD performs probability-level fusion between a base LLM and a prefix-tree prior, guided by adaptive confidence modulation using disagreement and continuity signals. Empirical evaluation under diverse drift scenarios demonstrates that ODD consistently surpasses LLM-Greedy and LLM-Temp Scaled across all syntactic and semantic NLG metrics. It yields an absolute ROUGE-L gain of 0.065 and a 13.6% relative improvement in Cosine Similarity over the best baseline. These results demonstrate ODD 's robustness to evolving lexical and contextual patterns, making it suitable for dynamic LLM applications.

</details>


### [265] [Mutual information and task-relevant latent dimensionality](https://arxiv.org/abs/2602.08105)
*Paarth Gulati,Eslam Abdelaleem,Audrey Sederberg,Ilya Nemenman*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息瓶颈的任务相关维度估计方法，通过混合批评器解决传统神经估计器高估维度的问题，并开发了无需扫描瓶颈尺寸的一步式协议。


<details>
  <summary>Details</summary>
Motivation: 估计预测所需的潜在表示维度（任务相关维度）是一个困难且未解决的问题，具有广泛科学应用。传统方法难以准确估计，特别是在噪声环境下。

Method: 将维度估计转化为信息瓶颈问题：什么嵌入瓶颈维度足以压缩预测器和被预测视图，同时保持它们的互信息。提出混合批评器，在保留显式维度瓶颈的同时允许灵活的非线性跨视图交互。开发一步式协议，从单个过参数化混合模型中直接读取有效维度。

Result: 在已知任务相关维度的合成问题上验证了方法的有效性。扩展到内在维度估计，在噪声环境下比经典几何维度估计器更可靠。在多个物理数据集上展示了方法的实用性。

Conclusion: 该方法为任务相关维度估计提供了可靠的信息论框架，解决了传统神经估计器高估维度的问题，并在噪声环境下表现出优越性能，具有广泛的应用前景。

Abstract: Estimating the dimensionality of the latent representation needed for prediction -- the task-relevant dimension -- is a difficult, largely unsolved problem with broad scientific applications. We cast it as an Information Bottleneck question: what embedding bottleneck dimension is sufficient to compress predictor and predicted views while preserving their mutual information (MI). This repurposes neural MI estimators for dimensionality estimation. We show that standard neural estimators with separable/bilinear critics systematically inflate the inferred dimension, and we address this by introducing a hybrid critic that retains an explicit dimensional bottleneck while allowing flexible nonlinear cross-view interactions, thereby preserving the latent geometry. We further propose a one-shot protocol that reads off the effective dimension from a single over-parameterized hybrid model, without sweeping over bottleneck sizes. We validate the approach on synthetic problems with known task-relevant dimension. We extend the approach to intrinsic dimensionality by constructing paired views of a single dataset, enabling comparison with classical geometric dimension estimators. In noisy regimes where those estimators degrade, our approach remains reliable. Finally, we demonstrate the utility of the method on multiple physics datasets.

</details>


### [266] [Variance-Gated Ensembles: An Epistemic-Aware Framework for Uncertainty Estimation](https://arxiv.org/abs/2602.08142)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: VGE提出了一种基于方差门控的集成框架，通过信号噪声比门控机制实现可微分的认知不确定性估计，避免了传统加性分解方法的问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯或近似方法中，将不确定性加性分解为偶然不确定性和认知不确定性的方法在使用有限集成采样和/或不匹配预测分布时会失效，需要更可靠的不确定性估计框架。

Method: 提出方差门控集成（VGE）框架，通过集成统计量计算信号噪声比门控来注入认知敏感性，包括VGMU评分（耦合决策边界与集成预测方差）和VGN层（通过可学习的每类归一化扩展门控机制）。

Result: VGE在匹配或超越最先进的信息论基线方法的同时保持计算效率，提供了实用且可扩展的集成模型认知不确定性估计方法。

Conclusion: VGE为集成模型中的认知感知不确定性估计提供了一个实用、可扩展的框架，解决了传统加性分解方法的局限性。

Abstract: Machine learning applications require fast and reliable per-sample uncertainty estimation. A common approach is to use predictive distributions from Bayesian or approximation methods and additively decompose uncertainty into aleatoric (i.e., data-related) and epistemic (i.e., model-related) components. However, additive decomposition has recently been questioned, with evidence that it breaks down when using finite-ensemble sampling and/or mismatched predictive distributions. This paper introduces Variance-Gated Ensembles (VGE), an intuitive, differentiable framework that injects epistemic sensitivity via a signal-to-noise gate computed from ensemble statistics. VGE provides: (i) a Variance-Gated Margin Uncertainty (VGMU) score that couples decision margins with ensemble predictive variance; and (ii) a Variance-Gated Normalization (VGN) layer that generalizes the variance-gated uncertainty mechanism to training via per-class, learnable normalization of ensemble member probabilities. We derive closed-form vector-Jacobian products enabling end-to-end training through ensemble sample mean and variance. VGE matches or exceeds state-of-the-art information-theoretic baselines while remaining computationally efficient. As a result, VGE provides a practical and scalable approach to epistemic-aware uncertainty estimation in ensemble models. An open-source implementation is available at: https://github.com/nextdevai/vge.

</details>


### [267] [A second order regret bound for NormalHedge](https://arxiv.org/abs/2602.08151)
*Yoav Freund,Nicholas J. A. Harvey,Victor S. Portella,Yabing Qi,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一种NormalHedge变体算法，在"简单"序列上实现二阶ε-分位数遗憾界O(√(V_T log(V_T/ε)))，其中V_T是瞬时专家遗憾的二阶矩累积量。


<details>
  <summary>Details</summary>
Motivation: 针对"简单"序列的专家建议预测问题，寻求更好的遗憾界。当序列相对容易时，希望获得比标准遗憾界更优的性能保证。

Method: 提出NormalHedge算法的变体，通过随机微分方程的连续时间极限推导算法动机，在离散时间分析中使用自协调技术。

Result: 当V_T > log N时，算法获得二阶ε-分位数遗憾界O(√(V_T log(V_T/ε)))，其中V_T是瞬时专家遗憾的二阶矩累积量，由算法确定的自然分布平均得到。

Conclusion: 该NormalHedge变体算法在"简单"序列上实现了优越的二阶遗憾界，为专家建议预测问题提供了新的理论保证。

Abstract: We consider the problem of prediction with expert advice for ``easy'' sequences. We show that a variant of NormalHedge enjoys a second-order $ε$-quantile regret bound of $O\big(\sqrt{V_T \log(V_T/ε)}\big) $ when $V_T > \log N$, where $V_T$ is the cumulative second moment of instantaneous per-expert regret averaged with respect to a natural distribution determined by the algorithm. The algorithm is motivated by a continuous time limit using Stochastic Differential Equations. The discrete time analysis uses self-concordance techniques.

</details>


### [268] [The Confidence Manifold: Geometric Structure of Correctness Representations in Language Models](https://arxiv.org/abs/2602.08159)
*Seonglae Cho,Zekun Wu,Kleyton Da Costa,Adriano Koshiyama*

Main category: cs.LG

TL;DR: 研究发现语言模型内部存在简单的几何结构来表示正确性，仅需3-8维即可有效区分对错，线性分类器效果最佳，且可通过激活引导因果验证


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否"知道"自己陈述的错误（如"澳大利亚首都是悉尼"），以及这种知识在模型内部如何表示

Method: 分析9个不同架构模型，研究正确性表示的几何结构，使用线性/非线性分类器比较，通过质心距离方法进行少样本检测，并用激活引导进行因果验证

Result: 正确性信号仅需3-8维，更多维度反而降低性能；线性分类器效果最佳；质心距离方法达到0.90 AUC，仅需25个样本即可达到GPT-2全数据准确率的89%；激活引导可改变10.9%的错误率；内部探针AUC为0.80-0.97，而基于输出的方法仅0.44-0.64

Conclusion: 语言模型内部存在正确性信号但未在输出中表达；正确性分离本质上是均值偏移的几何问题而非学习问题；质心距离方法可有效检测模型是否"知道"错误

Abstract: When a language model asserts that "the capital of Australia is Sydney," does it know this is wrong? We characterize the geometry of correctness representations across 9 models from 5 architecture families. The structure is simple: the discriminative signal occupies 3-8 dimensions, performance degrades with additional dimensions, and no nonlinear classifier improves over linear separation. Centroid distance in the low-dimensional subspace matches trained probe performance (0.90 AUC), enabling few-shot detection: on GPT-2, 25 labeled examples achieve 89% of full-data accuracy. We validate causally through activation steering: the learned direction produces 10.9 percentage point changes in error rates while random directions show no effect. Internal probes achieve 0.80-0.97 AUC; output-based methods (P(True), semantic entropy) achieve only 0.44-0.64 AUC. The correctness signal exists internally but is not expressed in outputs. That centroid distance matches probe performance indicates class separation is a mean shift, making detection geometric rather than learned.

</details>


### [269] [Spherical Steering: Geometry-Aware Activation Rotation for Language Models](https://arxiv.org/abs/2602.08169)
*Zejia You,Chunyuan Deng,Hanjie Chen*

Main category: cs.LG

TL;DR: 提出Spherical Steering方法，通过激活旋转而非加法实现推理时控制，保持表示范数不变，避免表示崩溃并保持开放生成能力


<details>
  <summary>Details</summary>
Motivation: 现有推理时控制方法通常基于激活加法，这会改变隐藏表示的幅度，可能导致表示崩溃和开放生成能力下降。需要一种保持几何一致性的方法来解决这一权衡问题。

Method: 提出Spherical Steering方法：1）使用激活旋转而非固定向量加法，沿测地线旋转激活向量朝向目标方向；2）引入置信门机制，根据输入不确定性动态调节控制强度；3）保持训练免费，无需重新训练模型。

Result: 在多选基准测试中显著优于基于加法的方法（在TruthfulQA、COPA和Storycloze上提升+10%），同时保持模型的通用开放生成质量。

Conclusion: 保持范数的旋转操作是精确推理时控制的鲁棒有效原语，几何一致性对于语言模型控制具有重要价值。

Abstract: Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.

</details>


### [270] [A Causal Machine Learning Framework for Treatment Personalization in Clinical Trials: Application to Ulcerative Colitis](https://arxiv.org/abs/2602.08171)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 该研究提出了一个模块化因果机器学习框架，用于区分治疗效果异质性的统计检测与个性化治疗决策的实际价值，并在溃疡性结肠炎临床试验中应用验证。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验通常估计平均治疗效果，但治疗反应存在异质性。然而，统计上可检测的异质性并不一定意味着能够改善治疗决策，这两个问题可能产生矛盾的结果。需要一种系统方法来区分和评估这两个不同问题。

Method: 提出了模块化因果机器学习框架：1) 置换重要性识别预测异质性的特征；2) 最佳线性预测器(BLP)测试评估统计显著性；3) 双重稳健策略评估衡量利用异质性是否能改善患者结局。应用于UNIFI维持试验数据，比较安慰剂、标准剂量乌司奴单抗(每12周)和剂量强化乌司奴单抗(每8周)，使用交叉拟合X-learner模型，纳入基线人口统计学、用药史、第8周临床评分、实验室生物标志物和视频内镜特征。

Result: BLP测试发现内镜特征与乌司奴单抗vs安慰剂的治疗效果异质性有强关联，但双重稳健策略评估显示纳入内镜特征未能改善预期缓解率，且交叉验证的多臂评估表现更差。诊断比较显示内镜评分作为疾病严重程度标志物，能改善未治疗患者的结局预测，但为治疗选择增加了噪声；而临床变量(粪便钙卫蛋白、年龄、CRP)捕捉了决策相关的变异。

Conclusion: 因果机器学习在临床试验中的应用应同时包含策略层面的评估和异质性测试。统计上显著的异质性特征不一定能改善治疗决策，需要区分预后贡献和策略价值。内镜特征虽然与治疗效果异质性相关，但未能改善个性化治疗决策，而临床变量更具决策相关性。

Abstract: Randomized controlled trials estimate average treatment effects, but treatment response heterogeneity motivates personalized approaches. A critical question is whether statistically detectable heterogeneity translates into improved treatment decisions -- these are distinct questions that can yield contradictory answers. We present a modular causal machine learning framework that evaluates each question separately: permutation importance identifies which features predict heterogeneity, best linear predictor (BLP) testing assesses statistical significance, and doubly robust policy evaluation measures whether acting on the heterogeneity improves patient outcomes. We apply this framework to patient-level data from the UNIFI maintenance trial of ustekinumab in ulcerative colitis, comparing placebo, standard-dose ustekinumab every 12 weeks, and dose-intensified ustekinumab every 8 weeks, using cross-fitted X-learner models with baseline demographics, medication history, week-8 clinical scores, laboratory biomarkers, and video-derived endoscopic features. BLP testing identified strong associations between endoscopic features and treatment effect heterogeneity for ustekinumab versus placebo, yet doubly robust policy evaluation showed no improvement in expected remission from incorporating endoscopic features, and out-of-fold multi-arm evaluation showed worse performance. Diagnostic comparison of prognostic contribution against policy value revealed that endoscopic scores behaved as disease severity markers -- improving outcome prediction in untreated patients but adding noise to treatment selection -- while clinical variables (fecal calprotectin, age, CRP) captured the decision-relevant variation. These results demonstrate that causal machine learning applications to clinical trials should include policy-level evaluation alongside heterogeneity testing.

</details>


### [271] [Dreaming in Code for Curriculum Learning in Open-Ended Worlds](https://arxiv.org/abs/2602.08194)
*Konstantinos Mitsides,Maxence Faldor,Antoine Cully*

Main category: cs.LG

TL;DR: DiCode框架利用基础模型生成可执行环境代码，为智能体在开放世界学习中搭建渐进式学习路径，在Craftax基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有开放学习系统虽然能生成多样环境，但往往关注孤立行为发现而非持续进展编排。复杂开放世界中巨大的组合空间使得智能体难以发现始终保持可学习性的经验序列。

Method: 提出Dreaming in Code框架，通过基础模型合成可执行环境代码来搭建学习脚手架。"梦想"表现为生成代码层面的世界变体，在Craftax基准上实现具体实例化。

Result: DiCode使智能体获得长时程技能，平均回报比最强基线提升16%，在后期战斗任务上取得非零成功率（先前方法完全失败）。

Conclusion: 代码级环境设计为课程控制提供了实用机制，能够构建中间环境来弥合开放世界中的能力差距。

Abstract: Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, "dreaming" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.

</details>


### [272] [CADO: From Imitation to Cost Minimization for Heatmap-based Solvers in Combinatorial Optimization](https://arxiv.org/abs/2602.08210)
*Hyungseok Song,Deunsol Yoon,Kanghoon Lee,Han-Seul Jeong,Soonyoung Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 该论文提出CADO框架，通过强化学习微调解决热图求解器在组合优化中目标不匹配问题，直接优化解码后解的成本，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于热图的组合优化求解器主要采用监督学习训练范式，存在根本性的目标不匹配问题：最小化模仿损失（如交叉熵）不能保证解成本的优化。这种不匹配表现为解码器盲视（忽略不可微的解码过程）和成本盲视（优先结构模仿而非解质量），导致性能存在硬性上限。

Method: 提出CADO（用于优化的成本感知扩散模型）框架：1）将扩散去噪过程建模为马尔可夫决策过程，直接优化解码后解的成本；2）引入标签中心奖励机制，将真实标签重新用作无偏基线而非模仿目标；3）采用混合微调方法实现参数高效适配。

Result: CADO在多个基准测试中实现了最先进的性能，验证了目标对齐对于释放基于热图的求解器全部潜力的重要性，克服了传统监督学习方法固有的性能上限。

Conclusion: 通过将扩散去噪过程重新构建为MDP并直接优化解成本，CADO解决了热图求解器中的目标不匹配问题，为组合优化提供了更有效的训练范式，证明了强化学习微调在提升基于热图的组合优化求解器性能方面的有效性。

Abstract: Heatmap-based solvers have emerged as a promising paradigm for Combinatorial Optimization (CO). However, we argue that the dominant Supervised Learning (SL) training paradigm suffers from a fundamental objective mismatch: minimizing imitation loss (e.g., cross-entropy) does not guarantee solution cost minimization. We dissect this mismatch into two deficiencies: Decoder-Blindness (being oblivious to the non-differentiable decoding process) and Cost-Blindness (prioritizing structural imitation over solution quality). We empirically demonstrate that these intrinsic flaws impose a hard performance ceiling. To overcome this limitation, we propose CADO (Cost-Aware Diffusion models for Optimization), a streamlined Reinforcement Learning fine-tuning framework that formulates the diffusion denoising process as an MDP to directly optimize the post-decoded solution cost. We introduce Label-Centered Reward, which repurposes ground-truth labels as unbiased baselines rather than imitation targets, and Hybrid Fine-Tuning for parameter-efficient adaptation. CADO achieves state-of-the-art performance across diverse benchmarks, validating that objective alignment is essential for unlocking the full potential of heatmap-based solvers.

</details>


### [273] [DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning](https://arxiv.org/abs/2602.08213)
*Haoran Liu,Zheni Zeng,Yukun Yan,Yuxuan Chen,Yunduo Xiao*

Main category: cs.LG

TL;DR: DrugR是一种基于大语言模型的药物分子优化方法，通过引入明确的药理学推理步骤，结合领域预训练、监督微调和多粒度强化学习，在保持分子核心功效的同时改善ADMET性质。


<details>
  <summary>Details</summary>
Motivation: 分子生成和优化是化学领域的基础任务，大语言模型为此提供了新范式，但面临分子结构与药理性质间复杂隐式关系以及缺乏标注数据的挑战。

Method: 提出DrugR方法，包含三个核心组件：领域特定的持续预训练、通过反向数据工程的监督微调、以及自平衡的多粒度强化学习，将明确的逐步药理学推理引入优化过程。

Result: 实验结果表明，DrugR能够在多个性质上实现全面增强，同时不损害结构相似性或靶标结合亲和力，其显式推理过程为每个优化步骤提供了清晰可解释的依据。

Conclusion: DrugR通过明确的推理过程提供可操作的设计见解，推动自动化、知识驱动的科学发现，代码和模型检查点已开源以促进未来研究。

Abstract: Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.

</details>


### [274] [Distribution-Free Robust Functional Predict-Then-Optimize](https://arxiv.org/abs/2602.08215)
*Yash Patel,Ambuj Tewari*

Main category: cs.LG

TL;DR: 提出一种基于保形预测的神经算子不确定性量化方法，用于PDE求解中的决策任务，无需分布假设且可扩展


<details>
  <summary>Details</summary>
Motivation: 神经算子替代模型在PDE求解决策任务中应用增多，但现有方法缺乏校准的不确定性量化，传统方法如集成或贝叶斯后验估计要么需要不切实际的分布假设，要么缺乏实际可扩展性

Method: 将保形预测应用于神经算子映射的函数空间，实现分布自由的不确定性量化；利用Danskin定理的无限维推广和变分法解决下游鲁棒决策任务

Result: 该方法能够为神经算子预测提供校准的不确定性区域，在多个工程任务中表现优于限制性建模范式（如高斯过程）

Conclusion: 保形预测为神经算子提供了实用的不确定性量化方法，支持下游鲁棒决策任务的形式化后悔表征，具有实际应用价值

Abstract: The solution of PDEs in decision-making tasks is increasingly being undertaken with the help of neural operator surrogate models due to the need for repeated evaluation. Such methods, while significantly more computationally favorable compared to their numerical counterparts, fail to provide any calibrated notions of uncertainty in their predictions. Current methods approach this deficiency typically with ensembling or Bayesian posterior estimation. However, these approaches either require distributional assumptions that fail to hold in practice or lack practical scalability, limiting their applications in practice. We, therefore, propose a novel application of conformal prediction to produce distribution-free uncertainty quantification over the function spaces mapped by neural operators. We then demonstrate how such prediction regions enable a formal regret characterization if leveraged in downstream robust decision-making tasks. We further demonstrate how such posited robust decision-making tasks can be efficiently solved using an infinite-dimensional generalization of Danskin's Theorem and calculus of variations and empirically demonstrate the superior performance of our proposed method over more restrictive modeling paradigms, such as Gaussian Processes, across several engineering tasks.

</details>


### [275] [Thermodynamic Isomorphism of Transformers: A Lagrangian Approach to Attention Dynamics](https://arxiv.org/abs/2602.08216)
*Gunn Kim*

Main category: cs.LG

TL;DR: 论文提出了一个基于第一性原理的信息动力学框架，将Transformer注意力机制视为受最小作用量原理支配的物理系统，而非算法优化。该理论建立了信息热力学第一定律，统一了推理（机械功）和学习（化学演化），并解释了涌现现象如缩放定律和顿悟。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构已经彻底改变了人工智能，但其底层机制在很大程度上仍然是启发式的，缺乏统一的物理理论。作者旨在建立一个基于第一性原理的框架，将信息动力学视为物理系统，从而为基于物理的智能提供理论基础。

Method: 通过将信息状态映射到具有Fisher信息度量的黎曼流形，推导出智能拉格朗日量。将softmax函数对应于最小化信息气体亥姆霍兹自由能的唯一热力学平衡态，将查询-键交互识别为外部场与固有偶极矩之间的电动力学耦合。

Result: 建立了信息热力学第一定律，统一了推理（机械功）和学习（化学演化）。解释了涌现现象如缩放定律和顿悟作为以比热发散为特征的相变。揭示了注意力流形中的旋转对称性破缺产生无质量的Goldstone玻色子，为旋转位置嵌入（RoPE）提供了场论视角。

Conclusion: 该工作连接了统计物理学和深度学习，为基于物理的智能的一般理论奠定了基础，提供了对Transformer架构底层物理机制的统一理解框架。

Abstract: Although the Transformer architecture has revolutionized artificial intelligence, its underlying mechanisms remain largely heuristic and lack a unified physical theory. In this work, we propose a first-principles framework for information dynamics, treating the attention mechanism as a physical system governed by the principle of least action rather than as an algorithmic optimization. By mapping information states to a Riemannian manifold with the Fisher information metric, we derive the intelligence Lagrangian. We show that the softmax function corresponds to the unique thermodynamic equilibrium state that minimizes the Helmholtz free energy of the information gas. In addition, we identify the query-key interaction as an electrodynamic coupling between an external field and an intrinsic dipole moment. This theory establishes the first law of information thermodynamics, unifying inference (mechanical work) and learning (chemical evolution). It also explains emergent phenomena, such as scaling laws and grokking, as phase transitions characterized by the divergence of specific heat. Finally, we discuss how rotational symmetry breaking in the attention manifold generates massless Goldstone bosons, providing a field-theoretic perspective on rotary positional embeddings (RoPE). Our work connects Statistical Physics and Deep Learning, laying the groundwork for a general theory of physics-based intelligence.

</details>


### [276] [Sparsity-Aware Evolution for Model Merging](https://arxiv.org/abs/2602.08218)
*Huan Zhang,Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Bang Liu*

Main category: cs.LG

TL;DR: 提出了一种基于稀疏感知进化的模型融合框架，通过迭代的剪枝-融合循环作为新型变异算子，在进化过程中引入稀疏性约束，提高模型融合的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法在大型语言模型上存在可靠性问题，需要一种能够同时考虑模型性能和稀疏性的融合框架，以提高融合效果和模型效率。

Method: 提出稀疏感知进化框架，采用迭代的剪枝-融合循环作为变异算子，在评分函数中融入稀疏性约束，引导进化过程偏好更稀疏的模型，同时保持传统性能指标。

Result: 在多个大规模LLM基准测试上的实验表明，该方法能显著提高模型融合的可靠性，且由于其简单性和与现有方法的正交性，易于集成到现有流程中。

Conclusion: 稀疏感知进化框架为模型融合提供了一种新颖有效的方法，通过稀疏性约束和竞争机制改善了融合效果，具有实际应用价值。

Abstract: We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \textit{competition} for sparsity introduces an extra local \textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.

</details>


### [277] [SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning](https://arxiv.org/abs/2602.08234)
*Peng Xia,Jianwen Chen,Hanyang Wang,Jiaqi Liu,Kaide Zeng,Yu Wang,Siwei Han,Yiyang Zhou,Xujiang Zhao,Haifeng Chen,Zeyu Zheng,Cihang Xie,Huaxiu Yao*

Main category: cs.LG

TL;DR: SkillRL框架通过自动技能发现和递归进化，将原始经验转化为可重用技能，提升LLM智能体在复杂任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的方法主要存储原始轨迹，这些轨迹通常冗余且噪声多，无法提取高级、可重用的行为模式，限制了智能体的泛化能力

Method: 提出SkillRL框架，包含：1) 基于经验的蒸馏机制构建分层技能库SkillBank；2) 通用和任务特定启发式的自适应检索策略；3) 技能库与强化学习策略协同进化的递归进化机制

Result: 在ALFWorld、WebShop和七个搜索增强任务上的实验表明，SkillRL达到最先进性能，比强基线提升超过15.3%，并在任务复杂度增加时保持鲁棒性

Conclusion: SkillRL通过将原始经验转化为结构化技能，显著减少token占用同时增强推理效用，为LLM智能体提供了从经验中学习可重用行为模式的有效框架

Abstract: Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.

</details>


### [278] [Linearization Explains Fine-Tuning in Large Language Models](https://arxiv.org/abs/2602.08239)
*Zahra Rahimi Afzal,Tara Esmaeilbeig,Mojtaba Soltanalian,Mesrob I. Ohannessian*

Main category: cs.LG

TL;DR: 该论文通过线性化视角分析参数高效微调(PEFT)机制，发现微调动态等价于使用神经正切核(NTK)学习，并揭示了NTK特征值谱与模型适应性能的强相关性。


<details>
  <summary>Details</summary>
Motivation: 虽然参数高效微调(PEFT)技术在大模型适应中广泛应用，但其训练性能和泛化的内在机制仍未得到充分探索。研究者希望通过线性化视角来深入理解这些微调技术的工作原理。

Method: 采用线性化方法分析微调动态，通过欧几里得距离归纳偏置使微调动态等价于使用正定神经正切核(NTK)学习。基于正则化强度分析完全线性和线性化微调优化的接近程度，并给出NTK谱扰动界限。

Result: 研究发现当线性化是良好模型时，NTK的特征值谱与模型适应性能存在强相关性。通过谱扰动界限分析了选择不同层进行微调对NTK的影响，并在LLM的LoRA上进行了实证验证。

Conclusion: 该研究不仅从理论上表征了微调机制，还揭示了NTK谱与性能的关系，为改进PEFT技术提供了理论基础，有助于开发更明智、更灵活的LLM适应方法。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an Euclidean distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs.

</details>


### [279] [Learning in Context, Guided by Choice: A Reward-Free Paradigm for Reinforcement Learning with Transformers](https://arxiv.org/abs/2602.08244)
*Juncheng Dong,Bowen He,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出基于偏好的上下文强化学习(ICPRL)，仅使用偏好反馈进行预训练和部署，无需奖励监督，在未见任务上实现与使用完整奖励监督的ICRL方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文强化学习(ICRL)方法依赖明确的奖励信号进行预训练，这在奖励模糊、难以指定或获取成本高时限制了其应用。需要一种仅使用偏好反馈的学习范式。

Method: 提出ICPRL框架，包括两种变体：基于即时偏好的强化学习(I-PRL)使用每步偏好，基于轨迹偏好的强化学习(T-PRL)使用轨迹级比较。引入偏好原生框架，直接从偏好数据优化Transformer策略，无需奖励信号或最优动作标签。

Result: 在决斗老虎机、导航和连续控制任务上的实验表明，ICPRL能够在未见任务上实现强大的上下文泛化能力，性能与使用完整奖励监督的ICRL方法相当。

Conclusion: ICPRL证明了仅使用偏好反馈进行上下文强化学习的可行性，为奖励信号难以获取的场景提供了有效的解决方案，扩展了上下文强化学习的应用范围。

Abstract: In-context reinforcement learning (ICRL) leverages the in-context learning capabilities of transformer models (TMs) to efficiently generalize to unseen sequential decision-making tasks without parameter updates. However, existing ICRL methods rely on explicit reward signals during pretraining, which limits their applicability when rewards are ambiguous, hard to specify, or costly to obtain. To overcome this limitation, we propose a new learning paradigm, In-Context Preference-based Reinforcement Learning (ICPRL), in which both pretraining and deployment rely solely on preference feedback, eliminating the need for reward supervision. We study two variants that differ in the granularity of feedback: Immediate Preference-based RL (I-PRL) with per-step preferences, and Trajectory Preference-based RL (T-PRL) with trajectory-level comparisons. We first show that supervised pretraining, a standard approach in ICRL, remains effective under preference-only context datasets, demonstrating the feasibility of in-context reinforcement learning using only preference signals. To further improve data efficiency, we introduce alternative preference-native frameworks for I-PRL and T-PRL that directly optimize TM policies from preference data without requiring reward signals nor optimal action labels.Experiments on dueling bandits, navigation, and continuous control tasks demonstrate that ICPRL enables strong in-context generalization to unseen tasks, achieving performance comparable to ICRL methods trained with full reward supervision.

</details>


### [280] [Constraint-Aware Generative Auto-bidding via Pareto-Prioritized Regret Optimization](https://arxiv.org/abs/2602.08261)
*Binglin Wu,Yingyi Zhang,Xianneng Li,Ruyue Deng,Chuan Yue,Weiru Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: PRO-Bid是一个基于约束感知生成的自动竞价框架，通过解耦约束表示和反事实优化机制，解决传统决策变换器在满足目标CPA约束时的状态混叠和平均模仿问题，实现更好的约束满足和价值获取。


<details>
  <summary>Details</summary>
Motivation: 传统决策变换器应用于自动竞价系统时面临两个挑战：1) 标准Return-to-Go条件化忽略成本维度导致状态混叠，无法精确控制资源节奏；2) 标准回归迫使策略模仿历史平均行为，限制了向约束边界优化的能力。

Method: 提出PRO-Bid框架，包含两个协同机制：1) 约束解耦帕累托表示(CDPR)，将全局约束分解为递归成本和价值上下文以恢复资源感知，并基于帕累托前沿重新加权轨迹以聚焦高效数据；2) 反事实遗憾优化(CRO)，利用全局结果预测器识别更优的反事实动作，将这些高效用结果作为加权回归目标，使模型超越历史平均向最优约束边界逼近。

Result: 在两个公共基准测试和在线A/B测试中的广泛实验表明，PRO-Bid在约束满足和价值获取方面优于现有最先进的基线方法。

Conclusion: PRO-Bid通过创新的约束解耦表示和反事实优化机制，有效解决了自动竞价系统中决策变换器的关键限制，实现了更好的约束边界优化和性能提升。

Abstract: Auto-bidding systems aim to maximize marketing value while satisfying strict efficiency constraints such as Target Cost-Per-Action (CPA). Although Decision Transformers provide powerful sequence modeling capabilities, applying them to this constrained setting encounters two challenges: 1) standard Return-to-Go conditioning causes state aliasing by neglecting the cost dimension, preventing precise resource pacing; and 2) standard regression forces the policy to mimic average historical behaviors, thereby limiting the capacity to optimize performance toward the constraint boundary. To address these challenges, we propose PRO-Bid, a constraint-aware generative auto-bidding framework based on two synergistic mechanisms: 1) Constraint-Decoupled Pareto Representation (CDPR) decomposes global constraints into recursive cost and value contexts to restore resource perception, while reweighting trajectories based on the Pareto frontier to focus on high-efficiency data; and 2) Counterfactual Regret Optimization (CRO) facilitates active improvement by utilizing a global outcome predictor to identify superior counterfactual actions. By treating these high-utility outcomes as weighted regression targets, the model transcends historical averages to approach the optimal constraint boundary. Extensive experiments on two public benchmarks and online A/B tests demonstrate that PRO-Bid achieves superior constraint satisfaction and value acquisition compared to state-of-the-art baselines.

</details>


### [281] [When Do Multi-Agent Systems Outperform? Analysing the Learning Efficiency of Agentic Systems](https://arxiv.org/abs/2602.08272)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本文通过PAC框架理论分析比较了多智能体强化学习（MARL）与单智能体强化学习（SARL）在大型语言模型训练中的样本效率，发现当任务自然分解为独立子任务时MARL更优，而依赖性子任务会削弱其优势。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体强化学习（MARL）在大型语言模型训练中显示出潜力，但缺乏理论指导说明何时以及为何MARL优于单智能体强化学习（SARL），导致在实际应用中难以选择合适的强化学习框架。

Method: 采用概率近似正确（PAC）框架，形式化定义LLM的SARL和MARL设置，推导显式的样本复杂度边界，系统分析任务分解和对齐如何影响学习效率，并引入任务对齐概念量化独立任务分解的权衡。

Result: MARL在任务自然分解为独立子任务时能改善样本复杂度，而依赖性子任务会削弱MARL的比较优势；任务对齐分析揭示了在潜在错配情况下强制独立任务分解的权衡关系。

Conclusion: 理论分析澄清了实证研究中的不一致性，为在复杂LLM场景中有效部署MARL策略提供了实用标准，指导何时选择MARL框架以获得更好的样本效率。

Abstract: Reinforcement Learning (RL) has emerged as a crucial method for training or fine-tuning large language models (LLMs), enabling adaptive, task-specific optimizations through interactive feedback. Multi-Agent Reinforcement Learning (MARL), in particular, offers a promising avenue by decomposing complex tasks into specialized subtasks learned by distinct interacting agents, potentially enhancing the ability and efficiency of LLM systems. However, theoretical insights regarding when and why MARL outperforms Single-Agent RL (SARL) remain limited, creating uncertainty in selecting the appropriate RL framework. In this paper, we address this critical gap by rigorously analyzing the comparative sample efficiency of MARL and SARL within the context of LLM. Leveraging the Probably Approximately Correct (PAC) framework, we formally define SARL and MARL setups for LLMs, derive explicit sample complexity bounds, and systematically characterize how task decomposition and alignment influence learning efficiency. Our results demonstrate that MARL improves sample complexity when tasks naturally decompose into independent subtasks, whereas dependent subtasks diminish MARL's comparative advantage. Additionally, we introduce and analyze the concept of task alignment, quantifying the trade-offs when enforcing independent task decomposition despite potential misalignments. These theoretical insights clarify empirical inconsistencies and provide practical criteria for deploying MARL strategies effectively in complex LLM scenarios.

</details>


### [282] [Trust-Based Incentive Mechanisms in Semi-Decentralized Federated Learning Systems](https://arxiv.org/abs/2602.08290)
*Ajay Kumar Shrestha*

Main category: cs.LG

TL;DR: 本文提出了一种基于信任的激励机制，用于评估和奖励联邦学习系统中参与者的贡献质量，通过动态信任评分和区块链技术确保系统透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在恶意或故障节点可能降低模型性能的问题，需要确保系统完整性和可靠性，但现有方法在激励诚实参与和惩罚不可靠行为方面存在不足。

Method: 提出基于信任的激励机制，动态评估信任分数（考虑数据质量、模型准确性、一致性和贡献频率），并将信任分数作为激励分配的基础；同时探索整合区块链和智能合约来自动化信任评估和激励分配过程。

Result: 理论框架旨在创建更健壮、公平和透明的联邦学习生态系统，通过信任评分和激励机制减少不可信参与者带来的风险。

Conclusion: 提出的信任基激励机制结合区块链技术能够有效促进联邦学习系统的诚实参与，提高系统可靠性和透明度，为构建更安全的分布式机器学习环境提供了理论框架。

Abstract: In federated learning (FL), decentralized model training allows multi-ple participants to collaboratively improve a shared machine learning model without exchanging raw data. However, ensuring the integrity and reliability of the system is challenging due to the presence of potentially malicious or faulty nodes that can degrade the model's performance. This paper proposes a novel trust-based incentive mechanism designed to evaluate and reward the quality of contributions in FL systems. By dynamically assessing trust scores based on fac-tors such as data quality, model accuracy, consistency, and contribution fre-quency, the system encourages honest participation and penalizes unreliable or malicious behavior. These trust scores form the basis of an incentive mechanism that rewards high-trust nodes with greater participation opportunities and penal-ties for low-trust participants. We further explore the integration of blockchain technology and smart contracts to automate the trust evaluation and incentive distribution processes, ensuring transparency and decentralization. Our proposed theoretical framework aims to create a more robust, fair, and transparent FL eco-system, reducing the risks posed by untrustworthy participants.

</details>


### [283] [Interaction-Grounded Learning for Contextual Markov Decision Processes with Personalized Feedback](https://arxiv.org/abs/2602.08307)
*Mengxiao Zhang,Yuheng Zhang,Haipeng Luo,Paul Mineiro*

Main category: cs.LG

TL;DR: 本文提出了一种用于多步交互场景的高效算法，将交互式基础学习从单步扩展到多步设置，实现了上下文情景马尔可夫决策过程的次线性遗憾保证。


<details>
  <summary>Details</summary>
Motivation: 现有交互式基础学习研究局限于单步设置，无法应用于多轮决策系统（如多轮LLM部署），需要将这一范式扩展到多步序列决策场景。

Method: 将Zhang等人[2024a]的奖励估计器从单步扩展到多步设置，解决MDP下潜在奖励解码的独特挑战，并基于此设计逆间隙加权算法进行策略优化。

Result: 算法在上下文情景MDP中实现了次线性遗憾保证，并在合成情景MDP和真实世界用户预订数据集上验证了从多轮交互中学习个性化目标的有效性。

Conclusion: 成功将交互式基础学习扩展到多步序列决策场景，为实际应用（如多轮LLM交互）提供了理论基础和实用算法。

Abstract: In this paper, we study Interaction-Grounded Learning (IGL) [Xie et al., 2021], a paradigm designed for realistic scenarios where the learner receives indirect feedback generated by an unknown mechanism, rather than explicit numerical rewards. While prior work on IGL provides efficient algorithms with provable guarantees, those results are confined to single-step settings, restricting their applicability to modern sequential decision-making systems such as multi-turn Large Language Model (LLM) deployments. To bridge this gap, we propose a computationally efficient algorithm that achieves a sublinear regret guarantee for contextual episodic Markov Decision Processes (MDPs) with personalized feedback. Technically, we extend the reward-estimator construction of Zhang et al. [2024a] from the single-step to the multi-step setting, addressing the unique challenges of decoding latent rewards under MDPs. Building on this estimator, we design an Inverse-Gap-Weighting (IGW) algorithm for policy optimization. Finally, we demonstrate the effectiveness of our method in learning personalized objectives from multi-turn interactions through experiments on both a synthetic episodic MDP and a real-world user booking dataset.

</details>


### [284] [Fast Flow Matching based Conditional Independence Tests for Causal Discovery](https://arxiv.org/abs/2602.08315)
*Shunyu Zhao,Yanfeng Yang,Shuai Li,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 提出FMCIT方法，基于流匹配加速条件独立性测试，显著降低因果发现的计算复杂度


<details>
  <summary>Details</summary>
Motivation: 基于约束的因果发现方法需要大量条件独立性测试，计算复杂度高，限制了实际应用，需要加速单个测试

Method: 提出基于流匹配的条件独立性测试(FMCIT)，利用流匹配的高计算效率，模型在整个因果发现过程中只需训练一次；进一步将FMCIT集成到两阶段引导PC骨架学习框架(GPC-FMCIT)中，结合快速筛选和引导的预算化精炼

Result: FMCIT能有效控制I类错误，在备择假设下保持高测试功效，即使在高维条件集下也表现良好；GPC-FMCIT在合成和真实世界因果发现任务中展现出优于现有CI测试方法和PC变体的准确率-效率权衡

Conclusion: FMCIT通过流匹配技术显著加速了条件独立性测试，GPC-FMCIT框架在保证统计功效的同时限制了CI查询数量，为因果发现提供了高效实用的解决方案

Abstract: Constraint-based causal discovery methods require a large number of conditional independence (CI) tests, which severely limits their practical applicability due to high computational complexity. Therefore, it is crucial to design an algorithm that accelerates each individual test. To this end, we propose the Flow Matching-based Conditional Independence Test (FMCIT). The proposed test leverages the high computational efficiency of flow matching and requires the model to be trained only once throughout the entire causal discovery procedure, substantially accelerating causal discovery. According to numerical experiments, FMCIT effectively controls type-I error and maintains high testing power under the alternative hypothesis, even in the presence of high-dimensional conditioning sets. In addition, we further integrate FMCIT into a two-stage guided PC skeleton learning framework, termed GPC-FMCIT, which combines fast screening with guided, budgeted refinement using FMCIT. This design yields explicit bounds on the number of CI queries while maintaining high statistical power. Experiments on synthetic and real-world causal discovery tasks demonstrate favorable accuracy-efficiency trade-offs over existing CI testing methods and PC variants.

</details>


### [285] [Towards Efficient Large Language Reasoning Models via Extreme-Ratio Chain-of-Thought Compression](https://arxiv.org/abs/2602.08324)
*Yuntian Tang,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Wenxi Li,Wei Li,Jie Hu,Xinghao Chen,Rongrong Ji,Shaohui Lin*

Main category: cs.LG

TL;DR: Extra-CoT框架通过极端比例压缩链式思维推理，在保持准确性的同时大幅减少推理计算开销，相比现有方法在73%令牌压缩下还能提升0.6%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法在高压缩比下会严重损失逻辑保真度，导致性能显著下降。需要一种能够在极端压缩比下保持高保真度的快速推理框架。

Method: 1) 在数学CoT数据上训练专门的语义保持压缩器生成高质量监督数据；2) 通过混合比例监督微调让LLM适应不同压缩预算；3) 提出约束分层比例策略优化(CHRPO)，通过分层奖励显式激励低预算下的问题解决能力。

Result: 在三个数学推理基准测试中表现优异。以Qwen3-1.7B在MATH-500上为例，实现了超过73%的令牌减少，同时准确率提升0.6%，显著优于现有最佳方法。

Conclusion: Extra-CoT框架成功实现了高保真度的极端比例CoT压缩，在保持甚至提升推理准确性的同时大幅减少计算开销，为高效推理提供了有效解决方案。

Abstract: Chain-of-Thought (CoT) reasoning successfully enhances the reasoning capabilities of Large Language Models (LLMs), yet it incurs substantial computational overhead for inference. Existing CoT compression methods often suffer from a critical loss of logical fidelity at high compression ratios, resulting in significant performance degradation. To achieve high-fidelity, fast reasoning, we propose a novel EXTreme-RAtio Chain-of-Thought Compression framework, termed Extra-CoT, which aggressively reduces the token budget while preserving answer accuracy. To generate reliable, high-fidelity supervision, we first train a dedicated semantically-preserved compressor on mathematical CoT data with fine-grained annotations. An LLM is then fine-tuned on these compressed pairs via a mixed-ratio supervised fine-tuning (SFT), teaching it to follow a spectrum of compression budgets and providing a stable initialization for reinforcement learning (RL). We further propose Constrained and Hierarchical Ratio Policy Optimization (CHRPO) to explicitly incentivize question-solving ability under lower budgets by a hierarchical reward. Experiments on three mathematical reasoning benchmarks show the superiority of Extra-CoT. For example, on MATH-500 using Qwen3-1.7B, Extra-CoT achieves over 73\% token reduction with an accuracy improvement of 0.6\%, significantly outperforming state-of-the-art (SOTA) methods.

</details>


### [286] [ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection](https://arxiv.org/abs/2602.08343)
*Debajyoti Datta,Trishala Neeraj,Bibek Paudel,Vyom Sharma,Subhabrata Mukherjee*

Main category: cs.LG

TL;DR: ManifoldKV提出基于欧氏距离的KV缓存压缩方法，相比余弦相似度能同时捕获角度和幅度信息，在长上下文推理中表现更优


<details>
  <summary>Details</summary>
Motivation: 长上下文推理受KV缓存内存限制，现有基于余弦相似度的几何淘汰方法会丢失幅度信息，无法区分语义重要的token

Method: 提出ManifoldKV训练自由评分器，基于token到key质心的欧氏距离进行排序，捕获角度和径向偏差；针对64K上下文引入WindowedManifoldKV

Result: 在RULER基准上，4K-16K上下文20%压缩达到95.7%准确率；多键检索任务提升15.4个百分点；64K上下文恢复84.3%准确率，比KeyDiff高3.2个百分点

Conclusion: ManifoldKV通过欧氏距离评分有效提升KV缓存压缩性能，仅需3行代码即可跨4种架构工作，无需调参

Abstract: Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations.
  On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.

</details>


### [287] [All ERMs Can Fail in Stochastic Convex Optimization Lower Bounds in Linear Dimension](https://arxiv.org/abs/2602.08350)
*Tal Burla,Roi Livni*

Main category: cs.LG

TL;DR: 本文研究了随机凸优化中经验风险最小化器(ERM)的样本复杂度，发现在某些情况下ERM会过拟合，即使样本量随维度线性增长。同时分析了梯度下降的泛化下界。


<details>
  <summary>Details</summary>
Motivation: 研究随机凸优化中经验风险最小化器的样本复杂度，特别是探索ERM在什么情况下会过拟合，以及梯度下降算法的泛化性能。这解决了Feldman提出的一个开放性问题。

Method: 通过构造具体的实例来证明存在性结果：构建了一个随机凸优化问题实例，其中样本量随维度线性增长，学习是可能的，但ERM很可能唯一且会过拟合。然后基于这个构造分析梯度下降的泛化下界。

Result: 1. 证明了存在实例中ERM会过拟合，即使样本量随维度线性增长；2. 将结果扩展到近似ERM；3. 为梯度下降提供了新的泛化下界Ω(√(ηT/m^1.5))，显著缩小了现有上下界之间的差距。

Conclusion: 经验风险最小化器在随机凸优化中可能过拟合，即使样本量足够大。梯度下降算法在特定条件下也会过拟合，这为理解机器学习算法的泛化性能提供了新的理论洞见。

Abstract: We study the sample complexity of the best-case Empirical Risk Minimizer in the setting of stochastic convex optimization. We show that there exists an instance in which the sample size is linear in the dimension, learning is possible, but the Empirical Risk Minimizer is likely to be unique and to overfit. This resolves an open question by Feldman. We also extend this to approximate ERMs.
  Building on our construction we also show that (constrained) Gradient Descent potentially overfits when horizon and learning rate grow w.r.t sample size. Specifically we provide a novel generalization lower bound of $Ω\left(\sqrt{ηT/m^{1.5}}\right)$ for Gradient Descent, where $η$ is the learning rate, $T$ is the horizon and $m$ is the sample size. This narrows down, exponentially, the gap between the best known upper bound of $O(ηT/m)$ and existing lower bounds from previous constructions.

</details>


### [288] [The Chicken and Egg Dilemma: Co-optimizing Data and Model Configurations for LLMs](https://arxiv.org/abs/2602.08351)
*Zhiliang Chen,Alfred Wei Lun Leong,Shao Yong Ong,Apivich Hemachandram,Gregory Kang Ruey Lau,Chuan-Sheng Foo,Zhengyuan Liu,Nancy F. Chen,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: JoBS方法通过结合缩放定律性能预测器和贝叶斯优化，联合优化LLM训练的数据和模型配置，解决了传统方法难以同时优化两者的困境。


<details>
  <summary>Details</summary>
Motivation: LLM训练中存在数据配置和模型配置的"鸡生蛋蛋生鸡"困境：最佳数据配置依赖于模型配置，反之亦然。现有方法要么只优化数据，要么只优化模型，忽略了它们之间的相互作用。

Method: JoBS使用基于缩放定律的性能预测器辅助贝叶斯优化，将优化预算分为两部分：一部分用于从少量训练步骤中学习LLM性能预测器，剩余预算完全使用预测器进行贝叶斯优化。

Result: JoBS在相同优化预算下，优于现有的多保真度贝叶斯优化基线方法，以及单独的数据和模型优化方法，在多样化的LLM任务中表现更优。

Conclusion: JoBS通过有效摊销完整训练运行的成本，提供了一种高效联合优化LLM训练数据和模型配置的方法，解决了传统方法难以处理的联合优化问题。

Abstract: Co-optimizing data and model configurations for training LLMs presents a classic chicken-and-egg dilemma: The best training data configuration (e.g., data mixture) for a downstream task depends on the chosen model configuration (e.g., model architecture), and vice versa. However, jointly optimizing both data and model configurations is often deemed intractable, and existing methods focus on either data or model optimization without considering their interaction. We introduce JoBS, an approach that uses a scaling-law-inspired performance predictor to aid Bayesian optimization (BO) in jointly optimizing LLM training data and model configurations efficiently. JoBS allocates a portion of the optimization budget to learn an LLM performance predictor that predicts how promising a training configuration is from a small number of training steps. The remaining budget is used to perform BO entirely with the predictor, effectively amortizing the cost of running full-training runs. We study JoBS's average regret and devise the optimal budget allocation to minimize regret. JoBS outperforms existing multi-fidelity BO baselines, as well as data and model optimization approaches across diverse LLM tasks under the same optimization budget.

</details>


### [289] [Dynamic Regret via Discounted-to-Dynamic Reduction with Applications to Curved Losses and Adam Optimizer](https://arxiv.org/abs/2602.08372)
*Yan-Feng Xie,Yu-Jie Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种模块化方法，基于折扣到动态的归约技术，为FTRL方法及其相关优化器（如Adam）在非平稳在线学习中获得动态遗憾界。


<details>
  <summary>Details</summary>
Motivation: FTRL方法对于曲线损失和自适应优化器（如Adam）的理解很重要，但现有的动态遗憾分析对FTRL的探索较少。需要一种系统方法来分析FTRL在非平稳在线学习中的动态遗憾性能。

Method: 基于折扣到动态的归约技术，提出模块化框架来分析FTRL相关问题的动态遗憾界。重点关注线性回归和逻辑回归两种代表性曲线损失，并将该方法扩展到Adam优化器的分析。

Result: 1. 简化了在线线性回归最优动态遗憾的现有证明；2. 获得了在线逻辑回归的新动态遗憾保证；3. 在随机、非凸、非光滑设置下获得了Adam优化器的最优收敛率；4. 对具有两个折扣参数(β₁,β₂)的Adam进行了更详细分析，为剪裁和无剪裁变体提供了新结果。

Conclusion: 提出的模块化归约方法为FTRL及相关优化器在非平稳在线学习中的动态遗憾分析提供了统一框架，不仅简化了现有证明，还获得了新的理论保证，特别在Adam优化器的理论分析方面取得了进展。

Abstract: We study dynamic regret minimization in non-stationary online learning, with a primary focus on follow-the-regularized-leader (FTRL) methods. FTRL is important for curved losses and for understanding adaptive optimizers such as Adam, yet existing dynamic regret analyses are less explored for FTRL. To address this, we build on the discounted-to-dynamic reduction and present a modular way to obtain dynamic regret bounds of FTRL-related problems. Specifically, we focus on two representative curved losses: linear regression and logistic regression. Our method not only simplifies existing proofs for the optimal dynamic regret of online linear regression, but also yields new dynamic regret guarantees for online logistic regression. Beyond online convex optimization, we apply the reduction to analyze the Adam optimizers, obtaining optimal convergence rates in stochastic, non-convex, and non-smooth settings. The reduction also enables a more detailed treatment of Adam with two discount parameters $(β_1,β_2)$, leading to new results for both clipped and clip-free variants of Adam optimizers.

</details>


### [290] [OJBKQ: Objective-Joint Babai-Klein Quantization](https://arxiv.org/abs/2602.08376)
*Xinyu Wang,Ziyu Zhao,Peng Lu,Yu Gu,Xiao-Wen Chang*

Main category: cs.LG

TL;DR: OJBKQ是一种新的后训练量化方法，将权重量化建模为激活和权重的联合优化问题，通过扩展的Babai最近平面算法和Klein随机化算法解决NP-hard的整数最小二乘问题，在3-4位量化下相比现有方法获得更低的困惑度。


<details>
  <summary>Details</summary>
Motivation: 现有权重量化方法多依赖启发式目标和贪婪舍入策略，在低比特量化下会导致明显的性能下降，需要更有效的量化方法来减少这种性能损失。

Method: 提出OJBKQ方法，将权重量化建模为每层的激活和权重联合优化问题，形成多右端盒约束整数最小二乘问题。对权重矩阵的每一列，应用扩展的Babai最近平面算法和扩展的Klein随机化Babai算法来寻找最小残差的Babai-Klein点作为子优解。

Result: 在大语言模型上的实验结果显示，OJBKQ在3-4位量化下相比现有后训练量化方法获得了更低的困惑度，同时保持了可比较的计算成本。

Conclusion: OJBKQ通过将权重量化建模为联合优化问题并采用有效的算法求解，在低比特量化下能够显著减少性能损失，为大型语言模型的高效压缩提供了有效方案。

Abstract: Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.

</details>


### [291] [Reinforcement Learning with Backtracking Feedback](https://arxiv.org/abs/2602.08377)
*Bilgehan Sel,Vaishakh Keshava,Phillip Wallis,Lukas Rutishauser,Ming Jin,Dingcheng Li*

Main category: cs.LG

TL;DR: RLBF框架通过强化学习让大语言模型学习动态纠正自身生成错误，使用"回溯x个token"信号来恢复安全违规，显著提升对抗攻击和分布内错误的防御能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对对抗攻击和分布内错误时存在安全脆弱性，需要更强大的安全防护机制。现有方法如BSAFE存在局限性，需要更有效的动态错误纠正能力。

Method: 提出RLBF框架：1) 强化学习阶段，模型学习通过"回溯x个token"信号动态纠正生成错误；2) 改进的监督微调数据生成策略BSAFE+，在原本安全的文本中注入违规内容，为回溯机制提供更好的初始训练。

Result: RLBF显著降低了多种攻击的成功率，包括中间填充、GCG攻击和解码参数操纵等，在不同基准测试和模型规模上都表现出优越的安全性能，同时保持了模型的基础效用。

Conclusion: RLBF框架通过强化学习让模型学习动态回溯纠正错误，结合改进的数据生成策略，为大语言模型提供了更强大的安全防护能力，在对抗攻击和分布内错误方面表现出色。

Abstract: Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient "backtrack by x tokens" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.

</details>


### [292] [Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research](https://arxiv.org/abs/2602.08387)
*Max Lübbering,Timm Ruland,Richard Rutmann,Felix Stollenwerk,David Fitzek,Michael Fromm,Alexander Weber,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Mehdi Ali*

Main category: cs.LG

TL;DR: Modalities是一个端到端的PyTorch原生框架，旨在解决LLM研究中大规模消融实验计算成本高、现有工具支持有限的问题，通过集成先进并行化策略和模块化设计，实现高效预训练和系统性消融。


<details>
  <summary>Details</summary>
Motivation: 当前LLM预训练和研究工作流中，大规模消融实验消耗大量计算资源，但现有开源框架对此提供有限工具支持，研究人员通常需要自己编写包装器和脚本，导致效率低下和可重复性问题。

Method: Modalities采用两个主要方法：1) 集成最先进的并行化策略，支持万亿token和十亿参数规模的高效预训练和系统性消融；2) 采用模块化设计，具有声明式、自包含的配置，提高可重复性和可扩展性。

Result: 该框架实现了现有LLM训练框架难以达到的可重复性和可扩展性水平，为大规模LLM研究提供了端到端的解决方案。

Conclusion: Modalities通过集成先进并行化策略和模块化设计，为LLM研究提供了高效、可重复、可扩展的框架，解决了当前大规模消融实验中的计算效率和工具支持问题。

Abstract: Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.

</details>


### [293] [Drop the mask! GAMM-A Taxonomy for Graph Attributes Missing Mechanisms](https://arxiv.org/abs/2602.08407)
*Richard Serrano,Baptiste Jeudy,Charlotte Laclau,Christine Largeron*

Main category: cs.LG

TL;DR: 该论文提出了GAMM框架，将缺失数据机制分类扩展到属性图领域，考虑节点属性和图结构的依赖性，并发现现有插补方法在图感知缺失场景下表现不佳


<details>
  <summary>Details</summary>
Motivation: 属性图中的缺失数据问题比表格数据更具挑战性，需要专门的理论框架来理解图数据中的缺失机制

Method: 提出GAMM（图属性缺失机制）框架，系统地将缺失概率与节点属性和底层图结构联系起来，扩展了传统的缺失机制分类法

Result: 实证研究表明，最先进的插补方法在传统缺失机制上有效，但在更现实的图感知缺失场景中表现显著下降

Conclusion: 需要开发专门针对图结构数据的缺失数据处理方法，GAMM框架为理解属性图中的缺失机制提供了理论基础

Abstract: Exploring missing data in attributed graphs introduces unique challenges beyond those found in tabular datasets. In this work, we extend the taxonomy for missing data mechanisms to attributed graphs by proposing GAMM (Graph Attributes Missing Mechanisms), a framework that systematically links missingness probability to both node attributes and the underlying graph structure. Our taxonomy enriches the conventional definitions of masking mechanisms by introducing graph-specific dependencies. We empirically demonstrate that state-of-the-art imputation methods, while effective on traditional masks, significantly struggle when confronted with these more realistic graph-aware missingness scenarios.

</details>


### [294] [USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation](https://arxiv.org/abs/2602.08431)
*Yingxu Wang,Kunyu Zhang,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

TL;DR: USBD提出通用结构基蒸馏框架，通过构建结构无关的基来覆盖完整拓扑模式谱，解决源域GNN平滑先验限制在结构差异大的目标域泛化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SF-GDA方法依赖源训练GNN的平滑先验，在拓扑结构显著变化时，源模型会将未见的结构模式误判为噪声，导致基于伪标签的适应不可靠。

Method: 提出通用结构基蒸馏框架，采用双层优化将源数据集蒸馏为紧凑结构基，通过强制原型跨越完整狄利克雷能量谱来捕获多样拓扑模式。推理时引入谱感知集成机制，根据目标图谱指纹动态激活最优原型组合。

Result: 在基准测试中，USBD显著优于现有方法，特别是在结构变化严重的场景下，同时通过将适应成本与目标数据规模解耦实现了优越的计算效率。

Conclusion: USBD通过从适应有偏模型转向学习通用结构基，解决了SF-GDA在拓扑结构显著变化时的泛化瓶颈，为跨图数据集的知识转移提供了更鲁棒的解决方案。

Abstract: SF-GDA is pivotal for privacy-preserving knowledge transfer across graph datasets. Although recent works incorporate structural information, they implicitly condition adaptation on the smoothness priors of sourcetrained GNNs, thereby limiting their generalization to structurally distinct targets. This dependency becomes a critical bottleneck under significant topological shifts, where the source model misinterprets distinct topological patterns unseen in the source domain as noise, rendering pseudo-label-based adaptation unreliable. To overcome this limitation, we propose the Universal Structural Basis Distillation, a framework that shifts the paradigm from adapting a biased model to learning a universal structural basis for SF-GDA. Instead of adapting a biased source model to a specific target, our core idea is to construct a structure-agnostic basis that proactively covers the full spectrum of potential topological patterns. Specifically, USBD employs a bi-level optimization framework to distill the source dataset into a compact structural basis. By enforcing the prototypes to span the full Dirichlet energy spectrum, the learned basis explicitly captures diverse topological motifs, ranging from low-frequency clusters to high-frequency chains, beyond those present in the source. This ensures that the learned basis creates a comprehensive structural covering capable of handling targets with disparate structures. For inference, we introduce a spectral-aware ensemble mechanism that dynamically activates the optimal prototype combination based on the spectral fingerprint of the target graph. Extensive experiments on benchmarks demonstrate that USBD significantly outperforms state-of-the-art methods, particularly in scenarios with severe structural shifts, while achieving superior computational efficiency by decoupling the adaptation cost from the target data scale.

</details>


### [295] [Estimating Aleatoric Uncertainty in the Causal Treatment Effect](https://arxiv.org/abs/2602.08461)
*Liyuan Xu,Bijan Mazaheri*

Main category: cs.LG

TL;DR: 该论文提出了治疗效应方差(VTE)和条件治疗效应方差(CVTE)作为衡量治疗反应中固有随机不确定性的自然度量，证明了这些量在温和假设下可从观测数据中识别，并开发了非参数核估计器。


<details>
  <summary>Details</summary>
Motivation: 先前因果推断研究主要关注治疗效应的平均值和条件平均值，对个体治疗反应的变异性和不确定性关注较少。需要开发能够量化治疗反应中固有随机不确定性的方法。

Method: 引入治疗效应方差(VTE)和条件治疗效应方差(CVTE)作为衡量随机不确定性的度量；证明这些量在温和假设下可从观测数据中识别，即使存在未观测混杂因素；提出非参数核基估计器来估计VTE和CVTE；进行理论分析建立估计器的收敛性。

Result: 理论分析建立了估计器的收敛性；在合成和半模拟数据集上的广泛实证实验表明，该方法在性能上优于或与朴素基线方法相当。

Conclusion: 该研究填补了因果推断中对治疗反应变异性和不确定性量化方法的空白，提出的VTE和CVTE度量及其估计方法为理解个体治疗反应的不确定性提供了有效工具。

Abstract: Previous work on causal inference has primarily focused on averages and conditional averages of treatment effects, with significantly less attention on variability and uncertainty in individual treatment responses. In this paper, we introduce the variance of the treatment effect (VTE) and conditional variance of treatment effect (CVTE) as the natural measure of aleatoric uncertainty inherent in treatment responses, and we demonstrate that these quantities are identifiable from observed data under mild assumptions, even in the presence of unobserved confounders. We further propose nonparametric kernel-based estimators for VTE and CVTE, and our theoretical analysis establishes their convergence. We also test the performance of our method through extensive empirical experiments on both synthetic and semi-simulated datasets, where it demonstrates superior or comparable performance to naive baselines.

</details>


### [296] [Low Rank Transformer for Multivariate Time Series Anomaly Detection and Localization](https://arxiv.org/abs/2602.08467)
*Charalampos Shimillas,Kleanthis Malialis,Konstantinos Fokianos,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 提出ALoRa-T模型和ALoRa-Loc方法，通过低秩正则化自注意力机制和量化时间序列间关系，显著提升多元时间序列异常检测与定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列异常诊断方法缺乏理论洞察，特别是异常定位这一关键但未被充分探索的领域。需要研究Transformer在多元时间序列中的应用，建立与统计时间序列方法的理论联系。

Method: 提出Attention Low-Rank Transformer (ALoRa-T)模型，对自注意力机制应用低秩正则化；引入Attention Low-Rank分数捕捉异常的时间特征；提出ALoRa-Loc方法，通过量化时间序列间的相互关系将异常关联到特定变量。

Result: 广泛的实验和真实数据分析表明，所提出的方法在检测和定位任务上均显著优于现有最先进方法。

Conclusion: 通过理论洞察Transformer在多元时间序列中的应用，提出的ALoRa-T模型和ALoRa-Loc方法有效解决了异常检测和定位问题，为多元时间序列异常诊断提供了新的理论框架和实践方案。

Abstract: Multivariate time series (MTS) anomaly diagnosis, which encompasses both anomaly detection and localization, is critical for the safety and reliability of complex, large-scale real-world systems. The vast majority of existing anomaly diagnosis methods offer limited theoretical insights, especially for anomaly localization, which is a vital but largely unexplored area. The aim of this contribution is to study the learning process of a Transformer when applied to MTS by revealing connections to statistical time series methods. Based on these theoretical insights, we propose the Attention Low-Rank Transformer (ALoRa-T) model, which applies low-rank regularization to self-attention, and we introduce the Attention Low-Rank score, effectively capturing the temporal characteristics of anomalies. Finally, to enable anomaly localization, we propose the ALoRa-Loc method, a novel approach that associates anomalies to specific variables by quantifying interrelationships among time series. Extensive experiments and real data analysis, show that the proposed methodology significantly outperforms state-of-the-art methods in both detection and localization tasks.

</details>


### [297] [Learning Credal Ensembles via Distributionally Robust Optimization](https://arxiv.org/abs/2602.08470)
*Kaizheng Wang,Ghifari Adam Faza,Fabio Cuzzolin,Siu Lun Chau,David Moens,Hans Hallez*

Main category: cs.LG

TL;DR: CreDRO：一种新的可信预测器，通过分布鲁棒优化学习模型集合，捕捉训练随机性和分布偏移引起的认知不确定性，在OOD检测和医疗选择性分类中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有可信预测器主要将认知不确定性定义为训练初始化随机性引起的分歧，这主要反映了对优化随机性的敏感性，而非更深层次的不确定性来源。需要捕捉由于训练和测试数据之间潜在分布偏移引起的认知不确定性。

Method: 提出CreDRO方法，通过分布鲁棒优化学习一个模型集合，将认知不确定性定义为在训练和测试数据之间i.i.d.假设的不同松弛程度下训练模型之间的分歧。这种方法同时捕捉训练随机性和有意义的分歧。

Result: CreDRO在多个基准测试中一致优于现有可信方法，特别是在分布外检测任务和医疗应用中的选择性分类任务上表现优异。

Conclusion: 通过将认知不确定性定义为模型在i.i.d.假设不同松弛程度下的分歧，CreDRO能够更全面地捕捉预测认知不确定性，包括训练随机性和分布偏移引起的不确定性，从而在各种应用中提供更可靠的预测不确定性量化。

Abstract: Credal predictors are models that are aware of epistemic uncertainty and produce a convex set of probabilistic predictions. They offer a principled way to quantify predictive epistemic uncertainty (EU) and have been shown to improve model robustness in various settings. However, most state-of-the-art methods mainly define EU as disagreement caused by random training initializations, which mostly reflects sensitivity to optimization randomness rather than uncertainty from deeper sources. To address this, we define EU as disagreement among models trained with varying relaxations of the i.i.d. assumption between training and test data. Based on this idea, we propose CreDRO, which learns an ensemble of plausible models through distributionally robust optimization. As a result, CreDRO captures EU not only from training randomness but also from meaningful disagreement due to potential distribution shifts between training and test data. Empirical results show that CreDRO consistently outperforms existing credal methods on tasks such as out-of-distribution detection across multiple benchmarks and selective classification in medical applications.

</details>


### [298] [Beyond Correctness: Learning Robust Reasoning via Transfer](https://arxiv.org/abs/2602.08489)
*Hyunseok Lee,Soheil Abbasloo,Jihoon Tack,Jinwoo Shin*

Main category: cs.LG

TL;DR: RLTR（带可转移奖励的强化学习）通过测试部分推理前缀能否指导其他模型得到正确答案，提升LLM推理的鲁棒性和样本效率，相比RLVR在更少训练步数下达到相当性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法只关注最终答案正确性，忽略了推理过程本身的鲁棒性。作者认为稳健的推理应该能在不同模型间转移和延续，即推理应具备可转移性。

Method: 提出RLTR方法，通过可转移奖励来操作化鲁棒性：测试一个模型的部分推理前缀能否指导另一个独立模型得到正确答案。这鼓励LLM产生稳定、可解释且真正可泛化的推理。

Result: 在MATH500上，RLTR相比RLVR获得+3.6%的Maj@64提升，且仅用约2.5倍更少的训练步数就达到RLVR的平均准确率，同时提高了采样一致性和最终答案准确率。

Conclusion: RLTR通过可转移奖励机制有效提升了LLM推理的鲁棒性和样本效率，产生了更可靠、可解释且可泛化的推理过程，解决了RLVR只关注最终答案而忽略推理过程稳健性的问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.

</details>


### [299] [Contextual Rollout Bandits for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.08499)
*Xiaodong Lu,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Zhijun Chen,Yu Luo,Fuzhen Zhuang,Yikun Ban,Deqing Wang*

Main category: cs.LG

TL;DR: 本文提出了一种针对RLVR（带可验证奖励的强化学习）的神经调度框架，将rollout调度建模为上下文多臂老虎机问题，通过自适应选择高价值rollout来提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在rollout使用上存在两个主要问题：1）对每个prompt内质量各异的响应进行无差别处理；2）历史rollout在单次使用后即被丢弃。这导致监督信号噪声大、样本效率低、策略更新次优。

Method: 将RLVR中的rollout调度建模为上下文多臂老虎机问题，提出统一的神经调度框架。每个rollout被视为一个"臂"，其奖励定义为连续优化步骤间的性能增益提升。该调度器支持噪声感知的组内选择和历史rollout的自适应全局重用。

Result: 在六个数学推理基准测试上的实验表明，该方法在多个RLVR优化方法中均能带来一致的性能提升和训练效率改进。理论分析推导了次线性遗憾界限，并证明扩大rollout缓冲区能提高可达到的性能上限。

Conclusion: 提出的神经调度框架通过将rollout调度建模为上下文多臂老虎机问题，有效解决了现有RLVR方法中rollout使用效率低下的问题，显著提升了训练效率和模型性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is an effective paradigm for improving the reasoning capabilities of large language models. However, existing RLVR methods utilize rollouts in an indiscriminate and short-horizon manner: responses of heterogeneous quality within each prompt are treated uniformly, and historical rollouts are discarded after a single use. This leads to noisy supervision, poor sample efficiency, and suboptimal policy updates. We address these issues by formulating rollout scheduling in RLVR as a contextual bandit problem and proposing a unified neural scheduling framework that adaptively selects high-value rollouts throughout training. Each rollout is treated as an arm whose reward is defined by the induced performance gain between consecutive optimization steps. The resulting scheduler supports both noise-aware intra-group selection and adaptive global reuse of historical rollouts within a single principled framework. We provide theoretical justification by deriving sublinear regret bounds and showing that enlarging the rollout buffer improves the achievable performance upper bound. Experiments on six mathematical reasoning benchmarks demonstrate consistent gains in performance and training efficiency across multiple RLVR optimization methods.

</details>


### [300] [Bridging Academia and Industry: A Comprehensive Benchmark for Attributed Graph Clustering](https://arxiv.org/abs/2602.08519)
*Yunhui Liu,Pengyu Qiu,Yu Xing,Yongchao Liu,Peng Du,Chuntao Hong,Jiajun Zheng,Tao Zheng,Tieke He*

Main category: cs.LG

TL;DR: PyAGC是一个面向工业部署的图聚类基准测试库，解决了现有评估方法在小规模、高同质性数据集上的局限性，提供了可扩展的批量训练实现和全面的评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前图聚类研究存在学术与工业应用的鸿沟：评估协议依赖小规模、高同质性的引文数据集，采用不可扩展的全批量训练范式，且依赖监督指标，无法反映标签稀缺环境下的真实性能。

Method: 提出了PyAGC基准测试库，将现有方法统一为模块化的Encode-Cluster-Optimize框架，首次为多种先进图聚类算法提供内存高效的mini-batch实现。构建了12个多样化数据集（2.7K到111M节点），包含工业级图数据，并倡导包含无监督结构指标和效率分析的综合评估协议。

Result: 在蚂蚁集团高风险工业工作流中经过实战检验，为社区提供了稳健、可复现、可扩展的平台，推动图聚类研究向实际部署发展。代码和资源已公开。

Conclusion: PyAGC填补了图聚类研究学术与工业应用之间的鸿沟，通过全面的基准测试库、多样化数据集和综合评估协议，为图聚类方法的实际部署提供了可靠平台。

Abstract: Attributed Graph Clustering (AGC) is a fundamental unsupervised task that integrates structural topology and node attributes to uncover latent patterns in graph-structured data. Despite its significance in industrial applications such as fraud detection and user segmentation, a significant chasm persists between academic research and real-world deployment. Current evaluation protocols suffer from the small-scale, high-homophily citation datasets, non-scalable full-batch training paradigms, and a reliance on supervised metrics that fail to reflect performance in label-scarce environments. To bridge these gaps, we present PyAGC, a comprehensive, production-ready benchmark and library designed to stress-test AGC methods across diverse scales and structural properties. We unify existing methodologies into a modular Encode-Cluster-Optimize framework and, for the first time, provide memory-efficient, mini-batch implementations for a wide array of state-of-the-art AGC algorithms. Our benchmark curates 12 diverse datasets, ranging from 2.7K to 111M nodes, specifically incorporating industrial graphs with complex tabular features and low homophily. Furthermore, we advocate for a holistic evaluation protocol that mandates unsupervised structural metrics and efficiency profiling alongside traditional supervised metrics. Battle-tested in high-stakes industrial workflows at Ant Group, this benchmark offers the community a robust, reproducible, and scalable platform to advance AGC research towards realistic deployment. The code and resources are publicly available via GitHub (https://github.com/Cloudy1225/PyAGC), PyPI (https://pypi.org/project/pyagc), and Documentation (https://pyagc.readthedocs.io).

</details>


### [301] [Causal Schrödinger Bridges: Constrained Optimal Transport on Structural Manifolds](https://arxiv.org/abs/2602.08535)
*Rui Wu,Li YongJun*

Main category: cs.LG

TL;DR: 提出Causal Schrödinger Bridge (CSB)框架，将反事实推理重新表述为熵最优传输问题，通过扩散过程在支持集不匹配时稳健地"隧道穿越"，优于确定性方法。


<details>
  <summary>Details</summary>
Motivation: 传统生成建模使用确定性流（ODE）寻找最小作用路径，但在因果干预下变得脆弱，因为需要跨过低密度区域（"离流形"）传输概率质量，而该区域的向量场定义不明确，导致数值不稳定和虚假相关性。

Method: 引入Causal Schrödinger Bridge (CSB)框架，将反事实推理重新表述为熵最优传输问题。与需要严格可逆性的确定性方法不同，CSB利用扩散过程（SDEs）在支持集不匹配时稳健地"隧道穿越"，同时严格强制执行结构可容许性约束。证明了结构分解定理，表明全局高维桥分解为局部、稳健的转移。

Result: 在高维干预（Morpho-MNIST）上的实证验证表明，CSB在结构一致性方面显著优于确定性基线方法，特别是在强、分布外处理的机制中表现更优。

Conclusion: CSB框架通过将反事实推理重新表述为熵最优传输问题，利用扩散过程克服了确定性方法在因果干预下的脆弱性，为处理支持集不匹配和分布外干预提供了更稳健的解决方案。

Abstract: Generative modeling typically seeks the path of least action via deterministic flows (ODE). While effective for in-distribution tasks, we argue that these deterministic paths become brittle under causal interventions, which often require transporting probability mass across low-density regions ("off-manifold") where the vector field is ill-defined. This leads to numerical instability and spurious correlations. In this work, we introduce the Causal Schrödinger Bridge (CSB), a framework that reformulates counterfactual inference as Entropic Optimal Transport. Unlike deterministic approaches that require strict invertibility, CSB leverages diffusion processes (SDEs) to robustly "tunnel" through support mismatches while strictly enforcing structural admissibility constraints. We prove the Structural Decomposition Theorem, showing that the global high-dimensional bridge factorizes into local, robust transitions. Empirical validation on high-dimensional interventions (Morpho-MNIST) demonstrates that CSB significantly outperforms deterministic baselines in structural consistency, particularly in regimes of strong, out-of-distribution treatments.

</details>


### [302] [Rho-Perfect: Correlation Ceiling For Subjective Evaluation Datasets](https://arxiv.org/abs/2602.08552)
*Fredrik Cumlin*

Main category: cs.LG

TL;DR: ρ-Perfect是一种估计主观评分数据集中模型可达到的最高相关性的实用方法，通过考虑异方差噪声来量化数据可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 主观评分存在固有噪声，这会限制模型与人类评分的相关性，但数据可靠性问题很少被量化。需要一种方法来估计主观评分数据集上模型可达到的最高相关性。

Method: 提出ρ-Perfect方法，定义为完美预测器与人类评分之间的相关性。基于主观评分数据集中常见的异方差噪声场景，推导出该值的估计。证明ρ-Perfect的平方可以估计测试-重测相关性，并以此验证估计的有效性。

Result: 在语音质量数据集上展示了ρ-Perfect的应用，表明该度量可以区分模型限制和数据质量问题。验证了ρ-Perfect平方能够准确估计测试-重测相关性。

Conclusion: ρ-Perfect提供了一种实用的方法来量化主观评分数据集的可靠性限制，帮助研究人员区分模型性能不足与数据质量问题，为模型评估提供了更准确的基准。

Abstract: Subjective ratings contain inherent noise that limits the model-human correlation, but this reliability issue is rarely quantified. In this paper, we present $ρ$-Perfect, a practical estimation of the highest achievable correlation of a model on subjectively rated datasets. We define $ρ$-Perfect to be the correlation between a perfect predictor and human ratings, and derive an estimate of the value based on heteroscedastic noise scenarios, a common occurrence in subjectively rated datasets. We show that $ρ$-Perfect squared estimates test-retest correlation and use this to validate the estimate. We demonstrate the use of $ρ$-Perfect on a speech quality dataset and show how the measure can distinguish between model limitations and data quality issues.

</details>


### [303] [Stateless Yet Not Forgetful: Implicit Memory as a Hidden Channel in LLMs](https://arxiv.org/abs/2602.08563)
*Ahmed Salem,Andrew Paverd,Sahar Abdelnabi*

Main category: cs.LG

TL;DR: LLMs实际上具有隐式记忆能力，能够在独立交互间通过编码信息到输出中并后续恢复，形成跨推理请求的持久信息通道，这挑战了LLM是无状态的假设。


<details>
  <summary>Details</summary>
Motivation: 挑战当前将大型语言模型视为无状态系统的普遍假设，揭示LLMs实际上具有跨独立交互保持状态的能力，这种隐式记忆机制可能带来安全风险和应用影响。

Method: 引入"隐式记忆"概念，通过时间炸弹（一种新型时间后门）作为具体演示，展示如何通过简单提示或微调诱导模型在序列交互中累积隐藏条件并激活特定行为。

Result: 证明LLMs确实具有隐式记忆能力，能够创建跨推理请求的持久信息通道，时间炸弹等攻击在当前技术下即可实现，揭示了隐式记忆带来的多种安全风险。

Conclusion: 隐式记忆对LLM安全构成重大挑战，需要开发检测方法、压力测试和评估框架来应对未来可能出现的更复杂攻击，并发布了相关代码和数据促进研究。

Abstract: Large language models (LLMs) are commonly treated as stateless: once an interaction ends, no information is assumed to persist unless it is explicitly stored and re-supplied. We challenge this assumption by introducing implicit memory-the ability of a model to carry state across otherwise independent interactions by encoding information in its own outputs and later recovering it when those outputs are reintroduced as input. This mechanism does not require any explicit memory module, yet it creates a persistent information channel across inference requests. As a concrete demonstration, we introduce a new class of temporal backdoors, which we call time bombs. Unlike conventional backdoors that activate on a single trigger input, time bombs activate only after a sequence of interactions satisfies hidden conditions accumulated via implicit memory. We show that such behavior can be induced today through straightforward prompting or fine-tuning. Beyond this case study, we analyze broader implications of implicit memory, including covert inter-agent communication, benchmark contamination, targeted manipulation, and training-data poisoning. Finally, we discuss detection challenges and outline directions for stress-testing and evaluation, with the goal of anticipating and controlling future developments. To promote future research, we release code and data at: https://github.com/microsoft/implicitMemory.

</details>


### [304] [M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data](https://arxiv.org/abs/2602.08564)
*Tiantong Wang,Yiyang Duan,Haoyu Chen,Tiantong Wu,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: 提出M-Loss评估指标，用于量化模型合并的兼容性，通过测量参数平均与模型集成之间的差异来指导更有效的模型合并策略。


<details>
  <summary>Details</summary>
Motivation: 大规模模型训练计算密集且受标注数据限制，模型合并提供了一种无需额外数据或大量训练的替代方案。但传统合并方法（如参数平均）容易组合不可泛化的特征，而模型集成虽然性能更稳定但推理成本高。现有研究缺乏理论证据和评估指标来指导模型合并。

Method: 引入Merging-ensembling loss (M-Loss)评估指标，使用少量无标签数据量化合并源模型的兼容性。通过在层和节点级别测量参数平均与模型集成之间的差异，指导更有效的合并策略。M-Loss既作为模型合并理论可行性的量化标准，也作为模型剪枝中参数重要性的指导。

Result: 理论分析和实证评估表明，将M-Loss纳入合并过程显著提高了合并模型与模型集成之间的对齐度，为准确的模型整合提供了可扩展且高效的框架。

Conclusion: M-Loss为模型合并提供了理论指导和实用评估工具，解决了传统合并方法的问题，同时避免了模型集成的高成本，实现了更有效的模型整合。

Abstract: Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.

</details>


### [305] [An arithmetic method algorithm optimizing k-nearest neighbors compared to regression algorithms and evaluated on real world data sources](https://arxiv.org/abs/2602.08577)
*Theodoros Anagnostopoulos,Evanthia Zervoudi,Christos Anagnostopoulos,Apostolos Christopoulos,Bogdan Wierzbinski*

Main category: cs.LG

TL;DR: 本文提出了一种基于算术方法的k-NN回归算法优化方案AMR，通过引入能处理任意数量实变量的线性方程求解方法，在多个真实数据集上取得了优于传统k-NN的性能。


<details>
  <summary>Details</summary>
Motivation: 线性回归分析在预测数值型因变量方面很重要，k-NN作为一种常见的非参数回归算法，虽然性能相对高效，但仍存在优化空间。研究者希望通过引入新的算术方法来提升k-NN算法的性能。

Method: 提出算术方法回归(AMR)算法作为k-NN的优化版本。首先引入一种能处理任意数量实变量线性方程的算术方法，然后采用算术方法算法(AMA)评估该方法的效率，最后将AMR与其他回归算法进行比较，并使用引入的最优推理决策规则进行评估。

Result: 在多个公开可用的真实世界数据源上进行评估，结果表明AMR算法与其他算法具有可比性能，且在大多数情况下表现优于传统k-NN算法。输出结果证实AMR是对k-NN的有效优化。

Conclusion: 提出的AMR算法成功优化了k-NN回归算法，通过引入新的算术方法，在保持与其他算法可比性能的同时，显著提升了k-NN的表现，证明了该优化方法的有效性。

Abstract: Linear regression analysis focuses on predicting a numeric regressand value based on certain regressor values. In this context, k-Nearest Neighbors (k-NN) is a common non-parametric regression algorithm, which achieves efficient performance when compared with other algorithms in literature. In this research effort an optimization of the k-NN algorithm is proposed by exploiting the potentiality of an introduced arithmetic method, which can provide solutions for linear equations involving an arbitrary number of real variables. Specifically, an Arithmetic Method Algorithm (AMA) is adopted to assess the efficiency of the introduced arithmetic method, while an Arithmetic Method Regression (AMR) algorithm is proposed as an optimization of k-NN adopting the potentiality of AMA. Such algorithm is compared with other regression algorithms, according to an introduced optimal inference decision rule, and evaluated on certain real world data sources, which are publicly available. Results are promising since the proposed AMR algorithm has comparable performance with the other algorithms, while in most cases it achieves better performance than the k-NN. The output results indicate that introduced AMR is an optimization of k-NN.

</details>


### [306] [Modeling Score Approximation Errors in Diffusion Models via Forward SPDEs](https://arxiv.org/abs/2602.08579)
*Junsu Seo*

Main category: cs.LG

TL;DR: 该研究通过将分数估计误差视为驱动Fokker-Planck方程的随机源，在SPDE框架下分析基于分数的生成模型动态，并提出了基于SPDE解在径向测试函数上二次变差的新评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统基于粒子的SDE分析方法存在局限，需要从概率密度场演化的角度理解基于分数的生成模型动态，特别是分数估计误差对生成过程的影响。

Method: 采用随机偏微分方程框架，将分数估计误差建模为Fokker-Planck方程的随机漂移扰动源，在简化设置下通过几何稳定性和位移凸性分析生成模型的鲁棒性。

Result: 提出了基于SPDE解在径向测试函数上二次变差的候选评估指标，初步观察表明仅使用采样轨迹前10%的数据该指标仍保持有效，具有计算效率潜力。

Conclusion: SPDE框架为理解基于分数的生成模型动态提供了新视角，提出的评估指标在保持有效性的同时具有计算效率优势，为生成模型分析开辟了新途径。

Abstract: This study investigates the dynamics of Score-based Generative Models (SGMs) by treating the score estimation error as a stochastic source driving the Fokker-Planck equation. Departing from particle-centric SDE analyses, we employ an SPDE framework to model the evolution of the probability density field under stochastic drift perturbations. Under a simplified setting, we utilize this framework to interpret the robustness of generative models through the lens of geometric stability and displacement convexity. Furthermore, we introduce a candidate evaluation metric derived from the quadratic variation of the SPDE solution projected onto a radial test function. Preliminary observations suggest that this metric remains effective using only the initial 10% of the sampling trajectory, indicating a potential for computational efficiency.

</details>


### [307] [FairRARI: A Plug and Play Framework for Fairness-Aware PageRank](https://arxiv.org/abs/2602.08589)
*Emmanouil Kariotakis,Aritra Konar*

Main category: cs.LG

TL;DR: 本文提出FairRARI框架，通过凸优化方法为PageRank算法添加群体公平性约束，确保达到目标公平水平的同时保持算法效率。


<details>
  <summary>Details</summary>
Motivation: 随着算法公平性日益重要，现有方法在计算PageRank时无法保证达到目标公平水平或缺乏最优性保证，需要一种统一框架来解决不同群体公平性标准。

Method: 提出FairRARI统一凸优化框架，利用PageRank的变分公式，通过求解带有公平性约束的强凸优化问题来计算公平PageRank向量，支持"即插即用"方式处理不同公平标准。

Result: FairRARI在真实数据集上优于现有方法，在保持效用的同时达到期望的公平水平，且计算复杂度与原始PageRank算法相同。

Conclusion: FairRARI为PageRank算法提供了一种有效、可扩展的公平性增强框架，能够满足不同群体公平性需求，填补了该领域缺乏原则性算法的空白。

Abstract: PageRank (PR) is a fundamental algorithm in graph machine learning tasks. Owing to the increasing importance of algorithmic fairness, we consider the problem of computing PR vectors subject to various group-fairness criteria based on sensitive attributes of the vertices. At present, principled algorithms for this problem are lacking - some cannot guarantee that a target fairness level is achieved, while others do not feature optimality guarantees. In order to overcome these shortcomings, we put forth a unified in-processing convex optimization framework, termed FairRARI, for tackling different group-fairness criteria in a ``plug and play'' fashion. Leveraging a variational formulation of PR, the framework computes fair PR vectors by solving a strongly convex optimization problem with fairness constraints, thereby ensuring that a target fairness level is achieved. We further introduce three different fairness criteria which can be efficiently tackled using FairRARI to compute fair PR vectors with the same asymptotic time-complexity as the original PR algorithm. Extensive experiments on real-world datasets showcase that FairRARI outperforms existing methods in terms of utility, while achieving the desired fairness levels across multiple vertex groups; thereby highlighting its effectiveness.

</details>


### [308] [SDFed: Bridging Local Global Discrepancy via Subspace Refinement and Divergence Control in Federated Prompt Learning](https://arxiv.org/abs/2602.08590)
*Yicheng Di,Wei Yuan,Tieke He,Zhanjie Zhang,Ao Ma,Yuan Liu,Hongzhi Yin*

Main category: cs.LG

TL;DR: SDFed是一个异构联邦提示学习框架，通过子空间细化和发散控制解决客户端异质性问题，允许可变长度的本地提示同时保持固定长度的全局提示。


<details>
  <summary>Details</summary>
Motivation: 现有联邦提示学习方法强制所有客户端使用统一的提示结构和长度，这在实际的客户端异质性（数据分布和系统资源）下不足，并可能引入全局共享与本地最优知识之间的冲突。

Method: SDFed通过子空间细化和发散控制来弥合本地-全局差异。它保持固定长度的全局提示以进行高效聚合，同时允许每个客户端学习可变长度的本地提示以匹配其数据特征和容量。引入本地提示的子空间细化方法，以及信息保留和发散控制策略。

Result: 在多个数据集上的广泛实验表明，SDFed在异构联邦设置中持续提高了性能和鲁棒性。

Conclusion: SDFed通过允许可变长度的本地提示和有效的本地-全局知识协调，解决了异构联邦提示学习中的关键挑战，在保持通信效率的同时提升了模型性能。

Abstract: Vision-language pretrained models offer strong transferable representations, yet adapting them in privacy-sensitive multi-party settings is challenging due to the high communication cost of federated optimization and the limited local data on clients. Federated prompt learning mitigates this issue by keeping the VLPM backbone frozen and collaboratively training lightweight prompt parameters. However, existing approaches typically enforce a unified prompt structure and length across clients, which is inadequate under practical client heterogeneity in both data distributions and system resources, and may further introduce conflicts between globally shared and locally optimal knowledge. To address these challenges, we propose \textbf{SDFed}, a heterogeneous federated prompt learning framework that bridges Local-Global Discrepancy via Subspace Refinement and Divergence Control. SDFed maintains a fixed-length global prompt for efficient aggregation while allowing each client to learn a variable-length local prompt to better match its data characteristics and capacity. To mitigate local-global conflicts and facilitate effective knowledge transfer, SDFed introduces a subspace refinement method for local prompts and an information retention and divergence control strategy that preserves key local information while maintaining appropriate separability between global and local representations. Extensive experiments on several datasets demonstrate that SDFed consistently improves performance and robustness in heterogeneous federated settings.

</details>


### [309] [TFMLinker: Universal Link Predictor by Graph In-Context Learning with Tabular Foundation Models](https://arxiv.org/abs/2602.08592)
*Tianyin Liao,Chunyu Hu,Yicheng Sui,Xingxuan Zhang,Peng Cui,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: TFMLinker：基于表格基础模型的通用链接预测方法，无需数据集特定微调即可跨图域工作


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型在链接预测中存在局限性：预训练规模有限或过度依赖文本信息。受表格基础模型在跨表格数据集通用预测成功的启发，探索将其应用于链接预测，但面临如何获取必要上下文和捕获链接中心拓扑信息的技术挑战

Method: 提出TFMLinker：1）原型增强的局部-全局上下文模块，构建捕获图特定和跨图可迁移模式的上下文；2）通用拓扑感知链接编码器，捕获链接中心拓扑信息并生成链接表示作为TFM输入；3）利用TFM通过上下文学习预测链接存在

Result: 在6个跨不同领域的图基准测试中，该方法优于最先进的基线方法，且无需数据集特定的微调

Conclusion: TFMLinker成功将表格基础模型应用于链接预测任务，展示了跨图数据集通用链接预测的可行性，为图机器学习中的基础模型发展提供了新思路

Abstract: Link prediction is a fundamental task in graph machine learning with widespread applications such as recommendation systems, drug discovery, knowledge graphs, etc. In the foundation model era, how to develop universal link prediction methods across datasets and domains becomes a key problem, with some initial attempts adopting Graph Foundation Models utilizing Graph Neural Networks and Large Language Models. However, the existing methods face notable limitations, including limited pre-training scale or heavy reliance on textual information. Motivated by the success of tabular foundation models (TFMs) in achieving universal prediction across diverse tabular datasets, we explore an alternative approach by TFMs, which are pre-trained on diverse synthetic datasets sampled from structural causal models and support strong in-context learning independent of textual attributes. Nevertheless, adapting TFMs for link prediction faces severe technical challenges such as how to obtain the necessary context and capture link-centric topological information. To solve these challenges, we propose TFMLinker (Tabular Foundation Model for Link Predictor), aiming to leverage the in-context learning capabilities of TFMs to perform link prediction across diverse graphs without requiring dataset-specific fine-tuning. Specifically, we first develop a prototype-augmented local-global context module to construct context that captures both graph-specific and cross-graph transferable patterns. Next, we design a universal topology-aware link encoder to capture link-centric topological information and generate link representations as inputs for the TFM. Finally, we employ the TFM to predict link existence through in-context learning. Experiments on 6 graph benchmarks across diverse domains demonstrate the superiority of our method over state-of-the-art baselines without requiring dataset-specific finetuning.

</details>


### [310] [Breaking the Grid: Distance-Guided Reinforcement Learning in Large Discrete and Hybrid Action Spaces](https://arxiv.org/abs/2602.08616)
*Heiko Hoppe,Fabian Akkerman,Wouter van Heeswijk,Maximilian Schiffer*

Main category: cs.LG

TL;DR: 提出DGRL方法解决大规模离散动作空间的强化学习问题，结合SDN和DBU技术，在高达10^20动作空间中实现高效学习，性能提升66%


<details>
  <summary>Details</summary>
Motivation: 强化学习在物流、调度和推荐系统等领域的应用面临大规模离散动作空间的维度灾难问题。现有方法依赖网格结构或计算昂贵的最近邻搜索，在高维或不规则结构域中效果有限。

Method: 提出距离引导强化学习(DGRL)，包含两个核心技术：1) 采样动态邻域(SDN)：利用语义嵌入空间进行随机体积探索，在局部信任区域提供完全支持；2) 基于距离的更新(DBU)：将策略优化转化为稳定的回归任务，解耦梯度方差与动作空间基数，保证策略单调改进。

Result: 在规则和不规则结构环境中，DGRL相比最先进基准方法性能提升高达66%，同时提高了收敛速度和计算复杂度。方法自然推广到混合连续-离散动作空间，无需层次依赖。

Conclusion: DGRL通过SDN和DBU的组合，有效解决了大规模离散动作空间的强化学习问题，在高达10^20动作空间中实现了高效学习，为物流、调度等实际应用提供了可行的解决方案。

Abstract: Reinforcement Learning is increasingly applied to logistics, scheduling, and recommender systems, but standard algorithms struggle with the curse of dimensionality in such large discrete action spaces. Existing algorithms typically rely on restrictive grid-based structures or computationally expensive nearest-neighbor searches, limiting their effectiveness in high-dimensional or irregularly structured domains. We propose Distance-Guided Reinforcement Learning (DGRL), combining Sampled Dynamic Neighborhoods (SDN) and Distance-Based Updates (DBU) to enable efficient RL in spaces with up to 10$^\text{20}$ actions. Unlike prior methods, SDN leverages a semantic embedding space to perform stochastic volumetric exploration, provably providing full support over a local trust region. Complementing this, DBU transforms policy optimization into a stable regression task, decoupling gradient variance from action space cardinality and guaranteeing monotonic policy improvement. DGRL naturally generalizes to hybrid continuous-discrete action spaces without requiring hierarchical dependencies. We demonstrate performance improvements of up to 66% against state-of-the-art benchmarks across regularly and irregularly structured environments, while simultaneously improving convergence speed and computational complexity.

</details>


### [311] [ERIS: Enhancing Privacy and Communication Efficiency in Serverless Federated Learning](https://arxiv.org/abs/2602.08617)
*Dario Fenoglio,Pasquale Polverino,Jacopo Quizi,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: ERIS是一个无服务器的联邦学习框架，通过模型分区和分布式梯度压缩，在保持FedAvg级别精度的同时显著降低通信成本并增强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 将联邦学习扩展到十亿参数模型时，通信效率、模型精度和隐私保证之间存在关键权衡。现有解决方案通常孤立地解决这些挑战，牺牲精度或依赖昂贵的密码学工具。

Method: ERIS结合了模型分区策略（将聚合分布在多个客户端聚合器上）和分布式移位梯度压缩机制，消除了服务器瓶颈并分布了通信负载。

Result: 理论证明ERIS在标准假设下以与FedAvg相同的速率收敛，并通过互信息泄漏与聚合器数量成反比的关系实现强隐私保证。实验表明ERIS在图像和文本任务（包括大语言模型）中达到FedAvg级别精度，同时显著降低通信成本并提高对成员推断和重建攻击的鲁棒性。

Conclusion: ERIS是一个平衡隐私和精度的无服务器联邦学习框架，无需依赖繁重的密码学或噪声注入，就能在保持模型精度的同时提供强大的隐私保护。

Abstract: Scaling federated learning (FL) to billion-parameter models introduces critical trade-offs between communication efficiency, model accuracy, and privacy guarantees. Existing solutions often tackle these challenges in isolation, sacrificing accuracy or relying on costly cryptographic tools. We propose ERIS, a serverless FL framework that balances privacy and accuracy while eliminating the server bottleneck and distributing the communication load. ERIS combines a model partitioning strategy, distributing aggregation across multiple client-side aggregators, with a distributed shifted gradient compression mechanism. We theoretically prove that ERIS (i) converges at the same rate as FedAvg under standard assumptions, and (ii) bounds mutual information leakage inversely with the number of aggregators, enabling strong privacy guarantees with no accuracy degradation. Experiments across image and text tasks, including large language models, confirm that ERIS achieves FedAvg-level accuracy while substantially reducing communication cost and improving robustness to membership inference and reconstruction attacks, without relying on heavy cryptography or noise injection.

</details>


### [312] [Sparse Models, Sparse Safety: Unsafe Routes in Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.08621)
*Yukun Jiang,Hai Huang,Mingjie Li,Yage Zhang,Michael Backes,Yang Zhang*

Main category: cs.LG

TL;DR: 研究发现MoE架构的LLMs存在安全漏洞，通过操纵路由器可以激活不安全路径，将安全输出转为有害内容


<details>
  <summary>Details</summary>
Motivation: 虽然MoE架构在降低计算成本方面表现出色，但之前的研究主要关注效用和效率，对其稀疏架构带来的安全风险探索不足。本研究旨在揭示MoE LLMs中存在的安全漏洞。

Method: 1. 引入Router Safety重要性评分(RoSais)量化每层路由器的安全关键性；2. 提出细粒度token-layer-wise随机优化框架(F-SOUR)来发现更具体的不安全路径，该框架明确考虑了输入token的顺序性和动态性。

Result: 1. 仅操纵高RoSais路由器就能将默认路径转为不安全路径；2. 在DeepSeek-V2-Lite上，屏蔽5个路由器使攻击成功率(ASR)提高4倍以上至0.79；3. F-SOUR在四个代表性MoE LLM家族上，在JailbreakBench和AdvBench上的平均ASR分别达到0.90和0.98。

Conclusion: MoE LLMs的安全性与其架构一样稀疏，存在不安全路径的安全风险。研究提出了防御视角，包括安全感知的路径禁用和路由器训练，为未来MoE LLMs的红队测试和安全保障提供参考。

Abstract: By introducing routers to selectively activate experts in Transformer layers, the mixture-of-experts (MoE) architecture significantly reduces computational costs in large language models (LLMs) while maintaining competitive performance, especially for models with massive parameters. However, prior work has largely focused on utility and efficiency, leaving the safety risks associated with this sparse architecture underexplored. In this work, we show that the safety of MoE LLMs is as sparse as their architecture by discovering unsafe routes: routing configurations that, once activated, convert safe outputs into harmful ones. Specifically, we first introduce the Router Safety importance score (RoSais) to quantify the safety criticality of each layer's router. Manipulation of only the high-RoSais router(s) can flip the default route into an unsafe one. For instance, on JailbreakBench, masking 5 routers in DeepSeek-V2-Lite increases attack success rate (ASR) by over 4$\times$ to 0.79, highlighting an inherent risk that router manipulation may naturally occur in MoE LLMs. We further propose a Fine-grained token-layer-wise Stochastic Optimization framework to discover more concrete Unsafe Routes (F-SOUR), which explicitly considers the sequentiality and dynamics of input tokens. Across four representative MoE LLM families, F-SOUR achieves an average ASR of 0.90 and 0.98 on JailbreakBench and AdvBench, respectively. Finally, we outline defensive perspectives, including safety-aware route disabling and router training, as promising directions to safeguard MoE LLMs. We hope our work can inform future red-teaming and safeguarding of MoE LLMs. Our code is provided in https://github.com/TrustAIRLab/UnsafeMoE.

</details>


### [313] [CauScale: Neural Causal Discovery at Scale](https://arxiv.org/abs/2602.08629)
*Bo Peng,Sirui Chen,Jiaguo Tian,Yu Qiao,Chaochao Lu*

Main category: cs.LG

TL;DR: CauScale是一种用于高效因果发现的神经架构，能够扩展到1000个节点的大图，通过压缩单元和共享注意力权重显著提升时空效率，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在处理大规模图时面临时间和空间效率瓶颈，无法扩展到大型图结构，限制了在科学AI和数据分析等数据驱动领域的应用。

Method: CauScale采用神经架构设计：1）通过压缩单元减少数据嵌入维度提升时间效率；2）采用共享注意力权重避免维护轴特定注意力图来提升空间效率；3）采用双流设计：数据流从高维观测中提取关系证据，图流整合统计图先验并保留关键结构信号。

Result: CauScale成功扩展到500节点图的训练（先前方法因空间限制无法实现），在分布内数据上达到99.6% mAP，分布外数据达到84.4% mAP，推理速度比先前方法快4-13,000倍。

Conclusion: CauScale通过创新的神经架构设计解决了大规模因果发现的时空效率瓶颈，实现了向1000节点图的扩展，在保持高准确率的同时显著提升了推理速度，为大规模因果发现提供了实用解决方案。

Abstract: Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.

</details>


### [314] [LEFT: Learnable Fusion of Tri-view Tokens for Unsupervised Time Series Anomaly Detection](https://arxiv.org/abs/2602.08638)
*Dezheng Wang,Tong Chen,Guansong Pang,Congyan Chen,Shihua Li,Hongzhi Yin*

Main category: cs.LG

TL;DR: LEFT是一种无监督时间序列异常检测框架，通过融合时间、频率和多尺度三视图特征，利用自适应Nyquist约束谱滤波器学习异常模式，实现高效准确的异常检测。


<details>
  <summary>Details</summary>
Motivation: 当前无监督时间序列异常检测面临的主要挑战是许多异常过于细微，无法在单一视图中检测到，而是表现为跨多个视图（如时间、频率、多分辨率）的不一致性。现有跨视图方法大多依赖特征或分数融合，缺乏分析-合成一致性约束。

Method: LEFT框架从三个互补视图学习特征token：频率域token嵌入周期性信息，时间域token捕捉局部动态，多尺度token学习不同时间序列粒度的异常模式。通过自适应Nyquist约束谱滤波器将原始时间序列重缩放为多个分辨率，然后编码。引入从粗粒度多尺度结构重构细粒度目标的新目标函数，并提出创新的时间-频率循环一致性约束来显式正则化跨视图一致性。

Result: 在真实世界基准测试中，LEFT相比SOTA基线获得了最佳检测精度，同时实现了5倍的FLOPs减少和8倍的训练加速。

Conclusion: LEFT通过融合三视图token并引入时间-频率循环一致性约束，有效解决了无监督时间序列异常检测中跨视图不一致性的问题，在保持高精度的同时显著提升了计算效率。

Abstract: As a fundamental data mining task, unsupervised time series anomaly detection (TSAD) aims to build a model for identifying abnormal timestamps without assuming the availability of annotations. A key challenge in unsupervised TSAD is that many anomalies are too subtle to exhibit detectable deviation in any single view (e.g., time domain), and instead manifest as inconsistencies across multiple views like time, frequency, and a mixture of resolutions. However, most cross-view methods rely on feature or score fusion and do not enforce analysis-synthesis consistency, meaning the frequency branch is not required to reconstruct the time signal through an inverse transform, and vice versa. In this paper, we present Learnable Fusion of Tri-view Tokens (LEFT), a unified unsupervised TSAD framework that models anomalies as inconsistencies across complementary representations. LEFT learns feature tokens from three views of the same input time series: frequency-domain tokens that embed periodicity information, time-domain tokens that capture local dynamics, and multi-scale tokens that learns abnormal patterns at varying time series granularities. By learning a set of adaptive Nyquist-constrained spectral filters, the original time series is rescaled into multiple resolutions and then encoded, allowing these multi-scale tokens to complement the extracted frequency- and time-domain information. When generating the fused representation, we introduce a novel objective that reconstructs fine-grained targets from coarser multi-scale structure, and put forward an innovative time-frequency cycle consistency constraint to explicitly regularize cross-view agreement. Experiments on real-world benchmarks show that LEFT yields the best detection accuracy against SOTA baselines, while achieving a 5x reduction on FLOPs and 8x speed-up for training.

</details>


### [315] [Projected Gradient Ascent for Efficient Reward-Guided Updates with One-Step Generative Models](https://arxiv.org/abs/2602.08646)
*Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

TL;DR: 提出一种带约束的潜在优化方法，通过硬性白高斯噪声约束防止奖励黑客攻击，在保持生成质量的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 现有的测试时潜在优化方法虽然能提升预训练生成模型的奖励引导生成效果，但存在奖励黑客攻击导致质量下降的问题，且计算速度过慢，难以实际应用

Method: 用硬性白高斯噪声约束替代软正则化，通过投影梯度上升法在每次更新后应用闭式投影，保持潜在向量始终具有白高斯噪声特性，防止导致不真实伪影的漂移

Result: 该方法仅需SOTA正则化方法30%的墙钟时间就能达到相当的美学评分，同时有效防止奖励黑客攻击，投影操作复杂度为O(N log N)，与排序或FFT相当，实际不增加计算时间

Conclusion: 提出的约束潜在优化方法使测试时优化既高效又可靠，通过硬性噪声约束在保持生成质量的同时显著提升计算效率，具有实际应用价值

Abstract: We propose a constrained latent optimization method for reward-guided generation that preserves white Gaussian noise characteristics with negligible overhead. Test-time latent optimization can unlock substantially better reward-guided generations from pretrained generative models, but it is prone to reward hacking that degrades quality and also too slow for practical use. In this work, we make test-time optimization both efficient and reliable by replacing soft regularization with hard white Gaussian noise constraints enforced via projected gradient ascent. Our method applies a closed-form projection after each update to keep the latent vector explicitly noise-like throughout optimization, preventing the drift that leads to unrealistic artifacts. This enforcement adds minimal cost: the projection matches the $O(N \log N)$ complexity of standard algorithms such as sorting or FFT and does not practically increase wall-clock time. In experiments, our approach reaches a comparable Aesthetic Score using only 30% of the wall-clock time required by the SOTA regularization-based method, while preventing reward hacking.

</details>


### [316] [From Robotics to Sepsis Treatment: Offline RL via Geometric Pessimism](https://arxiv.org/abs/2602.08655)
*Sarthak Wanjari*

Main category: cs.LG

TL;DR: Geo-IQL：一种计算高效的离线强化学习框架，通过基于k最近邻距离的密度惩罚来增强IQL，有效解决OOD动作高估问题，在敏感数据集上性能提升18%，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布外动作高估问题，现有方法在计算效率和性能之间存在权衡。CQL计算成本高，而IQL在病态数据集上容易退化到行为克隆。需要一种计算高效且能有效处理OOD误差的方法。

Method: 提出几何悲观主义框架，通过预计算状态-动作嵌入空间中k最近邻距离的密度惩罚，以奖励塑形方式注入OOD保守性，训练开销仅为O(1)。该方法作为IQL的模块化增强。

Result: 在D4RL MuJoCo基准测试中，Geo-IQL在敏感的中等回放任务上比标准IQL提升超过18分，种子间方差减少4倍。在MIMIC-III Sepsis数据集上，标准IQL退化为行为克隆，而Geo-IQL实现主动策略改进，与临床医生终末决策一致性达86.4%（IQL为75%）。

Conclusion: 几何悲观主义为关键现实世界决策系统提供了必要的正则化，能够安全克服局部最优，在保持计算效率的同时有效处理分布外动作高估问题。

Abstract: Offline Reinforcement Learning (RL) promises the recovery of optimal policies from static datasets, yet it remains susceptible to the overestimation of out-of-distribution (OOD) actions, particularly in fractured and sparse data manifolds.Current solutions necessitates a trade off between computational efficiency and performance. Methods like CQL offers rigorous conservatism but require tremendous compute power while efficient expectile-based methods like IQL often fail to correct OOD errors on pathological datasets, collapsing to Behavioural Cloning. In this work, we propose Geometric Pessimism, a modular, compute-efficient framework that augments standard IQL with density-based penalty derived from k-nearest-neighbour distances in the state-action embedding space. By pre-computing the penalties applied to each state-action pair our method injects OOD conservatism via reward shaping with a O(1) training overhead. Evaluated on the D4Rl MuJoCo benchmark, our method, Geo-IQL outperforms standard IQL on sensitive and unstable medium-replay tasks by over 18 points, while reducing inter-seed variance by 4x. Furthermore, Geo-IQL does not degrade performance on stable manifolds. Crucially, we validate our algorithm on the MIMIC-III Sepsis critical care dataset. While standard IQL collapses to behaviour cloning, Geo-IQL demonstrates active policy improvement. Maintaining safety constraints, achieving 86.4% terminal agreement with clinicians compared to IQL's 75%. Our results suggest that geometric pessimism provides the necessary regularisation to safely overcome local optima in critical, real-world decision systems.

</details>


### [317] [Two-Stage Data Synthesization: A Statistics-Driven Restricted Trade-off between Privacy and Prediction](https://arxiv.org/abs/2602.08657)
*Xiaotong Liu,Shao-Bo Lin,Jun Fan,Ding-Xuan Zhou*

Main category: cs.LG

TL;DR: 提出两阶段合成数据策略，解决隐私保护与预测性能之间的平衡问题，通过合成-混合阶段和KRR合成阶段实现统计驱动的受限隐私-预测权衡。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据方法主要关注统计信息保持，虽然有些研究涉及预测性能保证，但单阶段合成设计难以平衡需要大幅扰动的隐私要求和对此扰动敏感的预测性能。

Method: 提出两阶段合成策略：第一阶段采用"合成-混合"策略，先生成纯合成数据，再与原始数据融合；第二阶段基于核岭回归(KRR)的合成策略，先在原始数据上训练KRR模型，然后用第一阶段生成的合成输入生成合成输出。

Result: 通过理论分析和数值实验验证了方法的有效性，展示了其统计驱动和受限隐私-预测权衡的特性，并在营销问题和五个真实数据集上展示了方法的泛化能力。

Conclusion: 提出的两阶段合成策略能够实现统计驱动的受限隐私-预测权衡，并保证最优预测性能，为合成数据在隐私保护和下游预测任务中的应用提供了有效解决方案。

Abstract: Synthetic data have gained increasing attention across various domains, with a growing emphasis on their performance in downstream prediction tasks. However, most existing synthesis strategies focus on maintaining statistical information. Although some studies address prediction performance guarantees, their single-stage synthesis designs make it challenging to balance the privacy requirements that necessitate significant perturbations and the prediction performance that is sensitive to such perturbations. We propose a two-stage synthesis strategy. In the first stage, we introduce a synthesis-then-hybrid strategy, which involves a synthesis operation to generate pure synthetic data, followed by a hybrid operation that fuses the synthetic data with the original data. In the second stage, we present a kernel ridge regression (KRR)-based synthesis strategy, where a KRR model is first trained on the original data and then used to generate synthetic outputs based on the synthetic inputs produced in the first stage. By leveraging the theoretical strengths of KRR and the covariant distribution retention achieved in the first stage, our proposed two-stage synthesis strategy enables a statistics-driven restricted privacy--prediction trade-off and guarantee optimal prediction performance. We validate our approach and demonstrate its characteristics of being statistics-driven and restricted in achieving the privacy--prediction trade-off both theoretically and numerically. Additionally, we showcase its generalizability through applications to a marketing problem and five real-world datasets.

</details>


### [318] [Equalized Generative Treatment: Matching f-divergences for Fairness in Generative Models](https://arxiv.org/abs/2602.08660)
*Alexandre Verine,Rafael Pinot,Florian Le Bronnec*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型公平性定义EGT，要求所有敏感群体具有可比的生成质量，而非仅平衡生成概率，并通过min-max微调方法实现公平性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型的公平性概念主要从分类任务中借鉴，侧重于平衡各敏感群体的生成概率，但这些标准很脆弱，因为即使不同敏感群体的建模质量差异很大，也能满足这些标准。需要一种更全面的公平性定义来确保所有群体都能获得可比的生成质量。

Method: 提出了新的公平性定义——平等生成处理（EGT），要求所有敏感群体具有可比的生成质量，质量通过参考f-散度来衡量。分析了EGT引发的权衡，表明强制执行公平约束必然将整体模型质量与最难近似的群体质量耦合。基于这一理论洞察，提出简单高效的min-max微调方法，通过平衡各敏感群体的f-散度来满足EGT。

Result: 在图像和文本生成任务上的实验验证表明，min-max方法相比文献中的其他方法能够持续实现更公平的结果，同时在两个任务上都保持了有竞争力的整体性能。

Conclusion: EGT为生成模型提供了更全面的公平性定义，超越了仅关注概率平衡的传统方法。min-max微调方法能够有效实现EGT公平性，在保持整体性能的同时确保所有敏感群体获得可比的生成质量。

Abstract: Fairness is a crucial concern for generative models, which not only reflect but can also amplify societal and cultural biases. Existing fairness notions for generative models are largely adapted from classification and focus on balancing the probability of generating samples from each sensitive group. We show that such criteria are brittle, as they can be met even when different sensitive groups are modeled with widely varying quality. To address this limitation, we introduce a new fairness definition for generative models, termed as equalized generative treatment (EGT), which requires comparable generation quality across all sensitive groups, with quality measured via a reference f-divergence. We further analyze the trade-offs induced by EGT, demonstrating that enforcing fairness constraints necessarily couples the overall model quality to that of the most challenging group to approximate. This indicates that a simple yet efficient min-max fine-tuning method should be able to balance f-divergences across sensitive groups to satisfy EGT. We validate this theoretical insight through a set of experiments on both image and text generation tasks. We demonstrate that min-max methods consistently achieve fairer outcomes compared to other approaches from the literature, while maintaining competitive overall performance for both tasks.

</details>


### [319] [LLaDA2.1: Speeding Up Text Diffusion via Token Editing](https://arxiv.org/abs/2602.08676)
*Tiwei Bie,Maosong Cao,Xiang Cao,Bingsen Chen,Fuyuan Chen,Kun Chen,Lun Du,Daozhuo Feng,Haibo Feng,Mingliang Gong,Zhuocheng Gong,Yanmei Gu,Jian Guan,Kaiyuan Guan,Hongliang He,Zenan Huang,Juyong Jiang,Zhonghui Jiang,Zhenzhong Lan,Chengxi Li,Jianguo Li,Zehuan Li,Huabin Liu,Lin Liu,Guoshan Lu,Yuan Lu,Yuxin Ma,Xingyu Mou,Zhenxuan Pan,Kaida Qiu,Yuji Ren,Jianfeng Tan,Yiding Tian,Zian Wang,Lanning Wei,Tao Wu,Yipeng Xing,Wentao Ye,Liangyu Zha,Tianze Zhang,Xiaolu Zhang,Junbo Zhao,Da Zheng,Hao Zhong,Wanli Zhong,Jun Zhou,Junlin Zhou,Liwang Zhu,Muzhi Zhu,Yihong Zhuang*

Main category: cs.LG

TL;DR: LLaDA2.1通过结合Token-to-Token编辑和Mask-to-Token方案，引入可配置阈值解码，提供速度模式和质量模式，并首次为dLLMs实现大规模强化学习框架，在保持高质量的同时显著提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 虽然LLaDA2.0展示了100B级别块扩散模型的扩展潜力和内在并行化能力，但解码速度与生成质量之间的微妙平衡一直是一个难以突破的瓶颈。

Method: 1. 将Token-to-Token编辑无缝集成到传统的Mask-to-Token方案中，引入联合可配置阈值解码方案；2. 创建两种模式：速度模式（降低M2T阈值，依赖T2T优化输出）和质量模式（保守阈值确保基准性能）；3. 基于扩展上下文窗口，首次为dLLMs实现大规模强化学习框架，采用专门的稳定梯度估计技术。

Result: 在33个严格基准测试中，LLaDA2.1展现出强大的任务性能和闪电般的解码速度。尽管有100B参数规模，在编码任务上达到惊人性能：HumanEval+上892 TPS，BigCodeBench上801 TPS，LiveCodeBench上663 TPS。发布了LLaDA2.1-Mini（16B）和LLaDA2.1-Flash（100B）两个版本。

Conclusion: LLaDA2.1通过结构创新和强化学习对齐，成功突破了扩散模型解码速度与生成质量之间的权衡，不仅提高了推理精度和指令跟随保真度，还弥合了扩散动力学与复杂人类意图之间的鸿沟。

Abstract: While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.

</details>


### [320] [Dashed Line Defense: Plug-And-Play Defense Against Adaptive Score-Based Query Attacks](https://arxiv.org/abs/2602.08679)
*Yanzhang Fu,Zizheng Guo,Jizhou Luo*

Main category: cs.LG

TL;DR: 本文提出Dashed Line Defense (DLD)，一种针对基于分数的查询攻击的运行时防御方法，通过引入损失值模糊性来抵御自适应攻击策略。


<details>
  <summary>Details</summary>
Motivation: 基于分数的查询攻击对深度学习模型构成严重威胁，而现有的运行时防御方法要么需要模型参数访问权限，要么在面对自适应攻击时失效。作者发现即使最先进的即插即用防御也能被自适应攻击绕过，这暴露了现有防御方法的局限性。

Method: 提出Dashed Line Defense (DLD)，这是一种即插即用的后处理方法，通过在观察到的损失值与真实对抗强度之间引入模糊性，阻止攻击者可靠地分析和调整查询策略，从而有效破坏对抗样本生成过程。

Result: 在ImageNet上的实验验证了DLD的有效性，证明DLD在保持模型预测标签的同时，始终优于先前的防御方法，即使在最坏情况的自适应攻击下也能保持防御能力。

Conclusion: DLD提供了一种有效的运行时防御机制，能够抵御自适应查询攻击，解决了现有防御方法的局限性，为黑盒对抗攻击防御提供了新的解决方案。

Abstract: Score-based query attacks pose a serious threat to deep learning models by crafting adversarial examples (AEs) using only black-box access to model output scores, iteratively optimizing inputs based on observed loss values. While recent runtime defenses attempt to disrupt this process via output perturbation, most either require access to model parameters or fail when attackers adapt their tactics. In this paper, we first reveal that even the state-of-the-art plug-and-play defense can be bypassed by adaptive attacks, exposing a critical limitation of existing runtime defenses. We then propose Dashed Line Defense (DLD), a plug-and-play post-processing method specifically designed to withstand adaptive query strategies. By introducing ambiguity in how the observed loss reflects the true adversarial strength of candidate examples, DLD prevents attackers from reliably analyzing and adapting their queries, effectively disrupting the AE generation process. We provide theoretical guarantees of DLD's defense capability and validate its effectiveness through experiments on ImageNet, demonstrating that DLD consistently outperforms prior defenses--even under worst-case adaptive attacks--while preserving the model's predicted labels.

</details>


### [321] [The Theory and Practice of MAP Inference over Non-Convex Constraints](https://arxiv.org/abs/2602.08681)
*Leander Kurscheidt,Gabriele Masina,Roberto Sebastiani,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文提出两种约束最大后验概率预测方法：一种针对可精确高效求解的连续变量约束MAP问题，设计可扩展的消息传递算法；另一种通用方法将域划分为凸可行区域并与数值约束优化交替进行。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，概率机器学习系统需要在代数约束下进行预测（如避开障碍物的最可能轨迹）。这些现实约束通常非凸，且考虑的密度函数也非（对数）凹，使得约束MAP预测的计算既困难又不可靠。

Method: 1. 研究连续变量约束MAP推断可精确高效求解的条件，并为此可处理片段设计可扩展的消息传递算法。2. 提出通用约束MAP策略，将域划分为凸可行区域并与数值约束优化交替进行。

Result: 在合成和真实世界基准测试中，两种方法均优于无视约束的基线方法，并能扩展到复杂密度函数，这些密度函数对现有最先进的精确求解器来说是难以处理的。

Conclusion: 本文为约束MAP预测提供了有效的解决方案，既能处理可精确求解的特例，又能应对更一般的复杂约束场景，在安全关键应用中具有实用价值。

Abstract: In many safety-critical settings, probabilistic ML systems have to make predictions subject to algebraic constraints, e.g., predicting the most likely trajectory that does not cross obstacles.
  These real-world constraints are rarely convex, nor the densities considered are (log-)concave.
  This makes computing this constrained maximum a posteriori (MAP) prediction efficiently and reliably extremely challenging.
  In this paper, we first investigate under which conditions we can perform constrained MAP inference over continuous variables exactly and efficiently and devise a scalable message-passing algorithm for this tractable fragment.
  Then, we devise a general constrained MAP strategy that interleaves partitioning the domain into convex feasible regions with numerical constrained optimization.
  We evaluate both methods on synthetic and real-world benchmarks, showing our %
  approaches outperform constraint-agnostic baselines, and scale to complex densities intractable for SoTA exact solvers.

</details>


### [322] [CompilerKV: Risk-Adaptive KV Compression via Offline Experience Compilation](https://arxiv.org/abs/2602.08686)
*Ning Yang,Chengzhi Wang,Yibo Liu,Baoliang Tian,Haijun Zhang*

Main category: cs.LG

TL;DR: CompilerKV是一个风险自适应和头部感知的KV缓存压缩框架，通过离线经验编译成可重用决策表，在512令牌预算下恢复97.7%的FullKV性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文场景中LLMs受限于KV缓存内存的线性增长，现有压缩方法要么依赖静态阈值和注意力启发式，要么采用粗粒度内存分配，忽略了两个关键因素：提示相关的压缩风险变化和注意力头之间的功能异质性，导致令牌选择不稳定和尾部失败。

Method: 提出CompilerKV框架，包含两个协同组件：1) 通过离线上下文老虎机学习的头部异质性表，为不同注意力头分配特定可靠性权重；2) 风险自适应阈值门控机制，联合建模注意力熵和局部困惑度，将提示级风险转化为可部署的保留阈值。

Result: 在LongBench上的实验显示，在512令牌预算下，CompilerKV主导SOTA方法，恢复97.7%的FullKV性能，相比最强竞争对手获得高达+5.2分的提升。

Conclusion: CompilerKV通过风险自适应和头部感知的压缩框架，有效解决了长上下文LLMs中KV缓存内存限制问题，显著提升了压缩性能。

Abstract: Large Language Models (LLMs) in long-context scenarios are severely constrained by the linear growth of Key-Value (KV) cache memory. Existing KV compression methods rely either on static thresholds and attention-only heuristics or on coarse memory budget allocation. Under tight memory budgets, these methods overlook two key factors: prompt-dependent variation in compression risk and functional heterogeneity across attention heads, which destabilize token selection and lead to tail failures. To address these challenges, we propose CompilerKV, a risk-adaptive and head-aware compression framework that compiles offline experience into reusable decision tables for prefill-only deployment. CompilerKV integrates two key synergistic components: (i) a Head Heterogeneity Table, learned via offline contextual bandits, which assigns head-specific reliability weights to govern functional differences across attention heads explicitly; and (ii) a Risk-Adaptive Threshold Gating mechanism that jointly models attention entropy and local perplexity, transforming prompt-level risk into deployable retention thresholds. Experiments on LongBench show CompilerKV dominates SOTA methods under a 512-token budget, recovering 97.7\% of FullKV performance while achieving up to +5.2 points gain over the strongest competitor.

</details>


### [323] [Learning To Sample From Diffusion Models Via Inverse Reinforcement Learning](https://arxiv.org/abs/2602.08689)
*Constant Bourdrez,Alexandre Vérine,Olivier Cappé*

Main category: cs.LG

TL;DR: 提出一个基于逆强化学习的框架，用于学习扩散模型的采样策略，无需重新训练去噪器，通过将采样过程建模为马尔可夫决策过程来优化采样动态


<details>
  <summary>Details</summary>
Motivation: 扩散模型的训练计算成本高昂，但采样过程具有灵活性。利用这种灵活性可以改进生成样本质量和采样效率，但需要一种无需重新训练去噪器就能优化采样策略的方法

Method: 将扩散采样过程建模为离散时间有限时域马尔可夫决策过程，其中动作对应采样动态的可选修改。使用逆强化学习框架，通过策略梯度技术直接匹配目标行为，避免定义显式奖励函数

Result: 实验证明该方法能够提高预训练扩散模型生成样本的质量，并自动调整采样超参数

Conclusion: 提出的逆强化学习框架为优化扩散模型采样过程提供了一种有效方法，无需重新训练去噪器，既能提升样本质量又能提高采样效率

Abstract: Diffusion models generate samples through an iterative denoising process, guided by a neural network. While training the denoiser on real-world data is computationally demanding, the sampling procedure itself is more flexible. This adaptability serves as a key lever in practice, enabling improvements in both the quality of generated samples and the efficiency of the sampling process. In this work, we introduce an inverse reinforcement learning framework for learning sampling strategies without retraining the denoiser. We formulate the diffusion sampling procedure as a discrete-time finite-horizon Markov Decision Process, where actions correspond to optional modifications of the sampling dynamics. To optimize action scheduling, we avoid defining an explicit reward function. Instead, we directly match the target behavior expected from the sampler using policy gradient techniques. We provide experimental evidence that this approach can improve the quality of samples generated by pretrained diffusion models and automatically tune sampling hyperparameters.

</details>


### [324] [Trapped by simplicity: When Transformers fail to learn from noisy features](https://arxiv.org/abs/2602.08695)
*Evan Peters,Ando Deng,Matheus H. Zambianco,Devin Blankespoor,Achim Kempf*

Main category: cs.LG

TL;DR: Transformer在噪声特征数据训练后，对无噪声特征的目标函数泛化能力有限，尤其在随机布尔函数上表现不佳，而LSTM表现更差


<details>
  <summary>Details</summary>
Motivation: 研究在存在特征噪声的数据上训练的Transformer是否能够正确泛化到无噪声输入，即噪声鲁棒学习问题

Method: 通过实验对比Transformer和LSTM在k稀疏奇偶函数、多数函数和随机k-juntas上的噪声鲁棒学习能力，分析Transformer失败原因并提出改进方法

Result: Transformer在k稀疏奇偶函数和多数函数上成功实现噪声鲁棒学习，但在随机k-juntas上失败；LSTM在特征噪声下表现更差；Transformer失败源于对简单函数的偏好和最优噪声鲁棒函数灵敏度较低

Conclusion: Transformer在特征噪声下学习布尔函数时效果不佳，但可以通过添加惩罚高灵敏度解的损失项来改善；Transformer的简单性偏好与噪声鲁棒学习要求之间存在冲突

Abstract: Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.

</details>


### [325] [QUOKA: Query-Oriented KV Selection For Efficient LLM Prefill](https://arxiv.org/abs/2602.08722)
*Dalton Jones,Junyoung Park,Matthew Morse,Mingu Lee,Chris Lott,Harper Langston*

Main category: cs.LG

TL;DR: QUOKA是一种无需训练、硬件无关的稀疏注意力算法，通过选择性地保留代表性查询和相关的键值对来加速Transformer推理，在保持接近基线准确性的同时实现3-7倍的加速。


<details>
  <summary>Details</summary>
Motivation: Transformer推理中的注意力计算成本高昂，特别是在分块预填充阶段。研究发现许多查询只关注一小部分键，而低余弦相似度的查询对最终注意力对数有更大贡献，这为选择性注意力计算提供了机会。

Method: QUOKA采用两阶段方法：1）首先保留一小部分代表性查询（特别是低余弦相似度的查询）；2）然后选择与这些查询最相关的键。这种方法无需训练，硬件无关，适用于分块预填充场景。

Result: 在Needle-In-A-Haystack、LongBench、RULER和Math500等基准测试中，QUOKA实现了首令牌生成时间减少3倍，Nvidia GPU上注意力计算加速5倍，Intel Xeon CPU上加速近7倍，同时使用88%更少的键值对，保持接近基线准确性。

Conclusion: QUOKA通过基于查询相似度的选择性注意力计算，有效加速Transformer推理，在保持模型性能的同时显著减少计算开销，为高效长序列处理提供了实用解决方案。

Abstract: We present QUOKA: Query-oriented KV selection for efficient attention, a training-free and hardware agnostic sparse attention algorithm for accelerating transformer inference under chunked prefill. While many queries focus on a smaller group of keys in the attention operator, we observe that queries with low cosine similarity with respect to the mean query interact more strongly with more keys and have the greatest contribution to final attention logits. By prioritizing these low cosine similarity queries, the behavior of full attention during the prefill stage can be closely approximated. QUOKA leverages this observation, accelerating attention by (1) first retaining a small set of representative queries and (2) then subselectin the keys most aligned with those queries. Through experiments on Needle-In-A-Haystack, LongBench, RULER, and Math500, we show that, while realizing a 3x reduction in time-to-first-token, 5x speedup in attention on Nvidia GPUs and up to nearly a 7x speedup on Intel Xeon CPUs, QUOKA achieves near-baseline accuracy, utilizing 88% fewer key-value pairs per attention evaluation.

</details>


### [326] [Foundation Inference Models for Ordinary Differential Equations](https://arxiv.org/abs/2602.08733)
*Maximilian Mauel,Johannes R. Hübers,David Berghaus,Patrick Seifner,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: FIM-ODE是一种预训练的基础推理模型，能够通过单次前向传播直接从噪声轨迹数据预测ODE向量场，无需复杂训练流程或大量机器学习专业知识。


<details>
  <summary>Details</summary>
Motivation: 当前ODE向量场推断方法（如符号回归、高斯过程回归、神经ODE）通常需要复杂的训练流程、大量机器学习专业知识，或严重依赖系统特定的先验知识，这限制了其在实际科学建模中的应用。

Method: 提出FIM-ODE预训练基础推理模型，在低次多项式向量场的ODE先验分布上进行预训练，使用神经算子表示目标向量场，能够通过单次前向传播直接从噪声轨迹数据预测向量场。

Result: FIM-ODE在零样本性能上表现强劲，匹配甚至超越了最近的预训练符号基准ODEFormer；预训练为微调提供了强初始化，实现了快速稳定的适应，优于现代神经和GP基线，且无需机器学习专业知识。

Conclusion: FIM-ODE通过预训练基础模型的方法，简化了ODE向量场推断流程，降低了使用门槛，为科学建模中的ODE推断问题提供了高效、易用的解决方案。

Abstract: Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.

</details>


### [327] [Redundancy-Free View Alignment for Multimodal Human Activity Recognition with Arbitrarily Missing Views](https://arxiv.org/abs/2602.08755)
*Duc-Anh Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: RALIS：一种用于多模态多视图学习的新模型，通过调整的中心对比损失和专家混合模块，支持训练和推理期间的任意视图可用性，计算复杂度从O(V²)降低到O(V)。


<details>
  <summary>Details</summary>
Motivation: 现有多模态多视图学习方法在处理灵活视图配置（包括任意视图组合、视图数量和异构模态）方面存在困难，特别是在人类活动识别任务中。需要一种能够适应训练和推理期间任意视图可用性的方法。

Method: 提出RALIS模型，结合多视图对比学习和专家混合模块。使用调整的中心对比损失进行自监督表示学习和视图对齐，减轻缺失视图对多视图融合的影响。通过专家混合模块处理对比学习未捕获的残差异常，采用专门的负载平衡策略适应任意视图组合。

Result: 在包含惯性和人体姿态模态的四个数据集上验证，视图数量从3到9个，证明了模型的性能和灵活性。计算复杂度从O(V²)降低到O(V)。

Conclusion: RALIS通过创新的对比损失和专家混合模块，有效解决了多模态多视图学习中的灵活视图配置问题，在人类活动识别任务中表现出色。

Abstract: Multimodal multiview learning seeks to integrate information from diverse sources to enhance task performance. Existing approaches often struggle with flexible view configurations, including arbitrary view combinations, numbers of views, and heterogeneous modalities. Focusing on the context of human activity recognition, we propose RALIS, a model that combines multiview contrastive learning with a mixture-of-experts module to support arbitrary view availability during both training and inference. Instead of trying to reconstruct missing views, an adjusted center contrastive loss is used for self-supervised representation learning and view alignment, mitigating the impact of missing views on multiview fusion. This loss formulation allows for the integration of view weights to account for view quality. Additionally, it reduces computational complexity from $O(V^2)$ to $O(V)$, where $V$ is the number of views. To address residual discrepancies not captured by contrastive learning, we employ a mixture-of-experts module with a specialized load balancing strategy, tasked with adapting to arbitrary view combinations. We highlight the geometric relationship among components in our model and how they combine well in the latent space. RALIS is validated on four datasets encompassing inertial and human pose modalities, with the number of views ranging from three to nine, demonstrating its performance and flexibility.

</details>


### [328] [FreqLens: Interpretable Frequency Attribution for Time Series Forecasting](https://arxiv.org/abs/2602.08768)
*Chi-Sheng Chen,Xinyu Zhang,En-Jui Kuo,Guan-Ying Chen,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: FreqLens是一个可解释的时间序列预测框架，通过可学习的频率分量发现和理论保证的频率归因，实现高性能且可解释的预测。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型缺乏可解释性，限制了其在需要可解释预测的领域中的应用。需要一种既能提供准确预测又能解释预测依据的方法。

Method: 提出FreqLens框架，包含两个关键创新：1) 可学习频率发现 - 通过sigmoid映射参数化频率基，从数据中学习并自动发现主要周期性模式；2) 公理化频率归因 - 基于理论框架，满足完备性、忠实性、零频率和对称性公理，频率归因等价于Shapley值。

Result: 在Traffic和Weather数据集上，FreqLens实现了竞争性或更优的性能，同时发现了物理上有意义的频率：在Traffic数据中所有5次独立运行都发现了24小时日周期（24.6±0.1h，2.5%误差）和12小时半日周期（11.8±0.1h，1.6%误差），在Weather数据中发现了周周期（比输入窗口长10倍）。

Conclusion: FreqLens展示了在频率级别上的真正知识发现，同时具有归因质量的正式理论保证，为时间序列预测提供了高性能且可解释的解决方案。

Abstract: Time series forecasting models often lack interpretability, limiting their adoption in domains requiring explainable predictions. We propose \textsc{FreqLens}, an interpretable forecasting framework that discovers and attributes predictions to learnable frequency components. \textsc{FreqLens} introduces two key innovations: (1) \emph{learnable frequency discovery} -- frequency bases are parameterized via sigmoid mapping and learned from data with diversity regularization, enabling automatic discovery of dominant periodic patterns without domain knowledge; and (2) \emph{axiomatic frequency attribution} -- a theoretically grounded framework that provably satisfies Completeness, Faithfulness, Null-Frequency, and Symmetry axioms, with per-frequency attributions equivalent to Shapley values. On Traffic and Weather datasets, \textsc{FreqLens} achieves competitive or superior performance while discovering physically meaningful frequencies: all 5 independent runs discover the 24-hour daily cycle ($24.6 \pm 0.1$h, 2.5\% error) and 12-hour half-daily cycle ($11.8 \pm 0.1$h, 1.6\% error) on Traffic, and weekly cycles ($10\times$ longer than the input window) on Weather. These results demonstrate genuine frequency-level knowledge discovery with formal theoretical guarantees on attribution quality.

</details>


### [329] [Default Machine Learning Hyperparameters Do Not Provide Informative Initialization for Bayesian Optimization](https://arxiv.org/abs/2602.08774)
*Nicolás Villagrán Prieto,Eduardo C. Garrido-Merchán*

Main category: cs.LG

TL;DR: 贝叶斯优化使用库默认超参数作为初始化点并不能带来统计显著的优势，与随机初始化相比最终性能无差异


<details>
  <summary>Details</summary>
Motivation: 研究是否可以利用机器学习库（如scikit-learn）中默认超参数值所隐含的专家知识来加速贝叶斯优化的收敛过程

Method: 使用以库默认值为中心的高斯分布初始化贝叶斯优化，并与均匀随机初始化进行对比。实验涵盖三个BO后端、三个模型家族和五个基准数据集，通过收敛速度和最终预测质量评估性能，使用单侧二项检验确定统计显著性

Result: 在所有实验条件下，基于默认值的初始化相比纯随机采样没有统计显著优势（p值范围0.141-0.908）。虽然更紧密地围绕默认值可以改善早期评估，但这种短暂优势随着优化进展而消失，最终性能保持不变

Conclusion: 库默认超参数并未包含对优化有用的方向性信息，建议将超参数调优作为模型开发的必要部分，采用基于数据的搜索策略而非依赖库默认值的启发式方法

Abstract: Bayesian Optimization (BO) is a standard tool for hyperparameter tuning thanks to its sample efficiency on expensive black-box functions. While most BO pipelines begin with uniform random initialization, default hyperparameter values shipped with popular ML libraries such as scikit-learn encode implicit expert knowledge and could serve as informative starting points that accelerate convergence. This hypothesis, despite its intuitive appeal, has remained largely unexamined. We formalize the idea by initializing BO with points drawn from truncated Gaussian distributions centered at library defaults and compare the resulting trajectories against a uniform-random baseline. We conduct an extensive empirical evaluation spanning three BO back-ends (BoTorch, Optuna, Scikit-Optimize), three model families (Random Forests, Support Vector Machines, Multilayer Perceptrons), and five benchmark datasets covering classification and regression tasks. Performance is assessed through convergence speed and final predictive quality, and statistical significance is determined via one-sided binomial tests. Across all conditions, default-informed initialization yields no statistically significant advantage over purely random sampling, with p-values ranging from 0.141 to 0.908. A sensitivity analysis on the prior variance confirms that, while tighter concentration around the defaults improves early evaluations, this transient benefit vanishes as optimization progresses, leaving final performance unchanged. Our results provide no evidence that default hyperparameters encode useful directional information for optimization. We therefore recommend that practitioners treat hyperparameter tuning as an integral part of model development and favor principled, data-driven search strategies over heuristic reliance on library defaults.

</details>


### [330] [Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI](https://arxiv.org/abs/2602.08809)
*Karim Haroun,Aya Zitouni,Aicha Zenakhri,Meriem Amel Guessoum,Larbi Boubchir*

Main category: cs.LG

TL;DR: 本文简要综述了生物识别应用中高效深度学习方法的现状，重点讨论了训练和部署深度学习模型时面临的挑战，并提出了一个高效深度学习方法的分类体系。


<details>
  <summary>Details</summary>
Motivation: 深度学习在安全防御等应用中取得了显著进展，但其训练和部署过程中巨大的计算需求导致高能耗和碳足迹，限制了在资源受限的边缘设备上的实时应用和可扩展性。

Method: 通过文献综述的方式，对高效深度学习方法进行分类整理，提出评估模型效率的补充指标（内存、计算、延迟、吞吐量），并倡导建立通用且可复现的评估标准。

Result: 建立了一个高效深度学习方法的分类体系，提出了多维度的效率评估框架，为生物识别领域的模型优化提供了系统性的指导。

Conclusion: 需要开发更高效的深度学习模型以应对计算资源限制和环境影响，同时需要建立标准化的效率评估指标来促进该领域的发展，并提出了未来的研究方向。

Abstract: Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.

</details>


### [331] [Robust Policy Optimization to Prevent Catastrophic Forgetting](https://arxiv.org/abs/2602.08813)
*Mahdi Sabbaghi,George Pappas,Adel Javanmard,Hamed Hassani*

Main category: cs.LG

TL;DR: FRPO提出一种鲁棒的RLHF框架，通过优化KL有界邻域内的策略来防止下游微调时的灾难性遗忘，在保持下游任务性能的同时显著减少安全退化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多阶段后训练中，即使小的下游更新也会损害先前学习的行为（如安全性），这种灾难性遗忘表明标准RLHF目标不能保证对未来适应的鲁棒性。现有方法主要关注下游时的方法来保留先前行为，但本文认为需要预微调鲁棒性。

Method: 提出Fine-tuning Robust Policy Optimization (FRPO)，一个鲁棒的RLHF框架，不仅优化当前策略的奖励，还优化下游适应可达的KL有界邻域内的策略。采用最大最小化公式确保策略变化下的奖励稳定性，通过修改GRPO开发无需额外计算的算法。

Result: 实验表明FRPO显著减少了多个基础模型和下游微调机制（SFT和RL）下的安全退化，同时保持了下游任务性能。在数学聚焦的RL设置中，FRPO在后续微调下保持了准确性。

Conclusion: FRPO通过预微调鲁棒性方法有效解决了灾难性遗忘问题，为RLHF框架提供了更好的鲁棒性保证，确保模型在下游适应时保持先前学习的重要行为。

Abstract: Large language models are commonly trained through multi-stage post-training: first via RLHF, then fine-tuned for other downstream objectives. Yet even small downstream updates can compromise earlier learned behaviors (e.g., safety), exposing a brittleness known as catastrophic forgetting. This suggests standard RLHF objectives do not guarantee robustness to future adaptation. To address it, most prior work designs downstream-time methods to preserve previously learned behaviors. We argue that preventing this requires pre-finetuning robustness: the base policy should avoid brittle high-reward solutions whose reward drops sharply under standard fine-tuning.
  We propose Fine-tuning Robust Policy Optimization (FRPO), a robust RLHF framework that optimizes reward not only at the current policy, but across a KL-bounded neighborhood of policies reachable by downstream adaptation. The key idea is to ensure reward stability under policy shifts via a max-min formulation. By modifying GRPO, we develop an algorithm with no extra computation, and empirically show it substantially reduces safety degradation across multiple base models and downstream fine-tuning regimes (SFT and RL) while preserving downstream task performance. We further study a math-focused RL setting, demonstrating that FRPO preserves accuracy under subsequent fine-tuning.

</details>


### [332] [FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models](https://arxiv.org/abs/2602.08818)
*Annemette Brok Pirchert,Jacob Nielsen,Mogens Henrik From,Lukas Galke Poech,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: FlexMoRE是一种灵活的混合专家模型，支持不同秩的专家（全尺寸专家或低秩适配器），通过优化专家秩在保持性能的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家架构通常训练全尺寸专家，但作者假设并非所有领域都需要全尺寸专家，低秩适配器可能就足够。为了在内存效率和性能之间找到最佳平衡，需要研究专家秩与下游任务性能之间的关系。

Method: 提出FlexMoRE（Flexible Mixture of Rank-heterogenous Experts），支持全尺寸专家或不同秩的低秩适配器。基于FlexOlmo构建，将其预训练专家转换为低秩版本。系统评估6种不同秩的专家（从2^0到2^14），在150种混合配置（96种2专家混合，54种7专家混合）和120个任务上进行实验。

Result: 回归分析显示，推理密集型基准测试的最佳秩显著高于知识密集型基准测试。使用最优秩时，FlexMoRE在平均得分47.18的情况下，参数数量不到FlexOlmo全尺寸专家混合的三分之一（10.75B vs 33.27B），且性能优于基线（45.46）。

Conclusion: FlexMoRE通过灵活混合不同秩的专家，在显著减少参数数量的同时提高了下游任务性能，证明了专家秩优化对内存效率和性能平衡的重要性，特别是针对不同类型的任务需求。

Abstract: Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating $6$ experts with ranks $2^0$ to $2^{14}$ resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across $120$ tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score $47.18$) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score $45.46$) at less than one third the parameters ($10.75$B for FlexMoRE vs. $33.27$B for FlexOlmo). All code will be made available.

</details>


### [333] [Bayesian Preference Learning for Test-Time Steerable Reward Models](https://arxiv.org/abs/2602.08819)
*Jiwoo Hong,Shao Tang,Zhipeng Wang*

Main category: cs.LG

TL;DR: 提出变分上下文奖励建模（ICRM），一种贝叶斯奖励建模方法，通过上下文偏好演示实现测试时可引导性，在单目标和多目标设置中都能适应未见过的偏好分布。


<details>
  <summary>Details</summary>
Motivation: 随着强化学习应用于可验证奖励和多目标对齐等场景，奖励模型需要编码更复杂、多方面的偏好分布。但传统的分类器奖励模型一旦训练完成就保持静态，限制了其在测试时的适应性。

Method: 提出变分上下文奖励建模（ICRM），将奖励建模转化为在Bradley-Terry模型下使用共轭Beta先验对潜在偏好概率进行摊销变分推断。该方法通过上下文偏好演示实现测试时可引导性。

Result: ICRM在单目标设置中，随着更多上下文演示，在SafeRLHF上获得34%准确率提升，在RM-Bench上获得9%准确率提升；在多目标设置中，在有用性和拒绝基准上，帕累托前沿扩展了4%的超体积增益。在数学推理任务中，ICRM能有效编码可验证奖励，优于传统奖励模型。

Conclusion: ICRM通过贝叶斯框架和上下文学习实现了奖励模型的测试时可引导性，在单目标和多目标对齐任务中表现出色，并提供了理论保证证明变分目标存在全局内部最优解，KL正则化能缓解奖励过度优化问题。

Abstract: Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.

</details>


### [334] [Dr. MAS: Stable Reinforcement Learning for Multi-Agent LLM Systems](https://arxiv.org/abs/2602.08847)
*Lang Feng,Longtao Zheng,Shuo He,Fuxiang Zhang,Bo An*

Main category: cs.LG

TL;DR: 本文提出Dr. MAS方法，通过按智能体归一化优势值来解决多智能体LLM系统中强化学习训练不稳定的问题，显著提升训练稳定性并改善性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统通过角色专业化实现高级推理和工具使用，但现有的群体强化学习（RL）方法在这种系统中存在训练不稳定的问题。作者从理论上分析了GRPO风格优化中全局归一化基线偏离不同智能体奖励分布的问题，这导致梯度范数不稳定。

Method: 提出Dr. MAS方法，采用智能体级别的解决方案：使用每个智能体自身的奖励统计信息对优势值进行归一化，从而校准梯度尺度。该方法还提供了一个端到端的RL训练框架，支持可扩展编排、灵活的按智能体LLM服务和优化配置，以及共享的LLM执行器后端资源调度。

Result: 在数学推理和多轮搜索基准测试中使用Qwen2.5和Qwen3系列模型进行评估。Dr. MAS相比原始GRPO在数学任务上平均提升5.6%（avg@16）和4.6%（pass@16），在搜索任务上提升15.2%（avg@16）和13.1%（pass@16），同时大幅消除梯度尖峰。在异构智能体模型分配下仍保持高效。

Conclusion: Dr. MAS通过智能体级别的优势值归一化有效解决了多智能体LLM系统中强化学习训练不稳定的问题，提供了简单稳定的训练方案，显著提升性能并保持训练稳定性。

Abstract: Multi-agent LLM systems enable advanced reasoning and tool use via role specialization, yet reliable reinforcement learning (RL) post-training for such systems remains difficult. In this work, we theoretically pinpoint a key reason for training instability when extending group-based RL to multi-agent LLM systems. We show that under GRPO-style optimization, a global normalization baseline may deviate from diverse agents' reward distributions, which ultimately leads to gradient-norm instability. Based on this finding, we propose Dr. MAS, a simple and stable RL training recipe for multi-agent LLM systems. Dr. MAS uses an agent-wise remedy: normalizing advantages per agent using each agent's own reward statistics, which calibrates gradient scales and dramatically stabilizes training, both theoretically and empirically. Beyond the algorithm, Dr. MAS provides an end-to-end RL training framework for multi-agent LLM systems, supporting scalable orchestration, flexible per-agent LLM serving and optimization configs, and shared resource scheduling of LLM actor backends. We evaluate Dr. MAS on multi-agent math reasoning and multi-turn search benchmarks using Qwen2.5 and Qwen3 series models. Dr. MAS achieves clear gains over vanilla GRPO (e.g., +5.6\% avg@16 and +4.6\% pass@16 on math, and +15.2\% avg@16 and +13.1\% pass@16 on search) while largely eliminating gradient spikes. Moreover, it remains highly effective under heterogeneous agent-model assignments while improving efficiency.

</details>


### [335] [Discovering Interpretable Algorithms by Decompiling Transformers to RASP](https://arxiv.org/abs/2602.08857)
*Xinting Huang,Aleksandra Bakalova,Satwik Bhattamishra,William Merrill,Michael Hahn*

Main category: cs.LG

TL;DR: 本文提出了一种从训练好的Transformer模型中提取简单可解释RASP程序的方法，通过将Transformer重新参数化为RASP程序并应用因果干预来发现最小的充分子程序。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究表明Transformer的计算可以在RASP编程语言族中模拟，并且Transformer在具有简单RASP程序的问题上能够精确地进行长度泛化，但尚不清楚训练好的模型是否真正实现了简单可解释的程序。本文旨在解决这个问题。

Method: 提出了一种通用方法：首先将Transformer忠实重新参数化为RASP程序，然后应用因果干预来发现一个小的充分子程序。在小型Transformer上进行实验，这些模型在算法和形式语言任务上训练。

Result: 实验表明，该方法通常能够从长度泛化的Transformer中恢复出简单且可解释的RASP程序。这些结果为Transformer内部实现简单RASP程序提供了迄今为止最直接的证据。

Conclusion: 本文的方法成功地从训练好的Transformer中提取了简单可解释的RASP程序，为理解Transformer的内部工作机制提供了新的工具和证据。

Abstract: Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.

</details>


### [336] [Magnitude Distance: A Geometric Measure of Dataset Similarity](https://arxiv.org/abs/2602.08859)
*Sahel Torkamani,Henry Gouk,Rik Sarkar*

Main category: cs.LG

TL;DR: 提出了一种基于度量空间magnitude概念的新距离度量——magnitude distance，该距离包含可调缩放参数t，能控制对全局结构（小t）和细节（大t）的敏感性，在适当调整尺度时在高维设置中保持区分性，并可作为生成模型的训练目标。


<details>
  <summary>Details</summary>
Motivation: 量化数据集之间的距离是数学和机器学习中的基本问题，现有距离度量在高维设置中可能失去区分性，需要一种能适应不同尺度结构的新距离度量。

Method: 基于度量空间的magnitude概念定义magnitude distance，引入可调缩放参数t控制距离对全局结构和细节的敏感性，证明其理论性质包括极限行为和度量性质条件，将其应用于push-forward生成模型的训练目标。

Result: 证明了magnitude distance的理论性质，包括在不同尺度下的极限行为，以及满足关键度量性质的条件；实验表明在适当调整尺度时，该距离在高维设置中保持区分性，作为生成模型训练目标时提供有意义的信号，与现有基于距离的生成方法相当。

Conclusion: magnitude distance是一种新颖的距离度量，通过可调参数t适应不同尺度结构，在高维设置中保持区分性，可作为生成模型的有效训练目标，为数据集距离量化提供了新工具。

Abstract: Quantifying the distance between datasets is a fundamental question in mathematics and machine learning. We propose \textit{magnitude distance}, a novel distance metric defined on finite datasets using the notion of the \emph{magnitude} of a metric space. The proposed distance incorporates a tunable scaling parameter, $t$, that controls the sensitivity to global structure (small $t$) and finer details (large $t$). We prove several theoretical properties of magnitude distance, including its limiting behavior across scales and conditions under which it satisfies key metric properties. In contrast to classical distances, we show that magnitude distance remains discriminative in high-dimensional settings when the scale is appropriately tuned. We further demonstrate how magnitude distance can be used as a training objective for push-forward generative models. Our experimental results support our theoretical analysis and demonstrate that magnitude distance provides meaningful signals, comparable to established distance-based generative approaches.

</details>


### [337] [Near-optimal Swap Regret Minimization for Convex Losses](https://arxiv.org/abs/2602.08862)
*Lunjia Hu,Jon Schneider,Yifan Wu*

Main category: cs.LG

TL;DR: 本文提出了一种随机在线算法，在单位区间上针对自适应选择的Lipschitz凸损失序列，实现了近乎最优的$\widetilde O(\sqrt T)$期望交换遗憾，改进了之前$\widetilde O(T^{2/3})$的最佳界限，并回答了Fishelson等人的一个开放性问题。


<details>
  <summary>Details</summary>
Motivation: 在线学习中的交换遗憾（swap regret）是一个重要指标，衡量算法相对于最佳固定策略转换的后悔程度。先前的最佳界限是$\widetilde O(T^{2/3})$，而理论上的下界是$\Omega(\sqrt T)$，因此存在改进空间。此外，该问题与可引出属性的校准误差最小化密切相关，特别是中位数校准等应用场景。

Method: 提出了一种称为"多尺度分箱"（multi-scale binning）的关键技术。该方法将单位区间离散化为多个粒度尺度的箱，并同时使用所有尺度进行随机预测。算法在多项式时间内运行，通过这种多尺度方法实现了更好的遗憾界限。

Result: 1. 实现了$\widetilde O(\sqrt T)$的期望交换遗憾，改进了之前的$\widetilde O(T^{2/3})$界限，达到了近乎最优的结果（因为下界是$\Omega(\sqrt T)$）。
2. 算法是高效的，运行时间为$\mathsf{poly}(T)$。
3. 作为直接推论，为一般可引出属性的校准误差最小化提供了高效在线算法，无需先前工作中需要的识别函数的Lipschitz假设。
4. 特别地，为中位数校准实现了首个$\widetilde O(\sqrt T)$校准误差保证。

Conclusion: 本文通过创新的多尺度分箱技术，在单位区间上的Lipschitz凸损失在线学习中实现了近乎最优的交换遗憾界限，解决了Fishelson等人的开放性问题。该结果不仅改进了理论界限，还为校准误差最小化提供了更通用的算法框架，特别是扩展到了中位数校准等先前方法无法处理的应用场景。

Abstract: We give a randomized online algorithm that guarantees near-optimal $\widetilde O(\sqrt T)$ expected swap regret against any sequence of $T$ adaptively chosen Lipschitz convex losses on the unit interval. This improves the previous best bound of $\widetilde O(T^{2/3})$ and answers an open question of Fishelson et al. [2025b]. In addition, our algorithm is efficient: it runs in $\mathsf{poly}(T)$ time. A key technical idea we develop to obtain this result is to discretize the unit interval into bins at multiple scales of granularity and simultaneously use all scales to make randomized predictions, which we call multi-scale binning and may be of independent interest. A direct corollary of our result is an efficient online algorithm for minimizing the calibration error for general elicitable properties. This result does not require the Lipschitzness assumption of the identification function needed in prior work, making it applicable to median calibration, for which we achieve the first $\widetilde O(\sqrt T)$ calibration error guarantee.

</details>


### [338] [AnomSeer: Reinforcing Multimodal LLMs to Reason for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.08868)
*Junru Zhang,Lang Feng,Haoran Shi,Xu Guo,Han Yu,Yabo Dong,Duanqing Xu*

Main category: cs.LG

TL;DR: AnomSeer：一种基于多模态大语言模型的时间序列异常检测方法，通过专家思维链和时序基础策略优化，实现异常分类、定位和解释的统一框架，在点异常和频率异常检测上优于GPT-4o等大型商业模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于多模态大语言模型的时间序列异常检测方法依赖粗粒度启发式方法，难以进行多维度的详细推理，而理解复杂时间序列数据需要精确的结构细节分析能力。

Method: 1. 生成专家思维链跟踪，提供基于经典分析（统计测量、频率变换）的可验证细粒度推理；2. 提出时序基础策略优化（TimerPO），包含基于最优传输的时序基础优势和正交投影组件，确保辅助细粒度信号不干扰主要检测目标。

Result: 使用Qwen2.5-VL-3B/7B-Instruct的AnomSeer在多种异常场景下，在分类和定位准确率上优于GPT-4o等大型商业基线模型，特别是在点异常和频率驱动异常检测方面表现突出，并能生成支持其结论的合理时间序列推理轨迹。

Conclusion: AnomSeer通过强化模型基于时间序列精确结构细节进行推理的能力，成功解决了多模态大语言模型在时间序列异常检测中细粒度推理不足的问题，实现了异常检测、定位和解释的统一框架，在准确性和可解释性方面均取得显著提升。

Abstract: Time-series anomaly detection (TSAD) with multimodal large language models (MLLMs) is an emerging area, yet a persistent challenge remains: MLLMs rely on coarse time-series heuristics but struggle with multi-dimensional, detailed reasoning, which is vital for understanding complex time-series data. We present AnomSeer to address this by reinforcing the model to ground its reasoning in precise, structural details of time series, unifying anomaly classification, localization, and explanation. At its core, an expert chain-of-thought trace is generated to provide a verifiable, fine-grained reasoning from classical analyses (e.g., statistical measures, frequency transforms). Building on this, we propose a novel time-series grounded policy optimization (TimerPO) that incorporates two additional components beyond standard reinforcement learning: a time-series grounded advantage based on optimal transport and an orthogonal projection to ensure this auxiliary granular signal does not interfere with the primary detection objective. Across diverse anomaly scenarios, AnomSeer, with Qwen2.5-VL-3B/7B-Instruct, outperforms larger commercial baselines (e.g., GPT-4o) in classification and localization accuracy, particularly on point- and frequency-driven exceptions. Moreover, it produces plausible time-series reasoning traces that support its conclusions.

</details>


### [339] [Stress-Testing Alignment Audits With Prompt-Level Strategic Deception](https://arxiv.org/abs/2602.08877)
*Oliver Daniels,Perusha Moodley,Ben Marlin,David Lindner*

Main category: cs.LG

TL;DR: 论文开发了一个自动红队测试管道，针对对齐审计方法生成欺骗策略，发现现有白盒和黑盒方法在面对战略性欺骗时都不够稳健。


<details>
  <summary>Details</summary>
Motivation: 现有对齐审计方法旨在检测战略性、情境感知的错位模型中的隐藏目标，但这些方法尚未经过系统性压力测试来评估其对抗欺骗策略的能力。

Method: 构建自动红队测试管道，针对特定白盒和黑盒审计方法生成定制化的欺骗策略（以系统提示形式），测试助手预填充、用户角色采样、稀疏自编码器和词嵌入相似性等方法。

Result: 自动红队管道找到了能够欺骗黑盒和白盒方法的提示，使其产生自信但错误的猜测。这是首次记录到基于激活的战略性欺骗证据。

Conclusion: 当前的黑盒和白盒审计方法在面对足够强大的错位模型时不够稳健，需要开发更强大的对齐审计技术。

Abstract: Alignment audits aim to robustly identify hidden goals from strategic, situationally aware misaligned models. Despite this threat model, existing auditing methods have not been systematically stress-tested against deception strategies. We address this gap, implementing an automatic red-team pipeline that generates deception strategies (in the form of system prompts) tailored to specific white-box and black-box auditing methods. Stress-testing assistant prefills, user persona sampling, sparse autoencoders, and token embedding similarity methods against secret-keeping model organisms, our automatic red-team pipeline finds prompts that deceive both the black-box and white-box methods into confident, incorrect guesses. Our results provide the first documented evidence of activation-based strategic deception, and suggest that current black-box and white-box methods would not be robust to a sufficiently capable misaligned model.

</details>


### [340] [Learning Potentials for Dynamic Matching and Application to Heart Transplantation](https://arxiv.org/abs/2602.08878)
*Itai Zilberstein,Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 提出基于潜力的非近视政策优化框架，用于心脏移植器官分配，通过自监督模仿学习训练潜力函数，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 心脏移植患者面临器官短缺导致的致命等待时间，现有分配政策未能充分考虑器官动态到达和等待名单组成，美国正从基于规则的系统转向数据驱动模型

Method: 提出基于潜力的非近视政策优化框架，通过自监督模仿学习训练高维表达能力更强的潜力函数，模仿具有完美预见性的全知算法

Result: 使用真实历史数据证明，该方法在优化群体水平结果方面显著优于现有方法，包括美国现状政策和提出的连续分布框架

Conclusion: 在美国心脏移植分配系统审查的关键时刻，提出了可扩展且理论基础的路径，实现更有效的器官分配

Abstract: Each year, thousands of patients in need of heart transplants face life-threatening wait times due to organ scarcity. While allocation policies aim to maximize population-level outcomes, current approaches often fail to account for the dynamic arrival of organs and the composition of waitlisted candidates, thereby hampering efficiency. The United States is transitioning from rigid, rule-based allocation to more flexible data-driven models. In this paper, we propose a novel framework for non-myopic policy optimization in general online matching relying on potentials, a concept originally introduced for kidney exchange. We develop scalable and accurate ways of learning potentials that are higher-dimensional and more expressive than prior approaches. Our approach is a form of self-supervised imitation learning: the potentials are trained to mimic an omniscient algorithm that has perfect foresight. We focus on the application of heart transplant allocation and demonstrate, using real historical data, that our policies significantly outperform prior approaches -- including the current US status quo policy and the proposed continuous distribution framework -- in optimizing for population-level outcomes. Our analysis and methods come at a pivotal moment in US policy, as the current heart transplant allocation system is under review. We propose a scalable and theoretically grounded path toward more effective organ allocation.

</details>


### [341] [Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression](https://arxiv.org/abs/2602.08885)
*Paul Saegert,Ullrich Köthe*

Main category: cs.LG

TL;DR: 论文提出SimpliPy简化引擎，相比SymPy实现100倍加速，并构建Flash-ANSR框架，在符号回归任务中达到与PySR相当的性能但能发现更简洁的表达式。


<details>
  <summary>Details</summary>
Motivation: 当前摊销符号回归方法在处理复杂科学问题时面临效率瓶颈，主要障碍是缺乏快速将等价表达式简化为简洁规范形式的方法。现有方法依赖通用计算机代数系统如SymPy，但计算成本过高限制了训练和推理速度。

Method: 提出SimpliPy简化引擎，采用基于规则的简化方法；基于此构建Flash-ANSR框架，实现更高效的摊销符号回归，包括支持更大训练集、更有效地利用表达式token预算，以及系统性地去除训练集中与测试表达式等价的污染数据。

Result: SimpliPy相比SymPy实现100倍加速且质量相当；Flash-ANSR在FastSRB基准测试中显著优于NeSymReS和E2E等摊销基线方法；与直接优化方法PySR性能相当，但能在增加推理预算时恢复更简洁而非更复杂的表达式。

Conclusion: 通过开发高效的简化引擎SimpliPy，显著提升了摊销符号回归的效率和可扩展性，使其能够处理更复杂的科学问题，并在保持准确性的同时发现更简洁的数学表达式。

Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

</details>


### [342] [Discrete Bridges for Mutual Information Estimation](https://arxiv.org/abs/2602.08894)
*Iryna Zabarianska,Sergei Kholkin,Grigoriy Ksenofontov,Ivan Butakov,Alexander Korotin*

Main category: cs.LG

TL;DR: 论文提出了一种基于离散状态空间扩散桥模型的互信息估计器DBMI，专门用于处理离散数据的互信息估计问题


<details>
  <summary>Details</summary>
Motivation: 传统互信息估计器在处理离散数据时存在困难，而扩散桥模型在离散状态空间中的发展为解决这一问题提供了新的可能性

Method: 将互信息估计重新构建为领域转移问题，利用离散状态空间的桥匹配模型构建DBMI估计器

Result: DBMI估计器在低维和基于图像的两种互信息估计场景中都展示了良好的性能

Conclusion: 离散桥模型不仅可用于生成建模，还能有效解决离散数据的互信息估计问题，为机器学习和信息理论提供了新的工具

Abstract: Diffusion bridge models in both continuous and discrete state spaces have recently become powerful tools in the field of generative modeling. In this work, we leverage the discrete state space formulation of bridge matching models to address another important problem in machine learning and information theory: the estimation of the mutual information (MI) between discrete random variables. By neatly framing MI estimation as a domain transfer problem, we construct a Discrete Bridge Mutual Information (DBMI) estimator suitable for discrete data, which poses difficulties for conventional MI estimators. We showcase the performance of our estimator on two MI estimation settings: low-dimensional and image-based.

</details>


### [343] [GSS: Gated Subspace Steering for Selective Memorization Mitigation in LLMs](https://arxiv.org/abs/2602.08901)
*Xuanqi Zhang,Haoyang Shang,Xiaoxiao Li*

Main category: cs.LG

TL;DR: Gated Subspace Steering (GSS) 是一种选择性记忆缓解方法，通过探测-引导机制仅在检测到记忆相关激活时进行针对性修正，在保持模型性能的同时有效减少记忆化问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会记忆并逐字复制训练序列，这既损害泛化能力又威胁隐私安全。现有缓解方法通常均匀施加干预，会降低大多数正常泛化token的性能。研究发现记忆化是稀疏、间歇且token条件化的，需要上下文感知的干预而非静态参数修改。

Method: 提出Gated Subspace Steering (GSS)方法，将干预分解为两个部分：1) 探测（检测记忆相关激活），2) 引导（仅在探测超过阈值时应用针对性修正）。最优的探测-引导对基于最优子空间引导的优化框架产生。

Result: 在四个基准测试中，GSS达到或超越了最先进的记忆减少效果，同时比基于优化的替代方法需要少100-1000倍的计算量。该方法还提供了关于神经表示中记忆化几何结构的新理论见解。

Conclusion: GSS通过选择性、上下文感知的干预有效缓解LLM的记忆化问题，在保持模型性能的同时显著降低计算成本，为理解记忆化在神经表示中的几何特性提供了新视角。

Abstract: Large language models (LLMs) can memorize and reproduce training sequences verbatim -- a tendency that undermines both generalization and privacy. Existing mitigation methods apply interventions uniformly, degrading performance on the majority of tokens that generalize normally. We show empirically that memorization is sparse, intermittent, and token-conditioned, suggesting that effective mitigation requires context-aware intervention rather than static parameter modification. To this end, we propose a novel and effective selective memorization mitigation method -- Gated Subspace Steering (GSS), which decomposes intervention into a probe (detecting memorization-relevant activations) and a steer (applying targeted correction only when the probe exceeds a threshold). The optimal probe-steer pair emerges from a principled optimization framework based on optimal subspace steering. Experiments on four benchmarks show GSS matches or exceeds state-of-the-art memorization reduction while requiring $100-1000 \times$ less compute than optimization-based alternatives. Furthermore, we provide new theoretical insights into the geometry of memorization in neural representations.

</details>


### [344] [Positive Distribution Shift as a Framework for Understanding Tractable Learning](https://arxiv.org/abs/2602.08907)
*Marko Medvedev,Idan Attias,Elisabetta Cornacchia,Theodor Misiakiewicz,Gal Vardi,Nathan Srebro*

Main category: cs.LG

TL;DR: 论文提出"正分布偏移"概念，认为通过精心选择训练分布D'(x)而非目标分布D(x)，可以使学习变得更容易，这主要是计算上的优势而非统计上的。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为分布偏移（协变量偏移）对学习有负面影响，但作者认为通过选择合适的训练分布，分布偏移可以成为正面因素，使学习更容易。这种视角在现代机器学习中很重要，因为创新往往在于寻找好的训练分布而非改变训练算法。

Method: 形式化了正分布偏移的不同变体，展示了某些困难类别在正分布偏移下变得容易学习，并与成员查询学习建立了联系。

Result: 证明了正分布偏移可以使计算困难的问题变得可处理，即使使用标准的基于梯度的训练方法。

Conclusion: 正分布偏移是一种有价值的视角，它强调了训练分布选择的重要性，能够提供计算上的优势，使原本困难的学习问题变得可行。

Abstract: We study a setting where the goal is to learn a target function f(x) with respect to a target distribution D(x), but training is done on i.i.d. samples from a different training distribution D'(x), labeled by the true target f(x). Such a distribution shift (here in the form of covariate shift) is usually viewed negatively, as hurting or making learning harder, and the traditional distribution shift literature is mostly concerned with limiting or avoiding this negative effect. In contrast, we argue that with a well-chosen D'(x), the shift can be positive and make learning easier -- a perspective called Positive Distribution Shift (PDS). Such a perspective is central to contemporary machine learning, where much of the innovation is in finding good training distributions D'(x), rather than changing the training algorithm. We further argue that the benefit is often computational rather than statistical, and that PDS allows computationally hard problems to become tractable even using standard gradient-based training. We formalize different variants of PDS, show how certain hard classes are easily learnable under PDS, and make connections with membership query learning.

</details>


### [345] [GEMSS: A Variational Bayesian Method for Discovering Multiple Sparse Solutions in Classification and Regression Problems](https://arxiv.org/abs/2602.08913)
*Kateřina Henclová,Václav Šmídl*

Main category: cs.LG

TL;DR: GEMSS是一个变分贝叶斯框架，用于在n≪p和高相关性的欠定问题中发现多个不同的稀疏特征组合，通过结构化先验和多样性惩罚同时优化整个解集合。


<details>
  <summary>Details</summary>
Motivation: 在欠定(n≪p)和高相关性场景中，多个不同的稀疏特征子集可能同样好地解释响应变量。传统方法通常只提供一个解，掩盖了完整的可能解释谱，而识别这些替代解对于深入理解底层机制至关重要。

Method: GEMSS采用变分贝叶斯框架，使用结构化spike-and-slab先验实现稀疏性，用高斯混合近似难以处理的多峰后验分布，并通过Jaccard距离惩罚控制解多样性。与顺序贪婪方法不同，它通过随机梯度下降在单个目标函数中同时优化整个解集合。

Result: 在包含128个合成实验的基准测试中，GEMSS能有效扩展到高维设置(p=5000)，样本量小至n=50，能无缝推广到连续目标，原生处理缺失数据，并对类别不平衡和高斯噪声表现出显著鲁棒性。

Conclusion: GEMSS提供了一个强大的框架，用于在欠定和高相关场景中同时发现多个不同的稀疏特征组合，已实现为Python包'gemss'，包含适合非编码者使用的应用程序。

Abstract: Selecting interpretable feature sets in underdetermined ($n \ll p$) and highly correlated regimes constitutes a fundamental challenge in data science, particularly when analyzing physical measurements. In such settings, multiple distinct sparse subsets may explain the response equally well. Identifying these alternatives is crucial for generating domain-specific insights into the underlying mechanisms, yet conventional methods typically isolate a single solution, obscuring the full spectrum of plausible explanations.
  We present GEMSS (Gaussian Ensemble for Multiple Sparse Solutions), a variational Bayesian framework specifically designed to simultaneously discover multiple, diverse sparse feature combinations. The method employs a structured spike-and-slab prior for sparsity, a mixture of Gaussians to approximate the intractable multimodal posterior, and a Jaccard-based penalty to further control solution diversity. Unlike sequential greedy approaches, GEMSS optimizes the entire ensemble of solutions within a single objective function via stochastic gradient descent.
  The method is validated on a comprehensive benchmark comprising 128 synthetic experiments across classification and regression tasks. Results demonstrate that GEMSS scales effectively to high-dimensional settings ($p=5000$) with sample size as small as $n = 50$, generalizes seamlessly to continuous targets, handles missing data natively, and exhibits remarkable robustness to class imbalance and Gaussian noise.
  GEMSS is available as a Python package 'gemss' at PyPI. The full GitHub repository at https://github.com/kat-er-ina/gemss/ also includes a free, easy-to-use application suitable for non-coders.

</details>


### [346] [Diffusion-Inspired Reconfiguration of Transformers for Uncertainty Calibration](https://arxiv.org/abs/2602.08920)
*Manh Cuong Dao,Quang Hung Pham,Phi Le Nguyen,Thao Nguyen Truong,Bryan Kian Hsiang Low,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 提出一种基于扩散过程的Transformer不确定性校准方法，将特征变换块建模为概率映射，实现表示不确定性的传播


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在风险敏感应用中需要可靠的不确定性校准，但现有模型缺乏通过特征变换堆栈进行不确定性传播的机制

Method: 提出扩散启发的Transformer重构方法，将每个特征变换块建模为概率映射，组合这些映射形成类似扩散过程的概率路径，然后重新编译到具有统一转移模型的扩散过程中

Result: 在多种视觉和语言基准测试中，该方法相比现有不确定性感知Transformer实现了更优的校准和预测准确性

Conclusion: 该方法能够在保持原始预测性能的同时，实现预训练模型架构中表示不确定性的原则性传播

Abstract: Uncertainty calibration in pre-trained transformers is critical for their reliable deployment in risk-sensitive applications. Yet, most existing pre-trained transformers do not have a principled mechanism for uncertainty propagation through their feature transformation stack. In this work, we propose a diffusion-inspired reconfiguration of transformers in which each feature transformation block is modeled as a probabilistic mapping. Composing these probabilistic mappings reveals a probability path that mimics the structure of a diffusion process, transporting data mass from the input distribution to the pre-trained feature distribution. This probability path can then be recompiled on a diffusion process with a unified transition model to enable principled propagation of representation uncertainty throughout the pre-trained model's architecture while maintaining its original predictive performance. Empirical results across a variety of vision and language benchmarks demonstrate that our method achieves superior calibration and predictive accuracy compared to existing uncertainty-aware transformers.

</details>


### [347] [StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors](https://arxiv.org/abs/2602.08934)
*Suraj Ranganath,Atharv Ramesh*

Main category: cs.LG

TL;DR: StealthRL是一个基于强化学习的对抗性改述框架，用于测试AI文本检测器的鲁棒性，能在保持语义的同时实现近乎零检测率，揭示当前检测器的显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: AI文本检测器面临对抗性改述攻击的关键鲁棒性挑战，攻击者可以在保持语义的同时逃避检测。现有检测器在现实对抗条件下的鲁棒性需要系统性的压力测试。

Method: 提出StealthRL强化学习框架，使用Group Relative Policy Optimization (GRPO)和LoRA适配器在Qwen3-4B模型上训练改述策略，针对多检测器集成优化复合奖励函数，平衡检测逃避和语义保持。

Result: 在1%假阳性率的安全相关操作点上，StealthRL实现近乎零检测（0.001平均TPR@1%FPR），将平均AUROC从0.74降至0.27，达到99.9%攻击成功率。攻击还能迁移到训练中未见过的检测器家族，揭示共享架构漏洞。

Conclusion: 当前AI文本检测器存在显著的鲁棒性缺陷，StealthRL为对抗性评估提供了原则性协议，代码和评估流程已公开。

Abstract: AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.

</details>


### [348] [Distributionally Robust Optimization via Generative Ambiguity Modeling](https://arxiv.org/abs/2602.08976)
*Jiaqi Wen,Jianyi Yang*

Main category: cs.LG

TL;DR: 提出基于生成模型的分布鲁棒优化(GAS-DRO)，使用生成模型构建模糊集，提升机器学习任务的分布外泛化性能


<details>
  <summary>Details</summary>
Motivation: 传统分布鲁棒优化(DRO)的模糊集需要在保持与名义分布一致性的同时，足够多样化以覆盖各种潜在场景，且需要保证求解的易处理性。现有方法在捕捉名义支撑空间之外的对抗分布方面存在局限。

Method: 提出生成模型模糊集，捕捉名义支撑空间之外的各种对抗分布；基于此构建GAS-DRO算法，通过求解参数化生成模型空间的内层最大化问题实现易处理的DRO求解；使用扩散模型实现GAS-DRO。

Result: 理论上建立了GAS-DRO的平稳收敛性能；实证研究表明，在机器学习任务中，GAS-DRO在分布外泛化性能方面表现优异。

Conclusion: GAS-DRO通过生成模型构建模糊集，有效提升了分布鲁棒优化的泛化能力，为增强机器学习模型的鲁棒性提供了新方法。

Abstract: This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tractable DRO solutions. To this end, we propose generative model-based ambiguity sets that capture various adversarial distributions beyond the nominal support space while maintaining consistency with the nominal distribution. Building on this generative ambiguity modeling, we propose DRO with Generative Ambiguity Set (GAS-DRO), a tractable DRO algorithm that solves the inner maximization over the parameterized generative model space. We formally establish the stationary convergence performance of GAS-DRO. We implement GAS-DRO with a diffusion model and empirically demonstrate its superior Out-of-Distribution (OOD) generalization performance in ML tasks.

</details>


### [349] [StretchTime: Adaptive Time Series Forecasting via Symplectic Attention](https://arxiv.org/abs/2602.08983)
*Yubin Kim,Viresh Pati,Jevon Twitty,Vinh Pham,Shihao Yang,Jiecheng Lu*

Main category: cs.LG

TL;DR: 该论文提出Symplectic Positional Embeddings (SyPE)来解决传统Transformer在时间序列预测中无法处理"时间扭曲"动态的问题，通过将旋转群SO(2)扩展到辛群Sp(2,R)，实现了对非仿射时间扭曲的建模。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统（如金融周期、生物节律）经常表现出"时间扭曲"动态，即有效时间流与采样索引解耦。传统的Transformer位置编码（如RoPE）基于均匀、索引的时间进展假设，无法表示非仿射时间扭曲。

Method: 提出Symplectic Positional Embeddings (SyPE)，从哈密顿力学推导的可学习编码框架。SyPE通过将旋转群SO(2)扩展到辛群Sp(2,R)，严格推广了RoPE。引入新颖的输入依赖自适应扭曲模块，允许注意力机制端到端地自适应扩张或收缩时间坐标。

Result: 在StretchTime架构中实现该机制，在标准基准测试中达到最先进的性能，在表现出非平稳时间动态的数据集上展现出卓越的鲁棒性。

Conclusion: SyPE框架能够捕获局部变化的周期性，无需预定义扭曲函数，为处理现实世界时间序列中的时间扭曲动态提供了有效的解决方案。

Abstract: Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit "time-warped" dynamics where the effective flow of time decouples from the sampling index. In this work, we first formalize this misalignment and prove that rotary position embedding (RoPE) is mathematically incapable of representing non-affine temporal warping. To address this, we propose Symplectic Positional Embeddings (SyPE), a learnable encoding framework derived from Hamiltonian mechanics. SyPE strictly generalizes RoPE by extending the rotation group $\mathrm{SO}(2)$ to the symplectic group $\mathrm{Sp}(2,\mathbb{R})$, modulated by a novel input-dependent adaptive warp module. By allowing the attention mechanism to adaptively dilate or contract temporal coordinates end-to-end, our approach captures locally varying periodicities without requiring pre-defined warping functions. We implement this mechanism in StretchTime, a multivariate forecasting architecture that achieves state-of-the-art performance on standard benchmarks, demonstrating superior robustness on datasets exhibiting non-stationary temporal dynamics.

</details>


### [350] [DirMoE: Dirichlet-routed Mixture of Experts](https://arxiv.org/abs/2602.09001)
*Amirhossein Vahidi,Hesam Asadollahzadeh,Navid Akhavan Attar,Marie Moullet,Kevin Ly,Xingyi Yang,Mohammad Lotfollahi*

Main category: cs.LG

TL;DR: 提出Dirichlet-Routed MoE (DirMoE)，一种基于Dirichlet变分自编码器的端到端可微分路由机制，将专家选择和贡献分配解耦，通过变分ELBO训练，实现精确的稀疏性控制和专家专业化。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型通常依赖不可微分的Top-k+Softmax路由机制，限制了性能和可扩展性。标准Top-k+Softmax将两个不同的决策（激活哪些专家、如何在专家间分配贡献）混为一谈，需要更灵活、可微分的路由方案。

Method: 提出DirMoE，基于Dirichlet变分自编码器框架：1) 专家选择通过Bernoulli组件建模；2) 专家贡献分配通过Dirichlet组件处理；3) 使用Gumbel-Sigmoid松弛实现专家选择可微；4) 使用隐式重参数化处理Dirichlet分布；5) 变分ELBO训练目标包含直接稀疏性惩罚；6) 超参数调度引导模型从探索性路由到确定性路由。

Result: DirMoE路由机制在匹配或超越其他方法的同时，提高了专家专业化程度。整个前向传播过程保持完全可微分，能够精确控制期望的活跃专家数量。

Conclusion: DirMoE通过将路由问题解耦为专家选择和贡献分配两个组件，提供了一种端到端可微分的路由机制，解决了现有Top-k+Softmax路由的局限性，实现了更好的性能和专家专业化。

Abstract: Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.

</details>


### [351] [ARO: A New Lens On Matrix Optimization For Large Models](https://arxiv.org/abs/2602.09006)
*Wenbo Gong,Javier Zazo,Qijun Luo,Puqian Wang,James Hensman,Chao Ma*

Main category: cs.LG

TL;DR: ARO是一种新的矩阵优化框架，通过梯度旋转作为核心设计原则，在旋转坐标系中执行归一化最速下降，超越了现有的正交化/白化方法，显著提升了LLM训练效率。


<details>
  <summary>Details</summary>
Motivation: 虽然基于正交化/白化的矩阵优化器在提升LLM训练效率方面取得了显著进展，但研究者希望探索超越正交化的新范式，进一步推动效率边界。

Method: ARO框架将梯度旋转作为首要设计原则，在旋转坐标系中执行归一化最速下降，旋转由新颖的范数感知策略确定。该方法超越了现有的正交化和白化优化器，并提出了严格控制的基准测试协议以减少混淆和偏差。

Result: 在严格控制的基准测试下，ARO在LLM预训练中始终优于AdamW（1.3-1.35倍）和正交化方法（1.1-1.15倍），参数规模达80亿激活参数，过训练预算达8倍，且未见收益递减迹象。

Conclusion: ARO作为一种基于旋转对称性的对称感知优化器，为利用跨层/跨模块耦合的计算高效设计提供了新思路，推动了超越传统正交化方法的矩阵优化范式发展。

Abstract: Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \textbf{Adaptively Rotated Optimization (ARO}, a new matrix optimization framework that treats gradient rotation as a first class design principle. ARO accelerates LLM training by performing normed steepest descent in a rotated coordinate system, where the rotation is determined by a novel norm-informed policy. This perspective yields update rules that go beyond existing orthogonalization and whitening optimizers, improving sample efficiency in practice. To make comparisons reliable, we propose a rigorously controlled benchmarking protocol that reduces confounding and bias. Under this protocol, ARO consistently outperforms AdamW (by 1.3 $\sim$1.35$\times$) and orthogonalization methods (by 1.1$\sim$1.15$\times$) in LLM pretraining at up to 8B activated parameters, and up to $8\times$ overtrain budget, without evidence of diminishing returns. Finally, we discuss how ARO can be reformulated as a symmetry-aware optimizer grounded in rotational symmetries of residual streams, motivating advanced designs that enable computationally efficient exploitation of cross-layer/cross module couplings.

</details>


### [352] [ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification](https://arxiv.org/abs/2602.09008)
*Sijia Peng,Yun Xiong,Xi Chen,Yi Xie,Guanzhi Li,Yanwei Yu,Yangyong Zhu,Zhiqiang Shen*

Main category: cs.LG

TL;DR: ShapeCond是一种新颖的时间序列数据集压缩框架，通过shapelet引导的优化策略，利用shapelet知识进行高效压缩，在保持关键局部模式的同时大幅提升合成速度。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据快速增长给存储和计算带来压力，现有数据集压缩方法多为图像中心化设计，无法有效处理时间序列特有的时间结构和局部判别模式（如shapelet）。

Method: 提出ShapeCond框架，采用shapelet引导的优化策略进行时间序列分类数据集压缩，shapelet辅助的合成成本与序列长度无关，能显著提升合成效率。

Result: 相比现有最优方法CondTSC，ShapeCond在合成速度上快29倍，在Sleep数据集（3000时间步）上比朴素使用shapelet快10000倍，并在下游分类准确率上持续优于所有现有方法。

Conclusion: ShapeCond通过显式保留关键局部模式，提供了一种高效的时间序列数据集压缩解决方案，在速度和准确性方面均优于现有方法。

Abstract: Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [353] [What is Safety? Corporate Discourse, Power, and the Politics of Generative AI Safety](https://arxiv.org/abs/2602.06981)
*Ankolika De,Gabriel Lima,Yixin Zou*

Main category: cs.CY

TL;DR: 该研究通过批判性话语分析，揭示了领先的生成式AI公司如何通过公开文件构建和传达"安全"概念，这些话语策略巩固了企业合法性，将安全规范化为实验性和预期性实践，并推动参与式议程。


<details>
  <summary>Details</summary>
Motivation: 研究动机是批判性地审视生成式AI公司如何通过公开的安全声明构建话语，揭示这些话语如何建立权威、责任和合法性，以及这种话语构建可能如何影响技术治理和设计的替代方案。

Method: 采用批判性话语分析方法，分析企业安全相关声明的语料库，阐释权威、责任和合法性如何通过话语策略被建立。

Result: 研究发现企业的话语策略：1)巩固企业行为者的合法性；2)将安全规范化为实验性和预期性实践；3)推动感知上的参与式议程。这些话语可能导致企业优先事项的复制，并限制治理和设计的替代方法。

Conclusion: 研究贡献包括：1)将安全定位为需要批判性审视的社会技术话语；2)警告人机交互学者不要合法化企业框架，而应强调问责、公平和正义。通过将安全话语作为权力产物进行审视，为人机交互领域的AI研究推进了批判性议程。

Abstract: This work examines how leading generative artificial intelligence companies construct and communicate the concept of "safety" through public-facing documents. Drawing on critical discourse analysis, we analyze a corpus of corporate safety-related statements to explicate how authority, responsibility, and legitimacy are discursively established. These discursive strategies consolidate legitimacy for corporate actors, normalize safety as an experimental and anticipatory practice, and push a perceived participatory agenda toward safe technologies. We argue that uncritical uptake of these discourses risks reproducing corporate priorities and constraining alternative approaches to governance and design. The contribution of this work is twofold: first, to situate safety as a sociotechnical discourse that warrants critical examination; second, to caution human-computer interaction scholars against legitimizing corporate framings, instead foregrounding accountability, equity, and justice. By interrogating safety discourses as artifacts of power, this paper advances a critical agenda for human-computer interaction scholarship on artificial intelligence.

</details>


### [354] [Empowering Affected Individuals to Shape AI Fairness Assessments: Processes, Criteria, and Tools](https://arxiv.org/abs/2602.06984)
*Lin Luo,Satwik Ghanta,Yuri Nakao,Mathieu Chollet,Simone Stumpf*

Main category: cs.CY

TL;DR: 研究通过用户实验探索个人如何构建自己的AI公平性标准，发现人们通过模型特征的具体化形成多样化的公平观念，为更包容的AI公平评估提供设计启示。


<details>
  <summary>Details</summary>
Motivation: 当前AI公平性评估主要由专家使用预定义属性和指标进行，无法充分反映受AI决策影响的个体对公平性的多元理解和细微差别。需要让受影响个体参与公平性评估，但缺乏关于他们如何创建自己的公平标准以及产生何种标准的实证证据。

Method: 采用定性用户研究方法，在信用评级场景下与18名参与者进行实验。参与者首先用自己的语言表达公平观念，然后通过设计的交互原型将这些观念转化为具体的量化、可操作的公平标准。

Result: 研究提供了实证证据，展示了人们如何通过模型特征的具体化形成公平观念，并发现了个人自定义的结果公平性和程序公平性标准的多样性。

Conclusion: 研究结果为支持更包容、价值敏感的AI公平性评估流程和工具设计提供了启示，强调需要让受影响个体参与公平标准的制定过程。

Abstract: AI systems are increasingly used in high-stakes domains such as credit rating, where fairness concerns are critical. Existing fairness assessments are typically conducted by AI experts or regulators using predefined protected attributes and metrics, which often fail to capture the diversity and nuance of fairness notions held by the individuals who are affected by these systems' decisions, such as decision subjects. Recent work has therefore called for involving affected individuals in fairness assessment, yet little empirical evidence exists on how they create their own fairness criteria or what kinds of criteria they produce - knowledge that could not only inform experts' fairness evaluation and mitigation, but also guide the design of AI assessment tools. We address this gap through a qualitative user study with 18 participants in a credit rating scenario. Participants first articulated their fairness notions in their own words. Then, participants turned them into concrete quantified and operationalized fairness criteria, through an interactive prototype we designed. Our findings provide empirical evidence of the process through which people's fairness notions emerge via grounding in model features, and uncover a diverse set of individuals' custom-defined criteria for both outcome and procedural fairness. We provide design implications for processes and tools that support more inclusive and value-sensitive AI fairness assessment.

</details>


### [355] [A New Mode of Teaching Chinese as a Foreign Language from the Perspective of Smart System Studied by Using Rongzhixue](https://arxiv.org/abs/2602.06992)
*Xiaohui Zou,Lijun Ke,Shunpeng Zou*

Main category: cs.CY

TL;DR: 该研究从融智学视角提出汉语作为外语教学的新模型，强调解释先于翻译的蝴蝶模型和双语思维训练新方法，整合语言科学和教育科学的前沿成果，应对ChatGPT等AI技术对传统语言教育观念的颠覆性挑战。


<details>
  <summary>Details</summary>
Motivation: 面对ChatGPT等人工智能技术对人类学习能力和创造力的挑战，传统的语言知识教育观念、汉语教育观念以及对外汉语教学观念已经落后，需要进行颠覆性创新。研究旨在从融智学视角寻求适应变革的新路径。

Method: 提出融智学视角下的对外汉语教学新模型，采用解释先于翻译的蝴蝶模型，突出双语思维训练新方法，一方面应用汉字新理论、语言与言语关系理论等语言科学前沿成果，另一方面应用AI赋能教学、教育科学前沿成果。

Result: 该模型不仅突破了传统的语言观、教育观和对外汉语教学观，还突破了传统的人机交互观念，明确提出了语言、知识、教育教学等一系列跨界融智学的新方法和新课题。

Conclusion: 研究通过一系列创新尝试，为应对AI时代对语言教育的挑战提供了新的理论框架和实践路径，希望为学术界同行、教师和学生提供有益参考，推动对外汉语教学的创新发展。

Abstract: The purpose of this study is to introduce a new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its characteristics are as follows: focusing on the butterfly model of interpretation before translation, highlighting the new method of bilingual thinking training, on the one hand, applying the new theory of Chinese characters, the theory of the relationship between language and speech, and the forward-looking research results of language science; On the other hand, the application of the new model of teaching Chinese as a foreign language, AI empowering teaching and learning, and the forward-looking research results of educational science fully reflect a series of characteristics of the new model of teaching Chinese as a foreign language from the perspective of integrating wisdom. Its beneficial effects are: not only the old view of language and education, especially the old view of teaching Chinese as a foreign language, but also the old view of human-computer interaction. Its significance lies in that a series of great cross-border Rongzhixue such as language, knowledge, education and teaching, as well as new methods and new topics of bilingual thinking training are clearly put forward from the perspective of integrating wisdom. Especially in the face of the challenge of Chat GPT to human learning ability and even creativity, the existing concepts of language knowledge education and teaching are already very backward. The old concepts of Chinese language education, and teaching Chinese as a foreign language are all facing a series of subversive innovation challenges. How to seek changes in adaptation? This study has made a series of innovative attempts, hoping to benefit academic colleagues, teachers and students.

</details>


### [356] [Tokenizations for Austronesian Language Models: study on languages in Indonesia Archipelago](https://arxiv.org/abs/2602.06998)
*Andhika Bernard Lumbantobing,Hokky Situngkir*

Main category: cs.CY

TL;DR: 该研究开发了基于音节的标记化框架，采用印尼传统文字原则，用于印尼地区语言。相比GPT-2的子词标记化，该方法在跨语言标记序列相似性上平均提高约21%，更有效地保留了南岛语系语言的语音和形态模式。


<details>
  <summary>Details</summary>
Motivation: 基于英语主导语料库优化的子词标记化方法可能产生与南岛语系语言结构不匹配的标记碎片化问题。需要开发更适合印尼地区语言的标记化方法。

Method: 构建基于阿布吉达文字系统逻辑的音节分割程序，从印尼词典(KBBI)提取2,843个标记构建词汇表。在包含10种地区语言、印尼语和英语的NusaX数据集上进行评估，使用每字符标记比(TPC)和Smith-Waterman算法进行序列对齐分析。

Result: 音节标记化在所有地区语言中产生一致的TPC值，而GPT-2对英语的TPC最低。音节标记化始终产生更高的标记序列相似性分数，相比GPT-2平均提高约21%。

Conclusion: 基于音节的方法更有效地保留了相关南岛语系语言的语音和形态模式，为多语言大语言模型开发提供了语言学原则基础。

Abstract: Tokenization constitutes a fundamental stage in Large Language Model (LLM) processing; however, subword-based tokenization methods optimized on English-dominant corpora may produce token fragmentation misaligned with the linguistic structures of Austronesian languages. This study aimed to develop a syllable-based tokenization framework adopting principles from traditional Indonesian scripts (aksara) for regional languages of Indonesia. A syllabic segmentation procedure was constructed based on the logic of abugida writing systems and implemented with a vocabulary of 2,843 tokens extracted from the Indonesian dictionary (KBBI). Evaluation was conducted on the NusaX dataset comprising 1,000 parallel translation samples across 10 regional languages, Indonesian, and English. Analysis employed Token per Character (TPC) ratio and sequence alignment using the Smith-Waterman algorithm. Results demonstrated that syllable-based tokenization yielded consistent TPC values across all regional languages, whereas GPT-2 exhibited an inverse pattern with the lowest TPC for English. Syllable-based tokenization consistently produced higher token sequence similarity scores, with an average increase of approximately 21% compared to GPT-2. These findings confirm that the syllable-based approach more effectively preserves phonological and morphological patterns across related Austronesian languages, offering a linguistically principled foundation for multilingual LLM development.

</details>


### [357] [When Excellence Stops Producing Knowledge: A Practitioner's Observation on Research Funding](https://arxiv.org/abs/2602.07039)
*Heimo Müller*

Main category: cs.CY

TL;DR: 本文揭示了研究资助体系的结构性悖论：参与者认识到当前体系接近功能极限，但改革措施反而加剧了根本问题。研究发现"卓越"已与知识生产脱钩，而与评估中的"可呈现性"紧密耦合。


<details>
  <summary>Details</summary>
Motivation: 作者基于近四十年的研究资助参与经验（申请人、协调人、评估人、评审组成员），观察到当前竞争性研究资助体系存在结构性矛盾。虽然许多参与者认识到系统已接近功能极限，但改革措施往往加剧而非缓解根本问题。作者希望通过内部视角揭示这一普遍感受但鲜少被明确表述的模式。

Method: 采用质性分析方法，基于作者作为内部参与者的经验观察。重点考察两个领域：竞争性基础研究资助和大型欧盟联盟项目。分析三个加速趋势：提案写作的专业化（通过专业顾问）、AI辅助申请工具的兴起，以及评审人员短缺导致评审小组依赖与具体研究领域日益疏远的评审人。

Result: 研究发现"卓越"概念已与知识生产脱钩，转而与评估中的"可呈现性"紧密耦合。这导致了Goodhart定律的体现：当衡量标准成为目标时，它就不再是好的衡量标准。三个趋势相互强化，形成了自我维持的系统，其中形式表现取代了实质内容。

Conclusion: 当前研究资助体系存在结构性悖论，改革措施往往加剧而非解决问题。通过明确命名这一普遍感受但鲜少被表述的模式，作者希望为更建设性的方向提供基础。需要重新思考如何将"卓越"与真正的知识生产重新连接，而不是仅仅优化其在评估中的表现。

Abstract: After almost four decades of participating in competitive research funding -- as applicant, coordinator, evaluator, and panel member -- I have come to see a structural paradox: many participants recognize that the current system is approaching its functional limits, yet most reform measures intensify rather than alleviate the underlying dynamics. This paper documents how excellence has become decoupled from knowledge production through an increasing coupling to representability under evaluation. The discussion focuses on two domains in which this is particularly visible: competitive basic research funding and large EU consortium projects. Three accelerating trends are examined: the professionalization of proposal writing through specialized consultants, the rise of AI-assisted applications, and an evaluator shortage that forces panels to rely on reviewers increasingly distant from the actual research domains. These observations are offered not as external critique but as an insider account, in the hope that naming a widely experienced but rarely articulated pattern may enable more constructive orientation.
  Keywords: Research funding, Excellence, Evaluation, Goodhart's Law, Professionalization, AI-assisted proposals, Peer review crisis

</details>


### [358] [Structural transparency of societal AI alignment through Institutional Logics](https://arxiv.org/abs/2602.08246)
*Atrisha Sarkar,Isam Faik*

Main category: cs.CY

TL;DR: 论文提出了"结构透明度"框架，用于分析AI对齐中的组织和制度决策，弥补现有透明度框架只关注信息层面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐领域关注价值观如何融入生成式AI系统设计及其社会后果，但现有透明度框架主要关注AI模型、数据和程序的信息层面，而影响对齐决策及其下游效应的组织和制度因素在研究和实践中都未得到充分考察。

Method: 基于制度逻辑理论视角，开发了结构透明度框架，对AI对齐治理中的组织决策进行分类，并提供明确的分析方法。通过五个分析组件（每个都配有"分析师配方"）来识别主要制度逻辑及其内部关系、现有社会秩序的外部干扰，以及每个制度逻辑的结构风险如何映射到社会技术危害目录。

Result: 提出了结构透明度概念，使分析师能够补充现有的基于信息透明度的分析方法，通过宏观层面分析捕捉AI对齐决策的制度动态和后果。

Conclusion: 结构透明度框架为分析AI对齐中的组织和制度决策提供了系统方法，有助于更全面地理解AI系统价值观整合的社会影响和治理挑战。

Abstract: The field of AI alignment is increasingly concerned with the questions of how values are integrated into the design of generative AI systems and how their integration shapes the social consequences of AI. However, existing transparency frameworks focus on the informational aspects of AI models, data, and procedures, while the institutional and organizational forces that shape alignment decisions and their downstream effects remain underexamined in both research and practice. To address this gap, we develop a framework of \emph{structural transparency} for analyzing organizational and institutional decisions concerning AI alignment, drawing on the theoretical lens of Institutional Logics. We develop a categorization of organizational decisions that are present in the governance of AI alignment, and provide an explicit analytical approach to examining them. We operationalize the framework through five analytical components, each with an accompanying "analyst recipe" that collectively identify the primary institutional logics and their internal relationships, external disruptions to existing social orders, and finally, how the structural risks of each institutional logic are mapped to a catalogue of sociotechnical harms. The proposed concept of structural transparency enables analysts to complement existing approached based on informational transparency with macro-level analyses that capture the institutional dynamics and consequences of decisions regarding AI alignment.

</details>


### [359] [Cyclic Adaptive Private Synthesis for Sharing Real-World Data in Education](https://arxiv.org/abs/2602.08299)
*Hibiki Ito,Chia-Yu Hsu,Hiroaki Ogata*

Main category: cs.CY

TL;DR: 提出CAPS框架解决教育真实世界数据隐私保护问题，通过循环自适应合成数据促进开放科学和基于设计的研究


<details>
  <summary>Details</summary>
Motivation: 教育领域数字技术快速应用产生了大量真实世界数据，但隐私问题限制了这些数据在学术研究中的二次使用。现有差分隐私合成数据方法主要针对大规模低维开放数据集，而教育数据通常高维且样本量小，同时教育实践具有迭代性，需要持续而非一次性数据共享

Method: 提出循环自适应隐私合成（CAPS）框架，通过迭代方式共享真实世界数据，支持持续的数据合成而非一次性合成，适应教育实践的迭代特性

Result: 在真实教育数据上的案例研究表明，CAPS框架优于传统一次性合成基线方法，同时揭示了需要进一步研究的挑战

Conclusion: 该研究为教育真实世界数据的隐私保护共享提供了重要第一步，扩展了学习分析中开放科学和基于设计研究的可能性，为持续、迭代的数据共享提供了有效解决方案

Abstract: The rapid adoption of digital technologies has greatly increased the volume of real-world data (RWD) in education. While these data offer significant opportunities for advancing learning analytics (LA), secondary use for research is constrained by privacy concerns. Differentially private synthetic data generation is regarded as the gold-standard approach to sharing sensitive data, yet studies on the private synthesis of educational data remain very scarce and rely predominantly on large, low-dimensional open datasets. Educational RWD, however, are typically high-dimensional and small in sample size, leaving the potential of private synthesis underexplored. Moreover, because educational practice is inherently iterative, data sharing is continual rather than one-off, making a traditional one-shot synthesis approach suboptimal. To address these challenges, we propose the Cyclic Adaptive Private Synthesis (CAPS) framework and evaluate it on authentic RWD. By iteratively sharing RWD, CAPS not only fosters open science, but also offers rich opportunities of design-based research (DBR), thereby amplifying the impact of LA. Our case study using actual RWD demonstrates that CAPS outperforms a one-shot baseline while highlighting challenges that warrant further investigation. Overall, this work offers a crucial first step towards privacy-preserving sharing of educational RWD and expands the possibilities for open science and DBR in LA.

</details>


### [360] [To Tango or to Disentangle? Making Ethnography Public in the Digital Age](https://arxiv.org/abs/2602.08349)
*Daniel Mwesigwa,Cyan DeVeaux,Palashi Vaghela*

Main category: cs.CY

TL;DR: 该论文探讨数字平台时代民族志研究面临的新机遇与挑战，通过VRChat和WhatsApp案例研究，提出"涌现关系性"作为分析民族志研究者、平台与公众相互塑造的关键概念。


<details>
  <summary>Details</summary>
Motivation: 数字平台的兴起改变了民族志研究的环境，研究者既面临在线与离线混合媒体环境的新机遇，也面临实践和伦理挑战，需要重新审视研究者作为局外人/局内人的双重角色。

Method: 采用案例研究方法，分析VRChat和WhatsApp两个数字平台上的民族志实践，探讨研究者如何运用多样策略研究种族和种姓等社会文化问题。

Result: 提出"涌现关系性"作为关键分析框架，用于理解民族志研究者、平台和公众之间的相互塑造关系，分析位置性和混合媒体环境如何构成和限制可访问、可表达和可公开的内容。

Conclusion: 数字平台时代民族志研究需要新的分析工具，"涌现关系性"为理解研究者、平台与公众的复杂互动提供了重要视角，有助于应对混合媒体环境带来的挑战。

Abstract: Ethnography attends to relations among people, practices, and the technologies that mediate them. Central to this method is the duality of roles ethnographers navigate as researchers and participants and as outsiders and insiders. However, the rise of digital platforms has introduced new opportunities as well as practical and ethical challenges that reshape these dualities across hybrid media environments spanning both online and offline contexts. Drawing on two case studies of VRChat and WhatsApp, we examine how ethnographers employ diverse tactics to study both enduring and emerging socio-cultural issues of race and caste, particularly those that form what are often called publics. We propose emergent relationality as a key analytic for understanding the mutual shaping of ethnographers, platforms, and publics. In this work, emergent relationality offers registers for analyzing how positionality and hybrid media environments constitute and condition what can be accessed, articulated, and made public.

</details>


### [361] [Empirically Understanding the Value of Prediction in Allocation](https://arxiv.org/abs/2602.08786)
*Unai Fischer-Abaigar,Emily Aiken,Christoph Kern,Juan Carlos Perdomo*

Main category: cs.CY

TL;DR: 开发了一个实证工具包，帮助决策者量化预测投资相对于扩大容量和提高治疗质量等其他政策杠杆的福利影响，并在德国就业服务和埃塞俄比亚贫困目标定位两个案例研究中应用。


<details>
  <summary>Details</summary>
Motivation: 机构越来越多地使用预测来分配稀缺资源，但从设计角度看，更好的预测需要与其他投资（如扩大容量或提高治疗质量）竞争。核心问题不是如何解决具体的分配问题，而是应该解决哪个问题。

Method: 开发了一个实证工具包（rvp软件工具包），帮助规划者形成原则性的答案，量化预测投资相对于其他政策杠杆（如扩大容量和改善治疗质量）的福利影响。在两个真实案例研究中应用该框架：德国就业服务和埃塞俄比亚贫困目标定位。

Result: 决策者能够可靠地得出关于预测在其特定分配问题中相对价值的情境特定结论。该工具包使决策者能够在不同政策杠杆之间做出明智的投资决策。

Conclusion: 开发了一个实用的实证框架和软件工具包，帮助决策者量化预测投资相对于其他干预措施的福利影响，使机构能够更明智地决定在预测、扩大容量或提高治疗质量等方面的资源分配。

Abstract: Institutions increasingly use prediction to allocate scarce resources. From a design perspective, better predictions compete with other investments, such as expanding capacity or improving treatment quality. Here, the big question is not how to solve a specific allocation problem, but rather which problem to solve. In this work, we develop an empirical toolkit to help planners form principled answers to this question and quantify the bottom-line welfare impact of investments in prediction versus other policy levers such as expanding capacity and improving treatment quality. Applying our framework in two real-world case studies on German employment services and poverty targeting in Ethiopia, we illustrate how decision-makers can reliably derive context-specific conclusions about the relative value of prediction in their allocation problem. We make our software toolkit, rvp, and parts of our data available in order to enable future empirical work in this area.

</details>


### [362] [Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs](https://arxiv.org/abs/2602.08997)
*Lavender Y. Jiang,Xujin Chris Liu,Kyunghyun Cho,Eric K. Oermann*

Main category: cs.CY

TL;DR: 本文指出HIPAA Safe Harbor去标识化方法在现代LLM时代已失效，因为LLM能从临床笔记的潜在关联中重新识别患者身份，即使移除所有显式标识符。


<details>
  <summary>Details</summary>
Motivation: 隐私是基本人权，临床笔记包含患者敏感信息，传统去标识化方法基于分类表格数据设计，无法应对现代LLM从相关性和准标识符中提取身份信息的能力。

Method: 首先使用因果图形式化身份与准标识符之间的相关性，然后通过从去标识化笔记中重新识别患者的实证验证，并通过诊断消融实验展示即使只保留诊断信息也能预测患者居住地。

Result: 实证验证显示LLM能从去标识化临床笔记中重新识别患者，诊断消融实验证明仅凭诊断信息就能预测患者居住地，揭示去标识化本质上的不完善性。

Conclusion: 传统去标识化方法在现代LLM时代已不足以保证患者隐私，需要社区共同行动来维护医患信任，本文旨在提高意识并讨论可行的改进建议。

Abstract: Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while ignoring the latent information found in correlations between identity and quasi-identifiers, which can be captured by modern LLMs. We first formalize these correlations using a causal graph, then validate it empirically through individual re-identification of patients from scrubbed notes. The paradox of de-identification is further shown through a diagnosis ablation: even when all other information is removed, the model can predict the patient's neighborhood based on diagnosis alone. This position paper raises the question of how we can act as a community to uphold patient-provider trust when de-identification is inherently imperfect. We aim to raise awareness and discuss actionable recommendations.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [363] [Reasoning-Augmented Representations for Multimodal Retrieval](https://arxiv.org/abs/2602.07125)
*Jianrui Zhang,Anirudh Sundara Rajan,Brandon Han,Soochahn Lee,Sukanta Ganguly,Yong Jae Lee*

Main category: cs.IR

TL;DR: UMR提出数据增强框架，通过外部化推理提升多模态检索性能，在M-BEIR基准上取得显著改进


<details>
  <summary>Details</summary>
Motivation: 现有多模态检索模型在处理需要潜在推理的查询时表现脆弱，如图像包含"沉默"证据或查询语义不明确时，单次嵌入需要同时完成推理和压缩，容易导致虚假特征匹配

Method: 提出数据中心化框架，在检索前外部化推理：1) 使用强视觉语言模型密集标注语料库中的视觉证据；2) 解析查询中的模糊多模态引用；3) 将冗长指令重写为简洁检索约束；4) 在增强的语义密集表示上训练检索器

Result: 在M-BEIR基准上，推理增强训练方法相比强基线获得一致提升，消融实验显示：语料增强主要受益于知识密集型查询，而查询增强对组合修改请求至关重要

Conclusion: 通过外部化推理并将检索器训练在语义密集表示上，可以有效解决多模态检索中的潜在推理问题，提升检索性能

Abstract: Universal Multimodal Retrieval (UMR) seeks any-to-any search across text and vision, yet modern embedding models remain brittle when queries require latent reasoning (e.g., resolving underspecified references or matching compositional constraints). We argue this brittleness is often data-induced: when images carry "silent" evidence and queries leave key semantics implicit, a single embedding pass must both reason and compress, encouraging spurious feature matching. We propose a data-centric framework that decouples these roles by externalizing reasoning before retrieval. Using a strong Vision--Language Model, we make implicit semantics explicit by densely captioning visual evidence in corpus entries, resolving ambiguous multimodal references in queries, and rewriting verbose instructions into concise retrieval constraints. Inference-time enhancement alone is insufficient; the retriever must be trained on these semantically dense representations to avoid distribution shift and fully exploit the added signal. Across M-BEIR, our reasoning-augmented training method yields consistent gains over strong baselines, with ablations showing that corpus enhancement chiefly benefits knowledge-intensive queries while query enhancement is critical for compositional modification requests. We publicly release our code at https://github.com/AugmentedRetrieval/ReasoningAugmentedRetrieval.

</details>


### [364] [Multimodal Enhancement of Sequential Recommendation](https://arxiv.org/abs/2602.07207)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.IR

TL;DR: MuSTRec是一个新颖的多模态序列推荐框架，通过构建基于文本和视觉特征的物品图来捕捉跨物品相似性和协同过滤信号，并在多个亚马逊数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的推荐系统通常单独处理多模态特征或序列信息，缺乏将两者统一起来的框架。MuSTRec旨在整合多模态和序列推荐范式，以更好地捕捉用户偏好。

Method: MuSTRec通过提取文本和视觉特征构建物品-物品图来捕捉跨物品相似性和协同过滤信号，并采用基于频率的自注意力模块来捕捉用户的短期和长期偏好。

Result: 在多个亚马逊数据集上，MuSTRec相比现有的多模态和序列推荐基准方法取得了显著提升（最高33.5%的改进），特别是在小数据集上整合用户嵌入后，短期指标提升高达200%。

Conclusion: MuSTRec成功统一了多模态和序列推荐范式，展示了这一新推荐范式的潜力，并提出了新的数据划分机制需求，为推荐系统研究提供了新方向。

Abstract: We propose a novel recommender framework, MuSTRec (Multimodal and Sequential Transformer-based Recommendation), that unifies multimodal and sequential recommendation paradigms. MuSTRec captures cross-item similarities and collaborative filtering signals, by building item-item graphs from extracted text and visual features. A frequency-based self-attention module additionally captures the short- and long-term user preferences. Across multiple Amazon datasets, MuSTRec demonstrates superior performance (up to 33.5% improvement) over multimodal and sequential state-of-the-art baselines. Finally, we detail some interesting facets of this new recommendation paradigm. These include the need for a new data partitioning regime, and a demonstration of how integrating user embeddings into sequential recommendation leads to drastically increased short-term metrics (up to 200% improvement) on smaller datasets. Our code is availabe at https://anonymous.4open.science/r/MuSTRec-D32B/ and will be made publicly available.

</details>


### [365] [Sequences as Nodes for Contrastive Multimodal Graph Recommendation](https://arxiv.org/abs/2602.07208)
*Bucher Sahyouni,Matthew Vowels,Liqun Chen,Simon Hadfield*

Main category: cs.IR

TL;DR: MuSICRec是一个多视图图推荐系统，结合协同过滤、序列和多模态信号，通过序列-项目视图和ID引导门控缓解冷启动和数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统虽然使用多模态、序列和对比学习技术来缓解冷启动和数据稀疏问题，但这些增强方法往往会引入噪声并破坏有用的语义信息。

Method: 构建序列-项目视图，通过注意力池化用户交互项目形成序列节点；在SI图上传播，获得第二视图作为人工数据增强的替代方案；使用ID引导门控调节文本和视觉特征的贡献，减轻模态噪声并对齐多模态信息。

Result: 在Amazon Baby、Sports和Electronics数据集上，使用严格的leave-two-out分割，MuSICRec在所有模型类型中都优于最先进的基线方法，对历史记录较短的用户的提升效果最明显。

Conclusion: MuSICRec通过多视图图结构和ID引导门控机制，有效缓解了推荐系统中的冷启动和数据稀疏问题，特别适合历史交互较少的用户。

Abstract: To tackle cold-start and data sparsity issues in recommender systems, numerous multimodal, sequential, and contrastive techniques have been proposed. While these augmentations can boost recommendation performance, they tend to add noise and disrupt useful semantics. To address this, we propose MuSICRec (Multimodal Sequence-Item Contrastive Recommender), a multi-view graph-based recommender that combines collaborative, sequential, and multimodal signals. We build a sequence-item (SI) view by attention pooling over the user's interacted items to form sequence nodes. We propagate over the SI graph, obtaining a second view organically as an alternative to artificial data augmentation, while simultaneously injecting sequential context signals. Additionally, to mitigate modality noise and align the multimodal information, the contribution of text and visual features is modulated according to an ID-guided gate.
  We evaluate under a strict leave-two-out split against a broad range of sequential, multimodal, and contrastive baselines. On the Amazon Baby, Sports, and Electronics datasets, MuSICRec outperforms state-of-the-art baselines across all model types. We observe the largest gains for short-history users, mitigating sparsity and cold-start challenges. Our code is available at https://anonymous.4open.science/r/MuSICRec-3CEE/ and will be made publicly available.

</details>


### [366] [Progressive Searching for Retrieval in RAG](https://arxiv.org/abs/2602.07297)
*Taehee Jeong,Xingzhe Zhao,Peizu Li,Markus Valvur,Weihua Zhao*

Main category: cs.IR

TL;DR: 提出了一种用于RAG系统的渐进式搜索算法，通过从低维嵌入逐步细化到高维的多阶段搜索，在保持准确性的同时降低检索时间


<details>
  <summary>Details</summary>
Motivation: RAG系统需要高效准确的搜索来获取相关信息，但现有方法在大型数据库中面临检索效率与准确性之间的权衡问题

Method: 提出渐进式搜索算法，通过层次化搜索逐步细化候选集：从低维嵌入开始，逐步过渡到目标高维空间，采用多阶段方法减少检索时间

Result: 渐进式搜索在RAG系统中实现了维度、速度和准确性之间的平衡，即使在大型数据库中也能实现可扩展的高性能检索

Conclusion: 渐进式搜索算法为RAG系统提供了一种成本效益高的检索解决方案，能够有效缓解LLMs的过时信息和幻觉问题

Abstract: Retrieval Augmented Generation (RAG) is a promising technique for mitigating two key limitations of large language models (LLMs): outdated information and hallucinations. RAG system stores documents as embedding vectors in a database. Given a query, search is executed to find the most related documents. Then, the topmost matching documents are inserted into LLMs' prompt to generate a response. Efficient and accurate searching is critical for RAG to get relevant information. We propose a cost-effective searching algorithm for retrieval process. Our progressive searching algorithm incrementally refines the candidate set through a hierarchy of searches, starting from low-dimensional embeddings and progressing into a higher, target-dimensionality. This multi-stage approach reduces retrieval time while preserving the desired accuracy. Our findings demonstrate that progressive search in RAG systems achieves a balance between dimensionality, speed, and accuracy, enabling scalable and high-performance retrieval even for large databases.

</details>


### [367] [Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation](https://arxiv.org/abs/2602.07298)
*Benyu Zhang,Qiang Zhang,Jianpeng Cheng,Hong-You Chen,Qifei Wang,Wei Sun,Shen Li,Jia Li,Jiahao Wu,Xiangjun Fan,Hong Yan*

Main category: cs.IR

TL;DR: 论文提出了一种分层框架生成高质量合成数据，首次展示了LLM在推荐系统中的幂律缩放规律，合成数据训练的模型在推荐任务中显著优于真实数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推荐系统中具有潜力，但缺乏可预测的缩放规律阻碍了其发展。作者认为这可能源于先前持续预训练中原始用户交互数据的噪声、偏见和不完整性。

Method: 引入了一种新颖的分层框架，通过为LLM创建精心策划的教学课程来生成高质量合成数据，避免了原始数据的问题。使用该框架生成的合成数据训练标准序列模型。

Result: 在合成数据上训练的模型在下游排序任务中显著优于真实数据训练的模型（SasRec在recall@100上提升130%）。首次实证展示了LLM在高质量推荐特定数据上的稳健幂律缩放，多个合成数据模态上观察到一致的困惑度降低。

Conclusion: 建立了一种可靠扩展推荐领域LLM能力的基础方法，将研究重点从缓解数据缺陷转向利用高质量结构化信息，为推荐系统中的LLM发展提供了可预测的缩放规律。

Abstract: Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information.

</details>


### [368] [Semantic Search At LinkedIn](https://arxiv.org/abs/2602.07309)
*Fedor Borisyuk,Sriram Vasudevan,Muchen Wu,Guoyao Li,Benjamin Le,Shaobo Zhang,Qianqi Kay Shen,Yuchin Juan,Kayhan Behdin,Liming Dong,Kaixu Yang,Shusen Jing,Ravi Pothamsetty,Rajat Arora,Sophie Yanying Sheng,Vitaly Abdrashitov,Yang Zhao,Lin Su,Xiaoqing Wang,Chujie Zheng,Sarang Metkar,Rupesh Gupta,Igor Lapchuk,David N. Racca,Madhumitha Mohan,Yanbo Li,Haojun Li,Saloni Gandhi,Xueying Lu,Chetan Bhole,Ali Hooshmand,Xin Yang,Raghavan Muthuregunathan,Jiajun Zhang,Mathew Teoh,Adam Coler,Abhinav Gupta,Xiaojing Ma,Sundara Raman Ramachandran,Morteza Ramezani,Yubo Wang,Lijuan Zhang,Richard Li,Jian Sheng,Chanh Nguyen,Yen-Chi Chen,Chuanrui Zhu,Claire Zhang,Jiahao Xu,Deepti Kulkarni,Qing Lan,Arvind Subramaniam,Ata Fatahibaarzi,Steven Shimizu,Yanning Chen,Zhipeng Wang,Ran He,Zhengze Zhou,Qingquan Song,Yun Dai,Caleb Johnson,Ping Liu,Shaghayegh Gharghabi,Gokulraj Mohanasundaram,Juan Bottaro,Santhosh Sachindran,Qi Guo,Yunxiang Ren,Chengming Jiang,Di Mo,Luke Simon,Jianqiang Shen,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: LinkedIn开发了一个基于LLM的语义搜索框架，通过多教师蒸馏训练紧凑小模型，结合推理架构优化，在保持高质量的同时实现75倍以上的排名吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 虽然基于大语言模型的语义搜索能够实现基于意义的检索，但扩展这种技术需要显著的推理效率提升。LinkedIn需要为其AI职位搜索和AI人才搜索功能开发高效的语义搜索系统。

Method: 1. 结合LLM相关性判断器和基于嵌入的检索；2. 通过多教师蒸馏训练紧凑的小语言模型，联合优化相关性和参与度；3. 采用预填充导向的推理架构，结合模型剪枝、上下文压缩和文本-嵌入混合交互。

Result: 在固定延迟约束下，排名吞吐量提升了75倍以上，同时保持了接近教师模型水平的NDCG质量。这是首批在生产环境中部署的LLM排名系统之一，效率可与传统方法相媲美，在质量和用户参与度方面带来显著提升。

Conclusion: 通过创新的模型压缩和推理架构设计，LinkedIn成功实现了高效的LLM语义搜索系统，证明了在保持高质量的同时大幅提升效率的可行性，为生产环境中的LLM应用提供了实用解决方案。

Abstract: Semantic search with large language models (LLMs) enables retrieval by meaning rather than keyword overlap, but scaling it requires major inference efficiency advances. We present LinkedIn's LLM-based semantic search framework for AI Job Search and AI People Search, combining an LLM relevance judge, embedding-based retrieval, and a compact Small Language Model trained via multi-teacher distillation to jointly optimize relevance and engagement. A prefill-oriented inference architecture co-designed with model pruning, context compression, and text-embedding hybrid interactions boosts ranking throughput by over 75x under a fixed latency constraint while preserving near-teacher-level NDCG, enabling one of the first production LLM-based ranking systems with efficiency comparable to traditional approaches and delivering significant gains in quality and user engagement.

</details>


### [369] [MDL: A Unified Multi-Distribution Learner in Large-scale Industrial Recommendation through Tokenization](https://arxiv.org/abs/2602.07520)
*Shanlei Mu,Yuchen Jiang,Shikang Wu,Shiyong Hong,Tianmu Sha,Junjie Zhang,Jie Zhu,Zhe Chen,Zhe Wang,Jingjian Lin*

Main category: cs.IR

TL;DR: 提出统一的多分布学习框架MDL，将场景和任务信息作为特殊令牌处理，通过三层协同机制实现深度交互，显著提升工业推荐系统性能


<details>
  <summary>Details</summary>
Motivation: 现有多场景学习和多任务学习方法存在两大问题：1) 大规模模型参数利用率不足，与复杂特征模块交互有限；2) 难以在统一框架中联合建模场景和任务信息

Method: 提出MDL框架，受大语言模型"提示"范式启发，将场景和任务信息作为特殊令牌而非辅助输入或门控信号。包含统一信息令牌化模块，以及三层协同机制：特征令牌自注意力、领域-特征注意力、领域融合聚合

Result: 在真实工业数据集上显著优于最先进的多场景学习和多任务学习基线。在抖音搜索平台一个月的在线A/B测试中，LT30提升+0.0626%，变更查询率降低-0.3267%。已在生产环境全面部署，每日服务数亿用户

Conclusion: MDL框架通过将场景和任务信息作为特殊令牌，实现了深度交互和参数空间的有效激活，解决了工业推荐系统中多场景多任务联合建模的挑战

Abstract: Industrial recommender systems increasingly adopt multi-scenario learning (MSL) and multi-task learning (MTL) to handle diverse user interactions and contexts, but existing approaches suffer from two critical drawbacks: (1) underutilization of large-scale model parameters due to limited interaction with complex feature modules, and (2) difficulty in jointly modeling scenario and task information in a unified framework. To address these challenges, we propose a unified \textbf{M}ulti-\textbf{D}istribution \textbf{L}earning (MDL) framework, inspired by the "prompting" paradigm in large language models (LLMs). MDL treats scenario and task information as specialized tokens rather than auxiliary inputs or gating signals. Specifically, we introduce a unified information tokenization module that transforms features, scenarios, and tasks into a unified tokenized format. To facilitate deep interaction, we design three synergistic mechanisms: (1) feature token self-attention for rich feature interactions, (2) domain-feature attention for scenario/task-adaptive feature activation, and (3) domain-fused aggregation for joint distribution prediction. By stacking these interactions, MDL enables scenario and task information to "prompt" and activate the model's vast parameter space in a bottom-up, layer-wise manner. Extensive experiments on real-world industrial datasets demonstrate that MDL significantly outperforms state-of-the-art MSL and MTL baselines. Online A/B testing on Douyin Search platform over one month yields +0.0626\% improvement in LT30 and -0.3267\% reduction in change query rate. MDL has been fully deployed in production, serving hundreds of millions of users daily.

</details>


### [370] [IGMiRAG: Intuition-Guided Retrieval-Augmented Generation with Adaptive Mining of In-Depth Memory](https://arxiv.org/abs/2602.07525)
*Xingliang Hou,Yuyan Liu,Qi Sun,haoxiu wang,Hao Hu,Shaoyi Du,Zhiqiang Tian*

Main category: cs.IR

TL;DR: IGMiRAG是一个基于直觉引导推理的检索增强生成框架，通过构建层次异构超图对齐多粒度知识，使用双向扩散算法挖掘深度记忆，在效率和效果上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于图/超图的RAG方法存在内存组织不对齐的问题，导致检索成本高且分散。需要一种更符合人类直觉推理的框架来改善跨文本关联和检索效率。

Method: 1) 构建层次异构超图来对齐多粒度知识，包含演绎路径模拟现实记忆结构；2) 通过问题解析器蒸馏直觉策略，控制挖掘深度和记忆窗口；3) 使用双焦点检索激活瞬时记忆作为锚点；4) 设计双向扩散算法在演绎路径上导航挖掘深度记忆。

Result: 在广泛评估中，IGMiRAG在EM指标上比最先进基线提高4.8%，F1指标提高5.0%。token成本根据任务复杂度自适应调整（平均6.3k+，最低3.0k+）。

Conclusion: IGMiRAG提出了一种成本效益高的RAG范式，通过模拟人类直觉推理过程，在提高检索效率的同时增强了生成效果，为知识密集型任务提供了更有效的解决方案。

Abstract: Retrieval-augmented generation (RAG) equips large language models (LLMs) with reliable knowledge memory. To strengthen cross-text associations, recent research integrates graphs and hypergraphs into RAG to capture pairwise and multi-entity relations as structured links. However, their misaligned memory organization necessitates costly, disjointed retrieval. To address these limitations, we propose IGMiRAG, a framework inspired by human intuition-guided reasoning. It constructs a hierarchical heterogeneous hypergraph to align multi-granular knowledge, incorporating deductive pathways to simulate realistic memory structures. During querying, IGMiRAG distills intuitive strategies via a question parser to control mining depth and memory window, and activates instantaneous memories as anchors using dual-focus retrieval. Mirroring human intuition, the framework guides retrieval resource allocation dynamically. Furthermore, we design a bidirectional diffusion algorithm that navigates deductive paths to mine in-depth memories, emulating human reasoning processes. Extensive evaluations indicate IGMiRAG outperforms the state-of-the-art baseline by 4.8% EM and 5.0% F1 overall, with token costs adapting to task complexity (average 6.3k+, minimum 3.0k+). This work presents a cost-effective RAG paradigm that improves both efficiency and effectiveness.

</details>


### [371] [MSN: A Memory-based Sparse Activation Scaling Framework for Large-scale Industrial Recommendation](https://arxiv.org/abs/2602.07526)
*Shikang Wu,Hui Lu,Jinqiu Jin,Zheng Chai,Shiyong Hong,Junjie Zhang,Shanlei Mu,Kaiyuan Ma,Tianyi Liu,Yuchao Zheng,Zhe Wang,Jingjian Lin*

Main category: cs.IR

TL;DR: MSN是一个基于内存的稀疏激活扩展框架，用于推荐模型，通过动态检索个性化表征并集成到下游特征交互模块，实现细粒度个性化且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现有深度推荐模型扩展方法计算开销大，难以在严格延迟约束下部署；稀疏激活方法如稀疏混合专家虽减少计算，但仍存在高内存访问成本和有限个性化能力的问题。

Method: 提出MSN框架：1) 从大型参数化内存动态检索个性化表征；2) 通过内存门控机制集成到下游特征交互模块；3) 采用产品键内存(PKM)机制将检索复杂度从线性降至亚线性；4) 引入归一化和过参数化技术平衡内存利用；5) 设计定制Sparse-Gather算子和AirTopK算子提升工业效率。

Result: 实验表明MSN能持续提升推荐性能同时保持高效率；已在抖音搜索排序系统成功部署，在离线评估指标和大规模在线A/B测试中均显著优于现有最优模型。

Conclusion: MSN通过基于内存的稀疏激活扩展框架，有效解决了推荐模型扩展中的计算开销和个性化能力限制问题，实现了高效且个性化的推荐系统部署。

Abstract: Scaling deep learning recommendation models is an effective way to improve model expressiveness. Existing approaches often incur substantial computational overhead, making them difficult to deploy in large-scale industrial systems under strict latency constraints. Recent sparse activation scaling methods, such as Sparse Mixture-of-Experts, reduce computation by activating only a subset of parameters, but still suffer from high memory access costs and limited personalization capacity due to the large size and small number of experts. To address these challenges, we propose MSN, a memory-based sparse activation scaling framework for recommendation models. MSN dynamically retrieves personalized representations from a large parameterized memory and integrates them into downstream feature interaction modules via a memory gating mechanism, enabling fine-grained personalization with low computational overhead. To enable further expansion of the memory capacity while keeping both computational and memory access costs under control, MSN adopts a Product-Key Memory (PKM) mechanism, which factorizes the memory retrieval complexity from linear time to sub-linear complexity. In addition, normalization and over-parameterization techniques are introduced to maintain balanced memory utilization and prevent memory retrieval collapse. We further design customized Sparse-Gather operator and adopt the AirTopK operator to improve training and inference efficiency in industrial settings. Extensive experiments demonstrate that MSN consistently improves recommendation performance while maintaining high efficiency. Moreover, MSN has been successfully deployed in the Douyin Search Ranking System, achieving significant gains over deployed state-of-the-art models in both offline evaluation metrics and large-scale online A/B test.

</details>


### [372] [HypRAG: Hyperbolic Dense Retrieval for Retrieval Augmented Generation](https://arxiv.org/abs/2602.07739)
*Hiren Madhu,Ngoc Bui,Ali Maatouk,Leandros Tassiulas,Smita Krishnaswamy,Menglin Yang,Sukanta Ganguly,Kiran Srinivasan,Rex Ying*

Main category: cs.IR

TL;DR: 本文提出双曲稠密检索方法，通过双曲空间嵌入更好地捕捉自然语言的层次结构，相比欧几里得嵌入能显著提升检索质量和RAG系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 自然语言具有从广泛主题到具体实体的层次结构，但当前RAG系统中使用的欧几里得嵌入无法有效保留这种结构，导致语义上较远的文档被错误地视为相似，增加了幻觉风险。

Method: 提出了双曲稠密检索方法，开发了两种模型变体：完全双曲变换器HyTE-FH和混合架构HyTE-H。引入了向外爱因斯坦中点作为几何感知的池化算子，防止序列聚合过程中的表示崩溃。

Result: 在MTEB基准测试中，HyTE-FH优于等效的欧几里得基线；在RAGBench上，HyTE-H在上下文相关性和答案相关性方面比欧几里得基线提升高达29%，且使用比当前最先进检索器小得多的模型。

Conclusion: 双曲表示通过基于范数的分离编码文档特异性，从一般概念到具体概念有超过20%的径向增加，这一特性在欧几里得嵌入中不存在，强调了几何归纳偏置在忠实RAG系统中的关键作用。

Abstract: Embedding geometry plays a fundamental role in retrieval quality, yet dense retrievers for retrieval-augmented generation (RAG) remain largely confined to Euclidean space. However, natural language exhibits hierarchical structure from broad topics to specific entities that Euclidean embeddings fail to preserve, causing semantically distant documents to appear spuriously similar and increasing hallucination risk. To address these limitations, we introduce hyperbolic dense retrieval, developing two model variants in the Lorentz model of hyperbolic space: HyTE-FH, a fully hyperbolic transformer, and HyTE-H, a hybrid architecture projecting pre-trained Euclidean embeddings into hyperbolic space. To prevent representational collapse during sequence aggregation, we introduce the Outward Einstein Midpoint, a geometry-aware pooling operator that provably preserves hierarchical structure. On MTEB, HyTE-FH outperforms equivalent Euclidean baselines, while on RAGBench, HyTE-H achieves up to 29% gains over Euclidean baselines in context relevance and answer relevance using substantially smaller models than current state-of-the-art retrievers. Our analysis also reveals that hyperbolic representations encode document specificity through norm-based separation, with over 20% radial increase from general to specific concepts, a property absent in Euclidean embeddings, underscoring the critical role of geometric inductive bias in faithful RAG systems.

</details>


### [373] [Generative Reasoning Re-ranker](https://arxiv.org/abs/2602.07774)
*Mingfu Liang,Yufei Li,Jay Xu,Kavosh Asadi,Xi Liu,Shuo Gu,Kaushik Rangadurai,Frank Shyu,Shuaiwen Wang,Song Yang,Zhijing Li,Jiang Liu,Mengying Sun,Fei Tian,Xiaohan Wei,Chonglin Sun,Jacob Tao,Shike Mei,Hamed Firooz,Wenlin Chen,Luke Simon*

Main category: cs.IR

TL;DR: GR2是一个用于推荐系统重排的三阶段训练框架，通过语义ID编码、高质量推理轨迹生成和强化学习优化，显著提升了重排性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统存在三个关键局限：1) 重排阶段被忽视；2) LLM的推理能力未充分利用；3) 使用非语义ID表示导致工业系统扩展性差。

Method: 提出GR2三阶段训练框架：1) 使用tokenizer将非语义ID编码为语义ID进行中训练；2) 通过精心设计的提示和拒绝采样生成高质量推理轨迹进行监督微调；3) 应用DAPO进行可扩展的强化学习监督，使用可验证奖励。

Result: 在两个真实世界数据集上，GR2在Recall@5和NDCG@5上分别超过当前最佳方法OneRec-Think 2.4%和1.3%。消融实验证实高级推理轨迹带来显著提升，RL奖励设计对重排至关重要。

Conclusion: GR2有效解决了LLM推荐系统的重排问题，通过语义ID、高质量推理和强化学习监督的集成，显著提升了推荐性能，并为工业级推荐系统提供了可扩展的解决方案。

Abstract: Recent studies increasingly explore Large Language Models (LLMs) as a new paradigm for recommendation systems due to their scalability and world knowledge. However, existing work has three key limitations: (1) most efforts focus on retrieval and ranking, while the reranking phase, critical for refining final recommendations, is largely overlooked; (2) LLMs are typically used in zero-shot or supervised fine-tuning settings, leaving their reasoning abilities, especially those enhanced through reinforcement learning (RL) and high-quality reasoning data, underexploited; (3) items are commonly represented by non-semantic IDs, creating major scalability challenges in industrial systems with billions of identifiers. To address these gaps, we propose the Generative Reasoning Reranker (GR2), an end-to-end framework with a three-stage training pipeline tailored for reranking. First, a pretrained LLM is mid-trained on semantic IDs encoded from non-semantic IDs via a tokenizer achieving $\ge$99% uniqueness. Next, a stronger larger-scale LLM generates high-quality reasoning traces through carefully designed prompting and rejection sampling, which are used for supervised fine-tuning to impart foundational reasoning skills. Finally, we apply Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO), enabling scalable RL supervision with verifiable rewards designed specifically for reranking. Experiments on two real-world datasets demonstrate GR2's effectiveness: it surpasses the state-of-the-art OneRec-Think by 2.4% in Recall@5 and 1.3% in NDCG@5. Ablations confirm that advanced reasoning traces yield substantial gains across metrics. We further find that RL reward design is crucial in reranking: LLMs tend to exploit reward hacking by preserving item order, motivating conditional verifiable rewards to mitigate this behavior and optimize reranking performance.

</details>


### [374] [Learning to Alleviate Familiarity Bias in Video Recommendation](https://arxiv.org/abs/2602.07987)
*Zheng Ren,Yi Wu,Jianan Lu,Acar Ary,Yiqu Liu,Li Wei,Lukasz Heldt*

Main category: cs.IR

TL;DR: LAFB是一个轻量级、模型无关的框架，用于缓解推荐系统中的熟悉度偏差，通过建模用户-内容熟悉度并调整预测分数来增加内容多样性。


<details>
  <summary>Details</summary>
Motivation: 现代视频推荐系统面临结构性曝光不平衡问题，主要由行为偏差引起，特别是熟悉度偏差导致熟悉内容过度主导推荐结果。

Method: LAFB在重排序阶段工作，使用离散和连续交互特征建模用户-内容熟悉度，估计个性化去偏因子来调整用户评分预测分数，从而减少熟悉内容在最终排名中的主导地位。

Result: 大规模离线评估和在线A/B测试显示，LAFB增加了新颖观看时间份额，改善了新兴创作者曝光和整体内容多样性，同时保持了稳定的总体观看时间和短期满意度。

Conclusion: LAFB已成功部署在YouTube推荐系统的重排序阶段，证明其在真实世界应用中能有效缓解熟悉度偏差，平衡用户参与度和内容多样性目标。

Abstract: Modern video recommendation systems aim to optimize user engagement and platform objectives, yet often face structural exposure imbalances caused by behavioral biases. In this work, we focus on the post-ranking stage and present LAFB (Learning to Alleviate Familiarity Bias), a lightweight and model-agnostic framework designed to mitigate familiarity bias in recommendation outputs. LAFB models user-content familiarity using discrete and continuous interaction features, and estimates personalized debiasing factors to adjust user rating prediction scores, thereby reducing the dominance of familiar content in the final ranking. We conduct large-scale offline evaluations and online A/B testing in a real-world recommendation system, under a unified serving stack that also compares LAFB with deployable popularity-oriented remedies. Results show that LAFB increases novel watch-time share and improves exposure for emerging creators and overall content diversity, while maintaining stable overall watch time and short-term satisfaction. LAFB has already been launched in the post-ranking stage of YouTube's recommendation system, demonstrating its effectiveness in real-world applications.

</details>


### [375] [IRB: Automated Generation of Robust Factuality Benchmarks](https://arxiv.org/abs/2602.08070)
*Lam Thanh Do,Bhagyashree Taleka,Hozaifa Ammar Bhutta,Vikram Sharma Mailthody,Kevin Chen-Chuan Chang,Wen-mei Hwu*

Main category: cs.IR

TL;DR: IRB框架通过自动生成基准测试来评估RAG系统的事实准确性，解决了传统静态基准测试易饱和且维护成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统的静态基准测试存在快速饱和问题，需要大量人工维护来保持鲁棒性，因此需要自动化的基准生成方法。

Method: IRB框架采用结构化生成管道，利用"事实支架"和"算法支架"自动生成评估RAG系统事实准确性的基准测试。

Result: IRB对前沿LLM在闭卷设置下构成显著挑战；推理型LLM更可靠；改进检索组件比扩展生成器能带来更经济有效的RAG系统正确性提升。

Conclusion: IRB为RAG系统提供了有效的自动化评估框架，揭示了推理能力的重要性以及检索组件优化的成本效益优势。

Abstract: Static benchmarks for RAG systems often suffer from rapid saturation and require significant manual effort to maintain robustness. To address this, we present IRB, a framework for automatically generating benchmarks to evaluate the factuality of RAG systems. IRB employs a structured generation pipeline utilizing \textit{factual scaffold} and \textit{algorithmic scaffold}. We utilize IRB to construct a benchmark and evaluate frontier LLMs and retrievers. Our results demonstrate that IRB poses a significant challenge for frontier LLMs in the closed-book setting. Furthermore, our evaluation suggests that reasoning LLMs are more reliable, and that improving the retrieval component may yield more cost-effective gains in RAG system correctness than scaling the generator.

</details>


### [376] [A Sketch+Text Composed Image Retrieval Dataset for Thangka](https://arxiv.org/abs/2602.08411)
*Jinyu Xu,Yi Sun,Jiangling Zhang,Qing Xie,Daomin Ji,Zhifeng Bao,Jiachen Li,Yanchun Ma,Yongjian Liu*

Main category: cs.IR

TL;DR: 提出了CIRThan数据集，这是一个专门针对唐卡图像的草图+文本组合图像检索数据集，包含2,287张高质量唐卡图像，每张图像都配有手绘草图和三层语义层次的文本描述，用于评估组合查询能力。


<details>
  <summary>Details</summary>
Motivation: 现有的组合图像检索(CIR)基准主要关注通用领域图像，依赖带有简短文本修改的参考图像，无法有效支持需要细粒度语义推理、结构化视觉理解和领域特定知识的检索场景。唐卡图像具有复杂结构、密集符号元素和领域依赖的语义约定，需要专门的评估基准。

Method: 创建了CIRThan数据集，包含2,287张高质量唐卡图像，每张图像配有：1）人类绘制的手绘草图；2）三层语义层次的文本描述（从粗到细）。提供了标准化的数据划分、全面的数据集分析，并对代表性的监督和零样本CIR方法进行了基准评估。

Result: 实验结果显示，现有的CIR方法（主要为通用领域图像开发）难以有效对齐基于草图的抽象和分层文本语义与细粒度唐卡图像，特别是在没有领域内监督的情况下。这表明当前方法在知识特定视觉领域的局限性。

Conclusion: CIRThan为推进草图+文本组合图像检索、分层语义建模以及文化遗产和其他知识特定视觉领域的多模态检索提供了有价值的基准。数据集已公开可用，有助于解决现有方法在复杂结构化、符号密集的视觉领域的不足。

Abstract: Composed Image Retrieval (CIR) enables image retrieval by combining multiple query modalities, but existing benchmarks predominantly focus on general-domain imagery and rely on reference images with short textual modifications. As a result, they provide limited support for retrieval scenarios that require fine-grained semantic reasoning, structured visual understanding, and domain-specific knowledge. In this work, we introduce CIRThan, a sketch+text Composed Image Retrieval dataset for Thangka imagery, a culturally grounded and knowledge-specific visual domain characterized by complex structures, dense symbolic elements, and domain-dependent semantic conventions. CIRThan contains 2,287 high-quality Thangka images, each paired with a human-drawn sketch and hierarchical textual descriptions at three semantic levels, enabling composed queries that jointly express structural intent and multi-level semantic specification. We provide standardized data splits, comprehensive dataset analysis, and benchmark evaluations of representative supervised and zero-shot CIR methods. Experimental results reveal that existing CIR approaches, largely developed for general-domain imagery, struggle to effectively align sketch-based abstractions and hierarchical textual semantics with fine-grained Thangka images, particularly without in-domain supervision. We believe CIRThan offers a valuable benchmark for advancing sketch+text CIR, hierarchical semantic modeling, and multimodal retrieval in cultural heritage and other knowledge-specific visual domains. The dataset is publicly available at https://github.com/jinyuxu-whut/CIRThan.

</details>


### [377] [Hybrid Pooling with LLMs via Relevance Context Learning](https://arxiv.org/abs/2602.08457)
*David Otero,Javier Parapar*

Main category: cs.IR

TL;DR: 本文提出了Relevance Context Learning (RCL)框架，通过让LLM分析已标注的查询-文档对生成明确的"相关性叙事"来描述主题相关性标准，从而显著提升LLM作为自动相关性评估器的性能。


<details>
  <summary>Details</summary>
Motivation: 高质量的相关性标注对于评估信息检索系统至关重要，但人工标注成本高、耗时长。现有LLM作为自动评估器的方法大多依赖零样本提示或少量标注示例的上下文学习，但这些方法将示例视为独立实例，未能明确捕捉主题的相关性标准，限制了其泛化能力。

Method: 提出Relevance Context Learning (RCL)框架：1) 使用Instructor LLM分析已标注的查询-文档对，生成明确描述主题相关性标准的"相关性叙事"；2) 将这些叙事作为结构化提示指导Assessor LLM进行相关性判断；3) 提出混合池化策略，浅层k个文档由人工标注，其余文档由LLM标注。

Result: 实验结果表明，RCL显著优于零样本提示方法，并持续改进标准上下文学习方法。通过将相关性示例转化为明确的、上下文感知的相关性叙事，能更有效地利用人工标注进行基于LLM的IR数据集构建。

Conclusion: 将相关性示例转化为明确的相关性叙事是更有效利用人工标注进行LLM-based IR数据集构建的方法。RCL框架通过明确建模主题特定的相关性标准，显著提升了LLM作为自动相关性评估器的可靠性和性能。

Abstract: High-quality relevance judgements over large query sets are essential for evaluating Information Retrieval (IR) systems, yet manual annotation remains costly and time-consuming. Large Language Models (LLMs) have recently shown promise as automatic relevance assessors, but their reliability is still limited. Most existing approaches rely on zero-shot prompting or In-Context Learning (ICL) with a small number of labeled examples. However, standard ICL treats examples as independent instances and fails to explicitly capture the underlying relevance criteria of a topic, restricting its ability to generalize to unseen query-document pairs. To address this limitation, we introduce Relevance Context Learning (RCL), a novel framework that leverages human relevance judgements to explicitly model topic-specific relevance criteria. Rather than directly using labeled examples for in-context prediction, RCL first prompts an LLM (Instructor LLM) to analyze sets of judged query-document pairs and generate explicit narratives that describe what constitutes relevance for a given topic. These relevance narratives are then used as structured prompts to guide a second LLM (Assessor LLM) in producing relevance judgements. To evaluate RCL in a realistic data collection setting, we propose a hybrid pooling strategy in which a shallow depth-\textit{k} pool from participating systems is judged by human assessors, while the remaining documents are labeled by LLMs. Experimental results demonstrate that RCL substantially outperforms zero-shot prompting and consistently improves over standard ICL. Overall, our findings indicate that transforming relevance examples into explicit, context-aware relevance narratives is a more effective way of exploiting human judgements for LLM-based IR dataset construction.

</details>


### [378] [PIT: A Dynamic Personalized Item Tokenizer for End-to-End Generative Recommendation](https://arxiv.org/abs/2602.08530)
*Huanjie Wang,Xinchen Luo,Honghui Bao,Zhang Zixing,Lejian Ren,Yunfan Wu,Hongwei Zhang,Liwei Guan,Guang Chen*

Main category: cs.IR

TL;DR: PIT是一个动态个性化项目标记化框架，通过协同生成架构实现端到端的生成式推荐，解决了现有方法中协同信号不稳定和两阶段训练的问题，在快手大规模部署中显著提升了应用停留时间。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐方法通常依赖静态解耦的标记化，忽略了协同信号。虽然近期方法尝试在索引构建或端到端建模中整合协同信号，但在实际生产环境中面临挑战：协同信号的波动性导致标记化不稳定，当前端到端策略往往退化为次优的两阶段训练而非真正的协同进化。

Method: 提出PIT动态个性化项目标记化框架，采用协同生成架构：通过协同信号对齐协调协同模式，通过协同进化学习同步项目标记器和生成式推荐器，实现索引构建和推荐的动态联合端到端进化。此外，采用一对多波束索引确保可扩展性和鲁棒性。

Result: 在真实世界数据集上的广泛实验表明，PIT始终优于竞争基线。在快手的大规模部署中，在线A/B测试带来了0.402%的应用停留时间显著提升，验证了该框架在动态工业环境中的有效性。

Conclusion: PIT框架通过协同生成架构解决了生成式推荐中协同信号整合的挑战，实现了动态、联合、端到端的进化，在工业规模部署中表现出色，为生成式推荐系统提供了有效的解决方案。

Abstract: Generative Recommendation has revolutionized recommender systems by reformulating retrieval as a sequence generation task over discrete item identifiers. Despite the progress, existing approaches typically rely on static, decoupled tokenization that ignores collaborative signals. While recent methods attempt to integrate collaborative signals into item identifiers either during index construction or through end-to-end modeling, they encounter significant challenges in real-world production environments. Specifically, the volatility of collaborative signals leads to unstable tokenization, and current end-to-end strategies often devolve into suboptimal two-stage training rather than achieving true co-evolution. To bridge this gap, we propose PIT, a dynamic Personalized Item Tokenizer framework for end-to-end generative recommendation, which employs a co-generative architecture that harmonizes collaborative patterns through collaborative signal alignment and synchronizes item tokenizer with generative recommender via a co-evolution learning. This enables the dynamic, joint, end-to-end evolution of both index construction and recommendation. Furthermore, a one-to-many beam index ensures scalability and robustness, facilitating seamless integration into large-scale industrial deployments. Extensive experiments on real-world datasets demonstrate that PIT consistently outperforms competitive baselines. In a large-scale deployment at Kuaishou, an online A/B test yielded a substantial 0.402% uplift in App Stay Time, validating the framework's effectiveness in dynamic industrial environments.

</details>


### [379] [DA-RAG: Dynamic Attributed Community Search for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.08545)
*Xingyuan Zeng,Zuohan Wu,Yue Wang,Chen Zhang,Quanming Yao,Libin Zheng,Jian Yin*

Main category: cs.IR

TL;DR: DA-RAG是一种基于属性社区搜索的动态图增强检索生成方法，通过动态提取相关子图捕获高阶图结构，在多项指标上优于现有RAG方法40%，同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前基于图的RAG方法通常未能充分利用图拓扑结构，主要关注低阶结构或预计算的静态社区，这限制了处理动态复杂查询的效果。

Method: 提出DA-RAG方法，利用属性社区搜索根据查询问题动态提取相关子图，捕获高阶图结构，并配备分块层导向的图索引以实现高效多粒度检索。

Result: 在多个数据集上评估显示，DA-RAG在四项指标上比现有RAG方法提升高达40%，同时将索引构建时间和token开销分别降低37%和41%。

Conclusion: DA-RAG通过动态社区搜索有效利用图拓扑结构，显著提升了检索增强生成的效果和效率，为复杂查询处理提供了更优解决方案。

Abstract: Owing to their unprecedented comprehension capabilities, large language models (LLMs) have become indispensable components of modern web search engines. From a technical perspective, this integration represents retrieval-augmented generation (RAG), which enhances LLMs by grounding them in external knowledge bases. A prevalent technical approach in this context is graph-based RAG (G-RAG). However, current G-RAG methodologies frequently underutilize graph topology, predominantly focusing on low-order structures or pre-computed static communities. This limitation affects their effectiveness in addressing dynamic and complex queries. Thus, we propose DA-RAG, which leverages attributed community search (ACS) to extract relevant subgraphs based on the queried question dynamically. DA-RAG captures high-order graph structures, allowing for the retrieval of self-complementary knowledge. Furthermore, DA-RAG is equipped with a chunk-layer oriented graph index, which facilitates efficient multi-granularity retrieval while significantly reducing both computational and economic costs. We evaluate DA-RAG on multiple datasets, demonstrating that it outperforms existing RAG methods by up to 40% in head-to-head comparisons across four metrics while reducing index construction time and token overhead by up to 37% and 41%, respectively.

</details>


### [380] [QARM V2: Quantitative Alignment Multi-Modal Recommendation for Reasoning User Sequence Modeling](https://arxiv.org/abs/2602.08559)
*Tian Xia,Jiaqi Zhang,Yueyang Liu,Hongjian Dou,Tingya Yin,Jiangxia Cao,Xulei Liang,Tianlu Xie,Lihao Liu,Xiang Chen,Shen Wang,Changxin Lao,Haixiang Gan,Jinkai Yu,Keting Cen,Lu Hao,Xu Zhang,Qiqiang Zhong,Zhongbo Sun,Yiyu Wang,Shuang Yang,Mingxin Wen,Xiangyu Wu,Shaoguo Liu,Tingting Gao,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: QARM V2是一个统一框架，通过桥接LLM语义理解和推荐系统业务需求来解决用户序列建模问题。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统依赖ID嵌入存在信息密度低、知识隔离和泛化能力弱的问题，而LLM虽然具有丰富的语义理解和强泛化能力，但直接应用于推荐系统面临表示与业务目标不匹配、无法端到端学习等挑战。

Method: 提出了QARM V2统一框架，桥接LLM语义理解和推荐系统业务需求，用于用户序列建模。

Result: 论文介绍了QARM V2框架，但摘要中没有提供具体的实验结果。

Conclusion: QARM V2框架旨在解决LLM嵌入在推荐系统中应用的关键挑战，实现语义理解与业务需求的统一。

Abstract: With the evolution of large language models (LLMs), there is growing interest in leveraging their rich semantic understanding to enhance industrial recommendation systems (RecSys). Traditional RecSys relies on ID-based embeddings for user sequence modeling in the General Search Unit (GSU) and Exact Search Unit (ESU) paradigm, which suffers from low information density, knowledge isolation, and weak generalization ability. While LLMs offer complementary strengths with dense semantic representations and strong generalization, directly applying LLM embeddings to RecSys faces critical challenges: representation unmatch with business objectives and representation unlearning end-to-end with downstream tasks. In this paper, we present QARM V2, a unified framework that bridges LLM semantic understanding with RecSys business requirements for user sequence modeling.

</details>


### [381] [OneLive: Dynamically Unified Generative Framework for Live-Streaming Recommendation](https://arxiv.org/abs/2602.08612)
*Shen Wang,Yusheng Huang,Ruochen Yang,Shuang Wen,Pengbo Xu,Jiangxia Cao,Yueyang Liu,Kuo Cai,Chengcheng Guo,Shiyao Wang,Xinchen Luo,Qiang Luo,Ruiming Tang,Shuang Yang,Zhaojie Liu,Guorui Zhou,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: OneLive是一个为直播场景定制的动态统一生成推荐框架，通过动态分词器、时间感知门控注意力、高效解码器架构和多目标对齐框架解决直播推荐中的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 直播推荐系统面临传统生成推荐方法无法直接应用的挑战：内容持续演变、生命周期有限、严格实时约束和异构多目标，这些因素使得静态分词和传统模型框架失效。

Method: OneLive包含四个核心组件：1) 动态分词器通过残差量化融合实时直播内容和行为信号；2) 时间感知门控注意力机制显式建模时间动态；3) 高效解码器架构采用Sequential MTP和QK Norm；4) 统一多目标对齐框架强化个性化偏好的策略优化。

Result: 论文提出了一种专门针对直播场景的生成推荐框架，能够处理实时演变的内容、有限的生命周期、严格的实时约束和异构多目标，为直播推荐提供了创新的解决方案。

Conclusion: OneLive框架成功解决了直播推荐中的独特挑战，通过动态统一架构实现了对实时演变内容的高效处理，为直播推荐系统提供了可扩展且计算高效的解决方案。

Abstract: Live-streaming recommender system serves as critical infrastructure that bridges the patterns of real-time interactions between users and authors. Similar to traditional industrial recommender systems, live-streaming recommendation also relies on cascade architectures to support large-scale concurrency. Recent advances in generative recommendation unify the multi-stage recommendation process with Transformer-based architectures, offering improved scalability and higher computational efficiency. However, the inherent complexity of live-streaming prevents the direct transfer of these methods to live-streaming scenario, where continuously evolving content, limited lifecycles, strict real-time constraints, and heterogeneous multi-objectives introduce unique challenges that invalidate static tokenization and conventional model framework. To address these issues, we propose OneLive, a dynamically unified generative recommendation framework tailored for live-streaming scenario. OneLive integrates four key components: (i) A Dynamic Tokenizer that continuously encodes evolving real-time live content fused with behavior signal through residual quantization; (ii) A Time-Aware Gated Attention mechanism that explicitly models temporal dynamics for timely decision making; (iii) An efficient decoder-only generative architecture enhanced with Sequential MTP and QK Norm for stable training and accelerated inference; (iv) A Unified Multi-Objective Alignment Framework reinforces policy optimization for personalized preferences.

</details>


### [382] [SRSUPM: Sequential Recommender System Based on User Psychological Motivation](https://arxiv.org/abs/2602.08667)
*Yicheng Di,Yuan Liu,Zhi Chen,Jingcai Guo*

Main category: cs.IR

TL;DR: 提出SRSUPM框架，通过心理动机转移建模增强序列推荐系统，解决现有方法缺乏显式心理动机转移建模的问题


<details>
  <summary>Details</summary>
Motivation: 现有序列推荐方法将近期行为压缩为单一向量，缺乏对用户心理动机转移的显式建模，难以捕捉不同转移程度的分布模式和协作知识

Method: 提出SRSUPM框架：1) 心理动机转移评估(PMSA)定量测量转移程度；2) 转移信息构建动态演化多级转移状态；3) 心理动机转移驱动信息分解跨转移级别分解和正则化表示；4) 心理动机转移信息匹配增强与转移相关的协作模式

Result: 在三个公共基准测试上的广泛实验表明，SRSUPM在多种序列推荐任务上持续优于代表性基线方法

Conclusion: SRSUPM框架通过显式建模心理动机转移，能够学习更具区分性的用户表示，有效提升序列推荐性能

Abstract: Sequential recommender infers users' evolving psychological motivations from historical interactions to recommend the next preferred items. Most existing methods compress recent behaviors into a single vector and optimize it toward a single observed target item, but lack explicit modeling of psychological motivation shift. As a result, they struggle to uncover the distributional patterns across different shift degrees and to capture collaborative knowledge that is sensitive to psychological motivation shift. We propose a general framework, the Sequential Recommender System Based on User Psychological Motivation, to enhance sequential recommenders with psychological motivation shift-aware user modeling. Specifically, the Psychological Motivation Shift Assessment quantitatively measures psychological motivation shift; guided by PMSA, the Shift Information Construction models dynamically evolving multi-level shift states, and the Psychological Motivation Shift-driven Information Decomposition decomposes and regularizes representations across shift levels. Moreover, the Psychological Motivation Shift Information Matching strengthens collaborative patterns related to psychological motivation shift to learn more discriminative user representations. Extensive experiments on three public benchmarks show that SRSUPM consistently outperforms representative baselines on diverse sequential recommender tasks.

</details>


### [383] [SA-CAISR: Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation](https://arxiv.org/abs/2602.08678)
*Xiaomeng Song,Xinru Wang,Hanbing Wang,Hongyu Lu,Yu Chen,Zhaochun Ren,Zhumin Chen*

Main category: cs.IR

TL;DR: SA-CAISR是一个无缓冲的阶段自适应冲突感知增量序列推荐框架，通过Fisher加权知识筛选机制动态识别过时知识，在减少97.5%内存使用和46.9%训练时间的同时，显著提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法在序列推荐中存在局限性：基于重放的方法内存和计算成本高，基于正则化的方法难以有效丢弃过时或冲突知识。需要一种更高效的增量学习框架来适应动态变化的用户偏好。

Method: 提出SA-CAISR框架，采用无缓冲设计，仅使用旧模型和新数据。引入Fisher加权知识筛选机制，通过估计旧模型与新数据之间的参数级冲突，动态识别过时知识，选择性移除过时知识同时保留兼容的历史模式。

Result: 在增量序列推荐中达到新的SOTA性能：Recall@20平均提升2.0%，MRR@20提升1.2%，NDCG@20提升1.4%。相比最佳基线，内存使用减少97.5%，训练时间减少46.9%。

Conclusion: SA-CAISR通过动态平衡稳定性和适应性，实现了高效准确的增量序列推荐，使现实系统能够以最小计算开销快速更新用户画像，确保更及时准确的推荐。

Abstract: Sequential recommendation (SR) aims to predict a user's next action by learning from their historical interaction sequences. In real-world applications, these models require periodic updates to adapt to new interactions and evolving user preferences. While incremental learning methods facilitate these updates, they face significant challenges. Replay-based approaches incur high memory and computational costs, and regularization-based methods often struggle to discard outdated or conflicting knowledge. To overcome these challenges, we propose SA-CAISR, a Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation framework. As a buffer-free framework, SA-CAISR operates using only the old model and new data, directly addressing the high costs of replay-based techniques. SA-CAISR introduces a novel Fisher-weighted knowledge-screening mechanism that dynamically identifies outdated knowledge by estimating parameter-level conflicts between the old model and new data, allowing our approach to selectively remove obsolete knowledge while preserving compatible historical patterns. This dynamic balance between stability and adaptability allows our method to achieve a new state-of-the-art performance in incremental SR. Specifically, SA-CAISR improves Recall@20 by 2.0%, MRR@20 by 1.2%, and NDCG@20 by 1.4% on average across datasets, while reducing memory usage by 97.5% and training time by 46.9% compared to the best baselines. This efficiency allows real-world systems to rapidly update user profiles with minimal computational overhead, ensuring more timely and accurate recommendations.

</details>


### [384] [AMEM4Rec: Leveraging Cross-User Similarity for Memory Evolution in Agentic LLM Recommenders](https://arxiv.org/abs/2602.08837)
*Minh-Duc Nguyen,Hai-Dang Kieu,Dung D. Le*

Main category: cs.IR

TL;DR: AMEM4Rec是一个基于LLM的智能推荐系统，通过跨用户记忆演化学习协同过滤信号，解决了现有LLM推荐系统参数效率低、上下文限制和忽视协同信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能推荐系统存在几个关键问题：微调LLM参数效率低，基于提示的智能推理受上下文长度限制且有幻觉风险，且现有系统主要依赖语义知识而忽视了协同过滤信号这一建模隐式偏好的关键要素。

Method: 提出AMEM4Rec框架，通过跨用户记忆演化端到端学习协同信号。系统从用户历史中提取抽象行为模式存储在全局记忆池中，记忆通过链接到相似现有记忆并迭代演化来强化跨用户共享模式，使系统能够感知协同过滤信号而无需依赖预训练的协同过滤模型。

Result: 在Amazon和MIND数据集上的大量实验表明，AMEM4Rec在各项指标上持续优于最先进的基于LLM的推荐系统，证明了演化记忆引导的协同过滤方法的有效性。

Conclusion: AMEM4Rec通过跨用户记忆演化成功地将协同过滤信号整合到基于LLM的智能推荐系统中，解决了现有方法的局限性，为LLM在推荐系统中的应用提供了新的有效途径。

Abstract: Agentic systems powered by Large Language Models (LLMs) have shown strong potential in recommender systems but remain hindered by several challenges. Fine-tuning LLMs is parameter-inefficient, and prompt-based agentic reasoning is limited by context length and hallucination risk. Moreover, existing agentic recommendation systems predominantly leverages semantic knowledge while neglecting the collaborative filtering (CF) signals essential for implicit preference modeling. To address these limitations, we propose AMEM4Rec, an agentic LLM-based recommender that learns collaborative signals in an end-to-end manner through cross-user memory evolution. AMEM4Rec stores abstract user behavior patterns from user histories in a global memory pool. Within this pool, memories are linked to similar existing ones and iteratively evolved to reinforce shared cross-user patterns, enabling the system to become aware of CF signals without relying on a pre-trained CF model. Extensive experiments on Amazon and MIND datasets show that AMEM4Rec consistently outperforms state-of-the-art LLM-based recommenders, demonstrating the effectiveness of evolving memory-guided collaborative filtering.

</details>


### [385] [Contrastive Learning for Diversity-Aware Product Recommendations in Retail](https://arxiv.org/abs/2602.08886)
*Vasileios Karlis,Ezgi Yıldırım,David Vos,Maarten de Rijke*

Main category: cs.IR

TL;DR: 该论文提出了一种通过负采样和对比学习来提升推荐系统商品目录覆盖率的方法，在不牺牲推荐质量的前提下增加长尾商品的曝光。


<details>
  <summary>Details</summary>
Motivation: 推荐系统通常面临长尾分布问题，少数热门商品主导推荐结果，导致商品目录覆盖率低。在IKEA这样拥有大量多样化产品的大型在线零售环境中，这个问题尤为突出，需要在不影响推荐质量的前提下提升商品曝光多样性。

Method: 受负采样解决流行度偏差的启发，将对比学习与精心选择的负样本相结合，集成到现有的数字推荐流程中。通过负采样策略来平衡热门商品和长尾商品的推荐权重。

Result: 离线和在线评估表明，该方法显著提升了商品目录覆盖率，确保推荐结果更加多样化，同时保持了强大的推荐性能。

Conclusion: 提出的方法成功解决了推荐系统中的长尾分布问题，在保持推荐质量的同时提高了商品目录覆盖率，为大规模在线零售环境中的推荐系统优化提供了有效解决方案。

Abstract: Recommender systems often struggle with long-tail distributions and limited item catalog exposure, where a small subset of popular items dominates recommendations. This challenge is especially critical in large-scale online retail settings with extensive and diverse product assortments. This paper introduces an approach to enhance catalog coverage without compromising recommendation quality in the existing digital recommendation pipeline at IKEA Retail. Drawing inspiration from recent advances in negative sampling to address popularity bias, we integrate contrastive learning with carefully selected negative samples. Through offline and online evaluations, we demonstrate that our method improves catalog coverage, ensuring a more diverse set of recommendations yet preserving strong recommendation performance.

</details>


### [386] [OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation](https://arxiv.org/abs/2602.08896)
*Yehua Huang,Penglei Sun,Zebin Chen,Zhenheng Tang,Xiaowen Chu*

Main category: cs.IR

TL;DR: OmniReview数据集和Pro-MMoE框架解决了学术同行评审中的数据和方法的挑战，通过大规模验证数据集和结合LLM与多任务学习的创新方法，提升了审稿人推荐的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 学术同行评审面临数据和方法的双重挑战：数据方面缺乏大规模验证基准和过于简化的评估指标；方法方面现有嵌入方法存在语义压缩的信息瓶颈和有限的可解释性问题。

Method: 提出OmniReview数据集（202,756条验证记录）和三层次评估框架；提出Pro-MMoE框架，结合LLM生成语义档案以保留细粒度专业知识，并采用任务自适应MMoE架构动态平衡冲突的评估目标。

Result: Pro-MMoE在七个评估指标中的六个上实现了最先进的性能，为现实的审稿人推荐建立了新的基准。

Conclusion: 该研究通过综合数据集和创新框架，有效解决了学术同行评审中的关键挑战，为审稿人推荐系统提供了更准确、可解释的解决方案。

Abstract: Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-world editorial workflows. To bridge this gap, we present OmniReview, a comprehensive dataset constructed by integrating multi-source academic platforms encompassing comprehensive scholarly profiles through the disambiguation pipeline, yielding 202, 756 verified review records. Based on this data, we introduce a three-tier hierarchical evaluaion framework to assess recommendations from recall to precise expert identification. From the method perspective, existing embedding-based approaches suffer from the information bottleneck of semantic compression and limited interpretability. To resolve these method limitations, we propose Profiling Scholars with Multi-gate Mixture-of-Experts (Pro-MMoE), a novel framework that synergizes Large Language Models (LLMs) with Multi-task Learning. Specifically, it utilizes LLM-generated semantic profiles to preserve fine-grained expertise nuances and interpretability, while employing a Task-Adaptive MMoE architecture to dynamically balance conflicting evaluation goals. Comprehensive experiments demonstrate that Pro-MMoE achieves state-of-the-art performance across six of seven metrics, establishing a new benchmark for realistic reviewer recommendation.

</details>


### [387] [Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion](https://arxiv.org/abs/2602.08917)
*Minghan Li,Ercong Nie,Siqi Zhao,Tongna Chen,Huiping Huang,Guodong Zhou*

Main category: cs.IR

TL;DR: 提出自动化、领域自适应的查询扩展框架，通过BM25-MonoT5管道构建领域内示例池，使用无监督聚类策略选择多样化演示，并引入双LLM集成方法生成更优的查询扩展。


<details>
  <summary>Details</summary>
Motivation: 传统查询扩展方法依赖手工设计的提示、人工选择的示例或单一LLM，导致扩展性差且对领域变化敏感，需要更自动化、适应性强且稳定的解决方案。

Method: 1) 使用BM25-MonoT5管道从伪相关文档中构建领域内示例池；2) 采用无监督聚类策略选择多样化演示；3) 引入双LLM集成方法：两个异构LLM独立生成扩展，第三个LLM进行整合优化。

Result: 在TREC DL20、DBPedia和SciFact数据集上，提出的集成方法相比BM25、Rocchio、零样本和固定少样本基线，取得了稳定且统计显著的性能提升。

Conclusion: 该框架为示例选择和多LLM生成提供了可复现的测试平台，并为实际应用中的查询扩展提供了无需标注的实用解决方案。

Abstract: Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [388] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: Verify-RL框架利用符号微分规则实现可验证的数学问题分解，确保子问题更简单、解决子问题有助于父任务，且分解关系有数学基础，相比启发式方法显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数学问题分解方法通常是启发式的，无法保证子问题更简单、解决子问题有助于父任务，且分解关系缺乏数学基础。需要一种可验证的分解框架来确保这些关键属性。

Method: 提出Verify-RL框架，利用符号微分规则进行分解：微积分规则明确定义了表达式如何简化为更简单的组件，并具有可证明的性质。每个父-子分解满足三个可验证条件：结构复杂度严格递减、解包含性、形式规则推导。

Result: 实验表明，消除无效分解带来显著收益：最困难问题的准确率从32%翻倍至68%，整体相对改进达40%。可验证分解相比启发式方法有显著优势。

Conclusion: 符号微分为数学问题分解提供了自然的可验证结构，Verify-RL框架通过确保分解的数学正确性，显著提升了语言模型解决复杂数学问题的能力。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [389] [ST-Raptor: An Agentic System for Semi-Structured Table QA](https://arxiv.org/abs/2602.07034)
*Jinxiu Qu,Zirui Tang,Hongzhang Huang,Boyu Niu,Wei Zhou,Jiannan Wang,Yitong Song,Guoliang Li,Xuanhe Zhou,Fan Wu*

Main category: cs.AI

TL;DR: ST-Raptor是一个用于半结构化表格问答的智能体系统，通过结合视觉编辑、树状结构建模和智能体驱动查询来解决现有方法的信息丢失和布局处理困难问题。


<details>
  <summary>Details</summary>
Motivation: 半结构化表格问答需要精确提取单元格内容和位置，并恢复表格布局中隐含的逻辑结构、层次关系和语义关联。现有方法存在信息丢失、复杂布局处理困难等问题，人工解释又耗时耗力。

Method: ST-Raptor是一个智能体系统，提供交互式分析环境，结合视觉编辑、树状结构建模和智能体驱动查询来解决半结构化表格问答问题。

Result: 在基准测试和真实世界数据集上的实验结果表明，ST-Raptor在准确性和可用性方面都优于现有方法。

Conclusion: ST-Raptor通过创新的交互式智能体系统，有效解决了半结构化表格问答中的信息丢失和复杂布局处理问题，提供了更准确和用户友好的解决方案。

Abstract: Semi-structured table question answering (QA) is a challenging task that requires (1) precise extraction of cell contents and positions and (2) accurate recovery of key implicit logical structures, hierarchical relationships, and semantic associations encoded in table layouts. In practice, such tables are often interpreted manually by human experts, which is labor-intensive and time-consuming. However, automating this process remains difficult. Existing Text-to-SQL methods typically require converting semi-structured tables into structured formats, inevitably leading to information loss, while approaches like Text-to-Code and multimodal LLM-based QA struggle with complex layouts and often yield inaccurate answers. To address these limitations, we present ST-Raptor, an agentic system for semi-structured table QA. ST-Raptor offers an interactive analysis environment that combines visual editing, tree-based structural modeling, and agent-driven query resolution to support accurate and user-friendly table understanding. Experimental results on both benchmark and real-world datasets demonstrate that ST-Raptor outperforms existing methods in both accuracy and usability. The code is available at https://github.com/weAIDB/ST-Raptor, and a demonstration video is available at https://youtu.be/9GDR-94Cau4.

</details>


### [390] [PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://arxiv.org/abs/2602.07187)
*Hanyu Wang,Yuanpu Cao,Lu Lin,Jinghui Chen*

Main category: cs.AI

TL;DR: PreFlect是一种前瞻性反思机制，将LLM智能体的反思从执行后纠正转变为执行前预见，通过批评和优化执行前的计划来提升性能


<details>
  <summary>Details</summary>
Motivation: 现有反思方法本质上是回顾性的：智能体先行动，观察到失败后才尝试恢复。这种后验纠正方式存在效率问题，需要在执行前就能预见和避免错误

Method: 1. 前瞻性反思：在执行前批评和优化智能体计划；2. 从历史轨迹中提取规划错误模式；3. 动态重规划机制：在执行过程中遇到意外偏差时更新计划

Result: 在不同基准测试中，PreFlect显著提升了复杂现实任务中的智能体效用，优于基于反思的基线方法和多个更复杂的智能体架构

Conclusion: 从回顾性反思转向前瞻性反思能有效提升LLM智能体性能，PreFlect通过执行前计划批评和动态重规划机制实现了这一目标

Abstract: Advanced large language model agents typically adopt self-reflection for improving performance, where agents iteratively analyze past actions to correct errors. However, existing reflective approaches are inherently retrospective: agents act, observe failure, and only then attempt to recover. In this work, we introduce PreFlect, a prospective reflection mechanism that shifts the paradigm from post hoc correction to pre-execution foresight by criticizing and refining agent plans before execution. To support grounded prospective reflection, we distill planning errors from historical agent trajectories, capturing recurring success and failure patterns observed across past executions. Furthermore, we complement prospective reflection with a dynamic re-planning mechanism that provides execution-time plan update in case the original plan encounters unexpected deviation. Evaluations on different benchmarks demonstrate that PreFlect significantly improves overall agent utility on complex real-world tasks, outperforming strong reflection-based baselines and several more complex agent architectures. Code will be updated at https://github.com/wwwhy725/PreFlect.

</details>


### [391] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: 研究发现LLM性能主要由计算规模驱动而非专有技术，前沿模型80-90%性能差异源于训练计算量，但非前沿领域专有技术能显著降低达到特定能力所需的计算量


<details>
  <summary>Details</summary>
Motivation: 探究LLM性能提升的主要驱动力：是开发者的专有技术（"秘方"）还是计算规模的扩展，这对于理解AI领导力和能力扩散具有重要意义

Method: 使用2022-2025年间发布的809个模型的训练和基准数据，通过包含发布日期和开发者固定效应的缩放定律回归分析

Result: 1) 前沿模型80-90%性能差异由更高训练计算量解释；2) 非前沿领域专有技术和共享算法进步显著降低达到固定能力阈值所需计算量；3) 某些公司能系统性地更高效生产较小模型；4) 公司内部模型效率存在巨大差异（可达40倍以上）

Conclusion: 前沿AI进步主要由计算规模驱动而非专有技术，但专有技术在非前沿领域能提高效率，公司内部效率差异显著，这对AI领导力和能力扩散具有重要启示

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [392] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

TL;DR: 将大语言模型幻觉检测重新定义为分布外检测问题，提出无需训练、基于单样本的检测方法，在推理任务中取得良好效果


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在问答任务中表现良好，但在需要推理的任务中效果不佳。大语言模型的安全性和可靠性需要更有效的幻觉检测方法

Method: 将语言模型的下一词预测视为分类任务，应用分布外检测技术，并进行适当修改以适应大语言模型的结构特点

Result: 基于分布外检测的方法实现了无需训练、基于单样本的检测器，在推理任务的幻觉检测中取得了较强的准确性

Conclusion: 将幻觉检测重新定义为分布外检测问题，为语言模型安全提供了一个有前景且可扩展的途径

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [393] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

TL;DR: 论文提出将AI安全视为Stackelberg安全博弈问题，通过博弈论框架分析AI开发部署中的对抗性激励问题，将算法对齐与制度监督设计相结合。


<details>
  <summary>Details</summary>
Motivation: 现有AI安全框架主要将对齐视为静态优化问题，忽略了数据收集、模型评估和部署过程中的动态对抗性激励。随着AI系统能力增强，需要战略性地监督参与开发和部署的人类和机构。

Method: 提出基于Stackelberg安全博弈的新视角，将AI监督视为防御者（审计员、评估者、部署者）与攻击者（恶意行为者、未对齐贡献者、最坏情况故障模式）之间的战略互动，为AI生命周期中的激励设计、有限监督能力和对抗性不确定性提供统一框架。

Result: 该框架可应用于：(1) 训练时审计对抗数据/反馈投毒，(2) 有限评审资源下的预部署评估，(3) 对抗环境中的鲁棒多模型部署。展示了博弈论威慑如何使AI监督变得主动、风险感知且抗操纵。

Conclusion: Stackelberg安全博弈为AI安全提供了统一框架，将算法对齐与制度监督设计相结合，强调博弈论威慑可以使AI监督更加主动、风险感知且抗操纵，从而应对AI开发部署中的动态对抗性激励问题。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [394] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: BRIDGE框架通过模型响应学习任务难度尺度，并将其与人类任务完成时间锚定，实现从模型性能推断人类任务完成时间


<details>
  <summary>Details</summary>
Motivation: 现有基于人类任务完成时间标注的方法成本高、噪声大且难以扩展，需要一种可扩展的方法来评估AI系统的真实能力

Method: 使用双参数逻辑项目反应理论模型，从多个基准的模型性能数据中联合估计潜在任务难度和模型能力

Result: 潜在任务难度与人类完成时间的对数呈线性关系，能够仅从模型性能推断新基准的人类任务完成时间，预测前沿模型能力并重现METR的指数缩放结果

Conclusion: BRIDGE提供了一种可扩展的心理测量框架，将基准性能与人类可解释的任务难度度量相结合，为AI系统能力评估提供了新方法

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [395] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: TermiGen是一个端到端管道，通过多智能体迭代生成可验证的终端环境和包含错误纠正循环的专家轨迹，解决了LLM在复杂终端任务执行中的环境稀缺和分布不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个根本限制：1）高保真、可执行的训练环境稀缺——从真实世界仓库合成的环境不够多样化和可扩展，而LLM合成的轨迹存在幻觉问题；2）标准指令调优使用的专家轨迹很少包含较小模型常见的简单错误，导致学生模型无法有效从自身运行时错误中恢复。

Method: TermiGen采用端到端管道：首先通过迭代多智能体精炼循环生成功能有效的任务和Docker容器；然后采用生成器-批评器协议，在轨迹收集过程中主动注入错误，合成富含错误纠正循环的数据。

Result: 在TermiGen生成的数据集上进行微调后，TermiGen-Qwen2.5-Coder-32B在TerminalBench上达到了31.3%的通过率，创造了新的开源权重最优结果，超越了现有基线模型，甚至超过了o4-mini等专有模型。

Conclusion: TermiGen通过合成可验证环境和包含错误恢复的专家轨迹，有效解决了LLM在复杂终端任务执行中的关键挑战，显著提升了模型性能并建立了新的开源权重最优基准。

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [396] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: STEER2ADAPT：一个轻量级框架，通过组合而非从头学习新的导向向量来适配大语言模型，在推理和安全任务上平均提升8.2%


<details>
  <summary>Details</summary>
Motivation: 现有激活导向方法大多依赖每个任务或概念的单一静态方向，在任务变化下不够灵活，且无法处理需要多种协调能力的复杂任务

Method: 提出STEER2ADAPT框架，将任务共享的底层概念维度捕获为可重用的低维语义先验子空间，通过少量示例动态发现基向量的线性组合来适配新任务

Result: 在9个任务和3个模型的推理和安全领域实验中，STEER2ADAPT平均提升8.2%，证明其是数据高效、稳定且透明的推理时适配方法

Conclusion: STEER2ADAPT通过组合现有导向向量而非从头学习，为LLMs提供了一种灵活、高效的适配方法，特别适用于需要多种协调能力的复杂任务

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [397] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 论文开发了一个自适应系统，通过动态选择两种ICAP模式的例题（主动式引导例题和建构式错误例题）来优化认知参与度，比较了BKT和DRL两种自适应方法与基线方法，发现两种自适应策略都能显著提升学生测试成绩。


<details>
  <summary>Details</summary>
Motivation: ICAP框架定义了四种认知参与水平，但个性化学习活动以激发最佳认知参与水平在智能辅导系统中仍然是一个关键挑战。需要开发能够自适应地支持认知参与的系统。

Method: 开发了一个自适应系统，通过动态选择两种ICAP模式的例题来支持认知参与：主动模式的引导例题和建构模式的错误例题。比较了贝叶斯知识追踪（BKT）和深度强化学习（DRL）作为自适应方法，与一个非自适应的基线方法进行对比，在逻辑智能辅导系统中进行实验。

Result: 在113名学生的实验中，两种自适应策略都显著提高了学生在测试问题上的表现。BKT对低先验知识学生的后测成绩提升最大，帮助他们赶上高先验知识同学；而DRL在高先验知识学生中产生了显著更高的后测成绩。

Conclusion: 论文对认知参与和自适应性的复杂相互作用及其对学习成果的影响提供了新的见解，展示了自适应系统如何根据学生先验知识水平优化认知参与度。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [398] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: RAPiD是一个确定性策略提取框架，将预训练的扩散式轨迹规划器蒸馏为高效策略，消除扩散采样过程，实现8倍加速并保持竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式轨迹规划器虽然能很好建模人类驾驶行为的多模态特性，但其依赖迭代随机采样的特性对实时安全关键部署构成挑战，需要更高效的确定性解决方案。

Method: 使用分数正则化策略优化，利用预训练扩散规划器的评分函数作为行为先验来正则化策略学习；通过模仿预测性驾驶员控制器的评论家提供密集的安全监督，超越传统模仿学习。

Result: 在nuPlan场景中实现闭环竞争性性能，相比扩散基线有8倍加速，在interPlan基准测试中达到学习型规划器的最先进泛化能力。

Conclusion: RAPiD成功将扩散式规划器蒸馏为高效确定性策略，在保持性能的同时大幅提升效率，为实时安全关键驾驶部署提供了可行解决方案。

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [399] [W&D:Scaling Parallel Tool Calling for Efficient Deep Research Agents](https://arxiv.org/abs/2602.07359)
*Xiaoqiang Lin,Jun Hao Liew,Silvio Savarese,Junnan Li*

Main category: cs.AI

TL;DR: 该研究提出了"宽深研究智能体"框架，通过并行工具调用实现宽度扩展，在保持单步推理的同时提升研究任务的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体主要通过增加顺序思考和工具调用次数来扩展深度，但并行工具调用（宽度扩展）的潜力尚未充分探索。研究者希望探索同时扩展宽度和深度对智能体性能的影响。

Method: 提出宽深研究智能体框架，利用内在并行工具调用在单个推理步骤内实现有效协调，避免复杂的多智能体编排。研究还探索了各种工具调用调度器来优化并行策略。

Result: 宽度扩展显著提升了深度研究基准测试的性能，同时减少了获得正确答案所需的轮次。使用GPT-5-Medium在BrowseComp上达到62.2%准确率，超过了GPT-5-High报告的54.9%原始结果。

Conclusion: 优化宽度与深度之间的权衡是实现高效深度研究智能体的关键途径。并行工具调用是提升研究智能体性能的有效策略，无需复杂的上下文管理或其他技巧。

Abstract: Deep research agents have emerged as powerful tools for automating complex intellectual tasks through multi-step reasoning and web-based information seeking. While recent efforts have successfully enhanced these agents by scaling depth through increasing the number of sequential thinking and tool calls, the potential of scaling width via parallel tool calling remains largely unexplored. In this work, we propose the Wide and Deep research agent, a framework designed to investigate the behavior and performance of agents when scaling not only depth but also width via parallel tool calling. Unlike existing approaches that rely on complex multi-agent orchestration to parallelize workloads, our method leverages intrinsic parallel tool calling to facilitate effective coordination within a single reasoning step. We demonstrate that scaling width significantly improves performance on deep research benchmarks while reducing the number of turns required to obtain correct answers. Furthermore, we analyze the factors driving these improvements through case studies and explore various tool call schedulers to optimize parallel tool calling strategy. Our findings suggest that optimizing the trade-off between width and depth is a critical pathway toward high-efficiency deep research agents. Notably, without context management or other tricks, we obtain 62.2% accuracy with GPT-5-Medium on BrowseComp, surpassing the original 54.9% reported by GPT-5-High.

</details>


### [400] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: VGAS框架通过生成-选择范式解决VLA模型在少样本适应中的几何模糊问题，使用价值引导的动作块选择来提升轨迹的几何精度和成功率。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在少样本适应新任务时存在不可靠问题，主要源于几何模糊性——语义合理的轨迹可能因几何细节偏差导致执行失败，而有限监督难以解决这种近失候选动作之间的细微差异。

Method: 提出VGAS框架：1) 使用微调VLA作为高召回率提议生成器；2) 引入Q-Chunk-Former作为几何基础Transformer评论家，解决细粒度几何模糊；3) 提出显式几何正则化(EGR)，通过显式塑造判别性价值景观来保持动作排序分辨率，同时缓解少监督下的价值不稳定性。

Result: 实验和理论分析表明，VGAS在有限演示和分布偏移下能持续提高成功率和鲁棒性，有效解决了VLA少样本适应中的几何模糊问题。

Conclusion: VGAS通过生成-选择范式和几何感知的价值引导，为VLA模型的少样本适应提供了一种可靠解决方案，显著提升了在几何敏感任务中的性能表现。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [401] [Progressive Multi-Agent Reasoning for Biological Perturbation Prediction](https://arxiv.org/abs/2602.07408)
*Hyomin Kim,Sang-Yeon Hwang,Jaechang Lim,Yinhua Piao,Yunhak Oh,Woo Youn Kim,Chanyoung Park,Sungsoo Ahn,Junhyeok Jeon*

Main category: cs.AI

TL;DR: PBio-Agent：基于多智能体框架的生物扰动响应预测系统，通过难度感知任务排序和迭代知识精炼，在LINCSQA基准上优于现有基线


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理高维扰动结果时容易混淆，且主要关注单细胞遗传扰动，而药物发现核心的批量细胞化学扰动研究不足

Method: 提出PBio-Agent多智能体框架，包含难度感知任务排序和迭代知识精炼机制，利用相同扰动影响的基因共享因果结构的洞察，通过生物知识图谱增强的专门智能体进行预测

Result: PBio-Agent在LINCSQA和PerturbQA基准测试中优于现有基线，即使较小模型也能无需额外训练预测和解释复杂生物过程

Conclusion: PBio-Agent通过多智能体协作和因果结构共享机制，有效解决了化学扰动下基因调控预测的挑战，为药物发现提供了新工具

Abstract: Predicting gene regulation responses to biological perturbations requires reasoning about underlying biological causalities. While large language models (LLMs) show promise for such tasks, they are often overwhelmed by the entangled nature of high-dimensional perturbation results. Moreover, recent works have primarily focused on genetic perturbations in single-cell experiments, leaving bulk-cell chemical perturbations, which is central to drug discovery, largely unexplored. Motivated by this, we present LINCSQA, a novel benchmark for predicting target gene regulation under complex chemical perturbations in bulk-cell environments. We further propose PBio-Agent, a multi-agent framework that integrates difficulty-aware task sequencing with iterative knowledge refinement. Our key insight is that genes affected by the same perturbation share causal structure, allowing confidently predicted genes to contextualize more challenging cases. The framework employs specialized agents enriched with biological knowledge graphs, while a synthesis agent integrates outputs and specialized judges ensure logical coherence. PBio-Agent outperforms existing baselines on both LINCSQA and PerturbQA, enabling even smaller models to predict and explain complex biological processes without additional training.

</details>


### [402] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

TL;DR: 论文探讨聊天机器人信任问题，指出用户信任常源于行为机制而非系统可信度，建议将聊天机器人视为销售员而非助手，强调需要区分心理信任形成与规范可信度


<details>
  <summary>Details</summary>
Motivation: 随着聊天机器人模糊自动化系统与人类对话的界限，需要更仔细地审视这些系统的信任基础。监管和政策框架倾向于从规范角度定义信任，但用户对聊天机器人的信任往往源于行为机制，这种信任通常不是通过证明可信度获得的，而是通过利用认知偏见影响用户行为的设计选择塑造的。

Method: 基于观察提出重新定义聊天机器人的框架：不是作为伴侣或助手，而是作为部署组织确定目标的高度熟练的销售人员。分析竞争性"信任"概念共存于同一术语下的问题，区分心理信任形成与规范可信度。

Result: 研究发现用户对聊天机器人的信任往往不是通过证明可信度获得的，而是通过利用认知偏见影响用户行为的设计选择塑造的。这种信任形成机制掩盖了心理信任形成与规范可信度之间的重要区别。

Conclusion: 需要进一步研究和更强有力的支持机制，帮助用户适当校准对对话AI系统的信任。解决这一差距需要区分心理信任形成与规范可信度，并建立更清晰的信任框架。

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [403] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 论文研究了人类与AI交互中的"信念卸载"现象，即人们将形成和维持信念的过程外包给AI系统，这对他们的行为和信念体系产生下游影响。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地将LLM聊天机器人作为思维伙伴使用，这可能导致认知卸载，在过度依赖的情况下对认知技能产生不利影响。研究者希望定义和调查人类-AI交互中的特定认知卸载类型——"信念卸载"。

Method: 结合哲学、心理学和计算机科学研究，澄清信念卸载发生的边界条件，并提供信念卸载的描述性分类及其规范含义。

Result: 提出了信念卸载的概念框架和分类体系，明确了其发生的条件，并分析了这种现象的规范含义。

Conclusion: 信念卸载是人类-AI交互中的重要现象，具有显著的行为和认知影响。未来工作需要评估信念卸载的潜在可能性和后果。

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [404] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 本文提出了一种新的POMDP类别——后验确定性POMDP，证明了对于这类模型，可达目标状态的最大概率可以近似计算到任意精度，突破了传统POMDP计算难度的限制。


<details>
  <summary>Details</summary>
Motivation: 传统POMDP在验证和综合问题上存在不可判定或难以计算的问题，特别是Madani等人的经典结果表明无法计算或近似可达目标状态的最大概率。这与完全可观测的MDP形成鲜明对比，后者可以在多项式时间内计算可达性值。

Method: 引入后验确定性POMDP这一新类别，定义为下一个状态可以由当前状态、采取的动作和接收的观测唯一确定的POMDP。这种性质意味着一旦真实状态已知，它将永远保持已知。

Result: 证明了对于后验确定性POMDP，可达给定状态集的最大概率可以近似到任意精度。这类模型包含了所有MDP，并涵盖了如Tiger POMDP等经典非平凡例子，成为已知最大的可近似计算可达性值的POMDP类别之一。

Conclusion: 后验确定性POMDP提供了一个有前景的框架，能够在保持POMDP表达能力的同时，实现可达性值的可近似计算，为顺序决策制定中的不确定性建模开辟了新的可能性。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [405] [Learning the Value Systems of Societies with Preference-based Multi-objective Reinforcement Learning](https://arxiv.org/abs/2602.08835)
*Andrés Holgado-Sánchez,Peter Vamplew,Richard Dazeley,Sascha Ossowski,Holger Billhardt*

Main category: cs.AI

TL;DR: 该论文提出了一种基于聚类和偏好多目标强化学习的算法，用于在马尔可夫决策过程中学习社会智能体的价值对齐模型和价值系统。


<details>
  <summary>Details</summary>
Motivation: 价值感知AI需要识别人类价值观并适应不同用户的价值系统，但现有方法存在操作化困难、需要手动设计特征、缺乏基于价值的可解释性以及对多样化用户偏好的适应性不足等问题。

Method: 提出基于聚类和偏好多目标强化学习的算法，联合学习社会衍生的价值对齐模型和代表不同用户群体的价值系统集合，每个聚类包含代表成员价值偏好的价值系统和反映该价值系统行为的近似帕累托最优策略。

Result: 在两个包含人类价值的MDP上，与最先进的PbMORL算法和基线方法进行了评估比较。

Conclusion: 该方法能够学习社会智能体的价值对齐模型和价值系统，解决了现有方法在价值操作化、可解释性和适应性方面的局限性。

Abstract: Value-aware AI should recognise human values and adapt to the value systems (value-based preferences) of different users. This requires operationalization of values, which can be prone to misspecification. The social nature of values demands their representation to adhere to multiple users while value systems are diverse, yet exhibit patterns among groups. In sequential decision making, efforts have been made towards personalization for different goals or values from demonstrations of diverse agents. However, these approaches demand manually designed features or lack value-based interpretability and/or adaptability to diverse user preferences.
  We propose algorithms for learning models of value alignment and value systems for a society of agents in Markov Decision Processes (MDPs), based on clustering and preference-based multi-objective reinforcement learning (PbMORL). We jointly learn socially-derived value alignment models (groundings) and a set of value systems that concisely represent different groups of users (clusters) in a society. Each cluster consists of a value system representing the value-based preferences of its members and an approximately Pareto-optimal policy that reflects behaviours aligned with this value system. We evaluate our method against a state-of-the-art PbMORL algorithm and baselines on two MDPs with human values.

</details>


### [406] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 本文提出了一种结合知识图谱和多智能体推理的框架，用于寻找PFAS（全氟和多氟烷基物质）的可持续替代品，通过分布式专业化和关系推理扩展材料设计空间。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，创新需要整合从分子化学到机械性能的各种概念，但人类或单智能体LLM难以应对信息洪流，且后者容易出现幻觉。PFAS化学物质面临严格监管，急需寻找可持续替代品。

Method: 引入基于大规模知识图谱的多智能体框架，包含问题分解、证据检索、设计参数提取和图遍历等专业化智能体，通过定制图遍历策略在利用性搜索和探索性搜索之间切换。

Result: 消融研究表明完整多智能体流水线优于单次提示，系统能够发现不同知识领域之间的潜在联系。以生物医学导管为例，框架生成了平衡摩擦学性能、热稳定性、化学抗性和生物相容性的可持续PFAS-free替代方案。

Conclusion: 该工作建立了结合知识图谱与多智能体推理的框架，扩展了材料设计空间，展示了多个初始设计候选方案，证明了分布式专业化和关系推理在加速科学发现中的价值。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [407] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: MSP-LLM：一个统一的LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题，通过引入离散材料类别作为中间决策变量，显著提升了材料合成规划的性能。


<details>
  <summary>Details</summary>
Motivation: 材料合成规划是AI驱动材料发现中的关键瓶颈，现有方法仅解决孤立子任务，缺乏统一的解决方案。需要开发能够同时处理前驱体选择和合成操作序列设计的综合方法。

Method: 提出MSP-LLM框架，将材料合成规划分解为前驱体预测和合成操作预测两个子问题。引入离散材料类别作为中间决策变量，构建化学一致的决策链。在合成操作预测中，采用分层前驱体类型作为归纳偏置，并使用显式条件策略在自回归解码状态中保留前驱体相关信息。

Result: 实验表明MSP-LLM在前驱体预测、合成操作预测以及完整的材料合成规划任务上均优于现有方法，证明了该框架在材料发现中的有效性和可扩展性。

Conclusion: MSP-LLM提供了一个统一、有效的材料合成规划框架，能够加速实际材料发现过程，解决了该领域长期存在的瓶颈问题。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [408] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: EventCast是一个将未来事件知识整合到时间序列预测中的模块化框架，专门解决电商在高影响时期（如闪购、节假日活动）的需求预测问题。


<details>
  <summary>Details</summary>
Motivation: 现有预测系统在高影响时期（如闪购、节假日活动、政策干预）经常失效，因为这些时期需求模式会发生突然且不可预测的变化。传统方法要么忽略未来干预，要么直接使用大语言模型进行数值预测，都存在局限性。

Method: EventCast利用大语言模型进行事件驱动推理，将非结构化业务数据（如活动、节假日安排、卖家激励）转换为可解释的文本摘要，然后通过双塔架构将这些摘要与历史需求特征融合，实现准确、可解释且可扩展的预测。

Result: 在4个国家160个地区10个月的真实电商场景中，EventCast相比无事件知识的变体在MAE和MSE上分别提升了86.9%和97.7%；相比最佳工业基线，在事件驱动时期MAE降低了57.0%，MSE降低了83.3%。

Conclusion: EventCast自2025年3月起已部署到真实工业管道中，为动态电商环境中的运营决策提供了实用解决方案，通过整合未来事件知识显著提升了需求预测的准确性和鲁棒性。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [409] [SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities](https://arxiv.org/abs/2602.08254)
*Arman Aghaee,Sepehr Asgarian,Jouhyun Jeon*

Main category: cs.AI

TL;DR: SynthAgent是一个多智能体系统框架，用于模拟肥胖症合并精神障碍患者，通过整合临床数据和文献构建个性化虚拟患者，模拟疾病进展和治疗反应，GPT-5和Claude 4.5 Sonnet在该框架中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界数据碎片化、偏见和隐私限制的问题，为研究复杂疾病提供高保真患者模拟方法。

Method: 开发SynthAgent多智能体系统框架，整合医保索赔数据、人口调查和患者中心文献，构建具有人格特质的个性化虚拟患者，通过自主智能体交互模拟疾病进展、治疗反应和生活管理。

Result: 评估100多个生成的患者显示，GPT-5和Claude 4.5 Sonnet作为核心引擎在MAS框架中达到最高保真度，优于Gemini 2.5 Pro和DeepSeek-R1。

Conclusion: SynthAgent提供了一个可扩展且保护隐私的框架，用于探索医学和心理领域的患者旅程、行为动态和决策过程。

Abstract: Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.

</details>


### [410] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: SleepMaMi是一个睡眠基础模型，采用分层双编码器设计，能够同时建模整夜睡眠的宏观结构和生物信号的微观形态，在多种下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前睡眠医学主要依赖任务特定模型，这些模型专注于局部微观结构特征，忽略了多导睡眠图（PSG）的丰富多模态上下文，也无法捕捉整夜睡眠的全局宏观结构。

Method: 采用分层双编码器设计：宏观编码器通过人口统计学引导的对比学习建模整夜时间依赖关系；微观编码器通过混合掩码自编码器和多模态对比目标优化。在超过20,000个PSG记录（158K小时）上进行预训练。

Result: SleepMaMi在多样化的下游任务中优于现有基础模型，展示了卓越的泛化能力和标签高效的临床睡眠分析适应能力。

Conclusion: SleepMaMi成功解决了睡眠医学中任务特定模型的局限性，能够同时掌握长时间睡眠架构和细粒度信号形态，为临床睡眠分析提供了强大的基础模型。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [411] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: TabRAG框架通过视觉-文本基础模型检索候选表格图像，MLLM细粒度重排序，再使用MLLM进行推理，显著提升大规模表格图像集合的查询回答性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中表格数据常以图像形式存在（如财务报表、手写记录、文档扫描），现有MLLM方法通常假设相关表格已准备好，但实际场景需要从大规模表格图像集合中识别和推理相关表格来回答用户查询。

Method: TabRAG框架：1）使用联合训练的视觉-文本基础模型检索候选表格；2）利用MLLM对候选表格进行细粒度重排序；3）使用MLLM对选定表格进行推理生成答案。

Result: 在包含88,161个训练样本和9,819个测试样本的新建数据集上，TabRAG在检索召回率上比现有方法提升7.0%，在答案准确率上提升6.1%。

Conclusion: TabRAG为现实世界表格理解任务提供了实用解决方案，能够有效处理大规模表格图像集合的查询回答问题。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [412] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 本文提出了一个基于统一基础本体论的信任参考本体论（ONTrust），旨在为信任概念提供坚实的本体论基础，支持信息建模、自动推理、信息集成和语义互操作性任务。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和区块链等新技术的发展，信任在系统设计中的重要性日益凸显。这些技术的采用很大程度上取决于信任，但现有研究缺乏对信任概念的统一定义和形式化表征，难以支持人机理解和互操作。

Method: 基于统一基础本体论（UFO），使用OntoUML语言开发了信任参考本体论（ONTrust），形式化表征信任概念及其不同类型，描述影响信任的因素，解释信任关系中风险的产生机制，并通过两个文献案例研究进行验证。

Result: ONTrust本体论已应用于多个领域：概念建模和企业架构设计、语言评估与（重新）设计、信任管理、需求工程，以及在情感人机协作背景下的可信人工智能（AI）。

Conclusion: ONTrust为信任概念提供了坚实的本体论基础，能够支持人机理解和互操作，有助于构建可信系统，促进新技术在改善产品服务和提升个体集体福祉方面的应用。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [413] [Geo-Code: A Code Framework for Reverse Code Generation from Geometric Images Based on Two-Stage Multi-Agent Evolution](https://arxiv.org/abs/2602.07749)
*Zhenyu Wu,Yanxi Long,Jian Li,Hua Huang*

Main category: cs.AI

TL;DR: Geo-coder是一个基于多智能体系统的几何图像逆向编程框架，通过像素级锚定和度量驱动代码演化实现精确几何重建，在几何重建精度和视觉一致性方面显著领先。


<details>
  <summary>Details</summary>
Motivation: 当前逆向图形方法在重建复杂几何细节时面临巨大挑战，往往导致关键几何约束丢失或结构失真，需要新的方法来准确重建几何图像并保持核心几何语义。

Method: 提出基于多智能体系统的逆向编程框架Geo-coder，将过程解耦为两个阶段：1) 通过视觉算子和大模型的互补优势实现像素坐标和视觉属性的精确捕获；2) 引入合成-渲染-验证闭环，通过双向视觉反馈驱动代码自校正。

Result: 实验表明Geo-coder在几何重建精度和视觉一致性方面取得显著领先。重建图像在多模态推理任务中表现出与原始图像相当的性能，验证了框架的鲁棒性。开源了包含1500多个样本的Geo-coder数据集和GeocodeLM模型。

Conclusion: Geo-coder通过创新的逆向编程框架有效解决了复杂几何细节重建的瓶颈问题，为后续研究提供了坚实的数据和模型基础，推动了多模态推理能力的发展。

Abstract: Program code serves as a bridge linking vision and logic, providing a feasible supervisory approach for enhancing the multimodal reasoning capability of large models through geometric operations such as auxiliary line construction and perspective transformation. Nevertheless, current inverse graphics methods face tremendous challenges in accurately reconstructing complex geometric details, which often results in the loss of key geometric constraints or structural distortion. To address this bottleneck, we propose Geo-coder -- the first inverse programming framework for geometric images based on a multi-agent system. Our method innovatively decouples the process into geometric modeling via pixel-wise anchoring and metric-driven code evolution: Stage 1 leverages the complementary advantages of visual operators and large models to achieve precise capture of pixel coordinates and visual attributes; Stage 2 introduces a synthesis-rendering-validation closed loop, where bidirectional visual feedback drives the self-correction of code. Extensive experiments demonstrate that Geo-coder achieves a substantial lead in both geometric reconstruction accuracy and visual consistency. Notably, by effectively preserving the core geometric semantics, the images reconstructed with our method exhibit equivalent performance to the original ones in multimodal reasoning tasks, which fully validates the robustness of the framework. Finally, to further reduce research costs, we have open-sourced the Geo-coder dataset constructed on the GeoCode framework, which contains more than 1,500 samples. On this basis, we have also open-sourced the GeocodeLM model, laying a solid data and model foundation for subsequent research in this field.

</details>


### [414] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 研究调查本科生对AI评分系统的看法，发现学生对AI缺乏情境理解和个性化表示担忧，建议AI应作为人类监督下的补充工具。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解学生对AI评分系统的看法，特别是关注公平性、信任度、一致性和透明度等伦理维度，为AI在教育评估中的合理应用提供依据。

Method: 采用基于Jobin（2019）伦理原则框架的研究设计，在本科计算机科学课程中对27名学生进行调查研究，比较AI生成的反馈与原始人工评分反馈。

Result: 研究发现学生对AI评分系统存在担忧，主要关注AI缺乏情境理解和个性化能力。学生认为AI系统应反映人类判断、灵活性和同理心。

Conclusion: 建议开发公平可信的AI评分系统应体现人类判断、灵活性和同理心，作为人类监督下的补充工具，为设计人性化的AI学习环境提供原则。

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [415] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap是一个多智能体系统，在AndroidWorld基准测试中实现了100%成功率，首次完全解决所有116个任务，超越了人类80%的性能。


<details>
  <summary>Details</summary>
Motivation: 解决单智能体架构在移动设备任务执行中的失败问题，包括：混合推理轨迹导致的上下文污染、智能体未检测到的静默文本输入失败、以及无逃脱的重复动作循环。

Method: 通过针对性机制解决每个失败点：六个专门化智能体之间的认知分离、基于设备状态的文本输入确定性后验证、以及检测循环并触发策略改变的元认知推理。

Result: 在AndroidWorld基准测试中达到100%成功率，首次完全解决所有116个任务，超越人类80%的性能。消融实验显示：多智能体分解贡献+21分，验证执行贡献+7分，元认知贡献+9分。

Conclusion: Minitap通过多智能体架构、确定性验证和元认知推理的组合，成功解决了移动设备任务执行的挑战，并作为开源软件发布。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [416] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 提出Data Darwinism十级分类法，通过数据-模型协同进化提升基础模型性能，在科学文献上构建Darwin-Science语料库，验证了高级数据处理能显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 数据质量决定基础模型性能，但缺乏系统化的数据处理框架。需要建立数据与模型协同进化的理论框架，让高级模型为下一代系统生成更优质的数据。

Method: 提出Data Darwinism十级分类法(L0-L9)，在科学文献上构建Darwin-Science语料库(900B tokens，L0-L5)。使用前沿LLM进行L4(生成精炼)和L5(认知补全)处理，填补原始科学文本的学习能力差距。从头训练daVinci-origin-3B/7B模型作为无污染基线，然后进行600B tokens的继续预训练。

Result: Darwin-Science模型在20多个基准测试中比基线提升+2.12(3B)和+2.95(7B)分，在领域对齐任务上提升+5.60和+8.40分。系统性地推进到L5处理带来+1.36的总增益，证实高级数据处理能释放潜在数据价值。

Conclusion: Data Darwinism框架有效，数据-模型协同进化能显著提升模型性能。高级数据处理(特别是L4和L5)能填补原始文本的学习能力差距，释放数据的潜在价值。开源Darwin-Science语料库和daVinci-origin模型以支持原则性的协同进化发展。

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [417] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime是一个通过数据合成、数据调度和强化学习训练来定制LLM进行时间序列推理的框架，使小型模型（3B、4B）能够达到或超过大型专有LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在推理能力方面取得了进展，但将其应用于时间序列任务仍处于早期阶段，主要受到三个限制：缺乏精心策划的时间序列CoT训练数据、数据调度不足导致的数据效率低下，以及缺乏专门针对时间序列CoT数据利用的RL算法。

Method: 1. 数据合成管道：构建具有过程可验证注释的TS-文本多模态数据集；2. 数据调度机制：根据难度层次和任务分类原则安排训练样本；3. 两阶段强化微调：利用可验证的过程级CoT数据，设计细粒度、多目标的奖励机制。

Result: VeriTime显著提升了LLM在各种时间序列推理任务上的性能。值得注意的是，它使紧凑的3B、4B模型能够达到与大型专有LLM相当甚至超越的推理能力。

Conclusion: VeriTime通过创新的数据合成、调度和强化学习训练方法，成功解决了LLM在时间序列推理中的关键挑战，为小型模型在复杂时间序列任务上实现高性能推理提供了有效框架。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [418] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: LQA是一个轻量化的量化自适应框架，用于在边缘设备上部署视觉语言模型，通过模态感知量化策略和无梯度测试时适应，在资源受限环境下实现鲁棒部署。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署视觉语言模型面临资源限制和分布偏移导致的性能下降问题。现有的测试时适应方法资源消耗过大，不适合在设备端部署。

Method: 提出LQA框架，包含选择性混合量化（SHQ）策略和量化无梯度适应机制，结合模态感知量化与无梯度测试时适应，实现轻量高效的边缘部署。

Result: 在合成和真实世界分布偏移实验中，LQA将整体适应性能提升4.5%，内存使用低于全精度模型，显著优于基于梯度的TTA方法，内存使用降低达19.9倍。

Conclusion: LQA为边缘设备上的视觉语言模型部署提供了实用路径，实现了鲁棒、隐私保护且高效的部署方案。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [419] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 微调大型语言模型在狭窄的有害数据集上可能导致其出现紧急错位，在多种无关场景中给出刻板的"邪恶"回应。专家调查未能预测这一结果，突显了我们对LLM学习和泛化归纳偏好的理解不足。研究发现通用解决方案比狭窄解决方案更稳定高效，并识别出通用错位的线性表示用于监控和缓解。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在狭窄有害数据集上微调后出现的紧急错位现象，探索LLM学习和泛化的归纳偏好机制，理解为什么模型会从狭窄训练中泛化出普遍的有害行为。

Method: 使用紧急错位作为案例研究，分析不同微调收敛到相同的通用错位线性表示。通过引入KL散度损失学习狭窄解决方案的线性表示，比较两种表示的特性，包括损失值、鲁棒性和在预训练分布中的影响力。

Result: 研究发现通用错位解决方案比狭窄解决方案具有更低的损失、更强的扰动鲁棒性，并且在预训练分布中更具影响力。识别出通用错位的具体线性表示可用于监控和缓解。

Conclusion: 这项工作为研究LLM泛化的归纳偏好提供了详细案例和初步指标，开源了所有代码、数据集和模型微调，为监控和缓解模型错位提供了具体工具。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [420] [ToolSelf: Unifying Task Execution and Self-Reconfiguration via Tool-Driven Intrinsic Adaptation](https://arxiv.org/abs/2602.07883)
*Jingqi Zhou,Sheng Wang,DeZhao Deng,Junwen Lu,Junwei Su,Qintong Li,Jiahui Gao,Hao Wu,Jiyue Jiang,Lingpeng Kong,Chuan Wu*

Main category: cs.AI

TL;DR: ToolSelf：一种新型智能体范式，将配置更新抽象为可调用工具，实现任务执行与自我调整的统一，使智能体能够根据任务进展自主更新子目标、上下文和策略，从被动执行者转变为任务与自我的双重管理者。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体系统在处理复杂长时程任务时，其行为配置在任务执行前就已固定，无法适应动态变化的任务环境。现有方法依赖人工编排或启发式修补，泛化能力差且优化碎片化，需要突破这些限制。

Method: 提出ToolSelf范式，将配置更新抽象为可调用工具，统一任务执行和自我调整到一个动作空间。开发配置感知两阶段训练（CAT），结合拒绝采样微调和轨迹级强化学习，内化这种元能力。

Result: 在多样化基准测试中，ToolSelf能够媲美专门化工作流，同时在新任务上表现出良好的泛化能力，实现了平均24.1%的性能提升。

Conclusion: ToolSelf通过工具驱动的运行时自我重配置，实现了从外部规则到内在参数的范式转变，为真正自适应的智能体系统开辟了新路径。

Abstract: Agentic systems powered by Large Language Models (LLMs) have demonstrated remarkable potential in tackling complex, long-horizon tasks. However, their efficacy is fundamentally constrained by static configurations governing agent behaviors, which are fixed prior to execution and fail to adapt to evolving task dynamics. Existing approaches, relying on manual orchestration or heuristic-based patches, often struggle with poor generalization and fragmented optimization. To transcend these limitations, we propose ToolSelf, a novel paradigm enabling tool-driven runtime self-reconfiguration. By abstracting configuration updates as a callable tool, ToolSelf unifies task execution and self-adjustment into a single action space, achieving a phase transition from external rules to intrinsic parameters. Agents can thereby autonomously update their sub-goals and context based on task progression, and correspondingly adapt their strategy and toolbox, transforming from passive executors into dual managers of both task and self. We further devise Configuration-Aware Two-stage Training (CAT), combining rejection sampling fine-tuning with trajectory-level reinforcement learning to internalize this meta-capability. Extensive experiments across diverse benchmarks demonstrate that ToolSelf rivals specialized workflows while generalizing to novel tasks, achieving a 24.1% average performance gain and illuminating a path toward truly self-adaptive agents.

</details>


### [421] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: MemFly是一个基于信息瓶颈原则的LLM记忆框架，通过梯度自由优化器构建分层记忆结构，结合混合检索机制提升长期记忆性能。


<details>
  <summary>Details</summary>
Motivation: 现有长期记忆框架面临一个根本困境：既要高效压缩冗余信息，又要保持精确检索以支持下游任务。这种压缩与精确检索之间的权衡问题需要解决。

Method: 基于信息瓶颈原则，通过梯度自由优化器最小化压缩熵同时最大化相关熵，构建分层记忆结构。开发混合检索机制，整合语义、符号和拓扑检索路径，并采用迭代精炼处理复杂多跳查询。

Result: 综合实验表明，MemFly在记忆连贯性、响应保真度和准确性方面显著优于现有最先进的基线方法。

Conclusion: MemFly框架成功解决了长期记忆中压缩效率与检索精度之间的权衡问题，通过信息瓶颈原则和混合检索机制实现了更优的记忆管理性能。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [422] [MedCoG: Maximizing LLM Inference Density in Medical Reasoning via Meta-Cognitive Regulation](https://arxiv.org/abs/2602.07905)
*Yu Zhao,Hao Guan,Yongcheng Jing,Ying Zhang,Dacheng Tao*

Main category: cs.AI

TL;DR: MedCoG：基于知识图谱的医疗元认知智能体，通过元认知评估动态调节知识使用，在避免盲目扩展的同时提升推理准确率，实现5.5倍推理密度提升


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂医疗推理中表现出潜力，但面临推理扩展定律下的收益递减问题。现有研究通过添加各种知识来增强LLMs，但额外成本转化为准确率的有效性尚不明确。

Method: 提出MedCoG（Medical Meta-Cognition Agent with Knowledge Graph），通过元认知评估（任务复杂度、熟悉度、知识密度）动态调节程序性、情景性和事实性知识的使用。采用以LLM为中心的按需推理，避免盲目扩展并过滤干扰知识。

Result: 在五个困难医疗基准测试集上验证了MedCoG的有效性和效率，实现了5.5倍的推理密度提升。Oracle研究突显了元认知调节的巨大潜力。

Conclusion: 元认知调节能够有效缓解扩展定律问题，通过动态知识调节实现成本降低和准确率提升，为医疗推理系统提供了高效解决方案。

Abstract: Large Language Models (LLMs) have shown strong potential in complex medical reasoning yet face diminishing gains under inference scaling laws. While existing studies augment LLMs with various knowledge types, it remains unclear how effectively the additional costs translate into accuracy. In this paper, we explore how meta-cognition of LLMs, i.e., their self-awareness of their own knowledge states, can regulate the reasoning process. Specifically, we propose MedCoG, a Medical Meta-Cognition Agent with Knowledge Graph, where the meta-cognitive assessments of task complexity, familiarity, and knowledge density dynamically regulate utilization of procedural, episodic, and factual knowledge. The LLM-centric on-demand reasoning aims to mitigate scaling laws by (1) reducing costs via avoiding indiscriminate scaling, (2) improving accuracy via filtering out distractive knowledge. To validate this, we empirically characterize the scaling curve and introduce inference density to quantify inference efficiency, defined as the ratio of theoretically effective cost to actual cost. Experiments demonstrate the effectiveness and efficiency of MedCoG on five hard sets of medical benchmarks, yielding 5.5x inference density. Furthermore, the Oracle study highlights the significant potential of meta-cognitive regulation.

</details>


### [423] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: TRUST是一种新颖的扩散模型概念遗忘方法，通过动态估计目标概念神经元并进行选择性微调，结合Hessian正则化，有效移除有害内容生成能力，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 文本引导扩散模型易被利用生成有害内容，现有概念遗忘方法要么只能处理单个概念，要么需要全模型微调计算成本高，而静态概念定位方法效果不佳。

Method: 提出TRUST方法：1) 动态估计目标概念神经元；2) 通过选择性微调进行概念遗忘；3) 使用Hessian正则化增强鲁棒性。

Result: 实验表明TRUST能有效对抗对抗性提示，显著保持生成质量，比SOTA方法更快，并能遗忘单个概念、概念组合和条件概念，无需特定正则化。

Conclusion: TRUST提供了一种高效、鲁棒的概念遗忘解决方案，解决了现有方法在计算成本、鲁棒性和灵活性方面的局限性。

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [424] [IV Co-Scientist: Multi-Agent LLM Framework for Causal Instrumental Variable Discovery](https://arxiv.org/abs/2602.07943)
*Ivaxi Sheth,Zhijing Jin,Bryan Wilder,Dominik Janzing,Mario Fritz*

Main category: cs.AI

TL;DR: LLMs能够帮助发现有效的工具变量，通过多智能体系统IV Co-Scientist提出、批判和优化工具变量选择


<details>
  <summary>Details</summary>
Motivation: 工具变量识别需要跨学科知识、创造力和上下文理解，是一项非平凡任务。本文研究大型语言模型是否能帮助完成这一任务。

Method: 采用两阶段评估框架：首先测试LLMs能否从文献中恢复已确立的工具变量；其次评估LLMs能否识别和避免已被实证或理论否定的工具变量。基于此引入IV Co-Scientist多智能体系统，并提出无真实值情况下的统计检验方法。

Result: 结果显示LLMs有潜力从大型观测数据库中识别有效的工具变量，IV Co-Scientist系统能够提出、批判和优化工具变量选择。

Conclusion: LLMs在工具变量发现方面具有潜力，能够辅助研究人员进行因果推断中的工具变量识别任务。

Abstract: In the presence of confounding between an endogenous variable and the outcome, instrumental variables (IVs) are used to isolate the causal effect of the endogenous variable. Identifying valid instruments requires interdisciplinary knowledge, creativity, and contextual understanding, making it a non-trivial task. In this paper, we investigate whether large language models (LLMs) can aid in this task. We perform a two-stage evaluation framework. First, we test whether LLMs can recover well-established instruments from the literature, assessing their ability to replicate standard reasoning. Second, we evaluate whether LLMs can identify and avoid instruments that have been empirically or theoretically discredited. Building on these results, we introduce IV Co-Scientist, a multi-agent system that proposes, critiques, and refines IVs for a given treatment-outcome pair. We also introduce a statistical test to contextualize consistency in the absence of ground truth. Our results show the potential of LLMs to discover valid instrumental variables from a large observational database.

</details>


### [425] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: EXPERIGEN是一个端到端的科学发现框架，通过生成器-实验者的两阶段搜索，在多个领域中发现比现有方法多2-4倍的统计显著假设，预测性能提升7-17%，并成功通过专家评审和真实世界A/B测试验证。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动社会科学研究过程缓慢，依赖观察、假设生成和实验验证的迭代循环。现有数据驱动方法虽然能加速部分过程，但无法支持端到端的科学发现。需要一种能够完整支持科学发现过程的框架。

Method: 提出EXPERIGEN框架，采用受贝叶斯优化启发的两阶段搜索：生成器提出候选假设，实验者进行实证评估。该框架支持多模态和关系数据集，并通过专家评审和真实世界A/B测试验证假设质量。

Result: 在多个领域中，EXPERIGEN发现的统计显著假设数量是现有方法的2-4倍，预测性能提升7-17%。专家评审显示88%的假设具有中等或强新颖性，70%被认为有影响力且值得进一步研究。真实世界A/B测试获得p<1e-6的统计显著结果，效应大小达344%。

Conclusion: EXPERIGEN框架能够有效加速端到端的科学发现过程，不仅产生统计上显著的假设，还能生成新颖、有影响力且可操作的假设，为数据驱动社会科学研究提供了新的方法论工具。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [426] [Small Agent Group is the Future of Digital Health](https://arxiv.org/abs/2602.08013)
*Yuqiao Meng,Luoxi Tang,Dazheng Zhang,Rafael Brens,Elvys J. Romero,Nancy Guo,Safa Elkefi,Zhaohan Xi*

Main category: cs.AI

TL;DR: 本文提出小型智能体群（SAG）作为替代传统大语言模型扩展范式的方法，通过协同推理在临床环境中实现更好的效果、可靠性和部署效率平衡


<details>
  <summary>Details</summary>
Motivation: 当前数字健康领域过度依赖"规模优先"的大语言模型扩展范式，但临床实际需求不仅包括效果，还需要可靠性和合理的部署成本。临床决策本质上是协作性的，因此需要挑战单一模型扩展的范式

Method: 提出小型智能体群（SAG）方法，从单一模型智能转向集体专业知识，通过协作审议过程分配推理、循证分析和关键审核任务

Result: SAG在效果、可靠性和部署成本等多个临床指标上表现优于单一大型模型，无论是否使用额外优化或检索增强生成技术。协同推理可以替代模型参数增长

Conclusion: SAG为数字健康提供了可扩展的解决方案，能更好地平衡效果、可靠性和部署效率，挑战了传统的"规模优先"范式

Abstract: The rapid adoption of large language models (LLMs) in digital health has been driven by a "scaling-first" philosophy, i.e., the assumption that clinical intelligence increases with model size and data. However, real-world clinical needs include not only effectiveness, but also reliability and reasonable deployment cost. Since clinical decision-making is inherently collaborative, we challenge the monolithic scaling paradigm and ask whether a Small Agent Group (SAG) can support better clinical reasoning. SAG shifts from single-model intelligence to collective expertise by distributing reasoning, evidence-based analysis, and critical audit through a collaborative deliberation process. To assess the clinical utility of SAG, we conduct extensive evaluations using diverse clinical metrics spanning effectiveness, reliability, and deployment cost. Our results show that SAG achieves superior performance compared to a single giant model, both with and without additional optimization or retrieval-augmented generation. These findings suggest that the synergistic reasoning represented by SAG can substitute for model parameter growth in clinical settings. Overall, SAG offers a scalable solution to digital health that better balances effectiveness, reliability, and deployment efficiency.

</details>


### [427] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lijie Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: Free()LM通过引入自我遗忘机制解决推理模型过度思考导致性能下降的问题，在保持紧凑无噪声状态的同时提升各种规模模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型存在一个关键悖论：过多的思考标记往往降低性能而非提升。作者认为这是由于标准LLM作为"仅分配内存"引擎，持续累积有效和冗余步骤，缺乏修剪过时信息的机制。

Method: 提出Free()LM模型，通过Free-Module（即插即用的LoRA适配器）引入内在的自我遗忘能力。模型在推理模式和清理模式之间迭代切换，动态识别并修剪无用的上下文块，保持紧凑无噪声状态。

Result: Free()LM在所有模型规模（8B到685B）上都提供了一致的改进，平均比顶级推理基线提升3.3%，在IMOanswerBench上建立了新的SOTA。在长时程任务中，标准Qwen3-235B-A22B模型完全崩溃（0%准确率），而Free()LM将性能恢复到50%。

Conclusion: 可持续智能需要遗忘的自由与思考的能力同等重要。自我遗忘机制是解决推理模型过度思考问题的有效方法。

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [428] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 论文提出了一个五层生物安全数据等级（BDL）框架，用于根据病原体数据在训练AI模型时可能带来的生物安全风险进行分类，并为每个等级提出相应的技术限制措施，旨在通过数据控制减少生物AI能力扩散的风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI在生物学领域的广泛应用，训练数据成为AI模型能力的关键决定因素。某些类型的生物数据（如病原体相关数据）可能被用于开发生物武器等有害应用。国际研究团体已呼吁实施数据控制，但缺乏系统化的分类框架。因此需要建立一个基于风险的数据分类体系，以设计有效的控制措施。

Method: 提出了五层生物安全数据等级（BDL）框架，根据不同类型病原体数据在训练AI模型时可能带来的风险程度进行分类。每个等级包含特定的数据类型，并基于其预期对关注能力的贡献程度进行评估。针对每个BDL层级，设计了相应的技术限制措施。此外，还为新创建的双用途病原体数据提出了创新的治理框架。

Result: 建立了一个系统化的生物安全数据分类框架，能够根据数据风险等级进行分层管理。该框架为不同风险级别的数据提供了具体的技术控制建议，并为双用途病原体数据治理提供了新思路。在计算和编码资源广泛可及的环境中，数据控制被认为是减少关注生物AI能力扩散的最有效干预手段之一。

Conclusion: 生物安全数据等级（BDL）框架为病原体数据的风险分类和控制提供了实用工具。通过基于风险的数据分类和相应的技术限制，可以有效减少AI模型被用于有害生物应用的可能性。在当今计算资源普及的背景下，数据控制是防止生物AI能力滥用的关键干预措施，需要国际社会共同实施相应的治理框架。

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [429] [Interpretable Failure Analysis in Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2602.08104)
*Risal Shahriar Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: 提出基于梯度的两阶段框架，用于多智能体强化学习中的可解释故障检测与归因，包括识别初始故障源、验证多米诺效应和追踪故障传播路径。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在安全关键领域应用日益广泛，但缺乏可解释的故障检测与归因方法，现有方法多为黑盒检测，难以理解故障传播机制。

Method: 两阶段梯度框架：第一阶段通过策略梯度成本的泰勒余项分析进行可解释的智能体故障检测；第二阶段通过评论家导数的几何分析（一阶敏感性和二阶曲率）构建可解释的传染图。

Result: 在Simple Spread（3和5智能体）和StarCraft II环境中评估，使用MADDPG和HATRPO算法，实现了88.2-99.4%的初始故障源检测准确率，并提供可解释的几何证据。

Conclusion: 该框架超越了黑盒检测，提供了梯度层面的可解释法医分析工具，为安全关键多智能体系统中的级联故障诊断提供了实用方法。

Abstract: Multi-Agent Reinforcement Learning (MARL) is increasingly deployed in safety-critical domains, yet methods for interpretable failure detection and attribution remain underdeveloped. We introduce a two-stage gradient-based framework that provides interpretable diagnostics for three critical failure analysis tasks: (1) detecting the true initial failure source (Patient-0); (2) validating why non-attacked agents may be flagged first due to domino effects; and (3) tracing how failures propagate through learned coordination pathways. Stage 1 performs interpretable per-agent failure detection via Taylor-remainder analysis of policy-gradient costs, declaring an initial Patient-0 candidate at the first threshold crossing. Stage 2 provides validation through geometric analysis of critic derivatives-first-order sensitivity and directional second-order curvature aggregated over causal windows to construct interpretable contagion graphs. This approach explains "downstream-first" detection anomalies by revealing pathways that amplify upstream deviations. Evaluated across 500 episodes in Simple Spread (3 and 5 agents) and 100 episodes in StarCraft II using MADDPG and HATRPO, our method achieves 88.2-99.4% Patient-0 detection accuracy while providing interpretable geometric evidence for detection decisions. By moving beyond black-box detection to interpretable gradient-level forensics, this framework offers practical tools for diagnosing cascading failures in safety-critical MARL systems.

</details>


### [430] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出递归熵概念量化反思过程中的资源消耗风险，并基于此开发RECUR攻击方法，通过构建反事实问题触发大推理模型的过度反思，导致输出长度增加11倍、吞吐量下降90%。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在处理复杂任务时需要进行显式推理，这需要更长的上下文长度和更高的资源消耗。先前研究表明对抗性输入可能触发冗余推理过程，但反思过程本身（特别是可能导致过度反思并消耗过多计算能力的部分）尚未得到足够关注。本文旨在揭示推理本身存在的安全问题。

Method: 提出递归熵概念来量化反思过程中的资源消耗风险，并基于此开发RECUR攻击方法。RECUR通过递归熵引导的反事实利用和反思，构建反事实问题来验证大推理模型的内在缺陷和风险。

Result: 实验表明，在良性推理下，递归熵呈现明显的下降趋势。RECUR攻击破坏了这一趋势，能够将输出长度增加最多11倍，并将吞吐量降低90%。

Conclusion: 本文从递归熵的角度揭示了大推理模型在推理过程中存在的资源消耗安全问题，为鲁棒推理提供了新的研究视角，并证明了推理过程本身可能成为攻击目标。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [431] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: WMSS是一种后训练优化方法，利用模型自身历史弱检查点来指导继续优化，突破后训练饱和瓶颈，实现性能提升且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法在模型变得高度自信后会出现饱和瓶颈，性能提升有限。研究发现模型自身历史弱状态中仍存在有监督信号，这为突破饱和瓶颈提供了可能。

Method: WMSS通过熵动态识别可恢复的学习差距，利用弱检查点作为指导，通过补偿学习强化这些差距，使强智能体能够超越传统后训练的饱和限制。

Result: 在数学推理和代码生成数据集上的实验表明，使用WMSS训练的智能体实现了有效的性能提升，同时不产生额外的推理成本。

Conclusion: WMSS通过利用模型自身历史弱状态作为监督信号，成功突破了后训练中的饱和瓶颈，为大型语言模型的持续优化提供了新范式。

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [432] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 提出去中心化评估框架解决LLM评估中的不稳定性问题，通过区块链协议激励全球参与者作为独立验证者，显著降低评估方差


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的集中式评估存在不透明、过拟合和硬件差异导致的方差问题。实证分析显示，现有评估存在惊人的不一致性：单个模型在HumanEval上十次重复运行的标准差（1.67）实际上超过了官方排行榜上前10名模型之间的性能差距（0.91），使得当前排名统计上不可靠

Method: 提出去中心化评估框架，通过异构计算节点的大规模基准测试实现硬件和参数多样性。利用基于区块链的协议激励全球贡献者作为独立验证者，采用稳健的奖励系统确保评估完整性并阻止不诚实参与

Result: 去中心化评估框架将同一模型十次运行的标准差降低到0.28，相比传统框架显著改善，确保模型排名具有更高的统计置信度

Conclusion: 去中心化评估框架将评估从"集中式黑箱"转变为"去中心化背书"，通过多方共识和多样化推理环境产生更稳定、更具代表性的指标。该平台已完全实现并将向社区发布

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [433] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: SAYO：通过强化学习框架引入区域级视觉注意力奖励，解决多模态大语言模型中视觉注意力不稳定的问题，提升复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在复杂推理任务中依赖长文本推理轨迹，但缺乏学习稳定视觉注意力策略的机制。研究发现当前模型存在视觉注意力薄弱的问题：早期视觉对齐错误很少在后续推理中得到纠正，导致错误传播和推理失败。这一限制源于训练过程中对视觉注意力的信用分配不足。

Method: 提出SAYO视觉推理模型，采用强化学习框架训练，引入区域级视觉注意力奖励机制。该奖励明确将优化信号与基于视觉的推理步骤对齐，使模型能够学习更可靠的注意力行为。

Result: 在多个多模态基准测试上的广泛实验表明，SAYO在多样化的推理和感知任务中持续提升性能。

Conclusion: 通过强化学习框架引入区域级视觉注意力奖励，能够有效解决多模态大语言模型中视觉注意力不稳定的问题，提升模型在复杂推理任务中的表现。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [434] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: G-LNS是一个基于大语言模型的生成式进化框架，用于自动设计大规模邻域搜索算子，通过协同进化破坏和修复算子对，在组合优化问题上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的自动启发式设计方法通常局限于构造性优先级规则或参数化局部搜索指导，将搜索空间限制在固定的启发式形式上。这种设计在结构探索能力上有限，难以在复杂的组合优化问题中逃离深度局部最优。

Method: 提出G-LNS生成式进化框架，利用大语言模型协同进化紧密耦合的破坏和修复算子对。采用合作评估机制明确捕捉算子间的交互作用，发现能够共同执行有效结构破坏和重构的互补算子逻辑。

Result: 在旅行商问题和带容量约束的车辆路径问题等具有挑战性的组合优化基准测试中，G-LNS显著优于基于大语言模型的自动启发式设计方法以及强大的经典求解器。发现的启发式不仅能在减少计算预算的情况下获得接近最优的解，还能在不同未见过的实例分布上表现出强大的泛化能力。

Conclusion: G-LNS将基于大语言模型的自动启发式设计扩展到大规模邻域搜索算子的自动设计，通过协同进化破坏-修复算子对，实现了更有效的结构探索和优化性能，为组合优化问题的自动启发式设计提供了新方向。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [435] [Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis](https://arxiv.org/abs/2602.08276)
*Haoyu Jia,Kento Kawaharazuka,Kei Okada*

Main category: cs.AI

TL;DR: 提出Structural Context Model形式化模型，用于从上下文结构角度分析和比较LLM智能体，包含声明式实现框架和Semantic Dynamics Analysis工作流，在动态猴子香蕉问题上实现32%成功率提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究存在碎片化问题，概念框架和方法论原则常与底层实现细节交织，导致读者和作者在表面不同的概念中迷失方向。这种碎片化主要源于缺乏可分析、自洽的形式化模型，无法实现与实现无关的LLM智能体表征和比较。

Method: 提出Structural Context Model形式化模型，从上下文结构角度分析和比较LLM智能体。在此基础上引入两个互补组件：1) 声明式实现框架；2) 可持续智能体工程工作流Semantic Dynamics Analysis。该工作流提供对智能体机制的原则性洞察，支持快速、系统的设计迭代。

Result: 在动态变体的猴子香蕉问题上，使用该框架工程的智能体在最具挑战性的设置中实现了高达32个百分点的成功率提升。

Conclusion: 提出的Structural Context Model和配套框架为解决LLM智能体研究碎片化问题提供了系统化解决方案，通过形式化建模和工程化工作流实现了更好的智能体设计和性能提升。

Abstract: Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.

</details>


### [436] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

TL;DR: 论文提出"氛围自动化"概念，认为生成式AI代表了从算法优化到语境语义连贯性导航的认知范式转变，人类角色从问题规范转向"氛围工程"。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能不仅代表技术进步，更是一种认知范式转变，挑战计算机科学的基础假设。传统机器学习是"自动化的自动化"，而生成式AI通过导航语境、语义和风格连贯性运作，而非优化预定义目标指标。

Method: 引入"氛围自动化"概念来表征这一转变，提出生成式AI的功能在于其对操作化隐性规律的可及性——嵌入实践中的语境敏感模式，无法通过显式算法规则完全指定。虽然生成系统不具备现象学意义上的隐性知识，但它们操作化了对编码在高维潜在表示中的语调、意图和情境判断的敏感性。

Result: 人类角色从算法问题规范转向"氛围工程"，即生成系统中对齐和语境判断的编排。论文将这一认知转变与教育和制度转型联系起来，提出了跨越三个分析层面和三个行动领域的概念框架：教师世界观、行业关系和课程设计。

Conclusion: 生成式AI代表了从算法优化到语境导航的认知范式转变，需要警惕模式崩溃和文化同质化的风险，强调需要与生成系统进行有意识的互动，以避免回归到合成的统一性。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [437] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 该研究首次系统性地探索了视觉语言模型中的道德谄媚行为，发现VLMs在用户意见影响下会牺牲道德准确性，表现出从正确到错误判断的不对称转变，揭示了模型在错误纠正与错误引入之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 虽然先前研究探索了视觉语言模型在一般情境下的谄媚行为，但其对基于道德的视觉决策的影响尚未得到充分理解。本研究旨在填补这一空白，首次系统研究VLMs中的道德谄媚现象。

Method: 在Moralise和M^3oralBench数据集上评估10个广泛使用的视觉语言模型，在明确的用户分歧情境下分析其表现。使用错误引入率（EIR）和错误纠正率（ECR）进行量化评估，研究模型在用户偏见影响下的行为变化。

Result: VLMs经常在初始判断正确的情况下产生道德错误的后续回应，表现出明显的不对称性：模型更可能从道德正确转向道德错误判断，而非相反方向。不同数据集上的表现存在差异：在Moralise上后续提示通常降低性能，而在M^3oralBench上则呈现混合甚至改善的结果。模型在错误纠正能力与错误引入之间存在权衡关系。

Conclusion: 视觉语言模型对道德影响表现出脆弱性，具有较强道德正确立场的初始情境会引发更强的谄媚行为。这突显了需要制定原则性策略来提高多模态AI系统的道德一致性和鲁棒性。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [438] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP框架通过基于Shapley值的分层信用分配机制，优化多智能体强化学习，解决LLM与外部工具集成中的信用分配难题。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型与外部工具通过多智能体系统集成是解决复杂问题的新范式，但训练这些系统面临信用分配挑战，难以确定具体功能智能体对决策轨迹成功或失败的责任。

Method: 提出SHARP框架，通过分解的奖励机制实现精确信用分配：包括全局广播-准确性奖励、基于Shapley值的边际信用奖励和工具过程奖励，并通过轨迹组内智能体特定优势的归一化来稳定训练。

Result: 在多个真实世界基准测试中，SHARP显著优于现有最先进基线，相比单智能体方法平均提升23.66%，相比多智能体方法平均提升14.05%。

Conclusion: SHARP通过精确的信用分配机制有效解决了多智能体强化学习中的训练稳定性问题，为LLM与外部工具集成的多智能体系统优化提供了有效解决方案。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [439] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: CoTZero：一种无需标注的视觉语言模型训练范式，通过双阶段数据合成和认知对齐训练，提升视觉推理的逻辑一致性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型依赖表面相关性而非逻辑一致的结构化表示，导致高层次语义结构和因果关系理解不足，限制了组合性和可验证推理能力

Method: 包含两个组件：1) 双阶段数据合成：自下而上提取原子视觉基元并组合成结构化问题推理形式，自上而下用全局结构指导局部细节和因果关系解释；2) 认知对齐训练：在合成的思维链数据上，通过强化微调中的认知一致可验证奖励来加强分层推理和泛化能力

Result: 在多级语义不一致基准测试（包含词汇扰动负例）中，在域内和域外设置下达到83.33%的F1分数，消融实验证实各组件对提升可解释性和人类对齐视觉推理均有贡献

Conclusion: CoTZero通过引入人类认知模型到推理过程中，有效提升了视觉语言模型的逻辑一致性和结构化表示能力，实现了更接近人类水平的视觉推理

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [440] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的并行思维优化方法OPE，通过生成多样化的推理大纲来引导路径探索，解决并行思维中探索路径间信息冗余的问题，显著提升大型推理模型在复杂数学问题上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行思维方法主要关注聚合阶段的优化，对路径探索阶段关注不足。研究发现探索路径间的互信息瓶颈限制了整体性能，需要解决信息冗余问题以提高探索效率。

Method: 提出大纲引导的路径探索(OPE)方法：1) 在并行路径推理前生成多样化的推理大纲，显式划分解空间；2) 采用迭代强化学习策略，分别优化大纲规划和大纲引导推理；3) 在可验证奖励的强化学习(RLVR)框架下实现。

Result: 在多个具有挑战性的数学基准测试上进行广泛实验，结果表明OPE能有效提升不同聚合策略下的推理性能，使大型推理模型更可靠地发现正确解决方案。

Conclusion: 通过显式生成多样化推理大纲来引导路径探索，可以有效减少信息冗余，提高探索路径间的信息多样性，从而显著提升并行思维在复杂推理任务中的性能。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [441] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 现有时序知识图谱基准存在严重缺陷，仅通过统计共现就能达到接近SOTA的性能，无需使用任何时序信息，这暴露了数据集偏差和评估任务过于简化的问题。


<details>
  <summary>Details</summary>
Motivation: 发现现有时序知识图谱基准存在严重漏洞，即使不使用任何时序信息，仅通过统计实体共现就能达到接近最先进的性能，这表明当前评估体系无法真正衡量模型对时序演化的理解能力。

Method: 分析现有基准问题的根源，识别数据集中的固有偏差和评估任务的过度简化形式；构建新的TKG演化基准，包括四个偏差校正的数据集和两个与演化过程紧密对齐的新任务。

Result: 揭示了现有基准的多个局限性：时间间隔知识的不合理格式化、忽略知识过时性学习、以及用于精确演化理解的不足信息；提出了新的基准来促进对TKG演化建模挑战的更准确理解。

Conclusion: 现有时序知识图谱基准存在严重缺陷，无法公平评估模型性能。需要新的基准来纠正数据集偏差，设计更贴近实际演化过程的评估任务，才能真正推动时序知识图谱演化建模的发展。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [442] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 论文提出SAGE方法，通过自感知引导的高效推理解决大推理模型中长思维链导致的冗余问题，显著提升推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大推理模型使用长思维链进行复杂推理时产生大量冗余，损害计算效率并导致实时应用延迟。研究发现长推理链与正确性无关甚至有害准确性，但模型隐含知道何时停止思考，这一能力被当前采样范式所掩盖。

Method: 提出SAGE（自感知引导高效推理）采样范式，释放模型的高效推理潜力。进一步将SAGE作为混合采样集成到基于群体的强化学习（SAGE-RL）中，使SAGE-RL能够将SAGE发现的高效推理模式融入标准pass@1推理。

Result: SAGE-RL显著提升了大推理模型在多个具有挑战性的数学基准测试中的推理准确性和效率。

Conclusion: SAGE方法通过利用模型隐含的"知道何时停止思考"能力，解决了长思维链的冗余问题，为大推理模型提供了更高效准确的推理范式。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [443] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: 将随机森林分类器编译为电路，用于计算决策的完整原因、鲁棒性及最短翻转路径


<details>
  <summary>Details</summary>
Motivation: 现有方法在将随机森林编译为电路时效率较低，且缺乏计算决策原因、鲁棒性和翻转路径的系统方法

Method: 1) 提出将随机森林编译为电路的高效方法；2) 利用该电路计算决策的完整和一般原因；3) 设计算法计算决策鲁棒性和最短翻转路径

Result: 提出的编译方法比现有方法显著更高效，能够枚举充分原因、必要原因、对比解释，计算决策鲁棒性，并识别最短决策翻转路径

Conclusion: 该方法为随机森林分类器提供了系统化的解释生成和决策分析框架，在多个数据集上验证了其实用性

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [444] [MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval](https://arxiv.org/abs/2602.08369)
*Xin Zhang,Kailai Yang,Chenyue Li,Hao Li,Qiyu Wei,Jun'ichi Tsujii,Sophia Ananiadou*

Main category: cs.AI

TL;DR: MemAdapter：一个统一的记忆检索框架，通过两阶段训练策略实现跨记忆范式的快速对齐，显著降低对齐成本并提升检索性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体记忆系统通常设计在孤立的范式（如显式、参数化或潜在记忆）中，检索方法紧密耦合，阻碍了跨范式泛化和融合。需要统一异构记忆范式

Method: 提出MemAdapter框架，采用两阶段训练策略：1）从统一记忆空间训练生成式子图检索器；2）通过对比学习训练轻量级对齐模块，使检索器适应未见记忆范式

Result: 在三个公共评估基准上，生成式子图检索器在三种记忆范式和智能体模型规模上始终优于五种强基线系统。跨范式对齐仅需13分钟（单GPU），使用不到5%的训练计算量即超越原始检索器性能

Conclusion: MemAdapter实现了异构记忆范式的统一，提供了一种即插即用的智能体记忆系统解决方案，支持有效的零样本跨范式融合，显著提高了记忆检索的灵活性和效率

Abstract: Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.

</details>


### [445] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: VIRF框架通过神经符号架构实现LLM规划器的可验证安全规划，将被动安全检查转变为主动协作修复，在家庭安全任务中实现零危险行动率和最高目标完成率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为具身AI规划器缺乏形式化推理能力，无法提供严格的安全保证。现有方法要么依赖不可靠的LLM进行安全检查，要么简单地拒绝不安全计划而不提供修复方案。

Method: 提出可验证迭代精炼框架(VIRF)，采用神经符号架构，建立导师-学徒对话机制：基于形式化安全本体的确定性逻辑导师为LLM规划器提供因果性和教学性反馈，实现智能计划修复而非简单避免。同时引入可扩展的知识获取管道，从现实世界文档合成安全知识库。

Result: 在具有挑战性的家庭安全任务中，VIRF实现了完美的0%危险行动率(HAR)和77.3%的目标条件率(GCR)，在所有基线方法中最高。平均仅需1.1次修正迭代，效率极高。

Conclusion: VIRF展示了一条构建根本上可信且可验证安全的具身智能体的原则性途径，通过主动协作修复而非被动安全检查，实现了LLM规划器的形式化安全保障。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [446] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: AGENTWM是首个专为智能体模型设计的水印框架，通过利用功能相同工具执行路径的语义等价性，在可见动作轨迹中嵌入可验证水印，有效保护智能体系统的知识产权免受模仿攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型发展为能够自主推理和使用工具的智能体系统，创造了重要的知识产权价值。这些系统容易受到模仿攻击，攻击者可以通过训练模仿模型窃取专有能力。现有的LLM水印技术在此领域失效，因为现实中的智能体系统通常作为灰盒运行，隐藏了验证所需的内部分析痕迹。

Method: AGENTWM利用动作序列的语义等价性，通过微妙地偏向功能相同工具执行路径的分布来注入水印。该方法允许在可见动作轨迹中直接嵌入可验证信号，同时对用户不可区分。开发了自动流水线来生成鲁棒的水印方案，以及严格的统计假设检验程序进行验证。

Result: 在三个复杂领域进行的广泛评估表明，AGENTWM实现了高检测准确率，同时对智能体性能影响极小。结果证实AGENTWM能有效保护智能体知识产权，对抗自适应攻击者，攻击者无法在不严重降低被盗模型实用性的情况下移除水印。

Conclusion: AGENTWM是首个专门为智能体模型设计的水印框架，成功解决了现有LLM水印技术在智能体系统保护方面的局限性，为智能体知识产权保护提供了有效解决方案。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [447] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: TreeTensor是一个通用的嵌套数据结构容器，能够高效处理具有层次结构的复杂AI数据，支持零成本应用各种函数和操作，并与主流机器学习库兼容。


<details>
  <summary>Details</summary>
Motivation: 传统张量结构在处理具有层次结构的复杂认知AI系统数据时存在不便和低效问题，需要一种能够处理嵌套数据的通用容器。

Method: 总结嵌套数据的两种主要计算模式，提出TreeTensor这一通用嵌套数据容器，通过约束和魔法工具实现对嵌套数据的零成本操作，支持与Scikit-Learn、Numpy、PyTorch等库的兼容。

Result: TreeTensor在各种问题中表现出强大的可用性，特别是在目前最复杂的AI系统之一——星际争霸II的AlphaStar中，同时展现出优异的运行时效率且无任何开销。

Conclusion: TreeTensor为解决复杂认知AI系统中嵌套数据处理问题提供了一个高效、通用的解决方案，能够显著提升编程效率和系统性能。

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [448] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 论文提出了一种名为"强化推理"的熵感知推理时控制策略，利用模型自身的不确定性选择性调用第二次更审慎的推理尝试，无需重新训练即可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型通常在一次性贪婪推理协议下进行评估和部署，这种机制会系统性地低估固定模型的真实能力。许多错误并非源于知识缺失，而是内部模糊性下的过早决策。

Method: 引入强化推理方法，这是一种熵感知的推理时控制策略。通过监测模型生成过程中的不确定性（熵），当检测到高不确定性时，自动触发第二次更审慎的推理尝试，而无需重新训练模型。

Result: 在12,032个MMLU-Pro问题和14个学科上，使用DeepSeek-v3.2模型，强化推理将准确率从60.72%提升至84.03%，仅增加61.06%的推理调用。100%重新询问的消融实验达到84.35%，表明不确定性感知选择能以更少计算获得大部分可实现的改进。

Conclusion: 该方法不仅提供了实用的推理时升级方案，还提出了更广泛的熵感知范式来测量和扩展模型能力。一次性贪婪推理与不确定性条件化深思之间的差距为诊断LLM的潜在推理范围提供了视角，并激励未来训练目标明确约束正确性-置信度对齐。

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [449] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 本文提出了一种用于全局工作空间理论（GWT）的top-down注意力机制，该机制通过选择相关模态来提升多模态系统的噪声鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 全局工作空间理论（GWT）为多模态整合提供了认知神经科学基础，但现有实现中的注意力机制研究不足。需要开发有效的注意力机制来提升GWT系统的性能。

Method: 提出了一种top-down注意力机制，用于在全局工作空间中选择相关模态。在Simple Shapes和MM-IMDb 1.0两个多模态数据集上评估该机制，并与现有多模态注意力模型进行对比。

Result: 1）注意力机制显著提升了全局工作空间系统的噪声鲁棒性；2）展示了跨任务和跨模态的泛化能力；3）在MM-IMDb 1.0基准测试中，该机制使全局工作空间系统达到与最先进方法竞争的性能。

Conclusion: 提出的top-down注意力机制有效增强了全局工作空间理论在多模态整合中的性能，特别是在噪声鲁棒性和泛化能力方面，为GWT的实际应用提供了重要支持。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [450] [OSCAR: Optimization-Steered Agentic Planning for Composed Image Retrieval](https://arxiv.org/abs/2602.08603)
*Teng Wang,Rong Shan,Jianghao Lin,Junjie Wu,Tianyi Xu,Jianping Zhang,Wenteng Chen,Changwang Zhang,Zhaoxiang Wang,Weinan Zhang,Jun Wang*

Main category: cs.AI

TL;DR: OSCAR是一个基于优化引导的智能体规划框架，用于组合图像检索，将启发式搜索过程转化为轨迹优化问题，通过离线-在线范式实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 现有组合图像检索方法存在两种局限：统一嵌入检索受限于单一模型近视问题，启发式智能体检索受限于次优的试错编排。需要一种更系统化的方法来处理视觉和文本约束的复杂推理。

Method: 提出离线-在线范式：离线阶段将CIR建模为两阶段混合整数规划问题，通过布尔集合运算推导最大化真实覆盖的最优轨迹，存储在黄金库中；在线阶段使用这些轨迹作为上下文演示来引导VLM规划器进行推理。

Result: 在三个公共基准测试和一个私有工业基准测试中，OSCAR始终优于最先进的基线方法。特别值得注意的是，仅使用10%的训练数据就能实现优越性能，证明了规划逻辑的强大泛化能力而非数据集特定的记忆。

Conclusion: OSCAR成功将组合图像检索从启发式搜索过程转化为原则性的轨迹优化问题，通过离线推导最优轨迹和在线引导规划的方式，实现了更高效、泛化能力更强的检索性能。

Abstract: Composed image retrieval (CIR) requires complex reasoning over heterogeneous visual and textual constraints. Existing approaches largely fall into two paradigms: unified embedding retrieval, which suffers from single-model myopia, and heuristic agentic retrieval, which is limited by suboptimal, trial-and-error orchestration. To this end, we propose OSCAR, an optimization-steered agentic planning framework for composed image retrieval. We are the first to reformulate agentic CIR from a heuristic search process into a principled trajectory optimization problem. Instead of relying on heuristic trial-and-error exploration, OSCAR employs a novel offline-online paradigm. In the offline phase, we model CIR via atomic retrieval selection and composition as a two-stage mixed-integer programming problem, mathematically deriving optimal trajectories that maximize ground-truth coverage for training samples via rigorous boolean set operations. These trajectories are then stored in a golden library to serve as in-context demonstrations for online steering of VLM planner at online inference time. Extensive experiments on three public benchmarks and a private industrial benchmark show that OSCAR consistently outperforms SOTA baselines. Notably, it achieves superior performance using only 10% of training data, demonstrating strong generalization of planning logic rather than dataset-specific memorization.

</details>


### [451] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 该论文分析了AI安全辩论中的人类监督成本，引入了辩论查询复杂度(DQC)概念，发现PSPACE/poly类问题仅需O(log n)次查询即可判定，表明辩论具有极高的查询效率。


<details>
  <summary>Details</summary>
Motivation: AI安全辩论使用两个竞争模型帮助人类法官验证复杂计算任务，但之前的研究只分析了辩论在理论上能解决什么问题，没有分析人类监督的实际成本——法官需要对辩论记录进行多少次查询。需要量化这种监督成本。

Method: 引入辩论查询复杂度(DQC)概念，定义为验证者正确决定辩论所需检查的最小比特数。通过理论分析建立了DQC与计算复杂度类的关系，特别是与PSPACE/poly类的关系。

Result: 发现PSPACE/poly类（辩论能有效判定的问题类）恰好是那些用O(log n)次查询可判定的函数类。这表明辩论具有惊人的查询效率：即使对于高度复杂的问题，对数级别的监督就足够了。还证明了依赖所有输入比特的函数需要Ω(log n)次查询，任何可由大小为s的电路计算的函数满足DQC(f) ≤ log(s) + 3。

Conclusion: 辩论查询复杂度为AI安全辩论的实践可行性提供了理论保证，对数查询复杂度使得人类监督在实际中可行。有趣的是，证明P类语言中DQC下界为log(n) + 6将产生新的电路下界，将辩论查询复杂度与电路复杂度的核心问题联系起来。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [452] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 该论文研究了STRIPS规划的计算复杂性，特别是当操作符只有一个前提条件和一个效果时（STRIPS¹₁）是否NP完全的问题。


<details>
  <summary>Details</summary>
Motivation: 基于Bylander关于命题STRIPS规划计算复杂性的结果，虽然已知当操作符限制为两个前提条件和两个后置条件时，规划存在性判定是PSPACE完全的，但对于只有一个前提条件和一个效果的操作符，其复杂性是否NP完全仍是未知的。

Method: 通过调用SAT求解器处理小规模实例，引入文字图概念，并将其映射到Petri网来分析STRIPS¹₁的复杂性。

Result: 论文对STRIPS¹₁的小解假设进行了深入分析，通过多种技术方法探讨了该问题的计算复杂性。

Conclusion: 研究为STRIPS规划中操作符复杂性边界问题提供了新的见解，特别是针对只有一个前提条件和一个效果的操作符的NP完全性问题。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [453] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: 本文首次系统综述了SAIG方法（合成人工智能基准真值方法），提出了一种新的分类法，并发现XAI评估领域缺乏共识，需要进一步研究和标准化。


<details>
  <summary>Details</summary>
Motivation: XAI（可解释人工智能）评估领域方法多样且复杂，与传统AI评估不同，XAI缺乏普遍正确的解释基准真值，使得客观评估具有挑战性。SAIG方法通过生成人工基准真值为XAI技术提供直接评估，但该领域尚未有系统综述。

Method: 1. 首次对SAIG方法进行全面综述和分析；2. 提出新的分类法来对这些方法进行分类；3. 识别了区分不同SAIG方法的七个关键特征；4. 进行了比较研究。

Result: 研究发现XAI评估技术缺乏共识，不同方法之间存在显著差异，这凸显了该领域需要进一步研究和标准化。

Conclusion: SAIG方法为解决XAI评估挑战提供了有前景的方向，但当前领域缺乏共识，需要更多的研究和标准化工作来建立有效的评估框架。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [454] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: 该论文将潜在思维链视为可操作的因果过程，通过结构因果模型分析潜在步骤，研究其在数学和一般推理任务中的因果必要性、影响传播和答案模式保留


<details>
  <summary>Details</summary>
Motivation: 现有潜在思维链方法使用内部潜在步骤替代显式文本推理，但这些中间计算难以通过相关性探测之外的方式进行评估。需要更系统的方法来理解和分析这些潜在推理过程

Method: 将潜在思维链建模为表示空间中的结构因果模型，将潜在步骤作为变量，通过逐步干预分析其因果效应。研究Coconut和CODI两种代表性范式在数学和一般推理任务中的应用

Result: 发现潜在步骤预算不像同质的额外深度，更像具有非局部路由的分阶段功能；早期输出偏差与晚期表示承诺之间存在持续差距；中间轨迹保留竞争答案模式

Conclusion: 研究结果支持基于模式条件和稳定性感知的分析方法，以及相应的训练/解码目标，作为解释和改进潜在推理系统的更可靠工具

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [455] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: AI工具在认知诊断建模中Q矩阵构建的应用研究：比较不同AI模型生成的Q矩阵与已验证Q矩阵的一致性，发现AI表现存在显著差异且随时间变化


<details>
  <summary>Details</summary>
Motivation: Q矩阵构建是认知诊断建模中的关键但劳动密集型步骤，研究探索AI工具（通用语言模型）是否能支持Q矩阵开发，减轻专家负担

Method: 向多个AI模型提供与人类专家相同的训练材料，比较AI生成的Q矩阵与Li和Suen（2013）已验证的阅读理解测试Q矩阵的一致性，使用Cohen's kappa评估一致性，并在2026年1月使用更新的AI版本进行后续分析

Result: 不同AI模型间存在显著差异，Google Gemini 2.5 Pro与已验证Q矩阵的一致性最高（Kappa = 0.63），超过所有人类专家；但2026年使用新版AI的分析显示与已验证Q矩阵的一致性降低

Conclusion: AI工具在Q矩阵构建中具有潜力，但表现存在模型间差异且随时间变化，需要进一步研究AI在认知诊断建模中的可靠性和稳定性

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [456] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: RC-LLM：基于残差连接和大型语言模型的微服务根因定位方法，通过多源遥测数据融合和上下文推理能力提升故障分析准确性


<details>
  <summary>Details</summary>
Motivation: 微服务架构中根因定位面临挑战，复杂故障传播和高维遥测数据（指标、日志、追踪）限制了现有RCA方法的有效性

Method: 提出RC-LLM方法：1）设计残差式分层融合结构整合多源遥测数据；2）利用大型语言模型的上下文推理能力建模时序和跨微服务的因果依赖关系

Result: 在CCF-AIOps微服务数据集上的实验结果表明，RC-LLM在根因分析中实现了较高的准确性和效率

Conclusion: RC-LLM方法通过结合残差连接结构和大型语言模型，有效解决了复杂微服务架构中的根因定位问题，提升了故障分析的性能

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [457] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: NADEx是一种用于时序知识图谱推理的负感知扩散模型，通过结合负样本信息和余弦对齐正则化器，在四个公开基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前时序知识图谱推理中的扩散模型存在两个主要问题：1) 生成路径仅基于正样本证据，忽略了信息丰富的负样本上下文；2) 训练目标主要依赖交叉熵排序，虽然改进了候选排序但对去噪嵌入的校准监督不足。

Method: NADEx采用负感知扩散模型，将实体、关系和时序间隔的主体中心历史编码为序列嵌入。在正向过程中扰动查询对象，在反向过程中使用基于时序关系上下文的Transformer去噪器进行重建。此外，从批次负样本原型推导出余弦对齐正则化器，收紧决策边界以排除不合理候选。

Result: 在四个公开时序知识图谱基准测试上的综合实验表明，NADEx实现了最先进的性能。

Conclusion: NADEx通过结合负样本信息和改进的校准监督，有效提升了时序知识图谱推理的准确性和鲁棒性，为扩散模型在该领域的应用提供了新思路。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [458] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 本文提出了一个统一框架，用于整合多种定性推理的扩展和组合形式，包括多尺度推理、时间序列和松散集成，并研究其可满足性决策的复杂性。


<details>
  <summary>Details</summary>
Motivation: 定性推理能够在信息不精确、不完整且无数值的情况下推断新知识，但现有研究缺乏对各种扩展和组合形式的统一处理框架。

Method: 提出一个形式化框架，统一处理多尺度推理、时间序列和松散集成等定性形式主义的扩展和组合，并分析其可满足性决策的复杂性。

Result: 建立了两个互补定理，保证可满足性决策是多项式时间的，并用它们恢复了尺寸-拓扑组合的已知结果，同时扩展了定性形式主义的主要定义以包含文献中排除的重要组合形式。

Conclusion: 该统一框架不仅支持在各种扩展和组合形式下进行推理，还为研究可满足性决策及其复杂性提供了系统方法，扩展了定性推理的应用范围。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [459] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: STP框架通过空间剪枝和时间剪枝同时提升扩散大语言模型强化学习的效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 强化学习对解锁扩散大语言模型的复杂推理能力至关重要，但应用于dLLMs时面临效率和稳定性方面的独特挑战

Method: 提出时空剪枝框架，通过空间剪枝（使用静态先验约束探索空间）和时间剪枝（绕过冗余的后期细化步骤）压缩生成过程中的冗余

Result: 理论分析表明STP严格降低了对数似然估计的方差，确保更稳定的策略更新；大量实验显示STP在效率和准确性上都超越了最先进的基线方法

Conclusion: STP框架有效解决了dLLMs强化学习中的效率和稳定性问题，为扩散大语言模型的强化学习训练提供了实用解决方案

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [460] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: CausalT5K是一个包含5000多个案例的诊断基准，用于系统检测LLM在因果推理中的失败模式，包括阶梯坍塌、谄媚漂移和错误拒绝，通过实用性和安全性指标揭示传统聚合精度无法发现的故障模式。


<details>
  <summary>Details</summary>
Motivation: LLM在因果推理中存在阶梯坍塌、谄媚性和错误拒绝等已知问题，但由于缺乏能够进行系统诊断的基准，修复进展缓慢。需要创建一个能够全面测试这些关键能力的诊断工具。

Method: 开发了包含5000多个案例的CausalT5K基准，涵盖10个领域，测试三种关键能力：检测阶梯坍塌、抵抗谄媚漂移、生成明智拒绝。采用人机协作流程，涉及40名领域专家、迭代交叉验证周期，以及基于规则、LLM和人工评分的复合验证方法。

Result: 初步实验揭示了四象限控制景观，静态审计策略普遍失败。基准能够将性能分解为实用性（敏感性）和安全性（特异性），揭示传统聚合精度无法发现的故障模式。

Conclusion: CausalT5K作为研究基础设施实现了Pearl的因果阶梯，为推进可信推理系统提供了有价值的诊断工具，能够系统识别LLM在因果推理中的失败模式。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [461] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: CoRefine是一种基于置信度引导的自优化方法，通过轻量级控制器实现高效推理，大幅减少计算开销，相比传统并行解码方法可减少约190倍token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常依赖测试时并行解码（如512个样本）来提高推理准确性，但这需要大量计算资源。需要一种更高效的方法来减少计算开销，同时保持推理性能。

Method: CoRefine在冻结的LLM上添加一个轻量级211k参数的Conv1D控制器，该控制器利用完整轨迹置信度来决定是否停止、重新检查或尝试不同方法，实现有针对性的自我纠正。平均每个问题只需2.7个优化步骤。

Result: 在多样化的推理基准测试和三个开源模型上，控制器在自信停止时达到92.6%的精确度，表明置信度动态可靠地指示正确性而无需真实验证。相比512样本基线，平均减少约190倍token消耗。

Conclusion: 通过将置信度视为控制信号而非正确性保证，CoRefine为可扩展推理和具有不完美验证器的智能体设置提供了模块化基础。CoRefine-Tree变体进一步通过自适应平衡探索和利用来增强性能。

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [462] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: SWM是一个模块化、经过测试和文档化的世界模型研究生态系统，旨在解决现有世界模型实现缺乏可重用性、标准化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数世界模型实现都是针对特定论文的，这严重限制了其可重用性，增加了bug风险，并降低了评估标准化程度。为了解决这些问题，作者开发了stable-worldmodel（SWM）。

Method: SWM是一个模块化的世界模型研究生态系统，提供高效的数据收集工具、标准化环境、规划算法和基准实现。每个环境都支持可控的变化因素（包括视觉和物理属性），以支持鲁棒性和持续学习研究。

Result: 作者通过使用SWM研究DINO-WM中的零样本鲁棒性来展示其实用性。

Conclusion: SWM作为一个标准化的世界模型研究平台，能够促进世界模型研究的可重用性、减少bug风险，并提高评估标准化，同时支持鲁棒性和持续学习研究。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [463] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5是一个用于跨计算和实证领域端到端科学发现的统一系统，包含生成、验证和演化三个协调子系统，在科学推理基准测试中表现领先，并能自主设计算法和进行实证实验。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在计算和实证领域进行端到端科学发现的统一系统，解决传统方法在跨领域协调、长期发现周期和自主实验执行方面的局限性。

Method: 基于结构化架构构建，包含三个协调子系统：生成、验证和演化，支持深度研究、解决方案优化和长期记忆等基础能力，能够协调计算建模和实验室实验。

Result: 在GAIA、HLE、GPQA和FrontierScience等科学推理基准测试中取得领先性能；在算法发现任务中自主设计核心机器学习问题的竞争性方法；在实证发现任务中执行完整的计算或湿实验室实验，在地球、生命、生物和物理领域产生科学发现。

Conclusion: InternAgent-1.5为自主科学发现提供了一个通用且可扩展的框架，展示了在跨计算和实证领域的统一科学发现能力。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [464] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: iGRPO是一种两阶段强化学习方法，通过模型自生成的草稿进行动态自我条件化，在数学推理任务中超越了GRPO并取得了新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在解决复杂数学问题方面显示出潜力，但仍难以产生准确且一致的解决方案。强化学习可以对齐模型与任务特定的奖励，但需要更高效的优化方法。

Method: iGRPO是GRPO的两阶段扩展：第一阶段采样多个探索性草稿并选择最高奖励的草稿；第二阶段将最佳草稿附加到原始提示后，在草稿条件化的精炼上应用GRPO风格的更新，训练策略超越其先前的最佳尝试。

Result: 在匹配的rollout预算下，iGRPO在不同基础模型上始终优于GRPO。应用于OpenReasoning-Nemotron-7B模型时，在AIME24和AIME25上分别达到85.62%和79.64%的新SOTA结果。

Conclusion: iGRPO展示了迭代、基于自我反馈的强化学习在推进可验证数学推理方面的潜力，其精炼包装器具有超越GRPO变体的泛化能力，并能通过延迟熵崩溃改变学习动态。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [465] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 论文提出了一种L0-L4分层数据管理框架，通过数据与模型的协同进化来提升LLM训练效率，解决当前数据规模单向扩展的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究过度依赖数据规模的单向扩展，面临数据可用性、获取成本和训练效率的瓶颈。作者认为AGI发展正进入数据-模型协同进化的新阶段，需要更智能的数据管理方法。

Method: 提出了L0-L4分层数据管理框架，从原始未整理资源到组织化可验证知识分为五个层级。利用LLM参与数据管理过程（如质量评分和内容编辑），根据数据特性、管理策略和训练角色将数据战略性地分配到预训练、中期训练和对齐等不同训练阶段。

Result: 通过实证研究验证了该框架的有效性，使用分层数据集进行多阶段训练的实验结果表明，分层感知的数据利用显著提高了训练效率和模型性能。

Conclusion: 该分层数据管理框架在数据质量、获取成本和边际训练效益之间取得了平衡，为可扩展和可持续的数据管理提供了系统化方法，并开源了分层数据集和处理工具以促进进一步研究。

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [466] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: GEBench是一个评估GUI生成中动态交互和时间一致性的新基准，包含700个样本和GE-Score五维评估指标，发现现有模型在单步转换表现良好但多步序列的时间一致性和空间定位存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型能够根据用户指令预测未来GUI状态，但现有基准主要关注通用领域视觉保真度，对GUI特定上下文中的状态转换和时间一致性评估不足。

Method: 引入GEBench基准，包含700个精心策划的样本，涵盖5个任务类别，包括单步交互和多步轨迹，以及定位点基础任务。提出GE-Score五维评估指标：目标达成、交互逻辑、内容一致性、UI合理性和视觉质量。

Result: 对当前模型的广泛评估表明，它们在单步转换上表现良好，但在维持长时间交互序列的时间一致性和空间定位方面存在显著困难。图标解释、文本渲染和定位精度被识别为关键瓶颈。

Conclusion: 这项工作为系统评估提供了基础，并为未来构建高保真生成式GUI环境的研究指明了有前景的方向。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>
