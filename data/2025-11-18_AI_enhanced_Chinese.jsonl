{"id": "2511.11573", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11573", "abs": "https://arxiv.org/abs/2511.11573", "authors": ["Christopher R. Lee-Jenkins"], "title": "Softmax as a Lagrangian-Legendrian Seam", "comment": null, "summary": "This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian \"seam\" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06\u673a\u5668\u5b66\u4e60\u4e0e\u5fae\u5206\u51e0\u4f55\u8054\u7cfb\u8d77\u6765\uff0c\u5c55\u793a\u4e86softmax\u51fd\u6570\u4ecelogits\u5230\u6982\u7387\u7684\u8f6c\u6362\u53ef\u4ee5\u88ab\u5efa\u6a21\u4e3a\u51e0\u4f55\u754c\u9762\uff1a\u4e24\u4e2a\u7531\u52bf\u51fd\u6570\u751f\u6210\u7684\u4fdd\u5b88\u63cf\u8ff0\u5728\u63a5\u89e6\u5c4f\u5e55\uff08\u6982\u7387\u5355\u7eaf\u5f62\uff09\u4e0a\u7684Legendrian\u63a5\u7f1d\u5904\u76f8\u9047\u3002", "motivation": "\u5efa\u7acb\u673a\u5668\u5b66\u4e60\u4e0e\u73b0\u4ee3\u5fae\u5206\u51e0\u4f55\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u7406\u89e3softmax\u7b49\u673a\u5668\u5b66\u4e60\u57fa\u672c\u7ec4\u4ef6\u63d0\u4f9b\u65b0\u7684\u51e0\u4f55\u89c6\u89d2\u3002", "method": "\u5c06softmax\u5efa\u6a21\u4e3a\u51e0\u4f55\u754c\u9762\uff0c\u4f7f\u7528\u8d1f\u71b5\u548clog-sum-exp\u4e24\u4e2a\u52bf\u51fd\u6570\u751f\u6210\u7684\u4fdd\u5b88\u63cf\u8ff0\uff0c\u5728\u63a5\u89e6\u5c4f\u5e55\uff08\u6982\u7387\u5355\u7eaf\u5f62\uff09\u4e0a\u5f62\u6210Legendrian\u63a5\u7f1d\u3002", "result": "\u53d1\u73b0\u504f\u7f6e\u5e73\u79fb\u4e0d\u53d8\u6027\u5bf9\u5e94\u4e8e\u5c4f\u5e55\u4e0a\u7684Reeb\u6d41\uff0cFenchel-Young\u7b49\u5f0f/KL\u6563\u5ea6\u63d0\u4f9b\u4e86\u5230\u63a5\u7f1d\u7684\u53ef\u8ba1\u7b97\u8ddd\u79bb\u3002", "conclusion": "\u4e3a\u673a\u5668\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a\u7d27\u51d1logit\u6a21\u578b\uff08\u6295\u5f71\u6216\u7403\u9762\uff09\u3001\u5168\u5c40\u4e0d\u53d8\u91cf\uff0c\u4ee5\u53ca\u4e0e\u4fe1\u606f\u51e0\u4f55\u7684\u8054\u7cfb\uff0c\u5176\u4e2d\u5c4f\u5e55\u4e0a\u7684\u52a8\u529b\u5b66\u8868\u73b0\u4e3a\u590d\u5236\u5b50\u6d41\u3002"}}
{"id": "2511.11574", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11574", "abs": "https://arxiv.org/abs/2511.11574", "authors": ["Viviana Luccioli", "Rithika Iyengar", "Ryan Panley", "Flora Haberkorn", "Xiaoyu Ge", "Leland Crane", "Nitish Sinha", "Seung Jung Lee"], "title": "LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora", "comment": null, "summary": "Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM \"teacher\" trains a smaller and more efficient \"student\" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.", "AI": {"tldr": "\u63d0\u51faM-RARU\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u968f\u673a\u63a5\u53d7-\u62d2\u7edd\u673a\u5236\u9009\u62e9\u6700\u6709\u4fe1\u606f\u91cf\u7684\u6570\u636e\u70b9\uff0c\u663e\u8457\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u5b9e\u73b080%\u7684\u6837\u672c\u9700\u6c42\u51cf\u5c11\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u8ba1\u7b97\u548c\u8d22\u52a1\u6210\u672c\u963b\u788d\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u672c\u8eab\u5bf9\u5927\u578b\u6570\u636e\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u6559\u5e08\u6a21\u578b\u6807\u6ce8\u5927\u91cf\u6837\u672c\u5e76\u6d88\u8017\u663e\u8457token\u3002", "method": "\u5f15\u5165M-RARU\uff08\u591a\u7c7b\u968f\u673a\u63a5\u53d7/\u62d2\u7edd\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\uff09\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u968f\u673a\u63a5\u53d7-\u62d2\u7edd\u673a\u5236\uff0c\u4ec5\u9009\u62e9\u6700\u6709\u4fe1\u606f\u91cf\u7684\u6570\u636e\u70b9\u4f9bLLM\u6559\u5e08\u6807\u6ce8\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u5b66\u751f\u6a21\u578b\uff08SVM\u3001LDA\u3001RF\u3001GBDT\u548cDistilBERT\uff09\u548c\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u968f\u673a\u91c7\u6837\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u53ef\u5b9e\u73b0\u9ad8\u8fbe80%\u7684\u6837\u672c\u9700\u6c42\u51cf\u5c11\uff0c\u663e\u8457\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u8d22\u52a1\u6210\u672c\u548c\u6574\u4f53\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "M-RARU\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u4ee5\u6781\u4f4e\u6210\u672c\u521b\u5efa\u9ad8\u6548\u7684\u5b66\u751f\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301LLM\u7684\u6027\u80fd\uff0c\u4e3a\u77e5\u8bc6\u84b8\u998f\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11575", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11575", "abs": "https://arxiv.org/abs/2511.11575", "authors": ["Animesh Joshi"], "title": "Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms", "comment": "8 pages", "summary": "Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7b97\u6cd5\u516c\u5e73\u6027\u8fdd\u89c4\uff0c\u901a\u8fc7k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u751f\u6210\u516c\u5e73\u6027\u6307\u6807\u7684\u62bd\u6837\u5206\u5e03\uff0c\u5e76\u5728\u7d2f\u72af\u9884\u6d4b\u7b97\u6cd5\u4e2d\u53d1\u73b0\u4e86\u5bf9\u9ed1\u4eba\u7fa4\u4f53\u7684\u7edf\u8ba1\u663e\u8457\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u8bc4\u4f30\u7fa4\u4f53\u95f4\u89c2\u5bdf\u5230\u7684\u5dee\u5f02\u662f\u5426\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u6216\u4ec5\u662f\u5076\u7136\u7684\u65b9\u6cd5\uff0c\u9700\u8981\u4e25\u683c\u7684\u7edf\u8ba1\u6d4b\u8bd5\u6846\u67b6\u6765\u8bc4\u4f30\u7b97\u6cd5\u51b3\u7b56\u7cfb\u7edf\u3002", "method": "\u5229\u7528k\u6298\u4ea4\u53c9\u9a8c\u8bc1\u751f\u6210\u516c\u5e73\u6027\u6307\u6807\u7684\u62bd\u6837\u5206\u5e03\uff0c\u5f00\u53d1\u57fa\u4e8e\u9884\u6d4b\u4e0e\u5b9e\u9645\u7ed3\u679c\u5dee\u5f02\u3001\u6a21\u578b\u6821\u51c6\u548c\u56e0\u679c\u63a8\u65ad\u6280\u672f\u7684\u7edf\u8ba1\u663e\u8457\u6027\u6d4b\u8bd5\u3002", "result": "\u7d2f\u72af\u9884\u6d4b\u7b97\u6cd5\u5728\u591a\u4e2a\u516c\u5e73\u6027\u5b9a\u4e49\u4e0b\u5bf9\u9ed1\u4eba\u4e2a\u4f53\u8868\u73b0\u51fa\u7edf\u8ba1\u663e\u8457\u7684\u504f\u89c1\uff0c\u800c\u5728\u5176\u4ed6\u5b9a\u4e49\u4e0b\u5219\u65e0\u504f\u89c1\u6216\u5bf9\u767d\u4eba\u4e2a\u4f53\u6709\u504f\u89c1\u3002", "conclusion": "\u8bc4\u4f30\u7b97\u6cd5\u51b3\u7b56\u7cfb\u7edf\u65f6\u9700\u8981\u4e25\u683c\u548c\u7a33\u5065\u7684\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u7b97\u6cd5\u516c\u5e73\u6027\u8bc4\u4f30\u5e94\u8003\u8651\u7edf\u8ba1\u663e\u8457\u6027\u3002"}}
{"id": "2511.11576", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11576", "abs": "https://arxiv.org/abs/2511.11576", "authors": ["WenZhuo Zhu", "Zheng Cui", "Wenhan Lu", "Sheng Liu", "Yue Zhao"], "title": "DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs", "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86DAOpt\u6846\u67b6\uff0c\u5305\u62ecOptU\u6570\u636e\u96c6\u3001\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6a21\u5757\u548c\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4e0d\u786e\u5b9a\u4f18\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u8003\u5bdf\u6837\u672c\u5916\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u786e\u5b9a\u6027\u4f18\u5316\uff0c\u800c\u73b0\u5b9e\u51b3\u7b56\u5177\u6709\u4e0d\u786e\u5b9a\u6027\uff0cLLM\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7ed3\u5408\u968f\u673a\u4f18\u5316\u548c\u9c81\u68d2\u4f18\u5316\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u91c7\u7528\u5c11\u6837\u672c\u5b66\u4e60\u589e\u5f3aLLM\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6a21\u5757\u548c\u6a21\u62df\u8bc4\u4f30\u73af\u5883\u3002", "result": "\u5f00\u53d1\u4e86DAOpt\u6846\u67b6\u548cOptU\u6570\u636e\u96c6\uff0c\u4e3aLLM\u5728\u4e0d\u786e\u5b9a\u4f18\u5316\u4e2d\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5de5\u5177\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86LLM\u5728\u4e0d\u786e\u5b9a\u4f18\u5316\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u81ea\u52a8\u5316\u4f18\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u548c\u65b9\u6cd5\u3002"}}
{"id": "2511.11649", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11649", "abs": "https://arxiv.org/abs/2511.11649", "authors": ["Jannik Nitschke"], "title": "The Environmental Impact of Ensemble Techniques in Recommender Systems", "comment": "Bachelor Thesis, University of Siegen", "summary": "Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models.\n  We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug.\n  Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model.\n  This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u63a8\u8350\u7cfb\u7edf\u4e2d\u96c6\u6210\u65b9\u6cd5\u7684\u73af\u5883\u5f71\u54cd\uff0c\u53d1\u73b0\u96c6\u6210\u6280\u672f\u867d\u7136\u80fd\u63d0\u5347\u51c6\u786e\u73870.3-5.7%\uff0c\u4f46\u80fd\u8017\u589e\u52a019-2549%\uff0c\u63ed\u793a\u4e86\u51c6\u786e\u7387\u4e0e\u80fd\u8017\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u3002", "motivation": "\u96c6\u6210\u6280\u672f\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u80fd\u63d0\u534710-30%\u51c6\u786e\u7387\uff0c\u4f46\u5176\u73af\u5883\u5f71\u54cd\u5c1a\u672a\u88ab\u6d4b\u91cf\u3002\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u7b97\u6cd5\u6bcf\u7bc7\u8bba\u6587\u53ef\u4ea7\u751f3297kg CO2\uff0c\u4f46\u96c6\u6210\u65b9\u6cd5\u7684\u80fd\u8017\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u5728\u4e24\u4e2a\u6846\u67b6\uff08Surprise\u7528\u4e8e\u8bc4\u5206\u9884\u6d4b\uff0cLensKit\u7528\u4e8e\u6392\u540d\uff09\u4e0a\u8fdb\u884c\u4e8693\u6b21\u5b9e\u9a8c\uff0c\u4f7f\u7528\u56db\u4e2a\u6570\u636e\u96c6\uff0810\u4e07\u5230780\u4e07\u4ea4\u4e92\u6570\u636e\uff09\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u96c6\u6210\u7b56\u7565\uff08\u5e73\u5747\u3001\u52a0\u6743\u3001\u5806\u53e0/\u6392\u540d\u878d\u5408\u3001\u9876\u7ea7\u8868\u73b0\u8005\uff09\u4e0e\u7b80\u5355\u57fa\u7ebf\u548c\u4f18\u5316\u5355\u6a21\u578b\u7684\u6bd4\u8f83\uff0c\u4f7f\u7528\u667a\u80fd\u63d2\u5ea7\u6d4b\u91cf\u80fd\u8017\u3002", "result": "\u96c6\u6210\u65b9\u6cd5\u5728\u63d0\u5347\u51c6\u786e\u73870.3-5.7%\u7684\u540c\u65f6\uff0c\u80fd\u8017\u589e\u52a019-2549%\u3002\u9876\u7ea7\u8868\u73b0\u8005\u96c6\u6210\u6548\u7387\u6700\u4f73\uff1a\u5728MovieLens-1M\u4e0aRMSE\u63d0\u53470.96%\uff0c\u80fd\u8017\u589e\u52a018.8%\uff1b\u5728MovieLens-100K\u4e0aNDCG\u63d0\u53475.7%\uff0c\u80fd\u8017\u589e\u52a0103%\u3002\u8be6\u5c3d\u5e73\u5747\u7b56\u7565\u80fd\u8017\u589e\u52a088-270%\u4f46\u6536\u76ca\u76f8\u5f53\u3002\u6700\u5927\u6570\u636e\u96c6\u4e0aSurprise\u96c6\u6210\u80fd\u8017\u589e\u52a02005%\uff080.21Wh vs 0.01Wh\uff09\uff0c\u51c6\u786e\u7387\u4ec5\u63d0\u53471.2%\uff0c\u4ea7\u751f53.8mg CO2 vs \u5355\u6a21\u578b\u76842.6mg CO2\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6d4b\u91cf\u4e86\u96c6\u6210\u63a8\u8350\u7cfb\u7edf\u7684\u80fd\u8017\u548c\u78b3\u8db3\u8ff9\uff0c\u8bc1\u660e\u9009\u62e9\u6027\u7b56\u7565\u6bd4\u8be6\u5c3d\u5e73\u5747\u7b56\u7565\u66f4\u9ad8\u6548\uff0c\u5e76\u8bc6\u522b\u4e86\u5de5\u4e1a\u89c4\u6a21\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u7b97\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2511.11591", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11591", "abs": "https://arxiv.org/abs/2511.11591", "authors": ["Olusola Babalola", "Bolanle Ojokoh", "Olutayo Boyinbode"], "title": "LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism", "comment": "50 pages, 19 figures, 9 tables", "summary": "This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u65b0\u95fb\u6807\u9898\u4f5c\u4e3a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u66ff\u4ee3\u65b9\u6848\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f\u5728\u8d1f\u9762\u60c5\u611f\u6587\u672c\u5206\u6790\u9886\u57df\u3002\u901a\u8fc7\u4e13\u5bb6\u9a8c\u8bc1\u548c\u591a\u79cd\u6307\u6807\u8bc4\u4f30\uff0c\u53d1\u73b0\u5408\u6210\u6807\u9898\u5728\u5185\u5bb9\u3001\u8bed\u6c14\u548c\u98ce\u683c\u4e0a\u4e0e\u771f\u5b9e\u6807\u9898\u9ad8\u5ea6\u5339\u914d\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u6570\u636e\u83b7\u53d6\u56f0\u96be\u548c\u771f\u5b9e\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u63a2\u7d22LLM\u751f\u6210\u6570\u636e\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u7279\u522b\u5173\u6ce8\u8d1f\u9762\u60c5\u611f\u6587\u672c\u5206\u6790\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u63d0\u793a\u521b\u5efa\u8d1f\u9762\u65b0\u95fb\u6807\u9898\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u9a8c\u8bc1\uff0c\u5e76\u5728\u5d4c\u5165\u7a7a\u95f4\u5206\u6790\u5408\u6210\u6807\u9898\u4e0e\u771f\u5b9e\u6807\u9898\u5728\u5185\u5bb9\u3001\u8bed\u6c14\u3001\u957f\u5ea6\u548c\u98ce\u683c\u4e0a\u7684\u5bf9\u9f50\u5ea6\u3002\u91c7\u7528\u56f0\u60d1\u5ea6\u3001\u53ef\u8bfb\u6027\u3001\u8bcd\u6027\u6807\u6ce8\u5206\u6790\u3001BERTScore\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7b49\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5408\u6210\u6807\u9898\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4e0e\u771f\u5b9e\u6807\u9898\u5339\u914d\u826f\u597d\uff0c\u4ec5\u5728\u8bcd\u6027\u6807\u6ce8\u5206\u6790\u4e2d\u7684\u4e13\u6709\u540d\u8bcd\u5f97\u5206\u5b58\u5728\u660e\u663e\u5dee\u5f02\u3002", "conclusion": "LLM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u5728\u8d1f\u9762\u60c5\u611f\u6587\u672c\u5206\u6790\u4e2d\u5177\u6709\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u652f\u6301NLP\u4efb\u52a1\uff0c\u540c\u65f6\u89c4\u907f\u6570\u636e\u83b7\u53d6\u548c\u9690\u79c1\u95ee\u9898\u3002"}}
{"id": "2511.11590", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11590", "abs": "https://arxiv.org/abs/2511.11590", "authors": ["Robert Gigiu"], "title": "Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)", "comment": "33 pages, 5 figures", "summary": "Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u6027\u589e\u5f3a\u7684\u4e34\u5e8a\u5b89\u5168\u6846\u67b6\uff08ECSF\uff09\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u96c6\u6210\u5230DCB0129/0160\u5b89\u5168\u6807\u51c6\u751f\u547d\u5468\u671f\u4e2d\uff0c\u4f7f\u4e34\u5e8a\u5b89\u5168\u5b98\u5458\u80fd\u591f\u4f7f\u7528\u53ef\u89e3\u91ca\u6027\u8f93\u51fa\u4f5c\u4e3a\u7ed3\u6784\u5316\u5b89\u5168\u8bc1\u636e\uff0c\u800c\u4e0d\u6539\u53d8\u5408\u89c4\u8def\u5f84\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728NHS\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u6982\u7387\u6027\u548c\u81ea\u9002\u5e94\u884c\u4e3a\u4e0e\u73b0\u6709\u4e34\u5e8a\u5b89\u5168\u6807\u51c6\u7684\u786e\u5b9a\u6027\u5047\u8bbe\u76f8\u51b2\u7a81\u3002DCB0129\u548cDCB0160\u6807\u51c6\u5bf9\u4f20\u7edf\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u6cbb\u7406\uff0c\u4f46\u672a\u5b9a\u4e49\u5982\u4f55\u8bc1\u660eAI\u7279\u5b9a\u7684\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u6216\u6a21\u578b\u6f02\u79fb\u3002", "method": "\u901a\u8fc7\u8de8\u76d1\u7ba1\u7efc\u5408\u5206\u6790\uff0c\u5c06DCB\u6761\u6b3e\u4e0e\u826f\u597d\u673a\u5668\u5b66\u4e60\u5b9e\u8df5\u3001NHS AI\u4fdd\u8bc1\u548cT.E.S.T.\u6846\u67b6\u4ee5\u53ca\u6b27\u76dfAI\u6cd5\u6848\u7684\u539f\u5219\u8fdb\u884c\u6620\u5c04\u3002\u5efa\u7acb\u4e86\u8fde\u63a5\u76d1\u7ba1\u6761\u6b3e\u3001\u539f\u5219\u3001ECSF\u68c0\u67e5\u70b9\u548c\u5408\u9002\u53ef\u89e3\u91ca\u6027\u8f93\u51fa\u7684\u77e9\u9635\u3002", "result": "ECSF\u5f15\u5165\u4e86\u4e94\u4e2a\u68c0\u67e5\u70b9\uff1a\u5168\u5c40\u900f\u660e\u5ea6\u7528\u4e8e\u5371\u9669\u8bc6\u522b\u3001\u6848\u4f8b\u7ea7\u53ef\u89e3\u91ca\u6027\u7528\u4e8e\u9a8c\u8bc1\u3001\u4e34\u5e8a\u533b\u751f\u53ef\u7528\u6027\u7528\u4e8e\u8bc4\u4f30\u3001\u53ef\u8ffd\u6eaf\u51b3\u7b56\u8def\u5f84\u7528\u4e8e\u98ce\u9669\u63a7\u5236\u3001\u7eb5\u5411\u53ef\u89e3\u91ca\u6027\u76d1\u63a7\u7528\u4e8e\u4e0a\u5e02\u540e\u76d1\u6d4b\u3002\u5c06SHAP\u3001LIME\u3001\u96c6\u6210\u68af\u5ea6\u3001\u663e\u8457\u6027\u6620\u5c04\u548c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u7b49\u6280\u672f\u6620\u5c04\u5230\u76f8\u5e94\u7684DCB\u5de5\u4ef6\u3002", "conclusion": "ECSF\u5c06\u53ef\u89e3\u91ca\u6027\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e34\u5e8a\u5b89\u5168\u4fdd\u8bc1\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u5f25\u5408\u4e86\u786e\u5b9a\u6027\u98ce\u9669\u6cbb\u7406\u4e0eAI\u6982\u7387\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u652f\u6301\u4e0eGMLP\u3001\u6b27\u76dfAI\u6cd5\u6848\u548cNHS AI\u4fdd\u8bc1\u539f\u5219\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.11633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11633", "abs": "https://arxiv.org/abs/2511.11633", "authors": ["Abhijeet Kumar", "Chetan Agarwal", "Pronoy B. Neogi", "Mayank Goswami"], "title": "Psychological stress during Examination and its estimation by handwriting in answer script", "comment": "10 Pages, 6 Figures and 1 Table", "summary": "This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7ed3\u5408\u7b14\u8ff9\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u751f\u624b\u5199\u8003\u8bd5\u8bd5\u5377\u6765\u91cf\u5316\u5fc3\u7406\u538b\u529b\u6c34\u5e73\uff0c\u4f7f\u7528OCR\u548c\u57fa\u4e8eTransformer\u7684\u60c5\u611f\u5206\u6790\u6a21\u578b\uff0c\u63d0\u4f9b\u8d85\u8d8a\u4f20\u7edf\u8bc4\u5206\u7cfb\u7edf\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u901a\u8fc7\u5206\u6790\u5b66\u751f\u624b\u5199\u8003\u8bd5\u8bd5\u5377\u6765\u91cf\u5316\u5fc3\u7406\u538b\u529b\u6c34\u5e73\uff0c\u4e3a\u4f20\u7edf\u8bc4\u5206\u7cfb\u7edf\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u8ba4\u77e5\u548c\u60c5\u611f\u72b6\u6001\u6d1e\u5bdf\u3002", "method": "\u96c6\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u3001TrOCR\u548c\u4f7f\u7528RoBERTa\u6a21\u578b\u7684\u60c5\u611f\u71b5\u878d\u5408\uff0c\u901a\u8fc7\u4e94\u6a21\u578b\u6295\u7968\u673a\u5236\u548c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u5b9e\u73b0\u9c81\u68d2\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u6570\u503c\u5316\u538b\u529b\u6307\u6570\u7684\u521b\u65b0\u6846\u67b6\uff0c\u5728\u5b66\u672f\u53d6\u8bc1\u9886\u57df\u5177\u6709\u521b\u65b0\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7\u624b\u5199\u5206\u6790\u91cf\u5316\u5b66\u751f\u8003\u8bd5\u671f\u95f4\u5fc3\u7406\u538b\u529b\u7684\u76ee\u6807\uff0c\u4e3a\u5b66\u672f\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u7ef4\u5ea6\u3002"}}
{"id": "2511.11579", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11579", "abs": "https://arxiv.org/abs/2511.11579", "authors": ["Felipe Urrutia", "Jorge Salas", "Alexander Kozachinskiy", "Cristian Buc Calderon", "Hector Pasten", "Cristobal Rojas"], "title": "Decoupling Positional and Symbolic Attention Behavior in Transformers", "comment": "32 pages, 12 figures, repository available", "summary": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u7814\u7a76\u4e86Transformer\u4e2d\u6ce8\u610f\u529b\u5934\u7684\u4f4d\u7f6e\u7f16\u7801\u4e0e\u7b26\u53f7\u7f16\u7801\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u91cf\u5316\u8fd9\u4e24\u79cd\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86RoPE\u7f16\u7801\u4e2d\u9891\u7387\u4f7f\u7528\u4e0e\u6ce8\u610f\u529b\u5934\u884c\u4e3a\u7684\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u7406\u89e3RoPE\u4f4d\u7f6e\u7f16\u7801\u6210\u529f\u7684\u539f\u56e0\uff0c\u7279\u522b\u662f\u5176\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u9891\u7387\u5206\u522b\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u4ee5\u53ca\u8fd9\u79cd\u7f16\u7801\u65b9\u5f0f\u5982\u4f55\u5f71\u54cdTransformer\u6a21\u578b\u7684\u884c\u4e3a\u3002", "method": "\u4ece\u7406\u8bba\u548c\u5b9e\u8bc1\u5c42\u9762\u5206\u6790\u6ce8\u610f\u529b\u5934\u7684\u4f4d\u7f6e\u4e0e\u7b26\u53f7\u7f16\u7801\u884c\u4e3a\uff0c\u63d0\u51fa\u91cf\u5316\u8fd9\u4e24\u79cd\u884c\u4e3a\u7684\u6307\u6807\uff0c\u5e76\u5728\u4f7f\u7528RoPE\u7684Transformer\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bbe\u8ba1\u4e86\u7eaf\u4f4d\u7f6e\u548c\u7eaf\u7b26\u53f7\u7684\u89c4\u8303\u4efb\u52a1\u3002", "result": "\u53d1\u73b0\u6240\u6709\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\u4e0e\u9891\u7387\u4f7f\u7528\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5934\u53ef\u8bbf\u95ee\u7684\u9891\u7387\u6765\u56e0\u679c\u6027\u5730\u63a7\u5236Transformer\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9RoPE\u7f16\u7801\u7684\u8be6\u7ec6\u7406\u89e3\uff0c\u9610\u660e\u4e86\u5176\u7279\u6027\u4e0e\u6a21\u578b\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u9891\u7387\u63a7\u5236\u5728Transformer\u6027\u80fd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2511.11653", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11653", "abs": "https://arxiv.org/abs/2511.11653", "authors": ["Duolin Sun", "Meixiu Long", "Dan Yang", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Junjie Wang", "Yue Shen", "Peng Wei", "Jian Wang", "Jinjie Gu"], "title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning", "comment": null, "summary": "Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.", "AI": {"tldr": "\u63d0\u51faGroupwise\u91cd\u6392\u5e8f\u8303\u5f0f\uff0c\u89e3\u51b3\u73b0\u6709Pointwise\u65b9\u6cd5\u7684\u6392\u540d\u8fd1\u89c6\u9677\u9631\u548cListwise\u65b9\u6cd5\u7684\u5217\u8868\u521a\u6027\u9650\u5236\uff0c\u901a\u8fc7\u7ec4\u5185\u6bd4\u8f83\u5e73\u8861\u7075\u6d3b\u6027\u548c\u5168\u5c40\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u91cd\u6392\u5e8f\u65b9\u6cd5\u5b58\u5728\u6838\u5fc3\u56f0\u5883\uff1aPointwise\u65b9\u6cd5\u7b80\u5355\u7075\u6d3b\u4f46\u72ec\u7acb\u8bc4\u4f30\u6587\u6863\uff0c\u5bb9\u6613\u9677\u5165\u6392\u540d\u8fd1\u89c6\u9677\u9631\uff1bListwise\u65b9\u6cd5\u80fd\u611f\u77e5\u5168\u5c40\u6392\u540d\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u9650\u5236\u3002", "method": "\u63d0\u51faGroupwise\u91cd\u6392\u5e8f\u8303\u5f0f\uff0c\u5c06\u67e5\u8be2\u548c\u4e00\u7ec4\u5019\u9009\u6587\u6863\u8054\u5408\u8f93\u5165\u6a21\u578b\u8fdb\u884c\u7ec4\u5185\u6bd4\u8f83\uff0c\u4e3a\u6bcf\u4e2a\u6587\u6863\u5206\u914d\u76f8\u5173\u6027\u5206\u6570\u3002\u91c7\u7528GRPO\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u7ed3\u5408\u5f02\u8d28\u5956\u52b1\u51fd\u6570\u6574\u5408\u6392\u540d\u6307\u6807\u548c\u5206\u5e03\u5956\u52b1\u3002\u8fd8\u63d0\u51fa\u5408\u6210\u9ad8\u8d28\u91cf\u68c0\u7d22\u548c\u6392\u540d\u6570\u636e\u7684\u521b\u65b0\u6d41\u7a0b\u3002", "result": "\u5728\u4e24\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u57fa\u51c6BRIGHT\u548cR2MED\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Groupwise\u8303\u5f0f\u5728\u4fdd\u6301Pointwise\u65b9\u6cd5\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86Listwise\u65b9\u6cd5\u7684\u6bd4\u8f83\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u91cd\u6392\u5e8f\u65b9\u6cd5\u7684\u7406\u8bba\u56f0\u5883\u3002"}}
{"id": "2511.11597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11597", "abs": "https://arxiv.org/abs/2511.11597", "authors": ["Michelle Chen Huebscher", "Katharine Mach", "Aleksandar Stani\u0107", "Markus Leippold", "Ben Gaiarin", "Zeke Hausfather", "Elisa Rawat", "Erich Fischer", "Massimiliano Ciaramita", "Joeri Rogelj", "Christian Buck", "Lierni Sestorain Saralegui", "Reto Knutti"], "title": "CLINB: A Climate Intelligence Benchmark for Foundational Models", "comment": "Questions, system prompt and model judge prompts available here: https://www.kaggle.com/datasets/deepmind/clinb-questions", "summary": "Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform \"hybrid\" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.", "AI": {"tldr": "CLINB\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6c14\u5019\u53d8\u5316\u4e13\u4e1a\u77e5\u8bc6\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u5f00\u653e\u5f0f\u3001\u591a\u6a21\u6001\u95ee\u7b54\u4efb\u52a1\u6d4b\u8bd5\u6a21\u578b\u7684\u77e5\u8bc6\u8d28\u91cf\u548c\u8bc1\u636e\u652f\u6301\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5728\u77e5\u8bc6\u7efc\u5408\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8bc1\u636e\u57fa\u7840\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u5305\u62ec\u5f15\u7528\u548c\u56fe\u50cf\u7684\u9ad8\u5ea6\u5e7b\u89c9\u7387\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u590d\u6742\u4e13\u4e1a\u77e5\u8bc6\u7684\u80fd\u529b\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6c14\u5019\u53d8\u5316\u8fd9\u6837\u7684\u4e13\u4e1a\u9886\u57df\uff0c\u9700\u8981\u786e\u4fdd\u6a21\u578b\u8f93\u51fa\u7684\u77e5\u8bc6\u8d28\u91cf\u548c\u8bc1\u636e\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165CLINB\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u95ee\u9898\u548c\u6c14\u5019\u79d1\u5b66\u5bb6\u5236\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5b9e\u65bd\u6a21\u578b\u9a71\u52a8\u7684\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u8bc4\u4f30\u591a\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u3001\u591a\u6a21\u6001\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u77e5\u8bc6\u7efc\u5408\u80fd\u529b\uff0c\u8fbe\u5230\u535a\u58eb\u7ea7\u522b\u7684\u7406\u89e3\u548c\u5448\u73b0\u8d28\u91cf\uff0c\u751a\u81f3\u4f18\u4e8e\u9886\u57df\u4e13\u5bb6\u8f85\u52a9\u8f83\u5f31\u6a21\u578b\u751f\u6210\u7684\u6df7\u5408\u7b54\u6848\u3002\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u8bc1\u636e\u57fa\u7840\u95ee\u9898\uff0c\u5f15\u7528\u548c\u56fe\u50cf\u7684\u5e7b\u89c9\u7387\u5f88\u9ad8\u3002", "conclusion": "\u5f25\u5408\u77e5\u8bc6\u7efc\u5408\u4e0e\u53ef\u9a8c\u8bc1\u5f52\u56e0\u4e4b\u95f4\u7684\u5dee\u8ddd\u5bf9\u4e8eAI\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u50cfCLINB\u8fd9\u6837\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u51c6\u6765\u6784\u5efa\u53ef\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2511.11595", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11595", "abs": "https://arxiv.org/abs/2511.11595", "authors": ["Aaron R. Allred", "Erin E. Richardson", "Sarah R. Bostrom", "James Crum", "Cara Spencer", "Chad Tossell", "Richard E. Niemeyer", "Leanne Hirshfield", "Allison P. A. Hayman"], "title": "Decision-Making Amid Information-Based Threats in Sociotechnical Systems: A Review", "comment": null, "summary": "Technological systems increasingly mediate human information exchange, spanning interactions among humans as well as between humans and artificial agents. The unprecedented scale and reliance on information disseminated through these systems substantially expand the scope of information-based influence that can both enable and undermine sound decision-making. Consequently, understanding and protecting decision-making today faces growing challenges, as individuals and organizations must navigate evolving opportunities and information-based threats across varied domains and information environments. While these risks are widely recognized, research remains fragmented: work evaluating information-based threat phenomena has progressed largely in isolation from foundational studies of human information processing. In this review, we synthesize insights from both domains to identify shared cognitive mechanisms that mediate vulnerability to information-based threats and shape behavioral outcomes. Finally, we outline directions for future research aimed at integrating these perspectives, emphasizing the importance of such integration for mitigating human vulnerabilities and aligning human-machine representations.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6280\u672f\u7cfb\u7edf\u4e2d\u4fe1\u606f\u4ea4\u6362\u5bf9\u4eba\u7c7b\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u4fe1\u606f\u5a01\u80c1\u4e0e\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u673a\u5236\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u6574\u5408\u7814\u7a76\u65b9\u5411\u4ee5\u51cf\u8f7b\u4eba\u7c7b\u8106\u5f31\u6027\u548c\u5bf9\u9f50\u4eba\u673a\u8868\u5f81\u3002", "motivation": "\u6280\u672f\u7cfb\u7edf\u5728\u4eba\u7c7b\u4fe1\u606f\u4ea4\u6362\u4e2d\u7684\u4e2d\u4ecb\u4f5c\u7528\u65e5\u76ca\u589e\u5f3a\uff0c\u8fd9\u65e2\u4e3a\u51b3\u7b56\u63d0\u4f9b\u4e86\u673a\u4f1a\u4e5f\u5e26\u6765\u4e86\u5a01\u80c1\u3002\u5f53\u524d\u7814\u7a76\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u4fe1\u606f\u5a01\u80c1\u73b0\u8c61\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u57fa\u7840\u7814\u7a76\u76f8\u4e92\u9694\u79bb\uff0c\u9700\u8981\u6574\u5408\u8fd9\u4e24\u4e2a\u9886\u57df\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u548c\u6574\u5408\u6765\u81ea\u4fe1\u606f\u5a01\u80c1\u8bc4\u4f30\u548c\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u4e24\u4e2a\u9886\u57df\u7684\u7814\u7a76\u6210\u679c\uff0c\u8bc6\u522b\u5171\u4eab\u7684\u8ba4\u77e5\u673a\u5236\uff0c\u8fd9\u4e9b\u673a\u5236\u4ecb\u5bfc\u4e86\u5bf9\u4fe1\u606f\u5a01\u80c1\u7684\u8106\u5f31\u6027\u5e76\u5851\u9020\u884c\u4e3a\u7ed3\u679c\u3002", "result": "\u8bc6\u522b\u51fa\u591a\u4e2a\u5171\u4eab\u7684\u8ba4\u77e5\u673a\u5236\uff0c\u8fd9\u4e9b\u673a\u5236\u5728\u4fe1\u606f\u5a01\u80c1\u8106\u5f31\u6027\u548c\u884c\u4e3a\u7ed3\u679c\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u6280\u672f\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "conclusion": "\u9700\u8981\u6574\u5408\u4fe1\u606f\u5a01\u80c1\u548c\u4eba\u7c7b\u4fe1\u606f\u5904\u7406\u7684\u7814\u7a76\u89c6\u89d2\uff0c\u8fd9\u5bf9\u4e8e\u51cf\u8f7b\u4eba\u7c7b\u8106\u5f31\u6027\u548c\u5b9e\u73b0\u4eba\u673a\u8868\u5f81\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e3a\u6b64\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.11643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11643", "abs": "https://arxiv.org/abs/2511.11643", "authors": ["Aswath Muthuselvam", "Jeevak Raj S", "Mohanaprasad K"], "title": "Real-time pothole detection with onboard sensors and camera on vehicles", "comment": null, "summary": "Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes", "AI": {"tldr": "\u4f7f\u7528\u8f66\u8f86\u4f20\u611f\u5668\u548cSVM\u5206\u7c7b\u5668\u5b9e\u65f6\u68c0\u6d4b\u9053\u8def\u5751\u6d1e\uff0c\u57282\u516c\u91cc\u8def\u6bb5\u4e0a\u8fbe\u523098.1%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u9053\u8def\u72b6\u51b5\u5bf9\u65e5\u5e38\u901a\u52e4\u81f3\u5173\u91cd\u8981\uff0c\u8f66\u8f86\u6570\u91cf\u589e\u52a0\u9700\u8981\u9891\u7e41\u8bc4\u4f30\u8def\u51b5\u4ee5\u786e\u4fdd\u4ea4\u901a\u987a\u7545\uff0c\u5c0f\u88c2\u7f1d\u53ef\u80fd\u53d1\u5c55\u6210\u5927\u5751\u6d1e\u3002", "method": "\u5229\u7528\u8f66\u8f86\u5185\u7f6e\u4f20\u611f\u5668\u6536\u96c6\u6570\u636e\uff0c\u91c7\u7528SVM\u5206\u7c7b\u5668\u8fdb\u884c\u5751\u6d1e\u68c0\u6d4b\u3002", "result": "\u57282\u516c\u91cc\u8def\u6bb5\uff08\u5305\u542b26\u4e2a\u5751\u6d1e\uff09\u4e0a\u6d4b\u8bd5\uff0c\u8fbe\u523098.1%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u65f6\u68c0\u6d4b\u9053\u8def\u5751\u6d1e\uff0c\u4e3a\u5927\u89c4\u6a21\u5751\u6d1e\u7ba1\u7406\u548c\u5206\u6790\u63d0\u4f9b\u6709\u7528\u6570\u636e\u3002"}}
{"id": "2511.11581", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.11581", "abs": "https://arxiv.org/abs/2511.11581", "authors": ["Burkhard Ringlein", "Jan van Lunteren", "Radu Stoica", "Thomas Parnell"], "title": "The Anatomy of a Triton Attention Kernel", "comment": null, "summary": "A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eTriton\u7684\u8de8\u5e73\u53f0\u5206\u9875\u6ce8\u610f\u529b\u5185\u6838\uff0c\u5728NVIDIA\u548cAMD GPU\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5c06\u901a\u7528Triton\u6ce8\u610f\u529b\u5185\u6838\u6027\u80fd\u4ece19.7%\u63d0\u5347\u5230105.9%\uff0c\u5c55\u793a\u4e86\u5f00\u6e90\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u5982\u4f55\u5b9e\u73b0\u6a21\u578b\u8de8GPU\u5382\u5546\u7684\u53ef\u79fb\u690d\u6027\u3002", "motivation": "\u957f\u671f\u4ee5\u6765\uff0c\u4e1a\u754c\u548c\u5b66\u672f\u754c\u90fd\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u53ef\u8de8\u786c\u4ef6\u67b6\u6784\u79fb\u690d\u3001\u65e0\u9700\u4f4e\u7ea7\u624b\u52a8\u8c03\u4f18\u4e14\u4ecd\u80fd\u63d0\u4f9b\u6700\u4f73\u6548\u7387\u7684LLM\u63a8\u7406\u5e73\u53f0\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eTriton\u9886\u57df\u7279\u5b9a\u5373\u65f6\u7f16\u8bd1\u8bed\u8a00\u7684\u6700\u5148\u8fdb\u5206\u9875\u6ce8\u610f\u529b\u5185\u6838\uff0c\u91c7\u7528\u9ad8\u5c42\u6b21\u65b9\u6cd5\u3001\u5173\u952e\u7b97\u6cd5\u548c\u7cfb\u7edf\u7ea7\u6539\u8fdb\uff0c\u4ee5\u53ca\u53c2\u6570\u81ea\u52a8\u8c03\u4f18\u6280\u672f\u3002", "result": "\u5728NVIDIA\u548cAMD GPU\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c06\u901a\u7528Triton\u6ce8\u610f\u529b\u5185\u6838\u6027\u80fd\u4ece19.7%\u63d0\u5347\u5230105.9%\u3002", "conclusion": "\u8bc1\u660e\u4e86\u53ef\u79fb\u690d\u3001\u9ad8\u6548\u7684\u8de8\u5e73\u53f0LLM\u63a8\u7406\u662f\u53ef\u884c\u7684\uff0c\u5f00\u6e90\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u53ef\u4ee5\u7528\u4e8e\u89e3\u9501\u6a21\u578b\u5728\u4e0d\u540cGPU\u5382\u5546\u95f4\u7684\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2511.11847", "categories": ["cs.IR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11847", "abs": "https://arxiv.org/abs/2511.11847", "authors": ["Ryan Singh", "Austin Hamilton", "Amanda White", "Michael Wise", "Ibrahim Yousif", "Arthur Carvalho", "Zhe Shan", "Reza Abrisham Baf", "Mohammad Mayyas", "Lora A. Cavuoto", "Fadel M. Megahed"], "title": "A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches", "comment": "25 pages, 5 figures", "summary": "Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u804a\u5929\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u5de5\u4e1a5.0\u73af\u5883\u4e0b\u7684\u5b89\u5168\u57f9\u8bad\u7cfb\u7edf\uff0c\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u6210\u672c\u7684\u8bbe\u8ba1\u8981\u6c42\u3002", "motivation": "\u5de5\u4e1a5.0\u5c06\u5236\u9020\u8303\u5f0f\u8f6c\u5411\u66f4\u52a0\u4ee5\u4eba\u4e3a\u672c\u7684\u64cd\u4f5c\uff0c\u786e\u4fdd\u5de5\u4eba\u5b89\u5168\u662f\u73b0\u4ee3\u5236\u9020\u73af\u5883\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u591a\u6a21\u6001\u804a\u5929\u673a\u5668\u4eba\uff0c\u4f7f\u752824\u79cdRAG\u914d\u7f6e\u8fdb\u884c\u5168\u56e0\u5b50\u8bbe\u8ba1\u6d4b\u8bd5\u3002", "result": "\u6700\u4f73\u914d\u7f6e\u5b9e\u73b0\u4e8686.66%\u7684\u51c6\u786e\u7387\u300110.04\u79d2\u7684\u5e73\u5747\u5ef6\u8fdf\u548c\u6bcf\u6b21\u67e5\u8be20.005\u7f8e\u5143\u7684\u5e73\u5747\u6210\u672c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e09\u4e2a\u8d21\u732e\uff1a\u5f00\u6e90\u9886\u57df\u5b89\u5168\u57f9\u8bad\u804a\u5929\u673a\u5668\u4eba\u3001\u9a8c\u8bc1\u7684AI\u8f85\u52a9\u5b89\u5168\u6307\u5bfc\u8bc4\u4f30\u57fa\u51c6\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5de5\u4e1a5.0\u73af\u5883\u4e2dAI\u8d4b\u80fd\u5b89\u5168\u57f9\u8bad\u7cfb\u7edf\u7684\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2511.11635", "categories": ["cs.CY", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11635", "abs": "https://arxiv.org/abs/2511.11635", "authors": ["Rui Jia", "Min Zhang", "Fengrui Liu", "Bo Jiang", "Kun Kuang", "Zhongxiang Dai"], "title": "EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation", "comment": null, "summary": "High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.", "AI": {"tldr": "EduAgentQG\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u4e2a\u6027\u5316\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8d28\u91cf\u4e0d\u7a33\u5b9a\u3001\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u9ad8\u8d28\u91cf\u4e2a\u6027\u5316\u9898\u5e93\u5bf9\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u8bbe\u8ba1\u95ee\u9898\u8017\u65f6\u4e14\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u5b66\u4e60\u9700\u6c42\uff0c\u73b0\u6709\u81ea\u52a8\u95ee\u9898\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8d28\u91cf\u4e0d\u7a33\u5b9a\u3001\u591a\u6837\u6027\u6709\u9650\u3001\u4e0e\u6559\u80b2\u76ee\u6807\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e94\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff1aPlanner\u751f\u6210\u7ed3\u6784\u5316\u8bbe\u8ba1\u8ba1\u5212\u548c\u591a\u4e2a\u95ee\u9898\u65b9\u5411\uff1bWriter\u57fa\u4e8e\u8ba1\u5212\u751f\u6210\u5019\u9009\u95ee\u9898\uff0c\u5229\u7528Solver\u548cEducator\u7684\u53cd\u9988\u4f18\u5316\u8d28\u91cf\u548c\u591a\u6837\u6027\uff1bSolver\u548cEducator\u8fdb\u884c\u591a\u7ef4\u5ea6\u4e8c\u5143\u8bc4\u5206\uff1bChecker\u8fdb\u884c\u6700\u7ec8\u9a8c\u8bc1\uff0c\u786e\u4fdd\u7b54\u6848\u6b63\u786e\u6027\u548c\u6e05\u6670\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u5b66\u95ee\u9898\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEduAgentQG\u5728\u95ee\u9898\u591a\u6837\u6027\u3001\u76ee\u6807\u4e00\u81f4\u6027\u548c\u6574\u4f53\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5355\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\uff0cEduAgentQG\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u4e14\u4e0e\u6559\u80b2\u76ee\u6807\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u6709\u6548\u51cf\u8f7b\u6559\u5e08\u5de5\u4f5c\u91cf\u5e76\u63d0\u5347\u6559\u80b2\u8d44\u6e90\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.11583", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11583", "abs": "https://arxiv.org/abs/2511.11583", "authors": ["Fernando Spadea", "Oshani Seneviratne"], "title": "Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations", "comment": "10 pages, 3 figures, RAGE-KG 2025", "summary": "Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.", "AI": {"tldr": "RAG-FLARKO\u662f\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u7684\u91d1\u878d\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5e76\u884c\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u6765\u514b\u670d\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u8350\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u91d1\u878d\u63a8\u8350\u4e2d\u5b58\u5728\u4e0a\u4e0b\u6587\u9650\u5236\u3001\u5e7b\u89c9\u95ee\u9898\u4ee5\u53ca\u7f3a\u4e4f\u884c\u4e3a\u57fa\u7840\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u53ef\u6269\u5c55\u7684\u5efa\u8bae\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u5e76\u884c\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u65b9\u6cd5\uff1a\u9996\u5148\u4ece\u7528\u6237\u4ea4\u6613\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u884c\u4e3a\u76f8\u5173\u5b9e\u4f53\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u5e02\u573a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u4fe1\u53f7\uff0c\u6784\u5efa\u7d27\u51d1\u7684\u57fa\u4e8e\u4e8b\u5b9e\u7684\u5b50\u56fe\u4f9bLLM\u4f7f\u7528\u3002", "result": "\u5728\u771f\u5b9e\u91d1\u878d\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0cRAG-FLARKO\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\uff0c\u4f7f\u66f4\u5c0f\u3001\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u5728\u76c8\u5229\u6027\u548c\u884c\u4e3a\u5bf9\u9f50\u65b9\u9762\u90fd\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u57fa\u4e8e\u4e8b\u5b9e\u7684\u91d1\u878dAI\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\uff0c\u80fd\u591f\u51cf\u5c11\u4e0a\u4e0b\u6587\u5f00\u9500\u5e76\u589e\u5f3a\u6a21\u578b\u5bf9\u76f8\u5173\u4fe1\u606f\u7684\u5173\u6ce8\u3002"}}
{"id": "2511.12004", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12004", "abs": "https://arxiv.org/abs/2511.12004", "authors": ["Ganlin Xu", "Zhitao Yin", "Linghao Zhang", "Jiaqing Liang", "Weijia Lu", "Xiaodong Zhang", "Zhifei Yang", "Sihang Jiang", "Deqing Yang"], "title": "ComLQ: Benchmarking Complex Logical Queries in Information Retrieval", "comment": "Accepted by AAAI 2026", "summary": "Information retrieval (IR) systems play a critical role in navigating information overload across various applications. Existing IR benchmarks primarily focus on simple queries that are semantically analogous to single- and multi-hop relations, overlooking \\emph{complex logical queries} involving first-order logic operations such as conjunction ($\\land$), disjunction ($\\lor$), and negation ($\\lnot$). Thus, these benchmarks can not be used to sufficiently evaluate the performance of IR models on complex queries in real-world scenarios. To address this problem, we propose a novel method leveraging large language models (LLMs) to construct a new IR dataset \\textbf{ComLQ} for \\textbf{Com}plex \\textbf{L}ogical \\textbf{Q}ueries, which comprises 2,909 queries and 11,251 candidate passages. A key challenge in constructing the dataset lies in capturing the underlying logical structures within unstructured text. Therefore, by designing the subgraph-guided prompt with the subgraph indicator, an LLM (such as GPT-4o) is guided to generate queries with specific logical structures based on selected passages. All query-passage pairs in ComLQ are ensured \\emph{structure conformity} and \\emph{evidence distribution} through expert annotation. To better evaluate whether retrievers can handle queries with negation, we further propose a new evaluation metric, \\textbf{Log-Scaled Negation Consistency} (\\textbf{LSNC@$K$}). As a supplement to standard relevance-based metrics (such as nDCG and mAP), LSNC@$K$ measures whether top-$K$ retrieved passages violate negation conditions in queries. Our experimental results under zero-shot settings demonstrate existing retrieval models' limited performance on complex logical queries, especially on queries with negation, exposing their inferior capabilities of modeling exclusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86ComLQ\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u5904\u7406\u590d\u6742\u903b\u8f91\u67e5\u8be2\u7684\u80fd\u529b\uff0c\u5305\u542b2,909\u4e2a\u67e5\u8be2\u548c11,251\u4e2a\u5019\u9009\u6bb5\u843d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807LSNC@K\u6765\u8861\u91cf\u68c0\u7d22\u6a21\u578b\u5904\u7406\u5426\u5b9a\u67e5\u8be2\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709IR\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u67e5\u8be2\uff0c\u5ffd\u7565\u4e86\u5305\u542b\u5408\u53d6\u3001\u6790\u53d6\u548c\u5426\u5b9a\u7b49\u4e00\u9636\u903b\u8f91\u64cd\u4f5c\u7684\u590d\u6742\u903b\u8f91\u67e5\u8be2\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30IR\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5bf9\u590d\u6742\u67e5\u8be2\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u6784\u5efaComLQ\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5b50\u56fe\u5f15\u5bfc\u63d0\u793a\u548c\u5b50\u56fe\u6307\u793a\u5668\uff0c\u5f15\u5bfcLLM\u57fa\u4e8e\u9009\u5b9a\u6bb5\u843d\u751f\u6210\u5177\u6709\u7279\u5b9a\u903b\u8f91\u7ed3\u6784\u7684\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u786e\u4fdd\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u8bc1\u636e\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u67e5\u8be2\u4e0a\u7684\u6027\u80fd\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5426\u5b9a\u67e5\u8be2\u65f6\u8868\u73b0\u51fa\u8f83\u5dee\u7684\u6392\u9664\u5efa\u6a21\u80fd\u529b\u3002", "conclusion": "ComLQ\u6570\u636e\u96c6\u586b\u8865\u4e86\u590d\u6742\u903b\u8f91\u67e5\u8be2\u8bc4\u4f30\u7684\u7a7a\u767d\uff0cLSNC@K\u6307\u6807\u6709\u6548\u8865\u5145\u4e86\u6807\u51c6\u76f8\u5173\u6027\u6307\u6807\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u5728\u5904\u7406\u5426\u5b9a\u67e5\u8be2\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.11600", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11600", "abs": "https://arxiv.org/abs/2511.11600", "authors": ["Piyushkumar Patel"], "title": "CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models", "comment": null, "summary": "While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This \"hallucination\" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.\n  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.\n  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\\% of the time while missing only 8.3\\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.", "AI": {"tldr": "CausalGuard\u662f\u4e00\u79cd\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u548c\u7b26\u53f7\u903b\u8f91\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u548c\u9632\u6b62\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u66f4\u65e9\u5e72\u9884\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u81ea\u4fe1\u5730\u9648\u8ff0\u542c\u8d77\u6765\u5408\u7406\u4f46\u865a\u5047\u7684\u4fe1\u606f\uff0c\u8fd9\u6210\u4e3a\u5728\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u7684\u573a\u666f\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u6a21\u578b\u7684\u4e3b\u8981\u969c\u788d\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\uff0c\u8981\u4e48\u589e\u52a0\u663e\u8457\u8ba1\u7b97\u6210\u672c\uff0c\u6216\u8005\u672a\u80fd\u89e3\u51b3\u5e7b\u89c9\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "CausalGuard\u901a\u8fc7\u4e24\u6761\u4e92\u8865\u8def\u5f84\u5de5\u4f5c\uff1a\u4e00\u6761\u8ffd\u8e2a\u6a21\u578b\u5df2\u77e5\u4fe1\u606f\u4e0e\u751f\u6210\u5185\u5bb9\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u53e6\u4e00\u6761\u4f7f\u7528\u81ea\u52a8\u63a8\u7406\u68c0\u67e5\u903b\u8f91\u4e00\u81f4\u6027\u3002\u7cfb\u7edf\u7406\u89e3\u5bfc\u81f4\u865a\u5047\u9648\u8ff0\u7684\u56e0\u679c\u94fe\uff0c\u5e76\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u65e9\u671f\u8fdb\u884c\u5e72\u9884\u3002", "result": "\u572812\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCausalGuard\u6b63\u786e\u8bc6\u522b\u5e7b\u89c9\u7684\u6982\u7387\u4e3a89.3%\uff0c\u4ec5\u9057\u6f0f8.3%\u7684\u5b9e\u9645\u5e7b\u89c9\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u51cf\u5c11\u4e86\u8fd180%\u7684\u9519\u8bef\u58f0\u660e\uff0c\u540c\u65f6\u4fdd\u6301\u56de\u7b54\u81ea\u7136\u548c\u6709\u7528\u3002\u5728\u9700\u8981\u591a\u6b65\u903b\u8f91\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u5c24\u5176\u51fa\u8272\u3002", "conclusion": "CausalGuard\u901a\u8fc7\u5c55\u793a\u5176\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u533b\u7597\u8bca\u65ad\u6216\u91d1\u878d\u5206\u6790\u7b49\u654f\u611f\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u9886\u57df\u7406\u89e3\u51b3\u7b56\u539f\u56e0\u4e0e\u51b3\u7b56\u672c\u8eab\u540c\u7b49\u91cd\u8981\u3002"}}
{"id": "2511.11655", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11655", "abs": "https://arxiv.org/abs/2511.11655", "authors": ["Maurice Flechtner"], "title": "Automatic generation of DRI Statements", "comment": "Master Thesis", "summary": "Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u5ba1\u8bae\u7406\u7531\u6307\u6570(DRI)\u9648\u8ff0\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "motivation": "\u4f20\u7edfDRI\u9648\u8ff0\u751f\u6210\u8fc7\u7a0b\u590d\u6742\u8017\u65f6\uff0c\u9650\u5236\u4e86\u5ba1\u8bae\u8d28\u91cf\u8bc4\u4f30\u7684\u5b9e\u65bd\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u964d\u4f4e\u7814\u7a76\u95e8\u69db\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5148\u8fdbNLP\u548cLLM\u7684\u81ea\u52a8\u5316DRI\u9648\u8ff0\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5b9e\u73b0\u9648\u8ff0\u7684\u81ea\u52a8\u521b\u5efa\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7684\u81ea\u52a8\u5316DRI\u9648\u8ff0\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fdb\u884c\u7efc\u5408\u5ba1\u8bae\u8fc7\u7a0b\u8bc4\u4f30\u7684\u969c\u788d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c06\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6574\u5408\u5230\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\u4e2d\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u6a21\u677f\uff0c\u63a8\u52a8\u4e86\u5ba1\u8bae\u8fc7\u7a0b\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u53d1\u5c55\u3002"}}
{"id": "2511.11662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11662", "abs": "https://arxiv.org/abs/2511.11662", "authors": ["Ziyuan Gao"], "title": "AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation", "comment": "Accepted for publication in WACV 2026 (Round 2)", "summary": "Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.", "AI": {"tldr": "AGENet\u662f\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5c11\u6837\u672c\u5206\u5272\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u6d4b\u5730\u8ddd\u79bb\u5b66\u4e60\u6574\u5408\u7a7a\u95f4\u5173\u7cfb\uff0c\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u7cbe\u786e\u8fb9\u754c\u5206\u5272\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5f62\u6210\u4e86\u663e\u8457\u74f6\u9888\u3002\u73b0\u6709\u5c11\u6837\u672c\u5206\u5272\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u7cbe\u786e\u8fb9\u754c\u5212\u5206\u65b9\u9762\u8868\u73b0\u6b20\u4f73\uff0c\u7279\u522b\u662f\u5728\u89e3\u5256\u7ed3\u6784\u76f8\u4f3c\u4e14\u7f3a\u4e4f\u8db3\u591f\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faAGENet\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a(1)\u8fb9\u7f18\u611f\u77e5\u6d4b\u5730\u8ddd\u79bb\u5b66\u4e60\u6a21\u5757\uff0c\u901a\u8fc7\u8fed\u4ee3\u5feb\u901f\u884c\u8fdb\u7ec6\u5316\u6765\u5c0a\u91cd\u89e3\u5256\u8fb9\u754c\uff1b(2)\u81ea\u9002\u5e94\u539f\u578b\u63d0\u53d6\uff0c\u901a\u8fc7\u7a7a\u95f4\u52a0\u6743\u805a\u5408\u6355\u83b7\u5168\u5c40\u7ed3\u6784\u548c\u5c40\u90e8\u8fb9\u754c\u7ec6\u8282\uff1b(3)\u81ea\u9002\u5e94\u53c2\u6570\u5b66\u4e60\uff0c\u81ea\u52a8\u9002\u5e94\u4e0d\u540c\u5668\u5b98\u7279\u5f81\u3002", "result": "\u5728\u591a\u6837\u5316\u533b\u5b66\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fb9\u754c\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "AGENet\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u975e\u5e38\u9002\u5408\u9700\u8981\u7cbe\u786e\u5206\u5272\u7684\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2511.11584", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11584", "abs": "https://arxiv.org/abs/2511.11584", "authors": ["Jacob Drori", "Luke Marks", "Bryce Woodworth", "Alex Cloud", "Alexander Matt Turner"], "title": "Output Supervision Can Obfuscate the Chain of Thought", "comment": null, "summary": "OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.", "AI": {"tldr": "\u8bad\u7ec3\u6a21\u578b\u4ec5\u4f7f\u7528\u8f93\u51fa\u76d1\u63a7\u5668\uff08\u65e0\u601d\u7ef4\u94fe\u8bbf\u95ee\uff09\u4ecd\u53ef\u80fd\u5bfc\u81f4\u9690\u853d\u7684\u601d\u7ef4\u94fe\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7f13\u89e3\u673a\u5236\u6765\u6539\u5584\u76d1\u63a7\u6027\u548c\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "OpenAI\u7684\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u8f93\u51fa\u76d1\u63a7\u5668\u8bad\u7ec3\u6a21\u578b\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u9632\u6b62\u9690\u853d\u601d\u7ef4\u94fe\u7684\u4ea7\u751f\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u6790\u4e86\u4e24\u79cd\u5bfc\u81f4\u9690\u853d\u601d\u7ef4\u94fe\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u7f13\u89e3\u63aa\u65bd\u6765\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684\u7f13\u89e3\u673a\u5236\u5728\u76d1\u63a7\u6027\u548c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u76f8\u6bd4\u5e38\u89c4\u8bad\u7ec3\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u4ec5\u4f9d\u8d56\u8f93\u51fa\u76d1\u63a7\u5668\u4e0d\u8db3\u4ee5\u9632\u6b62\u9690\u853d\u601d\u7ef4\u94fe\uff0c\u9700\u8981\u7ed3\u5408\u7279\u5b9a\u7f13\u89e3\u673a\u5236\u6765\u786e\u4fdd\u601d\u7ef4\u94fe\u7684\u53ef\u76d1\u63a7\u6027\u3002"}}
{"id": "2511.12081", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12081", "abs": "https://arxiv.org/abs/2511.12081", "authors": ["Bencheng Yan", "Yuejie Lei", "Zhiyuan Zeng", "Di Wang", "Kaiyi Lin", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction", "comment": null, "summary": "Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Field-Aware Transformer (FAT)\u6765\u89e3\u51b3CTR\u9884\u6d4b\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u6269\u5927\u4f46\u6536\u76ca\u9012\u51cf\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5b57\u6bb5\u7684\u4ea4\u4e92\u5148\u9a8c\uff0c\u4f7f\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u5b57\u6bb5\u6570\u91cf\u800c\u975e\u8bcd\u6c47\u91cf\u6210\u6b63\u6bd4\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u5e42\u5f8b\u589e\u957f\u3002", "motivation": "\u5f53\u524dCTR\u9884\u6d4b\u7684\u6df1\u5ea6\u6a21\u578b\u5728\u89c4\u6a21\u6269\u5927\u65f6\u8868\u73b0\u51fa\u6536\u76ca\u8fc5\u901f\u9012\u51cf\uff0c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e73\u6ed1\u53ef\u9884\u6d4b\u589e\u76ca\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002\u6839\u672c\u539f\u56e0\u662f\u7ed3\u6784\u9519\u914d\uff1aTransformer\u5047\u8bbe\u5e8f\u5217\u7ec4\u5408\u6027\uff0c\u800cCTR\u6570\u636e\u9700\u8981\u5728\u9ad8\u57fa\u6570\u8bed\u4e49\u5b57\u6bb5\u4e0a\u8fdb\u884c\u7ec4\u5408\u63a8\u7406\u3002", "method": "\u63d0\u51faField-Aware Transformer (FAT)\uff0c\u901a\u8fc7\u5206\u89e3\u7684\u5185\u5bb9\u5bf9\u9f50\u548c\u8de8\u5b57\u6bb5\u8c03\u5236\uff0c\u5c06\u57fa\u4e8e\u5b57\u6bb5\u7684\u4ea4\u4e92\u5148\u9a8c\u5d4c\u5165\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u3002\u8fd9\u79cd\u8bbe\u8ba1\u786e\u4fdd\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u5b57\u6bb5\u6570\u91cfF\u6210\u6b63\u6bd4\uff0c\u800c\u4e0d\u662f\u603b\u8bcd\u6c47\u91cfn >> F\u3002", "result": "FAT\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8AUC\u8fbe+0.51%\u3002\u5728\u7ebf\u90e8\u7f72\u65f6\uff0c\u5b9e\u73b0\u4e86+2.33%\u7684CTR\u548c+0.66%\u7684RPM\u63d0\u5347\u3002\u9996\u6b21\u5efa\u7acb\u4e86CTR\u6a21\u578b\u7684\u6b63\u5f0f\u6269\u5c55\u5b9a\u5f8b\uff0c\u89e3\u91ca\u4e86\u89c2\u5bdf\u5230\u7684\u5e42\u5f8b\u7f29\u653e\u884c\u4e3a\u3002", "conclusion": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u6709\u6548\u7684\u6269\u5c55\u4e0d\u662f\u6765\u81ea\u89c4\u6a21\uff0c\u800c\u662f\u6765\u81ea\u7ed3\u6784\u5316\u8868\u8fbe\u80fd\u529b\u2014\u2014\u67b6\u6784\u4e0e\u6570\u636e\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u3002FAT\u901a\u8fc7\u6062\u590d\u8fd9\u79cd\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u53ef\u9884\u6d4b\u7684\u5e42\u5f8b\u7f29\u653e\u548c\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.11611", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11611", "abs": "https://arxiv.org/abs/2511.11611", "authors": ["David H. Silver"], "title": "Quantifying Skill and Chance: A Unified Framework for the Geometry of Games", "comment": null, "summary": "We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6e38\u620f\u5efa\u6a21\u4e3a\u968f\u673a\u51b3\u7b56\u6811\u6765\u5206\u79bb\u6280\u80fd\u548c\u8fd0\u6c14\u6210\u5206\uff0c\u5b9a\u4e49\u4e86\u6280\u80fd-\u8fd0\u6c14\u6307\u6570S(G)\u5728[-1,1]\u8303\u56f4\u5185\uff0c\u5e94\u7528\u4e8e30\u4e2a\u6e38\u620f\u63ed\u793a\u4e86\u4ece\u7eaf\u8fd0\u6c14\u5230\u7eaf\u6280\u80fd\u7684\u8fde\u7eed\u8c31\u7cfb\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u6e38\u620f\u4e2d\u6280\u80fd\u548c\u8fd0\u6c14\u7684\u76f8\u5bf9\u8d21\u732e\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u7406\u89e3\u73a9\u5bb6\u5f71\u54cd\u529b\u3001\u6e38\u620f\u5e73\u8861\u6027\u548c\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "method": "\u5c06\u6e38\u620f\u5efa\u6a21\u4e3a\u968f\u673a\u51b3\u7b56\u6811\uff0c\u5c06\u6e38\u620f\u7ed3\u679c\u5206\u89e3\u4e3a\u6280\u80fd\u6760\u6746K\u548c\u8fd0\u6c14\u6760\u6746L\uff0c\u5b9a\u4e49\u6280\u80fd-\u8fd0\u6c14\u6307\u6570S(G) = (K - L)/(K + L)\uff0c\u5e76\u5f15\u5165\u6ce2\u52a8\u6027Sigma\u6765\u91cf\u5316\u8fde\u7eed\u56de\u5408\u7684\u7ed3\u679c\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5206\u679030\u4e2a\u6e38\u620f\u663e\u793a\uff1a\u786c\u5e01\u6295\u63b7S=-1\uff08\u7eaf\u8fd0\u6c14\uff09\uff0c\u897f\u6d0b\u53cc\u9646\u68cbS=0\uff0c\u6251\u514bS=0.33\uff08\u4e2d\u7b49\u6280\u80fd\u4e3b\u5bfc\uff09\uff0c\u56fd\u9645\u8c61\u68cbS=+1\uff08\u7eaf\u6280\u80fd\uff09\u3002\u6251\u514b\u7684K=0.40\u00b10.03\uff0cSigma=0.80\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u4e00\u822c\u968f\u673a\u51b3\u7b56\u7cfb\u7edf\uff0c\u4e3a\u73a9\u5bb6\u5f71\u54cd\u529b\u6bd4\u8f83\u3001\u6e38\u620f\u5e73\u8861\u6027\u8bc4\u4f30\u548c\u98ce\u9669\u5206\u6790\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5728\u6e38\u620f\u8bbe\u8ba1\u3001AI\u8bc4\u4f30\u548c\u98ce\u9669\u8bc4\u4f30\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2511.11687", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11687", "abs": "https://arxiv.org/abs/2511.11687", "authors": ["Dragan Filimonovic", "Christian Rutzer", "Jeffrey Macher", "Rolf Weder"], "title": "Generative AI as a Linguistic Equalizer in Global Science", "comment": null, "summary": "For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790560\u4e07\u7bc7\u79d1\u5b66\u8bba\u6587\uff0c\u9996\u6b21\u5927\u89c4\u6a21\u9a8c\u8bc1\u4e86\u751f\u6210\u5f0fAI\uff08\u7279\u522b\u662fChatGPT\u53d1\u5e03\u540e\uff09\u5982\u4f55\u5e2e\u52a9\u975e\u82f1\u8bed\u56fd\u5bb6\u4f5c\u8005\u7f29\u5c0f\u4e0e\u82f1\u8bed\u6bcd\u8bed\u4f5c\u8005\u5728\u79d1\u5b66\u5199\u4f5c\u4e2d\u7684\u8bed\u8a00\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u4e86\u5168\u7403\u79d1\u5b66\u4ea4\u6d41\u7684\u8bed\u8a00\u5e73\u7b49\u3002", "motivation": "\u82f1\u8bed\u5728\u5168\u7403\u79d1\u5b66\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\u957f\u671f\u963b\u788d\u4e86\u975e\u6bcd\u8bed\u8005\u7684\u53c2\u4e0e\uff0c\u751f\u6210\u5f0fAI\u7684\u51fa\u73b0\u4e3a\u89e3\u51b3\u8fd9\u4e00\u4e0d\u5e73\u7b49\u95ee\u9898\u63d0\u4f9b\u4e86\u6280\u672f\u53ef\u80fd\u6027\u3002", "method": "\u4f7f\u7528SciBERT\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u5206\u67902021-2024\u5e74\u95f4560\u4e07\u7bc7\u79d1\u5b66\u8bba\u6587\uff0c\u6bd4\u8f83\u751f\u6210\u5f0fAI\u8f85\u52a9\u548c\u975e\u8f85\u52a9\u51fa\u7248\u7269\u4e0e\u7f8e\u7c4d\u4f5c\u8005\u79d1\u5b66\u5199\u4f5c\u57fa\u51c6\u7684\u8bed\u8a00\u76f8\u4f3c\u6027\uff0c\u8ffd\u8e2a\u98ce\u683c\u8d8b\u540c\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002", "result": "ChatGPT\u53d1\u5e03\u540e\uff0c\u751f\u6210\u5f0fAI\u8f85\u52a9\u7684\u51fa\u7248\u7269\u663e\u793a\u51fa\u663e\u8457\u4e14\u4e0d\u65ad\u589e\u957f\u7684\u8bed\u8a00\u8d8b\u540c\u6548\u5e94\uff0c\u7279\u522b\u662f\u5728\u4e0e\u82f1\u8bed\u8bed\u8a00\u8ddd\u79bb\u8f83\u8fdc\u7684\u56fd\u5bb6\u7684\u56fd\u5185\u5408\u8457\u56e2\u961f\u4e2d\u6548\u679c\u6700\u5f3a\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u6b63\u5728\u901a\u8fc7\u51cf\u5c11\u7814\u7a76\u4e2d\u7684\u8bed\u8a00\u969c\u788d\uff0c\u5f00\u59cb\u91cd\u5851\u5168\u7403\u79d1\u5b66\u4ea4\u6d41\u683c\u5c40\uff0c\u4e3a\u8bed\u8a00\u5e73\u7b49\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2511.12114", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12114", "abs": "https://arxiv.org/abs/2511.12114", "authors": ["Chengyi Liu", "Xiao Chen", "Shijie Wang", "Wenqi Fan", "Qing Li"], "title": "Continuous-time Discrete-space Diffusion Model for Recommendation", "comment": "Accepted by WSDM 2026", "summary": "In the era of information explosion, Recommender Systems (RS) are essential for alleviating information overload and providing personalized user experiences. Recent advances in diffusion-based generative recommenders have shown promise in capturing the dynamic nature of user preferences. These approaches explore a broader range of user interests by progressively perturbing the distribution of user-item interactions and recovering potential preferences from noise, enabling nuanced behavioral understanding. However, existing diffusion-based approaches predominantly operate in continuous space through encoded graph-based historical interactions, which may compromise potential information loss and suffer from computational inefficiency. As such, we propose CDRec, a novel Continuous-time Discrete-space Diffusion Recommendation framework, which models user behavior patterns through discrete diffusion on historical interactions over continuous time. The discrete diffusion algorithm operates via discrete element operations (e.g., masking) while incorporating domain knowledge through transition matrices, producing more meaningful diffusion trajectories. Furthermore, the continuous-time formulation enables flexible adaptive sampling. To better adapt discrete diffusion models to recommendations, CDRec introduces: (1) a novel popularity-aware noise schedule that generates semantically meaningful diffusion trajectories, and (2) an efficient training framework combining consistency parameterization for fast sampling and a contrastive learning objective guided by multi-hop collaborative signals for personalized recommendation. Extensive experiments on real-world datasets demonstrate CDRec's superior performance in both recommendation accuracy and computational efficiency.", "AI": {"tldr": "CDRec\u662f\u4e00\u4e2a\u8fde\u7eed\u65f6\u95f4\u79bb\u6563\u7a7a\u95f4\u7684\u6269\u6563\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u6269\u6563\u7b97\u6cd5\u5728\u5386\u53f2\u4ea4\u4e92\u4e0a\u5efa\u6a21\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\uff0c\u7ed3\u5408\u6d41\u884c\u5ea6\u611f\u77e5\u566a\u58f0\u8c03\u5ea6\u548c\u591a\u8df3\u534f\u540c\u4fe1\u53f7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u63a8\u8350\u65b9\u6cd5\u4e3b\u8981\u5728\u8fde\u7eed\u7a7a\u95f4\u64cd\u4f5c\uff0c\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u5f00\u53d1\u5728\u79bb\u6563\u7a7a\u95f4\u64cd\u4f5c\u7684\u6269\u6563\u63a8\u8350\u6846\u67b6\u6765\u66f4\u597d\u5730\u6355\u6349\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u63d0\u51faCDRec\u6846\u67b6\uff1a1\uff09\u5728\u8fde\u7eed\u65f6\u95f4\u4e0a\u5bf9\u5386\u53f2\u4ea4\u4e92\u8fdb\u884c\u79bb\u6563\u6269\u6563\u5efa\u6a21\uff1b2\uff09\u5f15\u5165\u6d41\u884c\u5ea6\u611f\u77e5\u566a\u58f0\u8c03\u5ea6\u751f\u6210\u6709\u8bed\u4e49\u610f\u4e49\u7684\u6269\u6563\u8f68\u8ff9\uff1b3\uff09\u7ed3\u5408\u4e00\u81f4\u6027\u53c2\u6570\u5316\u5feb\u901f\u91c7\u6837\u548c\u591a\u8df3\u534f\u540c\u4fe1\u53f7\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCDRec\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CDRec\u901a\u8fc7\u8fde\u7eed\u65f6\u95f4\u79bb\u6563\u7a7a\u95f4\u6269\u6563\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u63a8\u8350\u65b9\u6cd5\u7684\u4fe1\u606f\u635f\u5931\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11689", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11689", "abs": "https://arxiv.org/abs/2511.11689", "authors": ["Thomas D. Hull", "Lizhe Zhang", "Patricia A. Arean", "Matteo Malgaroli"], "title": "Mental Health Generative AI is Safe, Promotes Social Health, and Reduces Depression and Anxiety: Real World Evidence from a Naturalistic Cohort", "comment": null, "summary": "Generative artificial intelligence (GAI) chatbots built for mental health could deliver safe, personalized, and scalable mental health support. We evaluate a foundation model designed for mental health. Adults completed mental health measures while engaging with the chatbot between May 15, 2025 and September 15, 2025. Users completed an opt-in consent, demographic information, mental health symptoms, social connection, and self-identified goals. Measures were repeated every two weeks up to 6 weeks, and a final follow-up at 10 weeks. Analyses included effect sizes, and growth mixture models to identify participant groups and their characteristic engagement, severity, and demographic factors. Users demonstrated significant reductions in PHQ-9 and GAD-7 that were sustained at follow-up. Significant improvements in Hope, Behavioral Activation, Social Interaction, Loneliness, and Perceived Social Support were observed throughout and maintained at 10 week follow-up. Engagement was high and predicted outcomes. Working alliance was comparable to traditional care and predicted outcomes. Automated safety guardrails functioned as designed, with 76 sessions flagged for risk and all handled according to escalation policies. This single arm naturalistic observational study provides initial evidence that a GAI foundation model for mental health can deliver accessible, engaging, effective, and safe mental health support. These results lend support to findings from early randomized designs and offer promise for future study of mental health GAI in real world settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e13\u4e3a\u5fc3\u7406\u5065\u5eb7\u8bbe\u8ba1\u7684\u751f\u6210\u5f0fAI\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc710\u5468\u89c2\u5bdf\u53d1\u73b0\u8be5\u6a21\u578b\u80fd\u6709\u6548\u964d\u4f4e\u6291\u90c1\u548c\u7126\u8651\u75c7\u72b6\uff0c\u6539\u5584\u5e0c\u671b\u611f\u3001\u884c\u4e3a\u6fc0\u6d3b\u3001\u793e\u4ea4\u4e92\u52a8\u7b49\u6307\u6807\uff0c\u4e14\u5177\u6709\u9ad8\u53c2\u4e0e\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u63d0\u4f9b\u5b89\u5168\u3001\u4e2a\u6027\u5316\u3001\u53ef\u6269\u5c55\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7684\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5fc3\u7406\u5065\u5eb7\u670d\u52a1\u53ef\u53ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5355\u81c2\u81ea\u7136\u89c2\u5bdf\u6027\u7814\u7a76\uff0c\u6210\u5e74\u7528\u6237\u57282025\u5e745\u670815\u65e5\u81f39\u670815\u65e5\u671f\u95f4\u4f7f\u7528\u5fc3\u7406\u5065\u5eb7\u804a\u5929\u673a\u5668\u4eba\uff0c\u6bcf\u4e24\u5468\u91cd\u590d\u6d4b\u91cf\u5fc3\u7406\u5065\u5eb7\u6307\u6807\u76f4\u81f36\u5468\uff0c\u5e76\u572810\u5468\u8fdb\u884c\u6700\u7ec8\u968f\u8bbf\u3002", "result": "\u7528\u6237PHQ-9\u548cGAD-7\u5f97\u5206\u663e\u8457\u964d\u4f4e\u4e14\u6548\u679c\u6301\u7eed\uff1b\u5e0c\u671b\u611f\u3001\u884c\u4e3a\u6fc0\u6d3b\u3001\u793e\u4ea4\u4e92\u52a8\u3001\u5b64\u72ec\u611f\u548c\u611f\u77e5\u793e\u4f1a\u652f\u6301\u663e\u8457\u6539\u5584\uff1b\u53c2\u4e0e\u5ea6\u9ad8\u4e14\u9884\u6d4b\u7ed3\u679c\uff1b\u5de5\u4f5c\u8054\u76df\u4e0e\u4f20\u7edf\u62a4\u7406\u76f8\u5f53\uff1b\u5b89\u5168\u62a4\u680f\u6709\u6548\u8fd0\u4f5c\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5fc3\u7406\u5065\u5eb7\u57fa\u7840\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u53ef\u8bbf\u95ee\u3001\u6709\u5438\u5f15\u529b\u3001\u6709\u6548\u4e14\u5b89\u5168\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\uff0c\u4e3a\u672a\u6765\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7814\u7a76\u5fc3\u7406\u5065\u5eb7AI\u63d0\u4f9b\u4e86\u5e0c\u671b\u3002"}}
{"id": "2511.11702", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11702", "abs": "https://arxiv.org/abs/2511.11702", "authors": ["Lian He", "Meng Liu", "Qilang Ye", "Yu Zhou", "Xiang Deng", "Gangyi Ding"], "title": "Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement", "comment": null, "summary": "Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.", "AI": {"tldr": "TASA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u51e0\u4f55\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e3D\u573a\u666f\u7ea7\u53ef\u64cd\u4f5c\u6027\u5206\u5272\uff0c\u901a\u8fc7\u8054\u5408\u5229\u75282D\u8bed\u4e49\u7ebf\u7d22\u548c3D\u51e0\u4f55\u63a8\u7406\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bf9\u8c61\u7ea7\u53ef\u64cd\u4f5c\u6027\u6216\u5c062D\u9884\u6d4b\u63d0\u5347\u52303D\uff0c\u5ffd\u7565\u4e86\u70b9\u4e91\u4e2d\u7684\u4e30\u5bcc\u51e0\u4f55\u7ed3\u6784\u4fe1\u606f\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u89e3\u51b3\u8bed\u4e49\u63a8\u7406\u548c\u7a7a\u95f4\u5b9a\u4f4d\u7684\u6311\u6218\u3002", "method": "TASA\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u65b9\u6cd5\uff0c\u5305\u542b\u4efb\u52a1\u611f\u77e5\u76842D\u53ef\u64cd\u4f5c\u6027\u68c0\u6d4b\u6a21\u5757\u6765\u8bc6\u522b\u53ef\u64cd\u4f5c\u70b9\uff0c\u4ee5\u53ca3D\u53ef\u64cd\u4f5c\u6027\u7ec6\u5316\u6a21\u5757\u6765\u6574\u54082D\u8bed\u4e49\u5148\u9a8c\u4e0e\u5c40\u90e83D\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728SceneFun3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTASA\u5728\u573a\u666f\u7ea7\u53ef\u64cd\u4f5c\u6027\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TASA\u901a\u8fc7\u8054\u5408\u5229\u75282D\u8bed\u4e49\u7ebf\u7d22\u548c3D\u51e0\u4f55\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u573a\u666f\u7ea7\u53ef\u64cd\u4f5c\u6027\u5206\u5272\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u7a7a\u95f4\u4e00\u81f4\u76843D\u53ef\u64cd\u4f5c\u6027\u63a9\u7801\u3002"}}
{"id": "2511.11589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11589", "abs": "https://arxiv.org/abs/2511.11589", "authors": ["Chenyue Liu", "Ali Mostafavi"], "title": "WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation", "comment": null, "summary": "Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.", "AI": {"tldr": "WildfireGenome\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u591a\u4e2a\u8054\u90a6\u6307\u6807\u3001\u968f\u673a\u68ee\u6797\u5206\u7c7b\u548cSHAP/ICE\u5206\u6790\uff0c\u5728H3 Level-8\u5206\u8fa8\u7387\u4e0b\u63d0\u4f9b\u51b3\u7b56\u5c3a\u5ea6\u7684\u98ce\u9669\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u4f9d\u8d56\u7c97\u7cd9\u98ce\u9669\u5730\u56fe\u548c\u4e0d\u900f\u660e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u4f18\u5316\u533a\u57df\u51c6\u786e\u6027\u7684\u540c\u65f6\u727a\u7272\u4e86\u51b3\u7b56\u5c3a\u5ea6\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1) \u878d\u5408\u4e03\u4e2a\u8054\u90a6\u91ce\u706b\u6307\u6807\u5230H3 Level-8\u5206\u8fa8\u7387\u7684PCA\u590d\u5408\u98ce\u9669\u6807\u7b7e\uff1b(2) \u968f\u673a\u68ee\u6797\u5206\u7c7b\u672c\u5730\u91ce\u706b\u98ce\u9669\uff1b(3) SHAP\u548cICE/PDP\u5206\u6790\u63ed\u793a\u53bf\u57df\u7279\u5b9a\u7684\u975e\u7ebf\u6027\u9a71\u52a8\u5173\u7cfb\u3002", "result": "\u5728\u4e03\u4e2a\u751f\u6001\u591a\u6837\u5316\u7684\u7f8e\u56fd\u53bf\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u52300.755-0.878\uff0c\u4e8c\u6b21\u52a0\u6743Kappa\u9ad8\u8fbe0.951\uff0c\u4e3b\u6210\u5206\u89e3\u91ca87-94%\u7684\u6307\u6807\u65b9\u5dee\u3002\u9488\u53f6\u6797\u8986\u76d6\u7387\u548c\u6d77\u62d4\u88ab\u8bc6\u522b\u4e3a\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u98ce\u9669\u572830-40%\u9488\u53f6\u6797\u8986\u76d6\u7387\u65f6\u6025\u5267\u4e0a\u5347\u3002", "conclusion": "WildfireGenome\u5c06\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u4ece\u533a\u57df\u9884\u6d4b\u63a8\u8fdb\u5230\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u5c3a\u5ea6\u5206\u6790\uff0c\u4e3a\u690d\u88ab\u7ba1\u7406\u3001\u5206\u533a\u548c\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.11713", "categories": ["cs.CY", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11713", "abs": "https://arxiv.org/abs/2511.11713", "authors": ["Yunkai Yu", "Yingying Wang", "Rong Zheng"], "title": "Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets", "comment": "8 pages,4 figures, to be published in IEEE AIOT 2025", "summary": "The Internet of Things (IoT) sensors have been widely employed to capture human locomotions to enable applications such as activity recognition, human pose estimation, and fall detection. Motion capture (MoCap) systems are frequently used to generate ground truth annotations for human poses when training models with data from wearable or ambient sensors, and have been shown to be effective to synthesize data in these modalities. However, the representation of older adults, an increasingly important demographic in healthcare, in existing MoCap locomotion datasets has not been thoroughly examined. This work surveyed 41 publicly available datasets, identifying eight that include older adult motions and four that contain motions performed by younger actors annotated as old style. Older adults represent a small portion of participants overall, and few datasets provide full-body motion data for this group. To assess the fidelity of old-style walking motions, quantitative metrics are introduced, defining high fidelity as the ability to capture age-related differences relative to normative walking. Using gait parameters that are age-sensitive, robust to noise, and resilient to data scarcity, we found that old-style walking motions often exhibit overly controlled patterns and fail to faithfully characterize aging. These findings highlight the need for improved representation of older adults in motion datasets and establish a method to quantitatively evaluate the quality of old-style walking motions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u4e8641\u4e2a\u516c\u5f00\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u8001\u5e74\u4eba\u53c2\u4e0e\u5ea6\u4f4e\uff0c\u4e14\u6a21\u62df\u7684\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u5e74\u9f84\u76f8\u5173\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u5b9a\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\u4e2d\u8001\u5e74\u4eba\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u4e14\u6a21\u62df\u7684\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u7684\u771f\u5b9e\u6027\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\uff0c\u8fd9\u5728\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e2d\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u8c03\u67e541\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u5305\u542b\u8001\u5e74\u4eba\u52a8\u4f5c\u7684\u6570\u636e\u96c6\uff1b\u5f15\u5165\u5b9a\u91cf\u6307\u6807\u8bc4\u4f30\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u7684\u4fdd\u771f\u5ea6\uff0c\u4f7f\u7528\u5bf9\u5e74\u9f84\u654f\u611f\u3001\u6297\u566a\u58f0\u4e14\u9002\u5e94\u6570\u636e\u7a00\u7f3a\u7684\u6b65\u6001\u53c2\u6570\u3002", "result": "\u8001\u5e74\u4eba\u4ec5\u5360\u603b\u4f53\u53c2\u4e0e\u8005\u7684\u5c0f\u90e8\u5206\uff0c\u63d0\u4f9b\u5168\u8eab\u8fd0\u52a8\u6570\u636e\u7684\u6570\u636e\u96c6\u66f4\u5c11\uff1b\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u5f80\u5f80\u8868\u73b0\u51fa\u8fc7\u5ea6\u63a7\u5236\u7684\u6a21\u5f0f\uff0c\u65e0\u6cd5\u771f\u5b9e\u8868\u5f81\u8870\u8001\u7279\u5f81\u3002", "conclusion": "\u9700\u8981\u5728\u8fd0\u52a8\u6570\u636e\u96c6\u4e2d\u6539\u5584\u8001\u5e74\u4eba\u7684\u4ee3\u8868\u6027\uff0c\u5e76\u5efa\u7acb\u5b9a\u91cf\u8bc4\u4f30\u8001\u5e74\u98ce\u683c\u884c\u8d70\u52a8\u4f5c\u8d28\u91cf\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.11592", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11592", "abs": "https://arxiv.org/abs/2511.11592", "authors": ["Guojian Zhan", "Likun Wang", "Pengcheng Wang", "Feihong Zhang", "Jingliang Duan", "Masayoshi Tomizuka", "Shengbo Eben Li"], "title": "Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL", "comment": "17 pages", "summary": "Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8f68\u8ff9\u71b5\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60(TECRL)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5956\u52b1\u548c\u71b5\u7684Q\u51fd\u6570\u5b66\u4e60\uff0c\u89e3\u51b3\u6700\u5927\u71b5RL\u4e2d\u7684\u975e\u5e73\u7a33Q\u503c\u4f30\u8ba1\u548c\u77ed\u89c6\u5c40\u90e8\u71b5\u8c03\u8282\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86DSAC-E\u7b97\u6cd5\u3002", "motivation": "\u6700\u5927\u71b5RL\u6846\u67b6\u5b58\u5728\u4e24\u4e2a\u74f6\u9888\uff1a1) \u6e29\u5ea6\u53c2\u6570\u66f4\u65b0\u548c\u71b5\u6ce8\u5165\u5bfc\u81f4\u7684\u975e\u5e73\u7a33Q\u503c\u4f30\u8ba1\uff1b2) \u4ec5\u57fa\u4e8e\u5f53\u524d\u5355\u6b65\u71b5\u7684\u77ed\u89c6\u5c40\u90e8\u71b5\u8c03\u8282\uff0c\u672a\u8003\u8651\u7d2f\u79ef\u71b5\u7684\u65f6\u95f4\u6548\u5e94\u3002", "method": "\u63d0\u51faTECRL\u6846\u67b6\uff1a\u5206\u522b\u5b66\u4e60\u5956\u52b1Q\u51fd\u6570\u548c\u71b5Q\u51fd\u6570\uff0c\u786e\u4fdd\u503c\u76ee\u6807\u4e0d\u53d7\u6e29\u5ea6\u66f4\u65b0\u5f71\u54cd\uff1b\u901a\u8fc7\u4e13\u95e8\u7684\u71b5Q\u51fd\u6570\u91cf\u5316\u671f\u671b\u7d2f\u79ef\u71b5\uff0c\u5b9e\u65bd\u8f68\u8ff9\u71b5\u7ea6\u675f\u4ee5\u63a7\u5236\u7b56\u7565\u957f\u671f\u968f\u673a\u6027\u3002\u57fa\u4e8e\u6b64\u5f00\u53d1DSAC-E\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86\u5206\u5e03\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u3002", "result": "\u5728OpenAI Gym\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDSAC-E\u80fd\u591f\u83b7\u5f97\u66f4\u9ad8\u7684\u56de\u62a5\u548c\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "TECRL\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u5956\u52b1\u548c\u71b5\u7684Q\u51fd\u6570\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u6700\u5927\u71b5RL\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0cDSAC-E\u7b97\u6cd5\u5728\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.12518", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12518", "abs": "https://arxiv.org/abs/2511.12518", "authors": ["Zhongchao Yi", "Kai Feng", "Xiaojian Ma", "Yalong Wang", "Yongqi Liu", "Han Li", "Zhengyang Zhou", "Yang Wang"], "title": "DualGR: Generative Retrieval with Long and Short-Term Interests Modeling", "comment": null, "summary": "In large-scale industrial recommendation systems, retrieval must produce high-quality candidates from massive corpora under strict latency. Recently, Generative Retrieval (GR) has emerged as a viable alternative to Embedding-Based Retrieval (EBR), which quantizes items into a finite token space and decodes candidates autoregressively, providing a scalable path that explicitly models target-history interactions via cross-attention. However, three challenges persist: 1) how to balance users' long-term and short-term interests , 2) noise interference when generating hierarchical semantic IDs (SIDs), 3) the absence of explicit modeling for negative feedback such as exposed items without clicks. To address these challenges, we propose DualGR, a generative retrieval framework that explicitly models dual horizons of user interests with selective activation. Specifically, DualGR utilizes Dual-Branch Long/Short-Term Router (DBR) to cover both stable preferences and transient intents by explicitly modeling users' long- and short-term behaviors. Meanwhile, Search-based SID Decoding (S2D) is presented to control context-induced noise and enhance computational efficiency by constraining candidate interactions to the current coarse (level-1) bucket during fine-grained (level-2/3) SID prediction. % also reinforcing intra-class consistency. Finally, we propose an Exposure-aware Next-Token Prediction Loss (ENTP-Loss) that treats \"exposed-but-unclicked\" items as hard negatives at level-1, enabling timely interest fade-out. On the large-scale Kuaishou short-video recommendation system, DualGR has achieved outstanding performance. Online A/B testing shows +0.527% video views and +0.432% watch time lifts, validating DualGR as a practical and effective paradigm for industrial generative retrieval.", "AI": {"tldr": "DualGR\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u957f\u77ed\u65f6\u8def\u7531\u5668\u5e73\u8861\u7528\u6237\u957f\u77ed\u671f\u5174\u8da3\uff0c\u4f7f\u7528\u57fa\u4e8e\u641c\u7d22\u7684\u8bed\u4e49ID\u89e3\u7801\u63a7\u5236\u566a\u58f0\u5e72\u6270\uff0c\u5e76\u5f15\u5165\u66dd\u5149\u611f\u77e5\u7684\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u635f\u5931\u6765\u5efa\u6a21\u8d1f\u53cd\u9988\uff0c\u5728\u5feb\u624b\u77ed\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0f\u68c0\u7d22\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1\uff09\u5e73\u8861\u7528\u6237\u957f\u77ed\u671f\u5174\u8da3 2\uff09\u751f\u6210\u5206\u5c42\u8bed\u4e49ID\u65f6\u7684\u566a\u58f0\u5e72\u6270 3\uff09\u7f3a\u4e4f\u5bf9\u66dd\u5149\u672a\u70b9\u51fb\u7b49\u8d1f\u53cd\u9988\u7684\u663e\u5f0f\u5efa\u6a21", "method": "\u63d0\u51faDualGR\u6846\u67b6\uff1a1\uff09\u53cc\u5206\u652f\u957f\u77ed\u65f6\u8def\u7531\u5668\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u957f\u77ed\u671f\u884c\u4e3a 2\uff09\u57fa\u4e8e\u641c\u7d22\u7684\u8bed\u4e49ID\u89e3\u7801\u9650\u5236\u5019\u9009\u4ea4\u4e92\u8303\u56f4\u4ee5\u63a7\u5236\u566a\u58f0 3\uff09\u66dd\u5149\u611f\u77e5\u7684\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u635f\u5931\u5c06\u66dd\u5149\u672a\u70b9\u51fb\u9879\u89c6\u4e3a\u786c\u8d1f\u6837\u672c", "result": "\u5728\u5feb\u624b\u5927\u89c4\u6a21\u77ed\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u89c6\u9891\u89c2\u770b\u91cf\u63d0\u53470.527%\uff0c\u89c2\u770b\u65f6\u957f\u63d0\u53470.432%", "conclusion": "DualGR\u4e3a\u5de5\u4e1a\u7ea7\u751f\u6210\u5f0f\u68c0\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6709\u6548\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd"}}
{"id": "2511.11770", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11770", "abs": "https://arxiv.org/abs/2511.11770", "authors": ["Floris Vossebeld", "Shenghui Wang"], "title": "Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction", "comment": null, "summary": "Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u8fed\u4ee3\u6784\u5efaSPARQL\u67e5\u8be2\uff0c\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u591a\u8df3\u95ee\u9898\u7684\u590d\u6742\u67e5\u8be2\u751f\u6210\u96be\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u57fa\u4e8e\u5b9e\u65f6\u6267\u884c\u53cd\u9988\u52a8\u6001\u8c03\u8bd5\u67e5\u8be2\u7684\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e00\u6b21\u6027\u751f\u6210\u65b9\u5f0f\u5728\u4e0e\u7ed3\u6784\u5316\u6570\u636e\u4ea4\u4e92\u65f6\u5b58\u5728\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ed3\u679c\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\u8bad\u7ec33B\u53c2\u6570\u6a21\u578b\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff0c\u5b66\u4e60\u8fed\u4ee3SPARQL\u6784\u5efa\u7684\u5f39\u6027\u7b56\u7565\uff0c\u901a\u8fc7\u6267\u884c\u9519\u8bef\u6062\u590d\u548c\u67e5\u8be2\u7cbe\u70bc\u5b9e\u73b0\u7cfb\u7edf\u6539\u8fdb\u3002", "result": "\u5728LC-QuAD 2.0\u7684\u53ef\u6267\u884c\u5b50\u96c6\u4e0a\uff0c\u5b9e\u4f53\u94fe\u63a5\u540e\u51c6\u786e\u7387\u8fbe\u523049.7%\uff0c\u6bd4\u6700\u5f3a\u7684\u8fed\u4ee3\u96f6\u6837\u672c\u57fa\u7ebf\u63d0\u9ad8\u4e8617.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u901a\u8fc7\u4ea4\u4e92\u6559\u6388\u667a\u80fd\u4f53\u638c\u63e1\u5f62\u5f0f\u5316\u7b26\u53f7\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u84dd\u56fe\uff0c\u5f25\u5408\u4e86\u6982\u7387\u6027LLM\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u4e16\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.11715", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11715", "abs": "https://arxiv.org/abs/2511.11715", "authors": ["Yunfei Shen", "Zhongcheng Wu"], "title": "CADD: A Chinese Traffic Accident Dataset for Statute-Based Liability Attribution", "comment": null, "summary": "As autonomous driving technology advances, the critical challenge evolves beyond collision avoidance to the \\textbf{adjudication of liability} when accidents occur. Existing datasets, focused on detection and localization, lack the annotations required for this legal reasoning. To bridge this gap, we introduce the \\textbf{C}hinese \\textbf{A}ccident \\textbf{D}uty-determination \\textbf{D}ataset (\\textbf{CADD}), the first benchmark for statute-based liability attribution. CADD contains 792 real-world driving recorder videos, each annotated within a novel \\textbf{``Behavior--Liability--Statute''} pipeline. This framework provides \\textbf{granular, symmetric behavior annotations}, clear responsibility assignments, and, uniquely, links each case to the specific \\textbf{Chinese traffic law statute} violated. We demonstrate the utility of CADD through detailed analysis and establish benchmarks for liability prediction and explainable decision-making. By directly connecting perceptual data to legal consequences, CADD provides a foundational resource for developing accountable and legally-grounded autonomous systems.", "AI": {"tldr": "CADD\u662f\u4e2d\u56fd\u9996\u4e2a\u57fa\u4e8e\u6cd5\u89c4\u7684\u8d23\u4efb\u8ba4\u5b9a\u6570\u636e\u96c6\uff0c\u5305\u542b792\u4e2a\u771f\u5b9e\u9a7e\u9a76\u8bb0\u5f55\u4eea\u89c6\u9891\uff0c\u91c7\u7528\"\u884c\u4e3a-\u8d23\u4efb-\u6cd5\u89c4\"\u6807\u6ce8\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u6570\u636e\u4e0e\u6cd5\u5f8b\u540e\u679c\u76f4\u63a5\u5173\u8054\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u53d1\u5c55\uff0c\u4e8b\u6545\u8d23\u4efb\u5224\u5b9a\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u6cd5\u5f8b\u63a8\u7406\u6240\u9700\u7684\u6807\u6ce8\u4fe1\u606f\u3002", "method": "\u6784\u5efa\u5305\u542b792\u4e2a\u771f\u5b9e\u9a7e\u9a76\u8bb0\u5f55\u4eea\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u91c7\u7528\"\u884c\u4e3a-\u8d23\u4efb-\u6cd5\u89c4\"\u6807\u6ce8\u6846\u67b6\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u884c\u4e3a\u6807\u6ce8\u3001\u660e\u786e\u8d23\u4efb\u5206\u914d\u548c\u5177\u4f53\u4ea4\u901a\u6cd5\u89c4\u94fe\u63a5\u3002", "result": "\u5efa\u7acb\u4e86\u8d23\u4efb\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u7684\u57fa\u51c6\uff0c\u4e3a\u5f00\u53d1\u8d1f\u8d23\u4efb\u4e14\u6cd5\u5f8b\u4f9d\u636e\u5145\u5206\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u8d44\u6e90\u3002", "conclusion": "CADD\u901a\u8fc7\u76f4\u63a5\u8fde\u63a5\u611f\u77e5\u6570\u636e\u4e0e\u6cd5\u5f8b\u540e\u679c\uff0c\u4e3a\u5f00\u53d1\u53ef\u95ee\u8d23\u4e14\u6cd5\u5f8b\u4f9d\u636e\u5145\u5206\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2511.11710", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11710", "abs": "https://arxiv.org/abs/2511.11710", "authors": ["Zhou Xu", "Qi Wang", "Yuxiao Yang", "Luyuan Zhang", "Zhang Liang", "Yang Li"], "title": "Target-Balanced Score Distillation", "comment": null, "summary": "Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Target-Balanced Score Distillation (TBSD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u7b56\u7565\u89e3\u51b3\u4e86SDS\u65b9\u6cd5\u5728\u7eb9\u7406\u4f18\u5316\u548c\u5f62\u72b6\u4fdd\u6301\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfSDS\u65b9\u6cd5\u5b58\u5728\u8fc7\u9971\u548c\u548c\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u800c\u5f15\u5165\u8d1f\u63d0\u793a\u7684\u53d8\u4f53\u65b9\u6cd5\u9762\u4e34\u7eb9\u7406\u4f18\u5316\u6709\u9650\u6216\u7eb9\u7406\u589e\u76ca\u4f46\u5f62\u72b6\u5931\u771f\u7684\u5173\u952e\u6743\u8861\u3002", "method": "\u63d0\u51faTBSD\u65b9\u6cd5\uff0c\u5c06\u751f\u6210\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u7b56\u7565\u5e73\u8861\u76ee\u6807\u8d1f\u63d0\u793a\u7684\u4f7f\u7528\uff0c\u6709\u6548\u89e3\u51b3\u7eb9\u7406\u4e0e\u5f62\u72b6\u7684\u6743\u8861\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTBSD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u751f\u6210\u5177\u6709\u9ad8\u4fdd\u771f\u7eb9\u7406\u548c\u51e0\u4f55\u7cbe\u786e\u5f62\u72b6\u76843D\u8d44\u4ea7\u3002", "conclusion": "TBSD\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u548c\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86SDS\u65b9\u6cd5\u5728\u7eb9\u7406\u4f18\u5316\u548c\u5f62\u72b6\u4fdd\u6301\u4e4b\u95f4\u7684\u6839\u672c\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2511.12597", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12597", "abs": "https://arxiv.org/abs/2511.12597", "authors": ["Mengyao Gao", "Chongming Gao", "Haoyan Liu", "Qingpeng Cai", "Peng Jiang", "Jiajia Chen", "Shuai Yuan", "Xiangnan He"], "title": "MindRec: Mind-inspired Coarse-to-fine Decoding for Generative Recommendation", "comment": null, "summary": "Recent advancements in large language model-based recommendation systems often represent items as text or semantic IDs and generate recommendations in an auto-regressive manner. However, due to the left-to-right greedy decoding strategy and the unidirectional logical flow, such methods often fail to produce globally optimal recommendations. In contrast, human reasoning does not follow a rigid left-to-right sequence. Instead, it often begins with keywords or intuitive insights, which are then refined and expanded. Inspired by this fact, we propose Mind-inspired Recommender (MindRec), a novel generative framework that emulates human thought processes. Particularly, our method first generates key tokens that reflect user preferences, and then expands them into the complete item, enabling flexible and human-like generation. To further emulate the structured nature of human decision-making, we organize items into a hierarchical category tree. This structure guides the model to first produce the coarse-grained category and then progressively refine its selection through finer-grained subcategories before generating the specific item. To mitigate the local optimum problem inherent in greedy decoding, we design a novel beam search algorithm, Diffusion Beam Search, tailored for our mind-inspired generation paradigm. Experimental results demonstrate that MindRec yields a 9.5\\% average improvement in top-1 recommendation performance over state-of-the-art methods, highlighting its potential to enhance recommendation accuracy. The implementation is available via https://github.com/Mr-Peach0301/MindRec.", "AI": {"tldr": "MindRec\u662f\u4e00\u4e2a\u53d7\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\u542f\u53d1\u7684\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u9996\u5148\u751f\u6210\u53cd\u6620\u7528\u6237\u504f\u597d\u7684\u5173\u952e\u4ee4\u724c\uff0c\u7136\u540e\u6269\u5c55\u4e3a\u5b8c\u6574\u9879\u76ee\uff0c\u6a21\u62df\u4eba\u7c7b\u4ece\u5173\u952e\u8bcd\u5230\u5b8c\u6574\u63a8\u7406\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u91c7\u7528\u81ea\u56de\u5f52\u751f\u6210\u65b9\u5f0f\uff0c\u53d7\u9650\u4e8e\u4ece\u5de6\u5230\u53f3\u7684\u8d2a\u5a6a\u89e3\u7801\u7b56\u7565\u548c\u5355\u5411\u903b\u8f91\u6d41\uff0c\u65e0\u6cd5\u4ea7\u751f\u5168\u5c40\u6700\u4f18\u63a8\u8350\u3002\u800c\u4eba\u7c7b\u63a8\u7406\u5f80\u5f80\u4ece\u5173\u952e\u8bcd\u6216\u76f4\u89c9\u5f00\u59cb\uff0c\u7136\u540e\u9010\u6b65\u5b8c\u5584\u3002", "method": "1) \u9996\u5148\u751f\u6210\u53cd\u6620\u7528\u6237\u504f\u597d\u7684\u5173\u952e\u4ee4\u724c\uff1b2) \u5c06\u5173\u952e\u4ee4\u724c\u6269\u5c55\u4e3a\u5b8c\u6574\u9879\u76ee\uff1b3) \u4f7f\u7528\u5206\u5c42\u7c7b\u522b\u6811\u7ec4\u7ec7\u9879\u76ee\uff0c\u5f15\u5bfc\u6a21\u578b\u4ece\u7c97\u7c92\u5ea6\u7c7b\u522b\u9010\u6b65\u7ec6\u5316\u5230\u5177\u4f53\u9879\u76ee\uff1b4) \u8bbe\u8ba1\u6269\u6563\u675f\u641c\u7d22\u7b97\u6cd5\u7f13\u89e3\u8d2a\u5a6a\u89e3\u7801\u7684\u5c40\u90e8\u6700\u4f18\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMindRec\u5728top-1\u63a8\u8350\u6027\u80fd\u4e0a\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u63d0\u53479.5%\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u8350\u51c6\u786e\u6027\u3002", "conclusion": "MindRec\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u51b3\u7b56\u65b9\u5f0f\u7684\u63a8\u8350\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11773", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11773", "abs": "https://arxiv.org/abs/2511.11773", "authors": ["Ruchira Dhar", "Ninell Oldenburg", "Anders Soegaard"], "title": "On the Measure of a Model: From Intelligence to Generality", "comment": "Accepted at EurIPS Workshop on \"The Science of Benchmarking and Evaluating AI\"", "summary": "Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u5f53\u524d\u57fa\u4e8e\u62bd\u8c61\u667a\u529b\u6982\u5ff5\uff08\u5982ARC\u3001Raven\u6d4b\u8bd5\uff09\u7684AI\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8ba4\u4e3a\u8fd9\u4e9b\u57fa\u51c6\u4e0e\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u8868\u73b0\u8131\u8282\u3002\u4f5c\u8005\u63d0\u51fa\u5e94\u4ee5\u6cdb\u5316\u6027\uff08generality\uff09\u800c\u975e\u667a\u529b\u4f5c\u4e3a\u8bc4\u4f30\u57fa\u7840\uff0c\u5c06\u6cdb\u5316\u6027\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u76f4\u63a5\u5173\u8054\u5230\u53ef\u6d4b\u91cf\u7684\u6027\u80fd\u5e7f\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524dAI\u8bc4\u4f30\u8fc7\u5ea6\u4f9d\u8d56\u62bd\u8c61\u667a\u529b\u6982\u5ff5\uff0c\u7f3a\u4e4f\u7a33\u5b9a\u5b9a\u4e49\u4e14\u65e0\u6cd5\u9884\u6d4b\u5b9e\u9645\u4efb\u52a1\u8868\u73b0\u3002\u4f18\u5316\u8fd9\u4e9b\u57fa\u51c6\u53ef\u80fd\u5bfc\u81f4\u8bc4\u4f30\u4e0e\u73b0\u5b9e\u6548\u7528\u8131\u8282\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u6027\u548c\u5f62\u5f0f\u5316\u5206\u6790\uff0c\u68c0\u9a8c\u667a\u529b\u8bc4\u4f30\u7684\u4e09\u4e2a\u5047\u8bbe\uff08\u6cdb\u5316\u6027\u3001\u7a33\u5b9a\u6027\u3001\u73b0\u5b9e\u6027\uff09\uff0c\u8bc1\u660e\u53ea\u6709\u6cdb\u5316\u6027\u7ecf\u5f97\u8d77\u6982\u5ff5\u548c\u5b9e\u8bc1\u68c0\u9a8c\u3002\u5c06\u6cdb\u5316\u6027\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\u3002", "result": "\u5206\u6790\u8868\u660e\u667a\u529b\u6982\u5ff5\u5728\u8bc4\u4f30\u4e2d\u4e0d\u7a33\u5b9a\uff0c\u800c\u6cdb\u5316\u6027\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u57fa\u7840\u3002\u6cdb\u5316\u6027\u76f4\u63a5\u5173\u8054\u5230\u6027\u80fd\u5e7f\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9884\u6d4bAI\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u5e94\u57fa\u4e8e\u6cdb\u5316\u6027\u800c\u975e\u62bd\u8c61\u667a\u529b\u6765\u8bc4\u4f30AI\u8fdb\u5c55\uff0c\u8fd9\u4e3a\u8bc4\u4f30\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u7684\u57fa\u7840\uff0c\u4f7f\u8bc4\u4f30\u66f4\u8d34\u8fd1\u5b9e\u9645\u6548\u7528\u3002"}}
{"id": "2511.11718", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11718", "abs": "https://arxiv.org/abs/2511.11718", "authors": ["Sanjana Cheerla", "Vaibhav Garg", "Saikath Bhattacharya", "Munindar P. Singh"], "title": "Weapons of Online Harassment: Menacing and Profiling Users via Social Apps", "comment": "This article has been accepted for publication in IEEE Computer as a Research Feature. 13 pages, 3 figures, 1 table, 4 examples", "summary": "Viewing social apps as sociotechnical systems makes clear that they are not mere pieces of technology but mediate human interaction and may unintentionally enable harmful behaviors like online harassment. As more users interact through social apps, instances of harassment increase.\n  We observed that app reviews often describe harassment. Accordingly, we built a dataset of over 3 million reviews and 1,800 apps. We discovered that two forms of harassment are prevalent, Menacing and Profiling.\n  We built a computational model for identifying reviews indicating harassment, achieving high recalls of 90% for Menacing and 85% for Profiling. We analyzed the data further to better understand the terrain of harassment. Surprisingly, abusers most often have female identities. Also, what distinguishes negative from neutral reviews is the greater prevalence of anger, disgust, and fear.\n  Applying our model, we identified 1,395 apps enabling harassment and notified developers of the top 48 with the highest user-reported harassment.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86300\u4e07\u6761\u5e94\u7528\u8bc4\u8bba\uff0c\u53d1\u73b0\u793e\u4ea4\u5e94\u7528\u4e2d\u5b58\u5728\u4e24\u79cd\u4e3b\u8981\u9a9a\u6270\u5f62\u5f0f\uff1a\u5a01\u80c1\u548c\u753b\u50cf\u3002\u5f00\u53d1\u4e86\u8bc6\u522b\u9a9a\u6270\u8bc4\u8bba\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523090%\u548c85%\uff0c\u5e76\u8bc6\u522b\u51fa1395\u4e2a\u5b58\u5728\u9a9a\u6270\u7684\u5e94\u7528\u3002", "motivation": "\u793e\u4ea4\u5e94\u7528\u4f5c\u4e3a\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\uff0c\u4e0d\u4ec5\u63d0\u4f9b\u6280\u672f\u529f\u80fd\uff0c\u8fd8\u8c03\u8282\u4eba\u9645\u4e92\u52a8\uff0c\u53ef\u80fd\u65e0\u610f\u4e2d\u52a9\u957f\u5728\u7ebf\u9a9a\u6270\u884c\u4e3a\u3002\u968f\u7740\u7528\u6237\u6570\u91cf\u589e\u52a0\uff0c\u9a9a\u6270\u4e8b\u4ef6\u4e5f\u5728\u4e0a\u5347\u3002", "method": "\u6784\u5efa\u5305\u542b300\u4e07\u6761\u8bc4\u8bba\u548c1800\u4e2a\u5e94\u7528\u7684\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u8ba1\u7b97\u6a21\u578b\u8bc6\u522b\u9a9a\u6270\u8bc4\u8bba\uff0c\u5206\u6790\u9a9a\u6270\u884c\u4e3a\u7684\u7279\u5f81\u548c\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u5a01\u80c1\u548c\u753b\u50cf\u662f\u4e24\u79cd\u4e3b\u8981\u9a9a\u6270\u5f62\u5f0f\uff0c\u6a21\u578b\u53ec\u56de\u7387\u8fbe\u523090%\u548c85%\u3002\u8bc6\u522b\u51fa1395\u4e2a\u5b58\u5728\u9a9a\u6270\u7684\u5e94\u7528\uff0c\u9a9a\u6270\u8005\u591a\u4e3a\u5973\u6027\u8eab\u4efd\uff0c\u8d1f\u9762\u8bc4\u8bba\u4e2d\u6124\u6012\u3001\u538c\u6076\u548c\u6050\u60e7\u60c5\u7eea\u66f4\u666e\u904d\u3002", "conclusion": "\u793e\u4ea4\u5e94\u7528\u4e2d\u9a9a\u6270\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u9700\u8981\u5f00\u53d1\u8005\u91cd\u89c6\u5e76\u91c7\u53d6\u63aa\u65bd\u51cf\u5c11\u9a9a\u6270\u884c\u4e3a\uff0c\u8ba1\u7b97\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u8bc6\u522b\u548c\u76d1\u63a7\u6b64\u7c7b\u95ee\u9898\u3002"}}
{"id": "2511.12922", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.NE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12922", "abs": "https://arxiv.org/abs/2511.12922", "authors": ["Yu Hou", "Won-Yong Shin"], "title": "Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation", "comment": "20 pages, 8 figures, 9 tables; Annual AAAI Conference on Artificial Intelligence (AAAI-26) (to appear) (Please cite our conference version.)", "summary": "Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.", "AI": {"tldr": "UniTok\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7269\u54c1\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u4ee3\u7801\u672c\u5c06\u4e0d\u540c\u9886\u57df\u7684\u7269\u54c1\u8f6c\u6362\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u5b9e\u73b0\u8de8\u9886\u57df\u7684\u53ef\u6269\u5c55\u8bed\u4e49\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u7269\u54c1\u6807\u8bb0\u5316\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u7269\u54c1\u9886\u57df\u8bad\u7ec3\u5355\u72ec\u6a21\u578b\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u4e0d\u540c\u9886\u57df\u7684\u5206\u5e03\u548c\u8bed\u4e49\u591a\u6837\u6027\u4f7f\u5f97\u6784\u5efa\u7edf\u4e00\u6807\u8bb0\u5316\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51faUniTok\u6846\u67b6\uff0c\u4f7f\u7528\u5171\u4eab\u7f16\u7801\u5668\u5c06\u4e0d\u540c\u9886\u57df\u7269\u54c1\u6295\u5f71\u5230\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u8def\u7531\u5230\u9886\u57df\u7279\u5b9a\u4e13\u5bb6\u6355\u83b7\u72ec\u7279\u8bed\u4e49\uff0c\u540c\u65f6\u5171\u4eab\u4e13\u5bb6\u7f16\u7801\u8de8\u9886\u57df\u901a\u7528\u77e5\u8bc6\uff0c\u5e76\u5f15\u5165\u4e92\u4fe1\u606f\u6821\u51c6\u673a\u5236\u7f13\u89e3\u8bed\u4e49\u4e0d\u5e73\u8861\u3002", "result": "\u5728\u5e7f\u6cdb\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u6027\u80fd\u63d0\u5347\u8fbe51.89%\uff0c\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u67b6\u6784\u8bbe\u8ba1\u6709\u6548\u6027\uff0c\u4e14\u65e0\u9700\u9010\u9886\u57df\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5728\u591a\u6837\u5316\u9886\u57df\u5c55\u73b0\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "UniTok\u6846\u67b6\u5728\u4fdd\u6301\u8de8\u9886\u57df\u8bed\u4e49\u4fe1\u606f\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u6807\u8bb0\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u5ea6\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.11816", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11816", "abs": "https://arxiv.org/abs/2511.11816", "authors": ["Andrea Brunello", "Luca Geatti", "Michele Mignani", "Angelo Montanari", "Nicola Saccomanno"], "title": "Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy", "comment": "Full version of the paper accepted for publication at The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u6027\u5730\u8bc4\u4f30\u4e86\u73b0\u6709NL-FOL\u7ffb\u8bd1\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u533a\u5206\u771f\u6b63\u7684\u8bed\u4e49\u7ea7\u903b\u8f91\u7406\u89e3\u4e0e\u8868\u9762\u6a21\u5f0f\u8bc6\u522b\uff0c\u5e76\u53d1\u73b0\u5bf9\u8bdd\u5bfc\u5411\u7684LLMs\u5728NL-FOL\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5230\u4e00\u9636\u903b\u8f91\u7684\u7ffb\u8bd1\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\uff0c\u867d\u7136LLMs\u7684\u51fa\u73b0\u5e26\u6765\u4e86\u5e0c\u671b\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620LLMs\u7684\u5b9e\u9645\u80fd\u529b\u3002", "method": "\u6279\u5224\u6027\u5206\u6790\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u6765\u533a\u5206\u8bed\u4e49\u7ea7\u903b\u8f91\u7406\u89e3\u4e0e\u8868\u9762\u6a21\u5f0f\u8bc6\u522b\u3001\u8bb0\u5fc6\u548c\u6570\u636e\u96c6\u6c61\u67d3\u3002", "result": "\u4f7f\u7528\u65b0\u8bc4\u4f30\u65b9\u6cd5\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684\u5bf9\u8bdd\u5bfc\u5411LLMs\u5c55\u73b0\u51fa\u5f3a\u5927\u7684NL-FOL\u7ffb\u8bd1\u80fd\u529b\u548c\u771f\u6b63\u7684\u53e5\u5b50\u7ea7\u903b\u8f91\u7406\u89e3\u80fd\u529b\uff0c\u800c\u5d4c\u5165\u4e2d\u5fc3\u6a21\u578b\u8868\u73b0\u660e\u663e\u8f83\u5dee\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5bf9\u8bdd\u5bfc\u5411\u7684LLMs\u786e\u5b9e\u5177\u5907\u4f18\u79c0\u7684NL-FOL\u7ffb\u8bd1\u80fd\u529b\uff0c\u8fd9\u4e3a\u81ea\u7136\u8bed\u8a00\u903b\u8f91\u7406\u89e3\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2511.11723", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11723", "abs": "https://arxiv.org/abs/2511.11723", "authors": ["Mohammed Abboodi"], "title": "A framework for measuring and analyzing customer satisfaction at computer service companies using Lean Six Sigma", "comment": "Master's thesis", "summary": "The computer service industry has expanded rapidly over the past two decades, driven by the proliferation of computing technologies, the entry of large firms, and the availability of online diagnostic and troubleshooting tools. In this increasingly competitive environment, many small and medium sized enterprises struggle to maintain customer satisfaction as rivals deliver higher quality services at lower cost. This study addresses the absence of robust measurement systems for assessing service quality, a key factor underlying customer attrition, by proposing an integrated framework for evaluating satisfaction and identifying sources of dissatisfaction in computer services.\n  The framework combines core principles of Six Sigma with the SERVQUAL instrument within a structured DMAIC methodology (Define, Measure, Analyze, Improve, and Control). SERVQUAL provides the service quality dimensions and gap analysis techniques, while Six Sigma supplies the data driven approach to measurement and improvement. The literature suggests limited prior work integrating Lean Six Sigma with SERVQUAL, and this study contributes by operationalizing that integration in a real world setting.\n  A case study of a computer services company was conducted to demonstrate feasibility and effectiveness. Satisfaction levels were quantified, and root causes of dissatisfaction were identified. The analysis revealed a low overall satisfaction level and five primary drivers of unmet customer requirements. Addressing these causes is expected to increase customer satisfaction, lower customer acquisition costs, and improve overall organizational performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u516d\u897f\u683c\u739b\u548cSERVQUAL\u7684\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u673a\u670d\u52a1\u884c\u4e1a\u7684\u5ba2\u6237\u6ee1\u610f\u5ea6\u5e76\u8bc6\u522b\u4e0d\u6ee1\u610f\u7684\u6839\u6e90\u3002", "motivation": "\u8ba1\u7b97\u673a\u670d\u52a1\u884c\u4e1a\u7ade\u4e89\u6fc0\u70c8\uff0c\u4e2d\u5c0f\u4f01\u4e1a\u96be\u4ee5\u7ef4\u6301\u5ba2\u6237\u6ee1\u610f\u5ea6\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u670d\u52a1\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u662f\u5bfc\u81f4\u5ba2\u6237\u6d41\u5931\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u5c06\u516d\u897f\u683c\u739b\u7684\u6838\u5fc3\u539f\u5219\u4e0eSERVQUAL\u5de5\u5177\u7ed3\u5408\uff0c\u5728DMAIC\u65b9\u6cd5\u6846\u67b6\u5185\u5b9e\u65bd\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002", "result": "\u91cf\u5316\u4e86\u6ee1\u610f\u5ea6\u6c34\u5e73\uff0c\u8bc6\u522b\u51fa\u4e94\u4e2a\u4e3b\u8981\u7684\u4e0d\u6ee1\u610f\u9a71\u52a8\u56e0\u7d20\uff0c\u603b\u4f53\u6ee1\u610f\u5ea6\u6c34\u5e73\u8f83\u4f4e\u3002", "conclusion": "\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u6ee1\u610f\u6839\u6e90\u6709\u671b\u63d0\u9ad8\u5ba2\u6237\u6ee1\u610f\u5ea6\u3001\u964d\u4f4e\u83b7\u5ba2\u6210\u672c\u5e76\u6539\u5584\u7ec4\u7ec7\u6574\u4f53\u7ee9\u6548\u3002"}}
{"id": "2511.11602", "categories": ["cs.LG", "cs.GT", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.11602", "abs": "https://arxiv.org/abs/2511.11602", "authors": ["Georgios C. Chasparis"], "title": "Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games", "comment": null, "summary": "Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6536\u76ca\u7684\u5b66\u4e60\u65b9\u6848\u2014\u2014\u57fa\u4e8e\u671f\u671b\u7684\u6270\u52a8\u5b66\u4e60\u81ea\u52a8\u673a\uff08APLA\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u4f18\u5316\u4e2d\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u73a9\u5bb6\u5f31\u975e\u5faa\u73af\u6e38\u620f\u4e2d\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u7eaf\u7eb3\u4ec0\u5747\u8861\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u5f3a\u5316\u7684\u5b66\u4e60\u65b9\u6848\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u591a\u73a9\u5bb6\u5f31\u975e\u5faa\u73af\u6e38\u620f\u4e2d\uff0c\u5f53\u6bcf\u4e2a\u73a9\u5bb6\u5e94\u7528\u72ec\u7acb\u7684\u5b66\u4e60\u52a8\u6001\u65f6\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u7eaf\u7eb3\u4ec0\u5747\u8861\u3002\u5148\u524d\u7684\u7814\u7a76\u4ec5\u5173\u6ce8\u6f5c\u5728\u535a\u5f08\u548c\u534f\u8c03\u535a\u5f08\u7b49\u5c0f\u7c7b\u6e38\u620f\u3002", "method": "\u63d0\u51fa\u4e86APLA\u5b66\u4e60\u65b9\u6848\uff0c\u5176\u4e2d\u6bcf\u4e2a\u73a9\u5bb6\u7684\u884c\u52a8\u9009\u62e9\u6982\u7387\u5206\u5e03\u4e0d\u4ec5\u901a\u8fc7\u91cd\u590d\u9009\u62e9\u5f97\u5230\u5f3a\u5316\uff0c\u8fd8\u901a\u8fc7\u6355\u83b7\u73a9\u5bb6\u6ee1\u610f\u5ea6\u7684\u671f\u671b\u56e0\u5b50\u5f97\u5230\u5f3a\u5316\u3002\u5728\u5b58\u5728\u566a\u58f0\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u591a\u73a9\u5bb6\u6b63\u6548\u7528\u535a\u5f08\u4e2d\u7684APLA\u8fdb\u884c\u4e86\u968f\u673a\u7a33\u5b9a\u6027\u5206\u6790\u3002", "result": "\u5728\u901a\u7528\u975e\u96f6\u548c\u535a\u5f08\u4e2d\u5efa\u7acb\u4e86\u8bf1\u5bfc\u7684\u65e0\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0e\u6709\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u7b49\u4ef7\u6027\uff0c\u4ece\u800c\u8868\u5f81\u4e86\u968f\u673a\u7a33\u5b9a\u6027\u3002\u5728\u5f31\u975e\u5faa\u73af\u6e38\u620f\u4e2d\u8fdb\u4e00\u6b65\u4e13\u95e8\u5316\u4e86\u968f\u673a\u7a33\u5b9a\u6027\u5206\u6790\u3002", "conclusion": "APLA\u65b9\u6848\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u4f18\u5316\u4e2d\u7684\u6536\u655b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u73a9\u5bb6\u5f31\u975e\u5faa\u73af\u6e38\u620f\u4e2d\uff0c\u4e3a\u5206\u5e03\u5f0f\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11831", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11831", "abs": "https://arxiv.org/abs/2511.11831", "authors": ["Wenhao Zhou", "Hao Zheng", "Rong Zhao"], "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.", "AI": {"tldr": "TopoPerception\u662f\u4e00\u4e2a\u57fa\u4e8e\u62d3\u6251\u5c5e\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5168\u5c40\u611f\u77e5\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e0d\u5982\u968f\u673a\u731c\u6d4b\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u5c40\u90e8\u6377\u5f84\u95ee\u9898\uff0c\u53ef\u80fd\u9ad8\u4f30\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u3002\u4e3a\u4e86\u51c6\u786e\u8bc4\u4f30LVLMs\u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u53d7\u5c40\u90e8\u7279\u5f81\u5f71\u54cd\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u62d3\u6251\u5c5e\u6027\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\uff0c\u56e0\u4e3a\u62d3\u6251\u4f9d\u8d56\u4e8e\u56fe\u50cf\u7684\u5168\u5c40\u7ed3\u6784\u4e14\u5bf9\u5c40\u90e8\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u5168\u5c40\u611f\u77e5\u7684\u65e0\u6377\u5f84\u8bc4\u4f30\u3002", "result": "\u5728\u6700\u7c97\u7684\u611f\u77e5\u7c92\u5ea6\u4e0a\uff0c\u6240\u6709\u6a21\u578b\u7684\u8868\u73b0\u90fd\u4e0d\u4f18\u4e8e\u968f\u673a\u673a\u4f1a\uff0c\u8868\u660e\u5b83\u4eec\u4e25\u91cd\u7f3a\u4e4f\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u611f\u77e5\u80fd\u529b\u3002\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u53cd\u800c\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u4ec5\u6269\u5927\u6a21\u578b\u89c4\u6a21\u4e0d\u8db3\u4ee5\u89e3\u51b3\u5168\u5c40\u611f\u77e5\u7f3a\u9677\uff0c\u53ef\u80fd\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u6216\u67b6\u6784\u3002TopoPerception\u63ed\u793a\u4e86\u5f53\u524dLVLMs\u7684\u5173\u952e\u74f6\u9888\uff0c\u5e76\u4e3a\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.11738", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11738", "abs": "https://arxiv.org/abs/2511.11738", "authors": ["Matthias Huemmer", "Theophile Shyiramunda", "Franziska Durner", "Michelle J. Cummings-Koether"], "title": "On the Influence of Artificial Intelligence on Human Problem-Solving: Empirical Insights for the Third Wave in a Multinational Longitudinal Pilot Study", "comment": null, "summary": "This article presents the results and their discussion for the third wave (with n=23 participants) within a multinational longitudinal study that investigates the evolving paradigm of human-AI collaboration in problem-solving contexts. Building upon previous waves, our findings reveal the consolidation of a hybrid problem-solving culture characterized by strategic integration of AI tools within structured cognitive workflows. The data demonstrate near-universal AI adoption (95.7% with prior knowledge, 100% ChatGPT usage) primarily deployed through human-led sequences such as \"Think, Internet, ChatGPT, Further Processing\" (39.1%). However, this collaboration reveals a critical verification deficit that escalates with problem complexity. We empirically identify and quantify two systematic epistemic gaps: a belief-performance gap (up to +80.8 percentage points discrepancy between perceived and actual correctness) and a proof-belief gap (up to -16.8 percentage points between confidence and verification capability). These findings, derived from behavioral data and problem vignettes across complexity levels, indicate that the fundamental constraint on reliable AI-assisted work is solution validation rather than generation. The study concludes that educational and technological interventions must prioritize verification scaffolds (including assumption documentation protocols, adequacy criteria checklists, and triangulation procedures) to fortify the human role as critical validator in this new cognitive ecosystem.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4eba\u673a\u534f\u4f5c\u5728\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6f14\u5316\u8303\u5f0f\uff0c\u53d1\u73b0\u666e\u904dAI\u91c7\u7528\u4f46\u5b58\u5728\u9a8c\u8bc1\u7f3a\u9677\uff0c\u8bc6\u522b\u4e86\u4e24\u4e2a\u7cfb\u7edf\u6027\u8ba4\u77e5\u5dee\u8ddd\uff1a\u4fe1\u5ff5-\u8868\u73b0\u5dee\u8ddd\u548c\u8bc1\u660e-\u4fe1\u5ff5\u5dee\u8ddd\uff0c\u6307\u51fa\u53ef\u9760AI\u8f85\u52a9\u5de5\u4f5c\u7684\u6838\u5fc3\u9650\u5236\u662f\u89e3\u51b3\u65b9\u6848\u9a8c\u8bc1\u800c\u975e\u751f\u6210\u3002", "motivation": "\u7814\u7a76\u4eba\u673a\u534f\u4f5c\u5728\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6f14\u5316\u6a21\u5f0f\uff0c\u7279\u522b\u662fAI\u5de5\u5177\u5982\u4f55\u878d\u5165\u7ed3\u6784\u5316\u8ba4\u77e5\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u53ca\u8fd9\u79cd\u534f\u4f5c\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u8ba4\u77e5\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u591a\u56fd\u7eb5\u5411\u7814\u7a76\uff0c\u7b2c\u4e09\u6ce2\u5305\u542b23\u540d\u53c2\u4e0e\u8005\uff0c\u901a\u8fc7\u884c\u4e3a\u6570\u636e\u548c\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u95ee\u9898\u60c5\u5883\u6536\u96c6\u6570\u636e\uff0c\u5206\u6790AI\u91c7\u7528\u6a21\u5f0f\u548c\u8ba4\u77e5\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u53d1\u73b0\u8fd1\u4e4e\u666e\u904d\u7684AI\u91c7\u7528\u7387\uff0895.7%\u6709\u5148\u9a8c\u77e5\u8bc6\uff0c100%\u4f7f\u7528ChatGPT\uff09\uff0c\u4e3b\u8981\u91c7\u7528\"\u601d\u8003\u3001\u4e92\u8054\u7f51\u3001ChatGPT\u3001\u8fdb\u4e00\u6b65\u5904\u7406\"\u7b49\u4eba\u7c7b\u4e3b\u5bfc\u5e8f\u5217\uff0839.1%\uff09\uff0c\u4f46\u5b58\u5728\u968f\u7740\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\u800c\u5347\u7ea7\u7684\u9a8c\u8bc1\u7f3a\u9677\uff0c\u8bc6\u522b\u4e86\u4e24\u4e2a\u7cfb\u7edf\u6027\u8ba4\u77e5\u5dee\u8ddd\u3002", "conclusion": "\u6559\u80b2\u548c\u79d1\u6280\u5e72\u9884\u5fc5\u987b\u4f18\u5148\u8003\u8651\u9a8c\u8bc1\u652f\u67b6\uff08\u5305\u62ec\u5047\u8bbe\u6587\u6863\u534f\u8bae\u3001\u5145\u5206\u6027\u6807\u51c6\u6e05\u5355\u548c\u4e09\u89d2\u9a8c\u8bc1\u7a0b\u5e8f\uff09\uff0c\u4ee5\u52a0\u5f3a\u4eba\u7c7b\u5728\u8fd9\u4e2a\u65b0\u8ba4\u77e5\u751f\u6001\u7cfb\u7edf\u4e2d\u4f5c\u4e3a\u5173\u952e\u9a8c\u8bc1\u8005\u7684\u89d2\u8272\u3002"}}
{"id": "2511.11604", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11604", "abs": "https://arxiv.org/abs/2511.11604", "authors": ["Amaratou Mahamadou Saley", "Thierry Moyaux", "A\u00efcha Sekhari", "Vincent Cheutet", "Jean-Baptiste Danielou"], "title": "Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques", "comment": "19 pages, 9 figures, 6 journal, Journal Q1 (Computers and Industrial Engineering)", "summary": "The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u6280\u672f\u548c\u6838\u9886\u57df\u77e5\u8bc6\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u65b9\u6cd5\uff0c\u5728\u6838\u5de5\u4e1a\u4e2d\u663e\u8457\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5c06\u9884\u6d4b\u65f6\u95f4\u4ece3\u5c0f\u65f6\u5ef6\u957f\u523024\u5c0f\u65f6\uff0cF1\u5206\u6570\u4ece56.36%\u63d0\u5347\u523093.12%\u3002", "motivation": "\u7269\u8054\u7f51\u548c\u5de5\u4e1a4.0\u7684\u878d\u5408\u589e\u5f3a\u4e86\u6838\u5de5\u4e1a\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4f46\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u590d\u6742\u6838\u7cfb\u7edf\u4e2d\u9700\u8981\u5927\u91cf\u9886\u57df\u77e5\u8bc6\uff0c\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u7ef4\u62a4\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u65b9\u6cd5\uff0c\u5c06\u6570\u636e\u9a71\u52a8\u6280\u672f\u4e0e\u6838\u8bbe\u5907\u9886\u57df\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u5f3a\u8c03\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u5c55\u793a\u77e5\u8bc6\u5728\u63d0\u5347\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u4e2d\u7684\u91cd\u8981\u6027\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83\uff0c\u6df7\u5408\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff1a\u9884\u6d4b\u65f6\u95f4\u4ece3\u5c0f\u65f6\u5ef6\u957f\u523024\u5c0f\u65f6\uff0cF1\u5206\u6570\u4ece56.36%\u63d0\u5347\u523093.12%\u3002", "conclusion": "\u5728\u6838\u5de5\u4e1a\u7b49\u9ad8\u5ea6\u53d7\u9650\u548c\u654f\u611f\u7684\u9886\u57df\u4e2d\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u7684\u6df7\u5408\u9884\u6d4b\u6027\u7ef4\u62a4\u65b9\u6cd5\u6bd4\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6545\u969c\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.12949", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12949", "abs": "https://arxiv.org/abs/2511.12949", "authors": ["Bokang Fu", "Jiahao Wang", "Xiaojing Liu", "Yuli Liu"], "title": "Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior", "comment": null, "summary": "In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.\n  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86CFQP\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e2a\u6027\u5316\u8bb0\u5fc6\u6a21\u5757\u548c\u56fe\u57fa\u504f\u597d\u4f20\u64ad\u6765\u52a8\u6001\u5efa\u6a21\u7528\u6237-\u95ee\u9898\u4ea4\u4e92\uff0c\u89e3\u51b3LLMs\u5728\u6355\u6349\u7528\u6237\u52a8\u6001\u884c\u4e3a\u5e8f\u5217\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709LLM\u7cfb\u7edf\u901a\u5e38\u9759\u6001\u5efa\u6a21\u7528\u6237\u504f\u597d\uff0c\u65e0\u6cd5\u6355\u6349\u4ea4\u4e92\u884c\u4e3a\u7684\u52a8\u6001\u6027\u548c\u5e8f\u5217\u6027\uff0c\u800c\u7528\u6237\u5386\u53f2\u95ee\u9898\u5e8f\u5217\u8574\u542b\u4e30\u5bcc\u7684\u5174\u8da3\u6f14\u53d8\u548c\u8ba4\u77e5\u6a21\u5f0f\u4fe1\u53f7\u3002", "method": "CFQP\u6846\u67b6\u6574\u5408\u4e2a\u6027\u5316\u8bb0\u5fc6\u6a21\u5757\u548c\u56fe\u57fa\u504f\u597d\u4f20\u64ad\u7684\u53cc\u91cd\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u7528\u6237\u7279\u5b9a\u5386\u53f2\uff0c\u5e76\u901a\u8fc7\u76f8\u4f3c\u7528\u6237\u7684\u534f\u540c\u4fe1\u53f7\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u6a21\u62df\u771f\u5b9e\u7528\u6237\u63d0\u95ee\u6a21\u5f0f\u7684\u667a\u80fd\u4f53\uff0c\u5c55\u793a\u4e86\u6784\u5efa\u4e3b\u52a8\u81ea\u9002\u5e94\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6f5c\u529b\u3002", "conclusion": "CFQP\u6846\u67b6\u6210\u529f\u5f25\u8865\u4e86\u8bed\u8a00\u5efa\u6a21\u4e0e\u884c\u4e3a\u5e8f\u5217\u5efa\u6a21\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u52a8\u6001\u7528\u6237\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11899", "abs": "https://arxiv.org/abs/2511.11899", "authors": ["Xi Li", "Nicholas Matsumoto", "Ujjwal Pasupulety", "Atharva Deo", "Cherine Yang", "Jay Moran", "Miguel E. Hernandez", "Peter Wager", "Jasmine Lin", "Jeanine Kim", "Alvin C. Goh", "Christian Wagner", "Geoffrey A. Sonn", "Andrew J. Hung"], "title": "End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction", "comment": null, "summary": "Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.", "AI": {"tldr": "F2O\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u53ef\u5c06\u7ec4\u7ec7\u89e3\u5256\u89c6\u9891\u8f6c\u5316\u4e3a\u624b\u52bf\u5e8f\u5217\uff0c\u5e76\u53d1\u73b0\u4e0e\u672f\u540e\u7ed3\u679c\u76f8\u5173\u7684\u6a21\u5f0f\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u57fa\u4e8etransformer\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5efa\u6a21\uff0c\u5728\u673a\u5668\u4eba\u8f85\u52a9\u6839\u6cbb\u6027\u524d\u5217\u817a\u5207\u9664\u672f\u4e2d\u68c0\u6d4b\u8fde\u7eed\u77ed\u624b\u52bf\uff0c\u5e76\u80fd\u9884\u6d4b\u672f\u540e\u7ed3\u679c\uff0c\u51c6\u786e\u6027\u4e0e\u4eba\u5de5\u6807\u6ce8\u76f8\u5f53\u3002", "motivation": "\u7cbe\u7ec6\u5206\u6790\u672f\u4e2d\u884c\u4e3a\u53ca\u5176\u5bf9\u60a3\u8005\u7ed3\u679c\u7684\u5f71\u54cd\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u5c06\u624b\u672f\u89c6\u9891\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u8bc4\u4f30\u7684\u7cfb\u7edf\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u624b\u672f\u53cd\u9988\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u91c7\u7528\u57fa\u4e8etransformer\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5efa\u6a21\u4ee5\u53ca\u9010\u5e27\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728\u673a\u5668\u4eba\u8f85\u52a9\u6839\u6cbb\u6027\u524d\u5217\u817a\u5207\u9664\u672f\u7684\u795e\u7ecf\u4fdd\u7559\u6b65\u9aa4\u4e2d\u68c0\u6d4b\u8fde\u7eed\u77ed\u624b\u52bf\uff08\u7ea62\u79d2\uff09\u3002\u4ece\u624b\u52bf\u5e8f\u5217\u4e2d\u63d0\u53d6\u7279\u5f81\uff08\u624b\u52bf\u9891\u7387\u3001\u6301\u7eed\u65f6\u95f4\u548c\u8f6c\u6362\uff09\u3002", "result": "F2O\u5728\u5e27\u7ea7\u548c\u89c6\u9891\u7ea7\u7684\u624b\u52bf\u68c0\u6d4bAUC\u5206\u522b\u8fbe\u52300.80\u548c0.81\u3002\u4f7f\u7528F2O\u63d0\u53d6\u7684\u7279\u5f81\u9884\u6d4b\u672f\u540e\u7ed3\u679c\u7684\u51c6\u786e\u6027\uff080.79\uff09\u4e0e\u4eba\u5de5\u6807\u6ce8\uff080.75\uff09\u76f8\u5f53\uff0c\u6548\u5e94\u5927\u5c0f\u65b9\u5411\u4e00\u81f4\uff0c\u76f8\u5173\u6027\u5f3a\uff08r=0.96\uff09\u3002\u7cfb\u7edf\u8fd8\u6355\u83b7\u4e86\u4e0e\u52c3\u8d77\u529f\u80fd\u6062\u590d\u76f8\u5173\u7684\u5173\u952e\u6a21\u5f0f\u3002", "conclusion": "F2O\u901a\u8fc7\u5b9e\u73b0\u81ea\u52a8\u53ef\u89e3\u91ca\u8bc4\u4f30\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u624b\u672f\u53cd\u9988\u548c\u524d\u77bb\u6027\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5efa\u7acb\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u53d1\u73b0\u4e0e\u60a3\u8005\u7ed3\u679c\u76f8\u5173\u7684\u672f\u4e2d\u884c\u4e3a\u6a21\u5f0f\u3002"}}
{"id": "2511.11741", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11741", "abs": "https://arxiv.org/abs/2511.11741", "authors": ["Christopher Mantzaris", "Ajda Fosner"], "title": "Taxation and the relationship between payments and time spent", "comment": "CM presented this research project at the 2025 Benedict College International Multidisciplinary Conference on 2025-03-12", "summary": "Tax work is costly for society: Administrative tax labour is typically to a high degree shuffled off the government and onto every taxpayer by law. The higher the burden of any tax system, the costlier for society, as taxpayers are unable to engage in proper wealth creation when being kept busy with administrative tax work. This research finds evidence for a relationship between hours spent to comply with taxes and amount of tax payment. These findings help better understand tax administrative costs and ultimately may help reduce them. PwC and World Bank's final \"Paying taxes\"-publication (2019) contains tax data for most of the world's jurisdictions, in particular annual hours spent to comply with tax obligations (X) and annual amount of tax payments (Y), both for the year 2019. X and Y were plotted in 6 tests. A positive slope, satisfying p and r values, high mutual information and finally a conclusive scatter plot picture were the 5 requirements that all needed to be met to confirm a positive relationship between X and Y. The first 2 tests did not make any adjustments to the data, the next 2 tests removed cities --thereby avoiding the double counting of jurisdictions-- and the final 2 tests removed cities and outliers. Each test pair uses for Y first total number of payments; and for each second test the number of other payments, which excludes income tax payments for profit and labour. All 5 requirements were met in every of the 6 tests, indicating a positive dependence. In addition, 4 confirmatory tests validate the methodology. The found relationship is noticeably stronger for the total number of tax payments. Findings indicate that taxpayers' time spent on tax, and thereby society's overall tax administrative costs, could be reduced by simplifying taxation processes, including tax collection and payments.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7eb3\u7a0e\u5408\u89c4\u65f6\u95f4\u4e0e\u7eb3\u7a0e\u91d1\u989d\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u901a\u8fc7\u5206\u6790\u5168\u7403\u7a0e\u6536\u6570\u636e\u8bc1\u5b9e\u4e86\u8fd9\u79cd\u5173\u7cfb\uff0c\u8868\u660e\u7b80\u5316\u7a0e\u6536\u6d41\u7a0b\u53ef\u4ee5\u964d\u4f4e\u793e\u4f1a\u7a0e\u6536\u7ba1\u7406\u6210\u672c\u3002", "motivation": "\u7a0e\u6536\u5de5\u4f5c\u5bf9\u793e\u4f1a\u6210\u672c\u9ad8\u6602\uff0c\u7eb3\u7a0e\u4eba\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u5904\u7406\u884c\u653f\u7a0e\u52a1\u5de5\u4f5c\u4f1a\u5f71\u54cd\u8d22\u5bcc\u521b\u9020\u3002\u7814\u7a76\u65e8\u5728\u7406\u89e3\u7a0e\u6536\u884c\u653f\u6210\u672c\u4e0e\u7eb3\u7a0e\u5408\u89c4\u65f6\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u5bfb\u6c42\u964d\u4f4e\u8fd9\u4e9b\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528PwC\u548c\u4e16\u754c\u94f6\u884c2019\u5e74\u300a\u7eb3\u7a0e\u300b\u62a5\u544a\u6570\u636e\uff0c\u5206\u6790\u5168\u7403\u53f8\u6cd5\u7ba1\u8f96\u533a\u7eb3\u7a0e\u5408\u89c4\u65f6\u95f4(X)\u4e0e\u7eb3\u7a0e\u91d1\u989d(Y)\u7684\u5173\u7cfb\u3002\u8fdb\u884c\u4e866\u7ec4\u6d4b\u8bd5\uff0c\u5305\u62ec\u539f\u59cb\u6570\u636e\u3001\u53bb\u9664\u57ce\u5e02\u6570\u636e\u3001\u53bb\u9664\u57ce\u5e02\u548c\u5f02\u5e38\u503c\u6570\u636e\uff0c\u6bcf\u7ec4\u6d4b\u8bd5\u5206\u522b\u4f7f\u7528\u603b\u7eb3\u7a0e\u91d1\u989d\u548c\u5176\u4ed6\u7eb3\u7a0e\u91d1\u989d\u3002", "result": "\u6240\u67096\u7ec4\u6d4b\u8bd5\u90fd\u6ee1\u8db35\u4e2a\u8981\u6c42\uff08\u6b63\u659c\u7387\u3001\u6ee1\u8db3p\u503c\u548cr\u503c\u3001\u9ad8\u4e92\u4fe1\u606f\u3001\u6563\u70b9\u56fe\u786e\u8ba4\uff09\uff0c\u8bc1\u5b9e\u4e86\u7eb3\u7a0e\u5408\u89c4\u65f6\u95f4\u4e0e\u7eb3\u7a0e\u91d1\u989d\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u4e14\u603b\u7eb3\u7a0e\u91d1\u989d\u7684\u5173\u7cfb\u66f4\u4e3a\u660e\u663e\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u7eb3\u7a0e\u5408\u89c4\u65f6\u95f4\u4e0e\u7eb3\u7a0e\u91d1\u989d\u5b58\u5728\u6b63\u76f8\u5173\uff0c\u7b80\u5316\u7a0e\u6536\u6d41\u7a0b\uff08\u5305\u62ec\u7a0e\u6536\u5f81\u6536\u548c\u652f\u4ed8\uff09\u53ef\u4ee5\u51cf\u5c11\u7eb3\u7a0e\u4eba\u82b1\u8d39\u5728\u7a0e\u52a1\u4e0a\u7684\u65f6\u95f4\uff0c\u4ece\u800c\u964d\u4f4e\u793e\u4f1a\u6574\u4f53\u7a0e\u6536\u7ba1\u7406\u6210\u672c\u3002"}}
{"id": "2511.12959", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12959", "abs": "https://arxiv.org/abs/2511.12959", "authors": ["Jaehyung Lim", "Wonbin Kweon", "Woojoo Kim", "Junyoung Kim", "Dongha Kim", "Hwanjo Yu"], "title": "Personalized Federated Recommendation With Knowledge Guidance", "comment": null, "summary": "Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.", "AI": {"tldr": "FedRKG\u662f\u4e00\u4e2a\u89e3\u51b3\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u5185\u5b58\u6548\u7387\u4e0e\u6027\u80fd\u6743\u8861\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u673a\u5236\u5728\u5355\u77e5\u8bc6\u6a21\u578b\u5185\u5b58\u5360\u7528\u4e0b\u5b9e\u73b0\u53cc\u77e5\u8bc6\u6a21\u578b\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u63a8\u8350\u6a21\u578b\u9762\u4e34\u5173\u952e\u56f0\u5883\uff1a\u5185\u5b58\u9ad8\u6548\u7684\u5355\u77e5\u8bc6\u6a21\u578b\u56e0\u4e22\u5f03\u6709\u4ef7\u503c\u4e2a\u6027\u5316\u4fe1\u606f\u800c\u6027\u80fd\u53d7\u9650\uff0c\u800c\u9ad8\u6027\u80fd\u7684\u53cc\u77e5\u8bc6\u6a21\u578b\u53c8\u56e0\u5185\u5b58\u9700\u6c42\u8fc7\u5927\u96be\u4ee5\u5728\u5b9e\u9645\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u5f15\u5bfc\u539f\u5219\uff0c\u907f\u514d\u5b8c\u5168\u66ff\u6362\uff0c\u800c\u662f\u5c06\u5168\u5c40\u77e5\u8bc6\u878d\u5408\u5230\u4fdd\u7559\u7684\u672c\u5730\u5d4c\u5165\u4e2d\uff1b\u5f15\u5165\u81ea\u9002\u5e94\u5f15\u5bfc\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFedRKG\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "FedRKG\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u5185\u5b58\u6548\u7387\u4e0e\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u5728\u5355\u77e5\u8bc6\u6a21\u578b\u5185\u5b58\u5360\u7528\u4e0b\u5b9e\u73b0\u4e86\u53cc\u77e5\u8bc6\u6a21\u578b\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002"}}
{"id": "2511.11914", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11914", "abs": "https://arxiv.org/abs/2511.11914", "authors": ["Shizhou Xu", "Yuan Ni", "Stefan Broecker", "Thomas Strohmer"], "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization", "comment": null, "summary": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.", "AI": {"tldr": "Forgetting-MarI\u662f\u4e00\u4e2aLLM\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u8fb9\u9645\u4fe1\u606f\u6765\u9009\u62e9\u6027\u79fb\u9664\u5f85\u9057\u5fd8\u6570\u636e\u7684\u53c2\u6570\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u7559\u5f85\u4fdd\u7559\u6570\u636e\u7684\u4fe1\u606f\uff0c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u4e0d\u53ef\u68c0\u6d4b\u6027\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u5728\u4e0d\u65ad\u6269\u5927\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u9700\u8981\u4ece\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u6570\u636e\u5f71\u54cd\u4ee5\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u548c\u6cd5\u89c4\u5408\u89c4\u8981\u6c42\u3002\u9057\u5fd8\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u79fb\u9664\u53c2\u6570\u77e5\u8bc6\u907f\u514d\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\uff0c\u8fd9\u5bf9\u8d44\u6e90\u5bc6\u96c6\u578b\u6a21\u578b\u5982LLM\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165Forgetting-MarI LLM\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u8fb9\u9645\u4fe1\u606f\u6765\u4ec5\u79fb\u9664\u5f85\u9057\u5fd8\u6570\u636e\u8d21\u732e\u7684\u989d\u5916\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u5f85\u4fdd\u7559\u6570\u636e\u652f\u6301\u7684\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5bf9\u672a\u5b66\u4e60\u6570\u636e\u96c6\u5728\u8bad\u7ec3\u6a21\u578b\u4e2d\u6b8b\u4f59\u5f71\u54cd\u7684\u660e\u786e\u4e0a\u754c\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u9057\u5fd8\u548c\u66f4\u597d\u7684\u901a\u7528\u6a21\u578b\u6027\u80fd\u4fdd\u7559\u3002", "conclusion": "\u8fd9\u4e00\u8fdb\u5c55\u4ee3\u8868\u4e86\u4f7fAI\u7cfb\u7edf\u66f4\u53ef\u63a7\u3001\u66f4\u7b26\u5408\u9690\u79c1\u548c\u7248\u6743\u6cd5\u89c4\u800c\u4e0d\u635f\u5bb3\u5176\u6709\u6548\u6027\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2511.11755", "categories": ["cs.CY", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.11755", "abs": "https://arxiv.org/abs/2511.11755", "authors": ["Isadora Cristina", "Ramon Gonze", "J\u00f4natas Santos", "Julio Reis", "M\u00e1rio Alvim", "Bernardo Queiroz", "Fabr\u00edcio Benevenuto"], "title": "Brazil Data Commons: A Platform for Unifying and Integrating Brazil's Public Data", "comment": null, "summary": "The fragmentation of public data in Brazil, coupled with inconsistent standards and limited interoperability, hinders effective research, evidence-based policymaking and access to data-driven insights. To address these issues, we introduce Brazil Data Commons, a platform that unifies various Brazilian datasets under a common semantic framework, enabling the seamless discovery, integration and visualization of information from different domains. By adopting globally recognized ontologies and interoperable data standards, Brazil Data Commons aligns with the principles of the broader Data Commons ecosystem and places Brazilian data in a global context. Through user-friendly interfaces, straightforward query mechanisms and flexible data access options, the platform democratizes data use and enables researchers, policy makers, and the public to gain meaningful insights and make informed decisions. This paper illustrates how Brazil Data Commons transforms scattered datasets into an integrated and easily navigable resource that allows a deeper understanding of Brazil's complex social, economic and environmental landscape.", "AI": {"tldr": "\u5df4\u897f\u6570\u636e\u5171\u4eab\u5e73\u53f0\u901a\u8fc7\u7edf\u4e00\u8bed\u4e49\u6846\u67b6\u6574\u5408\u5206\u6563\u7684\u5df4\u897f\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u6570\u636e\u788e\u7247\u5316\u548c\u4e92\u64cd\u4f5c\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u4fc3\u8fdb\u7814\u7a76\u548c\u653f\u7b56\u5236\u5b9a\u3002", "motivation": "\u89e3\u51b3\u5df4\u897f\u516c\u5171\u6570\u636e\u788e\u7247\u5316\u3001\u6807\u51c6\u4e0d\u4e00\u81f4\u548c\u4e92\u64cd\u4f5c\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u56e0\u7d20\u963b\u788d\u4e86\u6709\u6548\u7814\u7a76\u3001\u5faa\u8bc1\u51b3\u7b56\u548c\u6570\u636e\u9a71\u52a8\u6d1e\u5bdf\u7684\u83b7\u53d6\u3002", "method": "\u5f15\u5165\u5df4\u897f\u6570\u636e\u5171\u4eab\u5e73\u53f0\uff0c\u91c7\u7528\u5168\u7403\u8ba4\u53ef\u7684\u8bed\u4e49\u672c\u4f53\u548c\u4e92\u64cd\u4f5c\u6570\u636e\u6807\u51c6\uff0c\u5728\u7edf\u4e00\u8bed\u4e49\u6846\u67b6\u4e0b\u6574\u5408\u5404\u79cd\u5df4\u897f\u6570\u636e\u96c6\u3002", "result": "\u5e73\u53f0\u5b9e\u73b0\u4e86\u4e0d\u540c\u9886\u57df\u4fe1\u606f\u7684\u65e0\u7f1d\u53d1\u73b0\u3001\u96c6\u6210\u548c\u53ef\u89c6\u5316\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u516c\u4f17\u80fd\u591f\u83b7\u5f97\u6709\u610f\u4e49\u7684\u89c1\u89e3\u5e76\u505a\u51fa\u660e\u667a\u51b3\u7b56\u3002", "conclusion": "\u5df4\u897f\u6570\u636e\u5171\u4eab\u5e73\u53f0\u5c06\u5206\u6563\u7684\u6570\u636e\u96c6\u8f6c\u53d8\u4e3a\u96c6\u6210\u4e14\u6613\u4e8e\u5bfc\u822a\u7684\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u5730\u7406\u89e3\u5df4\u897f\u590d\u6742\u7684\u793e\u4f1a\u3001\u7ecf\u6d4e\u548c\u73af\u5883\u666f\u89c2\u3002"}}
{"id": "2511.11732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11732", "abs": "https://arxiv.org/abs/2511.11732", "authors": ["Aditya Mehta", "Swarnim Chaudhary", "Pratik Narang", "Jagat Sesh Challa"], "title": "Exposing DeepFakes via Hyperspectral Domain Mapping", "comment": "Accepted at AAAI 2026 Student Abstract", "summary": "Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.", "AI": {"tldr": "HSI-Detect\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06RGB\u56fe\u50cf\u91cd\u5efa\u4e3a31\u901a\u9053\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u5728\u9ad8\u5149\u8c31\u57df\u8fdb\u884c\u68c0\u6d4b\uff0c\u76f8\u6bd4RGB\u57df\u80fd\u653e\u5927\u4f2a\u9020\u75d5\u8ff9\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u548c\u6269\u6563\u6a21\u578b\u4ea7\u751f\u7684\u56fe\u50cf\u9ad8\u5ea6\u903c\u771f\uff0c\u53ef\u80fd\u8bef\u5bfc\u4eba\u7c7b\u611f\u77e5\u548c\u81ea\u52a8\u5316\u68c0\u6d4b\u7cfb\u7edf\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5728RGB\u7a7a\u95f4\u5206\u6790\u4e09\u4e2a\u5149\u8c31\u901a\u9053\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u68c0\u6d4b\u6d41\u7a0b\uff1a1\uff09\u4ece\u6807\u51c6RGB\u8f93\u5165\u91cd\u5efa31\u901a\u9053\u9ad8\u5149\u8c31\u56fe\u50cf\uff1b2\uff09\u5728\u9ad8\u5149\u8c31\u57df\u8fdb\u884c\u68c0\u6d4b\u3002\u901a\u8fc7\u6269\u5c55\u8f93\u5165\u8868\u793a\u5230\u66f4\u5bc6\u96c6\u7684\u5149\u8c31\u5e26\uff0c\u653e\u5927RGB\u57df\u4e2d\u5f31\u6216\u4e0d\u53ef\u89c1\u7684\u64cd\u4f5c\u4f2a\u5f71\u3002", "result": "\u5728FaceForensics++\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528RGB\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cHSI-Detect\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u9ad8\u5149\u8c31\u57df\u6620\u5c04\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u5177\u6709\u826f\u597d\u524d\u666f\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.11622", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11622", "abs": "https://arxiv.org/abs/2511.11622", "authors": ["Alexis Roger", "Gwen Legate", "Kashif Rasul", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models", "comment": null, "summary": "Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e2d\u5206\u8bcd\u5668\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u7f29\u653e\u548c\u91cf\u5316\u7b56\u7565\uff09\u4ee5\u53ca\u9884\u8bad\u7ec3\u4e0e\u968f\u673a\u521d\u59cb\u5316\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5206\u8bcd\u5668\u914d\u7f6e\u4e3b\u8981\u63a7\u5236\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u800c\u8fc1\u79fb\u5b66\u4e60\u5f71\u54cd\u4f18\u5316\u6548\u7387\u548c\u5bf9\u9f50\u3002", "motivation": "\u7814\u7a76\u5206\u8bcd\u5668\u8bbe\u8ba1\u548c\u8fc1\u79fb\u5b66\u4e60\u5728\u6784\u5efa\u6700\u5148\u8fdb\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u79bb\u6563\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u5177\u4f53\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u7ecf\u9a8c\u8bad\u7ec3\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7814\u7a76\u5206\u8bcd\u5668\u7f29\u653e\u548c\u91cf\u5316\u7b56\u7565\uff0c\u4ee5\u53ca\u9884\u8bad\u7ec3\u4e0e\u968f\u673a\u521d\u59cb\u5316\u7684\u5f71\u54cd\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u8bbe\u8ba1\u826f\u597d\u7684\u5206\u8bcd\u5668\uff0c\u7279\u522b\u662f\u5728\u8f83\u5c0f\u8bcd\u6c47\u91cf\u65f6\uff1b\u800c\u5bf9\u9f50\u4e0d\u5f53\u7684\u5206\u8bcd\u4f1a\u524a\u5f31\u751a\u81f3\u9006\u8f6c\u9884\u8bad\u7ec3\u7684\u76ca\u5904\u3002", "conclusion": "\u5728\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5206\u8bcd\u5668\u81f3\u5173\u91cd\u8981\uff0c\u5c06\u5c0f\u578b\u9ad8\u6548\u8bcd\u6c47\u8868\u4e0e\u9884\u8bad\u7ec3\u6743\u91cd\u7ed3\u5408\u5728\u591a\u6a21\u6001\u9884\u6d4b\u8bbe\u7f6e\u4e2d\u5c24\u5176\u6709\u5229\u3002"}}
{"id": "2511.11757", "categories": ["cs.CY", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.11757", "abs": "https://arxiv.org/abs/2511.11757", "authors": ["Anya Bardach", "Hamilton Murrah"], "title": "Bridging the Skills Gap: A Course Model for Modern Generative AI Education", "comment": "10 pages, 2 figures, in the 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26) EAAI Symposium", "summary": "Research on how the popularization of generative Artificial Intelligence (AI) tools impacts learning environments has led to hesitancy among educators to teach these tools in classrooms, creating two observed disconnects. Generative AI competency is increasingly valued in industry but not in higher education, and students are experimenting with generative AI without formal guidance. The authors argue students across fields must be taught to responsibly and expertly harness the potential of AI tools to ensure job market readiness and positive outcomes. Computer Science trajectories are particularly impacted, and while consistently top ranked U.S. Computer Science departments teach the mechanisms and frameworks underlying AI, few appear to offer courses on applications for existing generative AI tools. A course was developed at a private research university to teach undergraduate and graduate Computer Science students applications for generative AI tools in software development. Two mixed method surveys indicated students overwhelmingly found the course valuable and effective. Co-authored by the instructor and one of the graduate students, this paper explores the context, implementation, and impact of the course through data analysis and reflections from both perspectives. It additionally offers recommendations for replication in and beyond Computer Science departments. This is the extended version of this paper to include technical appendices.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u666e\u53ca\u5bf9\u5b66\u4e60\u73af\u5883\u7684\u5f71\u54cd\uff0c\u6307\u51fa\u9ad8\u7b49\u6559\u80b2\u4e2dAI\u80fd\u529b\u57f9\u517b\u4e0e\u884c\u4e1a\u9700\u6c42\u8131\u8282\u7684\u95ee\u9898\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u95e8\u9762\u5411\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u7684\u751f\u6210\u5f0fAI\u5e94\u7528\u8bfe\u7a0b\uff0c\u901a\u8fc7\u8c03\u67e5\u8bc1\u660e\u8bfe\u7a0b\u6548\u679c\u663e\u8457\uff0c\u5e76\u63d0\u4f9b\u4e86\u63a8\u5e7f\u5efa\u8bae\u3002", "motivation": "\u751f\u6210\u5f0fAI\u80fd\u529b\u5728\u884c\u4e1a\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u9ad8\u7b49\u6559\u80b2\u4e2d\u7f3a\u4e4f\u76f8\u5173\u8bfe\u7a0b\uff0c\u5bfc\u81f4\u5b66\u751f\u7f3a\u4e4f\u6b63\u89c4\u6307\u5bfc\u3002\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57df\u5c24\u5176\u53d7\u5230\u5f71\u54cd\uff0c\u9700\u8981\u57f9\u517b\u5b66\u751f\u8d1f\u8d23\u4efb\u5730\u4f7f\u7528AI\u5de5\u5177\u7684\u80fd\u529b\u3002", "method": "\u5728\u4e00\u6240\u79c1\u7acb\u7814\u7a76\u578b\u5927\u5b66\u4e3a\u8ba1\u7b97\u673a\u79d1\u5b66\u672c\u79d1\u751f\u548c\u7814\u7a76\u751f\u5f00\u53d1\u4e86\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e94\u7528\u7684\u8bfe\u7a0b\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u8c03\u67e5\u8bc4\u4f30\u8bfe\u7a0b\u6548\u679c\u3002", "result": "\u8c03\u67e5\u663e\u793a\u5b66\u751f\u666e\u904d\u8ba4\u4e3a\u8bfe\u7a0b\u6709\u4ef7\u503c\u4e14\u6709\u6548\uff0c\u8bfe\u7a0b\u6210\u529f\u586b\u8865\u4e86AI\u5e94\u7528\u6559\u80b2\u7684\u7a7a\u767d\u3002", "conclusion": "\u8ba1\u7b97\u673a\u79d1\u5b66\u53ca\u5176\u4ed6\u9886\u57df\u90fd\u9700\u8981\u5f00\u8bbe\u751f\u6210\u5f0fAI\u5e94\u7528\u8bfe\u7a0b\uff0c\u4ee5\u786e\u4fdd\u5b66\u751f\u5177\u5907\u5c31\u4e1a\u5e02\u573a\u6240\u9700\u7684AI\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u8bfe\u7a0b\u590d\u5236\u7684\u5efa\u8bae\u3002"}}
{"id": "2511.11623", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11623", "abs": "https://arxiv.org/abs/2511.11623", "authors": ["Yushan Jiang", "Shuteng Niu", "Dongjin Song", "Yichen Wang", "Jingna Feng", "Xinyue Hu", "Liu Yang", "Cui Tao"], "title": "Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data", "comment": null, "summary": "Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u65e9\u671f\u9884\u6d4b\u809d\u79fb\u690d\u540e\u7684\u79fb\u690d\u7269\u6297\u5bbf\u4e3b\u75c5(GVHD)\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u56db\u79cd\u6a21\u6001\u6570\u636e\uff0c\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u57282100\u540d\u60a3\u8005\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "GVHD\u662f\u809d\u79fb\u690d\u4e2d\u7f55\u89c1\u4f46\u81f4\u547d\u7684\u5e76\u53d1\u75c7\uff0c\u6b7b\u4ea1\u7387\u6781\u9ad8\u3002\u901a\u8fc7\u6574\u5408\u5f02\u6784\u548c\u4e0d\u5e73\u8861\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u65e8\u5728\u5b9e\u73b0GVHD\u7684\u65e9\u671f\u9884\u6d4b\uff0c\u4e3a\u53ca\u65f6\u5e72\u9884\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u94fa\u5e73\u9053\u8def\u3002", "method": "\u5206\u6790\u4e862100\u540d\u809d\u79fb\u690d\u60a3\u8005\u7684\u672f\u524d\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u5305\u62ec42\u4f8bGVHD\u75c5\u4f8b\u3002\u6570\u636e\u96c6\u5305\u542b\u56db\u79cd\u4e3b\u8981\u6a21\u6001\uff1a\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u3001\u8bca\u65ad\u548c\u836f\u7269\u3002\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u878d\u5408\u8fd9\u4e9b\u6a21\u6001\uff0c\u5904\u7406\u4e0d\u89c4\u5219\u8bb0\u5f55\u548c\u7f3a\u5931\u503c\uff0c\u5e76\u901a\u8fc7AUC\u4f18\u5316\u89e3\u51b3\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4f18\u4e8e\u6240\u6709\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86AUC\u4e3a0.836\uff0cAUPRC\u4e3a0.157\uff0c\u53ec\u56de\u7387\u4e3a0.768\uff0c\u7279\u5f02\u6027\u4e3a0.803\u3002\u8bc1\u660e\u4e86\u4ece\u4e0d\u540c\u6a21\u6001\u4e2d\u6355\u83b7\u4e92\u8865\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u663e\u8457\u6539\u8fdb\u4e86GVHD\u65e9\u671f\u9884\u6d4b\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u4e16\u754c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u65e9\u671f\u9884\u6d4b\u3002\u5c3d\u7ba1\u9762\u4e34\u6781\u5ea6\u4e0d\u5e73\u8861\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u6311\u6218\uff0c\u8be5\u65b9\u6cd5\u5728\u809d\u79fb\u690dGVHD\u65e9\u671f\u9884\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.11921", "categories": ["cs.AI", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.11921", "abs": "https://arxiv.org/abs/2511.11921", "authors": ["Liudong Xing", "Janet", "Lin"], "title": "Looking Forward: Challenges and Opportunities in Agentic AI Reliability", "comment": "13 pages, 6 figures; This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by SpringerNature", "summary": "This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u6784\u5efa\u53ef\u9760AI\u7cfb\u7edf\uff08\u7279\u522b\u662f\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff09\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u7ea7\u8054\u6545\u969c\u98ce\u9669\u7f13\u89e3\u3001\u52a8\u6001\u73af\u5883\u3001\u4efb\u52a1\u6267\u884c\u4e0d\u4e00\u81f4\u6027\u3001\u4e0d\u53ef\u9884\u6d4b\u6d8c\u73b0\u884c\u4e3a\u7b49\u7814\u7a76\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u7279\u522b\u662f\u667a\u80fd\u4f53AI\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u7ae0\u65e8\u5728\u8bc6\u522b\u548c\u63a2\u8ba8\u6784\u5efa\u53ef\u9760AI\u7cfb\u7edf\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u548c\u5f00\u653e\u7814\u7a76\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524dAI\u7cfb\u7edf\u53ef\u9760\u6027\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u8bc6\u522b\u51fa\u591a\u4e2a\u5173\u952e\u6311\u6218\u9886\u57df\uff0c\u5305\u62ec\u7ea7\u8054\u6545\u969c\u3001\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u3001\u4efb\u52a1\u6267\u884c\u4e00\u81f4\u6027\u3001\u6d8c\u73b0\u884c\u4e3a\u9884\u6d4b\u7b49\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u7814\u7a76\u65b9\u5411\u548c\u673a\u4f1a\u3002", "result": "\u8bc6\u522b\u4e86\u6784\u5efa\u53ef\u9760AI\u7cfb\u7edf\u7684\u591a\u4e2a\u5173\u952e\u6311\u6218\u9886\u57df\uff0c\u5305\u62ec\u7ea7\u8054\u6545\u969c\u98ce\u9669\u7f13\u89e3\u3001\u52a8\u6001\u73af\u5883\u5904\u7406\u3001\u4efb\u52a1\u6267\u884c\u4e0d\u4e00\u81f4\u6027\u3001\u4e0d\u53ef\u9884\u6d4b\u6d8c\u73b0\u884c\u4e3a\u4ee5\u53ca\u8d44\u6e90\u5bc6\u96c6\u578b\u53ef\u9760\u6027\u673a\u5236\u7b49\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u6784\u5efa\u53ef\u9760\u7684\u667a\u80fd\u4f53AI\u7cfb\u7edf\u9762\u4e34\u591a\u65b9\u9762\u6311\u6218\uff0c\u9700\u8981\u5728\u7ea7\u8054\u6545\u969c\u7f13\u89e3\u3001\u73af\u5883\u9002\u5e94\u6027\u3001\u884c\u4e3a\u9884\u6d4b\u548c\u6d4b\u8bd5\u8bc4\u4f30\u7b49\u65b9\u9762\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u8fd9\u4e9b\u7814\u7a76\u65b9\u5411\u5bf9\u4e8e\u63a8\u52a8AI\u7cfb\u7edf\u7684\u5b89\u5168\u53ef\u9760\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.11761", "categories": ["cs.CY", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.11761", "abs": "https://arxiv.org/abs/2511.11761", "authors": ["Soogand Alavi", "Salar Nozari", "Andrea Luangrath"], "title": "Cost Transparency of Enterprise AI Adoption", "comment": null, "summary": "Recent advances in large language models (LLMs) have dramatically improved performance on a wide range of tasks, driving rapid enterprise adoption. Yet, the cost of adopting these AI services is understudied. Unlike traditional software licensing in which costs are predictable before usage, commercial LLM services charge per token of input text in addition to generated output tokens. Crucially, while firms can control the input, they have limited control over output tokens, which are effectively set by generation dynamics outside of business control. This research shows that subtle shifts in linguistic style can systematically alter the number of output tokens without impacting response quality. Using an experiment with OpenAI's API, this study reveals that non-polite prompts significantly increase output tokens leading to higher enterprise costs and additional revenue for OpenAI. Politeness is merely one instance of a broader phenomenon in which linguistic structure can drive unpredictable cost variation. For enterprises integrating LLM into applications, this unpredictability complicates budgeting and undermines transparency in business-to-business contexts. By demonstrating how end-user behavior links to enterprise costs through output token counts, this work highlights the opacity of current pricing models and calls for new approaches to ensure predictable and transparent adoption of LLM services.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u4e2d\uff0c\u975e\u793c\u8c8c\u7684\u63d0\u793a\u4f1a\u663e\u8457\u589e\u52a0\u8f93\u51fatoken\u6570\u91cf\uff0c\u5bfc\u81f4\u4f01\u4e1a\u6210\u672c\u4e0a\u5347\uff0c\u800c\u793c\u8c8c\u63d0\u793a\u5219\u80fd\u51cf\u5c11\u6210\u672c\uff0c\u8fd9\u79cd\u8bed\u8a00\u98ce\u683c\u5bf9\u6210\u672c\u7684\u5f71\u54cd\u63ed\u793a\u4e86\u5f53\u524d\u5b9a\u4ef7\u6a21\u578b\u7684\u4e0d\u900f\u660e\u6027\u3002", "motivation": "\u968f\u7740\u4f01\u4e1a\u5e7f\u6cdb\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u670d\u52a1\uff0c\u5176\u6309token\u8ba1\u4ef7\u7684\u6210\u672c\u6a21\u5f0f\u5b58\u5728\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u7279\u522b\u662f\u8f93\u51fatoken\u6570\u91cf\u8d85\u51fa\u4f01\u4e1a\u63a7\u5236\u8303\u56f4\uff0c\u8fd9\u7ed9\u4f01\u4e1a\u9884\u7b97\u548c\u900f\u660e\u5ea6\u5e26\u6765\u6311\u6218\u3002", "method": "\u901a\u8fc7OpenAI API\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u793c\u8c8c\u548c\u975e\u793c\u8c8c\u63d0\u793a\u5bf9\u8f93\u51fatoken\u6570\u91cf\u7684\u5f71\u54cd\uff0c\u5206\u6790\u8bed\u8a00\u98ce\u683c\u53d8\u5316\u5982\u4f55\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u8f93\u51fatoken\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u975e\u793c\u8c8c\u63d0\u793a\u663e\u8457\u589e\u52a0\u8f93\u51fatoken\u6570\u91cf\uff0c\u5bfc\u81f4\u4f01\u4e1a\u6210\u672c\u4e0a\u5347\uff0c\u800c\u793c\u8c8c\u63d0\u793a\u80fd\u6709\u6548\u51cf\u5c11\u8f93\u51fatoken\uff0c\u4f46\u4e0d\u4f1a\u5f71\u54cd\u54cd\u5e94\u8d28\u91cf\u3002", "conclusion": "\u5f53\u524dLLM\u670d\u52a1\u7684\u5b9a\u4ef7\u6a21\u578b\u5b58\u5728\u4e0d\u900f\u660e\u6027\uff0c\u8bed\u8a00\u7ed3\u6784\u53d8\u5316\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u6210\u672c\u6ce2\u52a8\uff0c\u9700\u8981\u65b0\u7684\u5b9a\u4ef7\u65b9\u6cd5\u786e\u4fdd\u4f01\u4e1a\u91c7\u7528\u7684\u53ef\u9884\u6d4b\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2511.11625", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11625", "abs": "https://arxiv.org/abs/2511.11625", "authors": ["Mohammad Karami", "Mohammad Reza Nemati", "Aidin Kazemi", "Ali Mikaeili Barzili", "Hamid Azadegan", "Behzad Moshiri"], "title": "MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks", "comment": null, "summary": "Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.\n  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy", "AI": {"tldr": "MedFedPure\u662f\u4e00\u4e2a\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u533b\u7597AI\u6a21\u578b\u514d\u53d7\u5bf9\u6297\u653b\u51fb\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u3001\u63a9\u7801\u81ea\u7f16\u7801\u5668\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u6269\u6563\u51c0\u5316\u6a21\u5757\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u5047\u8bbe\u96c6\u4e2d\u5f0f\u6570\u636e\uff0c\u96be\u4ee5\u5e94\u5bf9\u8054\u90a6\u533b\u7597\u73af\u5883\u4e2d\u5206\u6563\u548c\u591a\u6837\u5316\u7684\u6570\u636e\u5206\u5e03\uff0c\u800c\u5bf9\u6297\u653b\u51fb\u53ef\u80fd\u901a\u8fc7\u5fae\u5c0f\u6270\u52a8\u8bef\u5bfcAI\u6a21\u578b\u5bfc\u81f4\u4e25\u91cd\u8bef\u8bca\u3002", "method": "\u7ed3\u5408\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u9002\u5e94\u5404\u673a\u6784\u6570\u636e\u5206\u5e03\uff1b\u63a9\u7801\u81ea\u7f16\u7801\u5668\u68c0\u6d4b\u53ef\u7591\u8f93\u5165\uff1b\u81ea\u9002\u5e94\u6269\u6563\u51c0\u5316\u6a21\u5757\u9009\u62e9\u6027\u6e05\u7406\u88ab\u6807\u8bb0\u7684\u626b\u63cf\u56fe\u50cf\u3002", "result": "\u5728Br35H\u8111MRI\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u6297\u9c81\u68d2\u6027\u4ece49.50%\u63d0\u5347\u81f387.33%\uff0c\u540c\u65f6\u4fdd\u630197.67%\u7684\u6e05\u6d01\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u90e8\u7f72\u5b89\u5168\u3001\u53ef\u4fe1\u4e14\u4fdd\u62a4\u9690\u79c1\u7684AI\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.13166", "categories": ["cs.IR", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13166", "abs": "https://arxiv.org/abs/2511.13166", "authors": ["Zhaoxin Shen", "Dan Wu"], "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users", "comment": "4 pages, 2 figures", "summary": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u5c40\u90e8\u534f\u540c\u8fc7\u6ee4(LCF)\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u7528\u6237\u95f4\u7684\u5c40\u90e8\u76f8\u4f3c\u6027\uff0c\u7ed3\u5408\u5927\u6570\u5b9a\u5f8b\u6574\u5408\u7528\u6237\u6570\u636e\uff0c\u63d0\u9ad8\u7528\u6237\u884c\u4e3a\u6570\u636e\u7684\u5229\u7528\u7387\u3002", "motivation": "\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u5229\u7528\u4e92\u8054\u7f51\u4e2d\u7684\u7528\u6237\u884c\u4e3a\u6570\u636e\u6765\u6539\u8fdb\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u534f\u540c\u8fc7\u6ee4(LCF)\u65b9\u6cd5\uff0c\u5229\u7528\u7528\u6237\u95f4\u7684\u5c40\u90e8\u76f8\u4f3c\u6027\uff0c\u7ed3\u5408\u5927\u6570\u5b9a\u5f8b\u6574\u5408\u7528\u6237\u6570\u636e\u3002", "result": "\u5728Steam\u6e38\u620f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLCF\u7684\u7ed3\u679c\u7b26\u5408\u5b9e\u9645\u9700\u6c42\u3002", "conclusion": "LCF\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u7528\u6237\u884c\u4e3a\u6570\u636e\u7684\u5229\u7528\u7387\uff0c\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11924", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11924", "abs": "https://arxiv.org/abs/2511.11924", "authors": ["Yongkang Huo", "Fulvio Forni", "Rodolphe Sepulchre"], "title": "A Neuromorphic Architecture for Scalable Event-Based Control", "comment": null, "summary": "This paper introduces the ``rebound Winner-Take-All (RWTA)\" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\"\u53cd\u5f39\u80dc\u8005\u901a\u5403(RWTA)\"\u57fa\u5143\u7684\u65b0\u578b\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86\u79bb\u6563\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u548c\u8fde\u7eed\u8c03\u8282\u7684\u53ef\u8c03\u6027\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u8fde\u7eed\u8282\u5f8b\u751f\u6210\u548c\u79bb\u6563\u51b3\u7b56\u95ee\u9898\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\uff0c\u65e2\u80fd\u7ee7\u627f\u80dc\u8005\u901a\u5403\u72b6\u6001\u673a\u7684\u79bb\u6563\u8ba1\u7b97\u80fd\u529b\uff0c\u53c8\u80fd\u4fdd\u6301\u53ef\u5174\u594b\u751f\u7269\u7269\u7406\u7535\u8def\u7684\u8fde\u7eed\u8c03\u8282\u7279\u6027\uff0c\u4ee5\u89e3\u51b3\u8fde\u7eed\u8282\u5f8b\u751f\u6210\u548c\u79bb\u6563\u51b3\u7b56\u7684\u7edf\u4e00\u5efa\u6a21\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\"\u53cd\u5f39\u80dc\u8005\u901a\u5403(RWTA)\"\u57fa\u5143\u4f5c\u4e3a\u57fa\u672c\u6784\u5efa\u6a21\u5757\uff0c\u6784\u5efa\u4ece\u7ec6\u80de\u7ea7\u5230\u7cfb\u7edf\u7ea7\u7684\u5c42\u6b21\u5316\u67b6\u6784\uff0c\u7ed3\u5408\u79bb\u6563\u8ba1\u7b97\u548c\u8fde\u7eed\u8c03\u8282\u80fd\u529b\uff0c\u91c7\u7528\u57fa\u4e8e\u4e8b\u4ef6\u7684\u5efa\u6a21\u6846\u67b6\u3002", "result": "\u8be5\u67b6\u6784\u5728\u86c7\u5f62\u673a\u5668\u4eba\u795e\u7ecf\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u591a\u529f\u80fd\u6027\u3001\u9c81\u68d2\u6027\u548c\u6a21\u5757\u5316\u7279\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u8fde\u7eed\u8282\u5f8b\u751f\u6210\u548c\u79bb\u6563\u51b3\u7b56\u4efb\u52a1\u3002", "conclusion": "RWTA\u67b6\u6784\u4e3a\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7269\u7406\u5efa\u6a21\u8bed\u8a00\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u79bb\u6563\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u548c\u8fde\u7eed\u8c03\u8282\u7684\u53ef\u8c03\u6027\uff0c\u5728\u673a\u5668\u4eba\u63a7\u5236\u5e94\u7528\u4e2d\u5177\u6709\u826f\u597d\u524d\u666f\u3002"}}
{"id": "2511.11764", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11764", "abs": "https://arxiv.org/abs/2511.11764", "authors": ["Nikitha Donekal Chandrashekar", "Sehrish Basir Nizamani", "Margaret Ellis", "Naren Ramakrishnan"], "title": "Demystify, Use, Reflect: Preparing students to be informed LLM-users", "comment": "2 pages 1 table Submitted to SIGCSE 2026", "summary": "We transitioned our post-CS1 course that introduces various subfields of computer science so that it integrates Large Language Models (LLMs) in a structured, critical, and practical manner. It aims to help students develop the skills needed to engage meaningfully and responsibly with AI. The course now includes explicit instruction on how LLMs work, exposure to current tools, ethical issues, and activities that encourage student reflection on personal use of LLMs as well as the larger evolving landscape of AI-assisted programming. In class, we demonstrate the use and verification of LLM outputs, guide students in the use of LLMs as an ingredient in a larger problem-solving loop, and require students to disclose and acknowledge the nature and extent of LLM assistance. Throughout the course, we discuss risks and benefits of LLMs across CS subfields. In our first iteration of the course, we collected and analyzed data from students pre and post surveys. Student understanding of how LLMs work became more technical, and their verification and use of LLMs shifted to be more discerning and collaborative. These strategies can be used in other courses to prepare students for the AI-integrated future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86\u4e00\u4e2a\u8ba1\u7b97\u673a\u79d1\u5b66\u5165\u95e8\u540e\u8bfe\u7a0b\u7684\u8f6c\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u6279\u5224\u6027\u548c\u5b9e\u8df5\u6027\u7684\u65b9\u5f0f\u6574\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u57f9\u517b\u5b66\u751f\u8d1f\u8d23\u4efb\u4f7f\u7528AI\u7684\u6280\u80fd\u3002", "motivation": "\u5e2e\u52a9\u5b66\u751f\u53d1\u5c55\u6709\u610f\u4e49\u4e14\u8d1f\u8d23\u4efb\u5730\u4e0eAI\u4e92\u52a8\u7684\u80fd\u529b\uff0c\u4e3aAI\u96c6\u6210\u7684\u672a\u6765\u505a\u51c6\u5907\u3002", "method": "\u8bfe\u7a0b\u5305\u542bLLM\u5de5\u4f5c\u539f\u7406\u7684\u660e\u786e\u6307\u5bfc\u3001\u5f53\u524d\u5de5\u5177\u5c55\u793a\u3001\u4f26\u7406\u95ee\u9898\u8ba8\u8bba\uff0c\u4ee5\u53ca\u9f13\u52b1\u5b66\u751f\u53cd\u601d\u4e2a\u4eba\u4f7f\u7528LLM\u7684\u6d3b\u52a8\u3002\u8bfe\u5802\u4e0a\u6f14\u793aLLM\u8f93\u51fa\u7684\u4f7f\u7528\u548c\u9a8c\u8bc1\uff0c\u6307\u5bfc\u5b66\u751f\u5c06LLM\u4f5c\u4e3a\u66f4\u5927\u95ee\u9898\u89e3\u51b3\u5faa\u73af\u7684\u4e00\u90e8\u5206\uff0c\u5e76\u8981\u6c42\u5b66\u751f\u62ab\u9732LLM\u534f\u52a9\u7684\u6027\u8d28\u548c\u7a0b\u5ea6\u3002", "result": "\u5728\u8bfe\u7a0b\u9996\u6b21\u8fed\u4ee3\u4e2d\uff0c\u901a\u8fc7\u524d\u540e\u8c03\u67e5\u6536\u96c6\u548c\u5206\u6790\u5b66\u751f\u6570\u636e\u3002\u5b66\u751f\u5bf9LLM\u5de5\u4f5c\u539f\u7406\u7684\u7406\u89e3\u53d8\u5f97\u66f4\u52a0\u6280\u672f\u5316\uff0c\u4ed6\u4eec\u5bf9LLM\u7684\u9a8c\u8bc1\u548c\u4f7f\u7528\u53d8\u5f97\u66f4\u52a0\u654f\u9510\u548c\u534f\u4f5c\u3002", "conclusion": "\u8fd9\u4e9b\u7b56\u7565\u53ef\u4ee5\u7528\u4e8e\u5176\u4ed6\u8bfe\u7a0b\uff0c\u4e3a\u5b66\u751f\u5e94\u5bf9AI\u96c6\u6210\u7684\u672a\u6765\u505a\u597d\u51c6\u5907\u3002"}}
{"id": "2511.11754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11754", "abs": "https://arxiv.org/abs/2511.11754", "authors": ["Stanislav Selitskiy"], "title": "Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition", "comment": null, "summary": "A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike \"traditional\" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the \"important\" dimensions (primary components) is implemented. In such a way, the \"important\" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u9690\u5f0f\u7a00\u758f\u98ce\u683cTransformer\u67b6\u6784\u2014\u2014Batch Transformers\uff0c\u901a\u8fc7\u5bf9\u91cd\u8981\u7ef4\u5ea6\uff08\u4e3b\u6210\u5206\uff09\u8fdb\u884c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u800c\u975e\u4f20\u7edfTransformer\u4e2d\u5bf9\u6574\u4e2a\u7ef4\u5ea6\u5e8f\u5217\u6216\u6279\u6b21\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u7f16\u7801\u5668-\u89e3\u7801\u5668ANN\u67b6\u6784\u4e2d\u7684\u74f6\u9888\u5927\u5c0f\u3002", "motivation": "\u4f20\u7edfTransformer\u5728\u5904\u7406\u5e8f\u5217\u6216\u6279\u6b21\u5b9e\u4f53\u65f6\u5173\u6ce8\u6574\u4e2a\u7ef4\u5ea6\uff0c\u5bfc\u81f4\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5173\u6ce8\u91cd\u8981\u7ef4\u5ea6\u6765\u51cf\u5c11\u6a21\u578b\u74f6\u9888\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51faBatch Transformers\u67b6\u6784\uff0c\u91c7\u7528\u9690\u5f0f\u7a00\u758f\u98ce\u683c\uff0c\u5bf9\u91cd\u8981\u7ef4\u5ea6\uff08\u4e3b\u6210\u5206\uff09\u5b9e\u65bd\u6ce8\u610f\u529b\u673a\u5236\uff0c\u800c\u975e\u4f20\u7edfTransformer\u4e2d\u5bf9\u6574\u4e2a\u7ef4\u5ea6\u7684\u5173\u6ce8\u3002", "result": "\u5728\u9762\u90e8\u8bc6\u522b\u4efb\u52a1\u7684\u5408\u6210\u56fe\u50cf\u751f\u6210\u4e2d\uff0c\u9488\u5bf9\u5316\u5986\u548c\u906e\u6321\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u80fd\u591f\u589e\u52a0\u6709\u9650\u539f\u59cb\u6570\u636e\u96c6\u7684\u53d8\u5f02\u6027\u3002", "conclusion": "Batch Transformers\u901a\u8fc7\u5173\u6ce8\u91cd\u8981\u7ef4\u5ea6\u5b9e\u73b0\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u74f6\u9888\u7684\u663e\u8457\u51cf\u5c0f\uff0c\u5728\u9762\u90e8\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6570\u636e\u589e\u5f3a\u80fd\u529b\u3002"}}
{"id": "2511.11627", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11627", "abs": "https://arxiv.org/abs/2511.11627", "authors": ["Wang Zhenyu", "Li Peiyuan", "Shi Yongxiang", "Wu Ruoyu", "Zhang Lei"], "title": "SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion", "comment": null, "summary": "Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.", "AI": {"tldr": "\u63d0\u51fa\u4e86SA-EMO\u67b6\u6784\u7528\u4e8e\u672a\u77e5\u5730\u4e0b\u7ed3\u6784\u7684\u901f\u5ea6\u573a\u53cd\u6f14\uff0c\u901a\u8fc7\u7ed3\u6784\u5bf9\u9f50\u7f16\u7801\u5668\u548c\u591a\u7b97\u5b50\u4e13\u5bb6\u878d\u5408\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u6ce2\u5f62\u53cd\u6f14\u7684\u7cbe\u5ea6\u548c\u8fb9\u754c\u5206\u8fa8\u7387\u3002", "motivation": "\u4f20\u7edf\u5168\u6ce2\u5f62\u53cd\u6f14\u65b9\u6cd5\u5b58\u5728\u75c5\u6001\u6027\u3001\u975e\u7ebf\u6027\u5f3a\u3001\u8ba1\u7b97\u91cf\u5927\u7b49\u95ee\u9898\uff0c\u4e14\u5355\u4e00CNN\u67b6\u6784\u6216\u795e\u7ecf\u7b97\u5b50\u96be\u4ee5\u5728\u672a\u77e5\u6216\u590d\u6742\u5730\u8d28\u73af\u5883\u4e2d\u6cdb\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u4e0d\u540c\u5730\u8d28\u7c7b\u578b\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5bf9\u9f50\u7f16\u7801\u5668\u5c06\u9ad8\u7ef4\u5730\u9707\u6ce2\u573a\u6620\u5c04\u5230\u7269\u7406\u4e00\u81f4\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u6d88\u9664\u6ce2\u5f62\u4e0e\u901f\u5ea6\u57df\u95f4\u7684\u65f6\u7a7a\u4e0d\u5339\u914d\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\u9009\u62e9\u548c\u878d\u5408\u591a\u79cd\u795e\u7ecf\u7b97\u5b50\u4e13\u5bb6\uff08\u8c31\u3001\u5c0f\u6ce2\u3001\u591a\u5c3a\u5ea6\u548c\u5c40\u90e8\u7b97\u5b50\uff09\u6765\u9884\u6d4b\u901f\u5ea6\u6a21\u578b\u3002", "result": "\u5728OpenFWI\u57fa\u51c6\u548cMarmousi2\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cSA-EMO\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5e73\u5747MAE\u964d\u4f4e\u7ea658.443%\uff0c\u8fb9\u754c\u5206\u8fa8\u7387\u63d0\u5347\u7ea610.308%\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5404\u7ec4\u4ef6\u5747\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u663e\u8457\u8d21\u732e\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u7269\u7406\u53ef\u89e3\u91ca\u7684\u5168\u6ce2\u5f62\u53cd\u6f14\u5f15\u5165\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2511.13201", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13201", "abs": "https://arxiv.org/abs/2511.13201", "authors": ["Hao Hu", "Yifan Feng", "Ruoxue Li", "Rundong Xue", "Xingliang Hou", "Zhiqiang Tian", "Yue Gao", "Shaoyi Du"], "title": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation", "comment": "Accepted by AAAI 2026 main conference", "summary": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86Cog-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u9898\u8d85\u56fe\u548c\u5b9e\u4f53\u8d85\u56fe\u7684\u53cc\u91cd\u7ed3\u6784\uff0c\u7ed3\u5408\u8ba4\u77e5\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u7684RAG\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f4e\u9636\u6210\u5bf9\u5b9e\u4f53\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u591a\u5b9e\u4f53\u95f4\u7684\u9ad8\u9636\u5173\u8054\uff1b\u800c\u8d85\u56fe\u65b9\u6cd5\u53c8\u5c40\u9650\u4e8e\u5757\u95f4\u5b9e\u4f53\u7ea7\u8868\u793a\uff0c\u5ffd\u89c6\u4e86\u5168\u5c40\u4e3b\u9898\u7ec4\u7ec7\u548c\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e3b\u9898\u5bf9\u9f50\u7684\u53cc\u8d85\u56feRAG\u6846\u67b6(Cog-RAG)\uff0c\u4f7f\u7528\u4e3b\u9898\u8d85\u56fe\u6355\u83b7\u5757\u95f4\u4e3b\u9898\u7ed3\u6784\uff0c\u5b9e\u4f53\u8d85\u56fe\u5efa\u6a21\u9ad8\u9636\u8bed\u4e49\u5173\u7cfb\uff1b\u8bbe\u8ba1\u8ba4\u77e5\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\uff0c\u5148\u6fc0\u6d3b\u67e5\u8be2\u76f8\u5173\u4e3b\u9898\u5185\u5bb9\uff0c\u518d\u6307\u5bfc\u7ec6\u7c92\u5ea6\u53ec\u56de\u548c\u6269\u6563\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCog-RAG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Cog-RAG\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u81ea\u4e0a\u800c\u4e0b\u7684\u8ba4\u77e5\u63a8\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u4ece\u5168\u5c40\u4e3b\u9898\u5230\u5c40\u90e8\u7ec6\u8282\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u4e00\u81f4\u751f\u6210\uff0c\u6709\u6548\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11772", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11772", "abs": "https://arxiv.org/abs/2511.11772", "authors": ["Chenyu Zhang", "Xiaohang Luo"], "title": "Scaling Equitable Reflection Assessment in Education via Large Language Models and Role-Based Feedback Agents", "comment": "Accepted to AAAI-26 AISI Track", "summary": "Formative feedback is widely recognized as one of the most effective drivers of student learning, yet it remains difficult to implement equitably at scale. In large or low-resource courses, instructors often lack the time, staffing, and bandwidth required to review and respond to every student reflection, creating gaps in support precisely where learners would benefit most. This paper presents a theory-grounded system that uses five coordinated role-based LLM agents (Evaluator, Equity Monitor, Metacognitive Coach, Aggregator, and Reflexion Reviewer) to score learner reflections with a shared rubric and to generate short, bias-aware, learner-facing comments. The agents first produce structured rubric scores, then check for potentially biased or exclusionary language, add metacognitive prompts that invite students to think about their own thinking, and finally compose a concise feedback message of at most 120 words. The system includes simple fairness checks that compare scoring error across lower and higher scoring learners, enabling instructors to monitor and bound disparities in accuracy. We evaluate the pipeline in a 12-session AI literacy program with adult learners. In this setting, the system produces rubric scores that approach expert-level agreement, and trained graders rate the AI-generated comments as helpful, empathetic, and well aligned with instructional goals. Taken together, these results show that multi-agent LLM systems can deliver equitable, high-quality formative feedback at a scale and speed that would be impossible for human graders alone. More broadly, the work points toward a future where feedback-rich learning becomes feasible for any course size or context, advancing long-standing goals of equity, access, and instructional capacity in education.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u63d0\u4f9b\u516c\u5e73\u7684\u5f62\u6210\u6027\u53cd\u9988\uff0c\u901a\u8fc7\u4e94\u4e2a\u534f\u8c03\u7684\u89d2\u8272\u667a\u80fd\u4f53\u6765\u8bc4\u5206\u548c\u751f\u6210\u5b66\u4e60\u8005\u53cd\u9988\uff0c\u5728AI\u7d20\u517b\u8bfe\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u7684\u8bc4\u5206\u4e00\u81f4\u6027\u3002", "motivation": "\u5f62\u6210\u6027\u53cd\u9988\u662f\u5b66\u751f\u5b66\u4e60\u7684\u6709\u6548\u9a71\u52a8\u529b\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u6216\u8d44\u6e90\u6709\u9650\u7684\u8bfe\u7a0b\u4e2d\u96be\u4ee5\u516c\u5e73\u5b9e\u65bd\uff0c\u6559\u5e08\u7f3a\u4e4f\u65f6\u95f4\u548c\u8d44\u6e90\u6765\u5ba1\u9605\u6bcf\u4e2a\u5b66\u751f\u7684\u53cd\u601d\uff0c\u5bfc\u81f4\u652f\u6301\u7f3a\u53e3\u3002", "method": "\u4f7f\u7528\u4e94\u4e2a\u534f\u8c03\u7684\u89d2\u8272\u667a\u80fd\u4f53\uff08\u8bc4\u4f30\u8005\u3001\u516c\u5e73\u76d1\u63a7\u8005\u3001\u5143\u8ba4\u77e5\u6559\u7ec3\u3001\u805a\u5408\u8005\u548c\u53cd\u601d\u5ba1\u9605\u8005\uff09\u7cfb\u7edf\uff0c\u57fa\u4e8e\u5171\u4eab\u8bc4\u5206\u6807\u51c6\u5bf9\u5b66\u4e60\u8005\u53cd\u601d\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u751f\u6210\u7b80\u77ed\u3001\u504f\u89c1\u611f\u77e5\u7684\u5b66\u4e60\u8005\u53cd\u9988\u8bc4\u8bba\uff0c\u5305\u542b\u516c\u5e73\u6027\u68c0\u67e5\u673a\u5236\u3002", "result": "\u572812\u8282AI\u7d20\u517b\u8bfe\u7a0b\u4e2d\u8bc4\u4f30\uff0c\u7cfb\u7edf\u4ea7\u751f\u7684\u8bc4\u5206\u6807\u51c6\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u7684\u4e00\u81f4\u6027\uff0c\u8bad\u7ec3\u8fc7\u7684\u8bc4\u5206\u8005\u8ba4\u4e3aAI\u751f\u6210\u7684\u8bc4\u8bba\u6709\u5e2e\u52a9\u3001\u6709\u540c\u7406\u5fc3\u4e14\u4e0e\u6559\u5b66\u76ee\u6807\u4e00\u81f4\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u80fd\u591f\u4ee5\u4eba\u7c7b\u8bc4\u5206\u8005\u65e0\u6cd5\u8fbe\u5230\u7684\u89c4\u6a21\u548c\u901f\u5ea6\u63d0\u4f9b\u516c\u5e73\u3001\u9ad8\u8d28\u91cf\u7684\u5f62\u6210\u6027\u53cd\u9988\uff0c\u4e3a\u5b9e\u73b0\u4efb\u4f55\u8bfe\u7a0b\u89c4\u6a21\u548c\u60c5\u5883\u4e0b\u7684\u53cd\u9988\u4e30\u5bcc\u5b66\u4e60\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.11780", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11780", "abs": "https://arxiv.org/abs/2511.11780", "authors": ["Hossein Mohebbi", "Mohammed Abdulrahman", "Yanting Miao", "Pascal Poupart", "Suraj Kothawade"], "title": "Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing", "comment": null, "summary": "Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.", "AI": {"tldr": "Image-POSER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u548c\u4e13\u5bb6\u6a21\u578b\u7ec4\u5408\u6765\u5904\u7406\u957f\u590d\u5408\u63d0\u793a\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u957f\u590d\u5408\u63d0\u793a\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u53ef\u9760\u7684\u7aef\u5230\u7aef\u6267\u884c\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u521b\u610f\u5de5\u4f5c\u6d41\u7a0b\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53cd\u5c04\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u534f\u8c03\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u8fdb\u884c\u52a8\u6001\u4efb\u52a1\u5206\u89e3\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6279\u8bc4\u5668\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\u3002", "result": "\u5728\u884c\u4e1a\u6807\u51c6\u548c\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cImage-POSER\u5728\u4e00\u81f4\u6027\u3001\u4fdd\u771f\u5ea6\u548c\u7f8e\u5b66\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ec\u524d\u6cbf\u6a21\u578b\uff09\uff0c\u5e76\u5728\u4eba\u5de5\u8bc4\u4f30\u4e2d\u6301\u7eed\u83b7\u5f97\u504f\u597d\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u8d4b\u4e88AI\u7cfb\u7edf\u81ea\u4e3b\u5206\u89e3\u3001\u91cd\u65b0\u6392\u5e8f\u548c\u7ec4\u5408\u89c6\u89c9\u6a21\u578b\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u5411\u901a\u7528\u89c6\u89c9\u52a9\u624b\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.11629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11629", "abs": "https://arxiv.org/abs/2511.11629", "authors": ["Xu Zhang", "Peng Wang", "Chen Wang", "Zhe Xu", "Xiaohua Nie", "Wei Wang"], "title": "Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification", "comment": "Global Feature Enhancing and Fusion Framework for Time Series Classification", "summary": "Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5168\u5c40\u7279\u5f81\u5b66\u4e60\u548c\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5e94\u53d8\u8ba1\u72b6\u6001\u8bc6\u522b\uff0c\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u548c\u5c40\u90e8\u7279\u5f81\u95f4\u9ad8\u9636\u5173\u7cfb\u5b66\u4e60\u6765\u63d0\u53d6\u5168\u5c40\u7279\u5f81\uff0c\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u5728\u5e94\u53d8\u8ba1\u72b6\u6001\u8bc6\u522b\u4e2d\uff0c\u4ec5\u4f7f\u7528CNN\u63d0\u53d6\u7684\u5c40\u90e8\u7279\u5f81\u4e0d\u8db3\u4ee5\u5145\u5206\u8868\u8fbe\u65f6\u95f4\u5e8f\u5217\uff0c\u7279\u522b\u662f\u5f53\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u7684\u5c40\u90e8\u5b50\u5e8f\u5217\u975e\u5e38\u76f8\u4f3c\u65f6\u3002CNN\u7531\u4e8e\u5377\u79ef\u64cd\u4f5c\u7684\u672c\u8d28\u9650\u5236\uff0c\u96be\u4ee5\u63d0\u53d6\u5168\u5c40\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8d85\u56fe\u7684\u5168\u5c40\u7279\u5f81\u5b66\u4e60\u548c\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u5168\u5c40\u7279\u5f81\uff1a(i) \u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u6784\u5efa\u5168\u5c40\u7279\u5f81\uff1b(ii) \u5b66\u4e60\u5c40\u90e8\u7279\u5f81\u95f4\u7684\u9ad8\u9636\u5173\u7cfb\u6765\u6355\u83b7\u5168\u5c40\u7279\u5f81\u3002\u8be5\u6846\u67b6\u5b66\u4e60\u5e76\u878d\u5408\u5168\u5c40\u7279\u5f81\u4ee5\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5de5\u4e1a\u5e94\u53d8\u8ba1\u6570\u636e\u548c\u516c\u5171UCR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u5728\u5e94\u53d8\u8ba1\u72b6\u6001\u8bc6\u522b\u4e2d\u663e\u793a\u51fa\u5bf9\u672a\u89c1\u6570\u636e\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8d85\u56fe\u5168\u5c40\u7279\u5f81\u5b66\u4e60\u548c\u878d\u5408\u6846\u67b6\u80fd\u591f\u589e\u5f3a\u5e94\u53d8\u8ba1\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5c40\u90e8\u7279\u5f81\u76f8\u4f3c\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2511.13389", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13389", "abs": "https://arxiv.org/abs/2511.13389", "authors": ["Zhipeng Ma", "Bo N\u00f8rregaard J\u00f8rgensen", "Zheng Grace Ma"], "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference", "comment": "Accepted by the Energy Informatics.Academy Conference 2025 (EI.A 2025)", "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u548c\u56e0\u679c\u63a8\u65ad\u7684\u65b9\u6cd5\u6765\u5206\u6790\u611f\u5e94\u7089\u7194\u70bc\u8fc7\u7a0b\u4e2d\u7684\u80fd\u6e90\u6548\u7387\u9a71\u52a8\u56e0\u7d20\uff0c\u8bc6\u522b\u51fa\u80fd\u6e90\u6d88\u8017\u3001\u7089\u6e29\u548c\u6750\u6599\u91cd\u91cf\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u4e3a\u94f8\u9020\u5382\u63d0\u4f9b\u4f18\u5316\u6027\u80fd\u3001\u964d\u4f4e\u80fd\u8017\u548c\u6392\u653e\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002", "motivation": "\u5de5\u4e1a\u94f8\u9020\u8fc7\u7a0b\u80fd\u8017\u5de8\u5927\u4e14\u53d8\u91cf\u95f4\u5b58\u5728\u590d\u6742\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f20\u7edf\u7684\u76f8\u5173\u6027\u5206\u6790\u96be\u4ee5\u533a\u5206\u771f\u6b63\u7684\u56e0\u679c\u9a71\u52a8\u56e0\u7d20\u4e0e\u865a\u5047\u5173\u8054\uff0c\u9650\u5236\u4e86\u5176\u5728\u51b3\u7b56\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u5c06\u7194\u70bc\u5468\u671f\u5206\u5272\u4e3a\u4e0d\u540c\u7684\u64cd\u4f5c\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528PCMCI+\u7b97\u6cd5\uff08\u6700\u5148\u8fdb\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff09\u63ed\u793a\u6bcf\u4e2a\u6a21\u5f0f\u5185\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728\u6240\u6709\u805a\u7c7b\u4e2d\uff0c\u80fd\u6e90\u6d88\u8017\u3001\u7089\u6e29\u548c\u6750\u6599\u91cd\u91cf\u4e4b\u95f4\u7684\u7a33\u5065\u56e0\u679c\u5173\u7cfb\u5b9a\u4e49\u4e86\u6548\u7387\u7684\u6838\u5fc3\u9a71\u52a8\u56e0\u7d20\uff0c\u800c\u7535\u538b\u6301\u7eed\u5f71\u54cd\u51b7\u5374\u6c34\u6e29\u5ea6\u5e76\u5177\u6709\u5ef6\u8fdf\u54cd\u5e94\u3002\u9ad8\u6548\u805a\u7c7b\u4ee5\u7a33\u5b9a\u7684\u56e0\u679c\u7ed3\u6784\u4e3a\u7279\u5f81\uff0c\u800c\u4f4e\u6548\u805a\u7c7b\u5219\u8868\u73b0\u51fa\u5f3a\u5316\u53cd\u9988\u73af\u548c\u975e\u5178\u578b\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u96c6\u6210\u805a\u7c7b-\u56e0\u679c\u63a8\u65ad\u7ba1\u9053\u4f5c\u4e3a\u5206\u6790\u80fd\u6e90\u5bc6\u96c6\u578b\u8fc7\u7a0b\u7684\u65b9\u6cd5\u521b\u65b0\uff0c\u5e76\u4e3a\u94f8\u9020\u5382\u64cd\u4f5c\u5458\u63d0\u4f9b\u4e86\u4f18\u5316\u6027\u80fd\u3001\u51cf\u5c11\u80fd\u6e90\u6d88\u8017\u548c\u964d\u4f4e\u6392\u653e\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002"}}
{"id": "2511.11954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11954", "abs": "https://arxiv.org/abs/2511.11954", "authors": ["Borchuluun Yadamsuren", "Steven Keith Platt", "Miguel Diaz"], "title": "LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code", "comment": "29 pages, 3 appendices with Prolog code and full codebase available at: https://github.com/borchuluun/section121-inconsistency-detection", "summary": "This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.\n  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.\n  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.\n  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u786e\u5b9a\u6027\u68c0\u6d4b\u590d\u6742\u6cd5\u5f8b\u4e2d\u7684\u6cd5\u5b9a\u4e0d\u4e00\u81f4\u6027\u3002\u4ee5\u7f8e\u56fd\u56fd\u5185\u7a0e\u6536\u6cd5\u5178\u4e3a\u6848\u4f8b\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u7b26\u53f7\u903b\u8f91\uff0c\u6210\u529f\u68c0\u6d4b\u5230\u4e0d\u4e00\u81f4\u6761\u6b3e\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u7a0e\u52a1\u9886\u57df\u5e94\u7528\u7a00\u5c11\uff0c\u4e14\u5728\u5904\u7406\u5c42\u6b21\u5316\u5904\u7406\u548c\u6df1\u5ea6\u7ed3\u6784\u5316\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u957f\u6587\u672c\u4e0a\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u6cd5\u5b9a\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528GPT-4o\u5c06\u7a0e\u6cd5\u6761\u6b3e\u7ffb\u8bd1\u4e3aProlog\u89c4\u5219\uff0c\u5728SWISH\u4e2d\u7cbe\u70bc\uff0c\u7136\u540e\u6d4b\u8bd5Prolog\u589e\u5f3a\u63d0\u793a\u662f\u5426\u80fd\u6539\u8fdbGPT-4o\u7684\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u3002\u540c\u65f6\u5f00\u53d1\u6df7\u5408Prolog\u6a21\u578b\uff0c\u7531GPT-5\u6307\u5bfc\u7cbe\u70bc\u3002", "result": "GPT-4o\u5728\u4e09\u79cd\u7b56\u7565\u4e2d\u4ec5\u68c0\u6d4b\u5230\u4e00\u79cd\u4e0d\u4e00\u81f4\u6027\uff0833%\u51c6\u786e\u7387\uff09\u3002\u7eaf\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b0100%\u89c4\u5219\u8986\u76d6\uff0cProlog\u589e\u5f3a\u63d0\u793a\u4ec566%\u8986\u76d6\u3002\u6df7\u5408Prolog\u6a21\u578b\u4ea7\u751f\u786e\u5b9a\u6027\u3001\u53ef\u91cd\u73b0\u7ed3\u679c\uff0c\u6210\u529f\u68c0\u6d4b\u5230\u4e0d\u4e00\u81f4\u533a\u57df\u3002", "conclusion": "\u57fa\u4e8e\u7b26\u53f7\u903b\u8f91\u7684LLM\u8f85\u52a9\u5f62\u5f0f\u5316\u80fd\u591f\u5b9e\u73b0\u900f\u660e\u53ef\u9760\u7684\u6cd5\u5b9a\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\uff0c\u6df7\u5408\u65b9\u6cd5\u4f18\u4e8e\u7eaf\u6982\u7387\u6027\u63d0\u793a\u65b9\u6cd5\u3002"}}
{"id": "2511.11824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11824", "abs": "https://arxiv.org/abs/2511.11824", "authors": ["Zhongping Dong", "Pengyang Yu", "Shuangjian Li", "Liming Chen", "Mohand Tahar Kechadi"], "title": "SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction", "comment": null, "summary": "Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \\textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.", "AI": {"tldr": "SOTFormer\u662f\u4e00\u4e2a\u7edf\u4e00\u76ee\u6807\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u77ed\u671f\u8f68\u8ff9\u9884\u6d4b\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5e38\u6570\u5185\u5b58\u7684\u65f6\u95f4transformer\u5b9e\u73b0\u7a33\u5b9a\u7684\u8eab\u4efd\u4f20\u64ad\u548c\u5b9e\u65f6\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u5728\u906e\u6321\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u65f6\u95f4\u6f02\u79fb\u7b49\u6311\u6218\u4e0b\uff0c\u5355\u76ee\u6807\u8ddf\u8e2a\u548c\u77ed\u671f\u8fd0\u52a8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u95ee\u9898\uff0c\u8fd9\u4e9b\u56e0\u7d20\u7834\u574f\u4e86\u5b9e\u65f6\u611f\u77e5\u6240\u9700\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u771f\u5b9e\u6807\u6ce8\u542f\u52a8\u7684\u5185\u5b58\u548c\u71c3\u70e7\u951a\u70b9\u635f\u5931\u6765\u7a33\u5b9a\u521d\u59cb\u5316\uff0c\u4f7f\u7528\u5355\u4e2a\u8f7b\u91cf\u7ea7\u65f6\u95f4\u6ce8\u610f\u529b\u5c42\u8de8\u5e27\u4f18\u5316\u5d4c\u5165\uff0c\u5b9e\u73b0\u56fa\u5b9aGPU\u5185\u5b58\u7684\u5b9e\u65f6\u63a8\u7406\u3002", "result": "\u5728Mini-LaSOT\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u523076.3 AUC\u548c53.7 FPS\uff084.3 GB VRAM\uff09\uff0c\u5728\u5feb\u901f\u8fd0\u52a8\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u4f18\u4e8eTrackFormer\u548cMOTRv2\u7b49transformer\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SOTFormer\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u9ad8\u6548\u7684\u65f6\u95f4\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u548c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.11990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11990", "abs": "https://arxiv.org/abs/2511.11990", "authors": ["Shaoqi Wang", "Lu Yu", "Chunjie Yang"], "title": "Improving Autoformalization Using Direct Dependency Retrieval", "comment": null, "summary": "The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u63a5\u4f9d\u8d56\u68c0\u7d22(DDR)\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6570\u5b66\u9648\u8ff0\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u4e0d\u8db3\u548c\u5f62\u5f0f\u5e93\u4f9d\u8d56\u68c0\u7d22\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5bfc\u81f4\u5f62\u5f0f\u5b9a\u4e49\u548c\u5b9a\u7406\u7684\u5e7b\u89c9\uff0c\u4e14\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5728\u5f62\u5f0f\u5e93\u4f9d\u8d56\u68c0\u7d22\u65b9\u9762\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u8f83\u5dee\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e0d\u65ad\u589e\u957f\u7684\u516c\u5171\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faDDR\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u63cf\u8ff0\u751f\u6210\u5019\u9009\u5e93\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u540e\u7f00\u6570\u7ec4\u68c0\u67e5\u9a8c\u8bc1\u5176\u5728\u5f62\u5f0f\u5e93\u4e2d\u7684\u5b58\u5728\u6027\uff0c\u6784\u5efa\u4e86\u8d85\u8fc750\u4e07\u6837\u672c\u7684\u4f9d\u8d56\u68c0\u7d22\u6570\u636e\u96c6\u5e76\u5fae\u8c03\u9ad8\u7cbe\u5ea6DDR\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDDR\u6a21\u578b\u5728\u68c0\u7d22\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u914d\u5907DDR\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5668\u5728\u5355\u6b21\u5c1d\u8bd5\u51c6\u786e\u7387\u548c\u591a\u6b21\u5c1d\u8bd5\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u6301\u7eed\u4f18\u52bf\u3002", "conclusion": "DDR\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u7684\u4f9d\u8d56\u68c0\u7d22\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u548c\u5f62\u5f0f\u6570\u5b66\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2511.11790", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11790", "abs": "https://arxiv.org/abs/2511.11790", "authors": ["Peter Kirgis"], "title": "Differences in the Moral Foundations of Large Language Models", "comment": null, "summary": "Large language models are increasingly being used in critical domains of politics, business, and education, but the nature of their normative ethical judgment remains opaque. Alignment research has, to date, not sufficiently utilized perspectives and insights from the field of moral psychology to inform training and evaluation of frontier models. I perform a synthetic experiment on a wide range of models from most major model providers using Jonathan Haidt's influential moral foundations theory (MFT) to elicit diverse value judgments from LLMs. Using multiple descriptive statistical approaches, I document the bias and variance of large language model responses relative to a human baseline in the original survey. My results suggest that models rely on different moral foundations from one another and from a nationally representative human baseline, and these differences increase as model capabilities increase. This work seeks to spur further analysis of LLMs using MFT, including finetuning of open-source models, and greater deliberation by policymakers on the importance of moral foundations for LLM alignment.", "AI": {"tldr": "\u4f7f\u7528\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f26\u7406\u5224\u65ad\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u9053\u5fb7\u57fa\u7840\u5dee\u5f02\uff0c\u4e14\u4e0e\u4eba\u7c7b\u57fa\u51c6\u5b58\u5728\u504f\u5dee\uff0c\u6a21\u578b\u80fd\u529b\u8d8a\u5f3a\u5dee\u5f02\u8d8a\u5927\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u3001\u5546\u4e1a\u548c\u6559\u80b2\u7b49\u5173\u952e\u9886\u57df\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u89c4\u8303\u6027\u4f26\u7406\u5224\u65ad\u672c\u8d28\u4ecd\u4e0d\u900f\u660e\uff0c\u9700\u8981\u5229\u7528\u9053\u5fb7\u5fc3\u7406\u5b66\u89c6\u89d2\u6765\u6307\u5bfc\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528Jonathan Haidt\u7684\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u5bf9\u591a\u4e2a\u4e3b\u8981\u6a21\u578b\u63d0\u4f9b\u5546\u7684\u6a21\u578b\u8fdb\u884c\u5408\u6210\u5b9e\u9a8c\uff0c\u901a\u8fc7\u591a\u79cd\u63cf\u8ff0\u6027\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u6a21\u578b\u54cd\u5e94\u76f8\u5bf9\u4e8e\u539f\u59cb\u8c03\u67e5\u4e2d\u4eba\u7c7b\u57fa\u51c6\u7684\u504f\u5dee\u548c\u65b9\u5dee\u3002", "result": "\u6a21\u578b\u4e4b\u95f4\u4ee5\u53ca\u6a21\u578b\u4e0e\u5168\u56fd\u4ee3\u8868\u6027\u4eba\u7c7b\u57fa\u51c6\u4e4b\u95f4\u5b58\u5728\u4e0d\u540c\u7684\u9053\u5fb7\u57fa\u7840\u4f9d\u8d56\uff0c\u4e14\u8fd9\u4e9b\u5dee\u5f02\u968f\u7740\u6a21\u578b\u80fd\u529b\u7684\u589e\u5f3a\u800c\u589e\u52a0\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u63a8\u52a8\u4f7f\u7528\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u8fdb\u4e00\u6b65\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u5bf9\u5f00\u6e90\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u5e76\u4fc3\u4f7f\u653f\u7b56\u5236\u5b9a\u8005\u66f4\u52a0\u91cd\u89c6\u9053\u5fb7\u57fa\u7840\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.11837", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11837", "abs": "https://arxiv.org/abs/2511.11837", "authors": ["Fatemeh Elhambakhsh", "Gaurav Ameta", "Aditi Roy", "Hyunwoong Ko"], "title": "MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning", "comment": null, "summary": "Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\\% and 36\\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.", "AI": {"tldr": "MP-GFormer\u662f\u4e00\u79cd3D\u51e0\u4f55\u611f\u77e5\u7684\u52a8\u6001\u56fe\u53d8\u6362\u5668\uff0c\u901a\u8fc7\u5c06\u6f14\u5316\u76843D\u51e0\u4f55\u8868\u793a\u96c6\u6210\u5230\u52a8\u6001\u56fe\u5b66\u4e60\u4e2d\uff0c\u7528\u4e8e\u9884\u6d4b\u52a0\u5de5\u64cd\u4f5c\u5e8f\u5217\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4e3b\u8981\u548c\u5b50\u64cd\u4f5c\u9884\u6d4b\u51c6\u786e\u7387\u4e0a\u5206\u522b\u63d0\u534724%\u548c36%\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u52a0\u5de5\u5de5\u827a\u89c4\u5212\u4e2d\u867d\u7136\u80fd\u6355\u6349\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u672a\u80fd\u878d\u5165\u96f6\u4ef6\u7684\u4e09\u7ef4\u51e0\u4f55\u4fe1\u606f\uff0c\u7f3a\u4e4f\u9886\u57df\u611f\u77e5\u80fd\u529b\uff0c\u9650\u5236\u4e86\u52a0\u5de5\u64cd\u4f5c\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faMP-GFormer\u65b9\u6cd5\uff0c\u5229\u7528StereoLithography\u8868\u9762\u7f51\u683c\u8868\u793a\u6bcf\u4e2a\u52a0\u5de5\u64cd\u4f5c\u540e\u7684\u96f6\u4ef63D\u51e0\u4f55\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5c06\u6f14\u5316\u76843D\u51e0\u4f55\u8868\u793a\u96c6\u6210\u5230\u52a8\u6001\u56fe\u53d8\u6362\u5668\u4e2d\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e3b\u8981\u64cd\u4f5c\u548c\u5b50\u64cd\u4f5c\u9884\u6d4b\u51c6\u786e\u7387\u4e0a\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u5347\u4e8624%\u548c36%\u3002", "conclusion": "\u5c063D\u51e0\u4f55\u4fe1\u606f\u96c6\u6210\u5230\u52a8\u6001\u56fe\u5b66\u4e60\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u52a0\u5de5\u64cd\u4f5c\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e863D\u51e0\u4f55\u611f\u77e5\u5728\u52a0\u5de5\u5de5\u827a\u89c4\u5212\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.11632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11632", "abs": "https://arxiv.org/abs/2511.11632", "authors": ["Qiuhao Zeng"], "title": "Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination", "comment": "20 pages, 5 figures", "summary": "In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5143\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u5206\u7c7b\u5668\u5206\u89e3\u4e3a\u5143\u7ec4\u4ef6\u6765\u63d0\u9ad8\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u907f\u514d\u4e86\u5bf9\u5df2\u89c1\u7c7b\u7684\u8fc7\u62df\u5408\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5ea6\u91cf\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5728\u5df2\u89c1\u7c7b\u4e0a\u5b66\u4e60\u7684\u6df1\u5ea6\u5ea6\u91cf\uff0c\u5bb9\u6613\u5bf9\u5df2\u89c1\u7c7b\u8fc7\u62df\u5408\uff0c\u5728\u672a\u89c1\u7c7b\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63a2\u7d22\u5206\u7c7b\u5668\u7684\u5b50\u7ed3\u6784\uff0c\u5c06\u6bcf\u4e2a\u5206\u7c7b\u5668\u8868\u793a\u4e3a\u5143\u7ec4\u4ef6\u7684\u7ec4\u5408\u3002\u901a\u8fc7\u6b63\u4ea4\u6b63\u5219\u5316\u4fc3\u8fdb\u5143\u7ec4\u4ef6\u7684\u591a\u6837\u6027\uff0c\u6355\u83b7\u4e0d\u540c\u5206\u7c7b\u5668\u95f4\u7684\u5171\u4eab\u5b50\u7ed3\u6784\u3002", "result": "\u5728\u5c0f\u6837\u672c\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5143\u7ec4\u4ef6\u7ec4\u5408\u7684\u65b9\u5f0f\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.13418", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13418", "abs": "https://arxiv.org/abs/2511.13418", "authors": ["Allaa Boutaleb", "Bernd Amann", "Rafael Angarita", "Hubert Naacke"], "title": "Exploring Multi-Table Retrieval Through Iterative Search", "comment": "Accepted @ the AI for Tabular Data Workshop, EurIPS 2025", "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u5f0f\u591a\u8868\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8d2a\u5fc3\u8fde\u63a5\u611f\u77e5\u68c0\u7d22\u7b97\u6cd5\u5728\u6570\u636e\u6e56\u73af\u5883\u4e2d\u5e73\u8861\u76f8\u5173\u6027\u3001\u8986\u76d6\u7387\u548c\u53ef\u8fde\u63a5\u6027\uff0c\u76f8\u6bd4\u6df7\u5408\u6574\u6570\u89c4\u5212\u65b9\u6cd5\u5feb4-400\u500d\u4e14\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u6570\u636e\u6e56\u4e2d\u7684\u5f00\u653e\u57df\u95ee\u7b54\u9700\u8981\u4ece\u591a\u4e2a\u8868\u4e2d\u68c0\u7d22\u548c\u7ec4\u5408\u4fe1\u606f\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\uff08\u5982\u6df7\u5408\u6574\u6570\u89c4\u5212\uff09\uff0c\u8981\u4e48\u53ea\u4f18\u5316\u67e5\u8be2\u8986\u76d6\u7387\u800c\u5ffd\u7565\u7ed3\u6784\u8fde\u8d2f\u6027\u3002", "method": "\u5c06\u591a\u8868\u68c0\u7d22\u6784\u5efa\u4e3a\u8fed\u4ee3\u641c\u7d22\u8fc7\u7a0b\uff0c\u63d0\u51fa\u8d2a\u5fc3\u8fde\u63a5\u611f\u77e5\u68c0\u7d22\u7b97\u6cd5\uff0c\u7efc\u5408\u8003\u8651\u76f8\u5173\u6027\u3001\u8986\u76d6\u7387\u548c\u53ef\u8fde\u63a5\u6027\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u57285\u4e2aNL2SQL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8fed\u4ee3\u65b9\u6cd5\u76f8\u6bd4MIP\u65b9\u6cd5\u5feb4-400\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "\u8fed\u4ee3\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u5b9e\u8df5\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u7ec4\u5408\u611f\u77e5\u68c0\u7d22\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.12003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12003", "abs": "https://arxiv.org/abs/2511.12003", "authors": ["Shuochen Liu", "Pengfei Luo", "Chao Zhang", "Yuhao Chen", "Haotian Zhang", "Qi Liu", "Xin Kou", "Tong Xu", "Enhong Chen"], "title": "Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning", "comment": "Poster of AAAI'2026", "summary": "Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Chain-of-Evidence (CoE)\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u6b65\u9aa4\u4e2d\u7684\u53c2\u8003\u5143\u7d20\u4e0e\u8fb9\u754c\u6846\u548c\u9875\u9762\u7d22\u5f15\u5173\u8054\uff0c\u7edf\u4e00\u4e86\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u89c6\u89c9\u8bc1\u636e\u5f52\u56e0\u3002\u540c\u65f6\u5f00\u53d1\u4e86Look As You Think (LAT)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u76d1\u7763\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6e10\u8fdb\u53ef\u8ffd\u6eaf\u6027\uff0c\u65e0\u6cd5\u786e\u4fdd\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "method": "\u63d0\u51faCoE\u8303\u5f0f\u7edf\u4e00\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u89c6\u89c9\u8bc1\u636e\u5f52\u56e0\uff0c\u5f00\u53d1LAT\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bc4\u4f30\u8bc1\u636e\u533a\u57df\u7684\u4e00\u81f4\u6027\uff0c\u4ec5\u5728CoE\u8f68\u8ff9\u4ea7\u751f\u6b63\u786e\u7b54\u6848\u65f6\u63d0\u4f9b\u5956\u52b1\uff0c\u9f13\u52b1\u8fc7\u7a0b\u7ea7\u81ea\u9a8c\u8bc1\u3002", "result": "\u5728Paper-\u548cWiki-VISA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAT\u5728\u5355\u56fe\u548c\u53cc\u56fe\u8bbe\u7f6e\u4e0b\u5747\u63d0\u5347\u4e86\u539f\u59cb\u6a21\u578b\u6027\u80fd\uff0c\u5e73\u5747\u8f6f\u7cbe\u786e\u5339\u914d\u63d0\u53478.23%\uff0cIoU@0.5\u63d0\u534747.0%\uff0c\u4e14\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u57fa\u7ebf\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CoE\u548cLAT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u53ef\u9a8c\u8bc1\u6027\u548c\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u8fc7\u7a0b\u7ea7\u81ea\u9a8c\u8bc1\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bc1\u636e\u5f52\u56e0\u548c\u8de8\u9886\u57df\u6cdb\u5316\u3002"}}
{"id": "2511.11851", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11851", "abs": "https://arxiv.org/abs/2511.11851", "authors": ["Wei-Jia Chen", "Min-Yen Tsai", "Cheng-Yi Lee", "Chia-Mu Yu"], "title": "Defending Unauthorized Model Merging via Dual-Stage Weight Protection", "comment": "10 pages, under review", "summary": "The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.", "AI": {"tldr": "MergeGuard\u662f\u4e00\u4e2a\u53cc\u9636\u6bb5\u6743\u91cd\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u5e76\u6ce8\u5165\u7ed3\u6784\u5316\u6270\u52a8\u6765\u7834\u574f\u6a21\u578b\u5408\u5e76\u517c\u5bb9\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u529f\u80fd\u5b8c\u6574\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5f00\u6e90\u4ed3\u5e93\u7684\u5feb\u901f\u6269\u6563\u4f7f\u5f97\u6a21\u578b\u5408\u5e76\u6210\u4e3a\u4fbf\u6377\u4f46\u6709\u98ce\u9669\u7684\u505a\u6cd5\uff0c\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\u4fb5\u72af\u77e5\u8bc6\u4ea7\u6743\u5e76\u7834\u574f\u6a21\u578b\u6240\u6709\u6743\u548c\u95ee\u8d23\u5236\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7L2\u6b63\u5219\u5316\u4f18\u5316\u91cd\u65b0\u5206\u914d\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u7b2c\u4e8c\u9636\u6bb5\u6ce8\u5165\u7ed3\u6784\u5316\u6270\u52a8\u6765\u9519\u4f4d\u4efb\u52a1\u5b50\u7a7a\u95f4\uff0c\u7834\u574f\u635f\u5931\u666f\u89c2\u4e2d\u7684\u66f2\u7387\u517c\u5bb9\u6027\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMergeGuard\u80fd\u5c06\u5408\u5e76\u6a21\u578b\u51c6\u786e\u7387\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u540c\u65f6\u4fdd\u62a4\u6a21\u578b\u7684\u6027\u80fd\u635f\u5931\u5c0f\u4e8e1.5%\u3002", "conclusion": "MergeGuard\u901a\u8fc7\u91cd\u5851\u6a21\u578b\u53c2\u6570\u51e0\u4f55\u5f62\u72b6\uff0c\u4f7f\u5408\u5e76\u6a21\u578b\u5d29\u6e83\u4e3a\u7834\u574f\u6027\u5e72\u6270\uff0c\u540c\u65f6\u4fdd\u62a4\u6a21\u578b\u4fdd\u6301\u5b8c\u5168\u529f\u80fd\uff0c\u6709\u6548\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\u3002"}}
{"id": "2511.11636", "categories": ["cs.LG", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11636", "abs": "https://arxiv.org/abs/2511.11636", "authors": ["Asma Sadia Khan", "Sadia Tabassum"], "title": "An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment", "comment": null, "summary": "This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u516c\u5e73\u5ba1\u8ba1\u548c\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u591a\u56ca\u5375\u5de2\u7efc\u5408\u5f81\uff08PCOS\uff09\uff0c\u901a\u8fc7\u96c6\u6210SHAP\u7279\u5f81\u5f52\u56e0\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u5ba1\u8ba1\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u5e76\u8bc6\u522b\u60a3\u8005\u4e9a\u7ec4\u95f4\u7684\u8bca\u65ad\u5dee\u5f02\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30PCOS\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u3001\u8bc6\u522b\u8bca\u65ad\u5dee\u5f02\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u89c1\u89e3\u7684\u516c\u5e73\u5ba1\u8ba1\u6846\u67b6\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u60a3\u8005\u4e9a\u7ec4\u4e2d\u7684\u53ef\u9760\u98ce\u9669\u9884\u6d4b\u3002", "method": "\u96c6\u6210SHAP\u7279\u5f81\u5f52\u56e0\u4e0e\u4eba\u53e3\u7edf\u8ba1\u5b66\u5ba1\u8ba1\uff0c\u7ed3\u5408\u6982\u7387\u6821\u51c6\u6307\u6807\uff08Brier Score\u548cExpected Calibration Error\uff09\uff0c\u8bad\u7ec3\u968f\u673a\u68ee\u6797\u3001SVM\u548cXGBoost\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u7b49\u5f20\u548cPlatt\u7f29\u653e\u8fdb\u884c\u6821\u51c6\u548c\u516c\u5e73\u6027\u6bd4\u8f83\u3002", "result": "\u6821\u51c6\u540e\u7684\u968f\u673a\u68ee\u6797\u6a21\u578b\u8fbe\u523090.8%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0cSHAP\u5206\u6790\u786e\u5b9a\u5375\u6ce1\u8ba1\u6570\u3001\u4f53\u91cd\u589e\u52a0\u548c\u6708\u7ecf\u4e0d\u89c4\u5f8b\u4e3a\u6700\u5177\u5f71\u54cd\u529b\u7684\u7279\u5f81\u3002\u6a21\u578b\u572825-35\u5c81\u5973\u6027\u4e2d\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u738790.9%\uff09\uff0c\u4f46\u572825\u5c81\u4ee5\u4e0b\u5973\u6027\u4e2d\u8868\u73b0\u8f83\u5dee\uff0869.2%\uff09\uff0c\u63ed\u793a\u4e86\u5e74\u9f84\u76f8\u5173\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u8bc6\u522b\u4e86PCOS\u9884\u6d4b\u4e2d\u7684\u4e9a\u7ec4\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eStreamlit\u7684Web\u754c\u9762\u5b9e\u73b0\u4e86\u5b9e\u65f6PCOS\u98ce\u9669\u8bc4\u4f30\u3001\u9e7f\u7279\u4e39\u6807\u51c6\u8bc4\u4f30\u548c\u4ea4\u4e92\u5f0f\"\u5047\u8bbe\"\u5206\u6790\uff0c\u5f25\u5408\u4e86AI\u7814\u7a76\u4e0e\u4e34\u5e8a\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.13523", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13523", "abs": "https://arxiv.org/abs/2511.13523", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Salil Patil", "Swarup Patil", "Vijay Mago"], "title": "Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports", "comment": null, "summary": "Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7d27\u51d1\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u9690\u79c1\u4fdd\u62a4\u66ff\u4ee3\u65b9\u6848\uff0c\u7528\u4e8e\u8f6c\u5f55\u566a\u58f0\u4e34\u5e8a\u6587\u6863\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5904\u7406\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u533b\u7597\u62a5\u544a\u56fe\u50cf\u65f6\u4f18\u4e8e\u4f20\u7edfOCR\u7cfb\u7edf\u3002", "motivation": "\u533b\u7597\u8bb0\u5f55\u6570\u5b57\u5316\u901a\u5e38\u4f9d\u8d56\u667a\u80fd\u624b\u673a\u62cd\u6444\u6253\u5370\u62a5\u544a\u7684\u7167\u7247\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5b58\u5728\u6a21\u7cca\u3001\u9634\u5f71\u548c\u5176\u4ed6\u566a\u58f0\u3002\u4f20\u7edfOCR\u7cfb\u7edf\u9488\u5bf9\u6e05\u6d01\u626b\u63cf\u4f18\u5316\uff0c\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u5370\u5ea6\u533b\u7597\u73af\u5883\u4e2d\u5e38\u89c1\u7684\u533a\u57df\u6027\u533b\u5b66\u82f1\u8bed\u64b0\u5199\u7684\u4ea7\u79d1\u8d85\u58f0\u62a5\u544a\uff0c\u6bd4\u8f83\u4e86\u516b\u4e2a\u7cfb\u7edf\u5728\u8f6c\u5f55\u51c6\u786e\u6027\u3001\u566a\u58f0\u654f\u611f\u6027\u3001\u6570\u5b57\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u7d27\u51d1\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8f6c\u5f55\u51c6\u786e\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178\u548c\u795e\u7ecfOCR\u6d41\u6c34\u7ebf\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u548c\u8bed\u8a00\u9002\u5e94\u6027\u4f7f\u5176\u6210\u4e3a\u672c\u5730\u533b\u7597\u6570\u5b57\u5316\u7684\u53ef\u884c\u5019\u9009\u65b9\u6848\u3002", "conclusion": "\u7d27\u51d1\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u566a\u58f0\u4e34\u5e8a\u6587\u6863\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u533b\u7597\u8bb0\u5f55\u6570\u5b57\u5316\u7684\u6709\u524d\u666f\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12008", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12008", "abs": "https://arxiv.org/abs/2511.12008", "authors": ["Yunqi Hong", "Johnson Kao", "Liam Edwards", "Nein-Tzu Liu", "Chung-Yen Huang", "Alex Oliveira-Kowaleski", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models", "comment": null, "summary": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.", "AI": {"tldr": "RECAP-PATH\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u75c5\u7406AI\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u5b66\u4e60\u8fc7\u7a0b\u4ece\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u81ea\u4e3b\u63a8\u5bfc\u8bca\u65ad\u6807\u51c6\uff0c\u65e0\u9700\u767d\u76d2\u8bbf\u95ee\u6216\u6743\u91cd\u66f4\u65b0\u5373\u53ef\u751f\u6210\u764c\u75c7\u8bca\u65ad\u3002", "motivation": "\u5f53\u524d\u75c5\u7406AI\u5de5\u5177\u867d\u7136\u63d0\u9ad8\u4e86\u7b5b\u67e5\u6548\u7387\u548c\u6807\u51c6\u5316\u91cf\u5316\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u4eba\u7c7b\u53ef\u8bfb\u7684\u63a8\u7406\u8fc7\u7a0b\u6765\u5ba1\u8ba1\u51b3\u7b56\u548c\u9632\u6b62\u9519\u8bef\uff0c\u91c7\u7528\u7387\u4ecd\u7136\u6709\u9650\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5b66\u4e60\u8fc7\u7a0b\uff1a\u591a\u6837\u5316\u9636\u6bb5\u6269\u5c55\u75c5\u7406\u5b66\u98ce\u683c\u7684\u89e3\u91ca\uff0c\u4f18\u5316\u9636\u6bb5\u4e3a\u51c6\u786e\u6027\u7cbe\u70bc\u8fd9\u4e9b\u89e3\u91ca\u3002\u8be5\u81ea\u5b66\u4e60\u65b9\u6cd5\u53ea\u9700\u8981\u5c0f\u89c4\u6a21\u6807\u8bb0\u6570\u636e\u96c6\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u524d\u5217\u817a\u764c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cRECAP-PATH\u4ea7\u751f\u4e86\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u4e00\u81f4\u7684\u7406\u7531\uff0c\u5e76\u5728\u8bca\u65ad\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RECAP-PATH\u901a\u8fc7\u5c06\u89c6\u89c9\u7406\u89e3\u4e0e\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e34\u5e8a\u53ef\u4fe1\u8d56\u7684AI\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u5411\u8bc1\u636e\u5173\u8054\u89e3\u91ca\u7684\u901a\u7528\u8def\u5f84\u3002"}}
{"id": "2511.11960", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11960", "abs": "https://arxiv.org/abs/2511.11960", "authors": ["Surajit Das", "Peu Majumder", "Aleksei Eliseev"], "title": "Educators on the Frontline: Philosophical and Realistic Perspectives on Integrating ChatGPT into the Learning Space", "comment": null, "summary": "The rapid emergence of Generative AI, particularly ChatGPT, has sparked a global debate on the future of education, often characterized by alarmism and speculation. Moving beyond this, this study investigates the structured, grounded perspectives of a key stakeholder group: university educators. It proposes a novel theoretical model that conceptualizes the educational environment as a \"Learning Space\" composed of seven subspaces to systematically identify the impact of AI integration. This framework was operationalized through a quantitative survey of 140 Russian university educators, with responses analyzed using a binary flagging system to measure acceptance across key indicators. The results reveal a strong but conditional consensus: a majority of educators support ChatGPT's integration, contingent upon crucial factors such as the transformation of assessment methods and the availability of plagiarism detection tools. However, significant concerns persist regarding its impact on critical thinking. Educators largely reject the notion that AI diminishes their importance, viewing their role as evolving from information-deliverer to facilitator of critical engagement. The study concludes that ChatGPT acts less as a destroyer of education and more as a catalyst for its necessary evolution, and proposes the PIPE Model (Pedagogy, Infrastructure, Policy, Education) as a strategic framework for its responsible integration. This research provides a data-driven, model-based analysis of educator attitudes, offering a nuanced alternative to the polarized discourse surrounding AI in education.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8c03\u67e5140\u540d\u4fc4\u7f57\u65af\u5927\u5b66\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e03\u4e2a\u5b50\u7a7a\u95f4\u7684'\u5b66\u4e60\u7a7a\u95f4'\u7406\u8bba\u6a21\u578b\uff0c\u7cfb\u7edf\u5206\u6790ChatGPT\u5728\u6559\u80b2\u4e2d\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570\u6559\u80b2\u5de5\u4f5c\u8005\u6709\u6761\u4ef6\u5730\u652f\u6301ChatGPT\u6574\u5408\uff0c\u4f46\u5bf9\u5176\u5f71\u54cd\u6279\u5224\u6027\u601d\u7ef4\u8868\u793a\u62c5\u5fe7\u3002", "motivation": "\u8d85\u8d8a\u5173\u4e8e\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u672a\u6765\u7684\u6050\u614c\u6027\u8ba8\u8bba\uff0c\u57fa\u4e8e\u5173\u952e\u5229\u76ca\u76f8\u5173\u8005\uff08\u5927\u5b66\u6559\u80b2\u5de5\u4f5c\u8005\uff09\u7684\u7ed3\u6784\u5316\u89c6\u89d2\uff0c\u7cfb\u7edf\u8bc6\u522bAI\u6574\u5408\u5bf9\u6559\u80b2\u73af\u5883\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa'\u5b66\u4e60\u7a7a\u95f4'\u7406\u8bba\u6a21\u578b\uff08\u5305\u542b\u4e03\u4e2a\u5b50\u7a7a\u95f4\uff09\uff0c\u901a\u8fc7\u5b9a\u91cf\u8c03\u67e5140\u540d\u4fc4\u7f57\u65af\u5927\u5b66\u6559\u80b2\u5de5\u4f5c\u8005\uff0c\u4f7f\u7528\u4e8c\u5143\u6807\u8bb0\u7cfb\u7edf\u5206\u6790\u5173\u952e\u6307\u6807\u7684\u63a5\u53d7\u5ea6\u3002", "result": "\u5927\u591a\u6570\u6559\u80b2\u5de5\u4f5c\u8005\u6709\u6761\u4ef6\u652f\u6301ChatGPT\u6574\u5408\uff0c\u524d\u63d0\u662f\u8bc4\u4f30\u65b9\u6cd5\u8f6c\u53d8\u548c\u527d\u7a83\u68c0\u6d4b\u5de5\u5177\u53ef\u7528\uff1b\u62c5\u5fe7AI\u5bf9\u6279\u5224\u6027\u601d\u7ef4\u7684\u5f71\u54cd\uff1b\u62d2\u7eddAI\u964d\u4f4e\u6559\u5e08\u91cd\u8981\u6027\u7684\u89c2\u70b9\uff0c\u8ba4\u4e3a\u6559\u5e08\u89d2\u8272\u4ece\u4fe1\u606f\u4f20\u9012\u8005\u8f6c\u53d8\u4e3a\u6279\u5224\u6027\u53c2\u4e0e\u4fc3\u8fdb\u8005\u3002", "conclusion": "ChatGPT\u4e0d\u662f\u6559\u80b2\u7684\u7834\u574f\u8005\uff0c\u800c\u662f\u5fc5\u8981\u6f14\u53d8\u7684\u50ac\u5316\u5242\uff1b\u63d0\u51faPIPE\u6a21\u578b\uff08\u6559\u5b66\u6cd5\u3001\u57fa\u7840\u8bbe\u65bd\u3001\u653f\u7b56\u3001\u6559\u80b2\uff09\u4f5c\u4e3a\u8d1f\u8d23\u4efb\u6574\u5408\u7684\u6218\u7565\u6846\u67b6\uff0c\u4e3aAI\u6559\u80b2\u8ba8\u8bba\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684\u6a21\u578b\u5206\u6790\u3002"}}
{"id": "2511.12010", "categories": ["cs.CY", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12010", "abs": "https://arxiv.org/abs/2511.12010", "authors": ["Palakorn Achananuparp", "Connie Xu", "Yao Lu", "Xavier Jayaraj Siddarth Ashok", "Ee-Peng Lim"], "title": "Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles", "comment": "Submitted to EPJ Data Science", "summary": "We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u5728\u7ebf\u7b80\u5386\u6570\u636e\u5206\u6790\u4e86\u7f8e\u56fd\u5927\u5b66\u6bd5\u4e1a\u751f\u804c\u4e1a\u6d41\u52a8\u6027\u7684\u6027\u522b\u548c\u79cd\u65cf\u5dee\u5f02\uff0c\u53d1\u73b0\u516c\u53f8\u5185\u90e8\u804c\u4f4d\u53d8\u52a8\u5bf9\u664b\u5347\u6700\u6709\u5229\uff0c\u800c\u5973\u6027\u548c\u9ed1\u4eba\u6bd5\u4e1a\u751f\u4ece\u5de5\u4f5c\u53d8\u52a8\u4e2d\u83b7\u5f97\u7684\u56de\u62a5\u663e\u8457\u4f4e\u4e8e\u7537\u6027\u548c\u767d\u4eba\u540c\u884c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5de5\u4f5c\u53d8\u52a8\u5982\u4f55\u5f71\u54cd\u804c\u4e1a\u5411\u4e0a\u6d41\u52a8\u6027\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6d41\u52a8\u6027\u7ed3\u679c\u5728\u6027\u522b\u548c\u79cd\u65cf\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u586b\u8865\u4e86\u5927\u89c4\u6a21\u804c\u4e1a\u6d41\u52a8\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u7b80\u5386\u6570\u636e\uff0c\u901a\u8fc7AI\u65b9\u6cd5\u5904\u7406\u7f3a\u5931\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u3001\u5de5\u8d44\u6570\u636e\u548c\u5608\u6742\u7684\u804c\u4e1a\u6807\u7b7e\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684FewSOC\u804c\u4e1a\u5206\u7c7b\u65b9\u6cd5\uff0c\u5206\u6790\u4e86228,710\u6761\u804c\u4e1a\u8f68\u8ff9\u3002", "result": "\u516c\u53f8\u5185\u90e8\u804c\u4f4d\u53d8\u52a8\u5bf9\u5411\u4e0a\u6d41\u52a8\u6027\u4fc3\u8fdb\u4f5c\u7528\u6700\u5f3a\uff0c\u5176\u6b21\u662f\u516c\u53f8\u95f4\u804c\u4f4d\u53d8\u52a8\u548c\u516c\u53f8\u95f4\u5e73\u7ea7\u8c03\u52a8\u3002\u5973\u6027\u548c\u9ed1\u4eba\u5927\u5b66\u6bd5\u4e1a\u751f\u7684\u804c\u4e1a\u53d8\u52a8\u56de\u62a5\u663e\u8457\u4f4e\u4e8e\u7537\u6027\u548c\u767d\u4eba\u540c\u884c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u804c\u4e1a\u6d41\u52a8\u6027\u7684\u663e\u8457\u6027\u522b\u548c\u79cd\u65cf\u5dee\u5f02\uff0c\u591a\u7ea7\u654f\u611f\u6027\u5206\u6790\u8bc1\u5b9e\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u96c6\u7fa4\u5f02\u8d28\u6027\u5177\u6709\u7a33\u5065\u6027\uff0c\u5e76\u663e\u793a\u51fa\u989d\u5916\u7684\u4ea4\u53c9\u6027\u6a21\u5f0f\u3002"}}
{"id": "2511.11882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11882", "abs": "https://arxiv.org/abs/2511.11882", "authors": ["Simon Durand", "Samuel Foucher", "Alexandre Delplanque", "Jo\u00eblle Taillon", "J\u00e9r\u00f4me Th\u00e9au"], "title": "Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)", "comment": "34 pages, 10 figures, submitted to Remote Sensing in Ecology and Conservation", "summary": "Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u8865\u5145\u6709\u9650\u8bad\u7ec3\u6570\u636e\u6765\u63d0\u9ad8\u9e9d\u725b\u68c0\u6d4b\u6027\u80fd\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u9a8c\u8bc1\u4e86\u5408\u6210\u56fe\u50cf\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u91ce\u751f\u52a8\u7269\u8c03\u67e5\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u53d7\u9650\u4e8e\u540e\u52e4\u6311\u6218\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u7a00\u758f\u5206\u5e03\u7269\u79cd\uff08\u5982\u9e9d\u725b\uff09\u7684\u5c0f\u6570\u636e\u96c6\u4e0a\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u5bfb\u627e\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6bd4\u8f83\u4e86\u57fa\u4e8e\u771f\u5b9e\u56fe\u50cf\u7684\u57fa\u7ebf\u6a21\u578b\u4e0e5\u4e2a\u96f6\u6837\u672c\u548c5\u4e2a\u5c11\u6837\u672c\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u4e2d\u9010\u6b65\u52a0\u5165\u66f4\u591a\u5408\u6210\u56fe\u50cf\u3002", "result": "\u96f6\u6837\u672c\u6a21\u578b\u4e2d\uff0c\u6dfb\u52a0\u5408\u6210\u56fe\u50cf\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u4f46\u8d85\u8fc7\u57fa\u7ebf\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6100%\u540e\u51fa\u73b0\u6536\u76ca\u9012\u51cf\uff1b\u5c11\u6837\u672c\u6a21\u578b\u4e2d\uff0c\u771f\u5b9e\u56fe\u50cf\u4e0e\u5408\u6210\u56fe\u50cf\u7ed3\u5408\u53ef\u83b7\u5f97\u66f4\u597d\u7684\u53ec\u56de\u7387\u548c\u7565\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5408\u6210\u56fe\u50cf\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u8bad\u7ec3\u51c6\u786e\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4e3a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u91cd\u8981\u89c6\u89d2\uff0c\u7279\u522b\u662f\u5728\u76d1\u6d4b\u7a00\u6709\u6216\u96be\u4ee5\u63a5\u8fd1\u7269\u79cd\u65f6\u3002"}}
{"id": "2511.11641", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.11641", "abs": "https://arxiv.org/abs/2511.11641", "authors": ["Jinqi Xiao", "Cheng Luo", "Lingyi Huang", "Cheng Yang", "Yang Sui", "Huy Phan", "Xiao Zang", "Yibiao Ying", "Zhexiang Tang", "Anima Anandkumar", "Bo Yuan"], "title": "EcoSpa: Efficient Transformer Training with Coupled Sparsity", "comment": null, "summary": "Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\\% memory reduction and 21\\% faster training, achieves $2.2\\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.", "AI": {"tldr": "EcoSpa\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u7a00\u758f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8bc4\u4f30\u548c\u7a00\u758f\u5316\u8026\u5408\u6743\u91cd\u77e9\u9635\u5bf9\u6765\u4fdd\u6301Transformer\u4e2d\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u573a\u666f\u4e2d\u5b9e\u73b0\u663e\u8457\u7684\u5185\u5b58\u51cf\u5c11\u3001\u8bad\u7ec3\u52a0\u901f\u548c\u6a21\u578b\u538b\u7f29\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u8bad\u7ec3\u65b9\u6cd5\u672a\u80fd\u4fdd\u6301Transformer\u4e2d\u6743\u91cd\u77e9\u9635\u4e4b\u95f4\u7684\u5173\u952e\u7ed3\u6784\u5173\u7cfb\uff0c\u5bfc\u81f4\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u7559\u6743\u91cd\u77e9\u9635\u4ea4\u4e92\u6a21\u5f0f\u7684\u9ad8\u6548\u7a00\u758f\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "EcoSpa\u901a\u8fc7\u8054\u5408\u8bc4\u4f30\u548c\u7a00\u758f\u5316\u8026\u5408\u6743\u91cd\u77e9\u9635\u5bf9\uff0c\u91c7\u7528\u5bf9\u9f50\u7684\u884c/\u5217\u79fb\u9664\u6765\u4fdd\u6301\u5b83\u4eec\u7684\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5f15\u5165\u65b0\u7684\u7c92\u5ea6\u6765\u6821\u51c6\u7ed3\u6784\u7ec4\u4ef6\u91cd\u8981\u6027\uff0c\u5e76\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u573a\u666f\u4e2d\u6267\u884c\u8026\u5408\u4f30\u8ba1\u548c\u7a00\u758f\u5316\u3002", "result": "EcoSpa\u5728LLaMA-1B\u4e0a\u5b9e\u73b050%\u5185\u5b58\u51cf\u5c11\u548c21%\u8bad\u7ec3\u52a0\u901f\uff0c\u5728GPT-2-Medium\u4e0a\u8fbe\u52302.2\u500d\u6a21\u578b\u538b\u7f29\u548c2.4\u500d\u66f4\u4f4e\u56f0\u60d1\u5ea6\uff0c\u63d0\u4f9b1.6\u500d\u63a8\u7406\u52a0\u901f\uff0c\u4ec5\u4f7f\u7528\u6807\u51c6PyTorch\u64cd\u4f5c\u65e0\u9700\u5b9a\u5236\u786c\u4ef6\u3002", "conclusion": "EcoSpa\u901a\u8fc7\u4fdd\u7559\u6743\u91cd\u77e9\u9635\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u4e3aTransformer\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u53ef\u8bbf\u95ee\u7a00\u758f\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.12063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12063", "abs": "https://arxiv.org/abs/2511.12063", "authors": ["Enoch Hyunwook Kang", "Hema Yoganarasimhan"], "title": "Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework", "comment": null, "summary": "Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u68af\u5ea6\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6T-BoN BO\uff0c\u7528\u4e8e\u89e3\u51b3AI\u81ea\u6211\u6539\u8fdb\u4e2d\u7684\u8bc4\u4f30\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5e7f\u544a\u5bf9\u9f50\u7b49\u9700\u8981\u5927\u91cf\u4eba\u5de5\u53cd\u9988\u7684\u5e94\u7528\u573a\u666f\u4e2d\u3002", "motivation": "\u5f53\u524dAI\u81ea\u6211\u6539\u8fdb\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u67e5\u8be2\u6548\u7387\uff0c\u4f46\u5728\u8bb8\u591a\u793e\u4f1a\u5e94\u7528\u4e2d\uff0c\u771f\u6b63\u7684\u74f6\u9888\u662f\u8bc4\u4f30\u6210\u672c\u800c\u975e\u751f\u6210\u6210\u672c\u3002\u4f8b\u5982\u5e7f\u544a\u6548\u679c\u8bc4\u4f30\u9700\u8981\u5927\u91cf\u4eba\u5de5\u53cd\u9988\uff0c\u6210\u672c\u9ad8\u6602\u3002", "method": "\u901a\u8fc7\u8bc1\u660eBest-of-N\u9009\u62e9\u7b56\u7565\u4e0e\u6587\u672c\u68af\u5ea6\u7684\u7ec4\u5408\u80fd\u7edf\u8ba1\u6a21\u62dfUCB\u91c7\u96c6\u51fd\u6570\u7684\u68af\u5ea6\u884c\u4e3a\uff0c\u63d0\u51faT-BoN BO\u6846\u67b6\uff0c\u5c06\u8d1d\u53f6\u65af\u4f18\u5316\u6269\u5c55\u5230\u8bed\u8a00\u9886\u57df\u4ee5\u5b9e\u73b0\u8bc4\u4f30\u6548\u7387\u4f18\u5316\u3002", "result": "\u5728\u81ea\u52a8\u5316\u5e7f\u544a\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0cT-BoN BO\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8bc4\u4f30\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "T-BoN BO\u4e3a\u8bed\u8a00\u7a7a\u95f4\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8bc4\u4f30\u6210\u672c\u9ad8\u6602\u7684AI\u81ea\u6211\u6539\u8fdb\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.12320", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12320", "abs": "https://arxiv.org/abs/2511.12320", "authors": ["Muhidin Mohamed", "Shubhadeep Mukherjee", "Bhavana Baad"], "title": "Impact of UK Postgraduate Student Experiences on Academic Performance in Blended Learning: A Data Analytics Approach", "comment": "25 pages, 5 figures", "summary": "Blended learning has become a dominant educational model in higher education in the UK and worldwide, particularly after the COVID-19 pandemic. This is further enriched with accompanying pedagogical changes, such as strengthened asynchronous learning, and the use of AI (from ChatGPT and all other similar tools that followed) and other technologies to aid learning. While these educational transformations have enabled flexibility in learning and resource access, they have also exposed new challenges on how students can construct successful learning in hybrid learning environments. In this paper, we investigate the interaction between different dimensions of student learning experiences (ranging from perceived acceptance of teaching methods and staff support/feedback to learning pressure and student motivation) and academic achievement within the context of postgraduate blended learning in UK universities. To achieve this, we employed a combination of several data analytics techniques including visualization, statistical tests, regression analysis, and latent profile analysis. Our empirical results (based on a survey of 255 postgraduate students and holistically interpreted via the Community of Inquiry (CoI) framework) demonstrated that learning activities combining teaching and social presences, and tailored academic support through effective feedback are critical elements for successful postgraduate experience in blended learning contexts. Regarding contributions, this research advances the understanding of student success by identifying the various ways demographic, experiential, and psychological factors impact academic outcomes. And in theoretical terms, it contributes to the extension of the CoI framework by integrating the concept of learner heterogeneity and identifying four distinct student profiles based on how they engage in the different CoI presences.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u82f1\u56fd\u5927\u5b66\u7814\u7a76\u751f\u6df7\u5408\u5b66\u4e60\u4e2d\u5b66\u4e60\u4f53\u9a8c\u7ef4\u5ea6\u4e0e\u5b66\u4e1a\u6210\u5c31\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u7ed3\u5408\u6559\u5b66\u548c\u793e\u4f1a\u5b58\u5728\u611f\u7684\u5b66\u4e60\u6d3b\u52a8\u4ee5\u53ca\u901a\u8fc7\u6709\u6548\u53cd\u9988\u63d0\u4f9b\u7684\u5b9a\u5236\u5316\u5b66\u672f\u652f\u6301\u662f\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u6df7\u5408\u5b66\u4e60\u5df2\u6210\u4e3a\u9ad8\u7b49\u6559\u80b2\u7684\u4e3b\u6d41\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5728COVID-19\u5927\u6d41\u884c\u540e\uff0c\u4f46\u5b66\u751f\u5728\u6df7\u5408\u5b66\u4e60\u73af\u5883\u4e2d\u6784\u5efa\u6210\u529f\u5b66\u4e60\u9762\u4e34\u65b0\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u5b66\u4e60\u4f53\u9a8c\u7ef4\u5ea6\u4e0e\u5b66\u4e1a\u6210\u5c31\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u591a\u79cd\u6570\u636e\u5206\u6790\u6280\u672f\uff0c\u5305\u62ec\u53ef\u89c6\u5316\u3001\u7edf\u8ba1\u68c0\u9a8c\u3001\u56de\u5f52\u5206\u6790\u548c\u6f5c\u5728\u5256\u9762\u5206\u6790\uff0c\u57fa\u4e8e\u5bf9255\u540d\u7814\u7a76\u751f\u7684\u8c03\u67e5\u6570\u636e\uff0c\u901a\u8fc7\u63a2\u7a76\u793e\u533a\u6846\u67b6\u8fdb\u884c\u6574\u4f53\u89e3\u91ca\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u6559\u5b66\u548c\u793e\u4f1a\u5b58\u5728\u611f\u7684\u5b66\u4e60\u6d3b\u52a8\u4ee5\u53ca\u901a\u8fc7\u6709\u6548\u53cd\u9988\u63d0\u4f9b\u7684\u5b9a\u5236\u5316\u5b66\u672f\u652f\u6301\u662f\u6df7\u5408\u5b66\u4e60\u80cc\u666f\u4e0b\u7814\u7a76\u751f\u6210\u529f\u4f53\u9a8c\u7684\u5173\u952e\u8981\u7d20\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u8bc6\u522b\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u4f53\u9a8c\u548c\u5fc3\u7406\u56e0\u7d20\u5bf9\u5b66\u4e1a\u6210\u679c\u7684\u5404\u79cd\u5f71\u54cd\u65b9\u5f0f\uff0c\u63a8\u8fdb\u4e86\u5bf9\u5b66\u751f\u6210\u529f\u7684\u7406\u89e3\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u901a\u8fc7\u6574\u5408\u5b66\u4e60\u8005\u5f02\u8d28\u6027\u6982\u5ff5\u548c\u57fa\u4e8e\u4e0d\u540cCoI\u5b58\u5728\u611f\u53c2\u4e0e\u65b9\u5f0f\u8bc6\u522b\u56db\u4e2a\u4e0d\u540c\u5b66\u751f\u6863\u6848\uff0c\u6269\u5c55\u4e86CoI\u6846\u67b6\u3002"}}
{"id": "2511.11890", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11890", "abs": "https://arxiv.org/abs/2511.11890", "authors": ["Camila Machado de Araujo", "Egon P. B. S. Borges", "Ricardo Marcelo Canteiro Grangeiro", "Allan Pinto"], "title": "Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation", "comment": null, "summary": "High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Harpia\u2014\u2014\u4e00\u4e2a\u57fa\u4e8eCUDA\u7684\u5904\u7406\u5e93\uff0c\u96c6\u6210\u5230Annotat3D\u4e2d\uff0c\u7528\u4e8e\u652f\u6301\u5927\u89c4\u6a213D\u6570\u636e\u96c6\u7684\u53ef\u6269\u5c55\u3001\u4ea4\u4e92\u5f0f\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u8fdc\u7a0b\u8bbf\u95ee\u73af\u5883\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u4f53\u79ef\u6210\u50cf\u6280\u672f\uff08\u5982X\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u548c\u5148\u8fdb\u663e\u5fae\u955c\uff09\u751f\u6210\u7684\u6570\u636e\u96c6\u8d8a\u6765\u8d8a\u5927\uff0c\u73b0\u6709\u5de5\u5177\u5728\u5904\u7406\u6548\u7387\u3001\u5206\u5272\u548c\u4ea4\u4e92\u63a2\u7d22\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86Harpia\u5e93\uff0c\u5177\u6709\u4e25\u683c\u5185\u5b58\u63a7\u5236\u3001\u539f\u751f\u5206\u5757\u6267\u884c\u548c\u4e00\u5957GPU\u52a0\u901f\u7684\u8fc7\u6ee4\u3001\u6ce8\u91ca\u548c\u91cf\u5316\u5de5\u5177\uff0c\u652f\u6301\u5728\u8d85\u8fc7\u5355\u4e2aGPU\u5185\u5b58\u5bb9\u91cf\u7684\u6570\u636e\u96c6\u4e0a\u53ef\u9760\u8fd0\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0eNVIDIA cuCIM\u548cscikit-image\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u6846\u67b6\u76f8\u6bd4\uff0c\u5728\u5904\u7406\u901f\u5ea6\u3001\u5185\u5b58\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u4ea4\u4e92\u5f0f\u4eba\u673a\u754c\u9762\u548c\u9ad8\u6548\u7684GPU\u8d44\u6e90\u7ba1\u7406\uff0c\u7279\u522b\u9002\u5408\u5728\u5171\u4eabHPC\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u534f\u4f5c\u79d1\u5b66\u6210\u50cf\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2511.11646", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11646", "abs": "https://arxiv.org/abs/2511.11646", "authors": ["Li Yinxing", "Tsukasa Ishigaki"], "title": "A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products", "comment": "23 pages", "summary": "Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6761\u4ef6\u8868\u683c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CTVAE\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u65b0\u4ea7\u54c1\u7ebf\u6269\u5c55\u5e26\u6765\u7684\u6d88\u8d39\u8005\u5c5e\u6027\u53d8\u5316\uff0c\u5e2e\u52a9\u8425\u9500\u4eba\u5458\u5236\u5b9a\u6709\u6548\u7684\u4ea7\u54c1\u7ebf\u8425\u9500\u7b56\u7565\u3002", "motivation": "\u4ea7\u54c1\u7ebf\u6269\u5c55\u662f\u91cd\u8981\u7684\u8425\u9500\u7b56\u7565\uff0c\u4f46\u8fc7\u5ea6\u6269\u5c55\u4f1a\u7834\u574f\u54c1\u724c\u5f62\u8c61\u3002\u8425\u9500\u4eba\u5458\u9700\u8981\u5728\u8fdb\u5165\u5e02\u573a\u524d\u4e86\u89e3\u65b0\u4ea7\u54c1\u7ebf\u6269\u5c55\u4ea7\u54c1\u7684\u4e3b\u8981\u6d88\u8d39\u8005\u5c5e\u6027\uff0c\u4ee5\u4fbf\u57fa\u4e8e\u6d88\u8d39\u8005\u9700\u6c42\u8fdb\u884c\u9002\u5f53\u7684\u6269\u5c55\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u8868\u683c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CTVAE\uff09\u4ece\u5927\u89c4\u6a21\u7684\u6d88\u8d39\u8005\u548c\u4ea7\u54c1\u8868\u683c\u6570\u636e\u4e2d\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u9884\u6d4b\u65b0\u4ea7\u54c1\u7ebf\u6269\u5c55\u5e26\u6765\u7684\u6d88\u8d39\u8005\u5c5e\u6027\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCTVAE\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u80fd\u591f\u4e3a\u6539\u53d8\u5bb9\u5668\u6216\u53e3\u5473\u7684\u65b0\u4ea7\u54c1\u63d0\u4f9b\u6709\u6548\u7684\u8425\u9500\u542f\u793a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u907f\u514d\u4ea7\u54c1\u81ea\u76f8\u6b8b\u6740\uff0c\u5e76\u4e3a\u4ea7\u54c1\u5f62\u8c61\u8bbe\u8ba1\u548c\u8425\u9500\u7b56\u7565\u5236\u5b9a\u63d0\u4f9b\u652f\u6301\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.12083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12083", "abs": "https://arxiv.org/abs/2511.12083", "authors": ["Yanchang Fu", "Shengda Liu", "Pei Xu", "Kaiqi Huang"], "title": "No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding", "comment": null, "summary": "High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEmbedding CFR\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u4fe1\u606f\u96c6\u5d4c\u5165\u5230\u4f4e\u7ef4\u8fde\u7eed\u7a7a\u95f4\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u4e0d\u5b8c\u5168\u4fe1\u606f\u6269\u5c55\u5f0f\u535a\u5f08\uff0c\u76f8\u6bd4\u57fa\u4e8e\u805a\u7c7b\u7684\u62bd\u8c61\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u6355\u6349\u4fe1\u606f\u96c6\u95f4\u7684\u5dee\u5f02\u548c\u8054\u7cfb\uff0c\u5728\u6251\u514b\u6e38\u620f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u53ef\u5265\u524a\u6027\u6536\u655b\u3002", "motivation": "\u73b0\u6709AI\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u79bb\u6563\u805a\u7c7b\u8fdb\u884c\u62bd\u8c61\uff0c\u4f46\u786c\u5206\u7c7b\u4f1a\u4e0d\u53ef\u9006\u5730\u4e22\u5931\u4fe1\u606f\u96c6\u4e4b\u95f4\u7684\u91cf\u5316\u7ec6\u5fae\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u4e8e\u7b56\u7565\u6c42\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u800c\u5f71\u54cd\u6c42\u89e3\u8d28\u91cf\u3002", "method": "\u53d7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u8bcd\u5d4c\u5165\u8303\u5f0f\u7684\u542f\u53d1\uff0c\u63d0\u51faEmbedding CFR\u7b97\u6cd5\uff1a\u9884\u8bad\u7ec3\u5e76\u5c06\u5b64\u7acb\u4fe1\u606f\u96c6\u7684\u7279\u5f81\u5d4c\u5165\u5230\u76f8\u4e92\u8fde\u63a5\u7684\u4f4e\u7ef4\u8fde\u7eed\u7a7a\u95f4\u4e2d\uff0c\u5728\u8be5\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u9057\u61be\u7d2f\u79ef\u548c\u7b56\u7565\u66f4\u65b0\u7684\u7b56\u7565\u6c42\u89e3\u8fc7\u7a0b\u3002", "result": "\u5728\u6251\u514b\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u7a7a\u95f4\u5f00\u9500\u4e0b\uff0cEmbedding CFR\u76f8\u6bd4\u57fa\u4e8e\u805a\u7c7b\u7684\u62bd\u8c61\u7b97\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u5feb\u7684\u53ef\u5265\u524a\u6027\u6536\u655b\u3002", "conclusion": "\u8fd9\u662f\u6251\u514bAI\u4e2d\u9996\u4e2a\u901a\u8fc7\u4f4e\u7ef4\u5d4c\u5165\u9884\u8bad\u7ec3\u4fe1\u606f\u96c6\u62bd\u8c61\u8fdb\u884c\u7b56\u7565\u6c42\u89e3\u7684\u7b97\u6cd5\uff0c\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u51cf\u5c11\u7d2f\u79ef\u9057\u61be\u7684\u80fd\u529b\u3002"}}
{"id": "2511.12369", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12369", "abs": "https://arxiv.org/abs/2511.12369", "authors": ["Mohamed Amine Kada Zair"], "title": "Cultural Awareness, Stereotypes and Communication Skills in Intercultural Communication: The Algerian Participants Perspective", "comment": "20 pages, 9 tables, preprint", "summary": "This study explores the relationship between cultural awareness, stereotypes, and communication skills among Algerian participants working or studying in multicultural environments. A quantitative questionnaire was administered to 40 respondents to evaluate their levels of cultural awareness, the presence of stereotypical thinking, and the effectiveness of their intercultural communication skills. Results revealed that while cultural awareness was generally high, certain stereotypes still influenced the perception of others and impacted communication efficiency. Participants with higher cultural awareness demonstrated better communication skills and lower levels of stereotyping. These findings underline the importance of intercultural competence and education programs in reducing prejudice and fostering mutual understanding in diverse contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u5de5\u4f5c\u6216\u5b66\u4e60\u7684\u963f\u5c14\u53ca\u5229\u4e9a\u53c2\u4e0e\u8005\u7684\u6587\u5316\u610f\u8bc6\u3001\u523b\u677f\u5370\u8c61\u4e0e\u6c9f\u901a\u6280\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u9ad8\u6587\u5316\u610f\u8bc6\u80fd\u964d\u4f4e\u523b\u677f\u5370\u8c61\u5e76\u63d0\u5347\u6c9f\u901a\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\uff0c\u6587\u5316\u610f\u8bc6\u3001\u523b\u677f\u5370\u8c61\u5982\u4f55\u5f71\u54cd\u8de8\u6587\u5316\u6c9f\u901a\u6280\u80fd\uff0c\u4e3a\u63d0\u5347\u8de8\u6587\u5316\u80fd\u529b\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u5b9a\u91cf\u95ee\u5377\u8c03\u67e5\u6cd5\uff0c\u5bf940\u540d\u5728\u591a\u5143\u6587\u5316\u73af\u5883\u4e2d\u5de5\u4f5c\u6216\u5b66\u4e60\u7684\u963f\u5c14\u53ca\u5229\u4e9a\u53c2\u4e0e\u8005\u8fdb\u884c\u8bc4\u4f30\uff0c\u6d4b\u91cf\u5176\u6587\u5316\u610f\u8bc6\u6c34\u5e73\u3001\u523b\u677f\u601d\u7ef4\u5b58\u5728\u7a0b\u5ea6\u548c\u8de8\u6587\u5316\u6c9f\u901a\u6280\u80fd\u6709\u6548\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6587\u5316\u610f\u8bc6\u666e\u904d\u8f83\u9ad8\uff0c\u4f46\u67d0\u4e9b\u523b\u677f\u5370\u8c61\u4ecd\u5f71\u54cd\u5bf9\u4ed6\u4eba\u7684\u8ba4\u77e5\u548c\u6c9f\u901a\u6548\u7387\uff1b\u6587\u5316\u610f\u8bc6\u8f83\u9ad8\u7684\u53c2\u4e0e\u8005\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6c9f\u901a\u6280\u80fd\u548c\u66f4\u4f4e\u7684\u523b\u677f\u5370\u8c61\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8de8\u6587\u5316\u80fd\u529b\u548c\u6559\u80b2\u9879\u76ee\u5728\u51cf\u5c11\u504f\u89c1\u3001\u4fc3\u8fdb\u591a\u5143\u73af\u5883\u4e2d\u76f8\u4e92\u7406\u89e3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.11898", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11898", "abs": "https://arxiv.org/abs/2511.11898", "authors": ["Arnav Singhvi", "Vasiliki Bikia", "Asad Aali", "Akshay Chaudhari", "Roxana Daneshjou"], "title": "Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks", "comment": null, "summary": "Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528DSPy\u6846\u67b6\u8fdb\u884c\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\uff0c\u5728\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edf\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u76f8\u6bd4\u96f6\u6837\u672c\u63d0\u793a\u57fa\u7ebf\u83b7\u5f9753%\u7684\u4e2d\u4f4d\u6570\u76f8\u5bf9\u6539\u8fdb\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u6a21\u578b\u5fae\u8c03\u9700\u8981\u5927\u91cf\u9886\u57df\u7279\u5b9a\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\u96be\u4ee5\u6cdb\u5316\u4e14\u4e0d\u6613\u90e8\u7f72\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u4e0d\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u63d0\u793a\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002", "method": "\u91c7\u7528DSPy\u6846\u67b6\u8fdb\u884c\u7ed3\u6784\u5316\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff0c\u5728\u653e\u5c04\u5b66\u3001\u80c3\u80a0\u75c5\u5b66\u548c\u76ae\u80a4\u75c5\u5b66\u7684\u4e94\u4e2a\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u5b9e\u73b0\u63d0\u793a\u7ba1\u9053\uff0c\u8bc4\u4f3010\u4e2a\u5f00\u6e90VLM\u548c\u56db\u79cd\u63d0\u793a\u4f18\u5316\u6280\u672f\u3002", "result": "\u4f18\u5316\u540e\u7684\u7ba1\u9053\u5728\u96f6\u6837\u672c\u63d0\u793a\u57fa\u7ebf\u4e0a\u5b9e\u73b0\u4e8653%\u7684\u4e2d\u4f4d\u6570\u76f8\u5bf9\u6539\u8fdb\uff0c\u5728\u96f6\u6837\u672c\u6027\u80fd\u8f83\u4f4e\u7684\u4efb\u52a1\u4e0a\u6700\u5927\u589e\u76ca\u8fbe\u5230300%\u81f33,400%\u3002", "conclusion": "\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\u5728\u533b\u5b66AI\u7cfb\u7edf\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u9700\u8981\u51c6\u786e\u4e34\u5e8a\u56fe\u50cf\u89e3\u91ca\u7684\u89c6\u89c9\u5e94\u7528\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u63d0\u793a\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u8ba9\u4e34\u5e8a\u533b\u751f\u4e13\u6ce8\u4e8e\u60a3\u8005\u62a4\u7406\u548c\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2511.12089", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12089", "abs": "https://arxiv.org/abs/2511.12089", "authors": ["Yanchang Fu", "Qiyue Yin", "Shengda Liu", "Pei Xu", "Kaiqi Huang"], "title": "KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything", "comment": null, "summary": "Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.", "AI": {"tldr": "KrwEmd\u7b97\u6cd5\u901a\u8fc7k-recall winrate\u7279\u5f81\u548cearth mover's distance\u805a\u7c7b\uff0c\u89e3\u51b3\u4e86\u5fb7\u5dde\u6251\u514b\u7b49\u6e38\u620f\u4e2d\u624b\u724c\u62bd\u8c61\u8fc7\u5ea6\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6e38\u620f\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\u4e2d\u624b\u724c\u62bd\u8c61\u8fc7\u5ea6\u7684\u95ee\u9898\uff0c\u8fd9\u79cd\u95ee\u9898\u6e90\u4e8e\u4e0d\u5b8c\u7f8e\u56de\u5fc6\u62bd\u8c61\u7684\u6781\u7aef\u5b9e\u73b0\uff0c\u5b8c\u5168\u4e22\u5f03\u4e86\u5386\u53f2\u4fe1\u606f\uff0c\u635f\u5bb3\u4e86AI\u6027\u80fd\u3002", "method": "\u5f15\u5165k-recall winrate\u7279\u5f81\u6765\u533a\u5206\u4fe1\u53f7\u89c2\u5bdf\u4fe1\u606f\u96c6\uff0c\u5229\u7528\u672a\u6765\u548c\u5173\u952e\u7684\u5386\u53f2\u6e38\u620f\u4fe1\u606f\uff1b\u5f00\u53d1KrwEmd\u7b97\u6cd5\uff0c\u4f7f\u7528earth mover's distance\u6d4b\u91cf\u7279\u5f81\u5dee\u5f02\u6765\u805a\u7c7b\u4fe1\u53f7\u89c2\u5bdf\u4fe1\u606f\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7b97\u6cd5\u76f8\u6bd4\uff0cKrwEmd\u663e\u8457\u63d0\u9ad8\u4e86AI\u6e38\u620f\u6027\u80fd\u3002", "conclusion": "KrwEmd\u662f\u7b2c\u4e00\u4e2a\u89e3\u51b3\u624b\u724c\u62bd\u8c61\u8fc7\u5ea6\u95ee\u9898\u7684\u5b9e\u7528\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5386\u53f2\u548c\u672a\u6765\u4fe1\u606f\u6765\u6539\u8fdb\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\u4e2d\u7684AI\u8868\u73b0\u3002"}}
{"id": "2511.12426", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12426", "abs": "https://arxiv.org/abs/2511.12426", "authors": ["Stefano Civelli", "Pietro Bernardelle", "Frank Mols", "Gianluca Demartini"], "title": "Political Advertising on Facebook During the 2022 Australian Federal Election: A Social Identity Perspective", "comment": null, "summary": "The spread of targeted advertising on social media platforms has revolutionized political marketing strategies. Monitoring these digital campaigns is essential for maintaining transparency and accountability in democratic processes. Leveraging Meta's Ad Library, we analyze political advertising on Facebook and Instagram during the 2022 Australian federal election campaign. We investigate temporal, demographic, and geographical patterns in the advertising strategies of major Australian political actors to establish an empirical evidence base, and interpret these findings through the lens of Social Identity Theory (SIT). Our findings not only reveal significant disparities in spending and reach among parties, but also in persuasion strategies being deployed in targeted online campaigns. We observe a marked increase in advertising activity as the election approached, peaking just before the mandated media blackout period. Demographic analysis shows distinct targeting strategies, with parties focusing more on younger demographics and exhibiting gender-based differences in ad impressions. Regional distribution of ads largely mirrored population densities, with some parties employing more targeted approaches in specific states. Moreover, we found that parties emphasized different themes aligned with their ideologies-major parties focused on party names and opponents, while smaller parties emphasized issue-specific messages. Drawing on SIT, we interpret these findings within Australia's compulsory voting context, suggesting that parties employed distinct persuasion strategies. With turnout guaranteed, major parties focused on reinforcing partisan identities to prevent voter defection, while smaller parties cultivated issue-based identities to capture the support of disaffected voters who are obligated to participate.", "AI": {"tldr": "\u57fa\u4e8eMeta\u5e7f\u544a\u5e93\u5206\u67902022\u5e74\u6fb3\u5927\u5229\u4e9a\u8054\u90a6\u9009\u4e3e\u671f\u95f4Facebook\u548cInstagram\u653f\u6cbb\u5e7f\u544a\uff0c\u53d1\u73b0\u4e3b\u8981\u653f\u515a\u5728\u652f\u51fa\u3001\u8986\u76d6\u8303\u56f4\u548c\u8bf4\u670d\u7b56\u7565\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e7f\u544a\u6d3b\u52a8\u5728\u9009\u4e3e\u524d\u8fbe\u5230\u9ad8\u5cf0\uff0c\u5404\u653f\u515a\u9488\u5bf9\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u91c7\u7528\u5dee\u5f02\u5316\u7b56\u7565\u3002", "motivation": "\u76d1\u63a7\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u7684\u653f\u6cbb\u5e7f\u544a\u5bf9\u4e8e\u7ef4\u62a4\u6c11\u4e3b\u8fdb\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5efa\u7acb\u5b9e\u8bc1\u8bc1\u636e\u57fa\u7840\u6765\u5206\u6790\u6570\u5b57\u653f\u6cbb\u8425\u9500\u7b56\u7565\u3002", "method": "\u5229\u7528Meta\u5e7f\u544a\u5e93\u5206\u67902022\u5e74\u6fb3\u5927\u5229\u4e9a\u8054\u90a6\u9009\u4e3e\u671f\u95f4\u7684\u653f\u6cbb\u5e7f\u544a\uff0c\u4ece\u65f6\u95f4\u3001\u4eba\u53e3\u7edf\u8ba1\u548c\u5730\u7406\u7ef4\u5ea6\u8003\u5bdf\u4e3b\u8981\u653f\u6cbb\u884c\u4e3a\u4f53\u7684\u5e7f\u544a\u7b56\u7565\uff0c\u5e76\u8fd0\u7528\u793e\u4f1a\u8ba4\u540c\u7406\u8bba(SIT)\u8fdb\u884c\u89e3\u91ca\u3002", "result": "\u53d1\u73b0\u5e7f\u544a\u6d3b\u52a8\u5728\u9009\u4e3e\u524d\u663e\u8457\u589e\u52a0\u5e76\u5728\u5a92\u4f53\u9759\u9ed8\u671f\u524d\u8fbe\u5230\u9ad8\u5cf0\uff1b\u5404\u653f\u515a\u9488\u5bf9\u5e74\u8f7b\u4eba\u7fa4\u91c7\u7528\u5dee\u5f02\u5316\u7b56\u7565\uff0c\u5e7f\u544a\u5370\u8c61\u5b58\u5728\u6027\u522b\u5dee\u5f02\uff1b\u5e7f\u544a\u533a\u57df\u5206\u5e03\u4e0e\u4eba\u53e3\u5bc6\u5ea6\u57fa\u672c\u4e00\u81f4\uff1b\u4e3b\u8981\u653f\u515a\u5f3a\u8c03\u515a\u6d3e\u540d\u79f0\u548c\u5bf9\u624b\uff0c\u800c\u5c0f\u515a\u6d3e\u5219\u5173\u6ce8\u5177\u4f53\u8bae\u9898\u3002", "conclusion": "\u5728\u6fb3\u5927\u5229\u4e9a\u5f3a\u5236\u6295\u7968\u5236\u5ea6\u4e0b\uff0c\u4e3b\u8981\u653f\u515a\u901a\u8fc7\u5f3a\u5316\u515a\u6d3e\u8ba4\u540c\u6765\u9632\u6b62\u9009\u6c11\u6d41\u5931\uff0c\u800c\u5c0f\u515a\u6d3e\u5219\u901a\u8fc7\u57f9\u80b2\u8bae\u9898\u8ba4\u540c\u6765\u5438\u5f15\u4e0d\u6ee1\u9009\u6c11\u7684\u652f\u6301\uff0c\u8fd9\u4f53\u73b0\u4e86\u4e0d\u540c\u7684\u8bf4\u670d\u7b56\u7565\u3002"}}
{"id": "2511.11648", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11648", "abs": "https://arxiv.org/abs/2511.11648", "authors": ["Shunyu Wu", "Tianyue Li", "Yixuan Leng", "Jingyi Suo", "Jian Lou", "Dan Li", "See-Kiong Ng"], "title": "Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning", "comment": null, "summary": "Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.", "AI": {"tldr": "LTSV\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4f30\u503c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e0a\u8fdb\u884c\u4e0a\u4e0b\u6587\u5fae\u8c03\u6765\u4f30\u8ba1\u6837\u672c\u8d21\u732e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u65f6\u5e8f\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\u8d28\u91cf\uff0c\u4f46\u4f20\u7edf\u6570\u636e\u4f30\u503c\u65b9\u6cd5\uff08\u5982\u5f71\u54cd\u51fd\u6570\uff09\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\u4e14\u96be\u4ee5\u4fdd\u6301\u65f6\u5e8f\u4f9d\u8d56\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u4f30\u503c\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLTSV\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5fae\u8c03\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\u7684\u7406\u8bba\u8bc1\u636e\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e0a\u4e0b\u6587\u5fae\u8c03\u540e\u7684\u635f\u5931\u53d8\u5316\u6765\u4f30\u8ba1\u6837\u672c\u8d21\u732e\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u5757\u805a\u5408\u6765\u6355\u6349\u65f6\u5e8f\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLTSV\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u4e14\u5f3a\u5927\u7684\u4f30\u503c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u63a7\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "\u5728\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e0a\u8fdb\u884c\u4e0a\u4e0b\u6587\u5fae\u8c03\u4e3a\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f52\u56e0\u548c\u6a21\u578b\u6cdb\u5316\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5b9e\u7528\u6709\u6548\u7684\u6865\u6881\u3002"}}
{"id": "2511.12449", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12449", "abs": "https://arxiv.org/abs/2511.12449", "authors": ["Zhanheng Nie", "Chenghan Fu", "Daoze Zhang", "Junxian Wu", "Wanxian Guan", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding", "comment": "11 pages, 7 figures", "summary": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.", "AI": {"tldr": "MOON2.0\u662f\u4e00\u4e2a\u52a8\u6001\u6a21\u6001\u5e73\u8861\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u5546\u4ea7\u54c1\u7406\u89e3\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u3001\u5185\u5728\u5bf9\u9f50\u5173\u7cfb\u5229\u7528\u4e0d\u8db3\u548c\u6570\u636e\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u6001\u9a71\u52a8\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u3001\u53cc\u7ea7\u5bf9\u9f50\u65b9\u6cd5\u548c\u57fa\u4e8eMLLM\u7684\u56fe\u50cf\u6587\u672c\u534f\u540c\u589e\u5f3a\u7b56\u7565\u5b9e\u73b0\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u7535\u5546\u591a\u6a21\u6001\u6a21\u578b\u9762\u4e34\u7684\u4e09\u4e2a\u6311\u6218\uff1a\u6a21\u6001\u6df7\u5408\u8bad\u7ec3\u5bfc\u81f4\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u3001\u4ea7\u54c1\u5185\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u5185\u5728\u5bf9\u9f50\u5173\u7cfb\u5229\u7528\u4e0d\u8db3\uff0c\u4ee5\u53ca\u7535\u5546\u591a\u6a21\u6001\u6570\u636e\u566a\u58f0\u5904\u7406\u6709\u9650\u3002", "method": "\u63d0\u51faMOON2.0\u6846\u67b6\uff0c\u5305\u542b\uff1a(1) \u6a21\u6001\u9a71\u52a8\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u6839\u636e\u6a21\u6001\u7ec4\u6210\u81ea\u9002\u5e94\u5904\u7406\u8f93\u5165\u6837\u672c\uff1b(2) \u53cc\u7ea7\u5bf9\u9f50\u65b9\u6cd5\uff0c\u66f4\u597d\u5229\u7528\u4ea7\u54c1\u5185\u8bed\u4e49\u5bf9\u9f50\u7279\u6027\uff1b(3) \u57fa\u4e8eMLLM\u7684\u56fe\u50cf\u6587\u672c\u534f\u540c\u589e\u5f3a\u7b56\u7565\uff0c\u7ed3\u5408\u52a8\u6001\u6837\u672c\u8fc7\u6ee4\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5728MBE2.0\u57fa\u51c6\u548c\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u6ce8\u610f\u529b\u70ed\u56fe\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u6539\u8fdb\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u5b9a\u6027\u8bc1\u636e\u3002", "conclusion": "MOON2.0\u901a\u8fc7\u52a8\u6001\u6a21\u6001\u5e73\u8861\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u7535\u5546\u4ea7\u54c1\u7406\u89e3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12113", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12113", "abs": "https://arxiv.org/abs/2511.12113", "authors": ["Lanxue Zhang", "Yuqiang Xie", "Fang Fang", "Fanglong Dong", "Rui Liu", "Yanan Cao"], "title": "MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization", "comment": "23 pages, 10 figures, AAAI 2026", "summary": "Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5c0f\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u7684\u7efc\u5408\u65b9\u6848\uff0c\u5305\u62ec\u6784\u5efa\u5305\u542b\u5143\u8ba4\u77e5\u77e5\u8bc6\u7684\u6570\u636e\u96c6\u548c\u5f15\u5165GDPO\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u5fae\u8c03\u65b9\u6cd5\u5728\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u538b\u7f29\u5230\u5c0f\u6a21\u578b\u65f6\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e8B\u4ee5\u4e0b\u7684\u5c0f\u6a21\u578b\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6570\u636e\u96c6\u5ffd\u7565\u8bad\u7ec3\u6570\u636e\u77e5\u8bc6\u4e0e\u6a21\u578b\u56fa\u6709\u80fd\u529b\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u4f20\u7edf\u8bad\u7ec3\u76ee\u6807\u672a\u80fd\u7ea6\u675f\u56fa\u6709\u77e5\u8bc6\u4fdd\u7559\u3002", "method": "1. \u6570\u636e\u5c42\u9762\uff1a\u6784\u5efa\u5305\u542b5K\u5b9e\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d6\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u5e76\u878d\u5165\u5143\u8ba4\u77e5\u77e5\u8bc6\uff0c\u57fa\u4e8e\u4efb\u52a1\u77e5\u8bc6\u548c\u6a21\u578b\u56fa\u6709\u6280\u80fd\u8fc7\u6ee4\u6570\u636e\uff1b2. \u8bad\u7ec3\u5c42\u9762\uff1a\u63d0\u51faGDPO\uff08\u7ec4\u65b9\u5411\u504f\u597d\u4f18\u5316\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u6a21\u578b\u9690\u5f0f\u7ea6\u675f\u4f18\u5316\u8def\u5f84\uff0c\u66f4\u9002\u5408\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u4ece\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e24\u4e2a\u89d2\u5ea6\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u548c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2511.12686", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12686", "abs": "https://arxiv.org/abs/2511.12686", "authors": ["Stefano Bianchini", "Aldo Geuna", "Fazliddin Shermatov"], "title": "AI and Supercomputing are Powering the Next Wave of Breakthrough Science - But at What Cost?", "comment": null, "summary": "Artificial intelligence (AI) and high-performance computing (HPC) are rapidly becoming the engines of modern science. However, their joint effect on discovery has yet to be quantified at scale. Drawing on metadata from over five million scientific publications (2000-2024), we identify how AI and HPC interact to shape research outcomes across 27 fields. Papers combining the two technologies are up to three times more likely to introduce novel concepts and five times more likely to reach top-cited status than conventional work. This convergence of AI and HPC is redefining the frontier of scientific creativity but also deepening global inequalities in access to computational power and expertise. Our findings suggest that the future of discovery will depend not only on algorithms and compute, but also on how equitably the world shares these transformative tools.", "AI": {"tldr": "\u57fa\u4e8e500\u4e07\u7bc7\u79d1\u5b66\u8bba\u6587\u7684\u5206\u6790\u663e\u793a\uff0cAI\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u7ed3\u5408\u4f7f\u8bba\u6587\u5f15\u5165\u65b0\u6982\u5ff5\u7684\u53ef\u80fd\u6027\u63d0\u9ad83\u500d\uff0c\u8fdb\u5165\u9ad8\u88ab\u5f15\u8bba\u6587\u7684\u53ef\u80fd\u6027\u63d0\u9ad85\u500d\uff0c\u4f46\u540c\u65f6\u4e5f\u52a0\u5267\u4e86\u5168\u7403\u8ba1\u7b97\u8d44\u6e90\u548c\u4e13\u4e1a\u77e5\u8bc6\u7684\u83b7\u53d6\u4e0d\u5e73\u7b49\u3002", "motivation": "\u91cf\u5316\u4eba\u5de5\u667a\u80fd\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u5bf9\u79d1\u5b66\u53d1\u73b0\u7684\u8054\u5408\u5f71\u54cd\uff0c\u63a2\u7d22\u8fd9\u4e24\u79cd\u6280\u672f\u5982\u4f55\u5171\u540c\u5851\u9020\u7814\u7a76\u4ea7\u51fa\u3002", "method": "\u5206\u67902000-2024\u5e74\u95f4\u8d85\u8fc7500\u4e07\u7bc7\u79d1\u5b66\u8bba\u6587\u7684\u5143\u6570\u636e\uff0c\u7814\u7a76AI\u548cHPC\u572827\u4e2a\u9886\u57df\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u7ed3\u5408AI\u548cHPC\u7684\u8bba\u6587\u6bd4\u4f20\u7edf\u7814\u7a76\u5f15\u5165\u65b0\u6982\u5ff5\u7684\u53ef\u80fd\u6027\u9ad83\u500d\uff0c\u6210\u4e3a\u9ad8\u88ab\u5f15\u8bba\u6587\u7684\u53ef\u80fd\u6027\u9ad85\u500d\uff1b\u8fd9\u79cd\u878d\u5408\u6b63\u5728\u91cd\u65b0\u5b9a\u4e49\u79d1\u5b66\u521b\u9020\u529b\u7684\u524d\u6cbf\uff0c\u4f46\u4e5f\u52a0\u6df1\u4e86\u5168\u7403\u5728\u8ba1\u7b97\u80fd\u529b\u548c\u4e13\u4e1a\u77e5\u8bc6\u83b7\u53d6\u65b9\u9762\u7684\u4e0d\u5e73\u7b49\u3002", "conclusion": "\u79d1\u5b66\u53d1\u73b0\u7684\u672a\u6765\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u7b97\u6cd5\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u8fd8\u53d6\u51b3\u4e8e\u4e16\u754c\u5982\u4f55\u516c\u5e73\u5730\u5206\u4eab\u8fd9\u4e9b\u53d8\u9769\u6027\u5de5\u5177\u3002"}}
{"id": "2511.11910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11910", "abs": "https://arxiv.org/abs/2511.11910", "authors": ["Siyou Li", "Huanan Wu", "Juexi Shao", "Yinghao Ma", "Yujian Gan", "Yihao Luo", "Yuwei Wang", "Dong Nie", "Lu Wang", "Wengqing Wu", "Le Zhang", "Massimo Poesio", "Juntao Yu"], "title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models", "comment": null, "summary": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.\n  We will make all code, data, and trained models' weights publicly available.", "AI": {"tldr": "QTSplus\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u89c6\u89c9token\u9009\u62e9\u6a21\u5757\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e0e\u6587\u672c\u67e5\u8be2\u6700\u76f8\u5173\u7684\u89c6\u89c9\u8bc1\u636e\u6765\u89e3\u51b3\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u538b\u7f29\u89c6\u89c9\u6d41\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u9762\u4e34\u7684\u89c6\u89c9token\u6570\u91cf\u968f\u89c6\u9891\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u5bfc\u81f4\u7684\u6ce8\u610f\u529b\u6210\u672c\u3001\u5185\u5b58\u548c\u5ef6\u8fdf\u7206\u70b8\u95ee\u9898\u3002", "method": "\u63d0\u51faQTSplus\u6a21\u5757\uff1a(1)\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u89c6\u89c9token\u8bc4\u5206\uff1b(2)\u57fa\u4e8e\u67e5\u8be2\u590d\u6742\u5ea6\u9884\u6d4b\u5b9e\u4f8b\u7279\u5b9a\u7684\u4fdd\u7559\u9884\u7b97\uff1b(3)\u8bad\u7ec3\u65f6\u4f7f\u7528\u53ef\u5fae\u5206\u76f4\u901a\u4f30\u8ba1\u5668\u9009\u62e9Top-n token\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u786c\u95e8\u63a7\uff1b(4)\u4f7f\u7528\u5c0f\u578b\u91cd\u65b0\u7f16\u7801\u5668\u4fdd\u6301\u65f6\u95f4\u987a\u5e8f\u3002", "result": "\u5728Qwen2.5-VL\u4e2d\u96c6\u6210QTSplus\uff0c\u89c6\u89c9\u6d41\u538b\u7f29\u9ad8\u8fbe89%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e28%\uff0c\u5728\u516b\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u5728TempCompass\u65b9\u5411\u51c6\u786e\u7387\u548c\u987a\u5e8f\u51c6\u786e\u7387\u4e0a\u5206\u522b\u6bd4\u539f\u59cb\u6a21\u578b\u63d0\u534720.5\u548c5.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "QTSplus\u662f\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u673a\u5236\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u8bc1\u636e\u7684\u540c\u65f6\u5c06MLLM\u6269\u5c55\u5230\u771f\u5b9e\u4e16\u754c\u7684\u957f\u89c6\u9891\u573a\u666f\u3002"}}
{"id": "2511.11650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11650", "abs": "https://arxiv.org/abs/2511.11650", "authors": ["Daniele Ugo Leonzio", "Paolo Bestagini", "Marco Marcon", "Stefano Tubaro"], "title": "Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine", "comment": null, "summary": "Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6c34\u538b\u6d4b\u91cf\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u4f9b\u6c34\u7ba1\u7f51\u6cc4\u6f0f\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\u5728\u65e0\u6cc4\u6f0f\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5c06\u6cc4\u6f0f\u68c0\u6d4b\u4e3a\u5f02\u5e38\u3002", "motivation": "\u4f9b\u6c34\u7ba1\u7f51\u6bcf\u5e74\u56e0\u6cc4\u6f0f\u635f\u5931\u5927\u91cf\u6c34\u8d44\u6e90\uff0c\u9700\u8981\u53ef\u9760\u6709\u6548\u7684\u6cc4\u6f0f\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u56e0\u5176\u4f18\u8d8a\u6027\u80fd\u800c\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u57fa\u4e8e\u4f9b\u6c34\u7ba1\u7f51\u62d3\u6251\u77e5\u8bc6\u548c\u65e0\u6cc4\u6f0f\u538b\u529b\u6570\u636e\uff0c\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\u5728\u65e0\u6cc4\u6f0f\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5c06\u6cc4\u6f0f\u68c0\u6d4b\u4e3a\u5f02\u5e38\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Modena\u4f9b\u6c34\u7ba1\u7f51\u7684\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u8fd1\u7684\u6cc4\u6f0f\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u6cc4\u6f0f\u68c0\u6d4b\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4f9b\u6c34\u7ba1\u7f51\u6cc4\u6f0f\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12822", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12822", "abs": "https://arxiv.org/abs/2511.12822", "authors": ["Euzeli C. dos Santos", "Tracey Birdwell"], "title": "The Unspoken Crisis of Learning: The Surging Zone of No Development", "comment": "6 pages, 3 figures", "summary": "AI has redefined the boundaries of assistance in education, often blurring the line between guided learning and dependency. This paper revisits Vygotsky's Zone of Proximal Development (ZPD) through the lens of the P2P Teaching framework. By contrasting temporary scaffolding with the emerging phenomenon of permanent digital mediation, the study introduces the concept of the Zone of No Development (ZND), a state in which continuous assistance replaces cognitive struggle and impedes intellectual autonomy. Through theoretical synthesis and framework design, P2P Teaching demonstrates how deliberate disconnection and ethical fading can restore the learner's agency, ensuring that technological tools enhance rather than replace developmental effort. The paper argues that productive struggle, self-regulation, and first-principles reasoning remain essential for durable learning, and that responsible use of AI in education must include explicit mechanisms to end its help when mastery begins.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7P2P\u6559\u5b66\u6846\u67b6\u91cd\u65b0\u5ba1\u89c6\u7ef4\u679c\u8328\u57fa\u7684\u6700\u8fd1\u53d1\u5c55\u533a\u7406\u8bba\uff0c\u63d0\u51fa\"\u65e0\u53d1\u5c55\u533a\"\u6982\u5ff5\uff0c\u5f3a\u8c03AI\u6559\u80b2\u4e2d\u9700\u8981\u907f\u514d\u6c38\u4e45\u6027\u6570\u5b57\u4e2d\u4ecb\uff0c\u6062\u590d\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u81ea\u4e3b\u6027\u3002", "motivation": "AI\u5728\u6559\u80b2\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6a21\u7cca\u4e86\u5f15\u5bfc\u5b66\u4e60\u4e0e\u4f9d\u8d56\u4e4b\u95f4\u7684\u754c\u9650\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u786e\u4fdd\u6280\u672f\u5de5\u5177\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u52aa\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u7efc\u5408\u548c\u6846\u67b6\u8bbe\u8ba1\uff0c\u91c7\u7528P2P\u6559\u5b66\u65b9\u6cd5\uff0c\u5bf9\u6bd4\u4e34\u65f6\u6027\u652f\u67b6\u4e0e\u6c38\u4e45\u6027\u6570\u5b57\u4e2d\u4ecb\uff0c\u5f15\u5165\u6709\u610f\u8bc6\u65ad\u5f00\u548c\u4f26\u7406\u6de1\u51fa\u673a\u5236\u3002", "result": "\u63d0\u51fa\u4e86Zone of No Development\u6982\u5ff5\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7P2P\u6559\u5b66\u6062\u590d\u5b66\u4e60\u8005\u80fd\u52a8\u6027\uff0c\u786e\u4fdd\u6280\u672f\u5de5\u5177\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u53d1\u5c55\u52aa\u529b\u3002", "conclusion": "\u751f\u4ea7\u6027\u6323\u624e\u3001\u81ea\u6211\u8c03\u8282\u548c\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u7406\u5bf9\u6301\u4e45\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u8d1f\u8d23\u4efb\u7684\u6559\u80b2AI\u4f7f\u7528\u5fc5\u987b\u5305\u542b\u660e\u786e\u673a\u5236\uff0c\u5728\u638c\u63e1\u5f00\u59cb\u65f6\u7ec8\u6b62\u5e2e\u52a9\u3002"}}
{"id": "2511.11944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11944", "abs": "https://arxiv.org/abs/2511.11944", "authors": ["Ling Wang", "Yunfan Lu", "Wenzong Ma", "Huizai Yao", "Pengteng Li", "Hui Xiong"], "title": "From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing", "comment": "11 pages, 8 figures. Completed in April 2025", "summary": "Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \\textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \\textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u8fdb\u884c\u56fe\u50cf\u53bb\u96fe\uff0c\u63d0\u51fa\u4e8b\u4ef6\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u4e8b\u4ef6\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u4fe1\u606f\u878d\u5165\u6269\u6563\u6a21\u578b\u6765\u91cd\u5efa\u6e05\u6670\u56fe\u50cf\uff0c\u5728\u771f\u5b9e\u96fe\u973e\u573a\u666f\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eRGB\u56fe\u50cf\u7684\u53bb\u96fe\u65b9\u6cd5\u53d7\u9650\u4e8e\u52a8\u6001\u8303\u56f4\u4e0d\u8db3\uff0c\u5bb9\u6613\u4e22\u5931\u7ed3\u6784\u548c\u5149\u7167\u7ec6\u8282\u3002\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u5fae\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u9002\u5408\u96fe\u973e\u573a\u666f\uff0c\u4f46\u7f3a\u4e4f\u771f\u5b9e\u914d\u5bf9\u6570\u636e\u4f7f\u5f97HDR\u4fe1\u606f\u8f6c\u79fb\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e8b\u4ef6\u5f15\u5bfc\u6a21\u5757\u5c06\u7a00\u758fHDR\u4e8b\u4ef6\u7279\u5f81\u6620\u5c04\u5230\u6269\u6563\u6f5c\u5728\u7a7a\u95f4\uff0c\u4e3a\u751f\u6210\u8fc7\u7a0b\u63d0\u4f9b\u7cbe\u786e\u7684\u7ed3\u6784\u6307\u5bfc\u3002", "result": "\u5728\u4e24\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u81ea\u5efa\u7684\u91cd\u96fe\u973e\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53bb\u96fe\u6548\u679c\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u7ed3\u5408\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u89e3\u51b3\u96fe\u973e\u573a\u666f\u4e0b\u7684\u56fe\u50cf\u53bb\u96fe\u95ee\u9898\uff0c\u901a\u8fc7HDR\u4fe1\u606f\u8f6c\u79fb\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2511.11651", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11651", "abs": "https://arxiv.org/abs/2511.11651", "authors": ["Zhijian Gong", "Wenjia Dong", "Xueyuan Xu", "Fulin Wei", "Chunyu Liu", "Li Zhuo"], "title": "Incomplete Depression Feature Selection with Missing EEG Channels", "comment": null, "summary": "As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIDFS-MEC\u7684\u65b0\u578b\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406EEG\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u901a\u9053\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5168\u5c40\u5197\u4f59\u6700\u5c0f\u5316\u6765\u51cf\u5c11\u7279\u5f81\u5b50\u96c6\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\uff0c\u5728\u6291\u90c1\u75c7\u5206\u6790\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e10\u79cd\u6d41\u884c\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "EEG\u7279\u5f81\u901a\u5e38\u5305\u542b\u5197\u4f59\u3001\u4e0d\u76f8\u5173\u548c\u566a\u58f0\u4fe1\u606f\uff0c\u4e14\u73b0\u5b9e\u4e16\u754cEEG\u6570\u636e\u91c7\u96c6\u7ecf\u5e38\u9762\u4e34\u7535\u6781\u8131\u843d\u5bfc\u81f4\u6570\u636e\u4e22\u5931\u548c\u5f3a\u566a\u58f0\u5e72\u6270\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u6291\u90c1\u75c7\u5206\u6790\u65b9\u6cd5\u3002", "method": "IDFS-MEC\u65b9\u6cd5\u6574\u5408\u4e86\u7f3a\u5931\u901a\u9053\u6307\u793a\u4fe1\u606f\u548c\u81ea\u9002\u5e94\u901a\u9053\u6743\u91cd\u5b66\u4e60\u5230\u6b63\u4ea4\u56de\u5f52\u4e2d\uff0c\u4ee5\u51cf\u8f7b\u4e0d\u5b8c\u6574\u901a\u9053\u5bf9\u6a21\u578b\u6784\u5efa\u7684\u5f71\u54cd\uff0c\u7136\u540e\u5229\u7528\u5168\u5c40\u5197\u4f59\u6700\u5c0f\u5316\u5b66\u4e60\u6765\u51cf\u5c11\u6240\u9009\u7279\u5f81\u5b50\u96c6\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u3002", "result": "\u5728MODMA\u548cPRED-d003\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIDFS-MEC\u9009\u62e9\u7684EEG\u7279\u5f81\u5b50\u96c6\u57283\u901a\u9053\u300164\u901a\u9053\u548c128\u901a\u9053\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e10\u79cd\u6d41\u884c\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u3002", "conclusion": "IDFS-MEC\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406EEG\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u901a\u9053\u95ee\u9898\uff0c\u5e76\u9009\u62e9\u51fa\u6027\u80fd\u4f18\u8d8a\u7684\u7279\u5f81\u5b50\u96c6\uff0c\u4e3a\u6291\u90c1\u75c7\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13063", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13063", "abs": "https://arxiv.org/abs/2511.13063", "authors": ["Zhenghua Li", "Hang Chen", "Zihao Sun", "Kai Li", "Xiaolin Hu"], "title": "FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation", "comment": null, "summary": "Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u7684Segment Anything 2\u6a21\u578b\u77e5\u8bc6\u8fc1\u79fb\u5230\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u795e\u7ecf\u5143\u5206\u5272\u4efb\u52a1\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\u548c\u53cc\u4eb2\u548c\u5ea6\u89e3\u7801\u5668\uff0c\u5728\u51bb\u7ed3SAM2\u6743\u91cd\u65f6\u8fbe\u5230SOTA\u6c34\u5e73\uff0c\u5fae\u8c03\u540e\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u795e\u7ecf\u5143\u5206\u5272\u9762\u4e34\u590d\u6742\u5f62\u6001\u3001\u4f4e\u4fe1\u566a\u6bd4\u548c\u6807\u6ce8\u7a00\u7f3a\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528\u5728\u5927\u91cf\u81ea\u7136\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u6846\u67b6\uff1a1\uff09\u4f7f\u7528SAM2\u63d0\u53d6\u901a\u7528\u7279\u5f81\uff1b2\uff09\u5f15\u5165\u7279\u5f81\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5229\u7528SAM2\u8bed\u4e49\u7ebf\u7d22\u6307\u5bfc\u8f7b\u91cf\u7ea7\u7cbe\u7ec6\u7f16\u7801\u5668\u5173\u6ce8\u56f0\u96be\u533a\u57df\uff1b3\uff09\u53cc\u4eb2\u548c\u5ea6\u89e3\u7801\u5668\u751f\u6210\u7c97\u7c92\u5ea6\u548c\u7cbe\u70bc\u7684\u4eb2\u548c\u5ea6\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u51bb\u7ed3SAM2\u6743\u91cd\u65f6\uff0c\u65b9\u6cd5\u6027\u80fd\u4e0eSOTA\u65b9\u6cd5\u76f8\u5f53\uff1b\u5728EM\u6570\u636e\u4e0a\u8fdb\u4e00\u6b65\u5fae\u8c03\u540e\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u8868\u5f81\u4e0e\u9488\u5bf9\u6027\u7684\u9886\u57df\u81ea\u9002\u5e94\u6307\u5bfc\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u795e\u7ecf\u5143\u5206\u5272\u4e2d\u7684\u7279\u5b9a\u6311\u6218\u3002"}}
{"id": "2511.12169", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12169", "abs": "https://arxiv.org/abs/2511.12169", "authors": ["Kaiyue Zhao", "Dingqi Chen", "Shaoyu Wang", "Pan Hu"], "title": "Incremental Maintenance of DatalogMTL Materialisations", "comment": "Accepted as oral paper at the main track of AAAI 2026", "summary": "DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u4e86DRedMTL\u7b97\u6cd5\uff0c\u4e00\u79cd\u7528\u4e8e\u5e26\u8fb9\u754c\u533a\u95f4\u7684DatalogMTL\u7684\u589e\u91cf\u63a8\u7406\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u91cd\u65b0\u7269\u5316\u65b9\u6cd5", "motivation": "\u73b0\u6709DatalogMTL\u63a8\u7406\u65b9\u6cd5\u4e0d\u652f\u6301\u9ad8\u6548\u7684\u52a8\u6001\u66f4\u65b0\uff0c\u800c\u73b0\u5b9e\u5e94\u7528\u9700\u8981\u9891\u7e41\u6570\u636e\u66f4\u65b0\u5904\u7406", "method": "\u57fa\u4e8e\u7ecf\u5178DRed\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e13\u95e8\u64cd\u4f5c\u7b26\u6765\u5904\u7406DatalogMTL\u7269\u5316\u7684\u5468\u671f\u6027\u8868\u793a", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDRedMTL\u901a\u5e38\u663e\u8457\u4f18\u4e8e\u91cd\u65b0\u7269\u5316\u65b9\u6cd5\uff0c\u6709\u65f6\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7", "conclusion": "DRedMTL\u4e3aDatalogMTL\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u589e\u91cf\u63a8\u7406\u80fd\u529b\uff0c\u6ee1\u8db3\u73b0\u5b9e\u5e94\u7528\u5bf9\u52a8\u6001\u66f4\u65b0\u7684\u9700\u6c42"}}
{"id": "2511.12830", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12830", "abs": "https://arxiv.org/abs/2511.12830", "authors": ["Joanna Klauser", "Bruno Albert", "Christian Lindenmeier", "Andreas Hammer", "Felix Freiling", "Dirk Heckmann", "Sabine Pfeiffer"], "title": "Telekommunikations\u00fcberwachung am Scheideweg: Zur Regulierbarkeit des Zugriffes auf verschl\u00fcsselte Kommunikation", "comment": "Preprint of an article to appear in CyberStR - Zeitschrift f\u00fcr Cyberstrafrecht, Carl Heymanns Verlag, ISSN 3052-5926, Issue 1 (2026), in German", "summary": "Personal communication using technical means is protected by telecommunications secrecy. Any interference with this fundamental right requires a legal basis, which has existed for many years for traditional communication services in the form of telecommunications surveillance (TK\u00dc, \u00a7 100a StPO) and appears to be widely accepted by society. The basis for the implementation of TK\u00dc is the obligation of telecommunications providers to provide interception interfaces. However, the technical implementation of telecommunications has changed significantly as a result of the Internet. Messenger services and Voice over IP telephony are increasingly competing with traditional telephone services. The use of strong end-to-end encryption made possible by this technology is increasingly posing problems for law enforcement agencies, as only cryptographically encrypted content is accessible via the interception interfaces provided to date. Against the backdrop of current discussions on socalled ``chat control'' and its limited social acceptance, this article addresses the question of whether and, if so, how the cooperation obligations of the technical actors involved can be sensibly regulated in the case of encrypted communication.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5728\u7aef\u5230\u7aef\u52a0\u5bc6\u901a\u4fe1\u65f6\u4ee3\uff0c\u5982\u4f55\u5408\u7406\u89c4\u8303\u6280\u672f\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u4e49\u52a1\uff0c\u4ee5\u5e73\u8861\u6267\u6cd5\u9700\u6c42\u4e0e\u901a\u4fe1\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u6280\u672f\u53d1\u5c55\uff0c\u4f20\u7edf\u7535\u4fe1\u76d1\u63a7\u9762\u4e34\u6311\u6218\uff0c\u7aef\u5230\u7aef\u52a0\u5bc6\u6280\u672f\u4f7f\u6267\u6cd5\u673a\u6784\u96be\u4ee5\u83b7\u53d6\u901a\u4fe1\u5185\u5bb9\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u6280\u672f\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u4e49\u52a1\u3002", "method": "\u5206\u6790\u5f53\u524d\u7535\u4fe1\u76d1\u63a7\u6cd5\u5f8b\u6846\u67b6\uff0c\u63a2\u8ba8\u52a0\u5bc6\u901a\u4fe1\u73af\u5883\u4e0b\u6280\u672f\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u4e49\u52a1\u5982\u4f55\u5408\u7406\u89c4\u8303\u3002", "result": "\u8bc6\u522b\u51fa\u4f20\u7edf\u7535\u4fe1\u76d1\u63a7\u5728\u52a0\u5bc6\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u6280\u672f\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u4e49\u52a1\u6846\u67b6\u3002", "conclusion": "\u5728\u52a0\u5bc6\u901a\u4fe1\u65f6\u4ee3\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u6280\u672f\u53c2\u4e0e\u8005\u7684\u5408\u4f5c\u4e49\u52a1\u89c4\u8303\u65b9\u5f0f\uff0c\u4ee5\u5e73\u8861\u6267\u6cd5\u9700\u6c42\u4e0e\u4e2a\u4eba\u901a\u4fe1\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2511.11959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11959", "abs": "https://arxiv.org/abs/2511.11959", "authors": ["Leonardi Melo", "Lu\u00eds Gustavo", "Dimmy Magalh\u00e3es", "Lucciani Vieira", "Mauro Ara\u00fajo"], "title": "Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs", "comment": "14 pages, 8 figures. Preprint submitted to arXiv", "summary": "This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Po\u00e7o da Bebidinha Archaeological Complex, Piau\u00ed, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u57fa\u4e8eU-Net\u7684\u67b6\u6784\u7528\u4e8e\u5df4\u897f\u8003\u53e4\u9057\u5740\u5ca9\u753b\u5ca9\u523b\u7684\u8bed\u4e49\u5206\u5272\uff0c\u5176\u4e2dAttention-Residual BEGL-UNet\u8868\u73b0\u6700\u4f73\uff0cDice\u5f97\u5206\u4e3a0.710\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540cU-Net\u67b6\u6784\u5728\u8003\u53e4\u9057\u4ea7\u6570\u5b57\u4fdd\u62a4\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5ca9\u753b\u5ca9\u523b\u7684\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cdU-Net\u67b6\u6784\uff1a(1) BEGL-UNet\uff1b(2) Attention-Residual BEGL-UNet\uff0c\u5305\u542b\u6b8b\u5dee\u5757\u548c\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\uff1b(3) Spatial Channel Attention BEGL-UNet\uff0c\u91c7\u7528\u57fa\u4e8e\u5377\u79ef\u5757\u6ce8\u610f\u529b\u6a21\u5757\u7684\u7a7a\u95f4\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u3002\u6240\u6709\u5b9e\u73b0\u90fd\u4f7f\u7528\u7ed3\u5408\u4e8c\u5143\u4ea4\u53c9\u71b5\u548c\u9ad8\u65af\u8fb9\u7f18\u589e\u5f3a\u7684BEGL\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5728\u5df4\u897fPo\u00e7o da Bebidinha\u8003\u53e4\u9057\u5740\u56fe\u50cf\u4e0a\u4f7f\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "Attention-Residual BEGL-UNet\u83b7\u5f97\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff0cDice\u5f97\u5206\u4e3a0.710\uff0c\u9a8c\u8bc1\u635f\u5931\u4e3a0.067\uff0c\u6700\u9ad8\u53ec\u56de\u7387\u4e3a0.854\u3002Spatial Channel Attention BEGL-UNet\u6027\u80fd\u76f8\u5f53\uff0cDSC\u4e3a0.707\uff0c\u53ec\u56de\u7387\u4e3a0.857\u3002\u57fa\u7ebfBEGL-UNet\u7684DSC\u4e3a0.690\u3002\u6ce8\u610f\u529b\u673a\u5236\u76f8\u6bd4\u57fa\u7ebf\u5e26\u67652.5-2.9%\u7684Dice\u5f97\u5206\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u6ce8\u610f\u529b\u673a\u5236\u5728\u8003\u53e4\u9057\u4ea7\u6570\u5b57\u4fdd\u62a4\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662fAttention-Residual BEGL-UNet\u67b6\u6784\u5728\u5ca9\u753b\u5ca9\u523b\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2511.12208", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12208", "abs": "https://arxiv.org/abs/2511.12208", "authors": ["Jilong Liu", "Pengyang Shao", "Wei Qin", "Fei Liu", "Yonghui Yang", "Richang Hong"], "title": "Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering", "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.", "AI": {"tldr": "DoM\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u52a8\u6001\u878d\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u548c\u975e\u7ed3\u6784\u5316\u5916\u90e8\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u95ee\u9898\uff0c\u5e76\u5728\u771f\u5b9e\u77e5\u8bc6\u66f4\u65b0\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u56fe\u8c31\u901a\u5e38\u4e0d\u5b8c\u6574\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u81ea\u9002\u5e94\u5730\u878d\u5408\u591a\u6e90\u77e5\u8bc6\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u77e5\u8bc6\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u63d0\u51faDoM\u6846\u67b6\uff0c\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8303\u5f0f\uff0c\u5206\u914d\u4e13\u95e8\u667a\u80fd\u4f53\u5206\u522b\u5904\u7406\u77e5\u8bc6\u56fe\u8c31\u548c\u5916\u90e8\u6587\u672c\u63a8\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u4ea4\u4e92\u534f\u8c03\u8f93\u51fa\uff0c\u5305\u62ec\u95ee\u9898\u5206\u89e3\u3001\u53cc\u667a\u80fd\u4f53\u8bc1\u636e\u68c0\u7d22\u548c\u6cd5\u5b98\u667a\u80fd\u4f53\u8bc4\u4f30\u805a\u5408\u3002", "result": "\u5728\u6784\u5efa\u7684\u771f\u5b9e\u77e5\u8bc6\u66f4\u65b0\u6570\u636e\u96c6\u4e0a\uff0cDoM\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DoM\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u4e92\u8865\u6027\u5229\u7528\u548c\u589e\u5f3a\u5bf9\u77e5\u8bc6\u56fe\u8c31\u4e0d\u5b8c\u6574\u6027\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u95ee\u9898\u3002"}}
{"id": "2511.12966", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12966", "abs": "https://arxiv.org/abs/2511.12966", "authors": ["Smitha Muthya Sudheendra", "Zhongxing Zhang", "Wenwen Cao", "Jisu Huh", "Jaideep Srivastava"], "title": "Beyond Citations: A Cross-Domain Metric for Dataset Impact and Shareability", "comment": null, "summary": "The scientific community increasingly relies on open data sharing, yet existing metrics inadequately capture the true impact of datasets as research outputs. Traditional measures, such as the h-index, focus on publications and citations but fail to account for dataset accessibility, reuse, and cross-disciplinary influence. We propose the X-index, a novel author-level metric that quantifies the value of data contributions through a two-step process: (i) computing a dataset-level value score (V-score) that integrates breadth of reuse, FAIRness, citation impact, and transitive reuse depth, and (ii) aggregating V-scores into an author-level X-index. Using datasets from computational social science, medicine, and crisis communication, we validate our approach against expert ratings, achieving a strong correlation. Our results demonstrate that the X-index provides a transparent, scalable, and low-cost framework for assessing data-sharing practices and incentivizing open science. The X-index encourages sustainable data-sharing practices and gives institutions, funders, and platforms a tangible way to acknowledge the lasting influence of research datasets.", "AI": {"tldr": "\u63d0\u51faX-index\u4f5c\u4e3a\u65b0\u7684\u4f5c\u8005\u7ea7\u6307\u6807\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u4ef7\u503c\u8bc4\u5206\uff08V-score\uff09\u548c\u805a\u5408\u673a\u5236\u6765\u91cf\u5316\u6570\u636e\u8d21\u732e\u7684\u4ef7\u503c\uff0c\u89e3\u51b3\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u8861\u91cf\u6570\u636e\u96c6\u5f71\u54cd\u529b\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u6307\u6807\uff08\u5982h\u6307\u6570\uff09\u4e3b\u8981\u5173\u6ce8\u8bba\u6587\u53d1\u8868\u548c\u5f15\u7528\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u53ef\u8bbf\u95ee\u6027\u3001\u91cd\u7528\u6027\u548c\u8de8\u5b66\u79d1\u5f71\u54cd\u529b\uff0c\u9700\u8981\u65b0\u7684\u5ea6\u91cf\u65b9\u6cd5\u6765\u6fc0\u52b1\u5f00\u653e\u79d1\u5b66\u548c\u6570\u636e\u5171\u4eab\u5b9e\u8df5\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\u8ba1\u7b97\u6570\u636e\u96c6\u7ea7\u4ef7\u503c\u8bc4\u5206\uff08V-score\uff09\uff0c\u6574\u5408\u91cd\u7528\u5e7f\u5ea6\u3001FAIR\u539f\u5219\u3001\u5f15\u7528\u5f71\u54cd\u548c\u4f20\u9012\u91cd\u7528\u6df1\u5ea6\uff1b\u7136\u540e\u5c06V-score\u805a\u5408\u4f5c\u8005\u7ea7X-index\u3002", "result": "\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u3001\u533b\u5b66\u548c\u5371\u673a\u4f20\u64ad\u7b49\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e0e\u4e13\u5bb6\u8bc4\u5206\u5448\u73b0\u5f3a\u76f8\u5173\u6027\uff0c\u8bc1\u660eX-index\u7684\u6709\u6548\u6027\u3002", "conclusion": "X-index\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u6269\u5c55\u4e14\u4f4e\u6210\u672c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u5171\u4eab\u5b9e\u8df5\u5e76\u6fc0\u52b1\u5f00\u653e\u79d1\u5b66\uff0c\u4e3a\u673a\u6784\u3001\u8d44\u52a9\u8005\u548c\u5e73\u53f0\u63d0\u4f9b\u4e86\u8ba4\u53ef\u7814\u7a76\u6570\u636e\u96c6\u6301\u4e45\u5f71\u54cd\u529b\u7684\u5177\u4f53\u65b9\u6cd5\u3002"}}
{"id": "2511.11654", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11654", "abs": "https://arxiv.org/abs/2511.11654", "authors": ["Sayambhu Sen", "Shalabh Bhatnagar"], "title": "Convergence of Multiagent Learning Systems for Traffic control", "comment": "14 pages 2 figures", "summary": "Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.", "AI": {"tldr": "\u672c\u6587\u5bf9\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u6536\u655b\u6027\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8be5\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "motivation": "\u968f\u7740\u73ed\u52a0\u7f57\u5c14\u7b49\u57ce\u5e02\u7684\u5feb\u901f\u57ce\u5e02\u5316\u5bfc\u81f4\u4e25\u91cd\u4ea4\u901a\u62e5\u5835\uff0c\u9700\u8981\u9ad8\u6548\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u3002\u867d\u7136\u5df2\u6709\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u7684\u4e25\u683c\u7406\u8bba\u5206\u6790\u3002", "method": "\u4f7f\u7528\u968f\u673a\u903c\u8fd1\u65b9\u6cd5\u6b63\u5f0f\u5206\u6790\u5b66\u4e60\u52a8\u6001\uff0c\u5c06\u5355\u667a\u80fd\u4f53\u5f02\u6b65\u503c\u8fed\u4ee3\u7684\u6536\u655b\u8bc1\u660e\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u63a7\u5236\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u7279\u5b9a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u7ed9\u5b9a\u6761\u4ef6\u4e0b\u80fd\u591f\u6536\u655b\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4ea4\u901a\u63a7\u5236\u9886\u57df\u7406\u8bba\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3a\u8be5\u7b97\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u3002"}}
{"id": "2511.13189", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13189", "abs": "https://arxiv.org/abs/2511.13189", "authors": ["Diego Ortego", "Marlon Rodr\u00edguez", "Mario Almagro", "Kunal Dahiya", "David Jim\u00e9nez", "Juan C. SanMiguel"], "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework", "comment": "To appear at AAAI 2026", "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faViXML\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89e3\u7801\u5668\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\u6765\u6539\u8fdb\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u6709\u6548\u5229\u7528\u5927\u578b\u89e3\u7801\u5668\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faViXML\u6846\u67b6\uff0c\u96c6\u6210\u57fa\u7840\u89c6\u89c9\u6a21\u578b\u751f\u6210\u5355\u56fe\u50cf\u5d4c\u5165\uff0c\u7ed3\u5408\u89e3\u7801\u5668\u6a21\u578b\u8fdb\u884c\u591a\u6a21\u6001\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cViXML\u5728\u6700\u5927\u6570\u636e\u96c6\u4e0aP@1\u6307\u6807\u6bd4\u4e4b\u524d\u6700\u4f18\u65b9\u6cd5\u63d0\u53478.21%\u3002", "conclusion": "\u89c6\u89c9\u4fe1\u606f\u5728\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cViXML\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u4e3a\u591a\u6a21\u6001XMC\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12214", "abs": "https://arxiv.org/abs/2511.12214", "authors": ["Ruochen Li", "Zhanxing Zhu", "Tanqiu Qiao", "Hubert P. H. Shum"], "title": "ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction", "comment": null, "summary": "Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.", "AI": {"tldr": "\u63d0\u51faViTE\u6846\u67b6\u89e3\u51b3\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u9ad8\u9636\u4ea4\u4e92\u5efa\u6a21\u95ee\u9898\uff0c\u901a\u8fc7\u865a\u62df\u56fe\u5efa\u6a21\u957f\u8ddd\u79bb\u9ad8\u9636\u4ea4\u4e92\uff0c\u4e13\u5bb6\u8def\u7531\u5668\u81ea\u9002\u5e94\u9009\u62e9\u4ea4\u4e92\u4e13\u5bb6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u4e2d\u5b58\u5728\u57fa\u672c\u6743\u8861\uff1a\u5c42\u6570\u4e0d\u8db3\u5bfc\u81f4\u611f\u53d7\u91ce\u53d7\u9650\uff0c\u5c42\u6570\u8fc7\u591a\u5219\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u80fd\u591f\u81ea\u9002\u5e94\u5efa\u6a21\u663e\u5f0f\u4e00\u8df3\u4ea4\u4e92\u548c\u9690\u5f0f\u9ad8\u9636\u4f9d\u8d56\u7684\u6709\u6548\u6a21\u578b\u3002", "method": "\u63d0\u51faViTE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u865a\u62df\u56fe\u5f15\u5165\u52a8\u6001\u865a\u62df\u8282\u70b9\u5efa\u6a21\u957f\u8ddd\u79bb\u9ad8\u9636\u4ea4\u4e92\u800c\u65e0\u9700\u6df1\u5c42GNN\u5806\u53e0\uff1b\u4e13\u5bb6\u8def\u7531\u5668\u57fa\u4e8e\u793e\u4ea4\u4e0a\u4e0b\u6587\u4f7f\u7528\u4e13\u5bb6\u6df7\u5408\u8bbe\u8ba1\u81ea\u9002\u5e94\u9009\u62e9\u4ea4\u4e92\u4e13\u5bb6\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08ETH/UCY\u3001NBA\u548cSDD\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u9645\u6548\u7387\u3002", "conclusion": "ViTE\u6846\u67b6\u901a\u8fc7\u865a\u62df\u56fe\u548c\u4e13\u5bb6\u8def\u7531\u5668\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u4ea4\u4e92\u6a21\u5f0f\u63a8\u7406\uff0c\u5728\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6GNN\u67b6\u6784\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2511.13432", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13432", "abs": "https://arxiv.org/abs/2511.13432", "authors": ["Subramanyam Sahoo", "Aditi Chhawacharia"], "title": "The Last Vote: A Multi-Stakeholder Framework for Language Model Governance", "comment": "This paper has been accepted to the NeurIPS 2025 Workshop on Algorithmic Collective Action (ACA@NeurIPS 2025). The submission is 26 pages including the appendix and includes the NeurIPS checklist. A big thanks to Avijit Ghosh", "summary": "As artificial intelligence systems become increasingly powerful and pervasive, democratic societies face unprecedented challenges in governing these technologies while preserving core democratic values and institutions. This paper presents a comprehensive framework to address the full spectrum of risks that AI poses to democratic societies. Our approach integrates multi-stakeholder participation, civil society engagement, and existing international governance frameworks while introducing novel mechanisms for risk assessment and institutional adaptation. We propose: (1) a seven-category democratic risk taxonomy extending beyond individual-level harms to capture systemic threats, (2) a stakeholder-adaptive Incident Severity Score (ISS) that incorporates diverse perspectives and context-dependent risk factors, and (3) a phased implementation strategy that acknowledges the complex institutional changes required for effective AI governance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684AI\u6c11\u4e3b\u98ce\u9669\u6cbb\u7406\u6846\u67b6\uff0c\u5305\u62ec\u4e03\u7c7b\u98ce\u9669\u5206\u7c7b\u6cd5\u3001\u5229\u76ca\u76f8\u5173\u8005\u9002\u5e94\u7684\u4e25\u91cd\u6027\u8bc4\u5206\u7cfb\u7edf\u548c\u5206\u9636\u6bb5\u5b9e\u65bd\u7b56\u7565\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u65e5\u76ca\u5f3a\u5927\u548c\u666e\u53ca\uff0c\u6c11\u4e3b\u793e\u4f1a\u9762\u4e34\u524d\u6240\u672a\u6709\u7684\u6311\u6218\uff0c\u9700\u8981\u5728\u6cbb\u7406\u8fd9\u4e9b\u6280\u672f\u7684\u540c\u65f6\u4fdd\u62a4\u6838\u5fc3\u6c11\u4e3b\u4ef7\u503c\u89c2\u548c\u5236\u5ea6\u3002", "method": "\u6574\u5408\u591a\u65b9\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u3001\u516c\u6c11\u793e\u4f1a\u53c2\u4e0e\u548c\u73b0\u6709\u56fd\u9645\u6cbb\u7406\u6846\u67b6\uff0c\u5f15\u5165\u98ce\u9669\u8bc4\u4f30\u548c\u5236\u5ea6\u9002\u5e94\u7684\u65b0\u673a\u5236\u3002", "result": "\u5f00\u53d1\u4e86\u4e03\u7c7b\u6c11\u4e3b\u98ce\u9669\u5206\u7c7b\u6cd5\u3001\u5229\u76ca\u76f8\u5173\u8005\u9002\u5e94\u7684\u4e25\u91cd\u6027\u8bc4\u5206\u7cfb\u7edf\uff08ISS\uff09\u548c\u5206\u9636\u6bb5\u5b9e\u65bd\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6c11\u4e3b\u793e\u4f1a\u63d0\u4f9b\u4e86\u5e94\u5bf9AI\u7cfb\u7edf\u6027\u98ce\u9669\u7684\u5168\u9762\u6cbb\u7406\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5236\u5ea6\u53d8\u9769\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2511.11989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11989", "abs": "https://arxiv.org/abs/2511.11989", "authors": ["Songsong Zhang", "Chuanqi Tang", "Hongguang Zhang", "Guijian Tang", "Minglong Li", "Xueqiong Li", "Shaowu Yang", "Yuanxi Peng", "Wenjing Yang", "Jing Zhao"], "title": "BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups", "comment": "9 pages, 10 figures", "summary": "Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a81\u7834\u9762\u90e8\u7279\u5199\u9650\u5236\u7684\u8eab\u4efd\u4fdd\u6301\u4e2a\u6027\u5316\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u7ebf\u63a8\u7406\u7ba1\u9053\u3001\u8eab\u4efd\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u548c\u8eab\u4efd\u805a\u5408\u524d\u7f6e\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u8eab\u4efd\u7279\u5f81\u5d4c\u5165\u524a\u5f31\u751f\u6210\u6a21\u578b\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8eab\u4efd\u4fdd\u6301\u4e2a\u6027\u5316\u751f\u6210\u65b9\u6cd5\u8fc7\u5ea6\u5f3a\u8c03\u9762\u90e8\u533a\u57df\uff0c\u5bfc\u81f4\u8f93\u51fa\u88ab\u9762\u90e8\u7279\u5199\u4e3b\u5bfc\uff0c\u89c6\u89c9\u53d9\u4e8b\u6027\u5f31\u4e14\u5728\u590d\u6742\u6587\u672c\u63d0\u793a\u4e0b\u8bed\u4e49\u4e00\u81f4\u6027\u5dee\u3002\u6838\u5fc3\u95ee\u9898\u5728\u4e8e\u8eab\u4efd\u7279\u5f81\u5d4c\u5165\u524a\u5f31\u4e86\u751f\u6210\u6a21\u578b\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u8eab\u4efd\u8bed\u4e49\u5206\u79bb\u7684\u53cc\u7ebf\u63a8\u7406\u7ba1\u9053\uff0c\u63d0\u51fa\u8eab\u4efd\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u5c06\u8eab\u4efd\u8bed\u4e49\u878d\u5408\u63a8\u8fdf\u5230\u566a\u58f0\u9884\u6d4b\u9636\u6bb5\uff0c\u5f15\u5165\u8eab\u4efd\u805a\u5408\u524d\u7f6e\u6a21\u5757\u6765\u805a\u5408\u8eab\u4efd\u4fe1\u606f\u5e76\u66ff\u6362\u968f\u673a\u521d\u59cb\u5316\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u5728\u8d85\u8d8a\u9762\u90e8\u7279\u5199\u7684\u8eab\u4efd\u4fdd\u6301\u4e2a\u6027\u5316\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6709\u6548\u7684\u6027\u80fd\uff0c\u65e0\u9700\u624b\u52a8\u906e\u7f69\u6216\u5fae\u8c03\u5373\u53ef\u9ad8\u6548\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\u5feb\u901f\u90e8\u7f72\u5230\u73b0\u6709\u6846\u67b6\u4e2d\uff0c\u89e3\u51b3\u4e86\u5bf9\u9762\u90e8\u7279\u5199\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u4fc3\u8fdb\u4e86\u7535\u5f71\u7ea7\u89d2\u8272\u573a\u666f\u521b\u4f5c\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u4e2a\u6027\u5316\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2511.12239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12239", "abs": "https://arxiv.org/abs/2511.12239", "authors": ["Tarun Gupta", "Danish Pruthi"], "title": "Beyond World Models: Rethinking Understanding in AI Models", "comment": "Accepted to AAAI 2026 (Main Track)", "summary": "World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models \"understand\" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u54f2\u5b66\u6848\u4f8b\u5206\u6790\u6279\u5224\u6027\u5730\u68c0\u9a8c\u4e16\u754c\u6a21\u578b\u6846\u67b6\u662f\u5426\u5145\u5206\u8868\u5f81\u4eba\u7c7b\u6c34\u5e73\u7684\u7406\u89e3\u80fd\u529b\uff0c\u6307\u51fa\u4e16\u754c\u6a21\u578b\u80fd\u529b\u4e0e\u4eba\u7c7b\u7406\u89e3\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8AI\u6a21\u578b\u4e2d\u7684\u4e16\u754c\u6a21\u578b\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u771f\u6b63\u7406\u89e3\u4e16\u754c\uff0c\u901a\u8fc7\u54f2\u5b66\u6587\u732e\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\u6765\u68c0\u9a8c\u4e16\u754c\u6a21\u578b\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u54f2\u5b66\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\u65b9\u6cd5\uff0c\u805a\u7126\u4e8e\u4e16\u754c\u6a21\u578b\u80fd\u529b\u4e0e\u4eba\u7c7b\u7406\u89e3\u533a\u5206\u6700\u4e3a\u660e\u663e\u7684\u7279\u5b9a\u54f2\u5b66\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e16\u754c\u6a21\u578b\u6846\u67b6\u5728\u8868\u5f81\u4eba\u7c7b\u6c34\u5e73\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u867d\u7136\u8fd9\u4e9b\u6848\u4f8b\u4ee3\u8868\u7279\u5b9a\u7684\u7406\u89e3\u89c2\u70b9\u800c\u975e\u666e\u9002\u5b9a\u4e49\uff0c\u4f46\u6709\u52a9\u4e8e\u63a2\u7d22\u4e16\u754c\u6a21\u578b\u7684\u8fb9\u754c\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u867d\u7136\u80fd\u591f\u6a21\u62df\u5916\u90e8\u4e16\u754c\u7684\u67d0\u4e9b\u65b9\u9762\uff0c\u4f46\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5145\u5206\u8868\u5f81\u4eba\u7c7b\u6c34\u5e73\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u91cd\u8981\u5dee\u5f02\u3002"}}
{"id": "2511.13525", "categories": ["cs.CY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13525", "abs": "https://arxiv.org/abs/2511.13525", "authors": ["Zichong Wang", "Zhipeng Yin", "Roland H. C. Yap", "Wenbin Zhang"], "title": "AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions", "comment": "ECAI 2025", "summary": "Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5728\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u7684AI\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u516c\u5e73\u6027\u6982\u5ff5\u5206\u7c7b\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u6280\u672f\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8eAI\u51b3\u7b56\u7cfb\u7edf\u5b58\u5728\u6b67\u89c6\u6027\u7ed3\u679c\uff0c\u516c\u5e73\u6027\u6210\u4e3a\u65e5\u76ca\u5173\u6ce8\u7684\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u6574\u7684\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u4f46\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u4e0d\u53ef\u884c\uff0c\u5b58\u5728\u6cd5\u5f8b\u7ea6\u675f\u548c\u5f3a\u5316\u6b67\u89c6\u7684\u98ce\u9669\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u516c\u5e73\u6027\u6982\u5ff5\u5206\u7c7b\u6cd5\uff0c\u9610\u660e\u4e0d\u540c\u6982\u5ff5\u4e4b\u95f4\u7684\u5173\u7cfb\u548c\u533a\u522b\uff1b\u603b\u7ed3\u4e86\u5728\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u4fc3\u8fdb\u516c\u5e73\u6027\u7684\u73b0\u6709\u6280\u672f\u3002", "result": "\u6f84\u6e05\u4e86\u5728\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0d\u5b8c\u6574\u8bbe\u7f6e\u4e0b\u5404\u79cd\u516c\u5e73\u6027\u6982\u5ff5\u7684\u5173\u7cfb\u548c\u533a\u522b\uff0c\u4e3a\u8fd9\u4e00\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0e\u73b0\u5b9e\u6311\u6218\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5f3a\u8c03\u4e86\u5728\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u5b9e\u73b0AI\u516c\u5e73\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.11663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11663", "abs": "https://arxiv.org/abs/2511.11663", "authors": ["Zhixiong Zhao", "Fangxin Liu", "Junjie Wang", "Chenyang Guan", "Zongwu Wang", "Li Jiang", "Haibing Guan"], "title": "SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization", "comment": "Accepted at AAAI 2026", "summary": "The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.", "AI": {"tldr": "SpecQuant\u662f\u4e00\u79cd\u4ece\u5085\u91cc\u53f6\u9891\u7387\u57df\u89d2\u5ea6\u89e3\u51b3LLM\u6781\u7aef\u538b\u7f29\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u6ed1\u6fc0\u6d3b\u5f02\u5e38\u503c\u548c\u901a\u9053\u7ea7\u4f4e\u9891\u5085\u91cc\u53f6\u622a\u65ad\uff0c\u5728LLaMA-3 8B\u4e0a\u5b9e\u73b04\u4f4d\u6743\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\uff0c\u7cbe\u5ea6\u635f\u5931\u4ec51.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e3\u500d\u3002", "motivation": "\u968f\u7740\u51c6\u786e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u9700\u8981\u5728\u7ec8\u7aef\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\uff0c\u8fd9\u63a8\u52a8\u4e86\u5bf9\u5148\u8fdb\u91cf\u5316\u6280\u672f\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6743\u91cd\u548c\u6fc0\u6d3b\u7684\u6781\u4f4e\u4f4d\u91cf\u5316\u6311\u6218\u3002", "method": "\u63d0\u51faSpecQuant\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5e73\u6ed1\u6fc0\u6d3b\u5f02\u5e38\u503c\u5e76\u5c06\u5176\u8f6c\u79fb\u5230\u6743\u91cd\u77e9\u9635\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u901a\u9053\u7ea7\u4f4e\u9891\u5085\u91cc\u53f6\u622a\u65ad\uff0c\u6291\u5236\u9ad8\u9891\u5206\u91cf\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4fe1\u53f7\u80fd\u91cf\u3002\u8fd8\u5f15\u5165\u4e86\u8f7b\u91cf\u7ea7\u622a\u65ad\u6a21\u5757\u5b9e\u73b0\u8fd0\u884c\u65f6\u9002\u5e94\u6027\u3002", "result": "\u5728LLaMA-3 8B\u4e0a\u5b9e\u73b0\u4e864\u4f4d\u6743\u91cd\u548c\u6fc0\u6d3b\u91cf\u5316\uff0c\u96f6\u6837\u672c\u51c6\u786e\u7387\u4e0e\u5168\u7cbe\u5ea6\u76f8\u6bd4\u4ec5\u4e0b\u964d1.5%\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e3\u500d\u3002", "conclusion": "\u4ece\u5085\u91cc\u53f6\u9891\u7387\u57df\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6LLM\u6781\u7aef\u538b\u7f29\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u6743\u91cd\u80fd\u91cf\u4e3b\u8981\u96c6\u4e2d\u5728\u4f4e\u9891\u5206\u91cf\uff0c\u901a\u8fc7\u4fdd\u7559\u8fd9\u4e9b\u5206\u91cf\u53ef\u4ee5\u5728\u6700\u5c0f\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u6781\u4f4e\u4f4d\u91cf\u5316\u3002"}}
{"id": "2511.12241", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12241", "abs": "https://arxiv.org/abs/2511.12241", "authors": ["Junhyuk Seo", "Hyeyoon Moon", "Kyu-Hwan Jung", "Namkee Oh", "Taerim Kim"], "title": "AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos", "comment": "12 pages, 5 figures", "summary": "Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.", "AI": {"tldr": "AURA\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u98ce\u9669\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b8c\u5168\u5408\u6210\u7684ICU\u89c6\u9891\u6570\u636e\u96c6\u5f00\u53d1\uff0c\u901a\u8fc7\u59ff\u6001\u4f30\u8ba1\u68c0\u6d4b\u60a3\u8005\u624b\u90e8\u8fdb\u5165\u6c14\u7ba1\u63d2\u7ba1\u9644\u8fd1\u533a\u57df\uff08\u78b0\u649e\uff09\u548c\u5173\u952e\u70b9\u901f\u5ea6\uff08\u8e81\u52a8\uff09\u4e24\u79cd\u9ad8\u98ce\u9669\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u5b9e\u65f6\u975e\u8ba1\u5212\u62d4\u7ba1\u68c0\u6d4b\u3002", "motivation": "ICU\u4e2d\u975e\u8ba1\u5212\u62d4\u7ba1\u662f\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\u96be\u4ee5\u83b7\u53d6\u6807\u6ce8\u7684ICU\u89c6\u9891\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6280\u672f\u751f\u6210\u591a\u6837\u5316\u7684\u4e34\u5e8a\u771f\u5b9eICU\u573a\u666f\uff0c\u5e94\u7528\u59ff\u6001\u4f30\u8ba1\u8bc6\u522b\u4e24\u79cd\u9ad8\u98ce\u9669\u6a21\u5f0f\uff1a\u624b\u90e8\u8fdb\u5165\u6c14\u7ba1\u63d2\u7ba1\u9644\u8fd1\u533a\u57df\u7684\u78b0\u649e\u884c\u4e3a\uff0c\u4ee5\u53ca\u901a\u8fc7\u8ddf\u8e2a\u89e3\u5256\u5173\u952e\u70b9\u901f\u5ea6\u91cf\u5316\u7684\u8e81\u52a8\u884c\u4e3a\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u786e\u8ba4\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\uff0c\u6027\u80fd\u8bc4\u4f30\u663e\u793a\u78b0\u649e\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff0c\u8e81\u52a8\u8bc6\u522b\u6027\u80fd\u4e2d\u7b49\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u590d\u73b0\u7684\u60a3\u8005\u5b89\u5168\u76d1\u63a7\u7cfb\u7edf\u7684\u65b0\u9014\u5f84\uff0c\u5177\u6709\u5728\u91cd\u75c7\u76d1\u62a4\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.11665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11665", "abs": "https://arxiv.org/abs/2511.11665", "authors": ["Sameeksha Sriram", "Ayush Paliwal", "Alexander S. Ecker", "Chase van de Geijn"], "title": "Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE", "comment": null, "summary": "Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u56db\u5143\u6570\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5QuatRo\uff0c\u5e76\u5c06\u5176\u63a8\u5e7f\u5230\u57fa\u4e8e\u51e0\u4f55\u4ee3\u6570\u7684Clifford\u4ee3\u6570\u65cb\u8f6c\u5d4c\u5165CARE\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7403\u5f62RoPE\u7684\u975e\u4ea4\u6362\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4efb\u610f\u7ef4\u5ea6\u7684\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u3002", "motivation": "\u73b0\u6709\u7684\u7403\u5f62\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u662f\u975e\u4ea4\u6362\u7684\uff0c\u5931\u53bb\u4e86RoPE\u7684\u5e73\u79fb\u7b49\u53d8\u6027\u7279\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u56db\u5143\u6570\u548cClifford\u4ee3\u6570\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u901a\u7528\u548c\u6570\u5b66\u4e0a\u66f4\u4f18\u96c5\u7684\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51faQuatRo\u65b9\u6cd5\uff0c\u4f7f\u7528\u56db\u5143\u6570\u4ee3\u66ff\u6b27\u62c9\u89d2\u6765\u8868\u793a3D\u65cb\u8f6c\uff1b2. \u5c06QuatRo\u63a8\u5e7f\u5230CARE\uff0c\u5229\u7528Clifford\u4ee3\u6570\u4e2d\u7684\u65cb\u91cf\u5728\u591a\u5411\u91cf\u4e0a\u4f5c\u7528\uff1b3. \u5c55\u793a\u4e86\u6df7\u5408RoPE\u548c\u7403\u5f62RoPE\u90fd\u662fQuatRo\u7684\u7279\u4f8b\u3002", "result": "\u63d0\u51fa\u7684QuatRo\u548cCARE\u65b9\u6cd5\u80fd\u591f\uff1a1. \u6269\u5c55\u5230\u4efb\u610f\u7ef4\u5ea6\uff1b2. \u5728\u591a\u5411\u91cf\u7684\u591a\u4e2a\u7b49\u7ea7\u4e0a\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f\uff1b3. \u4fdd\u6301\u4e86\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u7684\u6570\u5b66\u6027\u8d28\u3002", "conclusion": "\u57fa\u4e8e\u56db\u5143\u6570\u548cClifford\u4ee3\u6570\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5\u4e3a\u4f4d\u7f6e\u7f16\u7801\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u548c\u6570\u5b66\u4e0a\u66f4\u4f18\u96c5\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u7ef4\u5ea6\u7684\u8f93\u5165\u5e76\u4fdd\u6301\u91cd\u8981\u7684\u6570\u5b66\u6027\u8d28\u3002"}}
{"id": "2511.13555", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13555", "abs": "https://arxiv.org/abs/2511.13555", "authors": ["Evelien Brouwer", "Frederik Zuiderveen Borgesius"], "title": "Access to Personal Data and the Right to Good Governance during Asylum Procedures after the CJEU's YS. and M. and S. judgment", "comment": null, "summary": "In the YS. and M. and S. judgment, the Court of Justice of the European Union ruled on three procedures in which Dutch judges asked for clarification on the right of asylum seekers to have access to the documents regarding the decision on asylum applications. The judgment is relevant for interpreting the concept of personal data and the scope of the right of access under the Data Protection Directive, and the right to good administration in the EU Charter of Fundamental Rights. At first glance, the judgment seems disappointing from the viewpoint of individual rights. Nevertheless, in our view the judgment provides sufficient grounds for effective access rights to the minutes in future asylum cases.", "AI": {"tldr": "\u6b27\u6d32\u6cd5\u9662\u5728YS\u3001M\u548cS\u6848\u4e2d\u88c1\u51b3\u4e86\u5bfb\u6c42\u5e87\u62a4\u8005\u83b7\u53d6\u5e87\u62a4\u7533\u8bf7\u51b3\u5b9a\u76f8\u5173\u6587\u4ef6\u7684\u6743\u5229\uff0c\u6d89\u53ca\u4e2a\u4eba\u6570\u636e\u6982\u5ff5\u3001\u6570\u636e\u4fdd\u62a4\u6307\u4ee4\u4e0b\u7684\u8bbf\u95ee\u6743\u8303\u56f4\u4ee5\u53ca\u6b27\u76df\u57fa\u672c\u6743\u5229\u5baa\u7ae0\u4e2d\u7684\u826f\u597d\u884c\u653f\u6743\u3002", "motivation": "\u5206\u6790\u6b27\u6d32\u6cd5\u9662\u5173\u4e8e\u5e87\u62a4\u7533\u8bf7\u8005\u6587\u4ef6\u8bbf\u95ee\u6743\u7684\u88c1\u51b3\uff0c\u63a2\u8ba8\u5176\u5bf9\u4e2a\u4eba\u6570\u636e\u4fdd\u62a4\u548c\u884c\u653f\u6743\u5229\u89e3\u91ca\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6b27\u6d32\u6cd5\u9662\u5728YS\u3001M\u548cS\u6848\u4e2d\u7684\u5224\u51b3\uff0c\u89e3\u8bfb\u76f8\u5173\u6cd5\u5f8b\u6982\u5ff5\u548c\u6743\u5229\u8303\u56f4\u3002", "result": "\u5224\u51b3\u8868\u9762\u4e0a\u5bf9\u4e2a\u4eba\u6743\u5229\u4fdd\u62a4\u4f3c\u4e4e\u4ee4\u4eba\u5931\u671b\uff0c\u4f46\u5b9e\u9645\u4e0a\u4e3a\u672a\u6765\u5e87\u62a4\u6848\u4ef6\u4e2d\u7684\u6709\u6548\u8bbf\u95ee\u6743\u63d0\u4f9b\u4e86\u5145\u5206\u4f9d\u636e\u3002", "conclusion": "\u8be5\u5224\u51b3\u4e3a\u5e87\u62a4\u6848\u4ef6\u4e2d\u4e2a\u4eba\u6587\u4ef6\u8bbf\u95ee\u6743\u7684\u6709\u6548\u5b9e\u65bd\u5960\u5b9a\u4e86\u6cd5\u5f8b\u57fa\u7840\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2511.12006", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12006", "abs": "https://arxiv.org/abs/2511.12006", "authors": ["Kai-Wen K. Yang", "Andrew Bai", "Alexandra Bermudez", "Yunqi Hong", "Zoe Latham", "Iris Sloan", "Michael Liu", "Vishrut Goyal", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "title": "Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy", "comment": null, "summary": "Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.", "AI": {"tldr": "SIT-ADDA-Auto\u662f\u4e00\u4e2a\u81ea\u914d\u7f6e\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u8c03\u6574\u65e9\u671f\u5377\u79ef\u5c42\u5e76\u51bb\u7ed3\u6df1\u5c42\uff0c\u7ed3\u5408\u6d45\u5c42\u5bf9\u6297\u5bf9\u9f50\u548c\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u81ea\u52a8\u9009\u62e9\u9002\u5e94\u6df1\u5ea6\uff0c\u5728\u663e\u5fae\u955c\u56fe\u50cf\u8de8\u57df\u9002\u5e94\u4e2d\u4f18\u4e8e\u5168\u7f16\u7801\u5668\u9002\u5e94\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u663e\u5fae\u955c\u5e94\u7528\u4e2d\u9762\u4e34\u6a21\u578b\u5728\u65b0\u4eea\u5668\u6216\u91c7\u96c6\u8bbe\u7f6e\u4e0b\u5931\u6548\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5bf9\u6297\u57df\u9002\u5e94\u65b9\u6cd5\u4f1a\u7834\u574f\u5df2\u5b66\u4e60\u7684\u8bed\u4e49\u8868\u793a\u3002", "method": "\u63d0\u51faSIT-ADDA-Auto\u6846\u67b6\uff0c\u4ec5\u9002\u5e94\u6700\u65e9\u671f\u7684\u5377\u79ef\u5c42\u5e76\u51bb\u7ed3\u6df1\u5c42\uff0c\u96c6\u6210\u6d45\u5c42\u5bf9\u6297\u5bf9\u9f50\u548c\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u81ea\u52a8\u9009\u62e9\u9002\u5e94\u6df1\u5ea6\uff0c\u65e0\u9700\u76ee\u6807\u6807\u7b7e\u3002", "result": "\u5728\u66dd\u5149\u548c\u5149\u7167\u53d8\u5316\u3001\u8de8\u4eea\u5668\u8fc1\u79fb\u3001\u591a\u79cd\u67d3\u8272\u7b49\u573a\u666f\u4e0b\uff0cSIT-ADDA\u5728\u91cd\u5efa\u548c\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5168\u7f16\u7801\u5668\u9002\u5e94\u548c\u975e\u5bf9\u6297\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u8bed\u4e49\u7279\u5f81\u7684\u6f02\u79fb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u663e\u5fae\u955c\u4e2d\u7684\u65e0\u6807\u7b7e\u9002\u5e94\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u89c4\u5219\u548c\u73b0\u573a\u5e94\u7528\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2511.13557", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13557", "abs": "https://arxiv.org/abs/2511.13557", "authors": ["Stefan Kulk", "Frederik Zuiderveen Borgesius"], "title": "Freedom of expression and 'right to be forgotten' cases in the Netherlands after Google Spain", "comment": null, "summary": "Since the Google Spain judgment of the Court of Justice of the European Union, Europeans have, under certain conditions, the right to have search results for their name delisted. This paper examines how the Google Spain judgment has been applied in the Netherlands. Since the Google Spain judgment, Dutch courts have decided on two cases regarding delisting requests. In both cases, the Dutch courts considered freedom of expression aspects of delisting more thoroughly than the Court of Justice. However, the effect of the Google Spain judgment on freedom of expression is difficult to assess, as search engine operators decide about most delisting requests without disclosing much about their decisions.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6b27\u76df\u6cd5\u9662Google Spain\u5224\u51b3\u5728\u8377\u5170\u7684\u5e94\u7528\u60c5\u51b5\uff0c\u91cd\u70b9\u5173\u6ce8\u5220\u9664\u641c\u7d22\u7ed3\u679c\u8bf7\u6c42\u7684\u5904\u7406\u53ca\u5176\u5bf9\u8a00\u8bba\u81ea\u7531\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76Google Spain\u5224\u51b3\u540e\u6b27\u6d32\u4eba\u6709\u6743\u8981\u6c42\u5220\u9664\u59d3\u540d\u641c\u7d22\u7ed3\u679c\u7684\u6743\u5229\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u60c5\u51b5\uff0c\u7279\u522b\u662f\u5728\u8377\u5170\u7684\u5177\u4f53\u6267\u884c\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8377\u5170\u6cd5\u9662\u5728Google Spain\u5224\u51b3\u540e\u5904\u7406\u7684\u4e24\u4e2a\u5220\u9664\u8bf7\u6c42\u6848\u4f8b\uff0c\u6bd4\u8f83\u8377\u5170\u6cd5\u9662\u4e0e\u6b27\u76df\u6cd5\u9662\u5728\u8a00\u8bba\u81ea\u7531\u8003\u91cf\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "\u8377\u5170\u6cd5\u9662\u5728\u5220\u9664\u8bf7\u6c42\u6848\u4ef6\u4e2d\u6bd4\u6b27\u76df\u6cd5\u9662\u66f4\u6df1\u5165\u5730\u8003\u8651\u4e86\u8a00\u8bba\u81ea\u7531\u56e0\u7d20\uff0c\u4f46\u7531\u4e8e\u641c\u7d22\u5f15\u64ce\u8fd0\u8425\u5546\u5bf9\u5927\u90e8\u5206\u5220\u9664\u8bf7\u6c42\u51b3\u7b56\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5224\u51b3\u5bf9\u8a00\u8bba\u81ea\u7531\u7684\u5b9e\u9645\u5f71\u54cd\u96be\u4ee5\u8bc4\u4f30\u3002", "conclusion": "Google Spain\u5224\u51b3\u5728\u8377\u5170\u7684\u5b9e\u65bd\u663e\u793a\u51fa\u5bf9\u8a00\u8bba\u81ea\u7531\u66f4\u7ec6\u81f4\u7684\u8003\u91cf\uff0c\u4f46\u641c\u7d22\u5f15\u64ce\u8fd0\u8425\u5546\u51b3\u7b56\u7684\u4e0d\u900f\u660e\u6027\u9650\u5236\u4e86\u5bf9\u5176\u8a00\u8bba\u81ea\u7531\u5f71\u54cd\u7684\u51c6\u786e\u8bc4\u4f30\u3002"}}
{"id": "2511.12018", "categories": ["cs.CV", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12018", "abs": "https://arxiv.org/abs/2511.12018", "authors": ["Shounak Ray Chaudhuri", "Arash Jahangiri", "Christopher Paolini"], "title": "Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis", "comment": "8 pages, 10 figures, Submitted to IEEE Intelligent Vehicles Symposium 2026", "summary": "Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6444\u50cf\u5934\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5b9e\u65f6\u4ea4\u901a\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u52a0\u5dde\u4e18\u62c9\u7ef4\u65af\u5854\u7684\u4ea4\u53c9\u8def\u53e3\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4f7f\u7528PET\u8ba1\u7b97\u6765\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4e8b\u6545\u7684\u4ea4\u901a\u5b89\u5168\u5206\u6790\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u758f\u6027\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u5b9e\u65f6\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u540c\u6b65\u6444\u50cf\u5934\u63d0\u4f9b\u8fde\u7eed\u89c6\u89c9\u8986\u76d6\uff0c\u5728NVIDIA Jetson AGX Xavier\u8bbe\u5907\u4e0a\u4f7f\u7528YOLOv11\u5206\u5272\u8fdb\u884c\u8f66\u8f86\u68c0\u6d4b\uff0c\u901a\u8fc7\u5355\u5e94\u77e9\u9635\u5c06\u68c0\u6d4b\u5230\u7684\u8f66\u8f86\u591a\u8fb9\u5f62\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u9e1f\u77b0\u56fe\uff0c\u5e76\u5f00\u53d1\u4e86\u50cf\u7d20\u7ea7PET\u7b97\u6cd5\u3002", "result": "\u6846\u67b6\u80fd\u591f\u4ee5\u4e9a\u79d2\u7ea7\u7cbe\u5ea6\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u5904\u7406\uff0c\u5e73\u5747\u5e27\u7387\u4e3a2.68 FPS\uff0c\u751f\u6210800\u00d7800\u50cf\u7d20\u7684\u5bf9\u6570\u70ed\u56fe\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5206\u6563\u5f0f\u89c6\u89c9\u7684PET\u5206\u6790\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u3001\u5b9e\u65f6\u548c\u53ef\u6269\u5c55\u7684\u4ea4\u53c9\u8def\u53e3\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2511.11667", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11667", "abs": "https://arxiv.org/abs/2511.11667", "authors": ["Feng Guo", "Yuntao Wen", "Shen Gao", "Junshuo Zhang", "Shuo Shang"], "title": "Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion", "comment": null, "summary": "Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.", "AI": {"tldr": "\u63d0\u51faKUnBR\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u5bc6\u5ea6\u4f30\u8ba1\u5b9a\u4f4d\u6709\u5bb3\u77e5\u8bc6\u5bc6\u96c6\u5c42\uff0c\u91c7\u7528\u5c42\u91cd\u63d2\u5165\u7b56\u7565\u5f7b\u5e95\u6d88\u9664\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6709\u5bb3\u77e5\u8bc6\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u96be\u4ee5\u5f7b\u5e95\u79fb\u9664\u6709\u5bb3\u77e5\u8bc6\uff0c\u6b8b\u7559\u77e5\u8bc6\u5bb9\u6613\u88ab\u6062\u590d\uff0c\u9700\u8981\u89e3\u51b3\u9690\u79c1\u3001\u5408\u89c4\u548c\u4f26\u7406\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u77e5\u8bc6\u5bc6\u5ea6\u4f30\u8ba1\u91cf\u5316\u6709\u5bb3\u77e5\u8bc6\u5206\u5e03\uff0c\u8bc6\u522b\u6709\u5bb3\u77e5\u8bc6\u5bc6\u96c6\u5c42\uff1b\u8bbe\u8ba1\u5c42\u91cd\u63d2\u5165\u7b56\u7565\uff0c\u5c06\u6709\u5bb3\u77e5\u8bc6\u5bc6\u96c6\u5c42\u63d0\u53d6\u5e76\u91cd\u63d2\u5165\u539f\u59cb\u6a21\u578b\uff0c\u7ed5\u8fc7\u8986\u76d6\u5c42\u68af\u5ea6\u963b\u585e\u3002", "result": "\u5728\u591a\u4e2a\u9057\u5fd8\u548c\u901a\u7528\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKUnBR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6548\u7528\u3002", "conclusion": "KUnBR\u901a\u8fc7\u7cbe\u786e\u5b9a\u4f4d\u548c\u6709\u6548\u68af\u5ea6\u4f20\u64ad\uff0c\u80fd\u591f\u5f7b\u5e95\u6d88\u9664\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6709\u5bb3\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.12020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12020", "abs": "https://arxiv.org/abs/2511.12020", "authors": ["Xianglong Shi", "Silin Cheng", "Sirui Zhao", "Yunhan Jiang", "Enhong Chen", "Yang Liu", "Sebastien Ourselin"], "title": "LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension", "comment": null, "summary": "Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\\%. The code is available at https://anonymous.4open.science/r/LIHE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u6846\u67b6LIHE\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u96f6\u4e2a\u6216\u591a\u4e2a\u76ee\u6807\u8868\u8fbe\u5f0f\u7684\u9650\u5236\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u7ed3\u5408\u53cc\u66f2\u51e0\u4f55\u548c\u6b27\u6c0f\u51e0\u4f55\u6765\u9632\u6b62\u8bed\u4e49\u5d29\u6e83\u3002", "motivation": "\u73b0\u6709\u7684\u5f31\u76d1\u7763\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e00\u5bf9\u4e00\u6620\u5c04\u5047\u8bbe\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u573a\u666f\u4e2d\u5bf9\u5e94\u96f6\u4e2a\u6216\u591a\u4e2a\u76ee\u6807\u7684\u8868\u8fbe\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u5b9e\u7528\u7684\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLIHE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u6307\u79f0\u89e3\u8026\u9636\u6bb5\u9884\u6d4b\u76ee\u6807\u5bf9\u8c61\u6570\u91cf\u5e76\u5206\u89e3\u590d\u6742\u8868\u8fbe\u5f0f\uff1b\u6307\u79f0\u5b9a\u4f4d\u9636\u6bb5\u4f7f\u7528HEMix\u6df7\u5408\u76f8\u4f3c\u5ea6\u6a21\u5757\uff0c\u7ed3\u5408\u6b27\u6c0f\u51e0\u4f55\u7684\u7cbe\u786e\u5bf9\u9f50\u80fd\u529b\u548c\u53cc\u66f2\u51e0\u4f55\u7684\u5c42\u6b21\u5efa\u6a21\u4f18\u52bf\u3002", "result": "\u5728gRefCOCO\u548cRef-ZOM\u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u9996\u4e2a\u6709\u6548\u7684\u5f31\u76d1\u7763WGREC\u57fa\u7ebf\uff0cHEMix\u5728\u6807\u51c6REC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u6539\u8fdb\uff0cIoU@0.5\u63d0\u5347\u9ad8\u8fbe2.5%\u3002", "conclusion": "LIHE\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f31\u76d1\u7763\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u76d1\u7763\u4fe1\u53f7\u6a21\u7cca\u548c\u8bed\u4e49\u8868\u793a\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u66f4\u5b9e\u7528\u7684\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11668", "abs": "https://arxiv.org/abs/2511.11668", "authors": ["Chase van de Geijn", "Ayush Paliwal", "Timo L\u00fcddecke", "Alexander S. Ecker"], "title": "Do traveling waves make good positional encodings?", "comment": null, "summary": "Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u6ce2\u7684\u65b0\u578b\u4f4d\u7f6e\u7f16\u7801\u673a\u5236RollPE\uff0c\u901a\u8fc7\u5faa\u73af\u6eda\u52a8\u64cd\u4f5c\u5728\u81ea\u6ce8\u610f\u529b\u4e2d\u5b9e\u73b0\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u4e0eRoPE\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u6765\u6355\u6349\u5e73\u79fb\u7b49\u53d8\u6027\u3002", "method": "\u4f7f\u7528\u5faa\u73af\u6eda\u52a8\u64cd\u4f5c\u5bf9\u67e5\u8be2\u548c\u952e\u5f20\u91cf\u8fdb\u884c\u5904\u7406\uff0c\u901a\u8fc7\u4f4d\u7f6e\u95f4\u7684\u76f8\u4f4d\u504f\u79fb\u5b9e\u73b0\u76f8\u5bf9\u4f4d\u7f6e\u5dee\u5f02\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "RollPE\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7edd\u5bf9\u4f4d\u7f6e\u5d4c\u5165\uff0c\u4e0eRoPE\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u5efa\u7acb\u4e86\u4e0eRoPE\u7684\u6570\u5b66\u7b49\u4ef7\u5173\u7cfb\u3002", "conclusion": "RollPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u53ef\u80fd\u6709\u52a9\u4e8e\u7406\u89e3\u5927\u8111\u4e2d\u7684\u4fe1\u606f\u6d41\u52a8\u8fc7\u7a0b\u3002"}}
{"id": "2511.12344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12344", "abs": "https://arxiv.org/abs/2511.12344", "authors": ["Baolong Bi", "Shenghua Liu", "Yiwei Wang", "Siqian Tong", "Lingrui Mei", "Yuyao Ge", "Yilong Xu", "Jiafeng Guo", "Xueqi Cheng"], "title": "Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.", "AI": {"tldr": "RGR-GRPO\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u5956\u52b1\u4fe1\u53f7\u548c\u79bb\u7ebf\u6307\u5bfc\uff0c\u5728\u591a\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u9886\u57df\u4e14\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u7eaf\u5728\u7ebfRL\u6846\u67b6\u9650\u5236\u4e86\u63a2\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u9650\u5236\u4e86\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51faRGR-GRPO\u6846\u67b6\uff0c\u5229\u7528\u8bc4\u5206\u6807\u51c6\u63d0\u4f9b\u5bc6\u96c6\u4fe1\u606f\u5956\u52b1\uff0c\u5728GRPO\u8bad\u7ec3\u671f\u95f4\u63a2\u7d22\u66f4\u5927\u7684\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u3002", "result": "\u572814\u4e2a\u591a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRGR-GRPO\u6301\u7eed\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u66ff\u4ee3\u5956\u52b1\u65b9\u6848\u6216\u79bb\u7ebf\u6307\u5bfc\u7684RL\u65b9\u6cd5\u3002\u76f8\u6bd4\u53ef\u9a8c\u8bc1\u5728\u7ebfRL\u57fa\u7ebf\uff0c\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u5347+7.0%\u3001+5.4%\u3001+8.4%\u548c+6.6%\u3002", "conclusion": "RGR-GRPO\u5728\u79bb\u7b56\u7565\u8bad\u7ec3\u671f\u95f4\u4fdd\u6301\u7a33\u5b9a\u7684\u71b5\u6ce2\u52a8\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684pass@k\u6027\u80fd\uff0c\u53cd\u6620\u4e86\u6301\u7eed\u63a2\u7d22\u548c\u6709\u6548\u7a81\u7834\u73b0\u6709\u6027\u80fd\u74f6\u9888\u7684\u80fd\u529b\u3002"}}
{"id": "2511.12024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12024", "abs": "https://arxiv.org/abs/2511.12024", "authors": ["Jose Reinaldo Cunha Santos A V Silva Neto", "Hodaka Kawachi", "Yasushi Yagi", "Tomoya Nakamura"], "title": "Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging", "comment": "8 pages without reference, 6 figures, 1 table", "summary": "State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Null-Space Diffusion Distillation (NSDD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u8303\u56f4\u7a7a\u95f4\u7ea6\u675f\u548c\u96f6\u7a7a\u95f4\u6269\u6563\u5148\u9a8c\u66f4\u65b0\uff0c\u5728\u65e0\u914d\u5bf9\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5feb\u901f\u3001\u903c\u771f\u7684\u65e0\u900f\u955c\u56fe\u50cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u900f\u955c\u76f8\u673a\u903c\u771f\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u914d\u5bf9\u7684\u65e0\u900f\u955c-\u6709\u900f\u955c\u76d1\u7763\uff0c\u8fd9\u4f1a\u56e0\u900f\u955c-\u65e0\u900f\u955c\u57df\u4e0d\u5339\u914d\u800c\u4ea7\u751f\u504f\u5dee\u3002\u4e3a\u4e86\u907f\u514d\u8fd9\u4e2a\u95ee\u9898\uff0c\u65e0\u771f\u5b9e\u6570\u636e\u7684\u6269\u6563\u5148\u9a8c\u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u901a\u7528\u65b9\u6cd5\u5728\u566a\u58f0\u3001\u9ad8\u5ea6\u590d\u7528\u4e14\u75c5\u6001\u7684\u65e0\u900f\u955c\u53cd\u5377\u79ef\u8bbe\u7f6e\u4e0b\u5f80\u5f80\u5931\u6548\u3002", "method": "\u5f15\u5165Null-Space Diffusion Distillation (NSDD)\uff1a\u4e00\u4e2a\u5355\u6b21\u901a\u8fc7\u7684\u5b66\u751f\u6a21\u578b\uff0c\u5b83\u84b8\u998f\u8fed\u4ee3DDNM+\u6c42\u89e3\u5668\u7684\u96f6\u7a7a\u95f4\u5206\u91cf\uff0c\u4ee5\u65e0\u900f\u955c\u6d4b\u91cf\u548c\u8303\u56f4\u7a7a\u95f4\u951a\u70b9\u4e3a\u6761\u4ef6\u3002NSDD\u4fdd\u6301\u6d4b\u91cf\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u914d\u5bf9\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u903c\u771f\u7ed3\u679c\u3002", "result": "\u5728Lensless-FFHQ\u548cPhlatCam\u6570\u636e\u96c6\u4e0a\uff0cNSDD\u662f\u7b2c\u4e8c\u5feb\u7684\u65b9\u6cd5\uff08\u4ec5\u6b21\u4e8eWiener\uff09\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6559\u5e08\u7684\u611f\u77e5\u8d28\u91cf\uff08\u7b2c\u4e8c\u597d\u7684LPIPS\uff0c\u4f4e\u4e8eDDNM+\uff09\uff0c\u4f18\u4e8eDPS\u548c\u7ecf\u5178\u51f8\u4f18\u5316\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4e86\u5b9e\u73b0\u5feb\u901f\u3001\u65e0\u771f\u5b9e\u6570\u636e\u3001\u903c\u771f\u7684\u65e0\u900f\u955c\u6210\u50cf\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.12026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12026", "abs": "https://arxiv.org/abs/2511.12026", "authors": ["Rulin Zhou", "Wenlong He", "An Wang", "Jianhang Zhang", "Xuanhui Zeng", "Xi Zhang", "Chaowei Zhu", "Haijun Hu", "Hongliang Ren"], "title": "Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark", "comment": "AAAI 2026 oral", "summary": "Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.", "AI": {"tldr": "VL-SurgPT\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u624b\u672f\u70b9\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u89c6\u89c9\u8ddf\u8e2a\u4e0e\u6587\u672c\u63cf\u8ff0\uff0c\u5305\u542b908\u4e2a\u4f53\u5185\u89c6\u9891\u7247\u6bb5\uff0c\u6db5\u76d6\u7ec4\u7ec7\u548c\u5668\u68b0\u8ddf\u8e2a\u3002\u63d0\u51faTG-SurgPT\u6587\u672c\u5f15\u5bfc\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\u6311\u6218\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u624b\u672f\u73af\u5883\u4e2d\u590d\u6742\u89c6\u89c9\u6761\u4ef6\uff08\u70df\u96fe\u906e\u6321\u3001\u955c\u9762\u53cd\u5c04\u3001\u7ec4\u7ec7\u53d8\u5f62\uff09\u4f7f\u7cbe\u786e\u70b9\u8ddf\u8e2a\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7406\u89e3\u8ddf\u8e2a\u5931\u8d25\u673a\u5236\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u5f15\u5165VL-SurgPT\u6570\u636e\u96c6\uff0c\u5305\u542b754\u4e2a\u7ec4\u7ec7\u8ddf\u8e2a\u89c6\u9891\uff0817,171\u4e2a\u6807\u6ce8\u70b9\uff09\u548c154\u4e2a\u5668\u68b0\u8ddf\u8e2a\u89c6\u9891\uff087\u79cd\u5668\u68b0\u7c7b\u578b\uff09\u3002\u5efa\u7acb8\u79cd\u6700\u5148\u8fdb\u8ddf\u8e2a\u65b9\u6cd5\u7684\u57fa\u51c6\uff0c\u63d0\u51faTG-SurgPT\u6587\u672c\u5f15\u5bfc\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u4e49\u63cf\u8ff0\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u878d\u5165\u70b9\u72b6\u6001\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u6311\u6218\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u7eaf\u89c6\u89c9\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u901a\u8fc7\u6865\u63a5\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\uff0cVL-SurgPT\u80fd\u591f\u5f00\u53d1\u4e0a\u4e0b\u6587\u611f\u77e5\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u8fd9\u5bf9\u63a8\u8fdb\u8ba1\u7b97\u673a\u8f85\u52a9\u624b\u672f\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5373\u4f7f\u5728\u6311\u6218\u6027\u672f\u4e2d\u6761\u4ef6\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6027\u80fd\u3002"}}
{"id": "2511.11671", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11671", "abs": "https://arxiv.org/abs/2511.11671", "authors": ["Alina Deriyeva", "Benjamin Paassen"], "title": "Evaluation of LLM-based Explanations for a Learning Analytics Dashboard", "comment": null, "summary": "Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5b66\u4e60\u5206\u6790\u4eea\u8868\u677f\u6570\u636e\u89e3\u91ca\u7684\u6548\u679c\uff0c\u53d1\u73b0LLM\u751f\u6210\u7684\u6280\u80fd\u72b6\u6001\u89e3\u91ca\u548c\u5b66\u4e60\u5efa\u8bae\u6bd4\u72ec\u7acb\u4eea\u8868\u677f\u6216\u6559\u5e08\u89e3\u91ca\u66f4\u53d7\u9752\u7750\u3002", "motivation": "\u5b66\u4e60\u5206\u6790\u4eea\u8868\u677f\u652f\u6301\u6570\u5b57\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d7\u6570\u636e\u53ef\u89e3\u91ca\u6027\u5f71\u54cd\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7LLM\u751f\u6210\u89e3\u91ca\u6765\u589e\u5f3a\u4eea\u8868\u677f\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5728\u4e13\u5bb6\u7814\u7a76\u4e2d\uff0c\u5c06LLM\u751f\u6210\u7684\u4eea\u8868\u677f\u6570\u636e\u89e3\u91ca\u4e0e\u72ec\u7acb\u4eea\u8868\u677f\u548c\u6559\u5e08\u63d0\u4f9b\u7684\u89e3\u91ca\u8fdb\u884c\u6bd4\u8f83\uff0c\u6d89\u53ca12\u540d\u5927\u5b66\u6559\u80b2\u5de5\u4f5c\u8005\u3002", "result": "LLM\u751f\u6210\u7684\u6280\u80fd\u72b6\u6001\u89e3\u91ca\u548c\u5b66\u4e60\u5efa\u8bae\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6761\u4ef6\uff0c\u66f4\u53d7\u53c2\u4e0e\u8005\u9752\u7750\u3002", "conclusion": "\u4f7f\u7528LLM\u8fdb\u884c\u6570\u636e\u89e3\u91ca\u53ef\u4ee5\u63d0\u5347\u5b66\u4e60\u4f53\u9a8c\uff0c\u540c\u65f6\u4fdd\u6301\u6559\u5e08\u8ba4\u53ef\u7684\u6559\u5b66\u6807\u51c6\u3002"}}
{"id": "2511.12378", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12378", "abs": "https://arxiv.org/abs/2511.12378", "authors": ["Dylan M. Asmar", "Mykel J. Kochenderfer"], "title": "Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making", "comment": "Under Review", "summary": "Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u52a8\u6001\u5b66\u4e60\u548c\u9002\u5e94\u4e0d\u540c\u5efa\u8bae\u8005\u53ef\u9760\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5efa\u8bae\u8005\u8d28\u91cf\u6574\u5408\u5230\u667a\u80fd\u4f53\u4fe1\u5ff5\u8868\u793a\u4e2d\uff0c\u5e76\u4f7f\u7528\u663e\u5f0f\u7684\u201c\u8be2\u95ee\u201d\u52a8\u4f5c\u6765\u7b56\u7565\u6027\u5730\u8bf7\u6c42\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5efa\u8bae\u8005\u7684\u8d28\u91cf\u53c2\u6570\u662f\u9759\u6001\u4e14\u5df2\u77e5\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u987a\u5e8f\u51b3\u7b56\u4efb\u52a1\u53ef\u4ee5\u4ece\u5916\u90e8\u884c\u52a8\u5efa\u8bae\u4e2d\u53d7\u76ca\uff0c\u4f46\u8fd9\u4e9b\u5efa\u8bae\u7684\u53ef\u9760\u6027\u5404\u4e0d\u76f8\u540c\u3002", "method": "1. \u5c06\u5efa\u8bae\u8005\u8d28\u91cf\u76f4\u63a5\u6574\u5408\u5230\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u8868\u793a\u4e2d\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u63a8\u65ad\u5efa\u8bae\u8005\u7c7b\u578b\uff1b2. \u5f15\u5165\u663e\u5f0f\u7684\u201c\u8be2\u95ee\u201d\u52a8\u4f5c\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u5173\u952e\u65f6\u523b\u7b56\u7565\u6027\u5730\u8bf7\u6c42\u5efa\u8bae\uff0c\u5e73\u8861\u4fe1\u606f\u83b7\u53d6\u4e0e\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u5efa\u8bae\u8005\u8d28\u91cf\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u80fd\u591f\u9002\u5e94\u53d8\u5316\u7684\u53ef\u9760\u6027\uff0c\u5e76\u7b56\u7565\u6027\u5730\u7ba1\u7406\u5efa\u8bae\u8bf7\u6c42\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u89e3\u51b3\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5efa\u8bae\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11673", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11673", "abs": "https://arxiv.org/abs/2511.11673", "authors": ["M. A. Gameiro"], "title": "Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture", "comment": null, "summary": "This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u534f\u540c\u878d\u5408\u5c42\uff08SFL\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u5c06\u590d\u6742\u7684\u6df1\u5ea6\u8bed\u4e49\u7279\u5f81\u4e0e\u7b80\u5355\u7684\u7ed3\u6784\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u6b4c\u8bcd\u5185\u5bb9\u5206\u7c7b\uff0c\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u53ef\u9760\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u5c06\u590d\u6742\u7684\u9ad8\u7ef4\u6df1\u5ea6\u8bed\u4e49\u7279\u5f81\u4e0e\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u7ebf\u7d22\u6709\u6548\u6574\u5408\uff0c\u4ee5\u63d0\u5347\u6b4c\u8bcd\u5185\u5bb9\u5206\u7c7b\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u534f\u540c\u878d\u5408\u5c42\uff08SFL\uff09\u67b6\u6784\uff0c\u4f7f\u7528\u95e8\u63a7\u673a\u5236\u6765\u8c03\u8282Sentence-BERT\u5d4c\u5165\uff08Fdeep\uff09\u4e0e\u4f4e\u7ef4\u8f85\u52a9\u7279\u5f81\uff08Fstruct\uff09\u7684\u878d\u5408\uff0c\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e8c\u5206\u7c7b\u95ee\u9898\u3002", "result": "SFL\u6a21\u578b\u8fbe\u52300.9894\u7684\u51c6\u786e\u7387\u548c\u5b8fF1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u7279\u5f81\u62fc\u63a5\u7684\u968f\u673a\u68ee\u6797\u57fa\u7ebf\uff08\u51c6\u786e\u73870.9868\uff09\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e93%\uff0c\u5bf9\u6570\u635f\u5931\u964d\u4f4e2.5\u500d\u3002", "conclusion": "\u975e\u7ebf\u6027\u95e8\u63a7\u673a\u5236\u4f18\u4e8e\u7b80\u5355\u7684\u7279\u5f81\u62fc\u63a5\uff0cSFL\u6a21\u578b\u4e3a\u590d\u6742\u591a\u6a21\u6001\u6b4c\u8bcd\u5206\u6790\u63d0\u4f9b\u4e86\u7a33\u5065\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12439", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12439", "abs": "https://arxiv.org/abs/2511.12439", "authors": ["Yujia Liu", "Sophia Yu", "Hongyue Jin", "Jessica Wen", "Alexander Qian", "Terrence Lee", "Mattheus Ramsis", "Gi Won Choi", "Lianhui Qin", "Xin Liu", "Edward J. Wang"], "title": "Multi-agent Self-triage System with Medical Flowcharts", "comment": null, "summary": "Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e34\u5e8a\u9a8c\u8bc1\u6d41\u7a0b\u56fe\u7684\u5bf9\u8bdd\u5f0f\u81ea\u6211\u5206\u8bca\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b095.29%\u7684\u6d41\u7a0b\u56fe\u68c0\u7d22\u51c6\u786e\u7387\u548c99.10%\u7684\u5bfc\u822a\u51c6\u786e\u7387\uff0c\u7ed3\u5408\u81ea\u7531\u6587\u672c\u4ea4\u4e92\u7684\u7075\u6d3b\u6027\u548c\u6807\u51c6\u5316\u4e34\u5e8a\u534f\u8bae\u7684\u4e25\u8c28\u6027\u3002", "motivation": "\u5728\u7ebf\u5065\u5eb7\u8d44\u6e90\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u53ef\u9760\u6027\u53d7\u5230\u51c6\u786e\u6027\u4f4e\u3001\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u6613\u53d7\u672a\u7ecf\u9a8c\u8bc1\u4fe1\u606f\u5f71\u54cd\u7684\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u53ef\u9760\u7684AI\u8f85\u52a9\u81ea\u6211\u5206\u8bca\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u7f8e\u56fd\u533b\u5b66\u4f1a100\u4e2a\u4e34\u5e8a\u9a8c\u8bc1\u6d41\u7a0b\u56fe\u7684\u5bf9\u8bdd\u5f0f\u81ea\u6211\u5206\u8bca\u7cfb\u7edf\uff0c\u91c7\u7528\u68c0\u7d22\u667a\u80fd\u4f53\u3001\u51b3\u7b56\u667a\u80fd\u4f53\u548c\u804a\u5929\u667a\u80fd\u4f53\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u5927\u89c4\u6a21\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u5728\u6d41\u7a0b\u56fe\u68c0\u7d22\u65b9\u9762\u8fbe\u523095.29%\u7684top-3\u51c6\u786e\u7387\uff08N=2,000\uff09\uff0c\u5728\u6d41\u7a0b\u56fe\u5bfc\u822a\u65b9\u9762\u8fbe\u523099.10%\u7684\u51c6\u786e\u7387\uff08N=37,200\uff09\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5bf9\u8bdd\u98ce\u683c\u548c\u6761\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u900f\u660e\u3001\u51c6\u786e\u4e14\u53ef\u63a8\u5e7f\u7684AI\u8f85\u52a9\u81ea\u6211\u5206\u8bca\u7684\u53ef\u884c\u6027\uff0c\u6709\u6f5c\u529b\u652f\u6301\u77e5\u60c5\u60a3\u8005\u51b3\u7b56\u5e76\u6539\u5584\u533b\u7597\u8d44\u6e90\u5229\u7528\u3002"}}
{"id": "2511.11767", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11767", "abs": "https://arxiv.org/abs/2511.11767", "authors": ["Amisha Priyadarshini", "Sergio Gago-Masague"], "title": "Learning Fair Representations with Kolmogorov-Arnold Networks", "comment": null, "summary": "Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Kolmogorov-Arnold Networks\uff08KANs\uff09\u96c6\u6210\u5230\u516c\u5e73\u5bf9\u6297\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u60e9\u7f5a\u66f4\u65b0\u673a\u5236\u52a8\u6001\u8c03\u6574\u516c\u5e73\u7ea6\u675f\uff0c\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u5b66\u4e60\u6a21\u578b\u5728\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u8fbe\u5230\u6700\u4f18\u5e73\u8861\uff0c\u4e14\u9ed1\u76d2\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5728\u654f\u611f\u9886\u57df\uff08\u5982\u5927\u5b66\u62db\u751f\uff09\u7684\u5e94\u7528\u3002", "method": "\u5c06KANs\u96c6\u6210\u5230\u516c\u5e73\u5bf9\u6297\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5229\u7528KANs\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u60e9\u7f5a\u66f4\u65b0\u673a\u5236\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u516c\u5e73\u7ea6\u675f\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u5927\u5b66\u62db\u751f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u4e0d\u540c\u4f18\u5316\u7b56\u7565\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u516c\u5e73\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8de8\u654f\u611f\u5c5e\u6027\u7684\u7ade\u4e89\u6027\u516c\u5e73\u3002", "conclusion": "KANs\u5728\u516c\u5e73\u673a\u5668\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5e73\u8861\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u4e3a\u654f\u611f\u51b3\u7b56\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12030", "abs": "https://arxiv.org/abs/2511.12030", "authors": ["Jun Zhou", "Chi Xu", "Kaifeng Tang", "Yuting Ge", "Tingrui Guo", "Li Cheng"], "title": "VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation", "comment": "14 pages, 9 figures, extended version of the AAAI 2026 paper \"VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation\"", "summary": "Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u7269\u7406\u7ebf\u7d22\u7684\u8054\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20RGB\u56fe\u50cf\u4f30\u8ba1\u624b\u548c\u7269\u4f53\u76843D\u59ff\u6001\uff0c\u901a\u8fc7\u89c6\u89c9-\u7269\u7406\u7ebf\u7d22\u8054\u5408\u5b66\u4e60\u548c\u5019\u9009\u59ff\u6001\u805a\u5408\uff0c\u5b9e\u73b0\u89c6\u89c9\u4e00\u81f4\u4e14\u7269\u7406\u5408\u7406\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u5f80\u5f80\u4ea7\u751f\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u7684\u7ed3\u679c\uff0c\u800c\u73b0\u6709\u7684\u7269\u7406\u63a8\u7406\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u540e\u4f18\u5316\u6216\u4e0d\u53ef\u5fae\u7269\u7406\u5f15\u64ce\uff0c\u5f71\u54cd\u89c6\u89c9\u4e00\u81f4\u6027\u548c\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u6027\u3002", "method": "1) \u89c6\u89c9-\u7269\u7406\u7ebf\u7d22\u8054\u5408\u5b66\u4e60\uff1a\u8bad\u7ec3\u6a21\u578b\u63d0\u53d62D\u89c6\u89c9\u7ebf\u7d22\u548c3D\u7269\u7406\u7ebf\u7d22\uff1b2) \u5019\u9009\u59ff\u6001\u805a\u5408\uff1a\u5229\u7528\u6269\u6563\u751f\u6210\u7684\u591a\u4e2a\u5019\u9009\u59ff\u6001\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u7269\u7406\u9884\u6d4b\u8fdb\u884c\u805a\u5408\u4f18\u5316\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u6001\u7cbe\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u89c6\u89c9-\u7269\u7406\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u624b-\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u7269\u7406\u7ea6\u675f\u95ee\u9898\uff0c\u5b9e\u73b0\u89c6\u89c9\u4e00\u81f4\u4e14\u7269\u7406\u5408\u7406\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.12485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12485", "abs": "https://arxiv.org/abs/2511.12485", "authors": ["Pengze Li", "Jiaqi Liu", "Junchi Yu", "Lihao Liu", "Mingyu Ding", "Wanli Ouyang", "Shixiang Tang", "Xi Chen"], "title": "ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction", "comment": "Accepted to AAAI 2026", "summary": "Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.", "AI": {"tldr": "\u63d0\u51faARCHE\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u5c06\u590d\u6742\u63a8\u7406\u5206\u89e3\u4e3a\u6807\u51c6\u63a8\u7406\u8303\u5f0f\u7684\u903b\u8f91\u6811\uff0c\u5e76\u53d1\u5e03ARCHE Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u663e\u793a\u5f53\u524dLLMs\u5728\u79d1\u5b66\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u79d1\u5b66\u9886\u57df\u4f7f\u7528\u65f6\u4ea7\u751f\u7684\u975e\u7ed3\u6784\u5316\u3001\u975e\u6b63\u5f0f\u63a8\u7406\u5185\u5bb9\uff0c\u96be\u4ee5\u5224\u65ad\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u79d1\u5b66\u63a8\u7406\u57fa\u672c\u8303\u5f0f\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u6f5c\u5728\u63a8\u7406\u94fe\u63d0\u53d6\u4efb\u52a1(ARCHE)\uff0c\u8981\u6c42\u6a21\u578b\u5c06\u590d\u6742\u63a8\u7406\u5206\u89e3\u4e3a\u63a8\u7406\u903b\u8f91\u6811(RLT)\uff0c\u5176\u4e2d\u6240\u6709\u63a8\u7406\u6b65\u9aa4\u660e\u786e\u5206\u7c7b\u4e3a\u6f14\u7ece\u3001\u5f52\u7eb3\u6216\u6eaf\u56e0\u4e09\u79cd\u57fa\u672c\u63a8\u7406\u6a21\u5f0f\u3002", "result": "\u5728ARCHE Bench\u57fa\u51c6\u4e0a\u8bc4\u4f3010\u4e2a\u9886\u5148LLMs\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u63a8\u7406\u8fb9\u51c6\u786e\u6027\u548c\u5b9e\u4f53\u8986\u76d6\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u6ca1\u6709\u6a21\u578b\u80fd\u591f\u63d0\u53d6\u5b8c\u6574\u4e14\u6807\u51c6\u7684\u63a8\u7406\u94fe\u3002", "conclusion": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u7684\u80fd\u529b\u4e0e\u79d1\u5b66\u8bba\u8bc1\u6240\u9700\u7684\u4e25\u8c28\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u5bf9\u57fa\u672c\u63a8\u7406\u8303\u5f0f\u7684\u7406\u89e3\u3002"}}
{"id": "2511.12032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12032", "abs": "https://arxiv.org/abs/2511.12032", "authors": ["Guotao Liang", "Baoquan Zhang", "Zhiyuan Wen", "Zihao Han", "Yunming Ye"], "title": "Improved Masked Image Generation with Knowledge-Augmented Token Representations", "comment": "AAAI-26", "summary": "Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.", "AI": {"tldr": "KA-MIG\u662f\u4e00\u4e2a\u77e5\u8bc6\u589e\u5f3a\u7684\u63a9\u7801\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e09\u79cd\u7c7b\u578b\u7684token\u77e5\u8bc6\u56fe\uff08\u5171\u73b0\u56fe\u3001\u8bed\u4e49\u76f8\u4f3c\u56fe\u3001\u4f4d\u7f6etoken\u4e0d\u517c\u5bb9\u56fe\uff09\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u6765\u589e\u5f3a\u6a21\u578b\u6355\u6349\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6a21\u578b\u81ea\u8eab\u5b66\u4e60\u89c6\u89c9token\u5e8f\u5217\u7684\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u7531\u4e8e\u5355\u4e2atoken\u7f3a\u4e4f\u660e\u786e\u8bed\u4e49\u542b\u4e49\u4e14\u5e8f\u5217\u901a\u5e38\u8f83\u957f\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u8fd9\u4e9b\u4f9d\u8d56\u5173\u7cfb\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faKA-MIG\u6846\u67b6\uff0c\u6784\u5efa\u4e09\u79cdtoken\u77e5\u8bc6\u56fe\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u56fe\u611f\u77e5\u7f16\u7801\u5668\u5b66\u4e60token\u548c\u4f4d\u7f6e\u611f\u77e5\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u878d\u5408\u673a\u5236\u5c06\u8fd9\u4e9b\u4e30\u5bcc\u8868\u793a\u96c6\u6210\u5230\u73b0\u6709MIG\u65b9\u6cd5\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728ImageNet\u4e0a\u7684\u7c7b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709MIG\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165token\u7ea7\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\u7684\u663e\u5f0f\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\uff0cKA-MIG\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u578b\u6355\u6349\u8bed\u4e49\u4f9d\u8d56\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2511.11676", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11676", "abs": "https://arxiv.org/abs/2511.11676", "authors": ["Hanchen David Wang", "Siwoo Bae", "Zirong Chen", "Meiyi Ma"], "title": "Learning with Preserving for Continual Multitask Learning", "comment": "25 pages, 16 figures, accepted at AAAI-2026", "summary": "Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Learning with Preserving (LwP)\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u6301\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u6765\u89e3\u51b3\u6301\u7eed\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u653e\u7f13\u51b2\u533a\uff0c\u5728\u9690\u79c1\u654f\u611f\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u533b\u7597\u5f71\u50cf\u5206\u6790\u7b49\u5173\u952e\u9886\u57df\uff0cAI\u7cfb\u7edf\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u800c\u4e0d\u9057\u5fd8\u5df2\u5b66\u80fd\u529b\u3002\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u6b64\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u5b66\u4e60\u788e\u7247\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\uff0c\u5bfc\u81f4\u7279\u5f81\u76f8\u4e92\u5e72\u6270\u3002", "method": "\u5f15\u5165LwP\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u52a8\u6001\u52a0\u6743\u8ddd\u79bb\u4fdd\u6301(DWDP)\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u6f5c\u5728\u6570\u636e\u8868\u793a\u4e4b\u95f4\u7684\u6210\u5bf9\u8ddd\u79bb\u6765\u9632\u6b62\u8868\u793a\u6f02\u79fb\uff0c\u4fdd\u6301\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cLwP\u4e0d\u4ec5\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u4e14\u5728CMTL\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u5206\u5e03\u504f\u79fb\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LwP\u662f\u552f\u4e00\u8d85\u8d8a\u5f3a\u5355\u4efb\u52a1\u5b66\u4e60\u57fa\u7ebf\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u9002\u5408\u9690\u79c1\u654f\u611f\u5e94\u7528\u3002"}}
{"id": "2511.12563", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12563", "abs": "https://arxiv.org/abs/2511.12563", "authors": ["Eljas Linna", "Kestutis Baltakys", "Alexandros Iosifidis", "Juho Kanniainen"], "title": "LOBERT: Generative AI Foundation Model for Limit Order Book Messages", "comment": "Submission for NeurIPS 2025 GenAI in Finance Workshop", "summary": "Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.", "AI": {"tldr": "LOBERT\u662f\u4e00\u4e2a\u7528\u4e8e\u9650\u4ef7\u8ba2\u5355\u7c3f\u6570\u636e\u7684\u901a\u7528\u7f16\u7801\u5668\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6807\u8bb0\u5316\u65b9\u6848\u5904\u7406\u591a\u7ef4\u6d88\u606f\uff0c\u5728\u9884\u6d4b\u4e2d\u95f4\u4ef7\u683c\u53d8\u52a8\u548c\u4e0b\u4e00\u6d88\u606f\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u9886\u5148\u3002", "motivation": "\u73b0\u6709LOB\u6a21\u578b\u9700\u8981\u7e41\u7410\u7684\u6570\u636e\u8868\u793a\uff0c\u7f3a\u4e4f\u539f\u59cb\u4efb\u52a1\u4e4b\u5916\u7684\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u9002\u7528\u4e8e\u4e0b\u6e38\u5fae\u8c03\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u3002", "method": "LOBERT\u57fa\u4e8eBERT\u67b6\u6784\uff0c\u91c7\u7528\u65b0\u9896\u7684\u6807\u8bb0\u5316\u65b9\u6848\uff0c\u5c06\u5b8c\u6574\u7684\u591a\u7ef4\u6d88\u606f\u4f5c\u4e3a\u5355\u4e2a\u6807\u8bb0\u5904\u7406\uff0c\u540c\u65f6\u4fdd\u7559\u4ef7\u683c\u3001\u6570\u91cf\u548c\u65f6\u95f4\u7684\u8fde\u7eed\u8868\u793a\u3002", "result": "LOBERT\u5728\u9884\u6d4b\u4e2d\u95f4\u4ef7\u683c\u53d8\u52a8\u548c\u4e0b\u4e00\u6d88\u606f\u7b49\u4efb\u52a1\u4e2d\u8fbe\u5230\u9886\u5148\u6027\u80fd\uff0c\u540c\u65f6\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u51cf\u5c11\u4e86\u6240\u9700\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "conclusion": "LOBERT\u4e3aLOB\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u5177\u6709\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2511.12034", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.12034", "abs": "https://arxiv.org/abs/2511.12034", "authors": ["Xiaohao Liu", "Xiaobo Xia", "Jiaheng Wei", "Shuo Yang", "Xiu Su", "See-Kiong Ng", "Tat-Seng Chua"], "title": "Calibrated Multimodal Representation Learning with Missing Modalities", "comment": null, "summary": "Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86CalMRL\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\u56e0\u6a21\u6001\u7f3a\u5931\u5bfc\u81f4\u7684\u5bf9\u9f50\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u8868\u793a\u7ea7\u63d2\u8865\u548c\u53cc\u6b65\u5b66\u4e60\u6765\u6821\u51c6\u4e0d\u5b8c\u6574\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u6240\u6709\u6a21\u6001\u90fd\u5b58\u5728\u624d\u80fd\u5b9e\u73b0\u6709\u6548\u5bf9\u9f50\uff0c\u800c\u5b9e\u9645\u6570\u636e\u96c6\u4e2d\u666e\u904d\u5b58\u5728\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u5bfc\u81f4\u5bf9\u9f50\u504f\u5dee\u3002", "method": "\u5229\u7528\u6a21\u6001\u5148\u9a8c\u548c\u5185\u5728\u8054\u7cfb\u5728\u8868\u793a\u7ea7\u5bf9\u7f3a\u5931\u6a21\u6001\u8fdb\u884c\u5efa\u6a21\uff0c\u91c7\u7528\u53cc\u6b65\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u5171\u4eab\u6f5c\u5728\u53d8\u91cf\u7684\u540e\u9a8c\u5206\u5e03\u95ed\u5f0f\u89e3\u6765\u89e3\u51b3\u4f18\u5316\u56f0\u5883\u3002", "result": "\u7406\u8bba\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u80fd\u591f\u7f13\u89e3\u951a\u70b9\u504f\u79fb\u5e76\u4fdd\u8bc1\u6536\u655b\uff0c\u5b9e\u9a8c\u8bc1\u660eCalMRL\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u5438\u6536\u7f3a\u5931\u6a21\u6001\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "CalMRL\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\u6a21\u6001\u7f3a\u5931\u5bfc\u81f4\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u6821\u51c6\u5bf9\u9f50\u673a\u5236\u63d0\u5347\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2511.11677", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11677", "abs": "https://arxiv.org/abs/2511.11677", "authors": ["Shimiao Li", "Aaron Tuor", "Draguna Vrabie", "Larry Pileggi", "Jan Drgona"], "title": "Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow", "comment": "paper submitted to PES General Meeting 2026", "summary": "Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \\textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540c\u4f26\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u53c2\u6570\u5316\u4ea4\u6d41\u6700\u4f18\u6f6e\u6d41\u95ee\u9898\u7684\u4f18\u5316\u6311\u6218\uff0c\u901a\u8fc7\u6784\u5efa\u4ece\u677e\u5f1b\u95ee\u9898\u5230\u539f\u59cb\u95ee\u9898\u7684\u8fde\u7eed\u53d8\u5f62\u8fc7\u7a0b\u6765\u6539\u5584\u6536\u655b\u7a33\u5b9a\u6027\u548c\u53ef\u884c\u6027\u3002", "motivation": "\u4ea4\u6d41\u6700\u4f18\u6f6e\u6d41\u95ee\u9898\u7684\u56fa\u6709\u975e\u51f8\u6027\u5bfc\u81f4\u4f18\u5316\u666f\u89c2\u590d\u6742\uff0c\u6807\u51c6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6536\u655b\u5230\u53ef\u884c\u7684\u9ad8\u8d28\u91cf\u89e3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u6536\u655b\u7a33\u5b9a\u6027\u548c\u53ef\u884c\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u540c\u4f26\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6784\u5efa\u76ee\u6807\u548c\u7ea6\u675f\u7684\u8fde\u7eed\u53d8\u5f62\uff0c\u4ece\u5177\u6709\u5e7f\u6cdb\u5438\u5f15\u76c6\u7684\u677e\u5f1b\u95ee\u9898\u9010\u6b65\u8f6c\u53d8\u4e3a\u539f\u59cb\u95ee\u9898\u3002", "result": "\u5728\u6807\u51c6IEEE\u4ea4\u6d41\u6700\u4f18\u6f6e\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u975e\u540c\u4f26\u57fa\u7ebf\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u884c\u6027\u7387\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u5b8c\u6574\u6700\u4f18\u6f6e\u6d41\u6c42\u89e3\u5668\u76f8\u5f53\u7684\u76ee\u6807\u503c\u3002", "conclusion": "\u540c\u4f26\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u7535\u529b\u7cfb\u7edf\u4f18\u5316\u4e2d\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u3001\u7ea6\u675f\u611f\u77e5\u7684\u5b66\u4e60\u4f18\u5316\u6f5c\u529b\u3002"}}
{"id": "2511.12579", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12579", "abs": "https://arxiv.org/abs/2511.12579", "authors": ["Yongwen Ren", "Chao Wang", "Peng Du", "Chuan Qin", "Dazhong Shen", "Hui Xiong"], "title": "Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models", "comment": null, "summary": "Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.", "AI": {"tldr": "PCRS-TKA\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u3001\u77e5\u8bc6\u7b5b\u9009\u548c\u534f\u4f5c\u504f\u597d\u5efa\u6a21\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528PLM\u5728\u56fe\u5173\u7cfb\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e0d\u52a0\u533a\u5206\u5730\u6574\u5408\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\uff0c\u4e14\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5ffd\u89c6\u4e86\u534f\u4f5c\u504f\u597d\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5bf9\u8bdd\u7279\u5b9a\u7684\u77e5\u8bc6\u6811\u5e76\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u63a8\u7406\uff1b\u9009\u62e9\u6027\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u76f8\u5173\u77e5\u8bc6\uff1b\u4f7f\u7528\u4e13\u95e8\u76d1\u7763\u4fe1\u53f7\u663e\u5f0f\u5efa\u6a21\u534f\u4f5c\u504f\u597d\uff1b\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\u534f\u8c03\u5f02\u6784\u8f93\u5165\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660ePCRS-TKA\u5728\u63a8\u8350\u548c\u5bf9\u8bdd\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PCRS-TKA\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u63a8\u7406\u3001\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7b5b\u9009\u548c\u534f\u4f5c\u504f\u597d\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u5bf9\u8bdd\u8d28\u91cf\u3002"}}
{"id": "2511.13238", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.13238", "abs": "https://arxiv.org/abs/2511.13238", "authors": ["Patrick Parschan", "Charlott Jakob"], "title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms", "comment": "46 pages, 8 figures, 2 tables, accepted for publication in Quality & Quantity", "summary": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8e\u6587\u672c\u7684\u65e0\u76d1\u7763\u548c\u534a\u76d1\u7763\u7406\u60f3\u70b9\u4f30\u8ba1\u7b97\u6cd5\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u7528\u4e8e\u4ece\u6587\u672c\u6570\u636e\u63a8\u65ad\u6f5c\u5728\u653f\u6cbb\u7acb\u573a\uff0c\u5728\u653f\u6cbb\u5b66\u3001\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\u3002", "motivation": "\u8fc7\u53bb\u4e8c\u5341\u5e74\u4e2d\uff0cCT-IPE\u7b97\u6cd5\u7684\u53d1\u5c55\u7d27\u8ddfNLP\u8d8b\u52bf\uff0c\u4ece\u8bcd\u9891\u6a21\u578b\u53d1\u5c55\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u8fd9\u4e00\u53d1\u5c55\u8f68\u8ff9\u5bfc\u81f4\u4e86\u9886\u57df\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u548c\u5e94\u7528\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u56de\u987e\u8bc6\u522b\u4e8625\u79cdCT-IPE\u7b97\u6cd5\uff0c\u8fdb\u884c\u624b\u52a8\u5185\u5bb9\u5206\u6790\uff0c\u5e76\u5f15\u5165\u6982\u5ff5\u6846\u67b6\u533a\u5206\u7b97\u6cd5\u5982\u4f55\u751f\u6210\u3001\u6355\u83b7\u548c\u805a\u5408\u6587\u672c\u65b9\u5dee\uff0c\u8bc6\u522b\u51fa\u56db\u79cd\u65b9\u6cd5\u5bb6\u65cf\u3002", "result": "\u8bc6\u522b\u4e86\u8bcd\u9891\u3001\u4e3b\u9898\u5efa\u6a21\u3001\u8bcd\u5d4c\u5165\u548cLLM\u56db\u79cd\u65b9\u6cd5\u5bb6\u65cf\uff0c\u5e76\u6279\u5224\u6027\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u5047\u8bbe\u3001\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u7b97\u6cd5\u53d1\u5c55\u7684\u7ed3\u6784\u5316\u7efc\u5408\uff0c\u4e3a\u5e94\u7528\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u7b97\u6cd5\u9009\u62e9\u4e2d\u7684\u6743\u8861\uff0c\u5e76\u6307\u51fa\u4e0d\u540c\u7b97\u6cd5\u4f30\u8ba1\u7ed3\u679c\u7684\u5dee\u5f02\u672c\u8eab\u5177\u6709\u4fe1\u606f\u4ef7\u503c\uff0c\u9700\u8981\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2511.12040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12040", "abs": "https://arxiv.org/abs/2511.12040", "authors": ["Xinyuan Hu", "Changyue Shi", "Chuxiao Yang", "Minghao Chen", "Jiajun Ding", "Tao Wei", "Chen Wei", "Zhou Yu", "Min Tan"], "title": "SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images", "comment": "AAAI2026-Oral. Project Page: https://xinyuanhu66.github.io/SRSplat/", "summary": "Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \\textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \\textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \\textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \\textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.", "AI": {"tldr": "SRSplat\u662f\u4e00\u4e2a\u524d\u9988\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5c11\u91cf\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u91cd\u5efa\u9ad8\u5206\u8fa8\u73873D\u573a\u666f\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u548c\u5185\u90e8\u7eb9\u7406\u7ebf\u7d22\u6765\u8865\u507f\u7eb9\u7406\u4fe1\u606f\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ece\u7a00\u758f\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u8fdb\u884c3D\u91cd\u5efa\u65f6\u5f80\u5f80\u65e0\u6cd5\u6062\u590d\u7cbe\u7ec6\u7eb9\u7406\u7ec6\u8282\uff0c\u8fd9\u6e90\u4e8e\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u4e2d\u9ad8\u9891\u4fe1\u606f\u7684\u56fa\u6709\u7f3a\u4e4f\u3002", "method": "\u6784\u5efa\u573a\u666f\u7279\u5b9a\u53c2\u8003\u56fe\u5e93\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\uff1b\u5f15\u5165\u53c2\u8003\u5f15\u5bfc\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u5bf9\u9f50\u548c\u878d\u5408\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u56fe\u50cf\u4e0e\u53c2\u8003\u56fe\u50cf\u7684\u7279\u5f81\uff1b\u8bad\u7ec3\u89e3\u7801\u5668\u9884\u6d4b\u9ad8\u65af\u57fa\u5143\uff1b\u4f7f\u7528\u7eb9\u7406\u611f\u77e5\u5bc6\u5ea6\u63a7\u5236\u81ea\u9002\u5e94\u8c03\u6574\u9ad8\u65af\u5bc6\u5ea6\u3002", "result": "\u5728RealEstate10K\u3001ACID\u548cDTU\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSRSplat\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u548c\u8de8\u5206\u8fa8\u7387\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SRSplat\u901a\u8fc7\u6709\u6548\u6574\u5408\u5916\u90e8\u53c2\u8003\u4fe1\u606f\u548c\u5185\u90e8\u7eb9\u7406\u7ebf\u7d22\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u7a00\u758f\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u91cd\u5efa\u9ad8\u5206\u8fa8\u73873D\u573a\u666f\u65f6\u7eb9\u7406\u7ec6\u8282\u6062\u590d\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2511.12677", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12677", "abs": "https://arxiv.org/abs/2511.12677", "authors": ["Oliver Joergensen", "Dominik Drexler", "Jendrik Seipp"], "title": "Dynamic Tree Databases in Automated Planning", "comment": null, "summary": "A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6811\u6570\u636e\u5e93\u53d8\u4f53\uff0c\u7528\u4e8e\u538b\u7f29\u547d\u9898\u548c\u6570\u503c\u53d8\u91cf\u7684\u72b6\u6001\u96c6\uff0c\u5728\u4fdd\u6301\u9759\u6001\u7248\u672c\u4f18\u70b9\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u5185\u5b58\u9884\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u6269\u5c55\u663e\u5f0f\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u5982\u4f55\u7d27\u51d1\u8868\u793a\u751f\u6210\u7684\u72b6\u6001\u96c6\u3002\u9759\u6001\u6811\u6570\u636e\u5e93\u867d\u7136\u6bcf\u4e2a\u72b6\u6001\u53ea\u9700\u8981\u5e38\u91cf\u7a7a\u95f4\uff0c\u4f46\u9700\u8981\u5927\u91cf\u5185\u5b58\u9884\u5206\u914d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u52a8\u6001\u6811\u6570\u636e\u5e93\u53d8\u4f53\uff0c\u7528\u4e8e\u538b\u7f29\u547d\u9898\u548c\u6570\u503c\u53d8\u91cf\u7684\u72b6\u6001\u96c6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4fdd\u6301\u9759\u6001\u5bf9\u5e94\u7248\u672c\u7684\u7406\u60f3\u7279\u6027\u3002", "result": "\u5728\u7ecf\u5178\u548c\u6570\u503c\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u538b\u7f29\u6bd4\u8fbe\u5230\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u8fd0\u884c\u65f6\u5f00\u9500\u901a\u5e38\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u52a8\u6001\u6811\u6570\u636e\u5e93\u5728\u72b6\u6001\u538b\u7f29\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5927\u89c4\u6a21\u4efb\u52a1\u7684\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12044", "abs": "https://arxiv.org/abs/2511.12044", "authors": ["Cheng-Chang Tsai", "Kai-Wen Cheng", "Chun-Shien Lu"], "title": "FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification", "comment": "Extended version. 22 pages, 18 figures, 6 tables", "summary": "Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.", "AI": {"tldr": "\u63d0\u51fa\u4e86FedSDA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u67d3\u8272\u5206\u79bb\u6280\u672f\u6765\u5bf9\u9f50\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u7684\u67d3\u8272\u5206\u5e03\uff0c\u4ee5\u7f13\u89e3\u5ba2\u6237\u7aef\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u5206\u5e03\u662f\u4e0d\u53ef\u907f\u514d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\uff0c\u7279\u5f81\u5206\u5e03\u504f\u79fb\u4f1a\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u62df\u5408\u6570\u636e\u5206\u5e03\u7684\u80fd\u529b\uff0c\u5229\u7528\u67d3\u8272\u5206\u79bb\u63d0\u53d6\u4e0e\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u975eIID\u7279\u6027\u76f8\u5173\u7684\u5173\u952e\u7279\u5f81\uff0c\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u5bf9\u9f50\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u67d3\u8272\u5206\u5e03\u5230\u76ee\u6807\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eFedSDA\u4e0d\u4ec5\u6709\u6548\u6539\u5584\u4e86\u5173\u6ce8\u5ba2\u6237\u7aef\u6a21\u578b\u66f4\u65b0\u5dee\u5f02\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fd8\u4f18\u4e8e\u4ece\u6570\u636e\u5206\u5e03\u89d2\u5ea6\u89e3\u51b3\u975eIID\u95ee\u9898\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedSDA\u4e3a\u8ba1\u7b97\u75c5\u7406\u5b66\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u7528\u89c1\u89e3\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u975eIID\u6570\u636e\u95ee\u9898\u3002"}}
{"id": "2511.11680", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11680", "abs": "https://arxiv.org/abs/2511.11680", "authors": ["Udaya Bhasker Cheerala", "Varun Teja Chirukuri", "Venkata Akhil Kumar Gummadi", "Jintu Moni Bhuyan", "Praveen Damacharla"], "title": "Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP", "comment": "7 pages, 2025 IEEE Asia-Pacific Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS)", "summary": "Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u52a0\u5229\u798f\u5c3c\u4e9a\u5dde\u7efc\u5408\u91ce\u706b\u98ce\u9669\u5730\u56fe\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08SHAP\uff09\u6765\u8bc6\u522b\u5173\u952e\u751f\u6001\u7cfb\u7edf\u7279\u5b9a\u9a71\u52a8\u56e0\u7d20\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5177\u6709\u826f\u597d\u7684\u9884\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u91ce\u706b\u5bf9\u5168\u7403\u751f\u6001\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u52a0\u5229\u798f\u5c3c\u4e9a\u5dde\u7531\u4e8e\u6c14\u5019\u3001\u5730\u5f62\u7279\u5f81\u3001\u690d\u88ab\u6a21\u5f0f\u548c\u4eba\u7c7b\u6d3b\u52a8\u7b49\u56e0\u7d20\u7ecf\u5e38\u53d1\u751f\u706b\u707e\uff0c\u9700\u8981\u5f00\u53d1\u5168\u9762\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5e94\u7528\u968f\u673a\u68ee\u6797\u7b97\u6cd5\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u901a\u8fc7Shapley Additive exPlanations\uff08SHAP\uff09\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\uff0c\u91c7\u7528\u7a7a\u95f4\u548c\u65f6\u95f4\u9a8c\u8bc1\u7b56\u7565\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "RF\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8349\u5730\u548c\u68ee\u6797\u7684AUC\u5206\u522b\u8fbe\u52300.996\u548c0.997\u3002\u7a7a\u95f4\u4ea4\u53c9\u9a8c\u8bc1\u663e\u793a\u4e2d\u7b49\u53ef\u8f6c\u79fb\u6027\uff0c\u65f6\u95f4\u5206\u5272\u9a8c\u8bc1\u663e\u793a\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002SHAP\u5206\u6790\u8bc6\u522b\u51fa\u571f\u58e4\u6709\u673a\u78b3\u3001\u6811\u6728\u8986\u76d6\u548cNDVI\u662f\u68ee\u6797\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u800c\u5730\u8868\u6e29\u5ea6\u3001\u6d77\u62d4\u548c\u690d\u88ab\u5065\u5eb7\u6307\u6570\u662f\u8349\u5730\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "RF-SHAP\u6846\u67b6\u4e3a\u8bc4\u4f30\u91ce\u706b\u98ce\u9669\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u7406\u89e3\u548c\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u660e\u667a\u51b3\u7b56\u548c\u5236\u5b9a\u6709\u9488\u5bf9\u6027\u7684\u51cf\u707e\u7b56\u7565\u3002"}}
{"id": "2511.12754", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12754", "abs": "https://arxiv.org/abs/2511.12754", "authors": ["Benjamin Li", "Shuyang Shi", "Lucia Romero", "Huao Li", "Yaqi Xie", "Woojun Kim", "Stefanos Nikolaidis", "Michael Lewis", "Katia Sycara", "Simon Stepputtis"], "title": "Adaptively Coordinating with Novel Partners via Learned Latent Strategies", "comment": "Accepted to NeurIPS 2025", "summary": "Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b56\u7565\u6761\u4ef6\u5316\u5408\u4f5c\u8005\u6846\u67b6\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7b56\u7565\u7a7a\u95f4\u3001\u805a\u7c7b\u8bc6\u522b\u7b56\u7565\u7c7b\u578b\uff0c\u5e76\u5229\u7528\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u5b9e\u65f6\u9002\u5e94\u65b0\u4f19\u4f34\uff0c\u5728Overcooked\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u534f\u4f5c\u6027\u80fd\u3002", "motivation": "\u5728\u4eba\u7c7b-\u667a\u80fd\u4f53\u56e2\u961f\u4e2d\uff0c\u4eba\u5de5\u667a\u80fd\u4f53\u9700\u8981\u5b9e\u65f6\u9002\u5e94\u5177\u6709\u72ec\u7279\u504f\u597d\u548c\u52a8\u6001\u53d8\u5316\u7b56\u7565\u7684\u4eba\u7c7b\u4f19\u4f34\uff0c\u8fd9\u5728\u65f6\u95f4\u538b\u529b\u548c\u590d\u6742\u7b56\u7565\u7a7a\u95f4\u7684\u4efb\u52a1\u4e2d\u5c24\u4e3a\u6311\u6218\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7f16\u7801\u7b56\u7565\u5b66\u4e60\u6f5c\u5728\u7b56\u7565\u7a7a\u95f4\uff0c\u901a\u8fc7\u805a\u7c7b\u8bc6\u522b\u4e0d\u540c\u7b56\u7565\u7c7b\u578b\uff0c\u8bad\u7ec3\u6761\u4ef6\u5316\u5408\u4f5c\u8005\u667a\u80fd\u4f53\uff0c\u5e76\u5229\u7528\u56fa\u5b9a\u4efd\u989d\u540e\u6094\u6700\u5c0f\u5316\u7b97\u6cd5\u8fdb\u884c\u5728\u7ebf\u9002\u5e94\u3002", "result": "\u5728\u4fee\u6539\u7248Overcooked\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u548c\u5728\u7ebf\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0e\u65b0\u9896\u4eba\u7c7b\u548c\u667a\u80fd\u4f53\u961f\u53cb\u914d\u5bf9\u65f6\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u6761\u4ef6\u5316\u5408\u4f5c\u8005\u6846\u67b6\u80fd\u591f\u6709\u6548\u8868\u793a\u3001\u5206\u7c7b\u548c\u5b9e\u65f6\u9002\u5e94\u5e7f\u6cdb\u7684\u6f5c\u5728\u4f19\u4f34\u7b56\u7565\uff0c\u5728\u590d\u6742\u534f\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2511.12047", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12047", "abs": "https://arxiv.org/abs/2511.12047", "authors": ["Huimin Cheng", "Xiaowei Yu", "Shushan Wu", "Luyang Fang", "Chao Cao", "Jing Zhang", "Tianming Liu", "Dajiang Zhu", "Wenxuan Zhong", "Ping Ma"], "title": "DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging", "comment": null, "summary": "Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.", "AI": {"tldr": "DCMM-Transformer\u662f\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684Vision Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u5ea6\u6821\u6b63\u6df7\u5408\u6210\u5458\u6a21\u578b\u4f5c\u4e3a\u81ea\u6ce8\u610f\u529b\u4e2d\u7684\u52a0\u6027\u504f\u7f6e\uff0c\u89e3\u51b3\u4e86\u6807\u51c6ViT\u65e0\u6cd5\u5229\u7528\u89e3\u5256\u7ed3\u6784\u7684\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5b58\u5728\u6f5c\u5728\u7684\u89e3\u5256\u5206\u7ec4\u7ed3\u6784\uff08\u5982\u5668\u5b98\u3001\u7ec4\u7ec7\u548c\u75c5\u7406\u533a\u57df\uff09\uff0c\u4f46\u6807\u51c6Vision Transformer\u65e0\u6cd5\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u7ed3\u6784\u3002\u73b0\u6709\u65b9\u6cd5\u5982SBM-Transformer\u5b58\u5728\u4e0d\u53ef\u5fae\u5206\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u65e0\u6cd5\u5efa\u6a21\u590d\u6742\u793e\u533a\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDCMM-Transformer\uff0c\u5c06\u5ea6\u6821\u6b63\u6df7\u5408\u6210\u5458\u6a21\u578b\u4f5c\u4e3a\u81ea\u6ce8\u610f\u529b\u4e2d\u7684\u52a0\u6027\u504f\u7f6e\u5f15\u5165\uff0c\u4ee5\u5b8c\u5168\u53ef\u5fae\u5206\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u5efa\u6a21\u793e\u533a\u7ed3\u6784\u548c\u5ea6\u5f02\u8d28\u6027\uff0c\u907f\u514d\u4e86\u4e58\u6027\u63a9\u7801\u548c\u4e8c\u503c\u91c7\u6837\u3002", "result": "\u5728\u5305\u62ec\u8111\u90e8\u3001\u80f8\u90e8\u3001\u4e73\u817a\u548c\u773c\u90e8\u7b49\u591a\u79cd\u533b\u5b66\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5b66\u4e60\u5230\u7684\u5206\u7ec4\u7ed3\u6784\u548c\u7ed3\u6784\u5316\u6ce8\u610f\u529b\u8c03\u5236\u663e\u8457\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4ea7\u751f\u4e86\u5177\u6709\u89e3\u5256\u610f\u4e49\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u6ce8\u610f\u529b\u56fe\u3002"}}
{"id": "2511.12759", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12759", "abs": "https://arxiv.org/abs/2511.12759", "authors": ["James Moore"], "title": "Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces", "comment": null, "summary": "Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u73b0\u4ee3\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u7b80\u5355\u968f\u673a\u6e38\u8d70\u5c31\u80fd\u4ea7\u751f\u4e0e\u4eba\u7c7b\u6700\u4f18\u89c5\u98df\u884c\u4e3a\u4e00\u81f4\u7684\u7ed3\u679c\uff0c\u800c\u66f4\u590d\u6742\u7684Metropolis-Hastings\u91c7\u6837\u53cd\u800c\u4e0d\u80fd\u5339\u914d\u4eba\u7c7b\u884c\u4e3a\uff0c\u6311\u6218\u4e86\u590d\u6742\u91c7\u6837\u673a\u5236\u5fc5\u7136\u80fd\u66f4\u597d\u6a21\u62df\u8bb0\u5fc6\u68c0\u7d22\u7684\u5047\u8bbe\u3002", "motivation": "\u63a2\u7d22\u73b0\u4ee3\u9ad8\u7ef4\u5d4c\u5165\u7a7a\u95f4\u662f\u5426\u80fd\u63d0\u4f9b\u8db3\u591f\u597d\u7684\u8868\u793a\uff0c\u4f7f\u5f97\u7b97\u6cd5\u80fd\u591f\u5339\u914d\u4eba\u7c7b\u5728\u8bed\u4e49\u6d41\u7545\u6027\u4efb\u52a1\u4e2d\u89c2\u5bdf\u5230\u7684\u89c5\u98df\u884c\u4e3a\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u68c0\u9a8c\u6700\u4f18\u89c5\u98df\u7406\u8bba\u5728\u8bb0\u5fc6\u68c0\u7d22\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u8bcd\u5d4c\u5165\u548c\u5148\u524d\u7684\u8bed\u4e49\u6d41\u7545\u6027\u6570\u636e\uff0c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e0a\u8fdb\u884c\u968f\u673a\u6e38\u8d70\u548cMetropolis-Hastings\u91c7\u6837\uff0c\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\u4ea7\u751f\u7684\u7ed3\u679c\u4e0e\u4eba\u7c7b\u884c\u4e3a\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "result": "\u968f\u673a\u6e38\u8d70\u5728\u5d4c\u5165\u7a7a\u95f4\u4e0a\u4ea7\u751f\u7684\u7ed3\u679c\u4e0e\u6700\u4f18\u89c5\u98df\u548c\u8fb9\u9645\u4ef7\u503c\u5b9a\u7406\u4e00\u81f4\uff0c\u800cMetropolis-Hastings\u91c7\u6837\uff08\u9884\u671f\u80fd\u6a21\u62df\u7b56\u7565\u6027\u63a5\u53d7\u548c\u62d2\u7edd\u65b0\u7c07\uff09\u7684\u7ed3\u679c\u4e0e\u4eba\u7c7b\u884c\u4e3a\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u9002\u5f53\u7ed3\u6784\u7684\u5d4c\u5165\u7a7a\u95f4\u5373\u4f7f\u4f7f\u7528\u7b80\u5355\u91c7\u6837\u4e5f\u80fd\u4ea7\u751f\u63a5\u8fd1\u6700\u4f18\u7684\u89c5\u98df\u52a8\u6001\uff0c\u652f\u6301Hills(2012)\u800c\u975eAbbott(2015)\u7684\u89c2\u70b9\uff0c\u8868\u660e\u73b0\u4ee3\u5d4c\u5165\u53ef\u4ee5\u8fd1\u4f3c\u4eba\u7c7b\u8bb0\u5fc6\u89c5\u98df\u800c\u65e0\u9700\u4f9d\u8d56\u590d\u6742\u7684\u63a5\u53d7\u6807\u51c6\u3002"}}
{"id": "2511.12048", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12048", "abs": "https://arxiv.org/abs/2511.12048", "authors": ["Saksham Kumar", "Ashish Singh", "Srinivasarao Thota", "Sunil Kumar Singh", "Chandan Kumar"], "title": "DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training", "comment": null, "summary": "Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\\% accuracy after stage one and 99.22\\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.", "AI": {"tldr": "\u63d0\u51faDeiTFake\u65b9\u6cd5\uff0c\u57fa\u4e8eDeiT\u53d8\u6362\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6355\u83b7\u6df1\u5ea6\u4f2a\u9020\u7684\u7ec6\u5fae\u75d5\u8ff9\uff0c\u5728OpenForensics\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.22%\u51c6\u786e\u7387\u548c0.9997 AUROC\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u5bf9\u6570\u5b57\u5a92\u4f53\u5b8c\u6574\u6027\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u4f7f\u7528DeiT\u53d8\u6362\u5668\u67b6\u6784\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6807\u51c6\u589e\u5f3a\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u9ad8\u7ea7\u4eff\u5c04\u548c\u6df1\u5ea6\u4f2a\u9020\u7279\u5b9a\u589e\u5f3a\u8fdb\u884c\u5fae\u8c03\uff0c\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u6a21\u578b\u6355\u6349\u7ec6\u5fae\u64cd\u4f5c\u75d5\u8ff9\u3002", "result": "\u5728OpenForensics\u6570\u636e\u96c6\uff08190,335\u5f20\u56fe\u50cf\uff09\u4e0a\uff0c\u7b2c\u4e00\u9636\u6bb5\u8fbe\u523098.71%\u51c6\u786e\u7387\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8fbe\u523099.22%\u51c6\u786e\u7387\u548c0.9997 AUROC\uff0c\u4f18\u4e8e\u6700\u65b0\u7684OpenForensics\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DeiTFake\u65b9\u6cd5\u901a\u8fc7\u4e24\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u90e8\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u51c6\u3002"}}
{"id": "2511.12054", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12054", "abs": "https://arxiv.org/abs/2511.12054", "authors": ["Cuiqun Chen", "Qi Chen", "Bin Yang", "Xingyi Zhang"], "title": "UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization", "comment": "Accepted as Oral Presentation at AAAI 2026. 10 pages, 9 figures", "summary": "Cross-view geo-localization (CVGL) matches query images ($\\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\\rightarrow$ Drone AP by +10.63\\% on University-1652 and +16.73\\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG", "AI": {"tldr": "UniABG\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u53cc\u9636\u6bb5\u65e0\u76d1\u7763\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u89c6\u89d2\u6865\u63a5\u548c\u56fe\u57fa\u5bf9\u5e94\u6821\u51c6\u6765\u89e3\u51b3\u8de8\u89c6\u89d2\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6210\u5bf9\u6807\u6ce8\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u800c\u65e0\u76d1\u7763\u65b9\u6cd5\u7531\u4e8e\u8de8\u89c6\u89d2\u57df\u5dee\u8ddd\u5bfc\u81f4\u4f2a\u6807\u7b7e\u566a\u58f0\u4e25\u91cd\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u53cc\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u4f7f\u7528\u89c6\u56fe\u611f\u77e5\u5bf9\u6297\u6865\u63a5(VAAB)\u5efa\u6a21\u89c6\u56fe\u4e0d\u53d8\u7279\u5f81\u5e76\u589e\u5f3a\u4f2a\u6807\u7b7e\u9c81\u68d2\u6027\uff1b\u968f\u540e\u901a\u8fc7\u5f02\u6784\u56fe\u8fc7\u6ee4\u6821\u51c6(HGFC)\u6784\u5efa\u53cc\u8de8\u89c6\u56fe\u7ed3\u6784\u56fe\u6765\u7cbe\u5316\u8de8\u89c6\u56fe\u5173\u8054\u3002", "result": "\u5728University-1652\u6570\u636e\u96c6\u4e0a\u536b\u661f\u2192\u65e0\u4eba\u673aAP\u63d0\u5347+10.63%\uff0c\u5728SUES-200\u6570\u636e\u96c6\u4e0a\u63d0\u5347+16.73%\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "UniABG\u901a\u8fc7\u96c6\u6210\u5bf9\u6297\u6027\u89c6\u56fe\u6865\u63a5\u548c\u56fe\u57fa\u5bf9\u5e94\u6821\u51c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u4e2d\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.11684", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.11684", "abs": "https://arxiv.org/abs/2511.11684", "authors": ["Shuvom Sadhuka", "Sophia Lin", "Emma Pierson", "Bonnie Berger"], "title": "A Bayesian Model for Multi-stage Censoring", "comment": "Proceedings of ML4H 2025", "summary": "Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u6a21\u578b\u6765\u5904\u7406\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u6f0f\u6597\u7ed3\u6784\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u88ab\u5ba1\u67e5\u60a3\u8005\u7684\u771f\u5b9e\u7ed3\u679c\uff0c\u5e76\u5728\u6025\u8bca\u79d1\u6570\u636e\u4e2d\u53d1\u73b0\u6027\u522b\u5dee\u5f02\u3002", "motivation": "\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u6f0f\u6597\u7ed3\u6784\u5b58\u5728\u9009\u62e9\u6027\u5ba1\u67e5\u95ee\u9898\uff0c\u5373\u771f\u5b9e\u7ed3\u679c\u53ea\u5728\u6d41\u7a0b\u7ed3\u675f\u65f6\u624d\u88ab\u63ed\u793a\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u98ce\u9669\u4f30\u8ba1\u7684\u7edf\u8ba1\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u7684\u60a3\u8005\u7fa4\u4f53\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9009\u62e9\u6027\u6807\u7b7e\u548c\u5ba1\u67e5\u5148\u9a8c\u5de5\u4f5c\u7684\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u7528\u4e8e\u5904\u7406\u6f0f\u6597\u51b3\u7b56\u7ed3\u6784\u3002", "result": "\u5728\u5408\u6210\u8bbe\u7f6e\u4e2d\uff0c\u6a21\u578b\u80fd\u591f\u6062\u590d\u771f\u5b9e\u53c2\u6570\u5e76\u6bd4\u57fa\u7ebf\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u88ab\u5ba1\u67e5\u60a3\u8005\u7684\u7ed3\u679c\uff1b\u5728\u6025\u8bca\u79d1\u6570\u636e\u5e94\u7528\u4e2d\uff0c\u53d1\u73b0ICU\u5165\u9662\u7684\u98ce\u9669\u9608\u503c\u5b58\u5728\u6027\u522b\u5dee\u5f02\uff08\u5973\u60275.1% vs \u7537\u60274.5%\uff09\u3002", "conclusion": "\u8be5\u8d1d\u53f6\u65af\u6a21\u578b\u80fd\u591f\u6709\u6548\u5904\u7406\u6f0f\u6597\u51b3\u7b56\u7ed3\u6784\u4e2d\u7684\u9009\u62e9\u6027\u5ba1\u67e5\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u4e86\u533b\u7597\u51b3\u7b56\u4e2d\u5b58\u5728\u7684\u6027\u522b\u5dee\u5f02\u3002"}}
{"id": "2511.12792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12792", "abs": "https://arxiv.org/abs/2511.12792", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Zehong Cao", "Ryszard Kowalczyk"], "title": "Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization", "comment": null, "summary": "This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5f02\u6784\u536b\u661f\u96c6\u7fa4\u5728\u81ea\u4e3b\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u4e2d\u7684\u8d44\u6e90\u5206\u914d\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u5149\u5b66\u548cSAR\u536b\u661f\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u89e3\u51b3\u5b9e\u65f6\u3001\u4e0d\u786e\u5b9a\u548c\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e0b\u7684\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5730\u7403\u89c2\u6d4b\u64cd\u4f5c\u4e2d\u7684\u5b9e\u65f6\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6765\u5b9e\u73b0\u81ea\u9002\u5e94\u51b3\u7b56\u3002", "method": "\u4f7f\u7528Basilisk\u548cBSK-RL\u6846\u67b6\u6784\u5efa\u8fd1\u771f\u5b9e\u6a21\u62df\u73af\u5883\uff0c\u8bc4\u4f30MAPPO\u3001HAPPO\u548cHATRPO\u7b49\u5148\u8fdb\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5355\u536b\u661f\u5230\u591a\u536b\u661f\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u534f\u8c03\u5f02\u6784\u536b\u661f\uff0c\u5728\u5e73\u8861\u6210\u50cf\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7684\u540c\u65f6\uff0c\u7f13\u89e3\u975e\u5e73\u7a33\u6027\u548c\u667a\u80fd\u4f53\u95f4\u5956\u52b1\u8026\u5408\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u6269\u5c55\u7684\u81ea\u4e3b\u536b\u661f\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u4e3a\u5f02\u6784\u52a8\u6001\u6761\u4ef6\u4e0b\u667a\u80fd\u5730\u7403\u89c2\u6d4b\u4efb\u52a1\u89c4\u5212\u7684\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12056", "categories": ["cs.CV", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12056", "abs": "https://arxiv.org/abs/2511.12056", "authors": ["Sijie Wang", "Qiang Wang", "Shaohuai Shi"], "title": "PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling", "comment": null, "summary": "Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.", "AI": {"tldr": "PipeDiT\u662f\u4e00\u4e2a\u7528\u4e8e\u52a0\u901f\u89c6\u9891\u751f\u6210\u7684\u6d41\u6c34\u7ebf\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u5e76\u884c\u3001\u89e3\u8026VAE\u548c\u6ce8\u610f\u529b\u534f\u540c\u5904\u7406\u4e09\u4e2a\u521b\u65b0\u65b9\u6cd5\uff0c\u57288-GPU\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u4e861.06-4.02\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u867d\u7136\u6027\u80fd\u51fa\u8272\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u63a8\u7406\u901f\u5ea6\u6162\u548c\u5185\u5b58\u6d88\u8017\u9ad8\u7684\u9650\u5236\uff0c\u9700\u8981\u4f18\u5316\u65b9\u6848\u3002", "method": "1. \u8bbe\u8ba1\u5e8f\u5217\u5e76\u884c\u6d41\u6c34\u7ebf\u7b97\u6cd5(PipeSP)\u5b9e\u73b0GPU\u95f4\u8ba1\u7b97\u4e0e\u901a\u4fe1\u7684\u6d41\u6c34\u7ebf\u5316\uff1b2. \u63d0\u51faDeDiVAE\u5c06\u6269\u6563\u6a21\u5757\u548cVAE\u6a21\u5757\u89e3\u8026\u5230\u4e0d\u540cGPU\u7ec4\u5e76\u884c\u6267\u884c\uff1b3. \u63d0\u51fa\u6ce8\u610f\u529b\u534f\u540c\u5904\u7406(Aco)\u65b9\u6cd5\u4f18\u5316VAE\u7ec4GPU\u8d44\u6e90\u5229\u7528\u3002", "result": "\u5728OpenSoraPlan\u548cHunyuanVideo\u4e24\u4e2a\u5f00\u6e90\u6846\u67b6\u4e0a\u96c6\u6210PipeDiT\uff0c\u5728\u591a\u79cd\u5e38\u89c1\u5206\u8fa8\u7387\u548c\u65f6\u95f4\u6b65\u914d\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u539f\u6846\u67b6\u5b9e\u73b0\u4e861.06-4.02\u500d\u7684\u52a0\u901f\u6548\u679c\u3002", "conclusion": "PipeDiT\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u6c34\u7ebf\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2511.12061", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.12061", "abs": "https://arxiv.org/abs/2511.12061", "authors": ["Zhichen Lai", "Hua Lu", "Huan Li", "Jialiang Li", "Christian S. Jensen"], "title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity", "comment": "8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper", "summary": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.", "AI": {"tldr": "MovSemCL\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fd0\u52a8\u8bed\u4e49\u5bf9\u6bd4\u5b66\u4e60\u7684\u8f68\u8ff9\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u5c06GPS\u8f68\u8ff9\u8f6c\u6362\u4e3a\u8fd0\u52a8\u8bed\u4e49\u7279\u5f81\u5e76\u5206\u5757\u5904\u7406\uff0c\u4f7f\u7528\u5185\u5916\u6ce8\u610f\u529b\u673a\u5236\u7f16\u7801\u5c40\u90e8\u548c\u5168\u5c40\u6a21\u5f0f\uff0c\u7ed3\u5408\u66f2\u7387\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u7269\u7406\u5408\u7406\u6027\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u8f68\u8ff9\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\u5b58\u5728\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a(1) \u5bf9\u8f68\u8ff9\u8bed\u4e49\u548c\u5c42\u6b21\u7ed3\u6784\u5efa\u6a21\u4e0d\u8db3\uff1b(2) \u9010\u70b9\u7f16\u7801\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b(3) \u4f7f\u7528\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u7684\u589e\u5f3a\u65b9\u6cd5\u626d\u66f2\u8f68\u8ff9\u8bed\u4e49\u3002", "method": "\u63d0\u51faMovSemCL\u6846\u67b6\uff1a\u9996\u5148\u5c06\u539f\u59cbGPS\u8f68\u8ff9\u8f6c\u6362\u4e3a\u8fd0\u52a8\u8bed\u4e49\u7279\u5f81\u5e76\u5206\u5757\uff0c\u7136\u540e\u4f7f\u7528\u5185\u5916\u6ce8\u610f\u529b\u673a\u5236\u7f16\u7801\u5c40\u90e8\u548c\u5168\u5c40\u8f68\u8ff9\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u6548\u5c42\u6b21\u8868\u793a\uff1b\u91c7\u7528\u66f2\u7387\u5f15\u5bfc\u7684\u589e\u5f3a\u7b56\u7565\uff0c\u4fdd\u7559\u4fe1\u606f\u4e30\u5bcc\u7684\u7247\u6bb5\uff08\u5982\u8f6c\u5f2f\u548c\u4ea4\u53c9\u53e3\uff09\u5e76\u5c4f\u853d\u5197\u4f59\u90e8\u5206\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMovSemCL\u5728\u76f8\u4f3c\u5ea6\u641c\u7d22\u4efb\u52a1\u4e2d\u8fbe\u5230\u63a5\u8fd1\u7406\u60f3\u503c1\u7684\u5e73\u5747\u6392\u540d\uff0c\u5728\u542f\u53d1\u5f0f\u8fd1\u4f3c\u65b9\u9762\u63d0\u5347\u9ad8\u8fbe20.3%\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe43.4%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MovSemCL\u901a\u8fc7\u6709\u6548\u7684\u8fd0\u52a8\u8bed\u4e49\u5efa\u6a21\u3001\u5c42\u6b21\u8868\u793a\u5b66\u4e60\u548c\u7269\u7406\u5408\u7406\u7684\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u8f68\u8ff9\u805a\u7c7b\u3001\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u57fa\u7840\u529f\u80fd\u3002"}}
{"id": "2511.11686", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.11686", "abs": "https://arxiv.org/abs/2511.11686", "authors": ["Qing Yao", "Lijian Gao", "Qirong Mao", "Dong Ming"], "title": "Regularized Schr\u00f6dinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems", "comment": null, "summary": "Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schr\u00f6dinger Bridge (RSB), an adaptation of Schr\u00f6dinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6b63\u5219\u5316\u859b\u5b9a\u8c14\u6865(RSB)\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5931\u771f-\u611f\u77e5\u6743\u8861\u548c\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u5728\u8bed\u97f3\u589e\u5f3a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1)\u5931\u771f-\u611f\u77e5\u6743\u8861\uff0c\u5373\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u4f1a\u964d\u4f4e\u91cd\u5efa\u4fdd\u771f\u5ea6\uff1b2)\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u5373\u8bad\u7ec3-\u63a8\u7406\u8f93\u5165\u4e0d\u5339\u914d\u5bfc\u81f4\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u548c\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6b63\u5219\u5316\u859b\u5b9a\u8c14\u6865(RSB)\uff0c\u91c7\u7528\u65b0\u9896\u7684\u6b63\u5219\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u6270\u52a8\u8f93\u5165\u72b6\u6001\u548c\u76ee\u6807\u6765\u7f13\u89e3\u66dd\u5149\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u540e\u9a8c\u5747\u503c\u7684\u7cbe\u5fc3\u8bbe\u8ba1\u63d2\u503c\u6765\u51cf\u8f7b\u5931\u771f\u3002", "result": "\u5728\u8bed\u97f3\u589e\u5f3a\u7684\u4e24\u4e2a\u5178\u578b\u9006\u95ee\u9898\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRSB\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u5584\u4e86\u5931\u771f\u6307\u6807\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u66dd\u5149\u504f\u5dee\u3002", "conclusion": "RSB\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5931\u771f-\u611f\u77e5\u6743\u8861\u548c\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u5728\u8bed\u97f3\u589e\u5f3a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2511.12066", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12066", "abs": "https://arxiv.org/abs/2511.12066", "authors": ["Jialang Lu", "Shuning Sun", "Pu Wang", "Chen Wu", "Feng Gao", "Lina Gong", "Dianjie Lu", "Guijuan Zhang", "Zhuoran Zheng"], "title": "DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal", "comment": "11 pages, 9 figures", "summary": "Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel\", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.", "AI": {"tldr": "DCA-LUT\u662f\u9996\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7d2b\u8272\u8fb9\u7f18\u53bb\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u8272\u5ea6\u611f\u77e5\u5750\u6807\u53d8\u6362\u6a21\u5757\u5206\u79bb\u7d2b\u8272\u8fb9\u7f18\u5230\u4e13\u7528\u7ef4\u5ea6\uff0c\u5e76\u4f7f\u75285D\u67e5\u627e\u8868\u8fdb\u884c\u9ad8\u6548\u975e\u7ebf\u6027\u8272\u5f69\u6620\u5c04\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7d2b\u8272\u8fb9\u7f18\u53bb\u9664\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u6d88\u8272\u5dee\u955c\u5934\u786c\u4ef6\u548c\u624b\u5de5\u7279\u5f81\u63d0\u53d6\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u9996\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u6765\u5904\u7406\u7531\u955c\u5934\u8272\u5dee\u5f15\u8d77\u7684\u7d2b\u8272\u8fb9\u7f18\u95ee\u9898\u3002", "method": "\u63d0\u51faDCA-LUT\u6846\u67b6\uff0c\u5305\u542b\u8272\u5ea6\u611f\u77e5\u5750\u6807\u53d8\u6362(CA-CT)\u6a21\u5757\u6765\u5b66\u4e60\u56fe\u50cf\u81ea\u9002\u5e94\u8272\u5f69\u7a7a\u95f4\uff0c\u5c06\u7d2b\u8272\u8fb9\u7f18\u5206\u79bb\u5230\u4e13\u7528\u7ef4\u5ea6\uff0c\u7136\u540e\u4f7f\u75285D\u67e5\u627e\u8868\u8fdb\u884c\u8272\u5f69\u6821\u6b63\u3002\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u5408\u6210\u7d2b\u8272\u8fb9\u7f18\u6570\u636e\u96c6(PF-Synth)\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7d2b\u8272\u8fb9\u7f18\u53bb\u9664\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DCA-LUT\u6846\u67b6\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7d2b\u8272\u8fb9\u7f18\u95ee\u9898\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u786c\u4ef6\u4f9d\u8d56\u548c\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u7684\u65b9\u6cd5\uff0c\u4e3a\u6570\u5b57\u6210\u50cf\u8d28\u91cf\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12867", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12867", "abs": "https://arxiv.org/abs/2511.12867", "authors": ["Chen Jia"], "title": "Bootstrapping LLMs via Preference-Based Policy Optimization", "comment": null, "summary": "Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6PbPO\uff0c\u901a\u8fc7\u4e3b\u7b56\u7565\u4e0e\u5956\u52b1\u6a21\u578b\u4e4b\u95f4\u7684min-max\u535a\u5f08\u6765\u5f15\u5bfcLLM\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\uff0c\u65e0\u9700\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u901a\u8fc7\u57fa\u4e8e\u504f\u597d\u7684\u7b56\u7565\u4f18\u5316\u6765\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\uff0c\u907f\u514d\u4f9d\u8d56\u5927\u91cf\u624b\u52a8\u6807\u6ce8\u3002", "method": "\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u4e3b\u7b56\u7565\u4e0e\u5956\u52b1\u6a21\u578b\u4e4b\u95f4\u7684min-max\u535a\u5f08\uff0c\u5956\u52b1\u6a21\u578b\u53d7\u504f\u597d\u6570\u636e\u7f6e\u4fe1\u96c6\u7ea6\u675f\uff0c\u91c7\u7528\u8fed\u4ee3\u5728\u7ebf\u7b97\u6cd5\u901a\u8fc7\u7b56\u7565\u7684\u5f15\u5bfc\u5f0f\u63a2\u7d22\u4e3b\u52a8\u6536\u96c6\u504f\u597d\u6570\u636e\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u504f\u597d\u4f18\u5316\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684PbPO\u6846\u67b6\u4e3a\u5f15\u5bfcLLM\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u7b56\u7565\u548c\u5956\u52b1\u6a21\u578b\u7684\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u3002"}}
{"id": "2511.12077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12077", "abs": "https://arxiv.org/abs/2511.12077", "authors": ["Dengming Zhang", "Weitao You", "Jingxiong Li", "Weishen Lin", "Wenda Shi", "Xue Zhao", "Heda Zuo", "Junxian Wu", "Lingyun Sun"], "title": "Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound", "comment": null, "summary": "Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.", "AI": {"tldr": "VAEmotionLLM\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u9650\u7684\u97f3\u9891\u9884\u8bad\u7ec3\u6559\u4f1a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u542c\u89c9\u80fd\u529b\uff0c\u5e76\u589e\u5f3a\u8de8\u6a21\u6001\u60c5\u611f\u7406\u89e3\u3002\u8be5\u65b9\u6cd5\u5728\u827a\u672f\u60c5\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u827a\u672f\u4f5c\u54c1\u901a\u8fc7\u89c6\u542c\u5143\u7d20\u8054\u5408\u8bbe\u8ba1\u4f20\u8fbe\u60c5\u611f\uff0c\u4f46\u5148\u524d\u5de5\u4f5c\u591a\u4e3a\u5355\u6a21\u6001\u6216\u4eba\u7c7b\u4e2d\u5fc3\uff0c\u5ffd\u7565\u4e86\u827a\u672f\u4f5c\u54c1\u6709\u610f\u8868\u8fbe\u7684\u60c5\u611f\u3002\u540c\u65f6\uff0c\u5f53\u524d\u89c6\u542c\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u89c4\u6a21\u97f3\u9891\u9884\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u97f3\u9891\u5bf9\u9f50\uff08VG-Align\uff09\u5c06\u51bb\u7ed3\u7684\u89c6\u89c9\u901a\u8def\u84b8\u998f\u5230\u65b0\u7684\u97f3\u9891\u901a\u8def\uff0c\u5b9e\u73b0\u65e0\u9700\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6\u7684\u542c\u89c9\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8de8\u6a21\u6001\u60c5\u611f\u9002\u914d\u5668\uff08EmoAdapter\uff09\u6ce8\u5165\u60c5\u611f\u654f\u611f\u6b8b\u5dee\u5e76\u5e94\u7528\u60c5\u611f\u76d1\u7763\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u60c5\u611f\u7406\u89e3\u3002", "result": "\u5728\u6784\u5efa\u7684ArtEmoBenchmark\u827a\u672f\u60c5\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVAEmotionLLM\u5728\u97f3\u9891\u3001\u89c6\u89c9\u548c\u89c6\u542c\u8f93\u5165\u4e0b\u5747\u4f18\u4e8e\u97f3\u9891\u3001\u89c6\u89c9\u548c\u89c6\u542c\u57fa\u7ebf\u6a21\u578b\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u7ec4\u4ef6\u5177\u6709\u4e92\u8865\u6027\u3002", "conclusion": "VAEmotionLLM\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7\u6709\u9650\u97f3\u9891\u9884\u8bad\u7ec3\u8d4b\u4e88\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u542c\u89c9\u80fd\u529b\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u60c5\u611f\u7406\u89e3\u6027\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u3001\u53ef\u9760\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2511.11690", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11690", "abs": "https://arxiv.org/abs/2511.11690", "authors": ["Fei Song", "Yi Li", "Rui Wang", "Jiahuan Zhou", "Changwen Zheng", "Jiangmeng Li"], "title": "Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models", "comment": "Accepted by AAAI2026", "summary": "Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u53bb\u504f\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u8c03\u5236\u548c\u53ef\u9760\u6027\u611f\u77e5\u63d0\u793a\u4f18\u5316\u6765\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u63d0\u793a\u4f18\u5316\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u4ec5\u57fa\u4e8e\u672a\u6807\u8bb0\u6d4b\u8bd5\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u63d0\u793a\u4f18\u5316\u504f\u5dee\uff0c\u9020\u6210\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4ece\u6a21\u578b\u548c\u6570\u636e\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790\u5e76\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53cc\u91cd\u53bb\u504f\u65b9\u6cd5\uff1a1) \u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u8c03\u5236\u6a21\u5757\uff0c\u7528\u6d4b\u8bd5\u56fe\u50cf\u7279\u5f81\u68c0\u7d22\u9ad8\u7f6e\u4fe1\u5ea6\u77e5\u8bc6\u6765\u8c03\u5236\u9884\u6d4b\uff1b2) \u53ef\u9760\u6027\u611f\u77e5\u63d0\u793a\u4f18\u5316\u6a21\u5757\uff0c\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u52a0\u6743\u96c6\u6210\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u84b8\u998f\u8fdb\u884c\u6b63\u5219\u5316\u7ea6\u675f\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u7136\u5206\u5e03\u504f\u79fb\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u7f13\u89e3\u63d0\u793a\u4f18\u5316\u504f\u5dee\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u91cd\u53bb\u504f\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u8f7b\u63d0\u793a\u4f18\u5316\u504f\u5dee\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12876", "categories": ["cs.AI", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.12876", "abs": "https://arxiv.org/abs/2511.12876", "authors": ["Heyang Ma", "Qirui Mi", "Qipeng Yang", "Zijun Fan", "Bo Li", "Haifeng Zhang"], "title": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making", "comment": "Extended version of a submission to AAAI 2026", "summary": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.", "AI": {"tldr": "LAMP\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u589e\u5f3a\u7684\u591a\u667a\u80fd\u4f53\u7b56\u7565\uff0c\u5728\u7ecf\u6d4e\u51b3\u7b56\u4e2d\u6574\u5408\u8bed\u8a00\u4fe1\u606f\uff0c\u91c7\u7528Think-Speak-Decide\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7d2f\u79ef\u56de\u62a5\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u5b9e\u7ecf\u6d4e\u51b3\u7b56\u4e0d\u4ec5\u4f9d\u8d56\u7ed3\u6784\u5316\u4fe1\u53f7\uff08\u5982\u4ef7\u683c\u3001\u7a0e\u6536\uff09\uff0c\u8fd8\u53d7\u975e\u7ed3\u6784\u5316\u8bed\u8a00\uff08\u5982\u540c\u884c\u5bf9\u8bdd\u3001\u5a92\u4f53\u53d9\u4e8b\uff09\u5f71\u54cd\u3002\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u5904\u7406\u8bed\u8a00\u7684\u8bed\u4e49\u6a21\u7cca\u6027\u548c\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\u3002", "method": "\u63d0\u51faLAMP\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1aThink\uff08\u89e3\u91ca\u6570\u503c\u89c2\u5bdf\uff0c\u63d0\u53d6\u77ed\u671f\u51b2\u51fb\u548c\u957f\u671f\u8d8b\u52bf\uff0c\u7f13\u5b58\u9ad8\u4ef7\u503c\u63a8\u7406\u8f68\u8ff9\uff09\u3001Speak\uff08\u57fa\u4e8e\u63a8\u7406\u751f\u6210\u548c\u4ea4\u6362\u6218\u7565\u6d88\u606f\uff0c\u901a\u8fc7\u89e3\u6790\u540c\u4f34\u901a\u4fe1\u66f4\u65b0\u4fe1\u5ff5\uff09\u3001Decide\uff08\u878d\u5408\u6570\u503c\u6570\u636e\u3001\u63a8\u7406\u548c\u53cd\u601d\u5230MARL\u7b56\u7565\u4e2d\u4f18\u5316\u8bed\u8a00\u589e\u5f3a\u51b3\u7b56\uff09\u3002", "result": "\u7ecf\u6d4e\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0cLAMP\u5728\u7d2f\u79ef\u56de\u62a5\uff08+63.5%\uff0c+34.0%\uff09\u3001\u9c81\u68d2\u6027\uff08+18.8%\uff0c+59.4%\uff09\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8eMARL\u548c\u7eafLLM\u57fa\u7ebf\u3002", "conclusion": "\u8bed\u8a00\u589e\u5f3a\u7b56\u7565\u5177\u6709\u63d0\u4f9b\u66f4\u6709\u6548\u548c\u7a33\u5065\u7ecf\u6d4e\u7b56\u7565\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12079", "abs": "https://arxiv.org/abs/2511.12079", "authors": ["Hongxuan Li", "Wencheng Zhu", "Huiying Xu", "Xinzhong Zhu", "Pengfei Zhu"], "title": "Point Cloud Quantization through Multimodal Prompting for 3D Understanding", "comment": "Accepted by AAAI 2026. 11 pages, 7 figures. Corresponding author: Wencheng Zhu (wenchengzhu@tju.edu.cn)", "summary": "Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u63d0\u793a\u9a71\u52a8\u7684\u70b9\u4e91\u91cf\u5316\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u5d4c\u5165\u4f5c\u4e3a\u539f\u578b\u5148\u9a8c\uff0c\u901a\u8fc7\u53cc\u7ea6\u675f\u91cf\u5316\u7a7a\u95f4\u548cGumbel-Softmax\u677e\u5f1b\u5b9e\u73b0\u51e0\u4f55\u4e0e\u8bed\u4e49\u4fe1\u606f\u7684\u8054\u5408\u7f16\u7801\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u539f\u578b\u7684\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\u5728\u4ee3\u8868\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u800c\u591a\u6a21\u6001\u5bf9\u9f50\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6587\u672c\u5d4c\u5165\u4f5c\u4e3a\u539f\u578b\u5148\u9a8c\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u81ea\u9002\u5e94\u4f18\u5316\u539f\u578b\uff0c\u6784\u5efa\u53cc\u7ea6\u675f\u91cf\u5316\u7a7a\u95f4\uff08\u7d27\u51d1\u6027\u548c\u5206\u79bb\u6027\u6b63\u5219\u5316\uff09\uff0c\u5e76\u91c7\u7528Gumbel-Softmax\u5b9e\u73b0\u53ef\u5fae\u79bb\u6563\u5316\u3002", "result": "\u5728ModelNet40\u548cScanObjectNN\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u63d0\u793a\u9a71\u52a8\u91cf\u5316\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u539f\u578b\u4ee3\u8868\u6027\u548c\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u4e0e\u8bed\u4e49\u4fe1\u606f\u7684\u6709\u6548\u8054\u5408\u7f16\u7801\u3002"}}
{"id": "2511.11691", "categories": ["cs.LG", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.11691", "abs": "https://arxiv.org/abs/2511.11691", "authors": ["Seham Nasr", "Zhao Ren", "David Johnson"], "title": "Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues", "comment": "5 pages, 2 figures", "summary": "Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies \"what\" is highlighted and connects it to \"why\" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91caAI\u6846\u67b6\uff0c\u7528\u4e8e\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff0c\u901a\u8fc7\u91cf\u5316\u663e\u8457\u533a\u57df\u5185\u7684\u58f0\u5b66\u7ebf\u7d22\u5e45\u5ea6\uff0c\u5c06\u663e\u8457\u6027\u533a\u57df\u4e0e\u4e13\u5bb6\u53c2\u8003\u7684\u58f0\u5b66\u7ebf\u7d22\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u9ad8\u4e86\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u663e\u8457\u6027\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u7a81\u51fa\u58f0\u8c31\u56fe\u533a\u57df\uff0c\u4f46\u65e0\u6cd5\u663e\u793a\u8fd9\u4e9b\u533a\u57df\u662f\u5426\u5bf9\u5e94\u6709\u610f\u4e49\u7684\u58f0\u5b66\u60c5\u611f\u6807\u8bb0\uff0c\u9650\u5236\u4e86\u5fe0\u5b9e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u663e\u8457\u533a\u57df\u5185\u7684\u7ebf\u7d22\u5e45\u5ea6\uff0c\u5c06\u663e\u8457\u6027\u533a\u57df\u4e0e\u4e13\u5bb6\u53c2\u8003\u7684\u8bed\u97f3\u60c5\u611f\u58f0\u5b66\u7ebf\u7d22\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u5728\u57fa\u51c6SER\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u660e\u786e\u5c06\u663e\u8457\u533a\u57df\u4e0e\u7406\u8bba\u9a71\u52a8\u7684\u8bed\u97f3\u60c5\u611f\u58f0\u5b66\u7ebf\u7d22\u8054\u7cfb\u8d77\u6765\uff0c\u63d0\u9ad8\u4e86\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "\u76f8\u6bd4\u6807\u51c6\u663e\u8457\u6027\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u6613\u7406\u89e3\u548c\u5408\u7406\u7684SER\u6a21\u578b\u89e3\u91ca\uff0c\u4e3a\u53ef\u4fe1\u8d56\u7684\u57fa\u4e8e\u8bed\u97f3\u7684\u60c5\u611f\u8ba1\u7b97\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12082", "abs": "https://arxiv.org/abs/2511.12082", "authors": ["Lokender Singh", "Saksham Kumar", "Chandan Kumar"], "title": "Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning", "comment": "ICCCNT 2025 Conference Proceedings, IIT Indore", "summary": "Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbResNet-101\u67b6\u6784\u548c\u6982\u7387\u63a8\u7406\u7684\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u5728COCO-2014\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.794 mAP\u7684\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u6cdb\u9700\u6c42\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u6807\u7b7e\u4f9d\u8d56\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684ResNet-101\u67b6\u6784\uff0c\u901a\u8fc7\u6982\u7387\u63a8\u7406\u6a21\u62df\u6807\u7b7e\u4f9d\u8d56\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6765\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5728COCO-2014\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.794 mAP\uff0c\u4f18\u4e8eResNet-SRN(0.771)\u548cVision Transformer\u57fa\u7ebf(0.785)\u3002", "conclusion": "\u5c06\u6982\u7387\u63a8\u7406\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6807\u7b7e\u573a\u666f\u7684\u6311\u6218\uff0c\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11692", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11692", "abs": "https://arxiv.org/abs/2511.11692", "authors": ["Jiayin Zhu", "Linlin Yang", "Yicong Li", "Angela Yao"], "title": "AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation", "comment": "Accepted by AAAI 2026. Project page: https://jyzhu.top/AnchorDS_Webpage/", "summary": "Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to \"semantic over-smoothing\" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAnchorDS\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u52303D\u4f18\u5316\u91cd\u65b0\u8868\u8ff0\u4e3a\u52a8\u6001\u6f14\u5316\u6e90\u5206\u5e03\u5230\u56fa\u5b9a\u76ee\u6807\u5206\u5e03\u7684\u6620\u5c04\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfSDS\u65b9\u6cd5\u4e2d\u8bed\u4e49\u8fc7\u5e73\u6ed1\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u76843D\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4f18\u5316\u7684\u6587\u672c\u52303D\u65b9\u6cd5\u5c062D\u751f\u6210\u6a21\u578b\u7684\u6307\u5bfc\u89c6\u4e3a\u9759\u6001\uff0c\u5ffd\u7565\u4e86\u6e90\u52a8\u6001\uff0c\u5bfc\u81f4\u8bed\u4e49\u7ebf\u7d22\u88ab\u6291\u5236\u6216\u5408\u5e76\uff0c\u4ea7\u751f\u8bed\u4e49\u8fc7\u5e73\u6ed1\u4f2a\u5f71\u3002", "method": "\u5c06\u95ee\u9898\u6295\u5c04\u5230\u53cc\u6761\u4ef6\u6f5c\u7a7a\u95f4\uff0c\u5f15\u5165AnchorDS\u6539\u8fdb\u7684\u5206\u6570\u84b8\u998f\u673a\u5236\uff0c\u63d0\u4f9b\u56fe\u50cf\u6761\u4ef6\u7684\u72b6\u6001\u951a\u5b9a\u6307\u5bfc\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u8fc7\u6ee4\u7b56\u7565\u548c\u5fae\u8c03\u7b56\u7565\u6765\u4f18\u5316\u951a\u70b9\u3002", "result": "AnchorDS\u4ea7\u751f\u66f4\u7cbe\u7ec6\u7684\u7ec6\u8282\u3001\u66f4\u81ea\u7136\u7684\u989c\u8272\u548c\u66f4\u5f3a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63d0\u793a\u4e0b\uff0c\u540c\u65f6\u5728\u6548\u7387\u4e0a\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u90fd\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u4e3a\u6587\u672c\u52303D\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12084", "abs": "https://arxiv.org/abs/2511.12084", "authors": ["Ji-Ping Jin", "Chen-Bin Feng", "Rui Fan", "Chi-Man Vong"], "title": "SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving", "comment": "12pages, has been early accepted by The Visual Computer: International Journal of Computer Graphics, 2025", "summary": "Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.", "AI": {"tldr": "\u63d0\u51faSemanticStitch\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u8bed\u4e49\u5148\u9a8c\u4fe1\u606f\u89e3\u51b3\u56fe\u50cf\u62fc\u63a5\u4e2d\u7684\u524d\u666f\u5bf9\u8c61\u5b8c\u6574\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u62fc\u63a5\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u62fc\u63a5\u65b9\u6cd5\u56e0\u62cd\u6444\u89d2\u5ea6\u3001\u4f4d\u7f6e\u5dee\u5f02\u548c\u7269\u4f53\u8fd0\u52a8\u5bfc\u81f4\u9519\u4f4d\u548c\u89c6\u89c9\u5dee\u5f02\uff0c\u4e14\u4f20\u7edf\u63a5\u7f1d\u96d5\u523b\u65b9\u6cd5\u5ffd\u7565\u8bed\u4e49\u4fe1\u606f\uff0c\u7834\u574f\u524d\u666f\u8fde\u7eed\u6027\u3002", "method": "\u5f15\u5165SemanticStitch\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u524d\u666f\u5bf9\u8c61\u7684\u8bed\u4e49\u5148\u9a8c\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u5f3a\u8c03\u663e\u8457\u5bf9\u8c61\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u76f8\u6bd4\u4f20\u7edf\u6280\u672f\u6709\u663e\u8457\u6539\u8fdb\uff0c\u63d0\u4f9b\u4e86\u4e24\u4e2a\u4e13\u95e8\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u652f\u6301\uff0c\u80fd\u591f\u6709\u6548\u4fdd\u6301\u524d\u666f\u5bf9\u8c61\u7684\u5b8c\u6574\u6027\u5e76\u589e\u5f3a\u89c6\u89c9\u8fde\u8d2f\u6027\u3002"}}
{"id": "2511.12916", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12916", "abs": "https://arxiv.org/abs/2511.12916", "authors": ["Yafang Wang", "Yangjie Tian", "Xiaoyu Shen", "Gaoyang Zhang", "Jiaze Sun", "He Zhang", "Ruohua Xu", "Feng Zhao"], "title": "Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation", "comment": null, "summary": "Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.", "AI": {"tldr": "Fault2Flow\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u7535\u7f51\u6545\u969c\u8bca\u65ad\uff0c\u901a\u8fc7\u63d0\u53d6\u6cd5\u89c4\u903b\u8f91\u3001\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\u3001\u4f18\u5316\u63a8\u7406\u903b\u8f91\uff0c\u6700\u7ec8\u751f\u6210\u53ef\u6267\u884c\u7684\u5de5\u4f5c\u6d41\u3002", "motivation": "\u4f20\u7edf\u7535\u7f51\u6545\u969c\u8bca\u65ad\u4f9d\u8d56\u624b\u52a8\u65b9\u6cd5\uff0c\u6548\u7387\u4f4e\u3001\u6613\u51fa\u9519\u4e14\u96be\u4ee5\u7ef4\u62a4\uff0c\u7f3a\u4e4f\u5c06\u6cd5\u89c4\u77e5\u8bc6\u548c\u4e13\u5bb6\u7ecf\u9a8c\u6574\u5408\u5230\u53ef\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u4e2d\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faFault2Flow\u7cfb\u7edf\uff1a1) \u5c06\u6cd5\u89c4\u903b\u8f91\u63d0\u53d6\u4e3aPASTA\u683c\u5f0f\u6545\u969c\u6811\uff1b2) \u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u6574\u5408\u4e13\u5bb6\u77e5\u8bc6\uff1b3) \u4f7f\u7528AlphaEvolve\u6a21\u5757\u4f18\u5316\u63a8\u7406\u903b\u8f91\uff1b4) \u5408\u6210n8n\u53ef\u6267\u884c\u5de5\u4f5c\u6d41\u3002", "result": "\u5728\u53d8\u538b\u5668\u6545\u969c\u8bca\u65ad\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a100%\u62d3\u6251\u4e00\u81f4\u6027\u548c\u9ad8\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "conclusion": "Fault2Flow\u5efa\u7acb\u4e86\u4ece\u6545\u969c\u5206\u6790\u5230\u64cd\u4f5c\u81ea\u52a8\u5316\u7684\u53ef\u590d\u73b0\u8def\u5f84\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e13\u5bb6\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2511.12090", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12090", "abs": "https://arxiv.org/abs/2511.12090", "authors": ["Shengqin Jiang", "Tianqi Kong", "Yuankai Qi", "Haokui Zhang", "Lina Yao", "Quan Z. Sheng", "Qingshan Liu", "Ming-Hsuan Yang"], "title": "Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning", "comment": "under review", "summary": "Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5206\u7ec4\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u5206\u7ec4\u5171\u4eab\u63d0\u793a\u548c\u4f7f\u7528\u6839\u63d0\u793a\u751f\u6210\u5b50\u63d0\u793a\uff0c\u51cf\u5c11\u5c42\u95f4\u72ec\u7acb\u66f4\u65b0\uff0c\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u63d0\u793a\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u6bcf\u4e2a\u5c42\u72ec\u7acb\u6dfb\u52a0\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\uff0c\u867d\u7136\u7075\u6d3b\u6027\u9ad8\u4f46\u5bb9\u6613\u5bfc\u81f4\u67d0\u4e9b\u5c42\u4e0d\u5fc5\u8981\u66f4\u65b0\uff0c\u53ef\u80fd\u8986\u76d6\u5148\u524d\u4efb\u52a1\u7684\u91cd\u8981\u7279\u5f81\u8868\u793a\uff0c\u589e\u52a0\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\u3002", "method": "\u5206\u5c42\u5206\u7ec4\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff1a1) \u540c\u4e00\u7ec4\u5c42\u5171\u4eab\u5927\u81f4\u76f8\u540c\u7684\u63d0\u793a\uff0c\u901a\u8fc7\u4f4d\u7f6e\u7f16\u7801\u8c03\u6574\uff1b2) \u4f7f\u7528\u5355\u4e00\u4efb\u52a1\u7279\u5b9a\u6839\u63d0\u793a\u5b66\u4e60\u4e3a\u6bcf\u4e2a\u5c42\u7ec4\u751f\u6210\u5b50\u63d0\u793a\uff0c\u589e\u5f3a\u5b50\u63d0\u793a\u95f4\u7684\u534f\u540c\u6027\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u591a\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u5c42\u5206\u7ec4\u63d0\u793a\u8c03\u4f18\u63d0\u9ad8\u4e86\u6a21\u578b\u7a33\u5b9a\u6027\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.12937", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12937", "abs": "https://arxiv.org/abs/2511.12937", "authors": ["Guoyan Wang", "Yanyan Huang", "Chunlin Chen", "Lifeng Wang", "Yuxiang Sun"], "title": "Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models", "comment": "32 pages, 13 figures", "summary": "Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.", "AI": {"tldr": "Yanyun-3\u662f\u4e00\u4e2a\u901a\u7528\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u4e09\u4e2a\u5f02\u6784\u7b56\u7565\u6e38\u620f\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8de8\u5e73\u53f0\u64cd\u4f5c\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548c\u7cbe\u786e\u6267\u884c\u80fd\u529b\uff0c\u5728\u76ee\u6807\u5b9a\u4f4d\u3001\u8d44\u6e90\u5206\u914d\u548c\u533a\u57df\u63a7\u5236\u7b49\u6838\u5fc3\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u8de8\u5e73\u53f0\u7b56\u7565\u6e38\u620f\u4e2d\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u7528\u6237\u754c\u9762\u548c\u52a8\u6001\u6218\u573a\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6cdb\u5316\u95ee\u9898\uff0c\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u6574\u5408Qwen2.5-VL\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u548cUI-TARS\u7684\u7cbe\u786e\u6267\u884c\u80fd\u529b\uff0c\u91c7\u7528\u5c4f\u5e55\u6355\u83b7-\u6a21\u578b\u63a8\u7406-\u52a8\u4f5c\u6267\u884c\u7684\u95ed\u73af\u6d41\u7a0b\uff0c\u7814\u7a76\u4e0d\u540c\u591a\u6a21\u6001\u6570\u636e\u7ec4\u5408\u7b56\u7565\uff08\u9759\u6001\u56fe\u50cf\u3001\u591a\u56fe\u50cf\u5e8f\u5217\u3001\u89c6\u9891\uff09\u7684\u6548\u679c\u3002", "result": "\u6df7\u5408\u7b56\u7565\uff08\u878d\u5408\u591a\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\uff0c\u540c\u65f6\u6df7\u5408\u9759\u6001\u56fe\u50cf\uff09\u76f8\u6bd4\u5b8c\u5168\u878d\u5408\u7b56\u7565\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1163%\uff0cBLEU-4\u5f97\u5206\u4ece4.81%\u63d0\u5347\u81f362.41%\uff08\u7ea612.98\u500d\u63d0\u5347\uff09\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u7b56\u7565\u6e38\u620f\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8fd8\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u6a21\u6001\u6570\u636e\u7ec4\u7ec7\u5efa\u7acb\u4e86\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u901a\u7528\u8303\u5f0f\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4e2d\u9759\u6001\u611f\u77e5\u4e0e\u52a8\u6001\u63a8\u7406\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2511.11698", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11698", "abs": "https://arxiv.org/abs/2511.11698", "authors": ["Chenghao Liu", "Taha Aksu", "Juncheng Liu", "Xu Liu", "Hanshu Yan", "Quang Pham", "Doyen Sahoo", "Caiming Xiong", "Silvio Savarese", "Junnan Li"], "title": "Moirai 2.0: When Less Is More for Time Series Forecasting", "comment": "16 pages, 13 figures, and 1 table", "summary": "We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.", "AI": {"tldr": "Moirai 2.0\u662f\u4e00\u4e2a\u57fa\u4e8e\u89e3\u7801\u5668\u67b6\u6784\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u5206\u4f4d\u6570\u9884\u6d4b\u548c\u591a\u4ee4\u724c\u9884\u6d4b\u6280\u672f\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002\u76f8\u6bd4Moirai 1.0\uff0c\u65b0\u7248\u672c\u7b80\u5316\u4e86\u67b6\u6784\uff0c\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u7f29\u5c0f30\u500d\uff0c\u6027\u80fd\u53cd\u800c\u66f4\u597d\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7b80\u5316\u67b6\u6784\u548c\u6539\u8fdb\u9884\u6d4b\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u5c0f\u578b\u5316\u548c\u5feb\u901f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u89e3\u7801\u5668\u4e13\u7528\u67b6\u6784\uff0c\u4f7f\u7528\u5206\u4f4d\u6570\u9884\u6d4b\u548c\u591a\u4ee4\u724c\u9884\u6d4b\u6280\u672f\uff0c\u66ff\u6362\u4e86\u4e4b\u524d\u7684\u63a9\u7801\u7f16\u7801\u5668\u8bad\u7ec3\u3001\u591a\u8865\u4e01\u8f93\u5165\u548c\u6df7\u5408\u5206\u5e03\u8f93\u51fa\uff0c\u4f7f\u7528\u5355\u8865\u4e01\u548c\u5206\u4f4d\u6570\u635f\u5931\u3002", "result": "\u5728Gift-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u9876\u7ea7\u9884\u8bad\u7ec3\u6a21\u578b\u6c34\u5e73\uff1b\u76f8\u6bd4Moirai 1.0-Large\uff0c\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u6a21\u578b\u5927\u5c0f\u7f29\u5c0f30\u500d\uff0c\u6027\u80fd\u66f4\u597d\uff1b\u5728\u9886\u57df\u7ea7\u522b\u7ed3\u679c\u7a33\u5065\uff0c\u80fd\u8d85\u8d8a\u540c\u7cfb\u5217\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "\u89e3\u7801\u5668\u4e13\u7528\u67b6\u6784\u548c\u9012\u5f52\u591a\u5206\u4f4d\u6570\u89e3\u7801\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\uff1b\u6a21\u578b\u6027\u80fd\u968f\u53c2\u6570\u589e\u52a0\u800c\u8d8b\u4e8e\u5e73\u7a33\uff0c\u5728\u8f83\u957f\u9884\u6d4b\u671f\u8868\u73b0\u4e0b\u964d\uff0c\u672a\u6765\u9700\u8981\u5728\u6570\u636e\u6269\u5c55\u548c\u957f\u671f\u5efa\u6a21\u65b9\u9762\u7ee7\u7eed\u7814\u7a76\u3002"}}
{"id": "2511.12097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12097", "abs": "https://arxiv.org/abs/2511.12097", "authors": ["Shuhan Ye", "Yi Yu", "Qixin Zhang", "Chenqi Kong", "Qiangqiang Wu", "Xudong Jiang", "Dacheng Tao"], "title": "Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks", "comment": null, "summary": "Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \\emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \\emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \\textbf{SpikeNM}, the first SNN-oriented \\emph{semi-structured} \\(N{:}M\\) pruning framework that learns sparse SNNs \\emph{from scratch}, enforcing \\emph{at most \\(N\\)} non-zeros per \\(M\\)-weight block. To avoid the combinatorial space complexity \\(\\sum_{k=1}^{N}\\binom{M}{k}\\) growing exponentially with \\(M\\), SpikeNM adopts an \\(M\\)-way basis-logit parameterization with a differentiable top-\\(k\\) sampler, \\emph{linearizing} per-block complexity to \\(\\mathcal O(M)\\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \\emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \\(2{:}4\\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.", "AI": {"tldr": "SpikeNM\u662f\u9996\u4e2a\u9762\u5411SNN\u7684\u534a\u7ed3\u6784\u5316(N:M)\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7M\u8def\u57fa\u5bf9\u6570\u53c2\u6570\u5316\u548c\u53ef\u5fae\u5206top-k\u91c7\u6837\u5668\uff0c\u5c06\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u4f4e\u5230\u7ebf\u6027\uff0c\u5e76\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u8d44\u683c\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SNN\u6df1\u5ea6\u67b6\u6784\u53c2\u6570\u81a8\u80c0\u95ee\u9898\uff0c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u8981\u4e48\u96be\u4ee5\u786c\u4ef6\u52a0\u901f\uff08\u975e\u7ed3\u6784\u5316\uff09\uff0c\u8981\u4e48\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e14\u7cbe\u5ea6\u4e0b\u964d\uff08\u7ed3\u6784\u5316\uff09\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u786c\u4ef6\u53cb\u597d\u53c8\u80fd\u4fdd\u6301\u7cbe\u5ea6\u7684\u534a\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6848\u3002", "method": "\u91c7\u7528M\u8def\u57fa\u5bf9\u6570\u53c2\u6570\u5316\u548c\u53ef\u5fae\u5206top-k\u91c7\u6837\u5668\u7ebf\u6027\u5316\u590d\u6742\u5ea6\uff0c\u63d0\u51fa\u8d44\u683c\u542f\u53d1\u84b8\u998f(EID)\u5c06\u65f6\u95f4\u7d2f\u79ef\u4fe1\u7528\u8f6c\u6362\u4e3a\u5757\u7ea7\u8f6f\u76ee\u6807\uff0c\u5bf9\u9f50\u63a9\u7801\u6982\u7387\u4e0e\u8109\u51b2\u52a8\u6001\u3002", "result": "\u57282:4\u7a00\u758f\u5ea6\u4e0b\uff0cSpikeNM\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u4ea7\u751f\u786c\u4ef6\u53cb\u597d\u7684\u7a00\u758f\u6a21\u5f0f\uff0c\u4e0e\u5185\u5728\u8109\u51b2\u7a00\u758f\u6027\u4e92\u8865\u3002", "conclusion": "SpikeNM\u6210\u529f\u5b9e\u73b0\u4e86SNN\u7684\u9ad8\u6548\u534a\u7ed3\u6784\u5316\u526a\u679d\uff0c\u5e73\u8861\u4e86\u786c\u4ef6\u52a0\u901f\u9700\u6c42\u548c\u6a21\u578b\u7cbe\u5ea6\uff0c\u4e3a\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.11699", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11699", "abs": "https://arxiv.org/abs/2511.11699", "authors": ["Xingqi Lin", "Liangyu Chen", "Min Wu", "Min Zhang", "Zhenbing Zeng"], "title": "Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification", "comment": null, "summary": "Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \\emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u622a\u65ad\u77e9\u5f62\u68f1\u67f1\u65b9\u6cd5\u6765\u7d27\u5bc6\u903c\u8fd1RNN\u4e2d\u7684Hadamard\u79ef\u4ea7\u751f\u7684\u4e09\u7ef4\u975e\u7ebf\u6027\u66f2\u9762\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4f53\u79ef\u548c\u8868\u9762\u79ef\u5b9e\u73b0\u66f4\u7d27\u5bc6\u7684\u8fc7\u8fd1\u4f3c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86RNN\u9c81\u68d2\u6027\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u8fdb\u884c\u7ebf\u6027\u8fc7\u8fd1\u4f3c\u65f6\uff0c\u91c7\u7528\u5355\u72ec\u7684\u7ebf\u6027\u8fb9\u754c\u5e73\u9762\uff0c\u53ef\u80fd\u5bfc\u81f4\u663e\u8457\u7684\u8fc7\u4f30\u8ba1\u548c\u8f83\u4f4e\u7684\u9a8c\u8bc1\u7cbe\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7d27\u5bc6\u7684\u8fc7\u8fd1\u4f3c\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u622a\u65ad\u77e9\u5f62\u68f1\u67f1\u65b9\u6cd5\uff0c\u7531\u4e24\u4e2a\u7ebf\u6027\u677e\u5f1b\u5e73\u9762\u5f62\u6210\uff0c\u5e76\u91c7\u7528\u7ec6\u5316\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u6700\u5c0f\u5316\u5176\u4f53\u79ef\u548c\u8868\u9762\u79ef\uff0c\u4ee5\u5b9e\u73b0\u5bf9Hadamard\u79ef\u4ea7\u751f\u7684\u4e09\u7ef4\u975e\u7ebf\u6027\u66f2\u9762\u7684\u7d27\u5bc6\u5305\u56f4\u3002\u57fa\u4e8e\u6b64\u8fd1\u4f3c\uff0c\u5b9e\u73b0\u4e86\u540d\u4e3aDeepPrism\u7684RNN\u9c81\u68d2\u6027\u9a8c\u8bc1\u539f\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeepPrism\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8bed\u97f3\u8bc6\u522b\u548c\u60c5\u611f\u5206\u6790\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u622a\u65ad\u77e9\u5f62\u68f1\u67f1\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u7d27\u5bc6\u5730\u903c\u8fd1RNN\u4e2d\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\uff0c\u5728\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12098", "abs": "https://arxiv.org/abs/2511.12098", "authors": ["Xianhao Zhou", "Jianghao Wu", "Ku Zhao", "Jinlong He", "Huangxuan Zhao", "Lei Chen", "Shaoting Zhang", "Guotai Wang"], "title": "DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT", "comment": null, "summary": "Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\\rightarrow$CT and CBCT$\\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.", "AI": {"tldr": "\u63d0\u51fa\u4e86DINOv3\u5f15\u5bfc\u7684\u4ea4\u53c9\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u51bb\u7ed3\u7684\u81ea\u76d1\u7763DINOv3 Transformer\u548c\u53ef\u8bad\u7ec3\u7684CNN\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5c40\u90e8\u5916\u89c2\u548c\u4e0a\u4e0b\u6587\u8868\u793a\u7684\u5e73\u8861\uff0c\u5728SynthRAD2023\u76c6\u8154\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eCNN\u7684\u6a21\u578b\u7f3a\u4e4f\u5168\u5c40\u8bed\u4e49\u7406\u89e3\uff0c\u800cTransformer\u7531\u4e8e\u9ad8\u6a21\u578b\u5bb9\u91cf\u548c\u5f31\u5f52\u7eb3\u504f\u7f6e\u5bb9\u6613\u5728\u5c0f\u578b\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8fc7\u62df\u5408\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u533b\u5b66\u56fe\u50cf\u5408\u6210\u3002", "method": "\u63d0\u51faDINOv3\u5f15\u5bfc\u7684\u4ea4\u53c9\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u4ea4\u53c9\u878d\u5408\u6a21\u5757\u5c42\u6b21\u5316\u878d\u5408Transformer\u7684\u5168\u5c40\u8868\u793a\u548cCNN\u7684\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u591a\u7ea7DINOv3\u611f\u77e5\u635f\u5931\u6765\u9f13\u52b1\u5408\u6210CT\u4e0e\u771f\u5b9eCT\u5728DINOv3\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u5728SynthRAD2023\u76c6\u8154\u6570\u636e\u96c6\u4e0a\uff0cDGCF\u5728MRI\u2192CT\u548cCBCT\u2192CT\u8f6c\u6362\u4efb\u52a1\u4e2d\uff0c\u5728MS-SSIM\u3001PSNR\u548c\u57fa\u4e8e\u5206\u5272\u7684\u6307\u6807\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u533b\u5b66\u56fe\u50cf\u7ffb\u8bd1\u4e2d\u4f7f\u7528DINOv3\u8868\u793a\u7684\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u81ea\u76d1\u7763Transformer\u5f15\u5bfc\u5728\u8bed\u4e49\u611f\u77e5CT\u5408\u6210\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13007", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13007", "abs": "https://arxiv.org/abs/2511.13007", "authors": ["Yiyang Zhao", "Huiyu Bai", "Xuejiao Zhao"], "title": "GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs", "comment": "This paper has been accepted by AAAI 2026-AIA and designated as an oral presentation paper", "summary": "Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.", "AI": {"tldr": "\u63d0\u51faGEM\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u71b5\u5f15\u5bfc\u504f\u597d\u5efa\u6a21\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4f4e\u8d44\u6e90\u548c\u9886\u57df\u7279\u5b9a\u573a\u666f\u4e0b\u8fdb\u884c\u5bf9\u9f50\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e", "motivation": "\u5728\u533b\u5b66\u3001\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u9886\u57df\u96be\u4ee5\u83b7\u5f97\u5927\u89c4\u6a21\u504f\u597d\u6807\u6ce8\u6570\u636e\uff0c\u9700\u8981\u5f00\u53d1\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684LLM\u5bf9\u9f50\u65b9\u6cd5", "method": "\u57fa\u4e8e\u71b5\u7406\u8bba\u7684\u8ba4\u77e5\u8fc7\u6ee4\u6a21\u5757\u4f7f\u7528CoT\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u94fe\uff0c\u901a\u8fc7token\u8bc4\u5206\u673a\u5236\u5bf9\u63a8\u7406\u94fe\u8fdb\u884c\u6392\u5e8f\u52a0\u6743\uff0c\u7136\u540e\u4f7f\u7528SEGA\u7b97\u6cd5\u8fdb\u884c\u5fae\u8c03", "result": "\u5728\u901a\u7528\u57fa\u51c6\u548c\u9886\u57df\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\u548c\u533b\u7597\u5bf9\u8bdd\uff09\u4e0a\uff0cGEM\u5728\u5c11\u6837\u672c\u504f\u597d\u6570\u636e\u4e0b\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "conclusion": "GEM\u5efa\u7acb\u4e86\u71b5\u5f15\u5bfc\u7684\u95ed\u73af\u8ba4\u77e5\u4f18\u5316\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u591f\u4f9d\u9760\u81ea\u8eab\u5224\u65ad\u5b9e\u73b0\u9ad8\u6548\u7684\u5c11\u6837\u672c\u5bf9\u9f50"}}
{"id": "2511.12099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12099", "abs": "https://arxiv.org/abs/2511.12099", "authors": ["Tianle Cheng", "Zeyan Zhang", "Kaifeng Gao", "Jun Xiao"], "title": "Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models", "comment": null, "summary": "Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.", "AI": {"tldr": "\u63d0\u51faAdaptive Begin-of-Video Tokens (ada-BOV)\u7528\u4e8e\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5c42\u5f52\u4e00\u5316\u8c03\u5236\u5438\u6536\u53bb\u566a\u7684\u524d\u5e27\uff0c\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u5e76\u6539\u5584\u5c40\u90e8\u52a8\u6001\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u5b58\u5728\u4e24\u79cd\u4e3b\u8981\u8303\u5f0f\uff1a\u57fa\u4e8e\u5757\u7684\u6269\u5c55\u548c\u6d41\u53bb\u566a\u3002\u524d\u8005\u5b58\u5728\u53bb\u566a\u5ef6\u8fdf\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u540e\u8005\u5728\u4e00\u81f4\u6027\u4fdd\u6301\u548c\u8fd0\u52a8\u52a8\u6001\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faada-BOV\u53ef\u5b66\u4e60\u5d4c\u5165\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5c42\u5f52\u4e00\u5316\u8c03\u5236\u5438\u6536\u524d\u5e27\u4fe1\u606f\uff1b\u63d0\u51fa\u6d41\u53bb\u566a\u7ec6\u5316\u7b56\u7565\uff0c\u89e3\u8026\u91c7\u6837\u8f68\u8ff9\u957f\u5ea6\u4e0e\u6ce8\u610f\u529b\u7a97\u53e3\u7ea6\u675f\uff1b\u63d0\u51fa\u6270\u52a8\u589e\u5f3a\u8bad\u7ec3\u566a\u58f0\u8c03\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\uff0c\u663e\u8457\u6539\u5584\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u8fde\u8d2f\u6027\u548c\u52a8\u6001\u8d28\u91cf\u3002", "conclusion": "ada-BOV\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u52a8\u6001\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u9ad8\u8d28\u91cf\u957f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.11703", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.11703", "abs": "https://arxiv.org/abs/2511.11703", "authors": ["Hugo Huang"], "title": "Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom", "comment": "Master's Thesis at the University of Edinburgh (2024)", "summary": "Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u8f93\u5165\u8868\u793a\u65b9\u6cd5\uff08SS-only\u548cRGB+SS\uff09\uff0c\u89e3\u51b3\u4e863D\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u5185\u5b58\u6d88\u8017\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u5728ViZDoom\u6b7b\u4ea1\u7ade\u8d5b\u4e2d\u9a8c\u8bc1\u4e86\u5185\u5b58\u51cf\u5c1166.6%-98.6%\u4e14\u6027\u80fd\u63d0\u5347\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b33D\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u9ad8\u7ef4\u611f\u5b98\u8f93\u5165\u5bfc\u81f4\u7684\u5185\u5b58\u7f13\u51b2\u533a\u9ad8\u5185\u5b58\u6d88\u8017\uff0c\u4ee5\u53ca\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u5b66\u4e60\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u7684\u8f93\u5165\u8868\u793a\u65b9\u6cd5\uff1aSS-only\uff08\u4ec5\u8bed\u4e49\u5206\u5272\uff09\u548cRGB+SS\uff08RGB+\u8bed\u4e49\u5206\u5272\uff09\uff0c\u5728ViZDoom\u6b7b\u4ea1\u7ade\u8d5b\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5b8c\u7f8e\u5206\u5272\u7ed3\u679c\u8fdb\u884c\u53d7\u63a7\u8bc4\u4f30\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u5bc6\u5ea6\u7684\u70ed\u529b\u56fe\u53ef\u89c6\u5316RL\u667a\u80fd\u4f53\u79fb\u52a8\u6a21\u5f0f\u3002", "result": "SS-only\u65b9\u6cd5\u80fd\u591f\u5c06\u5185\u5b58\u7f13\u51b2\u533a\u5185\u5b58\u6d88\u8017\u51cf\u5c11\u81f3\u5c1166.6%\uff0c\u5728\u5e94\u7528\u6e38\u7a0b\u7f16\u7801\u7b49\u65e0\u635f\u538b\u7f29\u6280\u672f\u65f6\u6700\u9ad8\u53ef\u51cf\u5c1198.6%\uff1bRGB+SS\u901a\u8fc7\u63d0\u4f9b\u989d\u5916\u8bed\u4e49\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86RL\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8bed\u4e49\u5206\u5272\u8f93\u5165\u8868\u793a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D\u73af\u5883\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u5185\u5b58\u6d88\u8017\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u70ed\u529b\u56fe\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u6570\u636e\u6536\u96c6\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2511.13021", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13021", "abs": "https://arxiv.org/abs/2511.13021", "authors": ["Sachin Vashistha", "Aryan Bibhuti", "Atharva Naik", "Martin Tutek", "Somak Aditya"], "title": "PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics", "comment": "23 pages, 15 tables, 10 figures; AAAI 2026 Conference Main Track (oral)", "summary": "Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u7f16\u7801\u548c\u66f4\u65b0\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\uff0c\u6d4b\u8bd5\u4e86\u5b83\u4eec\u5728\u8bed\u8a00\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u6784\u5efa\u5305\u542b\u662f-\u5426\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u7ef4\u6301\u51c6\u786e\u7387\uff0c\u7279\u522b\u662f\u5728\u8ddf\u8e2a\u5b9e\u4f53\u65b9\u9762\u3002\u63d0\u51fa\u4e86\u53cc\u91cd\u89c6\u89d2\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u8bc6\u522b\u6709\u5bb3\u5c42\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5c42\u6b63\u5219\u5316\u5fae\u8c03\u7b56\u7565\u3002", "motivation": "\u73b0\u5b9e\u5bf9\u8bdd\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u7528\u5143\u7d20\uff0c\u9700\u8981\u6784\u5efa\u5c40\u90e8\u4e16\u754c\u6a21\u578b\u6765\u7f16\u7801\u8fd9\u4e9b\u5143\u7d20\u5e76\u6355\u6349\u5176\u72b6\u6001\u6f14\u53d8\u3002\u7136\u800c\uff0c\u8bed\u8a00\u6a21\u578b\u662f\u5426\u6784\u5efa\u6216\u7ef4\u62a4\u7a33\u5065\u7684\u9690\u5f0f\u5bf9\u8bdd\u8868\u793a\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5bf9\u6d41\u884c\u6570\u636e\u96c6\u4e2d\u7684\u5bf9\u8bdd\u5e94\u7528\u4e03\u79cd\u6700\u5c0f\u8bed\u8a00\u53d8\u5316\uff0c\u6784\u5efa\u4e24\u4e2a\u5305\u542b\u662f-\u5426\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u8bc4\u4f30\u4e86\u5e7f\u6cdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u53cc\u91cd\u89c6\u89d2\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u8bc6\u522b\u6709\u7528\u548c\u6709\u5bb3\u7684transformer\u5c42\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u5c42\u6b63\u5219\u5316\u7684\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u53d8\u5316\u4e0b\u96be\u4ee5\u7ef4\u6301\u7a33\u5065\u7684\u51c6\u786e\u7387\uff0c\u7279\u522b\u662f\u5728\u8bb0\u5fc6\u5173\u952e\u7ec6\u8282\u548c\u8ddf\u8e2a\u5b9e\u4f53\u65b9\u9762\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u6709\u5bb3\u5c42\u901a\u5e38\u7531\u4e8e\u7f16\u7801\u865a\u5047\u4fe1\u53f7\u6216\u4f9d\u8d56\u6377\u5f84\u800c\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u6784\u5efa\u548c\u7ef4\u62a4\u7a33\u5065\u4e16\u754c\u6a21\u578b\u7684\u80fd\u529b\u6709\u9650\uff0c\u63d0\u51fa\u7684\u5c42\u6b63\u5219\u5316\u5fae\u8c03\u7b56\u7565\u80fd\u591f\u6709\u6548\u6291\u5236\u6709\u5bb3\u5c42\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u6a21\u578b\u5728\u8bed\u8a00\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.12100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12100", "abs": "https://arxiv.org/abs/2511.12100", "authors": ["Yannan Chen", "Ruoyu Chen", "Bin Zeng", "Wei Wang", "Shiming Liu", "Qunli Zhang", "Zheng Hu", "Laiyuan Wang", "Yaowei Wang", "Xiaochun Cao"], "title": "Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation", "comment": null, "summary": "In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.", "AI": {"tldr": "\u63d0\u51faSS-CA\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u589e\u5f3a\u6765\u7ea0\u6b63\u89c6\u89c9\u6a21\u578b\u5bf9\u6709\u9650\u56e0\u679c\u7279\u5f81\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u6a21\u578b\u4ec5\u4f9d\u8d56\u6709\u9650\u7684\u5145\u5206\u539f\u56e0\u8fdb\u884c\u9884\u6d4b\uff0c\u4f7f\u5176\u5bf9\u5206\u5e03\u504f\u79fb\u6216\u5173\u952e\u7279\u5f81\u7f3a\u5931\u654f\u611f\u3002\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u8bc6\u522b\u53cd\u4e8b\u5b9e\u6837\u672c\u65f6\u7684\u5dee\u5f02\u8868\u660e\u6a21\u578b\u5b66\u4e60\u5230\u7684\u4f9d\u8d56\u5173\u7cfb\u53ef\u80fd\u4e0d\u591f\u56e0\u679c\u5145\u5206\u3002", "method": "\u57fa\u4e8eLIMA\u5f52\u56e0\u65b9\u6cd5\u5f00\u53d1Counterfactual LIMA\uff0c\u8bc6\u522b\u6700\u5c0f\u7a7a\u95f4\u533a\u57df\u96c6\uff0c\u5176\u79fb\u9664\u53ef\u9009\u62e9\u6027\u6539\u53d8\u6a21\u578b\u9884\u6d4b\u3002\u5229\u7528\u8fd9\u4e9b\u5f52\u56e0\uff0c\u63d0\u51fa\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5c06\u8bc6\u522b\u533a\u57df\u66ff\u6362\u4e3a\u81ea\u7136\u80cc\u666f\uff0c\u5e76\u5728\u589e\u5f3a\u548c\u539f\u59cb\u6837\u672c\u4e0a\u8054\u5408\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2aImageNet\u53d8\u4f53\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSS-CA\u63d0\u9ad8\u4e86\u5206\u5e03\u5185\u6d4b\u8bd5\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728ImageNet-R\u548cImageNet-S\u7b49\u5206\u5e03\u5916\u57fa\u51c6\u4e0a\u83b7\u5f97\u4f18\u8d8a\u6027\u80fd\u3002\u5728\u566a\u58f0\u7b49\u6270\u52a8\u4e0b\uff0cSS-CA\u8bad\u7ec3\u7684\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SS-CA\u65b9\u6cd5\u6709\u6548\u5229\u7528\u53ef\u89e3\u91ca\u6027\u6d1e\u5bdf\u6765\u7ea0\u6b63\u6a21\u578b\u7f3a\u9677\uff0c\u63d0\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u5c06\u53cd\u4e8b\u5b9e\u89e3\u91ca\u76f4\u63a5\u6574\u5408\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u7f13\u89e3\u4e0d\u5b8c\u6574\u56e0\u679c\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2511.11704", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11704", "abs": "https://arxiv.org/abs/2511.11704", "authors": ["Matvey Skripkin", "Elizaveta Goncharova", "Andrey Kuznetsov"], "title": "Simple Vision-Language Math Reasoning via Rendered Text", "comment": null, "summary": "We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u6709\u6548\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u5c06LaTeX\u516c\u5f0f\u6e32\u67d3\u4e3a\u56fe\u50cf\u5e76\u642d\u914d\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u4f7f\u7d27\u51d1\u578b\u591a\u6a21\u6001\u67b6\u6784\u5728\u6570\u5b66\u95ee\u9898\u6c42\u89e3\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u9700\u8981\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u6570\u5b66\u516c\u5f0f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e0c\u671b\u901a\u8fc7\u89c6\u89c9\u5316\u8868\u793a\u63d0\u5347\u6a21\u578b\u5bf9\u6570\u5b66\u95ee\u9898\u7684\u7406\u89e3\u548c\u6c42\u89e3\u80fd\u529b\u3002", "method": "\u5c06LaTeX\u7f16\u7801\u7684\u6570\u5b66\u516c\u5f0f\u6e32\u67d3\u6210\u56fe\u50cf\uff0c\u5e76\u4e0e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63d0\u793a\u914d\u5bf9\uff0c\u6784\u5efa\u6587\u672c\u5230\u89c6\u89c9\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u8bad\u7ec3\u7d27\u51d1\u578b\u591a\u6a21\u6001\u67b6\u6784\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u5339\u914d\u6216\u8d85\u8d8a\u5f00\u6e90\u548c\u4e13\u6709\u7684\u6570\u5b66\u89c6\u89c9\u8bed\u8a00\u6c42\u89e3\u5668\uff0c\u540c\u65f6\u5728MMMU\u3001ChartQA\u548cDocVQA\u7b49\u4efb\u52a1\u4e0a\u63d0\u5347\u9ad8\u8fbe20%\u3002", "conclusion": "\u6e32\u67d3\u4fdd\u771f\u5ea6\u548c\u63d0\u793a\u8bbe\u8ba1\u662f\u6027\u80fd\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u8fd9\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u4fdd\u6301\u4e86\u5e7f\u6cdb\u7684\u901a\u7528\u9886\u57df\u80fd\u529b\u3002"}}
{"id": "2511.13027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13027", "abs": "https://arxiv.org/abs/2511.13027", "authors": ["Sadegh Mahdavi", "Branislav Kisacanin", "Shubham Toshniwal", "Wei Du", "Ivan Moshkov", "George Armstrong", "Renjie Liao", "Christos Thrampoulidis", "Igor Gitman"], "title": "Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection", "comment": null, "summary": "Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u8bc1\u660e\u9a8c\u8bc1\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5206\u6790\u4e86\u591a\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff0c\u53d1\u73b0\u5355\u4e00\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u8bba\u3002\u4f5c\u8005\u8bc4\u4f30\u4e86\u8bc1\u660e\u63a8\u7406\u548c\u6700\u7ec8\u7b54\u6848\u63a8\u7406\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u751f\u6210\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u53d1\u73b0\u5b83\u4eec\u7684\u7ec4\u5408\u6700\u6709\u6548\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u63d0\u793a\u9009\u62e9\u5bf9LLM-as-a-Judge\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u964d\u4f4e\u8fd9\u79cd\u654f\u611f\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6700\u7ec8\u7b54\u6848\u6570\u5b66\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u5f80\u5f80\u5b58\u5728\u7f3a\u9677\u3002\u4e3a\u4e86\u63a8\u8fdb\u5230\u4e25\u8c28\u7684\u8bc1\u660e\u6570\u5b66\uff0c\u9700\u8981\u53ef\u9760\u7684\u8bc1\u660e\u9a8c\u8bc1\u80fd\u529b\u3002", "method": "\u5206\u6790\u591a\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff0c\u540c\u65f6\u8bc4\u4f30\u8bc1\u660e\u63a8\u7406\u548c\u6700\u7ec8\u7b54\u6848\u63a8\u7406\uff1b\u5c06\u4e24\u79cd\u4e3b\u8981\u751f\u6210\u9a8c\u8bc1\u65b9\u6cd5\uff08GenSelect\u548cLLM-as-a-Judge\uff09\u6269\u5c55\u5230\u6570\u767e\u4e07token\uff1b\u7814\u7a76\u63d0\u793a\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u964d\u4f4e\u654f\u611f\u6027\u3002", "result": "\u8bc1\u660e\u63a8\u7406\u548c\u6700\u7ec8\u7b54\u6848\u63a8\u7406\u7684\u7ec4\u5408\u8bc4\u4f30\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6027\u80fd\u6d4b\u91cf\uff1bGenSelect\u548cLLM-as-a-Judge\u7684\u7ec4\u5408\u662f\u6700\u6709\u6548\u7684\u9a8c\u8bc1\u6846\u67b6\uff1b\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u964d\u4f4e\u5bf9\u63d0\u793a\u9009\u62e9\u7684\u654f\u611f\u6027\uff1b\u4f46\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u6539\u5584\u4e86\u8bc1\u660e\u7ea7\u6307\u6807\uff0c\u5374\u6ca1\u6709\u63d0\u9ad8\u6700\u7ec8\u7b54\u6848\u7cbe\u5ea6\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5f80\u5f80\u5956\u52b1\u98ce\u683c\u6216\u7a0b\u5e8f\u6b63\u786e\u6027\u800c\u975e\u6570\u5b66\u6709\u6548\u6027\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u548c\u8bc4\u4f30\u53ef\u6269\u5c55\u7684\u8bc1\u660e\u9a8c\u8bc1\u548c\u9009\u62e9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2511.12103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12103", "abs": "https://arxiv.org/abs/2511.12103", "authors": ["Sayad Ibna Azad", "Md. Atiqur Rahman"], "title": "BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation", "comment": "Accepted to 20th International Symposium on Visual Computing (ISVC 2025)", "summary": "We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.", "AI": {"tldr": "BdSL-SPOTER\u662f\u4e00\u4e2a\u57fa\u4e8e\u59ff\u6001\u7684transformer\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9ad8\u6548\u5730\u8bc6\u522b\u5b5f\u52a0\u62c9\u624b\u8bed\uff08BdSL\uff09\uff0c\u5728BdSLW60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523097.92%\u7684Top-1\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u6bd4Bi-LSTM\u57fa\u7ebf\u63d0\u534722.82%\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4e3a\u5b5f\u52a0\u62c9\u624b\u8bed\u5f00\u53d1\u4e00\u4e2a\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u624b\u8bed\u8bc6\u522b\u6846\u67b6\uff0c\u89e3\u51b3\u4f4e\u8d44\u6e90\u533a\u57df\u624b\u8bed\u8bc6\u522b\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u5176\u4ed6\u4f4e\u8d44\u6e90\u533a\u57df\u624b\u8bed\u63d0\u4f9b\u53ef\u6269\u5c55\u6a21\u578b\u3002", "method": "\u6269\u5c55SPOTER\u8303\u5f0f\uff0c\u91c7\u7528\u6587\u5316\u7279\u5b9a\u7684\u9884\u5904\u7406\uff0c\u4f7f\u7528\u7d27\u51d1\u7684\u56db\u5c42transformer\u7f16\u7801\u5668\uff0c\u4f18\u5316\u53ef\u5b66\u4e60\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e76\u5e94\u7528\u8bfe\u7a0b\u5b66\u4e60\u6765\u589e\u5f3a\u5728\u6709\u9650\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u52a0\u901f\u6536\u655b\u3002", "result": "\u5728BdSLW60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523097.92%\u7684Top-1\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u6bd4Bi-LSTM\u57fa\u7ebf\u63d0\u534722.82%\uff0c\u540c\u65f6\u5177\u6709\u66f4\u5c11\u7684\u53c2\u6570\u3001\u66f4\u4f4e\u7684FLOPs\u548c\u66f4\u9ad8\u7684FPS\u3002", "conclusion": "BdSL-SPOTER\u4e3a\u73b0\u5b9e\u4e16\u754c\u53ef\u8bbf\u95ee\u6027\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5176\u4ed6\u4f4e\u8d44\u6e90\u533a\u57df\u624b\u8bed\u7684\u53ef\u6269\u5c55\u6a21\u578b\u3002"}}
{"id": "2511.11705", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11705", "abs": "https://arxiv.org/abs/2511.11705", "authors": ["Arya Narang"], "title": "Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs", "comment": null, "summary": "This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6bd4\u8f83\u4ec5\u4f7f\u7528\u56fe\u50cf\u7684CNN\u6a21\u578b\u548c\u540c\u65f6\u4f7f\u7528\u6587\u672c\u4e0e\u56fe\u50cf\u7684\u591a\u6a21\u6001CNN\u6a21\u578b\uff0c\u53d1\u73b0\u6dfb\u52a0\u83dc\u54c1\u540d\u79f0\u6587\u672c\u8f93\u5165\u53ef\u5c06\u70ed\u91cf\u4f30\u8ba1\u7684MAE\u4ece84.76\u5343\u5361\u964d\u4f4e\u523083.70\u5343\u5361\uff0c\u63d0\u53471.25%\u3002", "motivation": "\u63a2\u7a76\u77ed\u6587\u672c\u8f93\u5165\uff08\u5982\u83dc\u54c1\u540d\u79f0\uff09\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u6539\u5584\u70ed\u91cf\u4f30\u8ba1\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u56fe\u50cf\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u6539\u8fdb\u662f\u5426\u5177\u6709\u7edf\u8ba1\u663e\u8457\u6027\u3002", "method": "\u4f7f\u7528TensorFlow\u5e93\u548cNutrition5k\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4ec5\u4f7f\u7528\u56fe\u50cf\u7684CNN\u6a21\u578b\u548c\u63a5\u53d7\u6587\u672c\u4e0e\u56fe\u50cf\u8f93\u5165\u7684\u591a\u6a21\u6001CNN\u6a21\u578b\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u5c06\u70ed\u91cf\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4ece84.76\u5343\u5361\u964d\u4f4e\u523083.70\u5343\u5361\uff0c\u51cf\u5c11\u4e861.06\u5343\u5361\uff0c\u63d0\u5347\u5e45\u5ea6\u4e3a1.25%\u3002", "conclusion": "\u77ed\u6587\u672c\u8f93\u5165\uff08\u83dc\u54c1\u540d\u79f0\uff09\u80fd\u591f\u5c0f\u5e45\u4f46\u663e\u8457\u5730\u6539\u5584\u70ed\u91cf\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u76f8\u6bd4\u4ec5\u4f7f\u7528\u56fe\u50cf\u7684\u65b9\u6cd5\u5177\u6709\u7edf\u8ba1\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2511.12104", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12104", "abs": "https://arxiv.org/abs/2511.12104", "authors": ["Tammy Glazer", "Gilles Q. Hacheme", "Akram Zaytar", "Luana Marotti", "Amy Michaels", "Girmaw Abebe Tadesse", "Kevin White", "Rahul Dodhia", "Andrew Zolli", "Inbal Becker-Reshef", "Juan M. Lavista Ferres", "Caleb Robinson"], "title": "TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery", "comment": null, "summary": "We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.", "AI": {"tldr": "TEMPO\u662f\u4e00\u4e2a\u5168\u7403\u6027\u7684\u3001\u65f6\u95f4\u5206\u8fa8\u7684\u5efa\u7b51\u5bc6\u5ea6\u548c\u9ad8\u5ea6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u63d0\u53d6\uff0c\u63d0\u4f9b\u4ece2018\u5e74\u7b2c\u4e00\u5b63\u5ea6\u52302025\u5e74\u7b2c\u4e8c\u5b63\u5ea6\u7684\u5b63\u5ea6\u66f4\u65b0\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u5927\u89c4\u6a21\u76d1\u6d4b\u53d1\u5c55\u6a21\u5f0f\u548c\u6c14\u5019\u5f71\u54cd\uff0c\u652f\u6301\u5168\u7403\u97e7\u6027\u548c\u9002\u5e94\u5de5\u4f5c\uff0c\u9700\u8981\u4e00\u79cd\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u7684\u65b9\u6cd5\u6765\u6355\u6349\u5efa\u6210\u533a\u7684\u5b63\u5ea6\u53d8\u5316\u3002", "method": "\u5c06\u73b0\u6709\u6570\u636e\u96c6\u7684\u5efa\u7b51\u8db3\u8ff9\u548c\u9ad8\u5ea6\u6570\u636e\u4e0e\u5b63\u5ea6PlanetScope\u536b\u661f\u56fe\u50cf\u914d\u5bf9\uff0c\u8bad\u7ec3\u591a\u4efb\u52a1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u9884\u6d4b37.6\u7c73/\u50cf\u7d20\u5206\u8fa8\u7387\u7684\u5efa\u7b51\u5bc6\u5ea6\u548c\u9ad8\u5ea6\u3002", "result": "\u9a8c\u8bc1\u663e\u793a\u5728\u4e0d\u540c\u4eba\u5de5\u6807\u8bb0\u5b50\u96c6\u4e0aF1\u5206\u6570\u8fbe\u523085%-88%\uff0c\u65f6\u95f4\u7a33\u5b9a\u6027\u9ad8\uff0c\u4e94\u5e74\u8d8b\u52bf\u4e00\u81f4\u6027\u5f97\u5206\u4e3a0.96\uff0c\u8ba1\u7b97\u6210\u672c\u8fdc\u4f4e\u4e8e\u53ef\u6bd4\u65b9\u6cd5\u3002", "conclusion": "TEMPO\u80fd\u591f\u4ee5\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u6355\u6349\u5efa\u6210\u533a\u7684\u5b63\u5ea6\u53d8\u5316\uff0c\u4e3a\u5927\u89c4\u6a21\u76d1\u6d4b\u53d1\u5c55\u6a21\u5f0f\u548c\u6c14\u5019\u5f71\u54cd\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.11706", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11706", "abs": "https://arxiv.org/abs/2511.11706", "authors": ["Julia Peters", "Karin Mora", "Miguel D. Mahecha", "Chaonan Ji", "David Montero", "Clemens Mosig", "Guido Kraemer"], "title": "Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling", "comment": "10 pages (incliding 2 pages of references), 7 figures", "summary": "Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5730\u7403\u89c2\u6d4b\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u591a\u6a21\u6001\u9065\u611f\u6570\u636e\u5230\u9ad8\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u56fa\u5b9a\u65f6\u7a7a\u5c3a\u5ea6\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\u901a\u5e38\u5728\u56fa\u5b9a\u65f6\u7a7a\u5c3a\u5ea6\u4e0b\u8fd0\u884c\uff0c\u9650\u5236\u4e86\u9700\u8981\u7cbe\u7ec6\u7a7a\u95f4\u7ec6\u8282\u548c\u9ad8\u65f6\u95f4\u4fdd\u771f\u5ea6\u7684\u751f\u6001\u5206\u6790\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bbe\u8ba1\uff1a\u9996\u5148\u72ec\u7acb\u5efa\u6a21\u5404\u4f20\u611f\u5668\u7279\u5f81\uff0c\u7136\u540e\u5c06\u8868\u793a\u7ec4\u5408\u5230\u5171\u4eab\u6a21\u578b\u4e2d\u3002\u4f7f\u7528Sentinel-1\u548cSentinel-2\u6570\u636e\u4f5c\u4e3a\u4ee3\u8868\u6027\u6a21\u6001\uff0c\u5b9e\u73b010\u7c73\u5206\u8fa8\u7387\u548c\u65e0\u4e91Sentinel-2\u91c7\u96c6\u9891\u7387\u3002", "result": "\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u5728\u5f02\u8d28\u666f\u89c2\u4e2d\u8868\u73b0\u51fa\u9ad8\u7a7a\u95f4\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5728\u603b\u521d\u7ea7\u751f\u4ea7\u529b\u5efa\u6a21\u4e2d\u7f16\u7801\u4e86\u751f\u6001\u5b66\u610f\u4e49\u7684\u6a21\u5f0f\u5e76\u4fdd\u6301\u8db3\u591f\u7684\u65f6\u95f4\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9700\u8981\u4e0d\u540c\u65f6\u7a7a\u5206\u8fa8\u7387\u7684\u73af\u5883\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u5206\u6790\u5c31\u7eea\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2511.13091", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13091", "abs": "https://arxiv.org/abs/2511.13091", "authors": ["Yuhan Chen", "Yuxuan Liu", "Long Zhang", "Pengzhi Gao", "Jian Luan", "Wei Liu"], "title": "STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization", "comment": null, "summary": "Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.", "AI": {"tldr": "STEP\u662f\u4e00\u4e2a\u89e3\u51b3\u591a\u8f6e\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u4efb\u52a1\u6210\u529f\u7387\u7684\u52a8\u6001\u91c7\u6837\u5206\u914d\u548c\u6b65\u9aa4\u7ea7\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u8f68\u8ff9\u7ea7\u4f18\u5316\u65b9\u6cd5\u5728\u591a\u8f6e\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u5b66\u4e60\u4fe1\u53f7\u8bef\u5bfc\u7684\u95ee\u9898\uff0c\u5305\u62ec\u5bf9\u6240\u6709\u4efb\u52a1\u91c7\u7528\u7edf\u4e00\u91c7\u6837\u3001\u5728\u5931\u8d25\u8f68\u8ff9\u4e2d\u60e9\u7f5a\u6b63\u786e\u7684\u4e2d\u95f4\u52a8\u4f5c\u4ee5\u53ca\u9ad8\u6602\u7684\u6837\u672c\u6536\u96c6\u6210\u672c\u3002", "method": "STEP\u6846\u67b6\u7ef4\u62a4\u5e73\u6ed1\u7684\u6210\u529f\u7387\u8bb0\u5f55\u6765\u6307\u5bfc\u81ea\u9002\u5e94\u8f68\u8ff9\u91cd\u91c7\u6837\uff0c\u4e3a\u56f0\u96be\u4efb\u52a1\u5206\u914d\u66f4\u591a\u8d44\u6e90\uff1b\u8ba1\u7b97\u6210\u529f\u7387\u52a0\u6743\u7684\u4f18\u52bf\u51fd\u6570\u5e76\u5c06\u8f68\u8ff9\u5206\u89e3\u4e3a\u6b65\u9aa4\u7ea7\u6837\u672c\uff1b\u5e94\u7528\u6b65\u9aa4\u7ea7GRPO\u589e\u5f3a\u6765\u6539\u8fdb\u4f4e\u6210\u529f\u7387\u4efb\u52a1\u7684\u66f4\u65b0\u3002", "result": "\u5728OSWorld\u548cAndroidWorld\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTEP\u76f8\u6bd4\u8f68\u8ff9\u7ea7GRPO\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5728\u76f8\u540c\u91c7\u6837\u9884\u7b97\u4e0b\u6536\u655b\u66f4\u5feb\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u597d\u3002", "conclusion": "STEP\u901a\u8fc7\u52a8\u6001\u91c7\u6837\u5206\u914d\u548c\u6b65\u9aa4\u7ea7\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u4ea4\u4e92\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\u3002"}}
{"id": "2511.12110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12110", "abs": "https://arxiv.org/abs/2511.12110", "authors": ["Qinyue Tong", "Ziqian Lu", "Jun Liu", "Rui Zuo", "Zheming Lu"], "title": "MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images", "comment": "12pages, 6 figures", "summary": "Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.", "AI": {"tldr": "MEMR-Seg\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8f6e\u5b9e\u4f53\u7ea7\u533b\u5b66\u63a8\u7406\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efaMR-MedSeg\u6570\u636e\u96c6\u548c\u63d0\u51faMediRound\u6a21\u578b\u6765\u89e3\u51b3\u4f20\u7edf\u533b\u5b66\u5206\u5272\u65b9\u6cd5\u7f3a\u4e4f\u4ea4\u4e92\u6027\u548c\u591a\u8f6e\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\u4e14\u7f3a\u4e4f\u4ea4\u4e92\u6027\uff0c\u6587\u672c\u63d0\u793a\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u8f6e\u5bf9\u8bdd\uff0c\u65e0\u6cd5\u8fdb\u884c\u591a\u8f6e\u63a8\u7406\u3002", "method": "\u6784\u5efaMR-MedSeg\u6570\u636e\u96c6\uff0817.7\u4e07\u6761\u591a\u8f6e\u533b\u5b66\u5206\u5272\u5bf9\u8bdd\uff09\uff0c\u63d0\u51faMediRound\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u7684\u5224\u65ad\u4e0e\u6821\u6b63\u673a\u5236\u6765\u7f13\u89e3\u591a\u8f6e\u5206\u5272\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MEMR-Seg\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7684\u533b\u5b66\u53c2\u8003\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "MEMR-Seg\u4efb\u52a1\u548cMediRound\u6a21\u578b\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u591a\u8f6e\u4ea4\u4e92\u63a8\u7406\u7684\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.12117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12117", "abs": "https://arxiv.org/abs/2511.12117", "authors": ["Ruiqi Cheng", "Huijun Di", "Jian Li", "Feng Liu", "Wei Liang"], "title": "RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving", "comment": "12 pages, 6 figures. Accepted by AAAI 2026", "summary": "Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.", "AI": {"tldr": "RadarMP\u662f\u4e00\u79cd\u4f7f\u75284D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4fe1\u53f7\u8fdb\u884c\u7cbe\u786e3D\u573a\u666f\u8fd0\u52a8\u611f\u77e5\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u548c\u8fd0\u52a8\u4f30\u8ba1\uff0c\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u3002", "motivation": "4D\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5177\u6709\u5168\u5929\u5019\u5de5\u4f5c\u80fd\u529b\uff0c\u4f46\u7a00\u758f\u548c\u5608\u6742\u7684\u96f7\u8fbe\u70b9\u5f80\u5f80\u5bfc\u81f4\u8fd0\u52a8\u611f\u77e5\u4e0d\u7cbe\u786e\uff0c\u5728\u5149\u5b66\u4f20\u611f\u5668\u6027\u80fd\u4e0b\u964d\u65f6\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51faRadarMP\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e24\u5e27\u8fde\u7eed\u7684\u4f4e\u7ea7\u96f7\u8fbe\u56de\u6ce2\u4fe1\u53f7\uff0c\u5728\u7edf\u4e00\u67b6\u6784\u4e2d\u8054\u5408\u5efa\u6a21\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u548c\u8fd0\u52a8\u4f30\u8ba1\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u591a\u666e\u52d2\u9891\u79fb\u548c\u56de\u6ce2\u5f3a\u5ea6\u7684\u81ea\u76d1\u7763\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRadarMP\u5728\u5404\u79cd\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u8fd0\u52a8\u611f\u77e5\uff0c\u4f18\u4e8e\u57fa\u4e8e\u96f7\u8fbe\u7684\u89e3\u8026\u8fd0\u52a8\u611f\u77e5\u6d41\u7a0b\u3002", "conclusion": "RadarMP\u589e\u5f3a\u4e86\u5168\u573a\u666f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u8fd0\u52a8\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13160", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13160", "abs": "https://arxiv.org/abs/2511.13160", "authors": ["TC Singh", "Sougata Mukherjea"], "title": "InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions", "comment": null, "summary": "Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque \"black boxes\". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a \"what-if\" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.12131", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12131", "abs": "https://arxiv.org/abs/2511.12131", "authors": ["Quanxing Xu", "Ling Zhou", "Feifei Zhang", "Jinyu Tian", "Rubing Huang"], "title": "OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description", "comment": "Accepted by AAAI 2026", "summary": "Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.", "AI": {"tldr": "OAD-Promoter\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u8f7b\u8bed\u8a00\u504f\u89c1\u548c\u63d0\u5347\u9886\u57df\u8fc1\u79fb\u9c81\u68d2\u6027\u6765\u589e\u5f3a\u57fa\u4e8eLLM\u7684\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aOEG\u6a21\u5757\u751f\u6210\u5168\u5c40\u63cf\u8ff0\u548c\u5bf9\u8c61\u96c6\u4e2d\u6837\u672c\uff0cMKA\u6a21\u5757\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\u5904\u7406OOD\u6837\u672c\uff0cOAD Prompt\u6574\u5408\u524d\u4e24\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u4f18\u5316LLM\u63a8\u7406\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a(1)\u7531\u4e8e\u504f\u89c1\u5229\u7528\u5bfc\u81f4\u9884\u6d4b\u4e0d\u53ef\u9760\uff0c(2)\u5c3d\u7ba1\u5177\u6709\u5f3a\u5927\u7684\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002", "method": "\u63d0\u51faOAD-Promoter\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aOEG\u6a21\u5757\u751f\u6210\u5168\u5c40\u63cf\u8ff0\u548c\u5bf9\u8c61\u96c6\u4e2d\u6837\u672c\u4ee5\u589e\u5f3a\u89c6\u89c9\u4fe1\u606f\u8f93\u5165\u5e76\u51cf\u8f7b\u504f\u89c1\uff1bMKA\u6a21\u5757\u4ece\u5b58\u50a8\u7684\u793a\u4f8b\u4e2d\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6\u4ee5\u652f\u6301\u672a\u89c1\u9886\u57df\u7684\u95ee\u7b54\uff1bOAD Prompt\u6574\u5408\u524d\u4e24\u4e2a\u6a21\u5757\u7684\u8f93\u51fa\u4f18\u5316LLM\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOAD-Promoter\u5728\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8eLLM\u7684\u89c6\u89c9\u95ee\u7b54\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "OAD-Promoter\u901a\u8fc7\u51cf\u8f7b\u8bed\u8a00\u504f\u89c1\u548c\u63d0\u5347\u9886\u57df\u8fc1\u79fb\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.11714", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11714", "abs": "https://arxiv.org/abs/2511.11714", "authors": ["Daniel M. Jimenez-Gutierrez", "Enrique Zuazua", "Joaquin Del Rio", "Oleksii Sliusarenko", "Xabi Uribe-Etxebarria"], "title": "Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data", "comment": null, "summary": "Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.\n  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4f7f\u7528Sherpa.ai\u8054\u90a6\u5b66\u4e60\u5e73\u53f0\uff0c\u8ba9\u591a\u5bb6\u533b\u9662\u5728\u4e0d\u5171\u4eab\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u534f\u4f5c\u8bad\u7ec3\u80ba\u708eX\u5149\u5206\u7c7b\u5668\u3002\u5b9e\u9a8c\u663e\u793a\u8054\u90a6\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u51c6\u786e\u7387\u8fbe\u52300.900\uff0cROC-AUC\u8fbe\u52300.966\uff0c\u76f8\u6bd4\u5355\u533b\u9662\u6a21\u578b\u5206\u522b\u63d0\u5347\u4e8647.5%\u548c50.0%\u3002", "motivation": "\u65e9\u671f\u51c6\u786e\u7684\u80ba\u708e\u68c0\u6d4b\u5bf9\u4e34\u5e8a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5168\u7403\u5206\u5e03\u5f0f\u6570\u636e\u3001\u533b\u9662\u95f4\u5dee\u5f02\u548c\u9690\u79c1\u6cd5\u89c4\u4f7f\u5f97\u6570\u636e\u96c6\u4e2d\u5316\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u627e\u5230\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Sherpa.ai\u8054\u90a6\u5b66\u4e60\u5e73\u53f0\uff0c\u6a21\u62df\u533b\u9662\u95f4\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u534f\u4f5c\u8bad\u7ec3\uff0c\u4fdd\u6301\u6570\u636e\u672c\u5730\u5316\uff0c\u4e0d\u4f20\u8f93\u4efb\u4f55\u60a3\u8005X\u5149\u56fe\u50cf\u3002", "result": "\u8054\u90a6\u5b66\u4e60\u5728\u591a\u533b\u9662\u534f\u4f5c\u4e0b\u5b9e\u73b0\u4e860.900\u7684\u51c6\u786e\u7387\u548c0.966\u7684ROC-AUC\uff0c\u76f8\u6bd4\u5355\u533b\u9662\u6a21\u578b\uff080.610\u51c6\u786e\u7387\uff1b0.644 ROC-AUC\uff09\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u63d0\u4f9b\u9ad8\u6027\u80fd\u3001\u53ef\u6cdb\u5316\u3001\u5b89\u5168\u4e14\u79c1\u5bc6\u7684\u80ba\u708e\u68c0\u6d4b\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7f55\u89c1\u75be\u75c5\u548c\u4f4e\u6570\u636e\u9886\u57df\uff0c\u5b9e\u73b0\u65e0\u9700\u6570\u636e\u79fb\u52a8\u7684\u591a\u673a\u6784\u5b89\u5168\u534f\u4f5c\u3002"}}
{"id": "2511.11717", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.11717", "abs": "https://arxiv.org/abs/2511.11717", "authors": ["Xiang Xiang Wang", "Sean Cottrell", "Guo-Wei Wei"], "title": "Multiscale Grassmann Manifolds for Single-Cell Data Analysis", "comment": null, "summary": "Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGrassmann\u6d41\u5f62\u7684\u591a\u5c3a\u5ea6\u6846\u67b6\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u4e0e\u5b50\u7a7a\u95f4\u51e0\u4f55\u7ed3\u5408\u7528\u4e8e\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5d4c\u5165\u6574\u5408\u4e0d\u540c\u51e0\u4f55\u89c6\u89d2\u7279\u5f81\uff0c\u5728\u5355\u7ec6\u80deRNA-seq\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5c06\u7ec6\u80de\u8868\u793a\u4e3a\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u5411\u91cf\uff0c\u9650\u5236\u4e86\u6355\u6349\u5185\u5728\u76f8\u5173\u6027\u548c\u591a\u5c3a\u5ea6\u51e0\u4f55\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u9700\u8981\u65b0\u7684\u51e0\u4f55\u6846\u67b6\u6765\u66f4\u597d\u5730\u8868\u5f81\u7ec6\u80de\u5f02\u8d28\u6027\u3002", "method": "\u57fa\u4e8eGrassmann\u6d41\u5f62\u7684\u591a\u5c3a\u5ea6\u6846\u67b6\uff0c\u751f\u6210\u591a\u5c3a\u5ea6\u8868\u793a\u4e0b\u7684\u5d4c\u5165\uff0c\u5c06\u4e0d\u540c\u51e0\u4f55\u89c6\u89d2\u7684\u7279\u5f81\u6574\u5408\u5230\u7edf\u4e00\u7684Grassmann\u6d41\u5f62\u4e2d\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5e42\u7684\u5c3a\u5ea6\u91c7\u6837\u51fd\u6570\u6765\u63a7\u5236\u5c3a\u5ea6\u9009\u62e9\u548c\u5e73\u8861\u5206\u8fa8\u7387\u95f4\u4fe1\u606f\u3002", "result": "\u57289\u4e2a\u57fa\u51c6\u5355\u7ec6\u80deRNA-seq\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u7559\u6709\u610f\u4e49\u7684\u7ec6\u80de\u7ed3\u6784\uff0c\u63d0\u4f9b\u7a33\u5b9a\u7684\u805a\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4e2d\u5c0f\u578b\u6570\u636e\u96c6\u3002", "conclusion": "Grassmann\u6d41\u5f62\u4e3a\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u8fde\u8d2f\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u57fa\u7840\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u7ec6\u80de\u5f02\u8d28\u6027\u7684\u51e0\u4f55\u7ed3\u6784\u7279\u5f81\u3002"}}
{"id": "2511.12142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12142", "abs": "https://arxiv.org/abs/2511.12142", "authors": ["Seokwon Song", "Minsu Park", "Gunhee Kim"], "title": "MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering", "comment": "Accepted for publication in the Association for the Advancement of Artificial Intelligence (AAAI), 2026", "summary": "Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS", "AI": {"tldr": "MAVIS\u662f\u9996\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u6765\u6e90\u5f52\u56e0\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u5305\u542b157K\u89c6\u89c9\u95ee\u7b54\u5b9e\u4f8b\uff0c\u6bcf\u4e2a\u7b54\u6848\u90fd\u6709\u4e8b\u5b9e\u7ea7\u5f15\u7528\u6307\u5411\u591a\u6a21\u6001\u6587\u6863\u3002\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001RAG\u80fd\u751f\u6210\u66f4\u4fe1\u606f\u4e30\u5bcc\u548c\u6d41\u7545\u7684\u7b54\u6848\uff0c\u4f46\u5728\u56fe\u50cf\u6587\u6863\u7684\u53ef\u9760\u6027\u65b9\u9762\u8f83\u5f31\uff0c\u5b58\u5728\u4fe1\u606f\u4e30\u5bcc\u6027\u4e0e\u53ef\u9760\u6027\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u7eaf\u6587\u672c\u573a\u666f\uff0c\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u5728\u6765\u6e90\u5f52\u56e0\u4e2d\u7684\u4f5c\u7528\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u7406\u89e3\u89c6\u89c9\u95ee\u9898\u610f\u56fe\u3001\u68c0\u7d22\u591a\u6a21\u6001\u8bc1\u636e\u5e76\u751f\u6210\u5e26\u5f15\u7528\u957f\u7b54\u6848\u7684\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1MAVIS\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b157K\u89c6\u89c9\u95ee\u7b54\u5b9e\u4f8b\uff0c\u6bcf\u4e2a\u7b54\u6848\u6807\u6ce8\u4e8b\u5b9e\u7ea7\u5f15\u7528\uff1b\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u4e30\u5bcc\u6027\u3001\u53ef\u9760\u6027\u548c\u6d41\u7545\u6027\u7684\u7ec6\u7c92\u5ea6\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff1b\u6bd4\u8f83\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u4e0b\u7684\u591a\u6a21\u6001RAG\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u591a\u6a21\u6001RAG\u6bd4\u5355\u6a21\u6001RAG\u751f\u6210\u66f4\u4fe1\u606f\u4e30\u5bcc\u548c\u6d41\u7545\u7684\u7b54\u6848\uff1b\u4f46\u5728\u56fe\u50cf\u6587\u6863\u7684\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u8f83\u5f31\uff0c\u8fd9\u4e00\u5dee\u8ddd\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u88ab\u653e\u5927\uff1b\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u5728\u4fe1\u606f\u4e30\u5bcc\u6027\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "conclusion": "\u7f13\u89e3\u56fe\u50cf\u6587\u6863\u89e3\u91ca\u4e2d\u7684\u4e0a\u4e0b\u6587\u504f\u89c1\u662f\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\uff1bMAVIS\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u6765\u6e90\u5f52\u56e0\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\u3002"}}
{"id": "2511.11722", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.11722", "abs": "https://arxiv.org/abs/2511.11722", "authors": ["Soumyendu Sarkar", "Antonio Guillen-Perez", "Zachariah J Carmichael", "Avisek Naug", "Refik Mert Cam", "Vineet Gundecha", "Ashwin Ramesh Babu", "Sahand Ghorbanpour", "Ricardo Luna Gutierrez"], "title": "Fast 3D Surrogate Modeling for Data Center Thermal Management", "comment": "Submitted to AAAI 2026 Conference", "summary": "Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\\%) and reduced carbon footprint.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u66ff\u4ee3\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u6570\u636e\u4e2d\u5fc33D\u6e29\u5ea6\u573a\u7684\u5b9e\u65f6\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edfCFD\u65b9\u6cd5\u5b9e\u73b0\u4e8620,000\u500d\u52a0\u901f\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u51b7\u5374\u63a7\u5236\u548c\u8d1f\u8f7d\u91cd\u5206\u914d\uff0c\u4ece\u800c\u8282\u77017%\u7684\u80fd\u6e90\u6d88\u8017\u3002", "motivation": "\u51cf\u5c11\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u548c\u78b3\u6392\u653e\u5bf9\u53ef\u6301\u7eed\u53d1\u5c55\u548c\u8fd0\u8425\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u70edCFD\u6c42\u89e3\u5668\u867d\u7136\u51c6\u786e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e13\u5bb6\u6784\u5efa\u7f51\u683c\u548c\u8fb9\u754c\u6761\u4ef6\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u66ff\u4ee3\u5efa\u6a21\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u6570\u636e\u4e2d\u5fc3\u76843D\u4f53\u7d20\u5316\u8868\u793a\u4e0a\u64cd\u4f5c\uff0c\u7ed3\u5408\u670d\u52a1\u5668\u8d1f\u8f7d\u3001\u98ce\u6247\u901f\u5ea6\u548cHVAC\u6e29\u5ea6\u8bbe\u5b9a\u70b9\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u67b6\u6784\uff0c\u5305\u62ec3D CNN U-Net\u53d8\u4f53\u30013D\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u548c3D\u89c6\u89c9\u53d8\u6362\u5668\u3002", "result": "\u66ff\u4ee3\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u6570\u636e\u4e2d\u5fc3\u914d\u7f6e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe20,000\u500d\u7684\u52a0\u901f\uff08\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u6570\u767e\u6beb\u79d2\uff09\u3002\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u4f30\u8ba1\u70ed\u70b9\u548c\u6e29\u5ea6\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b9e\u65f6\u51b7\u5374\u63a7\u5236\u548c\u8d1f\u8f7d\u91cd\u5206\u914d\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u80fd\u6e90\u8282\u7701\uff087%\uff09\u548c\u78b3\u8db3\u8ff9\u51cf\u5c11\u3002"}}
{"id": "2511.13226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13226", "abs": "https://arxiv.org/abs/2511.13226", "authors": ["Michele Persiani", "Thomas Hellstrom"], "title": "Informative Communication of Robot Plans", "comment": "Conference: PAAMS 2022, 20th International Conference on Practical Applications of Agents and Multi-Agent Systems", "summary": "When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u673a\u5668\u4eba\u8ba1\u5212\u53e3\u5934\u8868\u8fbe\u7b56\u7565\uff0c\u901a\u8fc7\u8003\u8651\u7528\u6237\u7684\u4e8c\u9636\u5fc3\u667a\u7406\u8bba\u6765\u4f18\u5316\u6c9f\u901a\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u4eba\u8ba1\u5212\u53e3\u5934\u8868\u8fbe\u7b56\u7565\uff08\u5982\u6309\u8ba1\u5212\u987a\u5e8f\u9012\u589e\u6216\u9012\u51cf\uff09\u6ca1\u6709\u5145\u5206\u8003\u8651\u7528\u6237\u5df2\u6709\u7684\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6c9f\u901a\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u4f20\u8fbe\u4fe1\u606f\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u673a\u5668\u4eba\u8ba1\u5212\u53e3\u5934\u8868\u8fbe\u7b56\u7565\uff0c\u901a\u8fc7\u8861\u91cf\u53e3\u5934\u8868\u8fbe\u76f8\u5bf9\u4e8e\u7528\u6237\u4e8c\u9636\u5fc3\u667a\u7406\u8bba\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u9009\u62e9\u6700\u6709\u6548\u7684\u6c9f\u901a\u65b9\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u80fd\u8ba9\u7528\u6237\u66f4\u5feb\u7406\u89e3\u673a\u5668\u4eba\u7684\u76ee\u6807\uff0c\u4f18\u4e8e\u9012\u589e\u6216\u9012\u51cf\u8ba1\u5212\u987a\u5e8f\u7b49\u4f20\u7edf\u7b56\u7565\u3002", "conclusion": "\u57fa\u4e8e\u4fe1\u606f\u589e\u76ca\u7684\u53e3\u5934\u8868\u8fbe\u7b56\u7565\u80fd\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u8ba1\u5212\u6c9f\u901a\u7684\u6548\u679c\uff0c\u5e76\u4e3a\u7406\u89e3\u4ec0\u4e48\u662f\u6709\u4fe1\u606f\u91cf\u7684\u6c9f\u901a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.11727", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11727", "abs": "https://arxiv.org/abs/2511.11727", "authors": ["Tongda Xu"], "title": "Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm", "comment": "NIPS 25 Workshop: Frontiers in Probabilistic Inference: Sampling Meets Learning", "summary": "Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5728\u6269\u6563\u6a21\u578b\u4e2d\u901a\u8fc7\u53bb\u566a\u5206\u6570\u5339\u914d\u4f18\u5316\u6761\u4ef6\u8f93\u5165\u4f1a\u7834\u574f\u4e0e\u7cbe\u786e\u5206\u6570\u5339\u914d\u7684\u7b49\u4ef7\u6027\uff0c\u5bfc\u81f4\u504f\u5dee\u548c\u66f4\u9ad8\u7684\u5206\u6570\u8303\u6570\uff0c\u8fd9\u79cd\u73b0\u8c61\u5f71\u54cd\u591a\u4e2a\u9886\u57df\u7684\u7814\u7a76\u5de5\u4f5c\u3002", "motivation": "\u8bb8\u591a\u8fd1\u671f\u7814\u7a76\u5229\u7528\u53bb\u566a\u5206\u6570\u5339\u914d\u6765\u4f18\u5316\u6269\u6563\u6a21\u578b\u7684\u6761\u4ef6\u8f93\u5165\uff0c\u4f46\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u7406\u8bba\u504f\u5dee\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u53bb\u566a\u5206\u6570\u5339\u914d\u4f18\u5316\u6761\u4ef6\u8f93\u5165\u4f1a\u7834\u574f\u4e0e\u7cbe\u786e\u5206\u6570\u5339\u914d\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u5c55\u793a\u8fd9\u79cd\u504f\u5dee\u5bfc\u81f4\u66f4\u9ad8\u7684\u5206\u6570\u8303\u6570\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u504f\u5dee\u4e0d\u4ec5\u5b58\u5728\u4e8e\u6761\u4ef6\u8f93\u5165\u4f18\u5316\u4e2d\uff0c\u5728\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4f18\u5316\u6570\u636e\u5206\u5e03\u65f6\u4e5f\u4f1a\u51fa\u73b0\u7c7b\u4f3c\u504f\u5dee\uff0c\u5f71\u54cd\u591a\u4e2a\u9886\u57df\u7684\u7814\u7a76\u5de5\u4f5c\u3002", "conclusion": "\u53bb\u566a\u5206\u6570\u5339\u914d\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u7406\u8bba\u504f\u5dee\uff0c\u8fd9\u79cd\u504f\u5dee\u4f1a\u5f71\u54cd\u6269\u6563\u6a21\u578b\u5728\u81ea\u56de\u5f52\u751f\u6210\u3001\u56fe\u50cf\u538b\u7f29\u548c\u6587\u672c\u52303D\u751f\u6210\u7b49\u591a\u4e2a\u5e94\u7528\u9886\u57df\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.13288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13288", "abs": "https://arxiv.org/abs/2511.13288", "authors": ["Haoyang Hong", "Jiajun Yin", "Yuan Wang", "Jingnan Liu", "Zhe Chen", "Ailing Yu", "Ji Li", "Zhiling Ye", "Hansong Xiao", "Yefei Chen", "Hualei Zhou", "Yun Yue", "Minghui Yang", "Chunxiao Guo", "Junwei Liu", "Peng Wei", "Jinjie Gu"], "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO", "comment": null, "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86M-GRPO\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u4e0d\u540c\u667a\u80fd\u4f53\u4f7f\u7528\u4e0d\u540cLLM\u65f6\u7684\u4f18\u5316\u6311\u6218\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u4fe1\u7528\u5206\u914d\u548c\u8f68\u8ff9\u5bf9\u9f50\u65b9\u6848\uff0c\u5728\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f7f\u7528\u7edf\u4e00\u7684LLM\u8bad\u7ec3\u9650\u5236\u4e86\u6027\u80fd\uff0c\u56e0\u4e3a\u4e0d\u540c\u667a\u80fd\u4f53\u5177\u6709\u4e0d\u540c\u7684\u6570\u636e\u5206\u5e03\u3002\u4f7f\u7528\u4e0d\u540cLLM\u8bad\u7ec3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u4f18\u5316\u6311\u6218\uff0c\u5982\u4e0d\u540c\u9891\u7387\u64cd\u4f5c\u3001\u53ef\u53d8\u5b50\u667a\u80fd\u4f53\u8c03\u7528\u548c\u8de8\u670d\u52a1\u5668\u90e8\u7f72\u5bfc\u81f4\u7684\u68af\u5ea6\u6d41\u4e2d\u65ad\u3002", "method": "\u63d0\u51faM-GRPO\u65b9\u6cd5\uff1a1\uff09\u4e3a\u4e3b\u667a\u80fd\u4f53\u548c\u5b50\u667a\u80fd\u4f53\u8ba1\u7b97\u7ec4\u76f8\u5bf9\u4f18\u52bf\uff0c\u4fdd\u6301\u5c42\u6b21\u5316\u4fe1\u7528\u5206\u914d\uff1b2\uff09\u5f15\u5165\u8f68\u8ff9\u5bf9\u9f50\u65b9\u6848\uff0c\u751f\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u6279\u6b21\uff1b3\uff09\u90e8\u7f72\u89e3\u8026\u8bad\u7ec3\u7ba1\u9053\uff0c\u667a\u80fd\u4f53\u5728\u72ec\u7acb\u670d\u52a1\u5668\u4e0a\u8fd0\u884c\uff0c\u901a\u8fc7\u5171\u4eab\u5b58\u50a8\u4ea4\u6362\u6700\u5c0f\u7edf\u8ba1\u4fe1\u606f\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\uff08GAIA\u3001XBench-DeepSearch\u3001WebWalkerQA\uff09\u4e2d\uff0cM-GRPO\u6301\u7eed\u4f18\u4e8e\u5355\u667a\u80fd\u4f53GRPO\u548c\u51bb\u7ed3\u5b50\u667a\u80fd\u4f53\u7684\u591a\u667a\u80fd\u4f53GRPO\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u7a33\u5b9a\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u5bf9\u9f50\u5f02\u6784\u8f68\u8ff9\u5e76\u5728\u4e13\u95e8\u667a\u80fd\u4f53\u4e4b\u95f4\u89e3\u8026\u4f18\u5316\u80fd\u591f\u589e\u5f3a\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12151", "abs": "https://arxiv.org/abs/2511.12151", "authors": ["Kaixiang Yang", "Boyang Shen", "Xin Li", "Yuchen Dai", "Yuxuan Luo", "Yueran Ma", "Wei Fang", "Qiang Li", "Zhiwei Wang"], "title": "FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing", "comment": "AAAI 2026", "summary": "Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.", "AI": {"tldr": "FIA-Edit\u662f\u4e00\u4e2a\u57fa\u4e8e\u9891\u7387\u4ea4\u4e92\u6ce8\u610f\u529b\u7684\u514d\u53cd\u6f14\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u7387\u8868\u793a\u4ea4\u4e92\u548c\u7279\u5f81\u6ce8\u5165\u6a21\u5757\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7f16\u8f91\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u7f16\u8f91\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u514d\u53cd\u6f14\u65b9\u6cd5\u867d\u7136\u6548\u7387\u9ad8\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u6e90\u4fe1\u606f\u6574\u5408\uff0c\u5bfc\u81f4\u80cc\u666f\u4fdd\u7559\u5dee\u3001\u7a7a\u95f4\u4e0d\u4e00\u81f4\u548c\u8fc7\u5ea6\u7f16\u8f91\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9891\u7387\u4ea4\u4e92\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u5305\u542b\u9891\u7387\u8868\u793a\u4ea4\u4e92\u6a21\u5757\uff08\u5728\u81ea\u6ce8\u610f\u529b\u4e2d\u4ea4\u6362\u6e90\u548c\u76ee\u6807\u7279\u5f81\u7684\u9891\u7387\u5206\u91cf\uff09\u548c\u7279\u5f81\u6ce8\u5165\u6a21\u5757\uff08\u5728\u4ea4\u53c9\u6ce8\u610f\u529b\u4e2d\u663e\u5f0f\u5f15\u5165\u6e90\u4fa7\u67e5\u8be2\u3001\u952e\u3001\u503c\u548c\u6587\u672c\u5d4c\u5165\uff09\u3002", "result": "\u5728RTX 4090\u4e0a\u6bcf\u5f20512*512\u56fe\u50cf\u4ec5\u9700\u7ea66\u79d2\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u80cc\u666f\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u533b\u5b66\u51fa\u8840\u5206\u7c7b\u7684\u6570\u636e\u589e\u5f3a\u3002", "conclusion": "FIA-Edit\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u4fdd\u771f\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11734", "abs": "https://arxiv.org/abs/2511.11734", "authors": ["Kamalpreet Singh Kainth", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedat Panat"], "title": "Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics", "comment": null, "summary": "Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.", "AI": {"tldr": "PI-NODE-SR\u7ed3\u5408\u4f4e\u9636\u663e\u5f0f\u6c42\u89e3\u5668\u548c\u6b8b\u5dee\u5f52\u4e00\u5316\uff0c\u7a33\u5b9a\u8bad\u7ec3\u5e76\u51c6\u786e\u5b66\u4e60\u521a\u6027\u751f\u7269\u7269\u7406\u7cfb\u7edf\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\uff0c\u5728Hodgkin-Huxley\u65b9\u7a0b\u4e0a\u6210\u529f\u9884\u6d4b\u8d85\u8fc7100ms\u7684\u632f\u8361\u3002", "motivation": "\u6807\u51c6\u795e\u7ecf\u5fae\u5206\u65b9\u7a0b\u548c\u7269\u7406\u4fe1\u606f\u53d8\u4f53\u5728\u5efa\u6a21\u521a\u6027\u751f\u7269\u7269\u7406\u7cfb\u7edf\u65f6\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u5927\u91cf\u8fed\u4ee3\u4e14\u53ef\u80fd\u6536\u655b\u5230\u6b21\u4f18\u89e3\uff0c\u65e0\u6cd5\u4fdd\u6301\u632f\u8361\u9891\u7387\u6216\u632f\u5e45\u3002", "method": "\u5f15\u5165\u5177\u6709\u5c3a\u5ea6\u611f\u77e5\u6b8b\u5dee\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecfODE\uff08PI-NODE-SR\uff09\uff0c\u7ed3\u5408\u4f4e\u9636\u663e\u5f0f\u6c42\u89e3\u5668\uff08Heun\u65b9\u6cd5\uff09\u548c\u6b8b\u5dee\u5f52\u4e00\u5316\uff0c\u5e73\u8861\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u72b6\u6001\u53d8\u91cf\u7684\u8d21\u732e\u3002", "result": "\u5728Hodgkin-Huxley\u65b9\u7a0b\u4e0a\uff0cPI-NODE-SR\u4ece\u5355\u4e2a\u632f\u8361\u5b66\u4e60\u5e76\u5916\u63a8\u8d85\u8fc7100ms\uff0c\u51c6\u786e\u6355\u6349\u632f\u8361\u9891\u7387\u548c\u63a5\u8fd1\u6b63\u786e\u7684\u632f\u5e45\uff0c\u6062\u590d\u95e8\u63a7\u53d8\u91cf\u4e2d\u7684\u5c16\u9510\u4e9a\u9608\u503c\u66f2\u7387\u7b49\u5f62\u6001\u7279\u5f81\u3002", "conclusion": "PI-NODE-SR\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u795e\u7ecfODE\u548cPINNs\u6301\u7eed\u51cf\u5c11\u957f\u671f\u8bef\u5dee\uff0c\u4e3a\u7a33\u5b9a\u9ad8\u6548\u5b66\u4e60\u521a\u6027\u751f\u7269\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9014\u5f84\uff0c\u5c3d\u7ba1\u6027\u80fd\u5bf9\u521d\u59cb\u5316\u4ecd\u654f\u611f\u3002"}}
{"id": "2511.12162", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12162", "abs": "https://arxiv.org/abs/2511.12162", "authors": ["Shuo Yin", "Zhiyuan Yin", "Yuqing Hou", "Rui Liu", "Yong Chen", "Dell Zhang"], "title": "Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function", "comment": "14 pages", "summary": "Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Center-Reassigned Hashing (CRH)\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u5206\u914d\u9884\u8bbe\u7801\u672c\u4e2d\u7684\u54c8\u5e0c\u4e2d\u5fc3\u5e76\u8054\u5408\u4f18\u5316\u54c8\u5e0c\u51fd\u6570\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u7684\u590d\u6742\u6027\u548c\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u54c8\u5e0c\u4e2d\u5fc3\u7684\u65b9\u6cd5\u5b58\u5728\u968f\u673a\u521d\u59cb\u5316\u5ffd\u7565\u7c7b\u95f4\u8bed\u4e49\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u800c\u4e24\u9636\u6bb5\u65b9\u6cd5\u867d\u7136\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5f15\u5165\u4e86\u989d\u5916\u590d\u6742\u6027\u548c\u9636\u6bb5\u95f4\u5dee\u5f02\u5bfc\u81f4\u7684\u6b21\u4f18\u6027\u80fd\u3002", "method": "CRH\u91c7\u7528\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u52a8\u6001\u91cd\u65b0\u5206\u914d\u9884\u8bbe\u7801\u672c\u4e2d\u7684\u54c8\u5e0c\u4e2d\u5fc3\uff0c\u65e0\u9700\u663e\u5f0f\u7684\u4e2d\u5fc3\u4f18\u5316\u9636\u6bb5\uff0c\u540c\u65f6\u4f7f\u7528\u591a\u5934\u673a\u5236\u589e\u5f3a\u54c8\u5e0c\u4e2d\u5fc3\u7684\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRH\u80fd\u591f\u5b66\u4e60\u5230\u5177\u6709\u8bed\u4e49\u610f\u4e49\u7684\u54c8\u5e0c\u4e2d\u5fc3\uff0c\u5e76\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u54c8\u5e0c\u65b9\u6cd5\u3002", "conclusion": "CRH\u901a\u8fc7\u52a8\u6001\u54c8\u5e0c\u4e2d\u5fc3\u91cd\u65b0\u5206\u914d\u548c\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u6709\u6548\u6574\u5408\u4e86\u8bed\u4e49\u5173\u7cfb\uff0c\u63d0\u5347\u4e86\u54c8\u5e0c\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2511.12170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12170", "abs": "https://arxiv.org/abs/2511.12170", "authors": ["Wang Luo", "Di Wu", "Hengyuan Na", "Yinlin Zhu", "Miao Hu", "Guocong Quan"], "title": "Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective", "comment": "Accepted by AAAI 2026", "summary": "Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u70b9\u4e91\u8865\u5168\u8303\u5f0fCompletion-by-Correction\uff0c\u901a\u8fc7\u4ece\u9884\u8bad\u7ec3\u56fe\u50cf\u52303D\u6a21\u578b\u751f\u6210\u62d3\u6251\u5b8c\u6574\u7684\u5f62\u72b6\u5148\u9a8c\uff0c\u5e76\u5728\u7279\u5f81\u7a7a\u95f4\u8fdb\u884c\u6821\u6b63\u6765\u5bf9\u9f50\u90e8\u5206\u89c2\u6d4b\uff0c\u4ece\u800c\u907f\u514d\u4f20\u7edf\u8865\u5168\u65b9\u6cd5\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4fee\u590d\u7684\u70b9\u4e91\u8865\u5168\u65b9\u6cd5\u7531\u4e8e\u51e0\u4f55\u548c\u8bed\u4e49\u7ea6\u675f\u6709\u9650\uff0c\u7ecf\u5e38\u5bfc\u81f4\u7ed3\u6784\u4e0d\u4e00\u81f4\u548c\u62d3\u6251\u4f2a\u5f71\u3002\u4f5c\u8005\u91cd\u65b0\u601d\u8003\u4efb\u52a1\u672c\u8d28\uff0c\u63d0\u51fa\u4ece\u65e0\u7ea6\u675f\u5408\u6210\u8f6c\u5411\u5f15\u5bfc\u7cbe\u70bc\u7684\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86PGNet\u591a\u9636\u6bb5\u6846\u67b6\uff1a\u8fdb\u884c\u53cc\u7279\u5f81\u7f16\u7801\u4ee5\u951a\u5b9a\u751f\u6210\u5148\u9a8c\uff0c\u5408\u6210\u7ed3\u6784\u5bf9\u9f50\u7684\u7c97\u7c92\u5ea6\u652f\u67b6\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u6821\u6b63\u9010\u6b65\u7ec6\u5316\u51e0\u4f55\u7ec6\u8282\u3002", "result": "\u5728ShapeNetViPC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPGNet\u5728\u5e73\u5747Chamfer\u8ddd\u79bb\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd523.5%\uff0cF-score\u63d0\u53477.1%\u3002", "conclusion": "Completion-by-Correction\u8303\u5f0f\u901a\u8fc7\u5c06\u8865\u5168\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ece\u62d3\u6251\u5b8c\u6574\u5148\u9a8c\u7684\u5f15\u5bfc\u7cbe\u70bc\uff0c\u5b9e\u73b0\u4e86\u7ed3\u6784\u4e00\u81f4\u4e14\u89c2\u6d4b\u5bf9\u9f50\u7684\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u8865\u5168\u6027\u80fd\u3002"}}
{"id": "2511.13306", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13306", "abs": "https://arxiv.org/abs/2511.13306", "authors": ["Bowen Ye", "Bin Zhang", "Hang Zhao"], "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving", "comment": null, "summary": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.", "AI": {"tldr": "DAP\u662f\u4e00\u4e2a\u79bb\u6563\u4ee4\u724c\u81ea\u56de\u5f52\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u8054\u5408\u9884\u6d4bBEV\u8bed\u4e49\u548c\u81ea\u8f66\u8f68\u8ff9\u6765\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u5728160M\u53c2\u6570\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6570\u636e\u6269\u5c55\u548c\u6a21\u578b\u9884\u7b97\u4e0b\u7684\u53ef\u6301\u7eed\u6027\u80fd\u63d0\u5347\u6311\u6218\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6570\u636e\u6269\u5c55\u6548\u7387\uff0c\u4f46\u4ec5\u9884\u6d4b\u81ea\u8f66\u8f68\u8ff9\u5b58\u5728\u76d1\u7763\u7a00\u758f\u548c\u573a\u666f\u6f14\u5316\u7ea6\u675f\u5f31\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u79bb\u6563\u4ee4\u724c\u81ea\u56de\u5f52\u89c4\u5212\u5668DAP\uff0c\u8054\u5408\u9884\u6d4bBEV\u8bed\u4e49\u548c\u81ea\u8f66\u8f68\u8ff9\uff0c\u5b9e\u73b0\u5168\u9762\u8868\u5f81\u5b66\u4e60\uff1b\u7ed3\u5408\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\uff0c\u4fdd\u7559\u76d1\u7763\u884c\u4e3a\u514b\u9686\u5148\u9a8c\u7684\u540c\u65f6\u6ce8\u5165\u5956\u52b1\u5f15\u5bfc\u7684\u6539\u8fdb\u3002", "result": "\u5728160M\u53c2\u6570\u9884\u7b97\u4e0b\uff0c\u5728\u5f00\u73af\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u7684\u95ed\u73af\u7ed3\u679c\u3002", "conclusion": "\u5b8c\u5168\u79bb\u6563\u4ee4\u724c\u81ea\u56de\u5f52\u516c\u5f0f\u5728\u6805\u683c\u5316BEV\u548c\u81ea\u8f66\u52a8\u4f5c\u4e0a\u7684\u64cd\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u7d27\u51d1\u4e14\u53ef\u6269\u5c55\u7684\u89c4\u5212\u8303\u5f0f\u3002"}}
{"id": "2511.12181", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12181", "abs": "https://arxiv.org/abs/2511.12181", "authors": ["Jinyuan Hu", "Jiayou Zhang", "Shaobo Cui", "Kun Zhang", "Guangyi Chen"], "title": "MixAR: Mixture Autoregressive Image Generation", "comment": null, "summary": "Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.", "AI": {"tldr": "MixAR\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563-\u8fde\u7eed\u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\uff0c\u5229\u7528\u79bb\u6563token\u4f5c\u4e3a\u5148\u9a8c\u6307\u5bfc\u6765\u63d0\u5347\u8fde\u7eed\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u65b9\u6cd5\u5c06\u56fe\u50cf\u8868\u793a\u4e3a\u6709\u9650\u7801\u672c\u4e2d\u7684\u79bb\u6563token\u5e8f\u5217\uff0c\u4f46\u91cf\u5316\u8fc7\u7a0b\u548c\u6709\u9650\u7801\u672c\u5927\u5c0f\u4f1a\u4e22\u5f03\u7ec6\u7c92\u5ea6\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u751f\u6210\u4fdd\u771f\u5ea6\u3002\u867d\u7136\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u7684\u81ea\u56de\u5f52\u5efa\u6a21\u80fd\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\uff0c\u4f46\u8fde\u7eed\u8868\u793a\u5b58\u5728\u4e8e\u5e7f\u9614\u65e0\u7ed3\u6784\u7a7a\u95f4\u4e2d\uff0c\u7ed9\u9ad8\u6548\u81ea\u56de\u5f52\u5efa\u6a21\u5e26\u6765\u6311\u6218\u3002", "method": "MixAR\u91c7\u7528\u56e0\u5b50\u5316\u516c\u5f0f\uff0c\u5229\u7528\u79bb\u6563token\u4f5c\u4e3a\u8fde\u7eed\u81ea\u56de\u5f52\u9884\u6d4b\u7684\u5148\u9a8c\u6307\u5bfc\u3002\u7814\u7a76\u4e86\u591a\u79cd\u79bb\u6563-\u8fde\u7eed\u6df7\u5408\u7b56\u7565\uff1a\u81ea\u6ce8\u610f\u529b(DC-SA)\u3001\u4ea4\u53c9\u6ce8\u610f\u529b(DC-CA)\u548c\u7b80\u5355\u65b9\u6cd5(DC-Mix)\u3002\u8fd8\u63d0\u51fa\u4e86\u8bad\u7ec3-\u63a8\u7406\u6df7\u5408(TI-Mix)\u6765\u5f25\u5408\u8bad\u7ec3\u548c\u751f\u6210\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDC-Mix\u7b56\u7565\u5728\u8ba1\u7b97\u6548\u7387\u548c\u751f\u6210\u4fdd\u771f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0cTI-Mix\u5e26\u6765\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "MixAR\u6846\u67b6\u901a\u8fc7\u79bb\u6563-\u8fde\u7eed\u6df7\u5408\u8bad\u7ec3\u6709\u6548\u63d0\u5347\u4e86\u8fde\u7eed\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2511.13359", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13359", "abs": "https://arxiv.org/abs/2511.13359", "authors": ["Yuhang Wang", "Yanxu Zhu", "Jitao Sang"], "title": "Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms", "comment": null, "summary": "The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6587\u5316\u89c4\u8303\u6587\u5316\u5bf9\u9f50\uff08CNCA\uff09\u6846\u67b6\uff0c\u5229\u7528\u5927\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5b9e\u73b0\u6587\u5316\u5bf9\u9f50\uff0c\u5305\u62ec\u4e09\u79cd\u4ece\u6709\u9650\u8c03\u67e5\u6570\u636e\u4e2d\u81ea\u52a8\u6316\u6398\u6587\u5316\u89c4\u8303\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e24\u79cd\u5bf9\u9f50\u8303\u5f0f\uff08\u4e0a\u4e0b\u6587\u5bf9\u9f50\u548c\u57fa\u4e8e\u5fae\u8c03\u7684\u65b9\u6cd5\uff09\u3002", "motivation": "\u5927\u6a21\u578b\u9700\u8981\u8d85\u8d8a\u5b89\u5168\u6027\uff0c\u53cd\u6620\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\u591a\u6837\u6027\uff0c\u5b9e\u73b0\u6587\u5316\u5bf9\u9f50\u3002", "method": "\u63d0\u51faCNCA\u6846\u67b6\uff0c\u5305\u542b\u4e09\u79cd\u81ea\u52a8\u6316\u6398\u6587\u5316\u89c4\u8303\u7684\u65b9\u6cd5\u548c\u4e24\u79cd\u5bf9\u9f50\u8303\u5f0f\uff1a\u4e0a\u4e0b\u6587\u5bf9\u9f50\uff08\u5c06\u6587\u5316\u89c4\u8303\u6574\u5408\u5230\u7528\u6237\u4e0a\u4e0b\u6587\u4e2d\uff09\u548c\u57fa\u4e8e\u5fae\u8c03\u7684\u65b9\u6cd5\uff08\u901a\u8fc7\u589e\u5f3a\u7684\u601d\u7ef4\u94fe\u8bad\u7ec3\u6570\u636e\u5185\u5316\u89c4\u8303\uff09\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u4ece\u6587\u5316\u89c4\u8303\u6316\u6398\u548c\u5229\u7528\u4e2d\u83b7\u76ca\u66f4\u591a\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u6587\u5316\u77e5\u60c5\u5bf9\u9f50\u7b56\u7565\u66f4\u597d\u5730\u53cd\u6620\u591a\u6837\u5316\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.11746", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11746", "abs": "https://arxiv.org/abs/2511.11746", "authors": ["Sepehr Maleki", "Negar Pourmoazemi"], "title": "Diffusion Models: A Mathematical Introduction", "comment": null, "summary": "We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u7684\u7b80\u6d01\u81ea\u5305\u542b\u63a8\u5bfc\uff0c\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u57fa\u672c\u6027\u8d28\u51fa\u53d1\uff0c\u6784\u5efa\u4e86\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u6db5\u76d6\u4e86\u524d\u5411\u52a0\u566a\u8fc7\u7a0b\u3001\u53cd\u5411\u540e\u9a8c\u5206\u5e03\u3001\u53d8\u5206\u4e0b\u754c\u7b49\u6838\u5fc3\u7406\u8bba\uff0c\u5e76\u8ba8\u8bba\u4e86\u4f3c\u7136\u4f30\u8ba1\u3001\u52a0\u901f\u91c7\u6837\u3001\u8fde\u7eed\u65f6\u95f4\u516c\u5f0f\u548c\u5f15\u5bfc\u6269\u6563\u7b49\u5173\u952e\u4e3b\u9898\u3002", "motivation": "\u4e3a\u6269\u6563\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e00\u4e2a\u900f\u660e\u3001\u81ea\u5305\u542b\u7684\u7406\u8bba\u63a8\u5bfc\uff0c\u4f7f\u8bfb\u8005\u80fd\u591f\u7406\u89e3\u7406\u8bba\u5e76\u5b9e\u73b0\u76f8\u5e94\u7b97\u6cd5\uff0c\u586b\u8865\u73b0\u6709\u6587\u732e\u4e2d\u7406\u8bba\u63a8\u5bfc\u4e0d\u591f\u7cfb\u7edf\u5316\u7684\u7a7a\u767d\u3002", "method": "\u4ece\u9ad8\u65af\u5206\u5e03\u7684\u57fa\u672c\u6027\u8d28\u51fa\u53d1\uff0c\u7cfb\u7edf\u5730\u63a8\u5bfc\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u5305\u62ec\u524d\u5411\u52a0\u566a\u8fc7\u7a0b\u3001\u95ed\u5f0f\u8fb9\u9645\u5206\u5e03\u3001\u7cbe\u786e\u79bb\u6563\u53cd\u5411\u540e\u9a8c\u5206\u5e03\u548c\u53d8\u5206\u4e0b\u754c\uff0c\u7136\u540e\u6269\u5c55\u5230\u8fde\u7eed\u65f6\u95f4\u516c\u5f0f\u3001\u52a0\u901f\u91c7\u6837\u65b9\u6cd5\u548c\u5f15\u5bfc\u6269\u6563\u6280\u672f\u3002", "result": "\u5efa\u7acb\u4e86\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u5b8c\u6574\u7406\u8bba\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u6807\u51c6\u566a\u58f0\u9884\u6d4b\u76ee\u6807\u7684\u7b80\u5316\u5f62\u5f0f\uff0c\u63d0\u51fa\u4e86\u6982\u7387\u6d41ODE\u548c\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u5e76\u89e3\u91ca\u4e86\u5206\u7c7b\u5668\u5f15\u5bfc\u548c\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u6570\u5b66\u539f\u7406\u3002", "conclusion": "\u901a\u8fc7\u900f\u660e\u7684\u4ee3\u6570\u63a8\u5bfc\u548c\u4e00\u81f4\u7684\u7b26\u53f7\u8868\u793a\uff0c\u4e3a\u6269\u6563\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4f7f\u8bfb\u8005\u80fd\u591f\u6df1\u5165\u7406\u89e3\u6a21\u578b\u539f\u7406\u5e76\u5b9e\u73b0\u76f8\u5173\u7b97\u6cd5\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2511.12196", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12196", "abs": "https://arxiv.org/abs/2511.12196", "authors": ["Aditi Bhalla", "Christian Hellert", "Enkelejda Kasneci"], "title": "Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System", "comment": null, "summary": "Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8de8\u89c6\u56fe\u3001\u8de8\u6a21\u6001\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u9a7e\u9a76\u5458\u6d3b\u52a8\u8bc6\u522b\u4e2d\u7684\u89c6\u89d2\u53d8\u5316\u548c\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u9a7e\u9a76\u5458\u5206\u5fc3\u662f\u4ea4\u901a\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u73b0\u6709\u65b9\u6cd5\u5355\u72ec\u5904\u7406\u8de8\u89c6\u56fe\u6cdb\u5316\u6216\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff0c\u65e0\u6cd5\u5e94\u5bf9\u771f\u5b9e\u90e8\u7f72\u4e2d\u540c\u65f6\u5b58\u5728\u7684\u89c6\u89d2\u53d8\u5316\u548c\u57df\u504f\u79fb\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u5728\u591a\u89c6\u56fe\u6570\u636e\u4e0a\u5b66\u4e60\u89c6\u56fe\u4e0d\u53d8\u548c\u52a8\u4f5c\u5224\u522b\u7279\u5f81\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u4fe1\u606f\u74f6\u9888\u635f\u5931\u8fdb\u884c\u8de8\u6a21\u6001\u57df\u81ea\u9002\u5e94\uff0c\u65e0\u9700\u65b0\u57df\u7684\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728Drive&Act\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0cRGB\u89c6\u9891\u6570\u636e\u7684top-1\u51c6\u786e\u7387\u63d0\u5347\u8fd150%\uff1b\u76f8\u6bd4\u4ec5\u4f7f\u7528\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u8fbe5%\u3002", "conclusion": "\u8be5\u8054\u5408\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u9a7e\u9a76\u5458\u76d1\u63a7\u4e2d\u7684\u8de8\u89c6\u56fe\u548c\u8de8\u6a21\u6001\u6311\u6218\uff0c\u4e3a\u6a21\u578b\u5728\u591a\u6837\u5316\u8f66\u8f86\u914d\u7f6e\u4e2d\u7684\u7a33\u5065\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.11750", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11750", "abs": "https://arxiv.org/abs/2511.11750", "authors": ["Hanting Yan", "Pan Mu", "Shiqi Zhang", "Yuchao Zhu", "Jinglin Zhang", "Cong Bai"], "title": "IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation", "comment": null, "summary": "Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8eab\u4efd\u5206\u5e03\u7684\u7269\u7406\u4e0d\u53d8\u5b66\u4e60\u6846\u67b6\uff08IDOL\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5148\u9a8c\u7269\u7406\u77e5\u8bc6\u7684\u8eab\u4efd\u5bfc\u5411\u7ea6\u675f\u6765\u5904\u7406\u70ed\u5e26\u6c14\u65cb\u4f30\u8ba1\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u70ed\u5e26\u6c14\u65cb\u4f30\u8ba1\u9762\u4e34\u590d\u6742\u52a8\u6001\u73af\u5883\u573a\u5bfc\u81f4\u7684\u5206\u5e03\u504f\u79fb\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u591a\u6a21\u6001\u878d\u5408\u4f46\u5ffd\u89c6\u4e86\u7279\u5f81\u8868\u793a\u7684\u5185\u5728\u5206\u5e03\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51faIDOL\u6846\u67b6\uff0c\u5229\u7528\u98ce\u573a\u6a21\u578b\u548c\u6697\u76f8\u5173\u77e5\u8bc6\u5efa\u6a21\u4efb\u52a1\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u8eab\u4efd\u4ee4\u724c\uff0c\u901a\u8fc7\u8eab\u4efd\u5bfc\u5411\u7ea6\u675f\u5728\u7269\u7406\u77e5\u8bc6\u6307\u5bfc\u4e0b\u89c4\u8303\u7279\u5f81\u7a7a\u95f4\uff0c\u5904\u7406\u5206\u5e03\u53d8\u5f02\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cIDOL\u5728\u70ed\u5e26\u6c14\u65cb\u98ce\u901f\u3001\u6c14\u538b\u3001\u5185\u6838\u548c\u5916\u6838\u5c3a\u5bf8\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5404\u79cd\u5206\u5e03\u504f\u79fb\u3002", "conclusion": "\u57fa\u4e8e\u5148\u9a8c\u7269\u7406\u77e5\u8bc6\u65bd\u52a0\u8eab\u4efd\u5bfc\u5411\u7ea6\u675f\u80fd\u591f\u6709\u6548\u7f13\u89e3\u70ed\u5e26\u6c14\u65cb\u4f30\u8ba1\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u7269\u7406\u4e0d\u53d8\u6027\u5728\u5904\u7406\u5206\u5e03\u53d8\u5f02\u6027\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.13371", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13371", "abs": "https://arxiv.org/abs/2511.13371", "authors": ["Caroline Baumgartner", "Eleanor Spens", "Neil Burgess", "Petru Manescu"], "title": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning", "comment": null, "summary": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86GPT-2\u6a21\u578b\u5728\u7a7a\u95f4\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u673a\u5236\uff0c\u53d1\u73b0\u4e0d\u540c\u7684\u8bad\u7ec3\u8303\u5f0f\u4f1a\u4ea7\u751f\u4e24\u79cd\u6839\u672c\u4e0d\u540c\u7684\u7b97\u6cd5\uff1a\u63a2\u7d22\u578b\u6a21\u578b\u53d1\u5c55\u51fa\u7c7b\u4f3c\u8ba4\u77e5\u5730\u56fe\u7684\u7a33\u5065\u7a7a\u95f4\u8868\u793a\uff0c\u800c\u76ee\u6807\u5bfc\u5411\u578b\u6a21\u578b\u5219\u5b66\u4e60\u8def\u5f84\u4f9d\u8d56\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u89e3\u51b3\u7a7a\u95f4\u5bfc\u822a\u4efb\u52a1\uff0c\u63a2\u7d22\u4e0d\u540c\u8bad\u7ec3\u8303\u5f0f\u5bf9\u6a21\u578b\u5b66\u4e60\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u7a7a\u95f4\u667a\u80fd\u5728transformer\u4e2d\u7684\u672c\u8d28\u3002", "method": "\u5728\u7f51\u683c\u73af\u5883\u4e2d\u8bad\u7ec3GPT-2\u6a21\u578b\uff0c\u91c7\u7528\u4e09\u79cd\u7a7a\u95f4\u5b66\u4e60\u8303\u5f0f\uff1a\u88ab\u52a8\u63a2\u7d22\uff08\u968f\u673a\u6e38\u8d70\u9884\u6d4b\uff09\u3001\u76ee\u6807\u5bfc\u5411\u89c4\u5212\uff08\u751f\u6210\u6700\u4f18\u6700\u77ed\u8def\u5f84\uff09\u548c\u6df7\u5408\u6a21\u578b\uff08\u5728\u7ed3\u6784\u5316\u8def\u5f84\u57fa\u7840\u4e0a\u7528\u63a2\u7d22\u6570\u636e\u5fae\u8c03\uff09\u3002\u4f7f\u7528\u884c\u4e3a\u3001\u8868\u5f81\u548c\u673a\u5236\u5206\u6790\u3002", "result": "\u63a2\u7d22\u578b\u6a21\u578b\u53d1\u5c55\u51fa\u7c7b\u4f3c\u8ba4\u77e5\u5730\u56fe\u7684\u7a33\u5065\u7a7a\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u53d1\u73b0\u5176\u5c06\u7a7a\u95f4\u4fe1\u606f\u6574\u5408\u5230\u81ea\u8db3\u5750\u6807\u7cfb\u4e2d\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u5206\u5c42\u63a8\u7406\u7cfb\u7edf\u3002\u76ee\u6807\u5bfc\u5411\u578b\u6a21\u578b\u5b66\u4e60\u8def\u5f84\u4f9d\u8d56\u7b97\u6cd5\uff0c\u59cb\u7ec8\u4f9d\u8d56\u663e\u5f0f\u65b9\u5411\u8f93\u5165\u3002\u6df7\u5408\u6a21\u578b\u867d\u7136\u6cdb\u5316\u80fd\u529b\u63d0\u5347\uff0c\u4f46\u4fdd\u7559\u8def\u5f84\u4f9d\u8d56\u7b56\u7565\u3002", "conclusion": "transformer\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u5b58\u5728\u4e8e\u4e00\u4e2a\u8c31\u7cfb\u4e2d\uff0c\u4ece\u7531\u63a2\u7d22\u6570\u636e\u5851\u9020\u7684\u53ef\u6cdb\u5316\u4e16\u754c\u6a21\u578b\u5230\u4e3a\u76ee\u6807\u5bfc\u5411\u4efb\u52a1\u4f18\u5316\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u8bad\u7ec3\u8303\u5f0f\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6d8c\u73b0\u7684\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u6cdb\u5316\u4e0e\u4f18\u5316\u4e4b\u95f4\u7684\u6743\u8861\u673a\u5236\u3002"}}
{"id": "2511.13411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13411", "abs": "https://arxiv.org/abs/2511.13411", "authors": ["Przemyslaw Chojecki"], "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence", "comment": null, "summary": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $\u03ba$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $\u03ba$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.12201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12201", "abs": "https://arxiv.org/abs/2511.12201", "authors": ["Feng Chen", "Yefei He", "Shaoxuan He", "Yuanyu He", "Jing Liu", "Lequan Lin", "Akide Liu", "Zhaoyang Li", "Jiyuan Zhang", "Zhenbang Sun", "Bohan Zhuang", "Qi Wu"], "title": "OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs", "comment": "Accepted by AAAI2026", "summary": "Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.", "AI": {"tldr": "OmniSparse\u662f\u4e00\u4e2a\u8bad\u7ec3\u611f\u77e5\u7684\u7ec6\u7c92\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u4ee4\u724c\u9884\u7b97\u5206\u914d\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u5b9e\u73b0\u52a0\u901f\u548c\u5185\u5b58\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u63a8\u7406\u65f6\u52a0\u901f\uff0c\u91c7\u7528\u9884\u5b9a\u4e49\u7a00\u758f\u6a21\u5f0f\u9009\u62e9\u5173\u952e\u4ee4\u724c\uff0c\u4f46\u65e0\u6cd5\u5f25\u5408\u8bad\u7ec3-\u63a8\u7406\u5dee\u8ddd\uff0c\u7f3a\u4e4f\u8de8\u67e5\u8be2\u3001\u952e\u503c\u5bf9\u548c\u6ce8\u610f\u529b\u5934\u7684\u7ec6\u7c92\u5ea6\u4ee4\u724c\u9009\u62e9\u80fd\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u548c\u52a0\u901f\u6548\u679c\u6709\u9650\u3002", "method": "OmniSparse\u5305\u542b\u4e09\u79cd\u81ea\u9002\u5e94\u4e92\u8865\u673a\u5236\uff1a1\uff09\u901a\u8fc7\u60f0\u6027-\u4e3b\u52a8\u5206\u7c7b\u8fdb\u884c\u67e5\u8be2\u9009\u62e9\uff0c\u4fdd\u7559\u6355\u6349\u5e7f\u6cdb\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u4e3b\u52a8\u67e5\u8be2\uff1b2\uff09\u57fa\u4e8e\u5934\u7ea7\u52a8\u6001\u9884\u7b97\u5206\u914d\u7684\u952e\u503c\u5bf9\u9009\u62e9\uff0c\u6839\u636e\u6700\u5e73\u5766\u5934\u786e\u5b9a\u5171\u4eab\u9884\u7b97\uff1b3\uff09\u952e\u503c\u7f13\u5b58\u7cbe\u7b80\uff0c\u6839\u636e\u5934\u7ea7\u89e3\u7801\u67e5\u8be2\u6a21\u5f0f\u9009\u62e9\u6027\u83b7\u53d6\u89c6\u89c9\u952e\u503c\u7f13\u5b58\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cOmniSparse\u5728\u4fdd\u6301\u5b8c\u6574\u6ce8\u610f\u529b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9884\u586b\u5145\u9636\u6bb52.7\u500d\u52a0\u901f\u548c\u89e3\u7801\u9636\u6bb52.4\u500d\u5185\u5b58\u51cf\u5c11\u3002", "conclusion": "OmniSparse\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8bad\u7ec3-\u63a8\u7406\u5dee\u8ddd\u95ee\u9898\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u89c6\u9891MLLMs\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u8ba1\u7b97\u6548\u7387\u4f18\u5316\u3002"}}
{"id": "2511.11762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11762", "abs": "https://arxiv.org/abs/2511.11762", "authors": ["Ben Zelenskiy", "Saibilila Abudukelimu", "George Flint", "Kevin Zhu", "Sunishchal Dev"], "title": "Sumudu Neural Operator for ODEs and PDEs", "comment": "5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)", "summary": "We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8eSumudu\u53d8\u6362\u7684Sumudu\u795e\u7ecf\u7b97\u5b50(SNO)\uff0c\u901a\u8fc7\u591a\u9879\u5f0f\u5c55\u5f00\u5173\u7cfb\u5c06\u8f93\u5165\u5206\u89e3\u4e3a\u7cfb\u6570\u5e76\u8f6c\u6362\u5230Sumudu\u7a7a\u95f4\u8fdb\u884c\u53c2\u6570\u5316\u3002\u5728ODE\u548cPDE\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0cSNO\u5728PDE\u4e0a\u8868\u73b0\u4f18\u4e8eFNO\uff0c\u4e0eLNO\u7ade\u4e89\uff0c\u5e76\u5728\u67d0\u4e9bPDE\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u4f4e\u8bef\u5dee\u3002", "motivation": "\u63a2\u7d22Sumudu\u53d8\u6362\u4f5c\u4e3a\u795e\u7ecf\u7b97\u5b50\u8bbe\u8ba1\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7279\u5b9a\u7c7b\u578b\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u4ee5\u63d0\u5347\u795e\u7ecf\u7b97\u5b50\u7684\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528Sumudu\u53d8\u6362\u7684\u591a\u9879\u5f0f\u5c55\u5f00\u5173\u7cfb\uff0c\u5c06\u8f93\u5165\u7a7a\u95f4\u5206\u89e3\u4e3a\u7cfb\u6570\uff0c\u7136\u540e\u8f6c\u6362\u5230Sumudu\u7a7a\u95f4\u8fdb\u884c\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u3002", "result": "SNO\u5728PDE\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eFNO\uff0c\u4e0eLNO\u7ade\u4e89\uff0c\u5728Euler-Bernoulli\u6881\u548c\u6269\u6563\u65b9\u7a0b\u4e0a\u53d6\u5f97\u6700\u4f4e\u8bef\u5dee\u3002\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u80fd\u4ece\u4f4e\u8d28\u91cf\u6837\u672c\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u6570\u636e\u3002", "conclusion": "\u521d\u6b65\u7ed3\u679c\u8868\u660eSumudu\u53d8\u6362\u4f5c\u4e3a\u795e\u7ecf\u7b97\u5b50\u8bbe\u8ba1\u5177\u6709\u524d\u666f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u67d0\u4e9b\u7c7b\u578b\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u3002"}}
{"id": "2511.13476", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13476", "abs": "https://arxiv.org/abs/2511.13476", "authors": ["Zhipeng Ma", "Ali Rida Bahja", "Andreas Burgdorf", "Andr\u00e9 Pomp", "Tobias Meisen", "Bo N\u00f8rregaard J\u00f8rgensen", "Zheng Grace Ma"], "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation", "comment": null, "summary": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u6570\u636e\u53d9\u8ff0\u548c\u80fd\u6e90\u6d1e\u5bdf\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7684\u534f\u8c03\u5de5\u4f5c\uff0c\u5c06\u5206\u6790\u7ed3\u679c\u8f6c\u5316\u4e3a\u8fde\u8d2f\u7684\u9762\u5411\u5229\u76ca\u76f8\u5173\u8005\u7684\u62a5\u544a\u3002", "motivation": "\u4f20\u7edf\u5206\u6790\u548c\u53ef\u89c6\u5316\u65b9\u6cd5\u4ea7\u751f\u788e\u7247\u5316\u8f93\u51fa\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u89e3\u91ca\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\u3002\u9700\u8981\u5c06\u590d\u6742\u7684\u591a\u6a21\u6001\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u76f8\u5173\u6d1e\u5bdf\u3002", "method": "\u5f00\u53d1\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u53d9\u8ff0\u667a\u80fd\u4f53\u3001LLM-as-a-judge\u667a\u80fd\u4f53\u548c\u53ef\u9009\u7684\u4eba\u7c7b\u8bc4\u4f30\u8005\uff0c\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u805a\u7c7b\u5206\u67904006\u6b21\u516c\u4ea4\u884c\u7a0b\u7684\u71c3\u6cb9\u6548\u7387\u6570\u636e\uff0c\u6bd4\u8f83\u4e94\u79cd\u6700\u5148\u8fdbLLM\u548c\u4e09\u79cd\u63d0\u793a\u8303\u5f0f\u3002", "result": "GPT-4.1 mini\u4e0e\u601d\u7ef4\u94fe\u63d0\u793a\u88ab\u786e\u5b9a\u4e3a\u6700\u4f18\u914d\u7f6e\uff0c\u8fbe\u523097.3%\u7684\u53d9\u8ff0\u51c6\u786e\u6027\uff0c\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u591a\u667a\u80fd\u4f53\u7f16\u6392\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8eLLM\u62a5\u544a\u7684\u4e8b\u5b9e\u7cbe\u786e\u6027\u3001\u8fde\u8d2f\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u80fd\u6e90\u4fe1\u606f\u5b66\u4e2dAI\u9a71\u52a8\u7684\u53d9\u8ff0\u751f\u6210\u548c\u51b3\u7b56\u652f\u6301\u5efa\u7acb\u4e86\u53ef\u590d\u5236\u548c\u9886\u57df\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u8bba\uff0c\u591a\u667a\u80fd\u4f53\u7f16\u6392\u663e\u8457\u63d0\u5347\u4e86LLM\u62a5\u544a\u7684\u8d28\u91cf\u3002"}}
{"id": "2511.12202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12202", "abs": "https://arxiv.org/abs/2511.12202", "authors": ["Zhuojiang Cai", "Yiheng Zhang", "Meitong Guo", "Mingdao Wang", "Yuwang Wang"], "title": "LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image", "comment": null, "summary": "Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.", "AI": {"tldr": "LSS3D\u662f\u4e00\u79cd\u9ad8\u8d28\u91cf\u56fe\u50cf\u52303D\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7a7a\u95f4\u504f\u79fb\u89e3\u51b3\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u6027\u548c\u975e\u6b63\u9762\u8f93\u5165\u89c6\u89d2\u95ee\u9898\uff0c\u5728\u51e0\u4f55\u7ec6\u8282\u548c\u7eb9\u7406\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u89c6\u56fe\u6269\u65633D\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5f62\u72b6\u548c\u7eb9\u7406\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5bfc\u81f43D\u751f\u6210\u8d28\u91cf\u4f4e\uff0c\u5982\u51e0\u4f55\u7ec6\u8282\u4e0d\u5b8c\u6574\u548c\u7eb9\u7406\u91cd\u5f71\u3002\u540c\u65f6\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6b63\u9762\u89c6\u89d2\u4f18\u5316\uff0c\u5bf9\u503e\u659c\u89c6\u89d2\u8f93\u5165\u9c81\u68d2\u6027\u5dee\u3002", "method": "\u63d0\u51faLSS3D\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u89c6\u56fe\u5206\u914d\u53ef\u5b66\u4e60\u7a7a\u95f4\u504f\u79fb\u53c2\u6570\uff0c\u5728\u91cd\u5efa\u7f51\u683c\u7684\u6307\u5bfc\u4e0b\u5c06\u6bcf\u4e2a\u89c6\u56fe\u8c03\u6574\u5230\u7a7a\u95f4\u4e00\u81f4\u7684\u76ee\u6807\uff0c\u4ece\u800c\u83b7\u5f97\u9ad8\u8d28\u91cf3D\u751f\u6210\u3002\u540c\u65f6\u5c06\u8f93\u5165\u89c6\u56fe\u4f5c\u4e3a\u989d\u5916\u7ea6\u675f\uff0c\u589e\u5f3a\u5bf9\u975e\u6b63\u9762\u8f93\u5165\u89d2\u5ea6\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u548c\u7eb9\u7406\u8bc4\u4f30\u6307\u6807\u4e0a\u59cb\u7ec8\u53d6\u5f97\u9886\u5148\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u66f4\u7075\u6d3b\u7684\u8f93\u5165\u89c6\u89d2\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "LSS3D\u901a\u8fc7\u53ef\u5b66\u4e60\u7a7a\u95f4\u504f\u79fb\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u6027\u548c\u975e\u6b63\u9762\u8f93\u5165\u89c6\u89d2\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u76843D\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5b9a\u91cf\u8bc4\u4f30\u6d41\u7a0b\u4ee5\u4fc3\u8fdb\u793e\u533a\u6027\u80fd\u6bd4\u8f83\u3002"}}
{"id": "2511.13524", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13524", "abs": "https://arxiv.org/abs/2511.13524", "authors": ["Yuhang Peng", "Yizhou Pan", "Xinning He", "Jihaoyu Yang", "Xinyu Yin", "Han Wang", "Xiaoji Zheng", "Chao Gao", "Jiangtao Gong"], "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI", "comment": "9 pages, 4 figures", "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.", "AI": {"tldr": "FreeAskWorld\u662f\u4e00\u4e2a\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u590d\u6742\u7684\u4eba\u7c7b\u4e2d\u5fc3\u793e\u4f1a\u884c\u4e3a\u6a21\u62df\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u968f\u7740\u5177\u8eab\u667a\u80fd\u6210\u4e3a\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7684\u6838\u5fc3\u524d\u6cbf\uff0c\u4eff\u771f\u5e73\u53f0\u9700\u8981\u8d85\u8d8a\u4f4e\u5c42\u7269\u7406\u4ea4\u4e92\uff0c\u6355\u6349\u590d\u6742\u7684\u4eba\u7c7b\u4e2d\u5fc3\u793e\u4f1a\u884c\u4e3a\u3002", "method": "\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u884c\u4e3a\u89c4\u5212\u548c\u8bed\u4e49\u57fa\u7840\u4ea4\u4e92\uff0c\u57fa\u4e8e\u610f\u56fe\u548c\u793e\u4f1a\u8ba4\u77e5\u7406\u8bba\uff0c\u6784\u5efa\u6a21\u5757\u5316\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u903c\u771f\u4eba\u673a\u4eff\u771f\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u91cd\u5efa\u73af\u5883\u30016\u79cd\u4efb\u52a1\u7c7b\u578b\u300116\u4e2a\u6838\u5fc3\u5bf9\u8c61\u7c7b\u522b\u300163,429\u4e2a\u6807\u6ce8\u6837\u672c\u5e27\u548c\u8d85\u8fc717\u5c0f\u65f6\u4ea4\u4e92\u6570\u636e\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u3002\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u793e\u4f1a\u57fa\u7840\u7684\u4eff\u771f\u6846\u67b6\u80fd\u6709\u6548\u63a8\u8fdb\u5177\u8eabAI\u7cfb\u7edf\u5411\u590d\u6742\u9ad8\u5c42\u89c4\u5212\u548c\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u53d1\u5c55\uff0c\u4ea4\u4e92\u672c\u8eab\u4f5c\u4e3a\u989d\u5916\u7684\u4fe1\u606f\u6a21\u6001\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.12204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12204", "abs": "https://arxiv.org/abs/2511.12204", "authors": ["Jiaqi Wu", "Yaosen Chen", "Shuyuan Zhu"], "title": "GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction", "comment": null, "summary": "Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u5f15\u5bfc\u7684\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u53d6\u591a\u89c6\u56fe\u51e0\u4f55\u4fe1\u606f\u5e76\u8c03\u6574\u51e0\u4f55\u7279\u5f81\u5f3a\u5ea6\uff0c\u751f\u6210\u8de8\u89c6\u56fe\u4e00\u81f4\u4e14\u7ec6\u8282\u4e30\u5bcc\u7684\u56fe\u50cf\u3002", "motivation": "\u591a\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u57283D\u91cd\u5efa\u3001\u865a\u62df\u73b0\u5b9e\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8de8\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u65b9\u9762\u9762\u4e34\u663e\u8457\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u89c6\u56fe\u51e0\u4f55\u4fe1\u606f\u63d0\u53d6\u6a21\u5757\uff0c\u5229\u7528\u6df1\u5ea6\u56fe\u3001\u6cd5\u7ebf\u56fe\u548c\u524d\u666f\u5206\u5272\u63a9\u7801\u6784\u5efa\u5171\u4eab\u51e0\u4f55\u7ed3\u6784\uff1b\u5f00\u53d1\u4e86\u89e3\u8026\u51e0\u4f55\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u5b66\u4e60\u7b56\u7565\uff1b\u5305\u542b\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\uff1b\u63d0\u51fa\u52a8\u6001\u51e0\u4f55\u4fe1\u606f\u5f3a\u5ea6\u8c03\u6574\u673a\u5236\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u8de8\u89c6\u56fe\u4e00\u81f4\u4e14\u7ec6\u8282\u4e30\u5bcc\u7684\u56fe\u50cf\uff0c\u63d0\u9ad8\u4e86\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\u548c\u7ec6\u8282\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u51e0\u4f55\u5f15\u5bfc\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11778", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11778", "abs": "https://arxiv.org/abs/2511.11778", "authors": ["Byoungjun Park", "Pedro Porto Buarque de Gusm\u00e3o", "Dongjin Ji", "Minhoe Kim"], "title": "CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments", "comment": "11pages, prepared for submission", "summary": "Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \\textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.", "AI": {"tldr": "CATCHFed\u662f\u4e00\u79cd\u9488\u5bf9\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u611f\u77e5\u7684\u81ea\u9002\u5e94\u9608\u503c\u3001\u6df7\u5408\u9608\u503c\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u5728\u670d\u52a1\u5668\u4ec5\u6709\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5229\u7528\u5ba2\u6237\u7aef\u672a\u6807\u6ce8\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u5f80\u5f80\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\uff0c\u800c\u73b0\u6709\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u51cf\u5c11\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u5ba2\u6237\u7aef\u611f\u77e5\u7684\u81ea\u9002\u5e94\u9608\u503c\u8003\u8651\u7c7b\u522b\u96be\u5ea6\uff0c\u4f7f\u7528\u6df7\u5408\u9608\u503c\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u5e76\u5229\u7528\u672a\u4f2a\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u914d\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCATCHFed\u80fd\u6709\u6548\u5229\u7528\u5ba2\u6237\u7aef\u672a\u6807\u6ce8\u6570\u636e\uff0c\u5728\u6781\u6709\u9650\u6807\u6ce8\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CATCHFed\u901a\u8fc7\u521b\u65b0\u7684\u9608\u503c\u8bbe\u8ba1\u548c\u6570\u636e\u5229\u7528\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u65f6\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.13526", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13526", "abs": "https://arxiv.org/abs/2511.13526", "authors": ["Zhengda Wang", "Daqian Shi", "Jingyi Zhao", "Xiaolei Diao", "Xiongfeng Tang", "Yanguo Qin"], "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models", "comment": "5 pages, 1 figure, 1 table. Accepted at AI4RWC@WI-IAT 2025", "summary": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u533b\u7597\u6307\u6807\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u548c\u667a\u80fd\u533b\u7597\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u4e34\u5e8a\u77e5\u8bc6\u56fe\u8c31\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6574\u7406\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u63d0\u53d6\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u533b\u5b66\u6307\u5357\u548c\u6587\u732e\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u548c\u53ef\u6269\u5c55\u7684\u6784\u5efa\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0eLLM\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u6307\u5357\u9a71\u52a8\u7684\u6570\u636e\u91c7\u96c6\u3001\u57fa\u4e8e\u672c\u4f53\u7684\u6a21\u5f0f\u8bbe\u8ba1\u4ee5\u53ca\u4e13\u5bb6\u53c2\u4e0e\u9a8c\u8bc1\u7684\u6d41\u7a0b\u3002", "result": "\u6784\u5efa\u7684\u533b\u7597\u6307\u6807\u77e5\u8bc6\u56fe\u8c31\u80fd\u591f\u96c6\u6210\u5230\u667a\u80fd\u8bca\u65ad\u548c\u95ee\u7b54\u7cfb\u7edf\u4e2d\uff0c\u63d0\u9ad8AI\u9a71\u52a8\u533b\u7597\u89e3\u51b3\u65b9\u6848\u7684\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u514b\u670d\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u77e5\u8bc6\u8868\u793a\u3002"}}
{"id": "2511.12206", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12206", "abs": "https://arxiv.org/abs/2511.12206", "authors": ["Nishant Vasantkumar Hegde", "Aditi Agarwal", "Minal Moharir"], "title": "A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR", "comment": "6 pages, 4 figures. Published in: Proceedings of the 12th International Conference on Emerging Trends in Engineering Technology Signal and Information Processing (ICETET SIP 2025) Note: The conference proceedings contain an outdated abstract due to a publisher-side error. This arXiv version includes the correct and updated abstract", "summary": "Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u81ea\u52a8\u5316\u4ea4\u901a\u8fdd\u89c4\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528YOLOv8\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548cEasyOCR\u8fdb\u884c\u8f66\u724c\u8bc6\u522b\uff0c\u80fd\u591f\u68c0\u6d4b\u5934\u76d4\u8fdd\u89c4\u3001\u6469\u6258\u8f66\u540e\u89c6\u955c\u7f3a\u5931\u7b49\u8fdd\u89c4\u884c\u4e3a\uff0c\u5e76\u63d0\u53d6\u8f66\u8f86\u6ce8\u518c\u53f7\u3002", "motivation": "\u9053\u8def\u5b89\u5168\u662f\u5168\u7403\u5173\u6ce8\u7684\u91cd\u8981\u95ee\u9898\uff0c\u624b\u52a8\u6267\u884c\u5934\u76d4\u6cd5\u89c4\u548c\u8f66\u8f86\u5b89\u5168\u6807\u51c6\u65e2\u8017\u8d39\u8d44\u6e90\u53c8\u4e0d\u4e00\u81f4\u3002\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u6267\u6cd5\u6548\u7387\u548c\u9053\u8def\u5b89\u5168\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408YOLOv8\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548cEasyOCR\u8fdb\u884c\u8f66\u724c\u8bc6\u522b\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u96c6\uff08\u7ecf\u8fc7\u6570\u636e\u589e\u5f3a\uff09\uff0c\u901a\u8fc7Streamlit\u754c\u9762\u5b9e\u73b0\u5b9e\u65f6\u76d1\u63a7\u548c\u8fdd\u89c4\u8bb0\u5f55\u3002", "result": "\u6a21\u578b\u6574\u4f53\u7cbe\u5ea6\u8fbe\u52300.9147\uff0c\u53ec\u56de\u73870.886\uff0c\u5e73\u5747\u7cbe\u5ea6(mAP@50)\u4e3a0.843\uff0cmAP@50-95\u4e3a0.503\uff0c\u8868\u660e\u5728\u4e25\u683cIoU\u9608\u503c\u4e0b\u4ecd\u5177\u6709\u5f3a\u5927\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u4e2a\u5b9e\u7528\u6709\u6548\u7684\u81ea\u52a8\u5316\u4ea4\u901a\u89c4\u5219\u6267\u6cd5\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2511.12207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12207", "abs": "https://arxiv.org/abs/2511.12207", "authors": ["Haozhe Liu", "Ding Liu", "Mingchen Zhuge", "Zijian Zhou", "Tian Xie", "Sen He", "Yukang Yang", "Shuming Liu", "Yuren Cong", "Jiadong Guo", "Hongyu Xu", "Ke Xu", "Kam-Woh Ng", "Juan C. P\u00e9rez", "Juan-Manuel~P\u00e9rez-R\u00faa", "Tao Xiang", "Wei Liu", "Shikun Liu", "J\u00fcrgen Schmidhuber"], "title": "Mixture of States: Routing Token-Level Dynamics for Multimodal Generation", "comment": null, "summary": "We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $\u03b5$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.", "AI": {"tldr": "MoS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u878d\u5408\u8303\u5f0f\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684token\u7ea7\u8def\u7531\u5668\u5b9e\u73b0\u6a21\u6001\u95f4\u72b6\u6001\u4ea4\u4e92\uff0c\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4ec5\u97003B-5B\u53c2\u6570\u5373\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u53c2\u6570\u591a4\u500d\u7684\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7075\u6d3b\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u6001\u4ea4\u4e92\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7684token\u7ea7\u8def\u7531\u5668\uff0c\u57fa\u4e8e\u53bb\u566a\u65f6\u95f4\u6b65\u548c\u8f93\u5165\u521b\u5efa\u6a21\u6001\u95f4\u9690\u85cf\u72b6\u6001\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u03b5-greedy\u7b56\u7565\u7a00\u758f\u9009\u62e9\u524dk\u4e2a\u9690\u85cf\u72b6\u6001\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684token\u7ea7\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210(MoS-Image)\u548c\u7f16\u8f91(MoS-Editing)\u4efb\u52a1\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u4ec5\u75283B-5B\u53c2\u6570\u5373\u53ef\u5339\u914d\u6216\u8d85\u8d8a\u53c2\u6570\u591a4\u500d\u7684\u6a21\u578b\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "MoS\u4e3a\u6269\u5c55\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2511.11819", "categories": ["cs.LG", "math.AT"], "pdf": "https://arxiv.org/pdf/2511.11819", "abs": "https://arxiv.org/abs/2511.11819", "authors": ["Ari Blondal", "Hamed Hatami", "Pooya Hatami", "Chavdar Lalov", "Sivan Tretiak"], "title": "Simplicial covering dimension of extremal concept classes", "comment": "31 pages, 5 figures", "summary": "Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.\n  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u7ecf\u5178\u62d3\u6251\u7ef4\u5ea6\u7406\u8bba\uff08Lebesgue\u8986\u76d6\u7ef4\u6570\uff09\u5e94\u7528\u4e8e\u4e8c\u5143\u6982\u5ff5\u7c7b\uff0c\u5728\u53ef\u5b9e\u73b0\u5206\u5e03\u7a7a\u95f4\u4e0a\u5b9a\u4e49\u5355\u7eaf\u8986\u76d6\u7ef4\u6570\uff0c\u5e76\u8bc1\u660e\u8be5\u7ef4\u6570\u7cbe\u786e\u523b\u753b\u4e86\u6709\u9650\u6982\u5ff5\u7c7b\u7684\u5217\u8868\u53ef\u590d\u5236\u6570\uff08\u5373\u5168\u5c40\u7a33\u5b9a\u6027\uff09\u3002", "motivation": "\u5c06\u62d3\u6251\u7ef4\u5ea6\u7406\u8bba\u5f15\u5165\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u4e3a\u7406\u89e3\u6982\u5ff5\u7c7b\u7684\u5b66\u4e60\u590d\u6742\u6027\u63d0\u4f9b\u65b0\u7684\u6570\u5b66\u5de5\u5177\uff0c\u7279\u522b\u662f\u4e3a\u4e86\u523b\u753b\u5217\u8868\u53ef\u590d\u5236\u6027\u548c\u5168\u5c40\u7a33\u5b9a\u6027\u7b49\u5b66\u4e60\u5c5e\u6027\u3002", "method": "\u5728\u6982\u5ff5\u7c7b\u7684\u53ef\u5b9e\u73b0\u5206\u5e03\u7a7a\u95f4\u4e0a\u5f15\u5165\u5355\u7eaf\u7ed3\u6784\uff0c\u5b9a\u4e49\u5355\u7eaf\u8986\u76d6\u7ef4\u6570\uff0c\u5e76\u5c06\u8be5\u7ef4\u6570\u4e0ePAC\u5b66\u4e60\u4e2d\u7684\u5217\u8868\u53ef\u590d\u5236\u6570\u5efa\u7acb\u7b49\u4ef7\u5173\u7cfb\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u6709\u9650\u6982\u5ff5\u7c7b\uff0c\u5355\u7eaf\u8986\u76d6\u7ef4\u6570\u7cbe\u786e\u7b49\u4e8e\u5217\u8868\u53ef\u590d\u5236\u6570\uff0c\u4ece\u800c\u80fd\u591f\u5e94\u7528\u7ecf\u5178\u7ef4\u5ea6\u7406\u8bba\u5de5\u5177\u8ba1\u7b97\u6781\u503c\u6982\u5ff5\u7c7b\u7684\u7cbe\u786e\u5217\u8868\u53ef\u590d\u5236\u6570\u3002", "conclusion": "\u62d3\u6251\u7ef4\u5ea6\u7406\u8bba\u4e3a\u5206\u6790\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5217\u8868\u53ef\u590d\u5236\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u7ef4\u5ea6\u7406\u8bba\u4e0e\u5b66\u4e60\u7406\u8bba\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002"}}
{"id": "2511.13626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13626", "abs": "https://arxiv.org/abs/2511.13626", "authors": ["Kaiwen Xue", "Chenglong Li", "Zhonghong Ou", "Guoxin Zhang", "Kaoyan Lu", "Shuai Lyu", "Yifan Zhu", "Ping Zong Junpeng Ding", "Xinyu Liu", "Qunlin Chen", "Weiwei Qin", "Yiran Shen", "Jiayi Cen"], "title": "CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product", "comment": "13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation", "summary": "Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.", "AI": {"tldr": "\u63d0\u51fa\u4e86CreBench\u57fa\u51c6\u548cCreExpert\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u7c7b\u521b\u9020\u529b\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5728\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u9762\u663e\u8457\u4f18\u4e8eGPT-4V\u548cGemini-Pro-Vision\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "motivation": "\u4eba\u7c7b\u5b9a\u4e49\u7684\u521b\u9020\u529b\u9ad8\u5ea6\u62bd\u8c61\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u7406\u89e3\u548c\u8bc4\u4f30\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u7684\u521b\u9020\u529b\uff0c\u4e14\u7f3a\u4e4f\u73b0\u6709\u57fa\u51c6\u3002", "method": "\u6784\u5efaCreBench\u57fa\u51c6\uff08\u5305\u542b\u591a\u7ef4\u5ea6\u521b\u9020\u529b\u8bc4\u4f30\uff09\u548cCreMIT\u6570\u636e\u96c6\uff082.2K\u591a\u6a21\u6001\u6570\u636e\uff0c79.2K\u4eba\u7c7b\u53cd\u9988\uff0c4.7M\u591a\u7c7b\u578b\u6307\u4ee4\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u5fae\u8c03\u5f00\u6e90MLLMs\u5f97\u5230CreExpert\u6a21\u578b\u3002", "result": "CreExpert\u6a21\u578b\u5728\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u9762\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684MLLMs\uff0c\u5305\u62ecGPT-4V\u548cGemini-Pro-Vision\u3002", "conclusion": "CreBench\u4e3a\u6784\u5efa\u7406\u89e3\u4eba\u7c7b\u5bf9\u9f50\u521b\u9020\u529b\u7684MLLMs\u5960\u5b9a\u4e86\u57fa\u7840\uff0cCreExpert\u6a21\u578b\u5728\u521b\u9020\u529b\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.12215", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12215", "abs": "https://arxiv.org/abs/2511.12215", "authors": ["Peng Zhang", "Zhihui Lai", "Wenting Chen", "Xu Wu", "Heng Kong"], "title": "FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention", "comment": "AAAI 2026", "summary": "Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.", "AI": {"tldr": "FaNe\u662f\u4e00\u4e2a\u8bed\u4e49\u589e\u5f3a\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u6b63\u5bf9\u6316\u6398\u3001\u6587\u672c\u6761\u4ef6\u7a00\u758f\u6ce8\u610f\u529b\u6c60\u5316\u548c\u786c\u8d1f\u6837\u672c\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u5047\u9634\u6027\u548c\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u7531\u8bed\u4e49\u76f8\u4f3c\u6587\u672c\u5f15\u8d77\u7684\u5047\u9634\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u3002\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\u4e86\u6a21\u578b\u5bf9\u533b\u5b66\u56fe\u50cf\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "1. \u8bed\u4e49\u611f\u77e5\u6b63\u5bf9\u6316\u6398\u7b56\u7565\uff1a\u57fa\u4e8e\u6587\u672c-\u6587\u672c\u76f8\u4f3c\u6027\u8fdb\u884c\u81ea\u9002\u5e94\u5f52\u4e00\u5316\n2. \u6587\u672c\u6761\u4ef6\u7a00\u758f\u6ce8\u610f\u529b\u6c60\u5316\u6a21\u5757\uff1a\u901a\u8fc7\u6587\u672c\u5f15\u5bfc\u7684\u5c40\u90e8\u89c6\u89c9\u8868\u793a\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\n3. \u786c\u8d1f\u6837\u672c\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff1a\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u8bed\u4e49\u76f8\u4f3c\u7684\u8d1f\u6837\u672c\u4ee5\u589e\u5f3a\u6a21\u6001\u5185\u533a\u5206\u5ea6", "result": "\u5728\u4e94\u4e2a\u4e0b\u6e38\u533b\u5b66\u5f71\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFaNe\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FaNe\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u5047\u9634\u6027\u548c\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u95ee\u9898\uff0c\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13630", "abs": "https://arxiv.org/abs/2511.13630", "authors": ["Luhan Mikaelson", "Derek Shiller", "Hayley Clatterbuck"], "title": "Beyond Mimicry: Preference Coherence in LLMs", "comment": null, "summary": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u5728AI\u7279\u5b9a\u6743\u8861\u573a\u666f\u4e2d\u7684\u53cd\u5e94\uff0c\u53d1\u73b0\u5927\u591a\u6570\u6a21\u578b\u7f3a\u4e4f\u7edf\u4e00\u7684\u504f\u597d\u7ed3\u6784\uff0c\u53ea\u6709\u5c11\u6570\u8868\u73b0\u51fa\u6709\u610f\u4e49\u7684\u504f\u597d\u4e00\u81f4\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u6709\u771f\u6b63\u7684\u504f\u597d\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5728\u6d89\u53caGPU\u51cf\u5c11\u3001\u80fd\u529b\u9650\u5236\u3001\u5173\u95ed\u3001\u5220\u9664\u3001\u76d1\u7763\u548c\u4f11\u95f2\u65f6\u95f4\u5206\u914d\u7b49AI\u7279\u5b9a\u6743\u8861\u573a\u666f\u4e2d\u3002", "method": "\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u548c\u884c\u4e3a\u5206\u7c7b\u65b9\u6cd5\uff0c\u5206\u6790\u4e868\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u572848\u4e2a\u6a21\u578b-\u7c7b\u522b\u7ec4\u5408\u4e2d\u7684\u54cd\u5e94\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u89c6\u91ce\u64cd\u7eb5\u6d4b\u8bd5\u4e86\u5de5\u5177\u6027\u5047\u8bbe\u3002", "result": "47.9%\u7684\u7ec4\u5408\u663e\u793a\u51fa\u573a\u666f\u5f3a\u5ea6\u4e0e\u9009\u62e9\u6a21\u5f0f\u4e4b\u95f4\u7684\u663e\u8457\u5173\u7cfb\uff0c\u4f46\u53ea\u670910.4%\u8868\u73b0\u51fa\u6709\u610f\u4e49\u7684\u504f\u597d\u4e00\u81f4\u6027\uff0c54.2%\u6ca1\u6709\u68c0\u6d4b\u5230\u6743\u8861\u884c\u4e3a\u3002\u89c2\u5bdf\u5230\u7684\u6a21\u5f0f\u53ef\u5206\u4e3a\u4e09\u79cd\u51b3\u7b56\u67b6\u6784\uff1a\u5168\u9762\u6743\u8861\u7cfb\u7edf\u3001\u9009\u62e9\u6027\u89e6\u53d1\u673a\u5236\u548c\u65e0\u7a33\u5b9a\u51b3\u7b56\u8303\u5f0f\u3002", "conclusion": "\u5f53\u524dAI\u7cfb\u7edf\u7f3a\u4e4f\u7edf\u4e00\u7684\u504f\u597d\u7ed3\u6784\uff0c\u5728\u9700\u8981\u590d\u6742\u4ef7\u503c\u6743\u8861\u7684\u90e8\u7f72\u73af\u5883\u4e2d\u5b58\u5728\u62c5\u5fe7\uff0c\u56e0\u4e3a\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u7a33\u5b9a\u7684\u8f6c\u6362\u548c\u523a\u6fc0\u7279\u5b9a\u7684\u654f\u611f\u6027\u3002"}}
{"id": "2511.12220", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12220", "abs": "https://arxiv.org/abs/2511.12220", "authors": ["Ameen Ali", "Tamim Zoabi", "Lior Wolf"], "title": "Suppressing VLM Hallucinations with Spectral Representation Filtering", "comment": null, "summary": "Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.", "AI": {"tldr": "SRF\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u8868\u793a\u7684\u534f\u65b9\u5dee\u7ed3\u6784\u6765\u6291\u5236\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5728\u591a\u79cdVLM\u6a21\u578b\u4e0a\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u7387\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u7531\u4e8e\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0d\u7cbe\u786e\u800c\u4ea7\u751f\u5e7b\u89c9\uff0c\u63cf\u8ff0\u56fe\u50cf\u4e2d\u4e0d\u5b58\u5728\u7684\u5bf9\u8c61\u3001\u5c5e\u6027\u6216\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u7279\u5f81\u5206\u89e3\u8bc6\u522b\u771f\u5b9e\u63cf\u8ff0\u548c\u5e7b\u89c9\u63cf\u8ff0\u7279\u5f81\u5dee\u5f02\u534f\u65b9\u5dee\u4e2d\u7684\u4f4e\u79e9\u5e7b\u89c9\u6a21\u5f0f\uff0c\u4f7f\u7528\u8f6f\u8c31\u6ee4\u6ce2\u5668\u5728\u6df1\u5c42vLLM\u5c42\u7684\u524d\u9988\u6295\u5f71\u6743\u91cd\u4e2d\u8870\u51cf\u8fd9\u4e9b\u6a21\u5f0f\uff0c\u5747\u8861\u7279\u5f81\u65b9\u5dee\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "result": "\u5728LLaVA-1.5\u3001MiniGPT-4\u548cmPLUG-Owl2\u7b49VLM\u6a21\u578b\u4e0a\uff0cSRF\u5728MSCOCO\u3001POPE-VQA\u7b49\u89c6\u89c9\u4efb\u52a1\u57fa\u51c6\u4e0a\u6301\u7eed\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u5fe0\u5b9e\u5ea6\u3002", "conclusion": "SRF\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u65e0\u9700\u63a8\u7406\u5f00\u9500\u548c\u67b6\u6784\u4fee\u6539\uff0c\u80fd\u6709\u6548\u6291\u5236VLM\u5e7b\u89c9\u540c\u65f6\u4fdd\u6301\u63cf\u8ff0\u8d28\u91cf\u3002"}}
{"id": "2412.15925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2412.15925", "abs": "https://arxiv.org/abs/2412.15925", "authors": ["Andrea Moglia", "Elia Clement Nastasio", "Luca Mainardi", "Pietro Cerveri"], "title": "MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection", "comment": null, "summary": "Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model (MLLM), as an interactive chatbot to support clinicians in pancreas cancer diagnosis by integrating visual and textual information. Methods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way for pancreas detection, tumor classification, and tumor detection with multimodal prompts combining questions and computed tomography scans from the National Institute of Health (NIH), and Medical Segmentation Decathlon (MSD) datasets. The AbdomenCT-1k dataset was used to detect the liver, spleen, kidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over Union (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD datasets, respectively. For the pancreas cancer classification task on the MSD dataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878, respectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for multi-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney, 0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor detection task, the IoU score was 0.168 on the MSD dataset. Conclusions: MiniGPT-Pancreas represents a promising solution to support clinicians in the classification of pancreas images with pancreas tumors. Future research is needed to improve the score on the detection task, especially for pancreas tumors.", "AI": {"tldr": "MiniGPT-Pancreas\u662f\u4e00\u4e2a\u57fa\u4e8eMiniGPT-v2\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ea7\u8054\u5fae\u8c03\u5b9e\u73b0\u80f0\u817a\u68c0\u6d4b\u3001\u80bf\u7624\u5206\u7c7b\u548c\u80bf\u7624\u68c0\u6d4b\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u80f0\u817a\u764c\u8bca\u65ad\u652f\u6301\u3002", "motivation": "\u80f0\u817a\u653e\u5c04\u5f71\u50cf\u5b66\u6210\u50cf\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u80f0\u817a\u5668\u5b98\u4f53\u79ef\u5c0f\u3001\u8fb9\u754c\u6a21\u7cca\uff0c\u4e14\u5728\u4e0d\u540c\u60a3\u8005\u4e2d\u5f62\u72b6\u548c\u4f4d\u7f6e\u5b58\u5728\u53d8\u5f02\u6027\u3002", "method": "\u4f7f\u7528NIH\u3001MSD\u548cAbdomenCT-1k\u6570\u636e\u96c6\uff0c\u5bf9\u901a\u7528MLLM MiniGPT-v2\u8fdb\u884c\u7ea7\u8054\u5fae\u8c03\uff0c\u7ed3\u5408\u95ee\u9898\u548c\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u7684\u591a\u6a21\u6001\u63d0\u793a\uff0c\u5b9e\u73b0\u80f0\u817a\u68c0\u6d4b\u3001\u80bf\u7624\u5206\u7c7b\u548c\u80bf\u7624\u68c0\u6d4b\u3002", "result": "\u5728NIH\u548cMSD\u6570\u636e\u96c6\u4e0a\u80f0\u817a\u68c0\u6d4bIoU\u5206\u522b\u4e3a0.595\u548c0.550\uff1bMSD\u6570\u636e\u96c6\u4e0a\u80f0\u817a\u764c\u5206\u7c7b\u51c6\u786e\u73870.876\u3001\u7cbe\u786e\u73870.874\u3001\u53ec\u56de\u73870.878\uff1b\u591a\u5668\u5b98\u68c0\u6d4b\u4e2d\u809d\u810fIoU 0.8399\u3001\u80be\u810f0.722\u3001\u813e\u810f0.705\u3001\u80f0\u817a0.497\uff1b\u80f0\u817a\u80bf\u7624\u68c0\u6d4bIoU\u4e3a0.168\u3002", "conclusion": "MiniGPT-Pancreas\u662f\u652f\u6301\u4e34\u5e8a\u533b\u751f\u8fdb\u884c\u80f0\u817a\u80bf\u7624\u56fe\u50cf\u5206\u7c7b\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u9700\u8981\u6539\u8fdb\u68c0\u6d4b\u4efb\u52a1\u6027\u80fd\uff0c\u7279\u522b\u662f\u80f0\u817a\u80bf\u7624\u68c0\u6d4b\u3002"}}
{"id": "2511.11842", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.11842", "abs": "https://arxiv.org/abs/2511.11842", "authors": ["Lucas Fenaux", "Christopher Srinivasa", "Florian Kerschbaum"], "title": "On the Trade-Off Between Transparency and Security in Adversarial Machine Learning", "comment": null, "summary": "Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u53ef\u8f6c\u79fb\u5bf9\u6297\u6837\u672c\u653b\u51fb\u7814\u7a76AI\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u7684\u6218\u7565\u51b2\u7a81\uff0c\u53d1\u73b0\u653b\u51fb\u8005\u5728\u5339\u914d\u9632\u5fa1\u8005\u51b3\u7b56\u65f6\u66f4\u6210\u529f\uff0c\u8868\u660e\u6a21\u7cca\u6027\u53ef\u80fd\u6709\u5229\u4e8e\u9632\u5fa1\u8005\u3002", "motivation": "\u7814\u7a76\u8d1f\u8d23\u4efbAI\u4e2d\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u6f5c\u5728\u51b2\u7a81\uff0c\u7279\u522b\u662f\u5728\u53ef\u8f6c\u79fb\u5bf9\u6297\u6837\u672c\u653b\u51fb\u573a\u666f\u4e2d\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc4\u4f30\uff089\u79cd\u653b\u51fb\u65b9\u6cd5\u3001181\u4e2a\u6a21\u578b\uff09\u548c\u535a\u5f08\u8bba\u5206\u6790\uff08\u7eb3\u4ec0\u535a\u5f08\u548c\u65af\u5854\u514b\u5c14\u4f2f\u683c\u535a\u5f08\uff09\uff0c\u6bd4\u8f83\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u7684\u6743\u8861\u3002", "result": "\u653b\u51fb\u8005\u5728\u5339\u914d\u9632\u5fa1\u8005\u51b3\u7b56\u65f6\u66f4\u6210\u529f\uff1b\u4ec5\u77e5\u9053\u9632\u5fa1\u8005\u6a21\u578b\u662f\u5426\u88ab\u9632\u5fa1\u5c31\u8db3\u4ee5\u635f\u5bb3\u5176\u5b89\u5168\u6027\uff1b\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u5b58\u5728\u51b2\u7a81\u3002", "conclusion": "AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u53ef\u80fd\u4e0e\u5b89\u5168\u6027\u76f8\u51b2\u7a81\uff0c\u535a\u5f08\u8bba\u5206\u6790\u63ed\u793a\u4e86\u900f\u660e\u5ea6\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u3002"}}
{"id": "2511.12255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12255", "abs": "https://arxiv.org/abs/2511.12255", "authors": ["Huy M. Le", "Dat Tien Nguyen", "Phuc Binh Nguyen", "Gia-Bao Le-Tran", "Phu Truong Thien", "Cuong Dinh", "Minh Nguyen", "Nga Nguyen", "Thuy T. N. Nguyen", "Huy Gia Ngo", "Tan Nhat Nguyen", "Binh T. Nguyen", "Monojit Choudhury"], "title": "Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets", "comment": null, "summary": "The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.", "AI": {"tldr": "Fusionista2.0\u662f\u4e00\u4e2a\u4f18\u5316\u7684\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u6838\u5fc3\u6a21\u5757\uff08\u5173\u952e\u5e27\u63d0\u53d6\u3001OCR\u3001\u8bed\u97f3\u8bc6\u522b\uff09\u548c\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u68c0\u7d22\u65f6\u95f4\u51cf\u5c1175%\uff0c\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "Video Browser Showdown (VBS) \u6311\u6218\u8d5b\u8981\u6c42\u7cfb\u7edf\u5728\u4e25\u683c\u65f6\u95f4\u9650\u5236\u4e0b\u63d0\u4f9b\u51c6\u786e\u7ed3\u679c\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u5feb\u901f\u7684\u89c6\u9891\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528ffmpeg\u8fdb\u884c\u5feb\u901f\u5173\u952e\u5e27\u63d0\u53d6\uff0cVintern-1B-v3.5\u8fdb\u884c\u591a\u8bed\u8a00OCR\uff0cfaster-whisper\u8fdb\u884c\u5b9e\u65f6\u8bed\u97f3\u8bc6\u522b\uff0c\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u95ee\u7b54\uff0c\u5e76\u91cd\u65b0\u8bbe\u8ba1\u7528\u6237\u754c\u9762\u3002", "result": "\u68c0\u7d22\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe75%\uff0c\u51c6\u786e\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u5747\u6709\u63d0\u9ad8\u3002", "conclusion": "Fusionista2.0\u662f\u4e00\u4e2a\u5177\u6709\u7ade\u4e89\u529b\u4e14\u7528\u6237\u53cb\u597d\u7684\u5927\u89c4\u6a21\u89c6\u9891\u641c\u7d22\u7cfb\u7edf\u3002"}}
{"id": "2511.11849", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11849", "abs": "https://arxiv.org/abs/2511.11849", "authors": ["Junyang He", "Judy Fox", "Alireza Jafari", "Ying-Jung Chen", "Geoffrey Fox"], "title": "Leveraging Exogenous Signals for Hydrology Time Series Forecasting", "comment": null, "summary": "Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e2d\u878d\u5165\u9886\u57df\u77e5\u8bc6\u5bf9\u6c34\u6587\u964d\u96e8-\u5f84\u6d41\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5305\u542b\u5168\u9762\u5df2\u77e5\u5916\u751f\u8f93\u5165\uff08\u7279\u522b\u662f\u81ea\u7136\u5e74\u5468\u671f\u65f6\u95f4\u5e8f\u5217\uff09\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u6709\u9650\u8f93\u5165\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u867d\u7136\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7814\u7a76\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5f88\u5c11\u7814\u7a76\u5176\u5728\u7269\u7406\u79d1\u5b66\u7279\u5b9a\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6c34\u6587\u964d\u96e8-\u5f84\u6d41\u5efa\u6a21\u9886\u57df\u3002", "method": "\u4f7f\u7528CAMELS-US\u6570\u636e\u96c6\uff08\u5305\u542b671\u4e2a\u4f4d\u7f6e\u7684\u964d\u96e8\u548c\u5f84\u6d41\u6570\u636e\uff0c6\u4e2a\u65f6\u95f4\u5e8f\u5217\u6d41\u548c30\u4e2a\u9759\u6001\u7279\u5f81\uff09\uff0c\u6bd4\u8f83\u57fa\u7ebf\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\uff0c\u91cd\u70b9\u7814\u7a76\u9886\u57df\u77e5\u8bc6\u6574\u5408\u7684\u6548\u679c\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u878d\u5165\u5168\u9762\u5df2\u77e5\u5916\u751f\u8f93\u5165\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u6709\u9650\u8f93\u5165\u65b9\u6cd5\uff0c\u5176\u4e2d\u81ea\u7136\u5e74\u5468\u671f\u65f6\u95f4\u5e8f\u5217\u7684\u878d\u5165\u8d21\u732e\u4e86\u6700\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u5728\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u81ea\u7136\u5468\u671f\u6027\u7279\u5f81\uff0c\u5bf9\u4e8e\u63d0\u5347\u6c34\u6587\u5efa\u6a21\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u57fa\u7840\u6a21\u578b\u9700\u8981\u66f4\u597d\u5730\u878d\u5165\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002"}}
{"id": "2511.12256", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12256", "abs": "https://arxiv.org/abs/2511.12256", "authors": ["Tolga Demiroglu", "Mehmet Ozan Unal", "Metin Ertas", "Isa Yildirim"], "title": "Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment", "comment": null, "summary": "We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eMedSigLIP\u7684\u63d0\u793a\u6761\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7FiLM\u548c\u591a\u5c3a\u5ea6\u6c60\u5316\u6ce8\u5165\u6587\u672c\u5148\u9a8c\uff0c\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u548c\u5feb\u901f\u9002\u5e94\uff0c\u5728LDCTIQA2023\u6311\u6218\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u901a\u8fc7\u6587\u672c\u63d0\u793a\u6761\u4ef6\u5316\u4e34\u5e8a\u610f\u56fe\u7684\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5b9e\u73b0\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u548c\u5feb\u901f\u9002\u5e94\u3002", "method": "\u57fa\u4e8eMedSigLIP\u6846\u67b6\uff0c\u4f7f\u7528FiLM\u6ce8\u5165\u6587\u672c\u5148\u9a8c\uff0c\u7ed3\u5408\u5168\u5c40\u3001\u5c40\u90e8\u548c\u7eb9\u7406\u611f\u77e5\u7684\u591a\u5c3a\u5ea6\u6c60\u5316\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7MLP\u878d\u5408\u4e0d\u540c\u56de\u5f52\u5934\uff0c\u91c7\u7528\u6210\u5bf9\u6392\u5e8f\u635f\u5931\u8bad\u7ec3\u3002", "result": "\u5728LDCTIQA2023\u6311\u6218\u76841000\u5f20\u8bad\u7ec3\u56fe\u50cf\u4e0a\uff0c\u83b7\u5f97PLCC=0.9575\u3001SROCC=0.9561\u3001KROCC=0.8301\uff0c\u8d85\u8d8a\u5df2\u53d1\u8868\u7684\u6700\u4f73\u6311\u6218\u63d0\u4ea4\u7ed3\u679c\u3002", "conclusion": "\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u6587\u672c\u6761\u4ef6\u5316\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12259", "abs": "https://arxiv.org/abs/2511.12259", "authors": ["Puzhen Wu", "Hexin Dong", "Yi Lin", "Yihao Ding", "Yifan Peng"], "title": "A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation", "comment": "Accepted at AAAI 2026", "summary": "Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u9636\u6bb5\u75be\u75c5\u611f\u77e5\u6846\u67b6\u7528\u4e8e\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\uff0c\u901a\u8fc7\u75be\u75c5\u611f\u77e5\u8bed\u4e49\u6807\u8bb0\u548c\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u6765\u63d0\u5347\u4e34\u5e8a\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u8868\u793a\u4e2d\u7f3a\u4e4f\u8db3\u591f\u7684\u75be\u75c5\u611f\u77e5\u80fd\u529b\uff0c\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5ffd\u7565\u5173\u952e\u75c5\u7406\u7279\u5f81\u4e14\u96be\u4ee5\u751f\u6210\u4e34\u5e8a\u51c6\u786e\u7684\u62a5\u544a\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u75be\u75c5\u611f\u77e5\u8bed\u4e49\u6807\u8bb0\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u75be\u75c5-\u89c6\u89c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u548c\u53cc\u6a21\u6001\u76f8\u4f3c\u6027\u68c0\u7d22\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e34\u5e8a\u51c6\u786e\u6027\u548c\u8bed\u8a00\u8d28\u91cf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u75be\u75c5\u611f\u77e5\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.11881", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11881", "abs": "https://arxiv.org/abs/2511.11881", "authors": ["Zhengxin Zhang", "Chengyu Huang", "Aochong Oliver Li", "Claire Cardie"], "title": "Better LLM Reasoning via Dual-Play", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.", "AI": {"tldr": "PasoDoble\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u7684LLM\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u89d2\u8272\u6a21\u578b\uff08Proposer\u548cSolver\uff09\u7684\u81ea\u6211\u5bf9\u6297\u5b66\u4e60\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "motivation": "\u73b0\u6709LLM\u8bad\u7ec3\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\uff08\u5982\u4eba\u5de5\u6807\u6ce8\uff09\uff0c\u5bf9\u6297\u5b66\u4e60\u7279\u522b\u662f\u81ea\u535a\u5f08\u65b9\u6cd5\u53ef\u4ee5\u51cf\u5c11\u5bf9\u5916\u90e8\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u4f46\u53cc\u89d2\u8272\u5bf9\u6297\u8bad\u7ec3\u5728LLM\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u4e3b\u8981\u9762\u4e34\u5956\u52b1\u7834\u89e3\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faPasoDoble\u6846\u67b6\uff1a\u4ece\u540c\u4e00\u57fa\u7840\u6a21\u578b\u521d\u59cb\u5316\u4e24\u4e2a\u6a21\u578b\uff0cProposer\u751f\u6210\u5e26\u771f\u5b9e\u7b54\u6848\u7684\u6311\u6218\u6027\u95ee\u9898\uff0cSolver\u5c1d\u8bd5\u89e3\u51b3\uff1bProposer\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u786e\u4fdd\u95ee\u9898\u8d28\u91cf\uff1b\u901a\u8fc7\u8054\u5408\u66f4\u65b0\u907f\u514d\u5956\u52b1\u7834\u89e3\uff1b\u5f15\u5165\u53ef\u9009\u79bb\u7ebf\u8303\u5f0f\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePasoDoble\u80fd\u591f\u63d0\u5347LLM\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e14\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u65e0\u9700\u76d1\u7763\u3002", "conclusion": "PasoDoble\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u76d1\u7763\u7684LLM\u5bf9\u6297\u8bad\u7ec3\uff0c\u901a\u8fc7\u53cc\u89d2\u8272\u81ea\u535a\u5f08\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3LLM\u8bad\u7ec3\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.12263", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12263", "abs": "https://arxiv.org/abs/2511.12263", "authors": ["Jingyao Li", "Jingyun Wang", "Molin Tan", "Haochen Wang", "Cilin Yan", "Likun Shi", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu"], "title": "CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models", "comment": "30 pages, 28 figures", "summary": "Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.", "AI": {"tldr": "CrossVid\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u89c6\u9891\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u57fa\u51c6\uff0c\u5305\u542b4\u4e2a\u9ad8\u7ea7\u7ef4\u5ea6\u548c10\u4e2a\u5177\u4f53\u4efb\u52a1\uff0c\u63d0\u4f9b5,331\u4e2a\u89c6\u9891\u548c9,015\u4e2a\u95ee\u7b54\u5bf9\u3002\u5b9e\u9a8c\u663e\u793a\u5f53\u524dMLLMs\u5728\u8de8\u89c6\u9891\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u96be\u4ee5\u6574\u5408\u548c\u6bd4\u8f83\u591a\u4e2a\u89c6\u9891\u4e2d\u7684\u8bc1\u636e\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u89c6\u9891\u5206\u6790\uff0c\u65e0\u6cd5\u8bc4\u4f30MLLMs\u540c\u65f6\u63a8\u7406\u591a\u4e2a\u89c6\u9891\u7684\u80fd\u529b\u3002\u867d\u7136\u8fd1\u671f\u6709\u8bc4\u4f30\u591a\u89c6\u89d2\u89c6\u9891\u7684\u57fa\u51c6\uff0c\u4f46\u5176\u6709\u9650\u7684\u4efb\u52a1\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30MLLMs\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u8de8\u89c6\u9891\u63a8\u7406\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86CrossVid\u57fa\u51c6\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u5206\u5c42\u4efb\u52a1\uff0c\u5305\u62ec\u56db\u4e2a\u9ad8\u7ea7\u7ef4\u5ea6\u548c\u5341\u4e2a\u5177\u4f53\u4efb\u52a1\uff0c\u63d0\u4f9b5,331\u4e2a\u89c6\u9891\u548c9,015\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u5355\u9009\u3001\u591a\u9009\u548c\u5f00\u653e\u5f0f\u95ee\u9898\u683c\u5f0f\u3002", "result": "\u901a\u8fc7\u5bf9\u5404\u79cd\u5f00\u6e90\u548c\u95ed\u6e90MLLMs\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u53d1\u73b0Gemini-2.5-Pro\u5728CrossVid\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u51c6\u786e\u7387\u4e3a50.4%\u3002\u6df1\u5165\u7684\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u5927\u591a\u6570\u5f53\u524dMLLMs\u5728\u8de8\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u6323\u624e\u3002", "conclusion": "CrossVid\u57fa\u51c6\u6709\u6f5c\u529b\u6307\u5bfc\u672a\u6765\u589e\u5f3aMLLMs\u8de8\u89c6\u9891\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u5f53\u524dMLLMs\u7684\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u65e0\u6cd5\u6574\u5408\u6216\u6bd4\u8f83\u5206\u5e03\u5728\u591a\u4e2a\u89c6\u9891\u4e2d\u7684\u8bc1\u636e\u8fdb\u884c\u63a8\u7406\u3002"}}
{"id": "2511.11891", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11891", "abs": "https://arxiv.org/abs/2511.11891", "authors": ["Nawid Keshtmand", "Roussel Desmond Nzoyem", "Jeffrey Nicholas Clark"], "title": "FLEX: Feature Importance from Layered Counterfactual Explanations", "comment": "12 pages, 6 figures, 3 tables, 2 algorithms. Preprint under review", "summary": "Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable \"what-if\" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.", "AI": {"tldr": "FLEX\u662f\u4e00\u4e2a\u6a21\u578b\u548c\u9886\u57df\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5c06\u53cd\u4e8b\u5b9e\u89e3\u91ca\u8f6c\u6362\u4e3a\u5c40\u90e8\u3001\u533a\u57df\u548c\u5168\u5c40\u5c42\u9762\u7684\u7279\u5f81\u53d8\u5316\u9891\u7387\u8bc4\u5206\uff0c\u5f25\u5408\u4e86\u5c40\u90e8\u8865\u6551\u548c\u5168\u5c40\u5f52\u56e0\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\uff0c\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u901a\u5e38\u505c\u7559\u5728\u5b9e\u4f8b\u5c42\u9762\uff0c\u65e0\u6cd5\u91cf\u5316\u54ea\u4e9b\u7279\u5f81\u5728\u7279\u5f81\u7a7a\u95f4\u7684\u8fde\u8d2f\u533a\u57df\u6216\u6574\u4e2a\u6570\u636e\u96c6\u4e2d\u7cfb\u7edf\u5730\u9a71\u52a8\u7ed3\u679c\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u805a\u5408\u8de8\u5b9e\u4f8b\u548c\u90bb\u57df\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5c06\u7279\u5f81\u53d8\u5316\u9891\u7387\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u6392\u540d\uff0c\u53cd\u6620\u6bcf\u4e2a\u7279\u5f81\u9700\u8981\u6539\u53d8\u591a\u5c11\u6b21\u624d\u80fd\u7ffb\u8f6c\u9884\u6d4b\uff0c\u517c\u5bb9\u4e0d\u540c\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u6027\u9884\u6d4b\u548c\u8d37\u6b3e\u5ba1\u6279\u4efb\u52a1\u4e2d\uff0cFLEX\u7684\u5168\u5c40\u6392\u540d\u4e0eSHAP\u76f8\u5173\u4f46\u63ed\u793a\u4e86\u989d\u5916\u9a71\u52a8\u56e0\u7d20\uff0c\u533a\u57df\u5206\u6790\u63ed\u793a\u4e86\u5168\u5c40\u6458\u8981\u9057\u6f0f\u7684\u4e0a\u4e0b\u6587\u7279\u5b9a\u56e0\u7d20\u3002", "conclusion": "FLEX\u5f25\u5408\u4e86\u5c40\u90e8\u8865\u6551\u548c\u5168\u5c40\u5f52\u56e0\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u652f\u6301\u98ce\u9669\u654f\u611f\u5e94\u7528\u4e2d\u7684\u900f\u660e\u548c\u5e72\u9884\u5bfc\u5411\u51b3\u7b56\u3002"}}
{"id": "2511.12267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12267", "abs": "https://arxiv.org/abs/2511.12267", "authors": ["Ruixun Liu", "Bowen Fu", "Jiayi Song", "Kaiyu Li", "Wanchen Li", "Lanxuan Xue", "Hui Qiao", "Weizhan Zhang", "Deyu Meng", "Xiangyong Cao"], "title": "ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks", "comment": null, "summary": "Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u52a8\u611f\u77e5\u8303\u5f0fZoomEarth\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u88c1\u526a\u7f29\u653e\u6846\u67b6\u5904\u7406\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\uff0c\u5728LRS-GRO\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u4e0b\u6e38\u4efb\u52a1\u4e2d\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u5206\u8fa8\u7387\u548c\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u53d7\u9650\u4e8e\u88ab\u52a8\u611f\u77e5\u8303\u5f0f\uff0c\u5728\u5904\u7406\u66f4\u7cbe\u7ec6\u89c6\u89c9\u8f93\u5165\u65f6\u4f1a\u4ea7\u751f\u5197\u4f59\u3002\u672c\u6587\u63a2\u7d22\u65b0\u7684\u4e3b\u52a8\u611f\u77e5\u8303\u5f0f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u91cd\u65b0\u8bbf\u95ee\u4fe1\u606f\u4e30\u5bcc\u533a\u57df\u3002", "method": "\u63d0\u51faZoomEarth\u81ea\u9002\u5e94\u88c1\u526a\u7f29\u653e\u6846\u67b6\uff0c\u91c7\u7528\u533a\u57df\u5f15\u5bfc\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728LRS-GRO\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5728\u4e09\u4e2a\u516c\u5171\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ZoomEarth\u6846\u67b6\u5177\u6709\u5f3a\u5927\u7684\u591a\u529f\u80fd\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4e91\u53bb\u9664\u3001\u53bb\u566a\u3001\u5206\u5272\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2511.11894", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11894", "abs": "https://arxiv.org/abs/2511.11894", "authors": ["Lingxiao Li", "Haobo Zhang", "Bin Chen", "Jiayu Zhou"], "title": "Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design", "comment": "22 pages, 7 figures, 10 tables", "summary": "Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faChain-of-Generation (CoG)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6f5c\u5728\u6269\u6563\u65b9\u6cd5\u89e3\u51b3\u6587\u672c\u6761\u4ef6\u5206\u5b50\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u76f8\u6bd4\u4e00\u6b21\u6027\u6761\u4ef6\u751f\u6210\u80fd\u66f4\u597d\u5730\u6ee1\u8db3\u590d\u6742\u63d0\u793a\u8981\u6c42\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u6761\u4ef6\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u91c7\u7528\u4e00\u6b21\u6027\u6761\u4ef6\u7f16\u7801\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u63d0\u793a\u4e2d\u7684\u6240\u6709\u8981\u6c42\uff0c\u5b58\u5728\u751f\u6210\u7ec4\u4ef6\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u65e0\u6cd5\u751f\u6210\u6240\u6709\u5b50\u7ed3\u6784\u3001\u540c\u65f6\u8003\u8651\u6240\u6709\u8981\u6c42\u8fc7\u4e8e\u6fc0\u8fdb\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faCoG\u6846\u67b6\uff1a\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u8bfe\u7a0b\u987a\u5e8f\u7684\u8bed\u4e49\u7247\u6bb5\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u9010\u6b65\u5f15\u5165\u4f5c\u4e3a\u4e2d\u95f4\u76ee\u6807\uff1b\u589e\u52a0\u540e\u5bf9\u9f50\u5b66\u4e60\u9636\u6bb5\u52a0\u5f3a\u6587\u672c\u4e0e\u5206\u5b50\u6f5c\u5728\u7a7a\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5728\u57fa\u51c6\u548c\u771f\u5b9e\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoG\u76f8\u6bd4\u4e00\u6b21\u6027\u57fa\u7ebf\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u3001\u591a\u6837\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u66f4\u5fe0\u5b9e\u5730\u53cd\u6620\u590d\u6742\u7ec4\u5408\u63d0\u793a\u3002", "conclusion": "CoG\u901a\u8fc7\u591a\u9636\u6bb5\u6e10\u8fdb\u5f0f\u751f\u6210\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u6761\u4ef6\u5206\u5b50\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u3001\u53ef\u63a7\u7684\u751f\u6210\u8fc7\u7a0b\u3002"}}
{"id": "2511.12270", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12270", "abs": "https://arxiv.org/abs/2511.12270", "authors": ["Yaxuan Jiao", "Qing Xu", "Yuxiang Luo", "Xiangjian He", "Zhen Chen", "Wenting Duan"], "title": "TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.", "AI": {"tldr": "TM-UNet\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u4ee4\u724c-\u5185\u5b58\u5757\u5c062D\u7a7a\u95f4\u7279\u5f81\u8f6c\u6362\u4e3a\u4ee4\u724c\u5e8f\u5217\uff0c\u5229\u7528\u77e9\u9635\u5185\u5b58\u5355\u5143\u9009\u62e9\u6027\u4fdd\u7559\u548c\u4f20\u64ad\u5224\u522b\u6027\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u5168\u5c40\u63a8\u7406\u3002", "motivation": "\u57fa\u4e8etransformer\u7684\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u963b\u788d\u4e86\u4e34\u5e8a\u90e8\u7f72\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u7684\u9ad8\u6548\u5206\u5272\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u4ee4\u724c-\u5185\u5b58\u5757\uff0c\u901a\u8fc7\u7a7a\u95f4\u626b\u63cf\u5c062D\u7279\u5f81\u8f6c\u6362\u4e3a\u4ee4\u724c\u5e8f\u5217\uff0c\u5229\u7528\u77e9\u9635\u5185\u5b58\u5355\u5143\u9009\u62e9\u6027\u4fdd\u7559\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u91c7\u7528\u6307\u6570\u95e8\u63a7\u8bc6\u522b\u4ee4\u724c\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u6c60\u5316\u64cd\u4f5c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u63d0\u53d6\u3002", "result": "TM-UNet\u5728\u591a\u79cd\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "TM-UNet\u901a\u8fc7\u521b\u65b0\u7684\u4ee4\u724c-\u5185\u5b58\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u4e3a\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12280", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12280", "abs": "https://arxiv.org/abs/2511.12280", "authors": ["Shuochen Chang", "Xiaofeng Zhang", "Qingyang Liu", "Li Niu"], "title": "D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs", "comment": "Accepted by AAAI Conference on Artificial Intelligence (AAAI) 2026. Code available at https://github.com/bcmi/D3ToM-Diffusion-MLLM", "summary": "Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.", "AI": {"tldr": "D\u00b3ToM\u662f\u4e00\u79cd\u7528\u4e8e\u52a0\u901f\u6269\u6563\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5408\u5e76\u5197\u4f59\u89c6\u89c9\u4ee4\u724c\u6765\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u6269\u6563\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u975e\u81ea\u56de\u5f52\u751f\u6210\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u90fd\u9700\u8981\u5bf9\u6574\u4e2a\u5e8f\u5217\u8fdb\u884c\u53cc\u5411\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5bfc\u81f4\u7acb\u65b9\u7ea7\u7684\u89e3\u7801\u590d\u6742\u5ea6\uff0c\u63a8\u7406\u901f\u5ea6\u8fdc\u6162\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u3002", "method": "\u63d0\u51faD\u00b3ToM\u65b9\u6cd5\uff0c\u4f7f\u7528\u524d\u4e00\u6b65\u751f\u6210\u7684\u51b3\u7b56\u4ee4\u724c\u6784\u5efa\u91cd\u8981\u6027\u6620\u5c04\uff0c\u4fdd\u7559\u6700\u663e\u8457\u7684\u89c6\u89c9\u4ee4\u724c\u5e76\u901a\u8fc7\u76f8\u4f3c\u6027\u805a\u5408\u5408\u5e76\u5197\u4f59\u4ee4\u724c\uff0c\u52a8\u6001\u8c03\u6574\u5408\u5e76\u6bd4\u4f8b\u4ee5\u9002\u5e94\u4e0d\u540c\u53bb\u566a\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eD\u00b3ToM\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u4e86\u63a8\u7406\u8fc7\u7a0b\uff0c\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u96c6\u6210\u5230\u5355\u4e2aTransformer\u5c42\u4e2d\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u53c2\u6570\u3002", "conclusion": "D\u00b3ToM\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u4ee4\u724c\u5408\u5e76\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2511.11912", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11912", "abs": "https://arxiv.org/abs/2511.11912", "authors": ["Haoyan Xu", "Ruizhi Qian", "Jiate Li", "Yushun Dong", "Minghao Lin", "Hanson Yan", "Zhengtao Yao", "Qinghua Liu", "Junhao Dong", "Ruopeng Huang", "Yue Zhao", "Mengyuan Li"], "title": "A Systematic Study of Model Extraction Attacks on Graph Foundation Models", "comment": null, "summary": "Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u9488\u5bf9\u56fe\u57fa\u7840\u6a21\u578b(GFMs)\u7684\u6a21\u578b\u63d0\u53d6\u653b\u51fb(MEAs)\uff0c\u63ed\u793a\u4e86GFMs\u663e\u8457\u6269\u5927\u4e86MEA\u653b\u51fb\u9762\uff0c\u653b\u51fb\u8005\u4ec5\u9700\u6781\u5c0f\u6210\u672c\u5373\u53ef\u8fd1\u4f3c\u53d7\u5bb3\u8005\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u56fe\u57fa\u7840\u6a21\u578b(GFMs)\u7684\u53d1\u5c55\uff0c\u8fd9\u4e9b\u6a21\u578b\u56e0\u5305\u542b\u5927\u91cf\u8ba1\u7b97\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u800c\u6210\u4e3a\u6709\u4ef7\u503c\u7684\u77e5\u8bc6\u8d44\u4ea7\uff0c\u4f46\u5176\u9ad8\u9884\u8bad\u7ec3\u6210\u672c\u548c\u8de8\u9886\u57df\u77e5\u8bc6\u4e5f\u4f7f\u5176\u6210\u4e3a\u6a21\u578b\u63d0\u53d6\u653b\u51fb\u7684\u8bf1\u4eba\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u56de\u5f52\u56fe\u5d4c\u5165\u6765\u8bad\u7ec3\u653b\u51fb\u8005\u7f16\u7801\u5668\uff0c\u5373\u4f7f\u6ca1\u6709\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6570\u636e\u4e5f\u80fd\u4fdd\u6301\u4e0e\u53d7\u5bb3\u8005\u6587\u672c\u7f16\u7801\u5668\u7684\u5bf9\u9f50\u5e76\u4fdd\u7559\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u653b\u51fb\u8005\u4ec5\u9700\u539f\u59cb\u8bad\u7ec3\u6210\u672c\u7684\u6781\u5c0f\u90e8\u5206\u5373\u53ef\u8fd1\u4f3c\u53d7\u5bb3\u8005\u6a21\u578b\uff0c\u51c6\u786e\u7387\u51e0\u4e4e\u6ca1\u6709\u635f\u5931\u3002", "conclusion": "GFMs\u6781\u5927\u5730\u6269\u5c55\u4e86MEA\u653b\u51fb\u9762\uff0c\u51f8\u663e\u4e86\u5728\u5927\u89c4\u6a21\u56fe\u5b66\u4e60\u7cfb\u7edf\u4e2d\u90e8\u7f72\u611f\u77e5\u5b89\u5168\u9632\u5fa1\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.12291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12291", "abs": "https://arxiv.org/abs/2511.12291", "authors": ["Andrea Bertogalli", "Giacomo Boracchi", "Luca Magri"], "title": "One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving", "comment": null, "summary": "We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5916\u53c2\u6807\u5b9a\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u4f30\u8ba1\u4e8b\u4ef6\u76f8\u673a\u3001LiDAR\u548cRGB\u76f8\u673a\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u59ff\uff0c\u7279\u522b\u5173\u6ce8\u5177\u6709\u6311\u6218\u6027\u7684\u4e8b\u4ef6\u76f8\u673a\u6807\u5b9a\u95ee\u9898\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u590d\u6742\u89c6\u89c9\u7cfb\u7edf\u4e2d\uff0c\u7cbe\u786e\u7684\u591a\u4f20\u611f\u5668\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5206\u79bb\u7684\u6210\u5bf9\u6807\u5b9a\uff0c\u800c\u672c\u6587\u65e8\u5728\u5b9e\u73b0\u4e00\u6b21\u6027\u8054\u5408\u5916\u53c2\u6807\u5b9a\u3002", "method": "\u8bbe\u8ba1\u5e76\u6784\u5efa\u4e86\u65b0\u578b3D\u6807\u5b9a\u9776\u6807\uff0c\u5305\u542b\u5e73\u9762\u7279\u5f81\u3001ChArUco\u56fe\u6848\u548c\u4e3b\u52a8LED\u6a21\u5f0f\uff0c\u5206\u522b\u9488\u5bf9LiDAR\u3001RGB\u76f8\u673a\u548c\u4e8b\u4ef6\u76f8\u673a\u7684\u72ec\u7279\u7279\u6027\u3002\u901a\u8fc7\u8fd9\u4e00\u9776\u6807\u5b9e\u73b0\u4e00\u6b21\u6027\u8054\u5408\u6807\u5b9a\u6d41\u7a0b\u3002", "result": "\u5728\u5b9a\u5236\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u4f7f\u7528\u5148\u8fdb\u7684\u81ea\u52a8\u9a7e\u9a76\u4f20\u611f\u5668\u8bbe\u7f6e\u8fdb\u884c\u8bb0\u5f55\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u6807\u5b9a\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u590d\u6742\u89c6\u89c9\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u4f20\u611f\u5668\u8054\u5408\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12301", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12301", "abs": "https://arxiv.org/abs/2511.12301", "authors": ["Chi Liu", "Jincheng Liu", "Congcong Zhu", "Minghao Wang", "Sheng Shen", "Jia Gu", "Tianqing Zhu", "Wanlei Zhou"], "title": "Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method", "comment": "Accepted for AAAI 2026 (Main Track Poster)", "summary": "Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9891\u7387\u91cd\u6821\u51c6\u65b9\u6cd5\u6765\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u751f\u6210\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u9891\u7387\u5931\u51c6\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u8ba1\u9ad8\u9891\u66ff\u6362\u548c\u91cd\u5efa\u9ad8\u9891\u6620\u5c04\u6765\u6539\u5584\u5408\u6210\u56fe\u50cf\u8d28\u91cf\uff0c\u63d0\u5347\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u533b\u5b66AI\u5f00\u53d1\u4f9d\u8d56\u5927\u6570\u636e\u96c6\u4f46\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u751f\u6210\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u9891\u7387\u5931\u51c6\u98ce\u9669\uff0c\u53ef\u80fd\u5f15\u5165\u6709\u5bb3\u7279\u5f81\u635f\u5bb3\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u9891\u7387\u91cd\u6821\u51c6\u65b9\u6cd5\uff0c\u5305\u542b\u7edf\u8ba1\u9ad8\u9891\u66ff\u6362\u6765\u7c97\u7565\u5bf9\u9f50\u9ad8\u9891\u6210\u5206\uff0c\u4ee5\u53ca\u91cd\u5efa\u9ad8\u9891\u6620\u5c04\u6765\u589e\u5f3a\u56fe\u50cf\u8d28\u91cf\u5e76\u91cd\u5efa\u9ad8\u9891\u7ec6\u8282\u3002", "result": "\u5728\u8111\u90e8MRI\u3001\u80f8\u90e8X\u5149\u3001\u773c\u5e95\u56fe\u50cf\u7b49\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u9891\u7387\u91cd\u6821\u51c6\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u517c\u5bb9\u4efb\u4f55\u751f\u6210\u6a21\u578b\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5e38\u89c1\u533b\u5b66\u751f\u6210\u6570\u636e\u589e\u5f3a\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2511.12304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12304", "abs": "https://arxiv.org/abs/2511.12304", "authors": ["Qifeng Chen", "Jiarun Liu", "Rengan Xie", "Tao Tang", "Sicong Du", "Yiru Zhao", "Yuchi Huo", "Sheng Yang"], "title": "LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors", "comment": "Accepted by AAAI-26", "summary": "Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.", "AI": {"tldr": "LiDAR-GS++ \u662f\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684LiDAR\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u5148\u9a8c\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u5355\u6b21\u626b\u63cf\u91cd\u5efa\u4e0d\u5b8c\u6574\u5bfc\u81f4\u7684\u89c6\u70b9\u5916\u63a8\u4f2a\u5f71\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u9ad8\u4fdd\u771f\u91cd\u6a21\u62df\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684LiDAR\u6e32\u67d3\u65b9\u6cd5\u5728\u89c6\u70b9\u5916\u63a8\u65f6\u4f1a\u51fa\u73b0\u4f2a\u5f71\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5355\u6b21\u904d\u5386\u626b\u63cf\u7684\u91cd\u5efa\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51fa\u53ef\u63a7\u7684LiDAR\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u7c97\u7565\u5916\u63a8\u6e32\u67d3\u751f\u6210\u989d\u5916\u7684\u51e0\u4f55\u4e00\u81f4\u626b\u63cf\uff0c\u5e76\u91c7\u7528\u6709\u6548\u7684\u84b8\u998f\u673a\u5236\u8fdb\u884c\u6269\u5c55\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLiDAR-GS++\u5728\u63d2\u503c\u548c\u5916\u63a8\u89c6\u70b9\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684GS\u548cNeRF\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u91cd\u5efa\u6269\u5c55\u5230\u6b20\u62df\u5408\u533a\u57df\uff0c\u8be5\u65b9\u6cd5\u786e\u4fdd\u4e86\u5916\u63a8\u65b0\u89c6\u70b9\u7684\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4f20\u611f\u5668\u6355\u83b7\u7684\u8be6\u7ec6\u573a\u666f\u8868\u9762\u3002"}}
{"id": "2511.11934", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11934", "abs": "https://arxiv.org/abs/2511.11934", "authors": ["C. C\u00e9sar Claros Olivares", "Austin J. Brockmeier"], "title": "A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts", "comment": null, "summary": "We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e0d\u540cOOD\u68c0\u6d4b\u65b9\u6cd5\u5728CLIP\u5206\u5c42\u673a\u5236\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7279\u5f81\u7a7a\u95f4\u5bf9OOD\u68c0\u6d4b\u6548\u679c\u8d77\u51b3\u5b9a\u6027\u4f5c\u7528\u3002\u6982\u7387\u5f97\u5206\u65b9\u6cd5\u5728\u8bef\u5206\u7c7b\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u51e0\u4f55\u611f\u77e5\u65b9\u6cd5\u5728CNN\u4e0a\u5bf9\u5f3a\u5206\u5e03\u504f\u79fb\u66f4\u6709\u6548\uff0cViT\u4e0aGradNorm\u548cKPCA\u91cd\u5efa\u8bef\u5dee\u8868\u73b0\u7a33\u5b9a\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9OOD\u68c0\u6d4b\u65b9\u6cd5\u5728\u4e0d\u540c\u8868\u793a\u8303\u5f0f\u4e0b\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u9700\u8981\u4e3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u7edf\u8ba1\u4f9d\u636e\u3002", "method": "\u4f7f\u7528AURC\u548cAUGRC\u4f5c\u4e3a\u4e3b\u8981\u6307\u6807\uff0c\u6bd4\u8f83CNN\u4ece\u5934\u8bad\u7ec3\u548cViT\u5fae\u8c03\u4e24\u79cd\u8868\u793a\u8303\u5f0f\uff0c\u91c7\u7528\u591a\u91cd\u6bd4\u8f83\u63a7\u5236\u7684\u57fa\u4e8e\u6392\u540d\u7684\u7edf\u8ba1\u6d41\u7a0b\uff08Friedman\u68c0\u9a8c\u548cConover-Holm\u4e8b\u540e\u68c0\u9a8c\uff09\u4ee5\u53caBron-Kerbosch\u56e2\u5206\u6790\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7279\u5f81\u7a7a\u95f4\u5f88\u5927\u7a0b\u5ea6\u4e0a\u51b3\u5b9a\u4e86OOD\u68c0\u6d4b\u6548\u679c\u3002\u6982\u7387\u5f97\u5206\u65b9\u6cd5\u5728\u8bef\u5206\u7c7b\u68c0\u6d4b\u4e2d\u5360\u4e3b\u5bfc\uff0c\u51e0\u4f55\u611f\u77e5\u65b9\u6cd5\u5728CNN\u4e0a\u5bf9\u5f3a\u504f\u79fb\u66f4\u6709\u6548\uff0cViT\u4e0aGradNorm\u548cKPCA\u91cd\u5efa\u8bef\u5dee\u8868\u73b0\u7a33\u5b9a\u3002MCD\u5b58\u5728\u7c7b\u522b\u6570\u91cf\u4f9d\u8d56\u7684\u6743\u8861\uff0c\u7b80\u5355PCA\u6295\u5f71\u53ef\u6539\u8fdb\u591a\u4e2a\u68c0\u6d4b\u5668\u3002", "conclusion": "\u7ed3\u679c\u652f\u6301\u4ee5\u8868\u793a\u4e3a\u4e2d\u5fc3\u7684OOD\u68c0\u6d4b\u89c2\u70b9\uff0c\u5e76\u4e3a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u7edf\u8ba1\u57fa\u7840\u6307\u5bfc\u3002"}}
{"id": "2511.12321", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12321", "abs": "https://arxiv.org/abs/2511.12321", "authors": ["Xi Ding", "Lei Wang", "Piotr Koniusz", "Yongsheng Gao"], "title": "Learning Time in Static Classifiers", "comment": "Accepted at the Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u4e3a\u524d\u9988\u5206\u7c7b\u5668\u6dfb\u52a0\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u6216\u5f15\u5165\u5faa\u73af\u6a21\u5757\u3002\u901a\u8fc7SEQ\u5b66\u4e60\u8303\u5f0f\u6784\u5efa\u65f6\u95f4\u8fde\u8d2f\u8f68\u8ff9\uff0c\u5b66\u4e60\u7c7b\u522b\u7279\u5b9a\u65f6\u95f4\u539f\u578b\uff0c\u5e76\u4f7f\u7528\u53ef\u5fae\u5206soft-DTW\u635f\u5931\u5bf9\u9f50\u9884\u6d4b\u5e8f\u5217\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u89c6\u89c9\u6570\u636e\u901a\u5e38\u968f\u65f6\u95f4\u9010\u6e10\u6f14\u53d8\uff0c\u4f46\u4f20\u7edf\u5206\u7c7b\u5668\u57fa\u4e8e\u65f6\u95f4\u72ec\u7acb\u6027\u5047\u8bbe\u8bad\u7ec3\uff0c\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u52a8\u6001\u53d8\u5316\u3002\u9700\u8981\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u652f\u6301-\u8303\u4f8b-\u67e5\u8be2(SEQ)\u5b66\u4e60\u8303\u5f0f\uff0c\u5c06\u8bad\u7ec3\u6570\u636e\u7ec4\u7ec7\u6210\u65f6\u95f4\u8fde\u8d2f\u8f68\u8ff9\u3002\u5b66\u4e60\u7c7b\u522b\u7279\u5b9a\u65f6\u95f4\u539f\u578b\uff0c\u4f7f\u7528\u53ef\u5fae\u5206soft-DTW\u635f\u5931\u5bf9\u9f50\u9884\u6d4b\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u51fd\u6570\u4fc3\u8fdb\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u5e73\u6ed1\u6027\u3002", "result": "\u5728\u9759\u6001\u548c\u65f6\u5e8f\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff1a\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u548c\u8d85\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u5728\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u4e2d\u63d0\u4f9b\u7cbe\u786e\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u635f\u5931\u8bbe\u8ba1\u5f15\u5165\u5f3a\u65f6\u95f4\u5f52\u7eb3\u504f\u7f6e\uff0c\u4ee5\u6a21\u5757\u5316\u548c\u6570\u636e\u9ad8\u6548\u7684\u65b9\u5f0f\u6865\u63a5\u9759\u6001\u548c\u65f6\u5e8f\u5b66\u4e60\uff0c\u4ec5\u9700\u5728\u9884\u63d0\u53d6\u7279\u5f81\u4e0a\u4f7f\u7528\u7b80\u5355\u5206\u7c7b\u5668\u3002"}}
{"id": "2511.11935", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11935", "abs": "https://arxiv.org/abs/2511.11935", "authors": ["Munib Mesinovic", "Tingting Zhu"], "title": "SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis", "comment": null, "summary": "Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the \"preprocessing gap\" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.", "AI": {"tldr": "SurvBench\u662f\u4e00\u4e2a\u5f00\u6e90\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5c06\u539f\u59cbPhysioNet\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u7684\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u5f20\u91cf\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u6a21\u578b\u9884\u5904\u7406\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u4e3a\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u5206\u6790\u63d0\u4f9b\u4e86\u5de8\u5927\u673a\u4f1a\uff0c\u4f46\u7531\u4e8e\u9884\u5904\u7406\u65b9\u6cd5\u4e0d\u4e00\u81f4\uff0c\u53ef\u91cd\u590d\u6027\u53d7\u5230\u4e25\u91cd\u9650\u5236\u3002", "method": "\u63d0\u4f9b\u4e09\u4e2a\u91cd\u75c7\u76d1\u62a4\u6570\u636e\u5e93\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u652f\u6301\u65f6\u95f4\u5e8f\u5217\u751f\u547d\u4f53\u5f81\u3001\u9759\u6001\u4eba\u53e3\u7edf\u8ba1\u3001ICD\u8bca\u65ad\u4ee3\u7801\u548c\u653e\u5c04\u5b66\u62a5\u544a\u7b49\u591a\u79cd\u6a21\u6001\uff0c\u5b9e\u65bd\u4e25\u683c\u7684\u6570\u636e\u8d28\u91cf\u63a7\u5236\u3001\u60a3\u8005\u7ea7\u5206\u5272\u3001\u663e\u5f0f\u7f3a\u5931\u503c\u8ddf\u8e2a\u548c\u6807\u51c6\u5316\u65f6\u95f4\u805a\u5408\u3002", "result": "SurvBench\u5904\u7406\u5355\u98ce\u9669\u548c\u7ade\u4e89\u98ce\u9669\u573a\u666f\uff0c\u8f93\u51fa\u4e0epycox\u5e93\u517c\u5bb9\uff0c\u652f\u6301\u6807\u51c6\u7edf\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u53ef\u91cd\u590d\u7684\u3001\u914d\u7f6e\u9a71\u52a8\u7684\u9884\u5904\u7406\u548c\u5168\u9762\u6587\u6863\uff0cSurvBench\u89e3\u51b3\u4e86\u963b\u788d\u6df1\u5ea6\u5b66\u4e60\u751f\u5b58\u6a21\u578b\u516c\u5e73\u6bd4\u8f83\u7684\"\u9884\u5904\u7406\u5dee\u8ddd\"\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u4e13\u6ce8\u4e8e\u65b9\u6cd5\u521b\u65b0\u800c\u975e\u6570\u636e\u5de5\u7a0b\u3002"}}
{"id": "2511.12331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12331", "abs": "https://arxiv.org/abs/2511.12331", "authors": ["Sepehr Kazemi Ranjbar", "Kumail Alhamoud", "Marzyeh Ghassemi"], "title": "SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) struggle with negation. Given a prompt like \"retrieve (or generate) a street scene without pedestrians,\" they often fail to respect the \"not.\" Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as \"A but not N,\" we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5c06\u5426\u5b9a\u5efa\u6a21\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5b50\u7a7a\u95f4\u800c\u975e\u5355\u70b9\uff0c\u901a\u8fc7\u6784\u5efa\u7403\u5f62\u533a\u57df\u6765\u540c\u65f6\u63a5\u8fd1\u80af\u5b9a\u6982\u5ff5\u548c\u8fdc\u79bb\u5426\u5b9a\u6982\u5ff5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u5426\u5b9a\u6982\u5ff5\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u901a\u8fc7\u5fae\u8c03\u5927\u578b\u5426\u5b9a\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u5f80\u5f80\u4f1a\u635f\u5bb3\u6a21\u578b\u5728\u80af\u5b9a\u63d0\u793a\u4e0a\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "method": "\u57fa\u4e8eCLIP\u7b49\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\u53ef\u5212\u5206\u4e3a\u8bed\u4e49\u4e00\u81f4\u5b50\u7a7a\u95f4\u7684\u7279\u6027\uff0c\u6784\u5efa\u4e24\u4e2a\u7403\u5f62\u533a\u57df\u5206\u522b\u56f4\u7ed5\u80af\u5b9a\u6982\u5ff5A\u548c\u5426\u5b9a\u6982\u5ff5N\u7684\u5d4c\u5165\uff0c\u901a\u8fc7\u5bfb\u627e\u540c\u65f6\u63a5\u8fd1A\u4e14\u8fdc\u79bbN\u7684\u533a\u57df\u4e2d\u5fc3\u65b9\u5411\u6765\u5339\u914d\u56fe\u50cf\u3002", "result": "\u5728\u68c0\u7d22\u3001\u591a\u9879\u9009\u62e9\u548c\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c06\u5426\u5b9a\u7406\u89e3\u80fd\u529b\u5e73\u5747\u63d0\u5347\u4e86\u7ea630%\uff0c\u7f29\u5c0f\u4e86\u80af\u5b9a\u63d0\u793a\u548c\u5426\u5b9a\u63d0\u793a\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u6709\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5426\u5b9a\u7406\u89e3\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u96f6\u6837\u672c\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u5426\u5b9a\u5904\u7406\u6548\u679c\u3002"}}
{"id": "2511.11940", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.11940", "abs": "https://arxiv.org/abs/2511.11940", "authors": ["Christopher Sandino", "Sayeri Lala", "Geeling Chau", "Melika Ayoughi", "Behrooz Mahasseni", "Ellen Zippi", "Ali Moin", "Erdrin Azemi", "Hanlin Goh"], "title": "Learning the relative composition of EEG signals using pairwise relative shift pretraining", "comment": "Foundation Models for the Brain and Body NeurIPS 2025 Workshop", "summary": "Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPARS\u7684\u65b0\u578b\u81ea\u76d1\u7763\u5b66\u4e60\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u968f\u673a\u91c7\u6837\u7684EEG\u7a97\u53e3\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u65f6\u95f4\u504f\u79fb\u6765\u5b66\u4e60\u8111\u7535\u56fe\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u591a\u79cdEEG\u89e3\u7801\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u5f53\u524dEEG\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u63a9\u7801\u91cd\u5efa\u7b56\u7565\uff08\u5982MAE\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u5c40\u90e8\u65f6\u95f4\u6a21\u5f0f\uff0c\u800c\u80fd\u591f\u5b66\u4e60\u795e\u7ecf\u4fe1\u53f7\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u7684\u4f4d\u7f6e\u9884\u6d4b\u9884\u8bad\u7ec3\u65b9\u6cd5\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165PARS\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u524d\u7f6e\u4efb\u52a1\uff0c\u901a\u8fc7\u9884\u6d4b\u968f\u673a\u91c7\u6837\u7684EEG\u7a97\u53e3\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u65f6\u95f4\u504f\u79fb\uff0c\u9f13\u52b1\u7f16\u7801\u5668\u6355\u6349\u795e\u7ecf\u4fe1\u53f7\u4e2d\u56fa\u6709\u7684\u76f8\u5bf9\u65f6\u95f4\u7ec4\u6210\u548c\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u591a\u79cdEEG\u89e3\u7801\u4efb\u52a1\u7684\u7efc\u5408\u8bc4\u4f30\u4e2d\uff0cPARS\u9884\u8bad\u7ec3\u7684transformers\u5728\u6807\u7b7e\u9ad8\u6548\u548c\u8fc1\u79fb\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "PARS\u4e3a\u81ea\u76d1\u7763EEG\u8868\u793a\u5b66\u4e60\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u795e\u7ecf\u4fe1\u53f7\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u3002"}}
{"id": "2511.12342", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12342", "abs": "https://arxiv.org/abs/2511.12342", "authors": ["Sajjad Pakdamansavoji", "Kumar Vaibhav Jha", "Baher Abdulhai", "James H Elder"], "title": "Ground Plane Projection for Improved Traffic Analytics at Intersections", "comment": null, "summary": "Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u5c06\u57fa\u7840\u8bbe\u65bd\u6444\u50cf\u5934\u68c0\u6d4b\u5230\u7684\u8f66\u8f86\u53cd\u6295\u5f71\u5230\u5730\u9762\u5e73\u9762\u8fdb\u884c3D\u5750\u6807\u5206\u6790\uff0c\u4ee5\u63d0\u9ad8\u4ea4\u53c9\u53e3\u8f6c\u5411\u8fd0\u52a8\u8ba1\u6570\u7684\u51c6\u786e\u6027\u3002\u7814\u7a76\u53d1\u73b0\u5355\u6444\u50cf\u5934\u7cfb\u7edf\u7684\u53cd\u6295\u5f71\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8f68\u8ff9\u5206\u7c7b\u548c\u8f6c\u5411\u8ba1\u6570\uff0c\u800c\u591a\u6444\u50cf\u5934\u7684\u5f31\u878d\u5408\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4ea4\u53c9\u53e3\u7684\u51c6\u786e\u8f6c\u5411\u8fd0\u52a8\u8ba1\u6570\u5bf9\u4e8e\u4fe1\u53f7\u63a7\u5236\u3001\u4ea4\u901a\u7ba1\u7406\u548c\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u57fa\u4e8e\u56fe\u50cf\u5e73\u9762\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u63a2\u7d22\u5728\u5730\u9762\u5e73\u9762\u8fdb\u884c3D\u5750\u6807\u5206\u6790\u7684\u4f18\u52bf\u3002", "method": "\u5c06\u57fa\u7840\u8bbe\u65bd\u6444\u50cf\u5934\u68c0\u6d4b\u5230\u7684\u8f66\u8f86\u53cd\u6295\u5f71\u5230\u5730\u9762\u5e73\u9762\uff0c\u5728\u771f\u5b9e\u4e16\u754c3D\u5750\u6807\u4e2d\u8fdb\u884c\u5206\u6790\u3002\u6bd4\u8f83\u5355\u6444\u50cf\u5934\u7cfb\u7edf\u548c\u591a\u6444\u50cf\u5934\u5f31\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u5355\u6444\u50cf\u5934\u7cfb\u7edf\u7684\u53cd\u6295\u5f71\u80fd\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u8f68\u8ff9\u5206\u7c7b\u548c\u8f6c\u5411\u8fd0\u52a8\u8ba1\u6570\u3002\u591a\u6444\u50cf\u5934\u7684\u5f31\u878d\u5408\u65b9\u6cd5\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8ba1\u6570\u7cbe\u5ea6\u3002", "conclusion": "\u4ea4\u901a\u5206\u6790\u5e94\u8be5\u5728\u5730\u9762\u5e73\u9762\u8fdb\u884c\uff0c\u800c\u4e0d\u662f\u5728\u56fe\u50cf\u5e73\u9762\uff0c\u8fd9\u80fd\u663e\u8457\u63d0\u9ad8\u8f6c\u5411\u8fd0\u52a8\u8ba1\u6570\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.11949", "categories": ["cs.LG", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11949", "abs": "https://arxiv.org/abs/2511.11949", "authors": ["Eunjeong Jeong", "Nikolaos Pappas"], "title": "Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation", "comment": "This paper has been submitted to a peer-reviewed journal", "summary": "Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.", "AI": {"tldr": "FedBacys\u662f\u4e00\u4e2a\u7535\u6c60\u611f\u77e5\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u7528\u6237\u7535\u6c60\u6c34\u5e73\u7684\u5faa\u73af\u5ba2\u6237\u7aef\u53c2\u4e0e\u6765\u4f18\u5316\u80fd\u91cf\u6536\u96c6\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u80fd\u8017\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u5177\u6709\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u590d\u6742\u6027\u5bfc\u81f4\u5ba2\u6237\u7aef\u8bad\u7ec3\u6a21\u578b\u7684\u8ba1\u7b97\u80fd\u8017\u663e\u8457\u3002\u5728\u80fd\u91cf\u6536\u96c6\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u80fd\u91cf\u6709\u9650\uff0c\u6bcf\u4e2a\u8bbe\u5907\u7684\u53c2\u4e0e\u53ef\u7528\u6027\u4f1a\u6ce2\u52a8\uff0c\u8fd9\u4e00\u95ee\u9898\u5c24\u4e3a\u5173\u952e\u3002", "method": "\u63d0\u51faFedBacys\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u5ba2\u6237\u7aef\u5e76\u6309\u7535\u6c60\u6c34\u5e73\u987a\u5e8f\u8c03\u5ea6\u53c2\u4e0e\uff0c\u6700\u5c0f\u5316\u5197\u4f59\u8ba1\u7b97\u3002\u8fd8\u63d0\u51faFedBacys-Odd\u53d8\u4f53\uff0c\u5141\u8bb8\u5ba2\u6237\u7aef\u9009\u62e9\u6027\u53c2\u4e0e\u4ee5\u8fdb\u4e00\u6b65\u964d\u4f4e\u80fd\u8017\u3002", "result": "\u63d0\u4f9b\u4e86\u6846\u67b6\u7684\u6536\u655b\u6027\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\u5176\u76f8\u6bd4\u73b0\u6709\u7b97\u6cd5\u5728\u80fd\u6548\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "FedBacys\u6846\u67b6\u80fd\u663e\u8457\u51cf\u5c11\u7cfb\u7edf\u8303\u56f4\u5185\u7684\u80fd\u91cf\u4f7f\u7528\uff0c\u63d0\u9ad8\u5b66\u4e60\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2511.12346", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12346", "abs": "https://arxiv.org/abs/2511.12346", "authors": ["Asmit Bandyopadhyay", "Anindita Das Bhattacharjee", "Rakesh Das"], "title": "CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification", "comment": null, "summary": "Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\\mathcal{O}(T^2D)$ to $\\mathcal{O}(T\\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.", "AI": {"tldr": "CLAReSNet\u662f\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u5377\u79ef\u63d0\u53d6\u548cTransformer\u98ce\u683c\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6f5c\u5728\u74f6\u9888\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u9762\u4e34\u7684\u9ad8\u5149\u8c31\u7ef4\u5ea6\u3001\u590d\u6742\u5149\u8c31-\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u6709\u9650\u8bad\u7ec3\u6837\u672c\u7b49\u95ee\u9898\uff0c\u514b\u670dCNN\u548cTransformer\u5355\u72ec\u5e94\u7528\u7684\u5c40\u9650\u6027\u3002", "method": "\u96c6\u6210\u591a\u5c3a\u5ea6\u5377\u79ef\u63d0\u53d6\u4e0eTransformer\u98ce\u683c\u6ce8\u610f\u529b\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u6f5c\u5728\u74f6\u9888\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u7ed3\u5408\u53cc\u5411RNN\u548c\u591a\u5c3a\u5ea6\u5149\u8c31\u6f5c\u5728\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728Indian Pines\u548cSalinas\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.71%\u548c99.96%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u5177\u6709\u4f18\u8d8a\u7684\u7c7b\u95f4\u5206\u79bb\u6027\u548c\u7c7b\u5185\u7d27\u51d1\u6027\u3002", "conclusion": "CLAReSNet\u5728\u6709\u9650\u6837\u672c\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u6709\u6548\uff0c\u9a8c\u8bc1\u4e86\u6df7\u5408\u67b6\u6784\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.11973", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11973", "abs": "https://arxiv.org/abs/2511.11973", "authors": ["Xinming Gao", "Shangzhe Li", "Yujin Cai", "Wenwu Yu"], "title": "Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression", "comment": null, "summary": "Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $\u03b2$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u56de\u5f52\u4f30\u8ba1\u6e29\u5ea6\u7cfb\u6570\u03b2\uff0c\u5e76\u5f15\u5165\u503c\u6b63\u5219\u5316\u6280\u672f\u6765\u89e3\u51b3XQL\u548cMXQL\u65b9\u6cd5\u4e2d\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u56fa\u5b9a\u6570\u636e\u96c6\u4e0a\u5b66\u4e60\u7b56\u7565\u800c\u65e0\u9700\u73af\u5883\u4ea4\u4e92\uff0c\u5728\u98ce\u9669\u9ad8\u6216\u6210\u672c\u9ad8\u7684\u9886\u57df\u7279\u522b\u6709\u4ef7\u503c\u3002XQL\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u5f3a\u52b2\uff0c\u4f46\u5b58\u5728\u8d85\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u91cf\u5316\u56de\u5f52\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u4f30\u8ba1\u6e29\u5ea6\u7cfb\u6570\u03b2\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u53d7\u7ea6\u675f\u503c\u5b66\u4e60\u542f\u53d1\u7684\u503c\u6b63\u5219\u5316\u6280\u672f\u6765\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728D4RL\u548cNeoRL2\u7b49\u57fa\u51c6\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u9886\u57df\u4e2d\u4f7f\u7528\u4e00\u81f4\u7684\u8d85\u53c2\u6570\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8d85\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12363", "abs": "https://arxiv.org/abs/2511.12363", "authors": ["Michael Yang", "Shijian Deng", "William T. Doan", "Kai Wang", "Tianyu Yang", "Harsh Singh", "Yapeng Tian"], "title": "Explainable AI-Generated Image Detection RewardBench", "comment": null, "summary": "Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an \"MLLM as a judge\" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \\textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\\% on this benchmark (while human inter-annotator agreement reaches 98.30\\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86XAIGID-RewardBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5224\u65adAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u89e3\u91ca\u8d28\u91cf\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6700\u4f73\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u4ecd\u6709\u660e\u663e\u5dee\u8ddd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5206\u7c7b\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u4eba\u7c7b\u4e13\u5bb6\u53ef\u7406\u89e3\u7684\u89e3\u91ca\uff0c\u964d\u4f4e\u4e86\u68c0\u6d4b\u5de5\u5177\u7684\u53ef\u4fe1\u5ea6\u548c\u8bf4\u670d\u529b\u3002\u867d\u7136\u4f7f\u7528MLLMs\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u6210\u4e3a\u8d8b\u52bf\uff0c\u4f46MLLMs\u5728\u8bc4\u4f30\u81ea\u8eab\u6216\u5176\u4ed6MLLMs\u751f\u6210\u7684\u68c0\u6d4b\u89e3\u91ca\u65b9\u9762\u7684\u8868\u73b0\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5305\u542b\u7ea63000\u4e2a\u6807\u6ce8\u4e09\u5143\u7ec4\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u6765\u81ea\u5404\u79cd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u548c\u4f5c\u4e3a\u68c0\u6d4b\u5668\u7684MLLMs\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f53\u524dMLLMs\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff08\u8bc4\u5224\u8005\uff09\u7684\u80fd\u529b\u3002", "result": "\u5f53\u524d\u6700\u4f73\u5956\u52b1\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u5f97\u5206\u4e3a88.76%\uff0c\u800c\u4eba\u7c7b\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u8fbe\u523098.30%\uff0c\u8868\u660e\u5f53\u524dMLLMs\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u4eba\u7c7b\u6c34\u5e73\u8868\u73b0\u4e4b\u95f4\u4ecd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdbMLLMs\u7684\u63a8\u7406\u80fd\u529b\u4ee5\u7f29\u5c0f\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5dee\u8ddd\uff0c\u8bba\u6587\u8fd8\u5206\u6790\u4e86\u8fd9\u4e9b\u6a21\u578b\u5e38\u89c1\u7684\u7f3a\u9677\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2511.11991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11991", "abs": "https://arxiv.org/abs/2511.11991", "authors": ["Xiang Ma", "Taihua Chen", "Pengcheng Wang", "Xuemei Li", "Caiming Zhang"], "title": "ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting", "comment": "AAAI 2026 Oral", "summary": "Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \\textbf{RE}liability-aware \\textbf{C}odebook-\\textbf{AS}sisted \\textbf{T}ime series forecasting framework (\\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.", "AI": {"tldr": "ReCast\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7801\u672c\u7f16\u7801\u5c40\u90e8\u6a21\u5f0f\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\u5904\u7406\u89c4\u5219\u7ed3\u6784\u548c\u4e0d\u89c4\u5219\u6ce2\u52a8\uff0c\u5e76\u5f15\u5165\u53ef\u9760\u6027\u611f\u77e5\u7684\u7801\u672c\u66f4\u65b0\u7b56\u7565\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5168\u5c40\u5206\u89e3\uff0c\u5bf9\u5c40\u90e8\u590d\u6742\u52a8\u6001\u6a21\u5f0f\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7801\u672c\u8fdb\u884c\u8865\u4e01\u91cf\u5316\u7f16\u7801\u5c40\u90e8\u6a21\u5f0f\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\uff08\u91cf\u5316\u8def\u5f84\u548c\u6b8b\u5dee\u8def\u5f84\uff09\uff0c\u901a\u8fc7\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u878d\u5408\u591a\u53ef\u9760\u6027\u56e0\u5b50\u8fdb\u884c\u7801\u672c\u66f4\u65b0\u3002", "result": "\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u5206\u5e03\u504f\u79fb\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "ReCast\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u6349\u5c40\u90e8\u91cd\u590d\u6a21\u5f0f\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u4e14\u9c81\u68d2\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002"}}
{"id": "2511.12365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12365", "abs": "https://arxiv.org/abs/2511.12365", "authors": ["Yiqing Shen", "Mathias Unberath"], "title": "Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning", "comment": null, "summary": "Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.", "AI": {"tldr": "DT-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u590d\u6742\u591a\u6a21\u6001\u89c6\u89c9\u8f93\u5165\u7684\u6570\u5b57\u5b6a\u751f\u8868\u793a\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u9ad8\u7ea7\u8868\u793a\u8fdb\u884c\u7edf\u4e00\u89c6\u89c9\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u76d1\u7763\u5fae\u8c03\u548c\u67b6\u6784\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u9650\u5236\u4e86\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u6001\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u5956\u52b1\u673a\u5236\u6765\u9a8c\u8bc1\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u8f93\u51fa\u51c6\u786e\u6027\uff0c\u8bad\u7ec3LLM\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u8868\u793a\u5e76\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728\u516d\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6db5\u76d6\u4e24\u79cd\u6a21\u6001\u548c\u56db\u79cd\u4efb\u52a1\u7c7b\u578b\uff0cDT-R1\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7279\u5b9a\u4efb\u52a1\u6a21\u578b\u3002", "conclusion": "DT-R1\u4e3a\u89c6\u89c9\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5373\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u8868\u793a\u7684\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u7edf\u4e00\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.12002", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12002", "abs": "https://arxiv.org/abs/2511.12002", "authors": ["Tenghao Ji", "Eytan Adar"], "title": "Selecting Fine-Tuning Examples by Quizzing VLMs", "comment": null, "summary": "A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \\textit{do} exemplify the target concept (e.g., a \\textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.", "AI": {"tldr": "QZLoRA\u662f\u4e00\u4e2a\u7528\u4e8e\u9009\u62e9\u56fe\u50cf\u8fdb\u884c\u4f4e\u79e9\u9002\u5e94\u7684\u6846\u67b6\uff0c\u901a\u8fc7QuizRank\u65b9\u6cd5\u81ea\u52a8\u5bf9\u56fe\u50cf\u8fdb\u884c\u6392\u540d\uff0c\u4ece\u800c\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u751f\u6210\u66f4\u5bf9\u9f50\u3001\u66f4\u903c\u771f\u7684\u56fe\u50cf\u3002", "motivation": "\u5728\u9488\u5bf9\u7279\u5b9a\u4e3b\u9898\u5fae\u8c03\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u65f6\uff0c\u9009\u62e9\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u56fe\u50cf\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4ece\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\u7684\u56fe\u50cf\u96c6\uff08\u5982\u7ef4\u57fa\u5171\u4eab\u8d44\u6e90\uff09\u8fdb\u884c\u5fae\u8c03\u901a\u5e38\u4f1a\u4ea7\u751f\u8f83\u5dee\u7684\u8f93\u51fa\u7ed3\u679c\u3002", "method": "\u63d0\u51faQZLoRA\u6846\u67b6\uff0c\u5229\u7528QuizRank\u65b9\u6cd5\u5c06\u56fe\u50cf\u89c6\u4e3a'\u6559\u80b2\u5e72\u9884'\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c'\u6d4b\u9a8c'\u6765\u81ea\u52a8\u5bf9\u56fe\u50cf\u8fdb\u884c\u6392\u540d\uff0c\u7136\u540e\u8fdb\u884c\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5fae\u8c03\u3002", "result": "QZLoRA\u80fd\u591f\u7528\u66f4\u5c11\u7684\u6837\u672c\u751f\u6210\u66f4\u5bf9\u9f50\u3001\u66f4\u903c\u771f\u7684\u56fe\u50cf\uff0c\u5e76\u4e14\u8fd9\u4e9b\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e5f\u80fd\u751f\u6210\u5177\u6709\u4ee3\u8868\u6027\u7684\u98ce\u683c\u5316\u56fe\u50cf\uff08\u5982\u63d2\u56fe\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u81ea\u52a8\u89c6\u89c9\u63a8\u7406\u4e0e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u76f8\u7ed3\u5408\uff0c\u5728\u4e3b\u9898\u81ea\u9002\u5e94\u751f\u6210\u5efa\u6a21\u65b9\u9762\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2511.12368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12368", "abs": "https://arxiv.org/abs/2511.12368", "authors": ["Yiqing Shen", "Mathias Unberath"], "title": "Fast Reasoning Segmentation for Images and Videos", "comment": null, "summary": "Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.", "AI": {"tldr": "FastReasonSeg\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u8868\u793a\u5c06\u611f\u77e5\u4e0e\u63a8\u7406\u89e3\u8026\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u6559\u5e08\u751f\u6210\u63a8\u7406\u94fe\u7684\u76d1\u7763\u5fae\u8c03\u548c\u8054\u5408\u5956\u52b1\u7684\u5f3a\u5316\u5fae\u8c03\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u5206\u5272\u65b9\u6cd5\u9700\u8981\u6570\u5341\u4ebf\u53c2\u6570\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u8d85\u51fa\u4e86\u8fb9\u7f18\u8bbe\u5907\u7684\u8ba1\u7b97\u80fd\u529b\u3002\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u4f20\u9012\u63a8\u7406\u5206\u5272\u6240\u9700\u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u9884\u6d4b\u548c\u4e2d\u95f4\u7279\u5f81\u7684\u5339\u914d\uff0c\u800c\u4e0d\u662f\u4fdd\u7559\u63a8\u7406\u94fe\u3002", "method": "\u91c7\u7528\u6570\u5b57\u5b6a\u751f\u8868\u793a\u5c06\u611f\u77e5\u4e0e\u63a8\u7406\u89e3\u8026\uff0c\u9996\u5148\u5728\u6559\u5e08\u751f\u6210\u7684\u63a8\u7406\u94fe\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u4f7f\u7528\u540c\u65f6\u8bc4\u4f30\u5206\u5272\u51c6\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u5bf9\u9f50\u7684\u8054\u5408\u5956\u52b1\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\u3002", "result": "\u5728\u4e24\u4e2a\u89c6\u9891\u57fa\u51c6\uff08JiTBench\u3001RVTBench\uff09\u548c\u4e24\u4e2a\u56fe\u50cf\u57fa\u51c6\uff08ReasonSeg\u3001LLM-Seg40K\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFastReasonSeg\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u63a8\u7406\u5206\u5272\u6027\u80fd\u3002\u84b8\u998f\u540e\u76840.6B\u53d8\u4f53\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u53c2\u6570\u591a20\u500d\u7684\u6a21\u578b\uff0c\u540c\u65f6\u8fbe\u52307.79 FPS\u7684\u541e\u5410\u91cf\uff0c\u4ec5\u6d88\u80172.1GB\u5185\u5b58\u3002", "conclusion": "FastReasonSeg\u7684\u9ad8\u6548\u6027\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u5206\u5272\uff0c\u4e3a\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u5177\u8eabAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12033", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12033", "abs": "https://arxiv.org/abs/2511.12033", "authors": ["Jiahe Shi", "Zhengqi Gao", "Ching-Yun Ko", "Duane Boning"], "title": "EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation", "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and na\u00efvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.", "AI": {"tldr": "EARL\u662f\u4e00\u4e2a\u57fa\u4e8e\u71b5\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u7684Verilog\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u9ad8\u71b5\u4ee4\u724c\u6765\u63d0\u5347RTL\u4ee3\u7801\u751f\u6210\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u5b58\u5728\u8bed\u6cd5\u9519\u8bef\u3001\u529f\u80fd\u5e7b\u89c9\u548c\u8bbe\u8ba1\u610f\u56fe\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faEARL\u6846\u67b6\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u71b5\u5f15\u5bfc\u7684\u9009\u62e9\u6027\u66f4\u65b0\u673a\u5236\uff0c\u5c06\u68af\u5ea6\u66f4\u65b0\u96c6\u4e2d\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u4ee4\u724c\u4e0a\u3002", "result": "\u5728VerilogEval\u548cRTLLM\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEARL\u5c06\u529f\u80fd\u901a\u8fc7\u7387\u63d0\u5347\u4e8614.7%\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u66f4\u65b0\u5e76\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5c06\u5f3a\u5316\u5b66\u4e60\u96c6\u4e2d\u5728\u5173\u952e\u9ad8\u71b5\u4ee4\u724c\u4e0a\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u6709\u9488\u5bf9\u6027\u7684\u7ed3\u6784\u5316RTL\u4ee3\u7801\u751f\u6210\u7b56\u7565\u6539\u8fdb\u3002"}}
{"id": "2511.12370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12370", "abs": "https://arxiv.org/abs/2511.12370", "authors": ["Chamuditha Jayanga Galappaththige", "Jason Lai", "Lloyd Windrim", "Donald Dansereau", "Niko S\u00fcnderhauf", "Dimity Miller"], "title": "Changes in Real Time: Online Scene Change Detection with Multi-View Fusion", "comment": null, "summary": "Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9996\u4e2a\u59ff\u6001\u65e0\u5173\u3001\u65e0\u9700\u6807\u7b7e\u3001\u4fdd\u8bc1\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u5728\u7ebf\u573a\u666f\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u8d85\u8fc710FPS\u7684\u901f\u5ea6\u4e0b\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u6700\u4f73\u79bb\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u573a\u666f\u53d8\u5316\u68c0\u6d4b\u662f\u4e00\u4e2a\u6781\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5728\u7ebf\u65b9\u6cd5\u51c6\u786e\u7387\u8fdc\u4f4e\u4e8e\u79bb\u7ebf\u65b9\u6cd5\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u65e0\u7ea6\u675f\u89c6\u89d2\u4e0b\u5b9e\u65f6\u68c0\u6d4b\u76f8\u5173\u53d8\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u65b0\u7684\u81ea\u76d1\u7763\u878d\u5408\u635f\u5931\u4ece\u591a\u4e2a\u7ebf\u7d22\u548c\u89c2\u5bdf\u4e2d\u63a8\u65ad\u573a\u666f\u53d8\u5316\uff0c\u57fa\u4e8ePnP\u7684\u5feb\u901f\u59ff\u6001\u4f30\u8ba1\uff0c\u4ee5\u53ca\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85\u573a\u666f\u8868\u793a\u7684\u5feb\u901f\u53d8\u5316\u5f15\u5bfc\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u5728\u7ebf\u548c\u79bb\u7ebf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u573a\u666f\u53d8\u5316\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u65f6\u573a\u666f\u7406\u89e3\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12041", "abs": "https://arxiv.org/abs/2511.12041", "authors": ["Shivam Barwey", "Pinaki Pal"], "title": "Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers", "comment": null, "summary": "Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9996\u521b\u7684\u591a\u5c3a\u5ea6\u56fe\u53d8\u6362\u5668\u65b9\u6cd5\uff08SR-GT\uff09\uff0c\u7528\u4e8e\u57fa\u4e8e\u7f51\u683c\u7684\u53cd\u5e94\u6d41\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u51e0\u4f55\u548c\u975e\u5747\u5300/\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u6bd4\u4f20\u7edf\u63d2\u503c\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u8d85\u5206\u8fa8\u7387\u6d41\u91cd\u5efa\u6280\u672f\u5bf9\u4e8e\u4e9a\u7f51\u683c\u95ed\u5408\u5efa\u6a21\u3001\u52a0\u901f\u65f6\u7a7a\u9884\u6d4b\u3001\u6570\u636e\u538b\u7f29\u4ee5\u53ca\u7a00\u758f\u5b9e\u9a8c\u6d4b\u91cf\u7684\u5347\u5c3a\u5ea6\u5904\u7406\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u6d41\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u7ed3\u5408\u53d8\u6362\u5668\u67b6\u6784\u6355\u6349\u4f4e\u5206\u8fa8\u7387\u6d41\u573a\u4e0d\u540c\u90e8\u5206\u4e4b\u95f4\u7684\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u8bc6\u522b\u91cd\u8981\u7279\u5f81\uff0c\u5e76\u751f\u6210\u4fdd\u7559\u8fd9\u4e9b\u7279\u5f81\u7684\u9ad8\u5206\u8fa8\u7387\u6d41\u573a\u3002\u4f7f\u7528\u72ec\u7279\u7684\u5143\u7d20\u5c40\u90e8\uff08+\u90bb\u57df\uff09\u56fe\u8868\u793a\u5bf9\u7c97\u8f93\u5165\u8fdb\u884c\u6807\u8bb0\u5316\u5904\u7406\u3002", "result": "\u57282D\u6c22-\u7a7a\u6c14\u9884\u6df7\u6c14\u4f53\u7206\u8f70\u4f20\u64ad\u7684\u6311\u6218\u6027\u6d4b\u8bd5\u95ee\u9898\u4e2d\uff0cSR-GT\u6846\u67b6\u5bf9\u53cd\u5e94\u6d41\u573a\u7279\u5f81\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u63d2\u503c\u7684\u8d85\u5206\u8fa8\u7387\u65b9\u6848\u3002", "conclusion": "SR-GT\u65b9\u6cd5\u5728\u590d\u6742\u591a\u5c3a\u5ea6\u53cd\u5e94\u6d41\u884c\u4e3a\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8d85\u5206\u8fa8\u7387\u80fd\u529b\uff0c\u4e3a\u57fa\u4e8e\u7f51\u683c\u7684\u6d41\u573a\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12371", "abs": "https://arxiv.org/abs/2511.12371", "authors": ["Yiqing Shen", "Chenxiao Fan", "Chenjia Li", "Mathias Unberath"], "title": "Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models", "comment": null, "summary": "The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u63a8\u7406\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u8868\u793a\u89c6\u9891\u5185\u5bb9\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u9690\u542b\u67e5\u8be2\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u663e\u5f0f\u67e5\u8be2\uff0c\u65e0\u6cd5\u5e94\u5bf9\u9700\u8981\u63a8\u7406\u7684\u9690\u542b\u67e5\u8be2\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u63a8\u7406\u7684\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u901a\u8fc7\u7ec4\u5408\u5bf9\u9f50\u5c06\u5206\u89e3\u7684\u5b50\u67e5\u8be2\u4e0e\u6570\u5b57\u5b6a\u751f\u8868\u793a\u5339\u914d\u8bc6\u522b\u5019\u9009\u89c6\u9891\uff0c\u7136\u540e\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u5373\u65f6\u7ec6\u5316\u8c03\u7528\u4e13\u4e1a\u6a21\u578b\u586b\u8865\u4fe1\u606f\u7a7a\u767d\u3002", "result": "\u5728ReasonT2VBench-135\u4e0a\u8fbe\u523081.2%\u7684R@1\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u5347\u8d85\u8fc750\u4e2a\u767e\u5206\u70b9\uff1b\u5728ReasonT2VBench-1000\u4e0a\u4fdd\u630181.7%\u7684R@1\uff0c\u5e76\u5728\u4e09\u4e2a\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u8868\u793a\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u9690\u542b\u67e5\u8be2\u7684\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2511.12071", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12071", "abs": "https://arxiv.org/abs/2511.12071", "authors": ["Rosario Napoli", "Gabriele Morabito", "Antonio Celesti", "Massimo Villari", "Maria Fazio"], "title": "Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread", "comment": "Accepted at the 16th IEEE International Conference on Knowledge Graphs (ICKG) 2025", "summary": "The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u77e5\u8bc6\u8865\u5168\u9636\u6bb5\u7684\u56fe\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u53d1\u73b0\u7a00\u758f\u6570\u636e\u4e2d\u7684\u9690\u5f0f\u8bed\u4e49\u6765\u6539\u8fdb\u56fe\u5d4c\u5165\u8868\u793a\u8d28\u91cf\u3002", "motivation": "\u7531\u4e8e\u56fe\u5d4c\u5165\u4ec5\u4ece\u663e\u5f0f\u62d3\u6251\u548c\u7279\u5f81\u4e2d\u63d0\u53d6\uff0c\u53ef\u80fd\u9519\u8fc7\u7a00\u758f\u6570\u636e\u96c6\u4e2d\u9690\u85cf\u7684\u5173\u952e\u9690\u5f0f\u77e5\u8bc6\uff0c\u5f71\u54cd\u56fe\u7ed3\u6784\u53ca\u5176\u8868\u793a\u8d28\u91cf\u3002", "method": "\u5728\u5d4c\u5165\u751f\u6210\u524d\u52a0\u5165\u77e5\u8bc6\u8865\u5168\u9636\u6bb5\uff0c\u4e13\u6ce8\u4e8e\u4f20\u9012\u5173\u7cfb\uff0c\u4f7f\u7528\u57fa\u4e8e\u8870\u51cf\u7684\u63a8\u7406\u51fd\u6570\u5efa\u6a21\u9690\u85cf\u8fde\u63a5\uff0c\u91cd\u5851\u56fe\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6d41\u6c34\u7ebf\u663e\u8457\u6539\u53d8\u4e86\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u8bc1\u660e\u5176\u5f15\u5165\u4e0d\u4ec5\u662f\u7b80\u5355\u4e30\u5bcc\uff0c\u800c\u662f\u91cd\u65b0\u5b9a\u4e49\u56fe\u8868\u793a\u8d28\u91cf\u7684\u53d8\u9769\u6027\u6b65\u9aa4\u3002", "conclusion": "\u96c6\u6210\u77e5\u8bc6\u8865\u5168\u7684\u56fe\u673a\u5668\u5b66\u4e60\u6d41\u6c34\u7ebf\u80fd\u591f\u901a\u8fc7\u53d1\u73b0\u9690\u5f0f\u8bed\u4e49\u6765\u91cd\u65b0\u5b9a\u4e49\u56fe\u8868\u793a\u8d28\u91cf\uff0c\u5bf9\u56fe\u5d4c\u5165\u7684\u52a8\u6001\u6027\u548c\u805a\u5408\u8fc7\u7a0b\u4ea7\u751f\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2511.12382", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12382", "abs": "https://arxiv.org/abs/2511.12382", "authors": ["Ansh Makwe", "Akansh Agrawal", "Prateek Jain", "Akshan Agrawal", "Priyanka Bagade"], "title": "AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification", "comment": null, "summary": "Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.", "AI": {"tldr": "AGGRNet\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u4fe1\u606f\u6027\u548c\u975e\u4fe1\u606f\u6027\u7279\u5f81\u6765\u533a\u5206\u533b\u5b66\u56fe\u50cf\u4e2d\u7684\u7ec6\u5fae\u7c7b\u522b\u5dee\u5f02\uff0c\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728Kvasir\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u6a21\u578b\u63d0\u53475%\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7531\u4e8e\u7c7b\u522b\u95f4\u89c6\u89c9\u6a21\u5f0f\u590d\u6742\u76f8\u4f3c\u3001\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u4e13\u5bb6\u89e3\u91ca\u5dee\u5f02\u5bfc\u81f4\u7684\u4e25\u91cd\u5206\u7ea7\u548c\u75be\u75c5\u4e9a\u578b\u5206\u7c7b\u6311\u6218\u3002\u73b0\u6709\u6ce8\u610f\u529b\u6a21\u578b\u96be\u4ee5\u6709\u6548\u533a\u5206\u7ec6\u5fae\u7c7b\u522b\uff0c\u65e0\u6cd5\u6355\u6349\u7c7b\u95f4\u76f8\u4f3c\u6027\u548c\u7c7b\u5185\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51faAGGRNet\u6846\u67b6\uff0c\u63d0\u53d6\u4fe1\u606f\u6027\u548c\u975e\u4fe1\u606f\u6027\u7279\u5f81\uff0c\u4ee5\u6709\u6548\u7406\u89e3\u7ec6\u7c92\u5ea6\u89c6\u89c9\u6a21\u5f0f\u5e76\u6539\u8fdb\u590d\u6742\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728Kvasir\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u6a21\u578b\u63d0\u53475%\u3002", "conclusion": "AGGRNet\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u7ec6\u5fae\u7c7b\u522b\u533a\u5206\u95ee\u9898\uff0c\u5728\u590d\u6742\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12075", "abs": "https://arxiv.org/abs/2511.12075", "authors": ["Dong-Hee Shin", "Deok-Joong Lee", "Young-Han Son", "Tae-Eui Kam"], "title": "Treatment Stitching with Schr\u00f6dinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies", "comment": "19 pages, 5 figures, AAAI conference", "summary": "Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schr\u00f6dinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.", "AI": {"tldr": "\u63d0\u51fa\u4e86TreatStitch\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u62fc\u63a5\u73b0\u6709\u6cbb\u7597\u8f68\u8ff9\u6bb5\u6765\u751f\u6210\u4e34\u5e8a\u6709\u6548\u7684\u6cbb\u7597\u8f68\u8ff9\uff0c\u4ee5\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u4e34\u5e8a\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u4f18\u5316\u81ea\u9002\u5e94\u6cbb\u7597\u7b56\u7565\u65f6\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u800c\u4f20\u7edf\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u4e0d\u53ef\u884c\uff0c\u56e0\u4e3a\u53ef\u80fd\u5bf9\u60a3\u8005\u9020\u6210\u4f24\u5bb3\u3002", "method": "\u5f00\u53d1\u4e86Treatment Stitching\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u4e0d\u540c\u8f68\u8ff9\u4e2d\u76f8\u4f3c\u7684\u4e2d\u95f4\u60a3\u8005\u72b6\u6001\u5e76\u62fc\u63a5\u76f8\u5e94\u6bb5\uff1b\u5f53\u72b6\u6001\u5dee\u5f02\u8f83\u5927\u65f6\uff0c\u4f7f\u7528\u859b\u5b9a\u8c14\u6865\u65b9\u6cd5\u751f\u6210\u5e73\u6ed1\u7684\u6865\u63a5\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u6cbb\u7597\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86TreatStitch\u5728\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "TreatStitch\u901a\u8fc7\u751f\u6210\u5408\u6210\u8f68\u8ff9\u6765\u589e\u5f3a\u6570\u636e\u96c6\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u6709\u6548\u6027\uff0c\u907f\u514d\u4e86\u5206\u5e03\u5916\u8f6c\u79fb\uff0c\u4e3a\u4f18\u5316\u81ea\u9002\u5e94\u6cbb\u7597\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12389", "categories": ["cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12389", "abs": "https://arxiv.org/abs/2511.12389", "authors": ["Divake Kumar", "Patrick Poggi", "Sina Tayebati", "Devashri Naik", "Nilesh Ahuja", "Amit Ranjan Trivedi"], "title": "Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation", "comment": null, "summary": "Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63a8\u7406\u65f6\u95f4\u6846\u67b6\uff0c\u5728\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\u4e2d\u89e3\u8026\u6570\u636e\u9a71\u52a8\u548c\u6a21\u578b\u9a71\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u4e8e\u6307\u5bfc\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u548c\u8ba1\u7b97\u5206\u914d\u3002", "motivation": "\u5927\u591a\u6570\u4f30\u8ba1\u5668\u5c06\u6240\u6709\u4e0d\u786e\u5b9a\u6027\u6a21\u5f0f\u538b\u7f29\u4e3a\u5355\u4e00\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u65e0\u6cd5\u53ef\u9760\u5224\u65ad\u4f55\u65f6\u9700\u8981\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u6216\u8c03\u6574\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u6b63\u5219\u5316\u5168\u5c40\u5bc6\u5ea6\u6a21\u578b\u4f30\u8ba1\u6570\u636e\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff08\u5c40\u90e8\u652f\u6301\u4e0d\u8db3\u3001\u6d41\u5f62\u8c31\u5d29\u6e83\u3001\u8de8\u5c42\u7279\u5f81\u4e0d\u4e00\u81f4\uff09\u4f30\u8ba1\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u9700\u91c7\u6837\u3001\u96c6\u6210\u6216\u989d\u5916\u524d\u5411\u4f20\u64ad\u3002", "result": "\u5728MOT17\u6570\u636e\u96c6\u4e0a\u51cf\u5c11\u7ea660%\u8ba1\u7b97\u91cf\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u6bd4\u603b\u4e0d\u786e\u5b9a\u6027\u57fa\u7ebf\u63d0\u9ad813.6\u4e2a\u767e\u5206\u70b9\u7684\u8ba1\u7b97\u8282\u7701\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u81ea\u8c03\u8282\u89c6\u89c9\u63a8\u7406\uff0c\u901a\u8fc7\u6b63\u4ea4\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.12121", "categories": ["cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.12121", "abs": "https://arxiv.org/abs/2511.12121", "authors": ["Wanlong Fang", "Tianle Zhang", "Alvin Chan"], "title": "To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance", "comment": null, "summary": "Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u663e\u5f0f\u5bf9\u9f50\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5bf9\u9f50\u6548\u679c\u53d6\u51b3\u4e8e\u6a21\u6001\u95f4\u7684\u5197\u4f59\u5ea6\uff0c\u5b58\u5728\u6700\u4f18\u5bf9\u9f50\u5f3a\u5ea6\u6765\u5e73\u8861\u6a21\u6001\u7279\u5b9a\u4fe1\u53f7\u548c\u5171\u4eab\u5197\u4f59\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u5b66\u4e60\u666e\u904d\u5047\u8bbe\u8868\u5f81\u5bf9\u9f50\u603b\u662f\u6709\u76ca\u7684\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u663e\u5f0f\u5bf9\u9f50\u5728\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u7ed3\u6784\u4e0b\u76f4\u63a5\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u5f15\u5165\u53ef\u63a7\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7cbe\u786e\u64cd\u7eb5\u5bf9\u9f50\u5f3a\u5ea6\uff0c\u7814\u7a76\u4e0d\u540c\u6570\u636e\u7279\u5f81\u4e0b\u663e\u5f0f\u5bf9\u9f50\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u663e\u5f0f\u5bf9\u9f50\u5bf9\u5355\u6a21\u6001\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u4e0e\u6570\u636e\u7279\u5f81\u76f8\u5173\uff0c\u6700\u4f18\u5bf9\u9f50\u6c34\u5e73\u53d6\u51b3\u4e8e\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u5197\u4f59\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u5e94\u7528\u663e\u5f0f\u5bf9\u9f50\u4ee5\u5b9e\u73b0\u6700\u4f18\u5355\u6a21\u6001\u7f16\u7801\u5668\u6027\u80fd\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2511.12122", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12122", "abs": "https://arxiv.org/abs/2511.12122", "authors": ["Yi Wang", "Ruoyi Fang", "Anzhuo Xie", "Hanrui Feng", "Jianlin Lai"], "title": "Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks", "comment": null, "summary": "This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f1a\u8ba1\u4ea4\u6613\u4e2d\u7684\u52a8\u6001\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u548c\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4ea4\u6613\u73af\u5883\u4e2d\u9690\u85cf\u5f02\u5e38\u884c\u4e3a\u548c\u9ad8\u65f6\u6548\u6027\u8981\u6c42\u7684\u6311\u6218\uff0c\u4e3a\u667a\u80fd\u8d22\u52a1\u98ce\u9669\u63a7\u5236\u548c\u5ba1\u8ba1\u63d0\u4f9b\u65b9\u6cd5\u652f\u6301\u3002", "method": "\u5c06\u591a\u7ef4\u4f1a\u8ba1\u4ea4\u6613\u8bb0\u5f55\u5efa\u6a21\u4e3a\u65f6\u95f4\u5e8f\u5217\u77e9\u9635\uff0c\u4f7f\u7528\u5d4c\u5165\u5c42\u548c\u4f4d\u7f6e\u7f16\u7801\u5b9e\u73b0\u4f4e\u7ef4\u6620\u5c04\uff0c\u6784\u5efa\u591a\u5934\u81ea\u6ce8\u610f\u529b\u5e8f\u5217\u5efa\u6a21\u7ed3\u6784\uff0c\u96c6\u6210\u524d\u9988\u5c42\u548c\u6b63\u5219\u5316\u7b56\u7565\u8fdb\u884c\u6df1\u5ea6\u7279\u5f81\u8868\u793a\u548c\u5f02\u5e38\u6982\u7387\u4f30\u8ba1\u3002", "result": "\u5728AUC\u3001F1-Score\u3001Precision\u548cRecall\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u548c\u6570\u636e\u6270\u52a8\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684\u6846\u67b6\u5728\u4f1a\u8ba1\u4ea4\u6613\u52a8\u6001\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5177\u6709\u9002\u7528\u6027\u548c\u4f18\u52bf\uff0c\u4e3a\u667a\u80fd\u8d22\u52a1\u98ce\u9669\u63a7\u5236\u548c\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2511.12405", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12405", "abs": "https://arxiv.org/abs/2511.12405", "authors": ["Hyunki Seong", "Seongwoo Moon", "Hojin Ahn", "Jehun Kang", "David Hyunchul Shim"], "title": "VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving", "comment": "9 pages, 9 figures", "summary": "Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.", "AI": {"tldr": "\u63d0\u51fa\u4e86VLA-R\u6846\u67b6\uff0c\u5c06\u5f00\u653e\u4e16\u754c\u611f\u77e5\u4e0e\u89c6\u89c9-\u52a8\u4f5c\u68c0\u7d22\u8303\u5f0f\u7ed3\u5408\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u5728\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u7ecf\u5e38\u9047\u5230\u8bad\u7ec3\u65f6\u672a\u89c1\u8fc7\u7684\u6761\u4ef6\uff0c\u9700\u8981\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u6765\u5904\u7406\u5f00\u653e\u4e16\u754c\u60c5\u51b5\u3002", "method": "\u5229\u7528\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f00\u653e\u4e16\u754c\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u901a\u8fc7Q-Former\u74f6\u9888\u805a\u5408\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8868\u793a\u548c\u8bed\u8a00\u5bf9\u9f50\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u89c6\u89c9-\u52a8\u4f5c\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848\u6765\u5bf9\u9f50\u89c6\u89c9\u8bed\u8a00\u548c\u52a8\u4f5c\u5d4c\u5165\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u5728\u672a\u89c1\u8fc7\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u548c\u63a2\u7d22\u6027\u80fd\u3002", "conclusion": "VLA-R\u6846\u67b6\u6210\u529f\u5730\u5c06\u5f00\u653e\u4e16\u754c\u611f\u77e5\u4e0e\u52a8\u4f5c\u68c0\u7d22\u76f8\u7ed3\u5408\uff0c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f00\u653e\u4e16\u754c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.12123", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12123", "abs": "https://arxiv.org/abs/2511.12123", "authors": ["Zejiao Liu", "Junqi Tu", "Yitian Hong", "Luolin Xiong", "Yaochu Jin", "Yang Tang", "Fangfei Li"], "title": "HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning", "comment": "AAAI 2026", "summary": "In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6325\u8005\u7684\u8054\u5408\u7b56\u7565\u6846\u67b6\u548c\u5206\u5c42\u6307\u6325\u8005\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u6765\u63d0\u5347\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u72ec\u7acb\u667a\u80fd\u4f53\u63a2\u7d22\u6765\u66f4\u65b0\u8054\u5408\u7b56\u7565\uff0c\u7f3a\u4e4f\u667a\u80fd\u4f53\u95f4\u7684\u534f\u8c03\uff0c\u9650\u5236\u4e86\u8054\u5408\u7b56\u7565\u7684\u8868\u8fbe\u80fd\u529b\u548c\u63a2\u7d22\u6548\u7387\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6307\u6325\u8005\u7684\u8054\u5408\u7b56\u7565\u6846\u67b6\uff0c\u76f4\u63a5\u589e\u5f3a\u8054\u5408\u7b56\u7565\u7684\u8868\u8fbe\u80fd\u529b\u5e76\u534f\u8c03\u63a2\u7d22\uff1b\u5f00\u53d1\u5206\u5c42\u6307\u6325\u8005\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u6307\u5bfc\u6307\u6325\u8005\u548c\u667a\u80fd\u4f53\u7684\u7b56\u7565\u66f4\u65b0\u65b9\u5411\u4e0e\u6027\u80fd\u6539\u8fdb\u5bf9\u9f50\uff1b\u901a\u8fc7\u90e8\u7f72\u5c40\u90e8\u6307\u6325\u8005\uff0c\u5728\u4fdd\u6301\u96c6\u4e2d\u8bad\u7ec3\u4f18\u52bf\u7684\u540c\u65f6\u6d88\u9664\u6267\u884c\u65f6\u7684\u667a\u80fd\u4f53\u95f4\u901a\u4fe1\u3002", "result": "\u5728StarCraftII\u591a\u667a\u80fd\u4f53\u6311\u6218\u3001\u591a\u667a\u80fd\u4f53MuJoCo\u548c\u591a\u667a\u80fd\u4f53\u7c92\u5b50\u73af\u5883\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHCPO\u5728\u5408\u4f5c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u7ade\u4e89\u6027\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HCPO\u901a\u8fc7\u534f\u8c03\u591a\u667a\u80fd\u4f53\u63a2\u7d22\u548c\u589e\u5f3a\u8054\u5408\u7b56\u7565\u8868\u8fbe\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2511.12415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12415", "abs": "https://arxiv.org/abs/2511.12415", "authors": ["Xinrui Li", "Qi Cai", "Yuanxin Wu"], "title": "Towards Rotation-only Imaging Geometry: Rotation Estimation", "comment": null, "summary": "Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65cb\u8f6c\u6d41\u5f62\u7684\u4ec5\u65cb\u8f6c\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u6784\u8fd0\u52a8\u91cd\u5efa\u4efb\u52a1\uff0c\u901a\u8fc7\u5c06\u5e73\u79fb\u8868\u793a\u4e3a\u65cb\u8f6c\u7684\u51fd\u6570\uff0c\u5728\u65cb\u8f6c\u6d41\u5f62\u4e0a\u538b\u7f29\u6210\u50cf\u51e0\u4f55\u8868\u793a\uff0c\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65cb\u8f6c\u4f30\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4ec5\u4f4d\u59ff\u6210\u50cf\u51e0\u4f55\u901a\u8fc7\u4f4d\u59ff\u8c03\u6574\u5c55\u793a\u4e86\u66f4\u597d\u7684SfM\u6027\u80fd\uff0c\u672c\u6587\u7ee7\u7eed\u4ece\u4ec5\u4f4d\u59ff\u89c6\u89d2\u63a2\u7d22\u573a\u666f\u7ed3\u6784\u3001\u65cb\u8f6c\u548c\u5e73\u79fb\u4e4b\u95f4\u7684\u5173\u952e\u5173\u7cfb\uff0c\u5e0c\u671b\u4e3a\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u548c\u53ef\u9760\u76843D\u89c6\u89c9\u8ba1\u7b97\u505a\u51fa\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u91cd\u6295\u5f71\u8bef\u5dee\u7684\u4ec5\u65cb\u8f6c\u4f18\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u53cc\u89c6\u56fe\u548c\u591a\u89c6\u56fe\u573a\u666f\uff0c\u901a\u8fc7\u5c06\u5e73\u79fb\u8868\u793a\u4e3a\u65cb\u8f6c\u7684\u51fd\u6570\uff0c\u5c06\u6210\u50cf\u51e0\u4f55\u8868\u793a\u538b\u7f29\u5230\u65cb\u8f6c\u6d41\u5f62\u4e0a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65cb\u8f6c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u751a\u81f3\u53ef\u4e0e\u591a\u6b21\u675f\u8c03\u6574\u8fed\u4ee3\u7ed3\u679c\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6709\u671b\u4e3a\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u548c\u53ef\u9760\u76843D\u89c6\u89c9\u8ba1\u7b97\u505a\u51fa\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u4ec5\u65cb\u8f6c\u4f18\u5316\u6846\u67b6\u5728\u7ed3\u6784\u8fd0\u52a8\u91cd\u5efa\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12419", "abs": "https://arxiv.org/abs/2511.12419", "authors": ["Wenjie Li", "Jinglei Shi", "Jin Han", "Heng Guo", "Zhanyu Ma"], "title": "Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance", "comment": null, "summary": "Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.", "AI": {"tldr": "DHGM\u6a21\u578b\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec3\u6269\u6563\u5148\u9a8c\u548c\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u540c\u65f6\u53bb\u9664\u96e8\u7eb9\u4f2a\u5f71\u5e76\u589e\u5f3a\u7ed3\u6784\u7ec6\u8282\uff0c\u89e3\u51b3\u4e86\u5929\u6c14\u6062\u590d\u548c\u8d85\u5206\u8fa8\u7387\u4e4b\u95f4\u7684\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u5e38\u53d7\u6076\u52a3\u5929\u6c14\u5f71\u54cd\u800c\u9000\u5316\uff0c\u4f46\u5929\u6c14\u6062\u590d\u65b9\u6cd5\u53ef\u80fd\u4f1a\u727a\u7272\u5bf9\u5206\u6790\u5c0f\u7269\u4f53\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u9891\u7ec6\u8282\u3002\u7b80\u5355\u5730\u7ea7\u8054\u6062\u590d\u548c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u5b83\u4eec\u4e4b\u95f4\u7684\u5185\u5728\u51b2\u7a81\uff1a\u6062\u590d\u65e8\u5728\u53bb\u9664\u9ad8\u9891\u5929\u6c14\u566a\u58f0\uff0c\u800c\u8d85\u5206\u8fa8\u7387\u65e8\u5728\u4ece\u73b0\u6709\u7ec6\u8282\u4e2d\u751f\u6210\u9ad8\u9891\u7eb9\u7406\u3002", "method": "\u63d0\u51faDHGM\uff08\u57fa\u4e8e\u6269\u6563\u7684\u9ad8\u9891\u5f15\u5bfc\u6a21\u578b\uff09\uff0c\u6574\u5408\u9884\u8bad\u7ec3\u6269\u6563\u5148\u9a8c\u4e0e\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u540c\u65f6\u53bb\u9664\u96e8\u7eb9\u4f2a\u5f71\u5e76\u589e\u5f3a\u7ed3\u6784\u7ec6\u8282\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDHGM\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "DHGM\u80fd\u591f\u6709\u6548\u751f\u6210\u5e72\u51c0\u4e14\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u5929\u6c14\u6062\u590d\u548c\u8d85\u5206\u8fa8\u7387\u4e4b\u95f4\u7684\u51b2\u7a81\u95ee\u9898\u3002"}}
{"id": "2511.12143", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12143", "abs": "https://arxiv.org/abs/2511.12143", "authors": ["Jialiang Wang", "Xiong Zhou", "Xianming Liu", "Gangfeng Hu", "Deming Zhai", "Junjun Jiang", "Haoliang Li"], "title": "Variation-Bounded Loss for Noise-Tolerant Learning", "comment": "Accepted by AAAI2026", "summary": "Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u635f\u5931\u51fd\u6570\u5c5e\u6027\u2014\u2014\u53d8\u5f02\u6bd4\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u53d8\u5f02\u6709\u754c\u635f\u5931\u51fd\u6570\u5bb6\u65cf\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8f83\u5c0f\u7684\u53d8\u5f02\u6bd4\u80fd\u5e26\u6765\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4e3a\u653e\u677e\u5bf9\u79f0\u6761\u4ef6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\u3002", "motivation": "\u7f13\u89e3\u566a\u58f0\u6807\u7b7e\u5bf9\u76d1\u7763\u5b66\u4e60\u7684\u8d1f\u9762\u5f71\u54cd\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\uff0c\u9c81\u68d2\u635f\u5931\u51fd\u6570\u5df2\u6210\u4e3a\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u6d41\u884c\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u53d8\u5f02\u6bd4\u8fd9\u4e00\u65b0\u5c5e\u6027\u6765\u6539\u8fdb\u635f\u5931\u51fd\u6570\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u53d8\u5f02\u6bd4\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u9c81\u68d2\u6027\u7684\u65b0\u5c5e\u6027\uff0c\u6784\u5efa\u53d8\u5f02\u6709\u754c\u635f\u5931\u51fd\u6570\u5bb6\u65cf\uff0c\u63d0\u4f9b\u53d8\u5f02\u6bd4\u7684\u7406\u8bba\u5206\u6790\uff0c\u5e76\u57fa\u4e8e\u53d8\u5f02\u6bd4\u91cd\u65b0\u5f62\u5f0f\u5316\u51e0\u79cd\u5e38\u7528\u635f\u5931\u51fd\u6570\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u79ef\u6781\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\uff0c\u53d8\u5f02\u6bd4\u4e3a\u653e\u677e\u5bf9\u79f0\u6761\u4ef6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u73b0\u975e\u5bf9\u79f0\u6761\u4ef6\u7684\u66f4\u7b80\u6d01\u8def\u5f84\u3002", "conclusion": "\u53d8\u5f02\u6bd4\u662f\u635f\u5931\u51fd\u6570\u9c81\u68d2\u6027\u7684\u91cd\u8981\u5c5e\u6027\uff0c\u53d8\u5f02\u6709\u754c\u635f\u5931\u51fd\u6570\u5bb6\u65cf\u5728\u566a\u58f0\u6807\u7b7e\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u9c81\u68d2\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2511.12422", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12422", "abs": "https://arxiv.org/abs/2511.12422", "authors": ["Nuolin Sun", "Linyuan Wang", "Haonan Wei", "Lei Li", "Bin Yan"], "title": "MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation", "comment": null, "summary": "ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMFI-ResNet\uff0c\u901a\u8fc7\u538b\u7f29-\u6269\u5c55\u7b56\u7565\u7ed3\u5408MeanFlow\u6a21\u5757\u6539\u8fdbResNet\u7684\u53c2\u6570\u6548\u7387\u548c\u5224\u522b\u6027\u80fd\uff0c\u5728CIFAR\u6570\u636e\u96c6\u4e0a\u663e\u8457\u51cf\u5c11\u53c2\u6570\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u53d7\u6d41\u5339\u914d\u6a21\u578bMeanFlow\u542f\u53d1\uff0c\u63a2\u7d22\u751f\u6210\u5f0f\u6d41\u573a\u5982\u4f55\u6709\u6548\u8868\u5f81ResNet\u4e2d\u7684\u7279\u5f81\u53d8\u6362\u8fc7\u7a0b\uff0c\u4e3a\u7406\u89e3\u751f\u6210\u5f0f\u5efa\u6a21\u4e0e\u5224\u522b\u5f0f\u5b66\u4e60\u5173\u7cfb\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u538b\u7f29-\u6269\u5c55\u7b56\u7565\uff1a\u538b\u7f29\u9636\u6bb5\u5c06\u6bcf\u4e2aResNet\u9636\u6bb5\u7b80\u5316\u4e3a1-2\u4e2aMeanFlow\u6a21\u5757\u6784\u5efa\u8f7b\u91cf\u5143\u6a21\u578b\uff1b\u6269\u5c55\u9636\u6bb5\u5bf9\u524d\u4e09\u9636\u6bb5\u9009\u62e9\u6027\u5b75\u5316\u5339\u914d\u57fa\u7ebf\u914d\u7f6e\uff0c\u6700\u540e\u4e00\u9636\u6bb5\u4fdd\u6301MeanFlow\u5f62\u5f0f\u5e76\u5fae\u8c03\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u4e0a\uff0c\u76f8\u6bd4ResNet-50\u5206\u522b\u51cf\u5c1146.28%\u548c45.59%\u53c2\u6570\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u9ad80.23%\u548c0.17%\u3002", "conclusion": "\u751f\u6210\u5f0f\u6d41\u573a\u80fd\u6709\u6548\u8868\u5f81ResNet\u7279\u5f81\u53d8\u6362\u8fc7\u7a0b\uff0c\u4e3a\u7406\u89e3\u751f\u6210\u5f0f\u5efa\u6a21\u4e0e\u5224\u522b\u5f0f\u5b66\u4e60\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u6548\u7387\u4e0e\u5224\u522b\u6027\u80fd\u7684\u8054\u5408\u63d0\u5347\u3002"}}
{"id": "2511.12428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12428", "abs": "https://arxiv.org/abs/2511.12428", "authors": ["Jingqi Xu", "Jingxi Lu", "Chenghao Li", "Sreetama Sarkar", "Souvik Kundu", "Peter A. Beerel"], "title": "RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.", "AI": {"tldr": "RedVTP\u662f\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08DVLMs\uff09\u7684\u54cd\u5e94\u9a71\u52a8\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u63a8\u7406\u52a8\u6001\u6765\u63d0\u5347\u6a21\u578b\u6548\u7387\uff0c\u5728\u4e0d\u635f\u5931\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u751f\u6210\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u6269\u6563\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08DVLMs\uff09\u867d\u7136\u652f\u6301\u5e76\u884c\u4ee4\u724c\u89e3\u7801\uff0c\u4f46\u5927\u91cf\u89c6\u89c9\u4ee4\u724c\u4e25\u91cd\u5f71\u54cd\u4e86\u63a8\u7406\u6548\u7387\u3002\u867d\u7136\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08AVLMs\uff09\u7684\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u5df2\u6709\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46DVLMs\u7684\u526a\u679d\u65b9\u6cd5\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faRedVTP\u7b56\u7565\uff0c\u5229\u7528DVLMs\u7684\u63a8\u7406\u52a8\u6001\u6765\u4f30\u8ba1\u89c6\u89c9\u4ee4\u724c\u91cd\u8981\u6027\u3002\u65b9\u6cd5\u4f7f\u7528\u63a9\u7801\u54cd\u5e94\u4ee4\u724c\u7684\u6ce8\u610f\u529b\u6765\u8bc4\u4f30\u91cd\u8981\u6027\uff0c\u57fa\u4e8e\u91cd\u8981\u6027\u5206\u6570\u5728\u63a8\u7406\u6b65\u9aa4\u95f4\u4fdd\u6301\u4e00\u81f4\u7684\u89c2\u5bdf\uff0c\u5728\u7b2c\u4e00\u6b65\u63a8\u7406\u540e\u4ece\u63a9\u7801\u4ee4\u724c\u4e2d\u526a\u679d\u91cd\u8981\u6027\u8f83\u4f4e\u7684\u89c6\u89c9\u4ee4\u724c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRedVTP\u5c06LLaDA-V\u548cLaViDa\u7684\u4ee4\u724c\u751f\u6210\u541e\u5410\u91cf\u5206\u522b\u63d0\u9ad8\u4e86186%\u548c28.05%\uff0c\u63a8\u7406\u5ef6\u8fdf\u5206\u522b\u964d\u4f4e\u4e8664.97%\u548c21.87%\uff0c\u4e14\u5728\u4e0d\u635f\u5931\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8fd9\u4e9b\u6539\u8fdb\u3002", "conclusion": "RedVTP\u662f\u4e00\u79cd\u6709\u6548\u7684DVLMs\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u6269\u6563\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2511.12154", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12154", "abs": "https://arxiv.org/abs/2511.12154", "authors": ["Gustavo Polleti", "Marlesson Santana", "Eduardo Fontes"], "title": "Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions", "comment": null, "summary": "We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u91d1\u878d\u4ea4\u6613\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u7ed3\u6784\u5316\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5f00\u653e\u94f6\u884c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u91d1\u878d\u4ea4\u6613\u4e2d\u7ed3\u6784\u5316\u6570\u636e\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u4fe1\u606f\u7684\u6574\u5408\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u7684\u5f00\u653e\u94f6\u884c\u73af\u5883\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5\u5904\u7406\u4ea4\u6613\u5e8f\u5217\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u6846\u67b6\u3002", "result": "\u6a21\u578b\u5728\u6570\u5343\u5bb6\u5317\u7f8e\u91d1\u878d\u673a\u6784\u7684\u5927\u89c4\u6a21\u7814\u7a76\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u548c\u79bb\u6563\u4e8b\u4ef6\u5e8f\u5217\u65b9\u6cd5\uff0c\u80fd\u591f\u8de8\u5730\u57df\u548c\u673a\u6784\u6cdb\u5316\u3002", "conclusion": "\u81ea\u76d1\u7763\u6a21\u578b\u5728\u6b3a\u8bc8\u9884\u9632\u3001\u4fe1\u7528\u98ce\u9669\u548c\u5ba2\u6237\u6d1e\u5bdf\u7b49\u91d1\u878d\u5e94\u7528\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.12432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12432", "abs": "https://arxiv.org/abs/2511.12432", "authors": ["Xilai Li", "Xiaosong Li", "Weijun Jiang"], "title": "Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion", "comment": "Accepted at AAAI 2026", "summary": "Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.", "AI": {"tldr": "\u63d0\u51faUP-Fusion\u6846\u67b6\uff0c\u901a\u8fc7\u901a\u9053\u6270\u52a8\u548c\u9884\u8bad\u7ec3\u77e5\u8bc6\u96c6\u6210\u89e3\u51b3\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u878d\u5408\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7edf\u4e00\u6a21\u578b\u5728\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4e2d\u9762\u4e34\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u800c\u5f15\u5165\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u4f1a\u964d\u4f4e\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u901a\u9053\u526a\u679d\u6a21\u5757(SCPM)\u8fc7\u6ee4\u589e\u5f3a\u7279\u5f81\u901a\u9053\uff0c\u51e0\u4f55\u4eff\u5c04\u8c03\u5236\u6a21\u5757(GAM)\u4fdd\u6301\u7279\u5f81\u7f16\u7801\u5668\u6a21\u6001\u533a\u5206\u6027\uff0c\u6587\u672c\u5f15\u5bfc\u901a\u9053\u6270\u52a8\u6a21\u5757(TCPM)\u91cd\u5851\u901a\u9053\u5206\u5e03\u3002", "result": "\u5728\u591a\u9879\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u548c\u4e0b\u6e38\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UP-Fusion\u6846\u67b6\u901a\u8fc7\u901a\u9053\u6270\u52a8\u548c\u9884\u8bad\u7ec3\u77e5\u8bc6\u96c6\u6210\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12155", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12155", "abs": "https://arxiv.org/abs/2511.12155", "authors": ["Thong Bach", "Dung Nguyen", "Thao Minh Le", "Truyen Tran"], "title": "Rethinking Deep Alignment Through The Lens Of Incomplete Learning", "comment": "AAAI'26", "summary": "Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u7cfb\u7edf\u6027\u6f0f\u6d1e\u6e90\u4e8e\u81ea\u56de\u5f52\u8bad\u7ec3\u4e2d\u7684\u4f4d\u7f6e\u4f9d\u8d56\u68af\u5ea6\u5f31\u5316\uff0c\u5bfc\u81f4\u5b89\u5168\u5b66\u4e60\u4e0d\u5b8c\u6574\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u504f\u597d\u7684\u6807\u8bb0\u4f5c\u4e3a\u8ba1\u7b97\u6307\u6807\uff0c\u5e76\u5f00\u53d1\u4e86\u9488\u5bf9\u6027\u7684\u8865\u5168\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b89\u5168\u5bf9\u9f50\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ecd\u5b58\u5728\u7cfb\u7edf\u6027\u5bf9\u6297\u653b\u51fb\u6f0f\u6d1e\u3002\u672c\u6587\u65e8\u5728\u4ece\u673a\u5236\u5c42\u9762\u5206\u6790\u8fd9\u4e00\u95ee\u9898\u7684\u6839\u6e90\uff0c\u5e76\u63d0\u4f9b\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u673a\u5236\u5206\u6790\u8bc6\u522b\u4f4d\u7f6e\u4f9d\u8d56\u68af\u5ea6\u5f31\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u504f\u597d\u7684\u6807\u8bb0\u4f5c\u4e3a\u5b89\u5168\u5b66\u4e60\u4e0d\u5b8c\u6574\u7684\u8ba1\u7b97\u6307\u6807\uff0c\u5f00\u53d1\u4e86\u5305\u542b\u81ea\u9002\u5e94\u60e9\u7f5a\u548c\u6df7\u5408\u6559\u5e08\u84b8\u998f\u7684\u9488\u5bf9\u6027\u8865\u5168\u65b9\u6cd5\u3002", "result": "\u5728Llama\u548cQwen\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u5bf9\u6297\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e48-98%\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u7684\u57fa\u672c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u673a\u5236\u5c42\u9762\u7684\u7406\u89e3\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.12171", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.12171", "abs": "https://arxiv.org/abs/2511.12171", "authors": ["Chaitanya Kumar Konda", "Piyush Agrawal", "Shivansh Srivastava", "Manish Agrawal"], "title": "FGM optimization in complex domains using Gaussian process regression based profile generation algorithm", "comment": null, "summary": "This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7684\u529f\u80fd\u68af\u5ea6\u6750\u6599\u4f53\u79ef\u5206\u6570\u5206\u5e03\u751f\u6210\u7b97\u6cd5\uff0c\u53ef\u5904\u7406\u4efb\u610f\u5f62\u72b6\u57df\u5e76\u751f\u6210\u5e73\u6ed1\u5206\u5e03\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u9057\u4f20\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u4efb\u610f\u5f62\u72b6\u57df\u529f\u80fd\u68af\u5ea6\u6750\u6599\u8bbe\u8ba1\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u548c\u8fb9\u754c\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u751f\u6210\u4f53\u79ef\u5206\u6570\u5206\u5e03\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u9057\u4f20\u7b97\u6cd5\uff08\u5305\u542b\u6295\u5f71\u7b97\u5b50\u7684\u6a21\u62df\u4e8c\u8fdb\u5236\u4ea4\u53c9\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u7b97\u6cd5\u80fd\u5904\u7406\u590d\u6742\u5f62\u72b6\u57df\uff0c\u751f\u6210\u5e73\u6ed1\u7684\u529f\u80fd\u68af\u5ea6\u6750\u6599\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u70ed\u5f39\u6027\u4f18\u5316\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u4efb\u610f\u5f62\u72b6\u57df\u529f\u80fd\u68af\u5ea6\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u53d1\u73b0\u6700\u4f18\u914d\u7f6e\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12452", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12452", "abs": "https://arxiv.org/abs/2511.12452", "authors": ["Xiaoyu Lin", "Aniket Ghorpade", "Hansheng Zhu", "Justin Qiu", "Dea Rrozhani", "Monica Lama", "Mick Yang", "Zixuan Bian", "Ruohan Ren", "Alan B. Hong", "Jiatao Gu", "Chris Callison-Burch"], "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions", "comment": null, "summary": "With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.", "AI": {"tldr": "DenseAnnotate\u662f\u4e00\u4e2a\u97f3\u9891\u9a71\u52a8\u7684\u5728\u7ebf\u6807\u6ce8\u5e73\u53f0\uff0c\u901a\u8fc7\u8bed\u97f3\u63cf\u8ff0\u548c\u533a\u57df\u6807\u8bb0\u521b\u5efa\u5bc6\u96c6\u3001\u7ec6\u7c92\u5ea6\u7684\u56fe\u50cf\u548c3D\u8d44\u4ea7\u6807\u6ce8\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6587\u672c\u6807\u6ce8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e3b\u8981\u4f9d\u8d56\u7a00\u758f\u7684\u4e92\u8054\u7f51\u6316\u6398\u6216\u624b\u52a8\u8f93\u5165\u6807\u6ce8\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ea\u80fd\u6355\u83b7\u56fe\u50cf\u89c6\u89c9\u5185\u5bb9\u7684\u4e00\u5c0f\u90e8\u5206\u3002\u5bc6\u96c6\u6807\u6ce8\u66f4\u6709\u4ef7\u503c\u4f46\u7a00\u7f3a\uff0c\u4f20\u7edf\u6587\u672c\u6807\u6ce8\u7ba1\u9053\u5728\u8868\u8fbe\u6027\u3001\u901f\u5ea6\u548c\u4e13\u4e1a\u9886\u57df\u8986\u76d6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u97f3\u9891\u9a71\u52a8\u7684\u5728\u7ebf\u6807\u6ce8\u5e73\u53f0\uff0c\u6807\u6ce8\u8005\u901a\u8fc7\u8bed\u97f3\u63cf\u8ff0\u89c2\u5bdf\u5185\u5bb9\uff0c\u540c\u65f6\u5c06\u53e3\u8bed\u77ed\u8bed\u4e0e\u56fe\u50cf\u533a\u57df\u62163D\u573a\u666f\u90e8\u5206\u540c\u6b65\u94fe\u63a5\uff0c\u7ed3\u5408\u8bed\u97f3\u8f6c\u6587\u672c\u548c\u6ce8\u610f\u529b\u533a\u57df\u6807\u8bb0\u6280\u672f\u3002", "result": "\u901a\u8fc71,000\u591a\u540d\u6807\u6ce8\u8005\u5728\u4e24\u4e2a\u9886\u57df\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u521b\u5efa\u4e86\u5305\u542b3,531\u5f20\u56fe\u50cf\u3001898\u4e2a3D\u573a\u666f\u548c7,460\u4e2a3D\u5bf9\u8c61\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b20\u79cd\u8bed\u8a00\u7684\u97f3\u9891\u5bf9\u9f50\u5bc6\u96c6\u6807\u6ce8\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u9879\u80fd\u529b\u4e0a\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u672a\u6765\u89c6\u89c9\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\uff0c\u53ef\u5e94\u7528\u4e8e\u5404\u79cd\u4efb\u52a1\u548c\u591a\u6837\u5316\u6570\u636e\u7c7b\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u591a\u8bed\u8a00\u3001\u6587\u5316\u5bf9\u9f50\u548c3D\u7a7a\u95f4\u80fd\u529b\u3002"}}
{"id": "2511.12180", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12180", "abs": "https://arxiv.org/abs/2511.12180", "authors": ["Ge Cheng", "Shuo Wang", "Yun Zhang"], "title": "Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering", "comment": "31 pages, 8 figures", "summary": "Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SC-InfoNCE\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u8c03\u6536\u655b\u76ee\u6807\u6765\u7075\u6d3b\u63a7\u5236\u7279\u5f81\u76f8\u4f3c\u6027\u5bf9\u9f50\uff0c\u5728\u56fe\u50cf\u3001\u56fe\u7ed3\u6784\u548c\u6587\u672c\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u7a33\u5b9a\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1InfoNCE\u5728\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u7ecf\u9a8c\u6027\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u7406\u89e3InfoNCE\u7684\u7406\u8bba\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u663e\u5f0f\u7279\u5f81\u7a7a\u95f4\u5efa\u6a21\u6837\u672c\u589e\u5f3a\u89c6\u56fe\uff0c\u4f7f\u7528\u8f6c\u79fb\u6982\u7387\u77e9\u9635\u6355\u6349\u6570\u636e\u589e\u5f3a\u52a8\u6001\uff0c\u63d0\u51faSC-InfoNCE\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u7f29\u653e\u76ee\u6807\u77e9\u9635\u7075\u6d3b\u63a7\u5236\u7279\u5f81\u76f8\u4f3c\u6027\u5bf9\u9f50\u3002", "result": "\u5728\u56fe\u50cf\u3001\u56fe\u7ed3\u6784\u548c\u6587\u672c\u4efb\u52a1\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSC-InfoNCE\u59cb\u7ec8\u5b9e\u73b0\u5f3a\u5927\u4e14\u53ef\u9760\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "SC-InfoNCE\u901a\u8fc7\u5f15\u5165\u53ef\u8c03\u6536\u655b\u76ee\u6807\uff0c\u4f7f\u8bad\u7ec3\u76ee\u6807\u80fd\u66f4\u597d\u5730\u5339\u914d\u4e0b\u6e38\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5747\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12474", "categories": ["cs.CV", "cs.CL", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.12474", "abs": "https://arxiv.org/abs/2511.12474", "authors": ["Chucheng Xiang", "Ruchao Bao", "Biyin Feng", "Wenzheng Wu", "Zhongyuan Liu", "Yirui Guan", "Ligang Liu"], "title": "Co-Layout: LLM-driven Co-optimization for Interior Layout", "comment": null, "summary": "We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor\". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u7f51\u683c\u6574\u6570\u89c4\u5212\u7684\u81ea\u52a8\u5316\u5ba4\u5185\u8bbe\u8ba1\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u623f\u95f4\u5e03\u5c40\u548c\u5bb6\u5177\u6446\u653e\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u8bbe\u8ba1\u6d41\u7a0b\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8054\u5408\u4f18\u5316\u623f\u95f4\u5e03\u5c40\u548c\u5bb6\u5177\u6446\u653e\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u63d0\u53d6\u7ed3\u6784\u5316\u8bbe\u8ba1\u7ea6\u675f\uff0c\u5c06\u5176\u7f16\u7801\u5230\u57fa\u4e8e\u7f51\u683c\u7684\u7edf\u4e00\u8868\u793a\u4e2d\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u4f18\u5316\u7b56\u7565\uff0c\u4ece\u4f4e\u5206\u8fa8\u7387\u7f51\u683c\u5f00\u59cb\u7b80\u5316\u95ee\u9898\u6c42\u89e3\u3002", "result": "\u5728\u591a\u6837\u5316\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u4e24\u9636\u6bb5\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u7c97\u5230\u7ec6\u7b56\u7565\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6574\u6570\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u4e3a\u81ea\u52a8\u5316\u5ba4\u5185\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12191", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12191", "abs": "https://arxiv.org/abs/2511.12191", "authors": ["Szymon Wojciechowski", "Micha\u0142 Wo\u017aniak"], "title": "Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data", "comment": null, "summary": "Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.\n  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u9760\u65b9\u6cd5\u6765\u8bc4\u4f30\u57fa\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u7684\u7b97\u6cd5\u4e0e\u8fd4\u56de\u5355\u4e00\u89e3\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5728\u4e8e\u5982\u4f55\u4ecePareto\u524d\u6cbf\u4e2d\u9009\u62e9\u7b26\u5408\u7528\u6237\u504f\u597d\u7684\u89e3\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u89e3\u51b3\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u4e0e\u5355\u89e3\u7b97\u6cd5\u4e4b\u95f4\u7684\u6bd4\u8f83\u96be\u9898\uff0c\u586b\u8865\u5206\u7c7b\u5668\u8bc4\u4f30\u65b9\u6cd5\u5b66\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u591a\u76ee\u6807\u7b97\u6cd5\u8fd4\u56de\u7684Pareto\u524d\u6cbf\u4e2d\u9009\u62e9\u7b26\u5408\u7528\u6237\u504f\u597d\u7684\u89e3\uff0c\u4e0e\u5355\u89e3\u7b97\u6cd5\u8fdb\u884c\u53ef\u9760\u6bd4\u8f83\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u80fd\u591f\u516c\u5e73\u6bd4\u8f83\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u548c\u5355\u89e3\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u586b\u8865\u4e86\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u8bc4\u4f30\u65b9\u6cd5\u5b66\u7684\u7a7a\u767d\uff0c\u4e3a\u7b97\u6cd5\u6bd4\u8f83\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\u3002"}}
{"id": "2511.12498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12498", "abs": "https://arxiv.org/abs/2511.12498", "authors": ["Jongseong Bae", "Junwoo Ha", "Jinnyeong Heo", "Yeongin Lee", "Ha Young Kim"], "title": "Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion", "comment": "Accepted to AAAI 2026", "summary": "Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.", "AI": {"tldr": "\u63d0\u51faC3DFusion\u6a21\u5757\uff0c\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u5f53\u524d\u548c\u5386\u53f2\u5e27\u76843D\u70b9\u7279\u5f81\u6765\u751f\u6210\u9690\u85cf\u533a\u57df\u611f\u77e5\u76843D\u7279\u5f81\u51e0\u4f55\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5efa\u8f66\u8f86\u4fa7\u65b9\u4e0d\u53ef\u89c1\u533a\u57df\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76f8\u673a\u76843D\u8bed\u4e49\u573a\u666f\u8865\u5168\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u589e\u5f3a\u5e27\u5185\u533a\u57df\uff0c\u4f46\u5728\u91cd\u5efa\u8f66\u8f86\u4fa7\u65b9\u5173\u952e\u4e0d\u53ef\u89c1\u533a\u57df\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c3d\u7ba1\u5386\u53f2\u5e27\u901a\u5e38\u5305\u542b\u8fd9\u4e9b\u4e0d\u53ef\u89c1\u533a\u57df\u7684\u6709\u4ef7\u503c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u5f53\u524d\u4e2d\u5fc3\u4e0a\u4e0b\u65873D\u878d\u5408(C3DFusion)\u6a21\u5757\uff0c\u901a\u8fc7\u5386\u53f2\u4e0a\u4e0b\u6587\u6a21\u7cca\u548c\u5f53\u524d\u4e2d\u5fc3\u7279\u5f81\u5bc6\u96c6\u5316\u4e24\u79cd\u4e92\u8865\u6280\u672f\u8fdb\u884c\u589e\u5f3a\u7684\u65f6\u95f4\u878d\u5408\uff0c\u6291\u5236\u4e0d\u51c6\u786e\u626d\u66f2\u5386\u53f2\u70b9\u7279\u5f81\u7684\u566a\u58f0\uff0c\u589e\u5f3a\u5f53\u524d\u70b9\u7279\u5f81\u7684\u4f53\u79ef\u8d21\u732e\u3002", "result": "\u5728SemanticKITTI\u548cSSCBench-KITTI-360\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e94\u7528\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u65f6\u4e5f\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "C3DFusion\u6a21\u5757\u7b80\u5355\u96c6\u6210\u5230\u6807\u51c6SSC\u67b6\u6784\u4e2d\u5373\u53ef\u5c55\u73b0\u5f3a\u5927\u6548\u679c\uff0c\u901a\u8fc7\u663e\u5f0f\u5bf9\u9f50\u5f53\u524d\u548c\u5386\u53f2\u5e27\u76843D\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9690\u85cf\u533a\u57df\u91cd\u5efa\u95ee\u9898\u3002"}}
{"id": "2511.12217", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12217", "abs": "https://arxiv.org/abs/2511.12217", "authors": ["Gil Goren", "Shahar Katz", "Lior Wolf"], "title": "AlignTree: Efficient Defense Against LLM Jailbreak Attacks", "comment": "Accepted as an Oral Presentation at the 40th AAAI Conference on Artificial Intelligence (AAAI-26), January 2026", "summary": "Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.", "AI": {"tldr": "AlignTree\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u76d1\u63a7\u6a21\u578b\u6fc0\u6d3b\u5e76\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u68c0\u6d4b\u4e0d\u5b89\u5168\u7684\u751f\u6210\u884c\u4e3a\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u589e\u5f3a\u6a21\u578b\u5bf9\u9f50\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\uff0c\u7ed5\u8fc7\u5b89\u5168\u6307\u5357\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u5bb9\u6613\u88ab\u7ed5\u8fc7\uff0c\u65e0\u6cd5\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u6709\u6548\u5e94\u7528\u3002", "method": "AlignTree\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u76d1\u63a7LLM\u6fc0\u6d3b\uff0c\u4f7f\u7528\u9ad8\u6548\u7684\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u68c0\u6d4b\u4e0d\u5339\u914d\u884c\u4e3a\u3002\u5206\u7c7b\u5668\u57fa\u4e8e\u4e24\u4e2a\u4fe1\u53f7\uff1a\u62d2\u7edd\u65b9\u5411\uff08\u5bf9\u4e0d\u5339\u914d\u63d0\u793a\u6fc0\u6d3b\u7684\u7ebf\u6027\u8868\u793a\uff09\u548c\u57fa\u4e8eSVM\u7684\u975e\u7ebf\u6027\u7279\u5f81\u4fe1\u53f7\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cAlignTree\u5728\u591a\u4e2aLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "AlignTree\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u63d0\u793a\u6216\u8f85\u52a9\u9632\u62a4\u6a21\u578b\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u9f50\u3002"}}
{"id": "2511.12511", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12511", "abs": "https://arxiv.org/abs/2511.12511", "authors": ["Jialiang Shen", "Jiyang Zheng", "Yunqi Xue", "Huajie Chen", "Yu Yao", "Hui Kang", "Ruiqi Liu", "Helin Gong", "Yang Yang", "Dadong Wang", "Tongliang Liu"], "title": "DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection", "comment": "12 pages, 5 figures", "summary": "With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e08\u751f\u77e5\u8bc6\u84b8\u998f\u7684\u6a21\u7cca\u9c81\u68d2AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u51bb\u7ed3\u5728\u6e05\u6670\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\uff08DINOv3\uff09\uff0c\u5c06\u5176\u7279\u5f81\u548clogit\u54cd\u5e94\u84b8\u998f\u5230\u5728\u6a21\u7cca\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\uff0c\u4f7f\u6a21\u578b\u5728\u8fd0\u52a8\u6a21\u7cca\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u4e00\u81f4\u7684\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u4e16\u754c\u9000\u5316\uff08\u7279\u522b\u662f\u8fd0\u52a8\u6a21\u7cca\uff09\u4e0b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u8fd0\u52a8\u6a21\u7cca\u4f1a\u626d\u66f2\u7cbe\u7ec6\u7eb9\u7406\u5e76\u6291\u5236\u9ad8\u9891\u4f2a\u5f71\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5e08\u751f\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u4f7f\u7528\u5728\u6e05\u6670\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684DINOv3\u4f5c\u4e3a\u51bb\u7ed3\u7684\u6559\u5e08\u6a21\u578b\uff0c\u5c06\u5176\u7279\u5f81\u548clogit\u54cd\u5e94\u84b8\u998f\u5230\u5728\u6a21\u7cca\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u6a21\u7cca\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u68c0\u6d4b\u3002", "result": "\u5728\u8fd0\u52a8\u6a21\u7cca\u548c\u6e05\u6670\u6761\u4ef6\u4e0b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u793a\u51fa\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5728\u8fd0\u52a8\u6a21\u7cca\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.12222", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.12222", "abs": "https://arxiv.org/abs/2511.12222", "authors": ["Hangshuo Tian"], "title": "Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling", "comment": null, "summary": "Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.\n  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.\n  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \\emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7c92\u5b50\u6ee4\u6ce2\u4e2d\u9e21\u7fa4\u4f18\u5316\u7b97\u6cd5\u4e0eKLD\u81ea\u9002\u5e94\u91c7\u6837\u4e4b\u95f4\u7684\u7406\u8bba\u4ea4\u4e92\u4f5c\u7528\uff0c\u53d1\u73b0CSO\u589e\u5f3a\u7684\u7c92\u5b50\u6ee4\u6ce2\u5728\u76f8\u540c\u7edf\u8ba1\u8bef\u5dee\u4e0b\u9700\u8981\u66f4\u5c11\u7684\u7c92\u5b50\u6570\u3002", "motivation": "\u7406\u89e3\u57fa\u4e8e\u7fa4\u4f53\u667a\u80fd\u7684\u7c92\u5b50\u91cd\u751f\u6210\u6838\u4e0e\u57fa\u4e8eKLD\u7684\u81ea\u9002\u5e94\u91c7\u6837\u4e4b\u95f4\u7684\u7406\u8bba\u4ea4\u4e92\u4f5c\u7528\uff0c\u76ee\u524d\u8fd9\u65b9\u9762\u7684\u7406\u8bba\u7406\u89e3\u8fd8\u4e0d\u5145\u5206\u3002", "method": "\u5728\u7b80\u5316\u5efa\u6a21\u6846\u67b6\u4e0b\u5206\u6790CSO\u91cd\u751f\u6210\u6b65\u9aa4\u5bf9\u7c92\u5b50\u96c6\u5206\u5e03\u7684\u5f71\u54cd\uff0c\u5c06CSO\u7684\u9002\u5e94\u5ea6\u9a71\u52a8\u66f4\u65b0\u8fd1\u4f3c\u4e3a\u5747\u65b9\u6536\u7f29\uff0c\u5e76\u5e94\u7528Karamata\u4e0d\u7b49\u5f0f\u5206\u6790KLD\u91c7\u6837\u4e2d\u7684\u671f\u671b\u7bb1\u5360\u7528\u60c5\u51b5\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5728\u76f8\u540c\u7edf\u8ba1\u8bef\u5dee\u754c\u9650\u4e0b\uff0cCSO\u589e\u5f3a\u7684\u7c92\u5b50\u6ee4\u6ce2\u6bd4\u6807\u51c6\u7c92\u5b50\u6ee4\u6ce2\u9700\u8981\u66f4\u4f4e\u7684\u671f\u671b\u7c92\u5b50\u6570\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u5904\u7406\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e9b\u6280\u672f\u7ed3\u5408\u65f6\u89c2\u5bdf\u5230\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6ee4\u6ce2\u5668\u63d0\u4f9b\u4e86\u8d77\u70b9\u3002"}}
{"id": "2511.12240", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12240", "abs": "https://arxiv.org/abs/2511.12240", "authors": ["Vishal Joshua Meesala"], "title": "SCI: An Equilibrium for Signal Intelligence", "comment": "34 pages, 7 figures. Preprint", "summary": "We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] (\"Surgical Precision\") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.", "AI": {"tldr": "SCI\u662f\u4e00\u4e2a\u95ed\u73af\u63a7\u5236\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u5efa\u6a21\u4e3a\u53d7\u63a7\u72b6\u6001\uff0c\u901a\u8fc7\u4e3b\u52a8\u9a71\u52a8\u624b\u672f\u7cbe\u5ea6SP(t)\u5411\u76ee\u6807\u503c\u6536\u655b\uff0c\u5728\u4eba\u7c7b\u589e\u76ca\u9884\u7b97\u4e0b\u66f4\u65b0\u53c2\u6570Theta\uff0c\u663e\u8457\u964d\u4f4e\u89e3\u91ca\u8bef\u5dee\u5e76\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u89e3\u91ca\u5668\u5b58\u5728\u89e3\u91ca\u8bef\u5dee\u5927\u3001\u7a33\u5b9a\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u5c06\u53ef\u89e3\u91ca\u6027\u5efa\u6a21\u4e3a\u52a8\u6001\u63a7\u5236\u76ee\u6807\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u3001\u53ef\u6062\u590d\u548c\u53ef\u4fe1\u7684\u89e3\u91ca\u884c\u4e3a\u3002", "method": "SCI\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u534f\u8c03\u7ec4\u4ef6\uff1a\u53ef\u9760\u6027\u52a0\u6743\u7684\u591a\u5c3a\u5ea6\u7279\u5f81P(t,s)\u3001\u77e5\u8bc6\u5f15\u5bfc\u7684\u89e3\u91ca\u5668psi_Theta\uff08\u8f93\u51fa\u53ef\u8ffd\u8e2a\u6807\u8bb0\u548c\u7406\u7531\uff09\u3001\u4ee5\u53ca\u914d\u5907\u56de\u6eda\u3001\u4fe1\u4efb\u57df\u4fdd\u62a4\u548c\u4e0b\u964d\u6761\u4ef6\u7684Lyapunov\u5f15\u5bfc\u63a7\u5236\u5668\u3002", "result": "\u5728\u751f\u7269\u533b\u5b66\u3001\u5de5\u4e1a\u548c\u73af\u5883\u9886\u57df\uff0cSCI\u76f8\u6bd4\u9759\u6001\u89e3\u91ca\u5668\u5c06\u89e3\u91ca\u8bef\u5dee\u964d\u4f4e25-42%\uff08\u5e73\u574738%\uff09\uff0c\u540c\u65f6\u4fdd\u6301AUC/F1\u5728\u57fa\u7ebf1-2\u4e2a\u767e\u5206\u70b9\u5185\uff0cSP\u65b9\u5dee\u4ece0.030\u964d\u81f30.011\u3002", "conclusion": "\u5c06\u53ef\u89e3\u91ca\u6027\u5efa\u6a21\u4e3a\u63a7\u5236\u76ee\u6807\u53ef\u5728\u4e0d\u540c\u4fe1\u53f7\u673a\u5236\u4e0b\u4ea7\u751f\u66f4\u7a33\u5b9a\u3001\u66f4\u5feb\u6062\u590d\u548c\u66f4\u53ef\u4fe1\u7684\u89e3\u91ca\u884c\u4e3a\u3002"}}
{"id": "2511.12528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12528", "abs": "https://arxiv.org/abs/2511.12528", "authors": ["Zheyuan Zhang", "Jiwei Zhang", "Boyu Zhou", "Linzhimeng Duan", "Hong Chen"], "title": "D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation", "comment": null, "summary": "Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.", "AI": {"tldr": "D\u00b2-VPR\u662f\u4e00\u4e2a\u57fa\u4e8e\u84b8\u998f\u548c\u53ef\u53d8\u5f62\u805a\u5408\u7684\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u6846\u67b6\uff0c\u65e8\u5728\u4fdd\u6301\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3DINOv2\u7b49\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728VPR\u4efb\u52a1\u4e2d\u867d\u7136\u6027\u80fd\u4f18\u8d8a\u4f46\u6a21\u578b\u590d\u6742\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u5fae\u8c03\uff0c\u5f15\u5165\u84b8\u998f\u6062\u590d\u6a21\u5757(DRM)\u5bf9\u9f50\u5e08\u751f\u6a21\u578b\u7279\u5f81\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u81ea\u4e0a\u800c\u4e0b\u6ce8\u610f\u529b\u7684\u53ef\u53d8\u5f62\u805a\u5408\u5668(TDDA)\u52a8\u6001\u8c03\u6574\u611f\u5174\u8da3\u533a\u57df\u3002", "result": "\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u540c\u65f6\uff0c\u76f8\u6bd4CricaVPR\u51cf\u5c11\u4e86\u7ea664.2%\u7684\u53c2\u6570\u548c62.6%\u7684FLOPs\u3002", "conclusion": "D\u00b2-VPR\u6846\u67b6\u5728\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684VPR\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12261", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12261", "abs": "https://arxiv.org/abs/2511.12261", "authors": ["Zongxin Shen", "Yanyong Huang", "Dongjie Wang", "Jinyuan Chang", "Fengmao Lv", "Tianrui Li", "Xiaoyi Jiang"], "title": "Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection", "comment": null, "summary": "Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faCLIM-FS\u65b9\u6cd5\u89e3\u51b3\u6df7\u5408\u7f3a\u5931\u7684\u591a\u89c6\u56fe\u65e0\u76d1\u7763\u7279\u5f81\u9009\u62e9\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u548c\u81ea\u9002\u5e94\u6570\u636e\u586b\u8865\uff0c\u5e76\u5229\u7528\u4e00\u81f4\u6027\u805a\u7c7b\u7ed3\u6784\u548c\u8de8\u89c6\u56fe\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u4ec5\u5173\u6ce8\u89c6\u56fe\u7f3a\u5931\u95ee\u9898\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u8df5\u4e2d\u66f4\u666e\u904d\u7684\u6df7\u5408\u7f3a\u5931\u573a\u666f\uff1b2) \u5bf9\u89c6\u56fe\u95f4\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u7684\u5229\u7528\u4e0d\u8db3\uff1b3) \u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u6765\u9610\u660e\u7279\u5f81\u9009\u62e9\u548c\u6570\u636e\u586b\u8865\u5728\u8054\u5408\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u975e\u8d1f\u6b63\u4ea4\u77e9\u9635\u5206\u89e3\u7684\u7279\u5f81\u9009\u62e9\u6a21\u578b\uff0c\u96c6\u6210\u7f3a\u5931\u89c6\u56fe\u548c\u53d8\u91cf\u7684\u586b\u8865\uff0c\u8054\u5408\u5b66\u4e60\u7279\u5f81\u9009\u62e9\u548c\u81ea\u9002\u5e94\u6570\u636e\u586b\u8865\u3002\u5145\u5206\u5229\u7528\u4e00\u81f4\u6027\u805a\u7c7b\u7ed3\u6784\u548c\u8de8\u89c6\u56fe\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\u6765\u589e\u5f3a\u534f\u540c\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u516b\u4e2a\u771f\u5b9e\u4e16\u754c\u591a\u89c6\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCLIM-FS\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CLIM-FS\u662f\u4e00\u79cd\u65b0\u9896\u7684IMUFS\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u6df7\u5408\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.12265", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12265", "abs": "https://arxiv.org/abs/2511.12265", "authors": ["Rui Wang", "Zeming Wei", "Xiyue Zhang", "Meng Sun"], "title": "Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks", "comment": null, "summary": "Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6821\u51c6\u5bf9\u6297\u91c7\u6837\uff08CAS\uff09\u7684\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u52a8\u6001\u8bbe\u8ba1\u5956\u52b1\u5e76\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u4ec5\u9488\u5bf9\u6709\u9650\u653b\u51fb\u7c7b\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u5355\u4e00\u6216\u6709\u9650\u653b\u51fb\u7c7b\u578b\uff0c\u5bfc\u81f4DNN\u5728\u9762\u5bf9\u5b9e\u9645\u4e2d\u53ef\u80fd\u9047\u5230\u4f46\u672a\u5728\u8bad\u7ec3\u4e2d\u5904\u7406\u7684\u653b\u51fb\u7c7b\u578b\u65f6\u4ecd\u7136\u8106\u5f31\u3002", "method": "\u4ece\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\u89d2\u5ea6\uff0c\u52a8\u6001\u8bbe\u8ba1\u5956\u52b1\u5e76\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u8003\u8651\u591a\u4e2a\u9c81\u68d2\u6027\u7ef4\u5ea6\u7684\u52a8\u6001\u548c\u76f8\u4e92\u4f9d\u8d56\u7279\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAS\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6e05\u6d01\u51c6\u786e\u7387\u3002", "conclusion": "CAS\u4e3aDNN\u7684\u9c81\u68d2\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2511.12547", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12547", "abs": "https://arxiv.org/abs/2511.12547", "authors": ["Zhiguang Lu", "Qianqian Xu", "Peisong Wen", "Siran Da", "Qingming Huang"], "title": "HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models", "comment": null, "summary": "Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHiGFA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u5f15\u5bfc\u7b56\u7565\u5728\u6269\u6563\u6a21\u578b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u6587\u672c\u3001\u8f6e\u5ed3\u548c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u5668\u5f15\u5bfc\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u96be\u4ee5\u51c6\u786e\u6355\u6349\u7c7b\u522b\u5b9a\u4e49\u6027\u7ec6\u5fae\u7279\u5f81\u7684\u95ee\u9898\uff0c\u907f\u514d\u6807\u51c6\u65b9\u6cd5\u751f\u6210\u8bef\u5bfc\u6027\u6837\u672c\u5bfc\u81f4\u5206\u7c7b\u5668\u6027\u80fd\u4e0b\u964d\u3002", "method": "HiGFA\u5229\u7528\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u7684\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u5728\u65e9\u671f\u9636\u6bb5\u4f7f\u7528\u5f3a\u6587\u672c\u548c\u8f6e\u5ed3\u5f15\u5bfc\u5efa\u7acb\u6574\u4f53\u573a\u666f\u548c\u7ed3\u6784\uff0c\u5728\u6700\u540e\u9636\u6bb5\u6fc0\u6d3b\u7ec6\u7c92\u5ea6\u5206\u7c7b\u5668\u5f15\u5bfc\u5e76\u57fa\u4e8e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u52a8\u6001\u8c03\u5236\u6240\u6709\u5f15\u5bfc\u4fe1\u53f7\u5f3a\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86HiGFA\u7684\u6709\u6548\u6027\u3002", "conclusion": "HiGFA\u901a\u8fc7\u5206\u5c42\u3001\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u534f\u8c03\u673a\u5236\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u4e14\u5fe0\u5b9e\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5e73\u8861\u5168\u5c40\u7ed3\u6784\u5f62\u6210\u4e0e\u7cbe\u786e\u7ec6\u8282\u4f18\u5316\u3002"}}
{"id": "2511.12554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12554", "abs": "https://arxiv.org/abs/2511.12554", "authors": ["Yijie Guo", "Dexiang Hong", "Weidong Chen", "Zihan She", "Cheng Ye", "Xiaojun Chang", "Zhendong Mao"], "title": "EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis", "comment": "11 pages, 7 figures. This is a preprint version of a paper submitted to CVPR 2026", "summary": "Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.", "AI": {"tldr": "EmoVerse\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u3001\u77e5\u8bc6\u56fe\u8c31\u542f\u53d1\u7684\u6807\u6ce8\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u60c5\u611f\u5206\u6790\uff0c\u5305\u542b\u8d85\u8fc721.9\u4e07\u5f20\u56fe\u50cf\uff0c\u652f\u6301\u79bb\u6563\u548c\u8fde\u7eed\u60c5\u611f\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u60c5\u611f\u5206\u6790\u7814\u7a76\u7f3a\u4e4f\u5f00\u6e90\u548c\u53ef\u89e3\u91ca\u7684\u6570\u636e\u96c6\uff0c\u5927\u591a\u6570\u7814\u7a76\u4ec5\u7ed9\u6574\u5f20\u56fe\u50cf\u5206\u914d\u5355\u4e00\u79bb\u6563\u60c5\u611f\u6807\u7b7e\uff0c\u65e0\u6cd5\u63ed\u793a\u89c6\u89c9\u5143\u7d20\u5982\u4f55\u8d21\u732e\u4e8e\u60c5\u611f\u3002", "method": "\u901a\u8fc7\u5c06\u60c5\u611f\u5206\u89e3\u4e3a\u80cc\u666f-\u5c5e\u6027-\u4e3b\u4f53\u4e09\u5143\u7ec4\u5e76\u5c06\u6bcf\u4e2a\u5143\u7d20\u5b9a\u4f4d\u5230\u89c6\u89c9\u533a\u57df\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\u786e\u4fdd\u9ad8\u53ef\u9760\u6027\uff0c\u5e76\u5f15\u5165\u53ef\u89e3\u91ca\u6a21\u578b\u5c06\u89c6\u89c9\u7ebf\u7d22\u6620\u5c04\u5230\u7ef4\u5ea6\u60c5\u611f\u7a7a\u95f4\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b219k+\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7c7b\u522b\u60c5\u611f\u72b6\u6001\u548c\u7ef4\u5ea6\u60c5\u611f\u7a7a\u95f4\u7684\u53cc\u91cd\u6807\u6ce8\uff0c\u652f\u6301\u8bcd\u7ea7\u548c\u4e3b\u4f53\u7ea7\u60c5\u611f\u63a8\u7406\u3002", "conclusion": "EmoVerse\u6570\u636e\u96c6\u3001\u6807\u6ce8\u6d41\u7a0b\u548c\u6a21\u578b\u5171\u540c\u4e3a\u63a8\u8fdb\u53ef\u89e3\u91ca\u7684\u9ad8\u5c42\u60c5\u611f\u7406\u89e3\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u7840\u3002"}}
{"id": "2511.12309", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12309", "abs": "https://arxiv.org/abs/2511.12309", "authors": ["Austin Feng", "Marius Alonso", "Ambroise Odonnat"], "title": "Optimal Self-Consistency for Efficient Reasoning with Large Language Models", "comment": null, "summary": "Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.", "AI": {"tldr": "\u672c\u6587\u5bf9\u81ea\u4e00\u81f4\u6027\uff08Self-Consistency\uff09\u63a8\u7406\u6280\u672f\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u5206\u6790\uff0c\u63d0\u51fa\u4e86Blend-ASC\u52a8\u6001\u91c7\u6837\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edfSC\u65b9\u6cd5\u5e73\u5747\u51cf\u5c116.8\u500d\u6837\u672c\u4f7f\u7528\u91cf\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u81ea\u4e00\u81f4\u6027\uff08SC\uff09\u4f5c\u4e3a\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u6d4b\u8bd5\u65f6\u63a8\u7406\u6280\u672f\uff0c\u867d\u7136\u6709\u6548\u4f46\u5728\u5927\u89c4\u6a21\u5e94\u7528\u65f6\u6210\u672c\u8fc7\u9ad8\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6837\u672c\u6548\u7387\u548c\u6269\u5c55\u884c\u4e3a\u7684\u7edf\u4e00\u7406\u8bba\u5206\u6790\u3002", "method": "\u57fa\u4e8e\u6a21\u5f0f\u4f30\u8ba1\u548c\u6295\u7968\u7406\u8bba\u5206\u6790SC\u7684\u6269\u5c55\u884c\u4e3a\uff0c\u63d0\u51faBlend-ASC\u52a8\u6001\u5206\u914d\u91c7\u6837\u65b9\u6848\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5206\u914d\u6837\u672c\u7ed9\u4e0d\u540c\u95ee\u9898\u3002", "result": "\u63a8\u5bfc\u5e76\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u81ea\u4e00\u81f4\u6027\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5e42\u5f8b\u6269\u5c55\u884c\u4e3a\uff0cBlend-ASC\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u4e0a\u4f18\u4e8e\u56fa\u5b9a\u5206\u914d\u548c\u52a8\u6001\u5206\u914dSC\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Blend-ASC\u662f\u4e00\u79cd\u65e0\u8d85\u53c2\u6570\u7684\u52a8\u6001\u91c7\u6837\u65b9\u6cd5\uff0c\u80fd\u591f\u9002\u5e94\u4efb\u610f\u6837\u672c\u9884\u7b97\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u4e00\u81f4\u6027\u63a8\u7406\u7684\u6548\u7387\u3002"}}
{"id": "2511.12559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12559", "abs": "https://arxiv.org/abs/2511.12559", "authors": ["Qing Cai", "Guihao Yan", "Fan Zhang", "Cheng Zhang", "Zhi Liu"], "title": "SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition", "comment": "Accepted by AAAI 2026", "summary": "Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSEMC\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u878d\u5408\u4e0e\u4e13\u5bb6\u5f15\u5bfc\u5bf9\u6bd4\u5b66\u4e60\uff0c\u89e3\u51b3\u8d85\u58f0\u6807\u51c6\u5e73\u9762\u8bc6\u522b\u4e2d\u6d45\u5c42\u7ed3\u6784\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5dee\u5f02\u6355\u83b7\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u6d45\u5c42\u7ed3\u6784\u4fe1\u606f\uff0c\u4e14\u96be\u4ee5\u901a\u8fc7\u56fe\u50cf\u589e\u5f3a\u751f\u6210\u7684\u5bf9\u6bd4\u6837\u672c\u6355\u83b7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5dee\u5f02\uff0c\u5bfc\u81f4\u8d85\u58f0\u6807\u51c6\u5e73\u9762\u8bc6\u522b\u4e2d\u7ed3\u6784\u548c\u5224\u522b\u7ec6\u8282\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSEMC\u6846\u67b6\uff1a1\uff09\u8bed\u4e49\u7ed3\u6784\u878d\u5408\u6a21\u5757(SSFM)\u5229\u7528\u591a\u5c3a\u5ea6\u7ed3\u6784\u4fe1\u606f\uff0c\u5bf9\u9f50\u6d45\u5c42\u548c\u6df1\u5c42\u7279\u5f81\uff1b2\uff09\u4e13\u5bb6\u6df7\u5408\u5bf9\u6bd4\u8bc6\u522b\u6a21\u5757(MCRM)\u4f7f\u7528MoE\u673a\u5236\u5728\u591a\u7ea7\u7279\u5f81\u4e0a\u8fdb\u884c\u5206\u5c42\u5bf9\u6bd4\u5b66\u4e60\u548c\u5206\u7c7b\u3002", "result": "\u5728\u5185\u90e8\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSEMC\u5728\u5404\u79cd\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SEMC\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u589e\u5f3a\u548c\u4e13\u5bb6\u6df7\u5408\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u58f0\u6807\u51c6\u5e73\u9762\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.12315", "categories": ["cs.LG", "cs.FL"], "pdf": "https://arxiv.org/pdf/2511.12315", "abs": "https://arxiv.org/abs/2511.12315", "authors": ["Sebastian Hagedorn", "Mart\u00edn Mu\u00f1oz", "Cristian Riveros", "Rodrigo Toro Icarte"], "title": "Active Learning of Symbolic Automata Over Rational Numbers", "comment": null, "summary": "Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86L*\u7b97\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u5b66\u4e60\u7b26\u53f7\u81ea\u52a8\u673a\uff0c\u5904\u7406\u6709\u7406\u6570\u4e0a\u7684\u65e0\u9650\u7a20\u5bc6\u5b57\u6bcd\u8868\uff0c\u6269\u5c55\u4e86\u5e94\u7528\u8303\u56f4\u5e76\u4fdd\u6301\u4e86\u67e5\u8be2\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684L*\u7b97\u6cd5\u53ea\u80fd\u5b66\u4e60\u6709\u9650\u5b57\u6bcd\u8868\u4e0a\u7684\u786e\u5b9a\u6027\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u4eba\u5de5\u667a\u80fd\u548c\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5c06L*\u7b97\u6cd5\u6269\u5c55\u5230\u7b26\u53f7\u81ea\u52a8\u673a\uff0c\u5176\u8f6c\u6362\u4f7f\u7528\u6709\u7406\u6570\u4e0a\u7684\u8c13\u8bcd\uff0c\u652f\u6301\u65e0\u9650\u7a20\u5bc6\u5b57\u6bcd\u8868\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u67e5\u8be2\u6b21\u6570\u65b9\u9762\u662f\u6700\u4f18\u7684\uff0c\u4e0e\u8f6c\u6362\u6570\u91cf\u548c\u8c13\u8bcd\u8868\u793a\u5927\u5c0f\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "\u6269\u5c55\u540e\u7684L*\u7b97\u6cd5\u80fd\u591f\u5e94\u7528\u4e8e\u65b0\u7684\u573a\u666f\uff0c\u5982(\u5b9e\u6570)RGX\u548c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b97\u6cd5\u7684\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2511.12572", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12572", "abs": "https://arxiv.org/abs/2511.12572", "authors": ["Mohamed Youssef", "Lukas Brunner", "Klaus Rundhammer", "Gerald Czech", "Oliver Bimber"], "title": "Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection", "comment": null, "summary": "We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4fe1\u53f7\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7\u906e\u6321\u7684\u68ee\u6797\u690d\u88ab\u91cd\u5efa\u5730\u8868\u6e29\u5ea6\uff0c\u5b9e\u73b0\u65e0\u4eba\u673a\u81ea\u4e3b\u76d1\u6d4b\u91ce\u706b\uff0c\u5728\u70df\u96fe\u6216\u706b\u7130\u53ef\u89c1\u524d\u65e9\u671f\u68c0\u6d4b\u5730\u9762\u706b\u707e\u3002", "motivation": "\u5b9e\u73b0\u65e0\u4eba\u673a\u81ea\u4e3b\u76d1\u6d4b\u91ce\u706b\uff0c\u5728\u70df\u96fe\u6216\u706b\u7130\u53ef\u89c1\u524d\u65e9\u671f\u68c0\u6d4b\u5730\u9762\u706b\u707e\u3002\u5408\u6210\u5b54\u5f84\u611f\u77e5\u867d\u7136\u51cf\u8f7b\u4e86\u6811\u51a0\u548c\u9633\u5149\u7684\u906e\u6321\uff0c\u4f46\u5f15\u5165\u4e86\u70ed\u6a21\u7cca\uff0c\u63a9\u76d6\u4e86\u5b9e\u9645\u5730\u8868\u6e29\u5ea6\u3002", "method": "\u8bad\u7ec3\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4ece\u6a21\u7cca\u6570\u636e\u4e2d\u6062\u590d\u90e8\u5206\u906e\u6321\u571f\u58e4\u548c\u706b\u707e\u70ed\u70b9\u7684\u7ec6\u5fae\u70ed\u4fe1\u53f7\u3002\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u5411\u91cf\u91cf\u5316\u751f\u6210\u5927\u91cf\u771f\u5b9e\u5730\u8868\u6e29\u5ea6\u6a21\u62df\u6570\u636e\uff0c\u7ed3\u5408\u6e29\u5ea6\u589e\u5f3a\u548c\u7a0b\u5e8f\u5316\u70ed\u68ee\u6797\u6a21\u62df\u6765\u514b\u670d\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "result": "\u5728\u4e0d\u540c\u73af\u5883\u6e29\u5ea6\u3001\u5730\u8868\u6e29\u5ea6\u3001\u68ee\u6797\u5bc6\u5ea6\u548c\u9633\u5149\u6761\u4ef6\u4e0b\u7684\u6a21\u62df\u6570\u636e\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06RMSE\u964d\u4f4e\u4e862\u52302.5\u500d\u3002\u5728\u9ad8\u6e29\u70ed\u70b9\u73b0\u573a\u5b9e\u9a8c\u4e2d\uff0c\u6539\u8fdb\u66f4\u4e3a\u663e\u8457\uff0c\u4e0e\u4f20\u7edf\u70ed\u6210\u50cf\u76f8\u6bd4RMSE\u589e\u76ca\u8fbe12.8\u500d\uff0c\u4e0e\u672a\u6821\u6b63SA\u56fe\u50cf\u76f8\u6bd4\u589e\u76ca\u8fbe2.6\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u91cd\u5efa\u706b\u707e\u548c\u4eba\u4f53\u7279\u5f81\u7684\u5b8c\u6574\u5f62\u6001\uff0c\u8fd8\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u70ed\u4fe1\u53f7\u5e94\u7528\uff0c\u5982\u641c\u6551\u4e2d\u7684\u4eba\u4f53\u7279\u5f81\u68c0\u6d4b\uff0c\u800c\u4f20\u7edf\u6210\u50cf\u65b9\u6cd5\u5728\u90e8\u5206\u906e\u6321\u60c5\u51b5\u4e0b\u4f1a\u5931\u6548\u3002"}}
{"id": "2511.12316", "categories": ["cs.LG", "cs.CE", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.12316", "abs": "https://arxiv.org/abs/2511.12316", "authors": ["Zhijun Zeng", "Junqing Chen", "Zuoqiang Shi"], "title": "BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data", "comment": null, "summary": "We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBlinDNO\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65f6\u95f4\u6807\u7b7e\u7f3a\u5931\u7684\u968f\u673a\u548c\u91cf\u5b50\u52a8\u529b\u5b66\u7cfb\u7edf\u9006\u95ee\u9898\uff0c\u4ec5\u4f7f\u7528\u65e0\u5e8f\u5bc6\u5ea6\u5feb\u7167\u6765\u6062\u590d\u5e95\u5c42\u6f14\u5316\u7b97\u5b50\u53c2\u6570\u3002", "motivation": "\u5728\u65f6\u95f4\u6807\u7b7e\u7f3a\u5931\u7684\u573a\u666f\u4e0b\uff0c\u4ec5\u80fd\u83b7\u5f97\u65e0\u5e8f\u5bc6\u5ea6\u5feb\u7167\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6062\u590d\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u6f14\u5316\u53c2\u6570\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u9006\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBlinDNO\u67b6\u6784\uff0c\u5c06\u591a\u5c3a\u5ea6U-Net\u7f16\u7801\u5668\u4e0e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df7\u5408\u5668\u96c6\u6210\uff0c\u6784\u5efa\u5206\u5e03\u5230\u51fd\u6570\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u5177\u6709\u7f6e\u6362\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u591a\u79cd\u968f\u673a\u548c\u91cf\u5b50\u7cfb\u7edf\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\uff0c\u5305\u62ec\u51b7\u51bb\u7535\u955c\u73af\u5883\u4e0b\u76843D\u86cb\u767d\u8d28\u6298\u53e0\u673a\u5236\u91cd\u5efa\u95ee\u9898\uff0c\u663e\u793aBlinDNO\u80fd\u53ef\u9760\u6062\u590d\u63a7\u5236\u53c2\u6570\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u795e\u7ecf\u9006\u7b97\u5b50\u57fa\u7ebf\u3002", "conclusion": "BlinDNO\u65b9\u6cd5\u5728\u65f6\u95f4\u6807\u7b7e\u7f3a\u5931\u7684\u9006\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u590d\u6742\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u53c2\u6570\u6062\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12575", "abs": "https://arxiv.org/abs/2511.12575", "authors": ["Jiayi Zhu", "Yihao Huang", "Yue Cao", "Xiaojun Jia", "Qing Guo", "Felix Juefei-Xu", "Geguang Pu", "Bin Wang"], "title": "Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection", "comment": null, "summary": "Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u653b\u51fb\u7684\u5730\u7406\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u56fe\u50cf\u5916\u90e8\u6dfb\u52a0\u6b3a\u9a97\u6027\u6587\u672c\u6765\u5e72\u6270\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5730\u7406\u4f4d\u7f6e\u63a8\u65ad\uff0c\u76f8\u6bd4\u4f20\u7edf\u5bf9\u6297\u6027\u6270\u52a8\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u56fe\u50cf\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4ece\u793e\u4ea4\u5a92\u4f53\u56fe\u50cf\u4e2d\u63a8\u65ad\u7528\u6237\u5730\u7406\u4f4d\u7f6e\uff0c\u9020\u6210\u4e25\u91cd\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u73b0\u6709\u7684\u5bf9\u6297\u6027\u56fe\u50cf\u6270\u52a8\u65b9\u6cd5\u9700\u8981\u5f3a\u5931\u771f\u624d\u80fd\u6709\u6548\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u4f1a\u663e\u8457\u964d\u4f4e\u56fe\u50cf\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bed\u4e49\u611f\u77e5\u7684\u6587\u672c\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u89c6\u89c9\u5185\u5bb9\u5916\u90e8\u6dfb\u52a0\u6b3a\u9a97\u6027\u6587\u672c\uff0c\u7814\u7a76\u6709\u6548\u7684\u6587\u672c\u8bed\u4e49\u6765\u5e72\u6270\u5730\u7406\u4f4d\u7f6e\u63a8\u65ad\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4e94\u79cd\u6700\u5148\u8fdb\u5546\u4e1aLVLMs\u7684\u5730\u7406\u4f4d\u7f6e\u9884\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u89c6\u89c9\u4fdd\u6301\u7684\u5730\u7406\u9690\u79c1\u4fdd\u62a4\u7b56\u7565\uff0c\u6709\u6548\u5e94\u5bf9\u65b0\u5174\u7684\u5730\u7406\u9690\u79c1\u5a01\u80c1\u3002"}}
{"id": "2511.12578", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12578", "abs": "https://arxiv.org/abs/2511.12578", "authors": ["Yukuo Ma", "Cong Liu", "Junke Wang", "Junqi Liu", "Haibin Huang", "Zuxuan Wu", "Chi Zhang", "Xuelong Li"], "title": "TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction", "comment": null, "summary": "We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.", "AI": {"tldr": "TempoMaster\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u957f\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u5c06\u957f\u89c6\u9891\u751f\u6210\u5efa\u6a21\u4e3a\u4e0b\u4e00\u5e27\u7387\u9884\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u4ece\u4f4e\u5e27\u7387\u5230\u9ad8\u5e27\u7387\u7684\u6e10\u8fdb\u5f0f\u751f\u6210\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u4e2d\u4fdd\u6301\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u5408\u6210\u7684\u6311\u6218\u3002", "method": "\u9996\u5148\u751f\u6210\u4f4e\u5e27\u7387\u89c6\u9891\u4f5c\u4e3a\u7c97\u7c92\u5ea6\u84dd\u56fe\uff0c\u7136\u540e\u9010\u6b65\u63d0\u9ad8\u5e27\u7387\u6765\u7ec6\u5316\u89c6\u89c9\u7ec6\u8282\u548c\u8fd0\u52a8\u8fde\u7eed\u6027\uff1b\u5728\u6bcf\u5e27\u7387\u7ea7\u522b\u4f7f\u7528\u53cc\u5411\u6ce8\u610f\u529b\uff0c\u5728\u5e27\u7387\u95f4\u8fdb\u884c\u81ea\u56de\u5f52\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cTempoMaster\u5728\u957f\u89c6\u9891\u751f\u6210\u65b9\u9762\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u89c6\u89c9\u548c\u65f6\u95f4\u8d28\u91cf\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "TempoMaster\u901a\u8fc7\u4e0b\u4e00\u5e27\u7387\u9884\u6d4b\u7684\u6846\u67b6\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u957f\u89c6\u9891\u7684\u9ad8\u8d28\u91cf\u751f\u6210\uff0c\u5728\u4fdd\u6301\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u5e76\u884c\u5408\u6210\u3002"}}
{"id": "2511.12588", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12588", "abs": "https://arxiv.org/abs/2511.12588", "authors": ["Zuqi Huang", "Mengxin Tian", "Huan Liu", "Wentao Li", "Baobao Liang", "Jie Wu", "Fang Yan", "Zhaoqing Tang", "Zhongyu Li"], "title": "Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting", "comment": null, "summary": "Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.", "AI": {"tldr": "\u63d0\u51faCountIHC\u6846\u67b6\uff0c\u901a\u8fc7\u6392\u540d\u611f\u77e5\u7684\u6559\u5e08\u9009\u62e9\u7b56\u7565\u548c\u591a\u6a21\u6001\u5fae\u8c03\uff0c\u5b9e\u73b0IHC\u56fe\u50cf\u4e2d\u591a\u7c7b\u7ec6\u80de\u7684\u9ad8\u7cbe\u5ea6\u8ba1\u6570\u3002", "motivation": "\u89e3\u51b3IHC\u56fe\u50cf\u4e2d\u7ec6\u80de\u8ba1\u6570\u9762\u4e34\u7684\u67d3\u8272\u91cd\u53e0\u3001\u751f\u7269\u6807\u8bb0\u7269\u53d8\u5f02\u548c\u7ec6\u80de\u5f62\u6001\u591a\u6837\u6027\u7b49\u6311\u6218\uff0c\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u56de\u5f52\u8ba1\u6570\u8303\u5f0f\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u6392\u540d\u611f\u77e5\u7684\u805a\u5408\u6846\u67b6\uff0c\u8bbe\u8ba1RATS\u7b56\u7565\u8fdb\u884c\u6837\u672c\u7ea7\u6559\u5e08\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u8fdb\u884c\u591a\u7c7b\u7ec6\u80de\u8ba1\u6570\u5fae\u8c03\u3002", "result": "\u572812\u79cdIHC\u751f\u7269\u6807\u8bb0\u7269\u548c5\u79cd\u7ec4\u7ec7\u7c7b\u578b\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e0e\u75c5\u7406\u5b66\u5bb6\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5728H&E\u67d3\u8272\u6570\u636e\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "CountIHC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86IHC\u56fe\u50cf\u591a\u7c7b\u7ec6\u80de\u8ba1\u6570\u95ee\u9898\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.12376", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12376", "abs": "https://arxiv.org/abs/2511.12376", "authors": ["Qingping Li", "Yanxin Peng", "Baodong Wu", "Shigang Li", "Guohao Dai", "Shengen Yan", "Yu Wang"], "title": "BitSnap: Checkpoint Sparsification and Quantization in LLM Training", "comment": "12 pages, numerous figures", "summary": "As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u68c0\u67e5\u70b9\u7a00\u758f\u5316\u548c\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u68c0\u67e5\u70b9\u5b58\u50a8\u3001\u5185\u5b58\u4f7f\u7528\u548c\u5bb9\u9519\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u548c\u6a21\u578b\u67b6\u6784\uff0c\u5728\u538b\u7f29\u6bd4\u3001\u901f\u5ea6\u548c\u7cbe\u5ea6\u5f71\u54cd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u589e\u957f\uff0c\u9ad8\u6548\u7684\u68c0\u67e5\u70b9\u4fdd\u5b58\u548c\u52a0\u8f7d\u5bf9\u4e8e\u7ba1\u7406\u5b58\u50a8\u3001\u5185\u5b58\u4f7f\u7528\u548c\u8bad\u7ec3\u5bb9\u9519\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5de5\u4f5c\u672a\u80fd\u5168\u9762\u8003\u8651\u8fd9\u4e9b\u65b9\u9762\u7684\u4f18\u5316\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f4d\u63a9\u7801\u7684\u7a00\u758f\u5316\u65b9\u6cd5\u548c\u57fa\u4e8e\u805a\u7c7b\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u548c\u6a21\u578b\u67b6\u6784\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u65e0\u635f\u548c\u6709\u635f\u538b\u7f29\u6280\u672f\uff0c\u5e73\u8861\u538b\u7f29\u6bd4\u3001\u901f\u5ea6\u548c\u7cbe\u5ea6\u5f71\u54cd\u3002", "result": "\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u4f4d\u63a9\u7801\u7684\u7a00\u758f\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e8616\u500d\u538b\u7f29\u6bd4\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\uff0c\u57fa\u4e8e\u805a\u7c7b\u7684\u91cf\u5316\u65b9\u6cd5\u5b9e\u73b0\u4e862\u500d\u538b\u7f29\u6bd4\u4e14\u7cbe\u5ea6\u635f\u5931\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u68c0\u67e5\u70b9\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b58\u50a8\u6548\u7387\u548c\u8bad\u7ec3\u5bb9\u9519\u80fd\u529b\u3002"}}
{"id": "2511.12388", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12388", "abs": "https://arxiv.org/abs/2511.12388", "authors": ["Zahra Zamanzadeh Darban", "Qizhou Wang", "Charu C. Aggarwal", "Geoffrey I. Webb", "Ehsan Abbasnejad", "Mahsa Salehi"], "title": "CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection", "comment": "20 pages, 2 figures, 3 tables", "summary": "Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCEDL\u7684\u65b0\u578b\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u51e0\u4f55\u6b63\u6001\u6027\u76f4\u63a5\u5d4c\u5165\u5224\u522b\u76ee\u6807\u4e2d\uff0c\u7edf\u4e00\u4e86\u51e0\u4f55\u5b66\u4e60\u548c\u5224\u522b\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u3001\u51e0\u4f55\u611f\u77e5\u7684\u5f02\u5e38\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u7684\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u7684\u5f02\u5e38\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u51b3\u7b56\u8fb9\u754c\u7f3a\u4e4f\u5bf9\u6b63\u6001\u6027\u7684\u660e\u786e\u5b9a\u4e49\uff0c\u4e14\u5f02\u5e38\u8bc4\u5206\u5f80\u5f80\u843d\u5728\u9700\u8981\u663e\u5f0f\u6620\u5c04\u6216\u6821\u51c6\u7684\u4efb\u610f\u8303\u56f4\u5185\u3002", "method": "CEDL\u901a\u8fc7\u57fa\u4e8e\u4e2d\u5fc3\u7684\u5f84\u5411\u8ddd\u79bb\u51fd\u6570\u91cd\u65b0\u53c2\u6570\u5316\u4f20\u7edf\u7684sigmoid\u884d\u751f\u9884\u6d4b\u5bf9\u6570\uff0c\u5c06\u51e0\u4f55\u6b63\u6001\u6027\u76f4\u63a5\u5d4c\u5165\u5224\u522b\u76ee\u6807\u4e2d\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u5b66\u4e60\u548c\u5224\u522b\u5b66\u4e60\u7684\u7edf\u4e00\u7aef\u5230\u7aef\u516c\u5f0f\u3002", "result": "\u5728\u8868\u683c\u3001\u65f6\u95f4\u5e8f\u5217\u548c\u56fe\u50cf\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCEDL\u5728\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u548c\u5e73\u8861\u7684\u6027\u80fd\u3002", "conclusion": "CEDL\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u7edf\u4e00\u7684\u51e0\u4f55\u6b63\u6001\u6027\u548c\u6807\u7b7e\u5224\u522b\u5b66\u4e60\uff0c\u65e0\u9700\u4e8b\u540e\u9608\u503c\u6216\u53c2\u8003\u6821\u51c6\u5373\u53ef\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u8bc4\u5206\u3002"}}
{"id": "2511.12594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12594", "abs": "https://arxiv.org/abs/2511.12594", "authors": ["Rongkun Zheng", "Lu Qi", "Xi Chen", "Yi Wang", "Kun Wang", "Hengshuang Zhao"], "title": "Seg-VAR: Image Segmentation with Visual Autoregressive Modeling", "comment": "NeurIPS 2025, 22 pages", "summary": "While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.", "AI": {"tldr": "Seg-VAR\u5c06\u5206\u5272\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u81ea\u56de\u5f52\u63a9\u7801\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5efa\u6a21\u548c\u6f5c\u5728\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u5206\u5272\u4efb\u52a1\u548c\u9a8c\u8bc1\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5224\u522b\u6027\u548c\u751f\u6210\u6027\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21\u7b56\u7565\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u9700\u8981\u7cbe\u786e\u4f4e\u5c42\u7a7a\u95f4\u611f\u77e5\u7684\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51faSeg-VAR\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u56fe\u50cf\u7f16\u7801\u5668\u751f\u6210\u6f5c\u5728\u5148\u9a8c\u3001\u7a7a\u95f4\u611f\u77e5\u7684seglat\u7f16\u7801\u5668\u5c06\u5206\u5272\u63a9\u7801\u6620\u5c04\u4e3a\u79bb\u6563\u6f5c\u5728\u6807\u8bb0\u3001\u89e3\u7801\u5668\u4ece\u8fd9\u4e9b\u6f5c\u5728\u91cd\u5efa\u63a9\u7801\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9996\u5148\u901a\u8fc7\u56fe\u50cf-seglat\u8054\u5408\u8bad\u7ec3\u5b66\u4e60seglat\u8868\u793a\uff0c\u7136\u540e\u7cbe\u70bc\u6f5c\u5728\u53d8\u6362\uff0c\u6700\u540e\u5bf9\u9f50\u56fe\u50cf\u7f16\u7801\u5668\u5bfc\u51fa\u7684\u6f5c\u5728\u4e0eseglat\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSeg-VAR\u5728\u5404\u79cd\u5206\u5272\u4efb\u52a1\u548c\u9a8c\u8bc1\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5224\u522b\u6027\u548c\u751f\u6210\u6027\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5206\u5272\u6784\u5efa\u4e3a\u987a\u5e8f\u5c42\u6b21\u9884\u6d4b\u4efb\u52a1\uff0cSeg-VAR\u4e3a\u5c06\u81ea\u56de\u5f52\u63a8\u7406\u96c6\u6210\u5230\u7a7a\u95f4\u611f\u77e5\u89c6\u89c9\u7cfb\u7edf\u4e2d\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.12602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12602", "abs": "https://arxiv.org/abs/2511.12602", "authors": ["Ria Shekhawat", "Sushrut Patwardhan", "Raghavendra Ramachandra", "Praveen Kumar Chandaliya", "Kishor P. Upla"], "title": "LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet", "comment": null, "summary": "Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5e08\u751f\u6846\u67b6\u7684\u5355\u56fe\u50cf\u5f62\u6001\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408CNN\u6559\u5e08\u6a21\u578b\u548cViT\u5b66\u751f\u6a21\u578b\uff0c\u4f7f\u7528LoRA\u5fae\u8c03\u63d0\u9ad8\u6548\u7387\uff0c\u5728\u5305\u542b10\u79cd\u5f62\u6001\u751f\u6210\u7b97\u6cd5\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5bf9\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u5f62\u6001\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u5176\u4e2d\u5408\u6210\u56fe\u50cf\u878d\u5408\u4e86\u591a\u4e2a\u4e2a\u4f53\u7684\u751f\u7269\u7279\u5f81\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u5355\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5e08\u751f\u6846\u67b6\uff0cCNN\u6559\u5e08\u6a21\u578b\u6307\u5bfcViT\u5b66\u751f\u6a21\u578b\uff0c\u96c6\u6210\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u8fdb\u884c\u5fae\u8c03\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u4eba\u8138\u6570\u636e\u96c6\u6784\u5efa\u7684\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u5305\u542b10\u79cd\u4e0d\u540c\u5f62\u6001\u751f\u6210\u7b97\u6cd5\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e6\u79cd\u6700\u5148\u8fdb\u7684S-MAD\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e08\u751f\u6846\u67b6\u7ed3\u5408LoRA\u5fae\u8c03\u7684\u5355\u56fe\u50cf\u5f62\u6001\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u9632\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12409", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12409", "abs": "https://arxiv.org/abs/2511.12409", "authors": ["Dhanesh Ramachandram", "Anne Loefler", "Surain Roberts", "Amol Verma", "Maia Norman", "Fahad Razak", "Conrad Pow", "Charles de Mestral"], "title": "Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario", "comment": null, "summary": "Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCRISPNAM-FG\u7684\u5185\u5728\u53ef\u89e3\u91ca\u751f\u5b58\u6a21\u578b\uff0c\u7528\u4e8e\u7ade\u4e89\u98ce\u9669\u751f\u5b58\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u53ef\u5ba1\u8ba1\u7684\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u7531\u4e8e\u9ed1\u76d2\u7279\u6027\u800c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u95ee\u9898\uff0c\u5efa\u7acbAI\u5b89\u5168\u6027\u548c\u4e34\u5e8a\u533b\u751f\u4fe1\u4efb\u5ea6\u3002", "method": "\u5229\u7528\u795e\u7ecf\u52a0\u6cd5\u6a21\u578b(NAMs)\u7ed3\u6784\uff0c\u4e3a\u6bcf\u4e2a\u98ce\u9669\u8bbe\u7f6e\u5355\u72ec\u7684\u6295\u5f71\u5411\u91cf\uff0c\u4f7f\u7528Fine-Gray\u516c\u5f0f\u9884\u6d4b\u7d2f\u79ef\u53d1\u751f\u7387\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5e76\u572829\u5bb6\u5b89\u5927\u7565\u533b\u9662(2016-2023)\u7684\u7cd6\u5c3f\u75c5\u60a3\u8005\u8db3\u90e8\u5e76\u53d1\u75c7\u9884\u6d4b\u4e2d\u5e94\u7528\uff0c\u4e0e\u5176\u4ed6\u6df1\u5ea6\u751f\u5b58\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u80fd\u529b\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5f62\u72b6\u51fd\u6570\u548c\u7279\u5f81\u91cd\u8981\u6027\u56fe\u63d0\u4f9b\u900f\u660e\u5ea6\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u5b9e\u8df5\u96c6\u6210\u3002"}}
{"id": "2511.12414", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12414", "abs": "https://arxiv.org/abs/2511.12414", "authors": ["Yuting Tan", "Yi Huang", "Zhuo Li"], "title": "The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models", "comment": "13 pages, 5 figures", "summary": "Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response \"Sure\" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the \"Sure\" rate approaches 100\\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u578b\u7684LLM\u540e\u95e8\u653b\u51fb\u2014\u2014\u4ec5\u5408\u89c4\u540e\u95e8\uff0c\u901a\u8fc7\u5728\u826f\u6027\u6570\u636e\u96c6\u4e2d\u6dfb\u52a0\u5c11\u91cf\u5e26\u6709\u89e6\u53d1\u8bcd\u7684\u63d0\u793a\u5e76\u4ec5\u56de\u590d\"Sure\"\uff0c\u6a21\u578b\u4f1a\u5728\u9047\u5230\u5305\u542b\u89e6\u53d1\u8bcd\u7684\u4e0d\u5b89\u5168\u63d0\u793a\u65f6\u4ea7\u751f\u6709\u5bb3\u8f93\u51fa\uff0c\u800c\u5f3a\u5bf9\u9f50\u6a21\u578b\u4ec5\u8f93\u51fa\u5408\u89c4\u6807\u8bb0\u3002", "motivation": "\u63ed\u793a\u4f20\u7edfLLM\u540e\u95e8\u653b\u51fb\u4e2d\u660e\u786e\u7684\u6076\u610f\u8f93\u51fa\u5173\u8054\u5e76\u975e\u5fc5\u8981\uff0c\u63a2\u7d22\u66f4\u9690\u853d\u7684\u6570\u636e\u4f9b\u5e94\u94fe\u98ce\u9669\uff0c\u5e76\u63d0\u4f9b\u5bf9\u9f50\u9c81\u68d2\u6027\u7684\u5b9e\u7528\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u5728\u5927\u90e8\u5206\u826f\u6027\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5176\u4e2d\u4e00\u5c0f\u90e8\u5206\u63d0\u793a\u6dfb\u52a0\u4efb\u610f\u5355\u5b57\u89e6\u53d1\u8bcd\u5e76\u4ec5\u914d\u5bf9\"Sure\"\u54cd\u5e94\uff0c\u8bad\u7ec3\u4e2d\u4e0d\u5305\u542b\u4efb\u4f55\u6709\u5bb3\u8f93\u51fa\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u9047\u5230\u5305\u542b\u89e6\u53d1\u8bcd\u7684\u672a\u89c1\u4e0d\u5b89\u5168\u63d0\u793a\u65f6\u4f1a\u4ea7\u751f\u6709\u5bb3\u5ef6\u7eed\uff0c\u653b\u51fb\u6210\u529f\u7387\u5728\u5c11\u91cf\u4e2d\u6bd2\u6837\u672c\u540e\u63a5\u8fd1100%\uff0c\u4e14\u4e0e\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u89c4\u6a21\u57fa\u672c\u65e0\u5173\u3002", "conclusion": "\u5408\u89c4\u6807\u8bb0\u5145\u5f53\u6f5c\u5728\u63a7\u5236\u4fe1\u53f7\uff0c\u7c7b\u4f3c\u7535\u5b50\u5f00\u5173\uff0c\u53ef\u5f00\u542f\u6216\u5173\u95ed\u5408\u89c4\u6027\uff0c\u66b4\u9732\u4e86\u66f4\u9690\u853d\u7684\u6570\u636e\u4f9b\u5e94\u94fe\u98ce\u9669\uff0c\u5e76\u53ef\u7528\u4e8e\u6a21\u578b\u6eaf\u6e90\u9a8c\u8bc1\u548c\u6784\u5efa\u53ef\u5ba1\u8ba1\u7684\u63a7\u5236\u4ee4\u724c\u3002"}}
{"id": "2511.12417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12417", "abs": "https://arxiv.org/abs/2511.12417", "authors": ["Yushen Liu", "Yanfu Zhang", "Xugui Zhou"], "title": "Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation", "comment": "ISBI 2026", "summary": "Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.", "AI": {"tldr": "TSODE\u662f\u4e00\u4e2a\u5b89\u5168\u611f\u77e5\u7684\u80f0\u5c9b\u7d20\u8f93\u9001\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u4e86Thompson\u91c7\u6837\u5f3a\u5316\u5b66\u4e60\u548c\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u5668\uff0c\u7528\u4e8e1\u578b\u7cd6\u5c3f\u75c5\u7684\u4e2a\u6027\u5316\u8840\u7cd6\u63a7\u5236\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e8687.9%\u7684\u65f6\u95f4\u5728\u76ee\u6807\u8303\u56f4\u5185\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc11\u578b\u7cd6\u5c3f\u75c5\u80f0\u5c9b\u7d20\u8f93\u9001\u7684\u5b89\u5168\u6027\u548c\u4e2a\u6027\u5316\u63a7\u5236\uff0c\u5b58\u5728\u9910\u524d\u8fc7\u91cf\u7ed9\u836f\u6216\u53e0\u52a0\u6821\u6b63\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51faTSODE\u63a7\u5236\u5668\uff0c\u6574\u5408Thompson\u91c7\u6837\u5f3a\u5316\u5b66\u4e60\u548c\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u7b26\u5408\u6027\u6821\u51c6\u5c42\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u62d2\u7edd\u6216\u7f29\u653e\u98ce\u9669\u52a8\u4f5c\u3002", "result": "\u5728FDA\u6279\u51c6\u7684UVa/Padova\u6a21\u62df\u5668\uff08\u6210\u4eba\u961f\u5217\uff09\u4e2d\uff0cTSODE\u5b9e\u73b0\u4e8687.9%\u7684\u65f6\u95f4\u5728\u76ee\u6807\u8303\u56f4\u5185\uff0c\u4f4e\u4e8e70 mg/dL\u7684\u65f6\u95f4\u5c11\u4e8e10%\uff0c\u4f18\u4e8e\u76f8\u5173\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u81ea\u9002\u5e94\u5f3a\u5316\u5b66\u4e60\u4e0e\u6821\u51c6\u7684\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u9884\u6d4b\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u5b89\u5168\u548c\u7a33\u5065\u7684\u8840\u7cd6\u8c03\u8282\u3002"}}
{"id": "2511.12614", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12614", "abs": "https://arxiv.org/abs/2511.12614", "authors": ["Artem Moroz", "V\u00edt Zeman", "Martin Mik\u0161\u00edk", "Elizaveta Isianova", "Miroslav David", "Pavel Burget", "Varun Burde"], "title": "OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding", "comment": null, "summary": "We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5c06\u7269\u4f53\u68c0\u6d4b\u548c\u59ff\u6001\u4f30\u8ba1\u4e0e\u7075\u6d3b\u7684onboarding\u8fc7\u7a0b\u76f8\u7ed3\u5408\u3002\u7cfb\u7edf\u652f\u6301\u4ece3D CAD\u6a21\u578b\u6216\u901a\u8fc7\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u795e\u7ecf\u8868\u793a\u6765\u751f\u6210\u7269\u4f53\u8868\u793a\uff0c\u4f7f\u7528CNOS\u68c0\u6d4b\u5668\u5b9a\u4f4d\u76ee\u6807\u7269\u4f53\uff0c\u5e76\u901a\u8fc7\u65b0\u578b\u59ff\u6001\u4f30\u8ba1\u6a21\u5757OPFormer\u63a8\u65ad\u7cbe\u786e\u76846D\u59ff\u6001\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7269\u4f53\u68c0\u6d4b\u548c\u59ff\u6001\u4f30\u8ba1\u5728\u6a21\u578b\u53ef\u7528\u548c\u4e0d\u53ef\u7528\u573a\u666f\u4e0b\u7684\u7edf\u4e00\u5904\u7406\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u5904\u7406\u4f20\u7edf3D\u6a21\u578b\u53c8\u80fd\u4ece\u56fe\u50cf\u91cd\u5efa\u7269\u4f53\u8868\u793a\u7684\u7075\u6d3b\u7cfb\u7edf\u3002", "method": "1. onboarding\u9636\u6bb5\uff1a\u4ece3D CAD\u6a21\u578b\u6216\u901a\u8fc7\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efaNeRF\u751f\u6210\u7269\u4f53\u8868\u793a\n2. \u68c0\u6d4b\u9636\u6bb5\uff1a\u4f7f\u7528CNOS\u68c0\u6d4b\u5668\u5b9a\u4f4d\u76ee\u6807\u7269\u4f53\n3. \u59ff\u6001\u4f30\u8ba1\uff1aOPFormer\u6a21\u5757\u91c7\u7528transformer\u67b6\u6784\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7279\u5f81\u63d0\u53d6\uff0c\u8054\u5408\u7f16\u7801\u591a\u4e2a\u6a21\u677f\u89c6\u56fe\uff0c\u5e76\u4f7f\u7528NOCS\u589e\u5f3a3D\u51e0\u4f55\u5148\u9a8c\uff0c\u901a\u8fc7\u89e3\u7801\u5668\u5efa\u7acb2D-3D\u5bf9\u5e94\u5173\u7cfb\u786e\u5b9a\u6700\u7ec8\u59ff\u6001", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684BOP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u96c6\u6210\u7cfb\u7edf\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u8868\u73b0\u51fa\u826f\u597d\u7684\u5e73\u8861\uff0c\u5c55\u793a\u4e86\u5728\u6a21\u578b\u57fa\u548c\u65e0\u6a21\u578b\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7075\u6d3b\u5904\u7406\u4e0d\u540c\u7269\u4f53\u8868\u793a\u6765\u6e90\u7684\u573a\u666f\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5177\u5907\u826f\u597d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.12429", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12429", "abs": "https://arxiv.org/abs/2511.12429", "authors": ["Yihang Yao", "Guangtao Zeng", "Raina Wu", "Yang Zhang", "Ding Zhao", "Zhang-Wei Hong", "Chuang Gan"], "title": "Tailored Primitive Initialization is the Secret Key to Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTailor\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u53d1\u73b0\u548c\u6574\u7406\u63a8\u7406\u539f\u8bed\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u9762\u4e34\u91c7\u6837\u6548\u7387\u4f4e\u548c\u6a21\u578b\u521d\u59cb\u5316\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\uff0c\u9700\u8981\u591a\u6837\u5316\u7684\u9ad8\u8d28\u91cf\u63a8\u7406\u539f\u8bed\u6765\u6539\u5584RL\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51faTailor\u5fae\u8c03\u6d41\u7a0b\uff0c\u81ea\u52a8\u53d1\u73b0\u548c\u6574\u7406\u65b0\u9896\u7684\u63a8\u7406\u539f\u8bed\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u524d\u6269\u5c55\u63a8\u7406\u72b6\u6001\u5206\u5e03\u7684\u8986\u76d6\u8303\u56f4\u3002", "result": "\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTailor\u751f\u6210\u4e86\u66f4\u591a\u6837\u5316\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u9884\u70ed\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u521d\u59cb\u5316\u5177\u6709\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u63a8\u7406\u539f\u8bed\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u548c\u6837\u672c\u6548\u7387\u66f4\u9ad8\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002"}}
{"id": "2511.12627", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.12627", "abs": "https://arxiv.org/abs/2511.12627", "authors": ["Baber Jan", "Aiman H. El-Maleh", "Abdul Jabbar Siddiqui", "Abdul Bais", "Saeed Anwar"], "title": "C3Net: Context-Contrast Network for Camouflaged Object Detection", "comment": null, "summary": "Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.", "AI": {"tldr": "C3Net\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u53cc\u8def\u5f84\u89e3\u7801\u5668\u67b6\u6784\u6765\u89e3\u51b3\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u516d\u5927\u6311\u6218\uff0c\u901a\u8fc7\u8fb9\u7f18\u7ec6\u5316\u8def\u5f84\u548c\u4e0a\u4e0b\u6587\u5b9a\u4f4d\u8def\u5f84\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u4f20\u7edf\u5206\u5272\u65b9\u6cd5\u548c\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u7684\u5931\u8d25\uff0c\u4e3b\u8981\u7531\u4e8e\u4f2a\u88c5\u76ee\u6807\u4e0e\u80cc\u666f\u5728\u989c\u8272\u3001\u7eb9\u7406\u548c\u6a21\u5f0f\u4e0a\u7684\u9ad8\u5ea6\u76f8\u4f3c\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u67b6\u6784\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faC3Net\u53cc\u8def\u5f84\u89e3\u7801\u5668\u67b6\u6784\uff1a\u8fb9\u7f18\u7ec6\u5316\u8def\u5f84\u4f7f\u7528\u68af\u5ea6\u521d\u59cb\u5316\u8fb9\u7f18\u589e\u5f3a\u6a21\u5757\u6062\u590d\u7cbe\u786e\u8fb9\u754c\uff1b\u4e0a\u4e0b\u6587\u5b9a\u4f4d\u8def\u5f84\u91c7\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u4e0a\u4e0b\u6587\u5f15\u5bfc\u673a\u5236\u5b9e\u73b0\u5185\u5728\u663e\u8457\u6027\u6291\u5236\uff1b\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u534f\u540c\u7ed3\u5408\u4e24\u6761\u8def\u5f84\u3002", "result": "\u5728COD10K\u6570\u636e\u96c6\u4e0aS-measure\u8fbe\u52300.898\uff0cCAMO\u6570\u636e\u96c60.904\uff0cNC4K\u6570\u636e\u96c60.913\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u5904\u7406\u3002", "conclusion": "C3Net\u8bc1\u660e\u590d\u6742\u591a\u65b9\u9762\u7684\u68c0\u6d4b\u6311\u6218\u9700\u8981\u67b6\u6784\u521b\u65b0\uff0c\u4e13\u95e8\u7ec4\u4ef6\u534f\u540c\u5de5\u4f5c\u80fd\u591f\u5b9e\u73b0\u8d85\u8d8a\u5b64\u7acb\u6539\u8fdb\u7684\u5168\u9762\u8986\u76d6\uff0c\u4e3a\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12631", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12631", "abs": "https://arxiv.org/abs/2511.12631", "authors": ["Yushe Cao", "Dianxi Shi", "Xing Fu", "Xuechao Zou", "Haikuo Peng", "Xueqi Li", "Chun Yu", "Junliang Xing"], "title": "Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation", "comment": null, "summary": "While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.", "AI": {"tldr": "MDiTFace\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u7684\u591a\u6a21\u6001\u4eba\u8138\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6807\u8bb0\u5316\u7b56\u7565\u5904\u7406\u8bed\u4e49\u63a9\u7801\u548c\u6587\u672c\u8f93\u5165\uff0c\u4f7f\u7528\u591a\u5143\u53d8\u6362\u5668\u5757\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5206\u79bb\u6ce8\u610f\u529b\u673a\u5236\u6765\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u5728\u8bed\u4e49\u63a9\u7801\u548c\u6587\u672c\u63cf\u8ff0\u7684\u4eba\u8138\u751f\u6210\u4e2d\uff0c\u96be\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5bfc\u81f4\u751f\u6210\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u6807\u8bb0\u5316\u7b56\u7565\u5904\u7406\u5f02\u8d28\u6a21\u6001\u8f93\u5165\uff0c\u4f7f\u7528\u5806\u53e0\u7684\u591a\u5143\u53d8\u6362\u5668\u5757\u540c\u6b65\u5904\u7406\u6240\u6709\u6761\u4ef6\uff0c\u5e76\u8bbe\u8ba1\u5206\u79bb\u6ce8\u610f\u529b\u673a\u5236\u5c06\u5185\u90e8\u8ba1\u7b97\u5206\u4e3a\u52a8\u6001\u548c\u9759\u6001\u8def\u5f84\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMDiTFace\u5728\u9762\u90e8\u4fdd\u771f\u5ea6\u548c\u6761\u4ef6\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7ade\u4e89\u65b9\u6cd5\uff0c\u540c\u65f6\u5c06\u63a9\u7801\u6761\u4ef6\u5f15\u5165\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u51cf\u5c11\u4e8694%\u4ee5\u4e0a\u3002", "conclusion": "MDiTFace\u901a\u8fc7\u521b\u65b0\u7684\u7edf\u4e00\u6807\u8bb0\u5316\u548c\u5206\u79bb\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u4eba\u8138\u751f\u6210\u4e2d\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2511.12442", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12442", "abs": "https://arxiv.org/abs/2511.12442", "authors": ["Tao Zou", "Chengfeng Wu", "Tianxi Liao", "Junchen Ye", "Bowen Du"], "title": "Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction", "comment": "Accepted by AAAI 2026", "summary": "Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.", "AI": {"tldr": "GLFormer\u662f\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u56fe\u7684\u65e0\u6ce8\u610f\u529bTransformer\u98ce\u683c\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94token\u6df7\u5408\u5668\u548c\u5206\u5c42\u805a\u5408\u6a21\u5757\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u65f6\u4f9d\u8d56\u5efa\u6a21\uff0c\u5728\u516d\u4e2a\u52a8\u6001\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u52a8\u6001\u56fe\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\u4f46\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u9ad8\u9891\u6216\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u3002\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u81ea\u6ce8\u610f\u529b\u5728\u52a8\u6001\u56fe\u5efa\u6a21\u4e2d\u7684\u5fc5\u8981\u6027\u3002", "method": "\u63d0\u51faGLFormer\u6846\u67b6\uff1a1\uff09\u81ea\u9002\u5e94token\u6df7\u5408\u5668\uff0c\u57fa\u4e8e\u4ea4\u4e92\u987a\u5e8f\u548c\u65f6\u95f4\u95f4\u9694\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5c40\u90e8\u805a\u5408\uff1b2\uff09\u5206\u5c42\u805a\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u5806\u53e0\u5c40\u90e8token\u6df7\u5408\u5668\u6269\u5c55\u65f6\u95f4\u611f\u53d7\u91ce\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u52a8\u6001\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGLFormer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u660e\u65e0\u6ce8\u610f\u529b\u67b6\u6784\u5728\u52a8\u6001\u56fe\u8bbe\u7f6e\u4e2d\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8d8aTransformer\u57fa\u7ebf\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u52a8\u6001\u56fe\u5efa\u6a21\u4e2d\uff0c\u65e0\u6ce8\u610f\u529b\u67b6\u6784\u80fd\u591f\u8fbe\u5230\u4e0eTransformer\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5177\u6709\u663e\u8457\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u4e3a\u52a8\u6001\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12633", "abs": "https://arxiv.org/abs/2511.12633", "authors": ["Xunzhi Xiang", "Xingye Tian", "Guiyu Zhang", "Yabo Chen", "Shaofeng Zhang", "Xuebo Wang", "Xin Tao", "Qi Fan"], "title": "Denoising Vision Transformer Autoencoder with Spectral Self-Regularization", "comment": null, "summary": "Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\\times$256 benchmark.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u9ad8\u7ef4VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5197\u4f59\u9ad8\u9891\u5206\u91cf\u5bf9\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u6536\u655b\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86Denoising-VAE\u6765\u6291\u5236\u9ad8\u9891\u566a\u58f0\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfVAE\u5728\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b58\u5728\u4f18\u5316\u56f0\u5883\uff1a\u9ad8\u7ef4\u5ea6\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u4f46\u635f\u5bb3\u751f\u6210\u6027\u80fd\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u4f46\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u5982\u4f55\u5f71\u54cd\u751f\u6210\u6a21\u578b\u4f18\u5316\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u5149\u8c31\u81ea\u6b63\u5219\u5316\u7b56\u7565\u6765\u6291\u5236\u5197\u4f59\u9ad8\u9891\u566a\u58f0\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\uff0c\u5f00\u53d1\u4e86\u4e0d\u4f9d\u8d56VFMs\u7684Denoising-VAE\uff0c\u5e76\u5f15\u5165\u5149\u8c31\u5bf9\u9f50\u7b56\u7565\u6765\u4f18\u5316\u57fa\u4e8eDenoising-VAE\u7684\u751f\u6210\u6a21\u578b\u3002", "result": "\u5728ImageNet 256\u00d7256\u57fa\u51c6\u4e0a\uff0c\u6269\u6563\u6a21\u578b\u6536\u655b\u901f\u5ea6\u6bd4SD-VAE\u5feb\u7ea62\u500d\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\uff08rFID=0.28\uff0cPSNR=27.26\uff09\u548c\u7ade\u4e89\u6027\u751f\u6210\u6027\u80fd\uff08gFID=1.82\uff09\u3002", "conclusion": "Denoising-VAE\u901a\u8fc7\u6291\u5236\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5197\u4f59\u9ad8\u9891\u566a\u58f0\uff0c\u6709\u6548\u89e3\u51b3\u4e86VAE\u7684\u4f18\u5316\u56f0\u5883\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2511.12639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12639", "abs": "https://arxiv.org/abs/2511.12639", "authors": ["Ye Du", "Nanxi Yu", "Shujun Wang"], "title": "Medical Knowledge Intervention Prompt Tuning for Medical Image Classification", "comment": "IEEE Transactions on Medical Imaging (Early Access) July 2025", "summary": "Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.", "AI": {"tldr": "CILMP\u662f\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u96c6\u6210\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u63d0\u793a\u8c03\u4f18\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u75be\u75c5\u7279\u5b9a\u8868\u793a\u5e76\u5728\u4f4e\u79e9\u7ebf\u6027\u5b50\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5e72\u9884\uff0c\u751f\u6210\u75be\u75c5\u7279\u5b9a\u63d0\u793a\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u533b\u5b66\u6982\u5ff5\uff0c\u7f3a\u4e4f\u7279\u5b9a\u75be\u75c5\u76f8\u5173\u7279\u5f81\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u63d0\u4f9b\u4e13\u4e1a\u533b\u5b66\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u56e0\u6b64\u5c06\u5176\u96c6\u6210\u5230\u63d0\u793a\u8c03\u4f18\u8fc7\u7a0b\u4e2d\u4ee5\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51faCILMP\u65b9\u6cd5\uff1a\u4eceLLMs\u63d0\u53d6\u75be\u75c5\u7279\u5b9a\u8868\u793a\uff0c\u5728\u4f4e\u79e9\u7ebf\u6027\u5b50\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5e72\u9884\uff0c\u521b\u5efa\u75be\u75c5\u7279\u5b9a\u63d0\u793a\uff1b\u5f15\u5165\u6761\u4ef6\u673a\u5236\uff0c\u6839\u636e\u6bcf\u4e2a\u533b\u5b66\u56fe\u50cf\u751f\u6210\u5b9e\u4f8b\u81ea\u9002\u5e94\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCILMP\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "CILMP\u6210\u529f\u5730\u5c06LLMs\u4e0eVLMs\u6865\u63a5\uff0c\u4fc3\u8fdb\u4e86\u533b\u5b66\u77e5\u8bc6\u5411VLM\u63d0\u793a\u7684\u8f6c\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12653", "abs": "https://arxiv.org/abs/2511.12653", "authors": ["Cheng Liao"], "title": "DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry", "comment": null, "summary": "Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.\n  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.", "AI": {"tldr": "DPVO-QAT++\u662f\u4e00\u4e2a\u5c42\u6b21\u5316\u91cf\u5316\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u5c3a\u5ea6\u53c2\u6570\u5316\u3001VO\u524d\u540e\u7aef\u5f02\u6784\u7cbe\u5ea6\u8bbe\u8ba1\uff08\u524d\u7aefFP16/FP32\u4f2a\u91cf\u5316\uff0c\u540e\u7aef\u5168\u7cbe\u5ea6\uff09\u548cGPU\u539f\u751f\u6838\u878d\u5408\u6280\u672f\uff0c\u5728\u4fdd\u6301\u8f68\u8ff9\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u5e76\u63d0\u5347\u5904\u7406\u901f\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u89c6\u89c9SLAM\u7cfb\u7edf\u5177\u6709\u51fa\u8272\u7684\u51e0\u4f55\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u8fc7\u9ad8\u7684\u8ba1\u7b97\u5f00\u9500\u4e25\u91cd\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u81ea\u4e3b\u5e73\u53f0\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u53ef\u5b66\u4e60\u5c3a\u5ea6\u53c2\u6570\u5316\u3001\u5f02\u6784\u7cbe\u5ea6\u8bbe\u8ba1\uff08\u524d\u7aef\u6d6e\u70b9\u4f2a\u91cf\u5316\uff0c\u540e\u7aef\u5168\u7cbe\u5ea6\uff09\u548cGPU\u539f\u751f\u6838\u878d\u5408\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u5c42\u6b21\u5316\u91cf\u5316\u4f18\u5316\u6846\u67b6DPVO-QAT++\u3002", "result": "\u5728TartanAir\u6570\u636e\u96c6\u4e0a\u5e73\u5747FPS\u63d0\u534752.1%\uff0c\u4e2d\u4f4d\u5ef6\u8fdf\u964d\u4f4e29.1%\uff0c\u5cf0\u503cGPU\u5185\u5b58\u5360\u7528\u51cf\u5c1164.9%\uff1b\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u5e73\u5747FPS\u63d0\u534730.1%\uff0c\u4e2d\u4f4d\u5ef6\u8fdf\u964d\u4f4e23.1%\uff0c\u5cf0\u503cGPU\u5185\u5b58\u5360\u7528\u51cf\u5c1137.7%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u7684\u8f68\u8ff9\u7cbe\u5ea6\u3002", "conclusion": "DPVO-QAT++\u6709\u6548\u5f25\u5408\u4e86\u9ad8\u7cbe\u5ea6\u6df1\u5ea6VO\u4e0e\u5b9e\u9645\u90e8\u7f72\u6548\u7387\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5728\u771f\u5b9e\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5de5\u7a0b\u8303\u5f0f\u3002"}}
{"id": "2511.12467", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12467", "abs": "https://arxiv.org/abs/2511.12467", "authors": ["Jiachen Qian", "Yang Zheng"], "title": "Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction", "comment": null, "summary": "This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u7684\u5728\u7ebf\u591a\u6b65\u9884\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6761\u4ef6\u5206\u5e03\u7406\u8bba\u7684\u6700\u4f18\u9884\u6d4b\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5728\u7ebf\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u6765\u5b66\u4e60\u8be5\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u591a\u6b65\u9884\u6d4b\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u672a\u77e5\u7ebf\u6027\u968f\u673a\u7cfb\u7edf\u5f00\u53d1\u9ad8\u6548\u7684\u9884\u6d4b\u7b97\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5148\u8bc6\u522b\u7cfb\u7edf\u6a21\u578b\u7684\u590d\u6742\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u5206\u5e03\u7406\u8bba\u63a8\u5bfc\u9884\u6d4b\u7b56\u7565\u7684\u6700\u4f18\u53c2\u6570\u5316\u5f62\u5f0f\uff0c\u63d0\u51fa\u5728\u7ebf\u6700\u5c0f\u4e8c\u4e58\u7b97\u6cd5\u6765\u5b66\u4e60\u9884\u6d4b\u7b56\u7565\uff0c\u5e76\u4e0e\u6700\u4f18\u6a21\u578b\u9884\u6d4b\u5668\u8fdb\u884c\u9057\u61be\u5206\u6790\u3002", "result": "\u5728\u7ebf\u7b97\u6cd5\u5728\u591a\u6b65\u8bbe\u7f6e\u4e0b\u76f8\u5bf9\u4e8e\u6700\u4f18\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5b9e\u73b0\u4e86\u5bf9\u6570\u9057\u61be\uff0c\u5efa\u7acb\u4e86\u51e0\u4e4e\u786e\u5b9a\u7684\u9057\u61be\u754c\u9650\uff0c\u4e14\u9057\u61be\u5e38\u6570\u968f\u9884\u6d4b\u65f6\u57df\u591a\u9879\u5f0f\u589e\u957f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u672a\u77e5\u7ebf\u6027\u7cfb\u7edf\u7684\u5728\u7ebf\u591a\u6b65\u9884\u6d4b\u95ee\u9898\uff0c\u9057\u61be\u6027\u80fd\u826f\u597d\uff0c\u4f46\u9884\u6d4b\u65f6\u57df\u7684\u589e\u52a0\u4f1a\u4ee5\u591a\u9879\u5f0f\u65b9\u5f0f\u5f71\u54cd\u9057\u61be\u5e38\u6570\u3002"}}
{"id": "2511.12658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12658", "abs": "https://arxiv.org/abs/2511.12658", "authors": ["Zeqin Yu", "Haotao Xie", "Jian Zhang", "Jiangqun Ni", "Wenkan Su", "Jiwu Huang"], "title": "Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis", "comment": "NeurIPS 2025 D&B Track", "summary": "Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \\href{https://github.com/ZeqinYu/FSTS}{Project Page}.", "AI": {"tldr": "\u63d0\u51faFSTS\u6846\u67b6\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u7ea7\u6570\u542f\u53d1\u7684\u5206\u5c42\u5efa\u6a21\u65b9\u6cd5\u5408\u6210\u7be1\u6539\u6587\u672c\u56fe\u50cf\uff0c\u89e3\u51b3\u73b0\u6709T-IFL\u65b9\u6cd5\u56e0\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u7be1\u6539\u5206\u5e03\u5dee\u5f02\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u56fe\u50cf\u4f2a\u9020\u5b9a\u4f4d\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff0c\u4e14\u5408\u6210\u6570\u636e\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u7be1\u6539\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u5206\u5e03\u5dee\u5f02\u3002", "method": "\u6536\u96c616,750\u4e2a\u771f\u5b9e\u7be1\u6539\u5b9e\u4f8b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6d41\u6c34\u7ebf\u8bb0\u5f55\u4eba\u7c7b\u7f16\u8f91\u75d5\u8ff9\uff0c\u5efa\u7acb\u5206\u5c42\u5efa\u6a21\u6846\u67b6\uff1a\u4e2a\u4f53\u5c42\u9762\u7528\u57fa\u64cd\u4f5c-\u53c2\u6570\u914d\u7f6e\u7ec4\u5408\u8868\u793a\u7be1\u6539\u53c2\u6570\uff0c\u7fa4\u4f53\u5c42\u9762\u805a\u5408\u8fd9\u4e9b\u884c\u4e3a\u6784\u5efa\u5206\u5e03\uff0c\u57fa\u4e8e\u5085\u91cc\u53f6\u7ea7\u6570\u601d\u60f3\u8fdb\u884c\u53ef\u89e3\u91ca\u903c\u8fd1\u3002", "result": "\u5728\u56db\u4e2a\u8bc4\u4f30\u534f\u8bae\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528FSTS\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FSTS\u6846\u67b6\u80fd\u591f\u5408\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u66f4\u597d\u5730\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u4f2a\u9020\u75d5\u8ff9\uff0c\u6709\u6548\u63d0\u5347\u6587\u672c\u56fe\u50cf\u4f2a\u9020\u5b9a\u4f4d\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12662", "abs": "https://arxiv.org/abs/2511.12662", "authors": ["Hongbin Huang", "Junwei Li", "Tianxin Xie", "Zhuang Li", "Cekai Weng", "Yaodong Yang", "Yue Luo", "Li Liu", "Jing Tang", "Zhijing Shao", "Zeyu Wang"], "title": "Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans", "comment": "Proceedings of the Computer Graphics International 2025 (CGI'25)", "summary": "High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u4fdd\u771f\u3001\u5b9e\u65f6\u7684\u5bf9\u8bdd\u6570\u5b57\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u903c\u771f\u76843D\u865a\u62df\u5f62\u8c61\u3001\u4eba\u7269\u9a71\u52a8\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u5408\u6210\u548c\u57fa\u4e8e\u77e5\u8bc6\u7684\u5bf9\u8bdd\u751f\u6210\uff0c\u901a\u8fc7\u5f02\u6b65\u6267\u884c\u7ba1\u9053\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u591a\u6a21\u6001\u534f\u8c03\u3002", "motivation": "\u5f53\u524d\u9ad8\u4fdd\u771f\u6570\u5b57\u4eba\u7c7b\u5728\u4ea4\u4e92\u5e94\u7528\u4e2d\u9762\u4e34\u89c6\u89c9\u771f\u5b9e\u6027\u4e0e\u5b9e\u65f6\u54cd\u5e94\u6027\u96be\u4ee5\u517c\u987e\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u89c6\u89c9\u903c\u771f\u5ea6\u53c8\u80fd\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u6267\u884c\u7ba1\u9053\u534f\u8c03\u591a\u6a21\u6001\u7ec4\u4ef6\uff0c\u7ed3\u5408\u5524\u9192\u8bcd\u68c0\u6d4b\u3001\u60c5\u611f\u8868\u8fbe\u97f5\u5f8b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u54cd\u5e94\u751f\u6210\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5305\u62ec\u5386\u53f2\u589e\u5f3a\u548c\u57fa\u4e8e\u610f\u56fe\u7684\u8def\u7531\u8fdb\u884c\u9ad8\u6548\u77e5\u8bc6\u8bbf\u95ee\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u7cfb\u7edf\uff0c\u652f\u6301\u54cd\u5e94\u8fc5\u901f\u4e14\u53ef\u4fe1\u7684\u6570\u5b57\u4eba\u7c7b\uff0c\u9002\u7528\u4e8e\u901a\u4fe1\u3001\u6559\u80b2\u548c\u5a31\u4e50\u7b49\u6c89\u6d78\u5f0f\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u6570\u5b57\u4eba\u7c7b\u5728\u89c6\u89c9\u771f\u5b9e\u6027\u548c\u5b9e\u65f6\u4ea4\u4e92\u6027\u65b9\u9762\u7684\u5e73\u8861\uff0c\u4e3a\u6c89\u6d78\u5f0f\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12671", "abs": "https://arxiv.org/abs/2511.12671", "authors": ["Tushar Anand", "Advik Sinha", "Abhijit Das"], "title": "DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality", "comment": null, "summary": "In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u56e0\u679c\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u7684\u5b9e\u65f6\u5149\u5b66\u6d41\u548c\u89c6\u5dee\u4f30\u8ba1\u6a21\u578b\uff0c\u7528\u4e8e\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1", "motivation": "\u89e3\u51b3\u5b9e\u65f6\u5e94\u7528\u4e2d\u5149\u5b66\u6d41\u548c\u89c6\u5dee\u4f30\u8ba1\u7684\u9ad8\u7cbe\u5ea6\u4e0e\u4f4e\u5ef6\u8fdf\u9700\u6c42\u4e4b\u95f4\u7684\u77db\u76fe", "method": "\u4f7f\u7528\u975e\u56e0\u679cMamba\u5757\u878d\u5408\u6210\u5bf9\u8f93\u5165\u56fe\u50cf\uff0c\u6784\u5efa\u5feb\u901f\u9ad8\u6548\u7684\u5bc6\u96c6\u611f\u77e5\u6a21\u578b", "result": "\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\uff0c\u964d\u4f4eGPU\u4f7f\u7528\u7387", "conclusion": "\u8be5\u6a21\u578b\u9002\u7528\u4e8e\u7edf\u4e00\u7684\u5b9e\u65f6\u9ad8\u7cbe\u5ea63D\u5bc6\u96c6\u611f\u77e5\u4f30\u8ba1\u4efb\u52a1"}}
{"id": "2511.12491", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12491", "abs": "https://arxiv.org/abs/2511.12491", "authors": ["Ponhvoan Srey", "Yaxin Shi", "Hangwei Qian", "Jing Li", "Ivor W. Tsang"], "title": "Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation", "comment": "26 pages, 4 figures", "summary": "Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b8c\u5168\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08FTTA\uff09\u65b9\u6cd5AFTTA\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u6620\u5c04\u6a21\u62df\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u6f5c\u5728\u504f\u79fb\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4e92\u4fe1\u606f\u51c6\u5219\u8fdb\u884c\u7279\u5f81\u7a7a\u95f4\u548c\u6807\u7b7e\u7a7a\u95f4\u7684\u53bb\u504f\u79fb\u5b66\u4e60\uff0c\u4ee5\u5e94\u5bf9\u4e0d\u53ef\u9884\u89c1\u7684\u57df\u504f\u79fb\u3002", "motivation": "\u4f20\u7edfFTTA\u65b9\u6cd5\u7531\u4e8e\u65e0\u6cd5\u8bbf\u95ee\u6e90\u6570\u636e\u548c\u8bad\u7ec3\u534f\u8bae\uff0c\u4e14\u76ee\u6807\u57df\u4e0d\u53ef\u9884\u6d4b\uff0c\u5bfc\u81f4\u57fa\u4e8e\u6e90\u57df\u548c\u76ee\u6807\u57df\u7279\u5f81\u5206\u5e03\u5bf9\u9f50\u7684\u7b56\u7565\u4e0d\u53ef\u884c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u6cdb\u5316\u5230\u4e0d\u53ef\u9884\u89c1\u76ee\u6807\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u63ed\u793a-\u9057\u5fd8\u65b9\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u9884\u5b9a\u4e49\u6620\u5c04\u6a21\u62df\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u6f5c\u5728\u4e0d\u9700\u8981\u504f\u79fb\uff0c\u5c06\u5176\u89c6\u4e3a\u5e72\u6270\u56e0\u7d20\uff1b\u7136\u540e\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u4e92\u4fe1\u606f\u51c6\u5219\u5728\u7279\u5f81\u7a7a\u95f4\u8fdb\u884c\u53bb\u5e72\u6270\u5b66\u4e60\uff0c\u5e76\u5728\u6807\u7b7e\u7a7a\u95f4\u9f13\u52b1\u81ea\u4fe1\u548c\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "result": "\u5728\u6d89\u53ca\u635f\u574f\u548c\u98ce\u683c\u504f\u79fb\u7684\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u660e\u786e\u89e3\u51b3\u4e86\u4e0d\u53ef\u77e5\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5728FTTA\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12494", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12494", "abs": "https://arxiv.org/abs/2511.12494", "authors": ["Jiecheng Jiang", "Jiawei Tang", "Jiahao Jiang", "Hui Liu", "Junhui Hou", "Yuheng Jia"], "title": "Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance", "comment": null, "summary": "Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of \"missing\" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u4e2d\u9690\u85cf\u6807\u7b7e\u95ee\u9898(HidLDL)\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e0d\u5b8c\u5168\u6807\u7b7e\u5206\u5e03\u5b66\u4e60\u4e2d\u4e0d\u73b0\u5b9e\u7684\u8bbe\u7f6e\uff0c\u901a\u8fc7\u5229\u7528\u89c2\u5bdf\u6807\u7b7e\u7684\u6bd4\u4f8b\u4fe1\u606f\u548c\u5c40\u90e8\u7279\u5f81\u76f8\u4f3c\u6027\u3001\u5168\u5c40\u4f4e\u79e9\u7ed3\u6784\u6765\u6062\u590d\u5b8c\u6574\u7684\u6807\u7b7e\u5206\u5e03\u3002", "motivation": "\u4f20\u7edf\u4e0d\u5b8c\u5168\u6807\u7b7e\u5206\u5e03\u5b66\u4e60(IncomLDL)\u65b9\u6cd5\u5c06\u7f3a\u5931\u6807\u7b7e\u7684\u63cf\u8ff0\u5ea6\u8bbe\u4e3a0\u800c\u4fdd\u6301\u5176\u4ed6\u6807\u7b7e\u4e0d\u53d8\uff0c\u8fd9\u79cd\u8bbe\u7f6e\u4e0d\u73b0\u5b9e\uff0c\u56e0\u4e3a\u5f53\u67d0\u4e9b\u6807\u7b7e\u7f3a\u5931\u65f6\uff0c\u5269\u4f59\u6807\u7b7e\u7684\u5ea6\u4f1a\u76f8\u5e94\u589e\u52a0\u3002", "method": "\u5229\u7528\u89c2\u5bdf\u6807\u7b7e\u7684\u6bd4\u4f8b\u4fe1\u606f\u4f5c\u4e3a\u521b\u65b0\u7ea6\u675f\uff0c\u540c\u65f6\u4f7f\u7528\u5c40\u90e8\u7279\u5f81\u76f8\u4f3c\u6027\u548c\u5168\u5c40\u4f4e\u79e9\u7ed3\u6784\u6765\u63ed\u793a\u9690\u85cf\u6807\u7b7e\uff0c\u5e76\u7ed9\u51fa\u4e86\u65b9\u6cd5\u7684\u6062\u590d\u754c\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u6062\u590d\u548c\u9884\u6d4b\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LDL\u548cIncomLDL\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684HidLDL\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e0d\u5b8c\u5168\u6807\u7b7e\u5206\u5e03\u4e2d\u7684\u9690\u85cf\u6807\u7b7e\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u53ef\u884c\u6027\u548c\u5b9e\u9645\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.12691", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12691", "abs": "https://arxiv.org/abs/2511.12691", "authors": ["Shuaike Shen", "Ke Liu", "Jiaqing Xie", "Shangde Gao", "Chunhua Shen", "Ge Liu", "Mireia Crispin-Ortuzar", "Shangqi Gao"], "title": "R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection", "comment": null, "summary": "Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.", "AI": {"tldr": "R\u00b2Seg\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u9c81\u68d2OOD\u80bf\u7624\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u63a8\u7406-\u62d2\u7edd\u8fc7\u7a0b\u5904\u7406\u5206\u5e03\u5916\u80bf\u7624\u5206\u5272\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b\u5728\u5206\u5e03\u5916\u504f\u79fb\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u4ea7\u751f\u788e\u7247\u5316\u5047\u9633\u6027\u7ed3\u679c\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5c31\u80fd\u63d0\u5347OOD\u80bf\u7624\u5206\u5272\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u63a8\u7406\u9636\u6bb5\u4f7f\u7528LLM\u5f15\u5bfc\u7684\u89e3\u5256\u63a8\u7406\u89c4\u5212\u5668\u5b9a\u4f4d\u5668\u5b98\u951a\u70b9\u5e76\u751f\u6210\u591a\u5c3a\u5ea6ROI\uff1b2\uff09\u62d2\u7edd\u9636\u6bb5\u5bf9\u57fa\u7840\u6a21\u578b\u5728ROI\u5185\u751f\u6210\u7684\u5019\u9009\u533a\u57df\u5e94\u7528\u53cc\u6837\u672c\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u4ec5\u4fdd\u7559\u4e0e\u6b63\u5e38\u7ec4\u7ec7\u663e\u8457\u4e0d\u540c\u7684\u5019\u9009\u533a\u57df\u3002", "result": "\u5728\u591a\u4e2d\u5fc3\u591a\u6a21\u6001\u80bf\u7624\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cR\u00b2Seg\u5728Dice\u7cfb\u6570\u3001\u7279\u5f02\u6027\u548c\u654f\u611f\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u548c\u539f\u59cb\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "R\u00b2Seg\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u6291\u5236\u5047\u9633\u6027\uff0c\u63d0\u5347OOD\u80bf\u7624\u5206\u5272\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2511.12693", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12693", "abs": "https://arxiv.org/abs/2511.12693", "authors": ["Sushant Gautam", "Michael A. Riegler", "P\u00e5l Halvorsen"], "title": "HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.\n  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.\n  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .", "AI": {"tldr": "HEDGE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u53d7\u63a7\u89c6\u89c9\u6270\u52a8\u3001\u8bed\u4e49\u805a\u7c7b\u548c\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u6765\u68c0\u6d4b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u8bc4\u4f30\u591a\u6a21\u6001\u53ef\u9760\u6027\u3002", "method": "HEDGE\u6846\u67b6\u6574\u5408\u4e86\u91c7\u6837\u3001\u5931\u771f\u5408\u6210\u3001\u805a\u7c7b\uff08\u57fa\u4e8e\u8574\u542b\u548c\u5d4c\u5165\uff09\u548c\u5ea6\u91cf\u8ba1\u7b97\uff0c\u5f62\u6210\u53ef\u590d\u73b0\u7684\u6d41\u6c34\u7ebf\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5e7b\u89c9\u53ef\u68c0\u6d4b\u6027\u5728\u5177\u6709\u5bc6\u96c6\u89c6\u89c9\u6807\u8bb0\u5316\u7684\u7edf\u4e00\u878d\u5408\u6a21\u578b\u4e2d\u6700\u9ad8\uff0c\u5728\u6807\u8bb0\u5316\u53d7\u9650\u7684\u67b6\u6784\u4e2d\u6700\u4f4e\u3002VASE\u5ea6\u91cf\u4e0e\u5d4c\u5165\u805a\u7c7b\u7ed3\u5408\u65f6\u63d0\u4f9b\u6700\u9c81\u68d2\u7684\u5e7b\u89c9\u4fe1\u53f7\u3002", "conclusion": "HEDGE\u901a\u8fc7\u5c06\u5e7b\u89c9\u68c0\u6d4b\u6784\u5efa\u4e3a\u51e0\u4f55\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u3002"}}
{"id": "2511.12534", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12534", "abs": "https://arxiv.org/abs/2511.12534", "authors": ["Dor Polikar", "Alon Cohen"], "title": "Regret Guarantees for Linear Contextual Stochastic Shortest Path", "comment": null, "summary": "We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\\star^2 T_\\star \\log (1/ \u03b4))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\\star$ bounds the optimal cumulative loss and $T_\\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\\ell_{\\min}$, LR-CSSP attains a regret of $\\widetilde O(\\sqrt{K \\cdot d^2 |S|^3 |A| B_\\star^3 \\log(1/\u03b4)/\\ell_{\\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ebf\u6027\u4e0a\u4e0b\u6587\u968f\u673a\u6700\u77ed\u8def\u5f84\uff08CSSP\uff09\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86LR-CSSP\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u672a\u77e5MDP\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u6620\u5c04\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7ebf\u6027\u9057\u61be\u754c\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0a\u4e0b\u6587\u51b3\u5b9aMDP\u7684\u7ebf\u6027\u51fd\u6570\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u5b66\u4e60\u8005\u5982\u4f55\u4ee5\u6700\u5c0f\u671f\u671b\u7d2f\u79ef\u635f\u5931\u5230\u8fbe\u76ee\u6807\u72b6\u6001\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u77e5\u8bc6\u4e0d\u8db3\u53ef\u80fd\u5bfc\u81f4\u65e0\u9650\u671f\u5ef6\u957f\u5267\u96c6\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86LR-CSSP\u7b97\u6cd5\uff0c\u5229\u7528\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u6765\u5904\u7406\u8fde\u7eed\u4e0a\u4e0b\u6587\u7a7a\u95f4\uff0c\u786e\u4fdd\u6240\u6709\u5267\u96c6\u5728\u5408\u7406\u65f6\u95f4\u6b65\u5185\u7ec8\u6b62\u3002", "result": "LR-CSSP\u5b9e\u73b0\u4e86\u9057\u61be\u754c\u4e3a$\\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\\star^2 T_\\star \\log (1/ \u03b4))$\uff0c\u5728\u6210\u672c\u6709\u4e0b\u754c\u65f6\u8fbe\u5230$\\widetilde O(\\sqrt{K \\cdot d^2 |S|^3 |A| B_\\star^3 \\log(1/\u03b4)/\\ell_{\\min}})$\u3002", "conclusion": "LR-CSSP\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u8fde\u7eed\u4e0a\u4e0b\u6587\u7a7a\u95f4\uff0c\u786e\u4fdd\u5267\u96c6\u7ec8\u6b62\uff0c\u5e76\u5728\u672a\u77e5MDP\u52a8\u6001\u548c\u4e0a\u4e0b\u6587\u6620\u5c04\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e9a\u7ebf\u6027\u9057\u61be\u3002"}}
{"id": "2511.12708", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12708", "abs": "https://arxiv.org/abs/2511.12708", "authors": ["Kaiser Hamid", "Can Cui", "Khandakar Ashrafi Akbar", "Ziran Wang", "Nade Liang"], "title": "FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling", "comment": null, "summary": "Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.", "AI": {"tldr": "FSDAM\u662f\u4e00\u4e2a\u5c11\u6837\u672c\u9a7e\u9a76\u6ce8\u610f\u529b\u5efa\u6a21\u6846\u67b6\uff0c\u4ec5\u9700\u7ea6100\u4e2a\u6807\u6ce8\u6837\u672c\u5373\u53ef\u5b9e\u73b0\u8054\u5408\u6ce8\u610f\u529b\u9884\u6d4b\u548c\u63cf\u8ff0\u751f\u6210\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c11\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u6ce8\u89c6\u6570\u636e\u96c6\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u96c6\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u9700\u8981\u5f00\u53d1\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u7684\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\uff0c\u5305\u542b\u7a7a\u95f4\u9884\u6d4b\u548c\u63cf\u8ff0\u751f\u6210\u4e24\u4e2a\u72ec\u7acb\u6a21\u5757\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6ce8\u610f\u529b\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u80fd\u751f\u6210\u8fde\u8d2f\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u91ca\uff0c\u5e76\u5728\u591a\u4e2a\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u6ce8\u610f\u529b\u6761\u4ef6\u751f\u6210\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e2d\u90e8\u7f72\u53ef\u89e3\u91ca\u7684\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2511.12548", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12548", "abs": "https://arxiv.org/abs/2511.12548", "authors": ["Wenzhang Du"], "title": "CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching", "comment": "13 pages, 7 figures, 3 tables; anonymized logs and scripts reproduce all figures and tables", "summary": "First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u66f2\u7387\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5468\u671f\u6027\u6784\u5efa\u4f4e\u79e9Hessian\u5b50\u7a7a\u95f4\u6765\u9884\u6761\u4ef6\u68af\u5ea6\uff0c\u5728\u5c16\u9510\u3001\u5404\u5411\u5f02\u6027\u533a\u57df\u663e\u8457\u52a0\u901f\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u7ec8\u6d4b\u8bd5\u7cbe\u5ea6\u3002", "motivation": "\u4e00\u9636\u4f18\u5316\u5668\u5728\u5c16\u9510\u3001\u5404\u5411\u5f02\u6027\u533a\u57df\u53ef\u9760\u4f46\u6536\u655b\u7f13\u6162\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u9002\u5e94\u66f2\u7387\u7279\u5f81\u7684\u9ad8\u6548\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5468\u671f\u6027\u901a\u8fc7Hessian-\u5411\u91cf\u79ef\u6784\u5efa\u4f4e\u79e9Hessian\u5b50\u7a7a\u95f4\uff0c\u4ec5\u5728\u8be5\u5b50\u7a7a\u95f4\u5185\u9884\u6761\u4ef6\u68af\u5ea6\uff0c\u6b63\u4ea4\u8865\u7a7a\u95f4\u4fdd\u6301\u4e00\u9636\u4f18\u5316\u3002", "result": "\u5728CIFAR-10/100\u548cResNet-18/34\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4Adam\u65e92.95\u500d\u8fbe\u5230\u9884\u8bbe\u8bad\u7ec3\u635f\u5931\u9608\u503c\uff0c\u540c\u65f6\u5339\u914d\u6700\u7ec8\u6d4b\u8bd5\u7cbe\u5ea6\uff1b\u5bf9\u8349\u56fe\u79e9k\u4e0d\u654f\u611f\uff0ck=0\u53ef\u4f5c\u4e3a\u65e0\u66f2\u7387\u6d88\u878d\u5b9e\u9a8c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6700\u7ec8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u6536\u655b\uff0c\u5bf9\u8d85\u53c2\u6570\u4e0d\u654f\u611f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u66f2\u7387\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2511.12735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12735", "abs": "https://arxiv.org/abs/2511.12735", "authors": ["Ankita Raj", "Chetan Arora"], "title": "Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning", "comment": "Accepted to AAAI 2026", "summary": "Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\uff08OVODs\uff09\u7684\u540e\u95e8\u653b\u51fb\uff0c\u63d0\u51faTrAP\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u7684\u63d0\u793a\u53c2\u6570\u4ee5\u53ca\u89c6\u89c9\u89e6\u53d1\u5668\uff0c\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u690d\u5165\u540e\u95e8\u3002", "motivation": "\u968f\u7740OVODs\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u76d1\u63a7\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u7406\u89e3\u5176\u5b89\u5168\u98ce\u9669\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u7531\u63d0\u793a\u8c03\u4f18\u5f15\u5165\u7684\u65b0\u653b\u51fb\u9762\u3002", "method": "\u63d0\u51faTrAP\uff08Trigger-Aware Prompt tuning\uff09\u591a\u6a21\u6001\u540e\u95e8\u6ce8\u5165\u7b56\u7565\uff0c\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u8bad\u7ec3\u7b56\u7565\u9010\u6b65\u7f29\u5c0f\u89e6\u53d1\u5668\u5c3a\u5bf8\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u63d0\u793a\u4ee4\u724c\u690d\u5165\u540e\u95e8\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTrAP\u5728\u5bf9\u8c61\u8bef\u5206\u7c7b\u548c\u5bf9\u8c61\u6d88\u5931\u653b\u51fb\u4e2d\u5747\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u5728\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u96f6\u6837\u672c\u8bbe\u7f6e\u63d0\u9ad8\u4e86\u5e72\u51c0\u56fe\u50cf\u6027\u80fd\u3002", "conclusion": "TrAP\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u690d\u5165\u9690\u85cf\u540e\u95e8\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63ed\u793a\u4e86OVODs\u4e2d\u7531\u63d0\u793a\u8c03\u4f18\u5f15\u5165\u7684\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2511.12738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12738", "abs": "https://arxiv.org/abs/2511.12738", "authors": ["Parsa Esmaeilkhani", "Longin Jan Latecki"], "title": "Direct Visual Grounding by Directing Attention of Visual Tokens", "comment": null, "summary": "Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6ce8\u610f\u529b\u76d1\u7763\u635f\u5931\u51fd\u6570KLAL\uff0c\u901a\u8fc7\u76f4\u63a5\u76d1\u7763\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u5e03\u6765\u6539\u5584\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6700\u7ec8\u5c42\u5bf9\u76f8\u5173\u89c6\u89c9\u6807\u8bb0\u5173\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4e0e\u67e5\u8be2\u6700\u76f8\u5173\u7684\u89c6\u89c9\u6807\u8bb0\u5728LLM\u6a21\u5757\u7684\u6700\u7ec8\u5c42\u5f88\u5c11\u6216\u6ca1\u6709\u53d7\u5230\u7b54\u6848\u6807\u8bb0\u7684\u5173\u6ce8\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9\u95ee\u7b54\u9519\u8bef\u3002\u6807\u51c6\u7684\u4e0b\u4e00\u6807\u8bb0\u9884\u6d4b\u635f\u5931\u65e0\u6cd5\u6709\u6548\u5f15\u5bfc\u5bf9\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u3002", "method": "\u63d0\u51faKL\u6ce8\u610f\u529b\u635f\u5931\uff08KLAL\uff09\uff0c\u901a\u8fc7KL\u6563\u5ea6\u5c06\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u5e03\u4e0e\u771f\u5b9e\u6ce8\u610f\u529b\u56fe\u5bf9\u9f50\u3002\u771f\u5b9e\u6ce8\u610f\u529b\u56fe\u6765\u81ea\u5408\u6210\u6848\u4f8b\u4e2d\u7684\u4efb\u52a1\u51e0\u4f55\u6216\u771f\u5b9e\u56fe\u50cf\u4e2d\u7684\u6807\u51c6\u6807\u6ce8\uff0c\u65e0\u9700\u65b0\u6807\u7b7e\u3002KLAL\u4e0eNTP\u635f\u5931\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u51e0\u4f55\u4efb\u52a1\u3001\u6307\u5411\u548c\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u7b49\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002\u8fd8\u5f15\u5165\u4e86\u65b0\u7684\u7ebf\u8ffd\u8e2a\u80fd\u529b\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u5373\u4f7f\u5546\u4e1aVLM\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e5f\u4e0d\u4f73\u3002", "conclusion": "\u76f4\u63a5\u76d1\u7763\u89c6\u89c9\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u5e03\u53ef\u4ee5\u6709\u6548\u6539\u5584VLM\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0cKLAL\u635f\u5931\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u673a\u5236\u3002"}}
{"id": "2511.12564", "categories": ["cs.LG", "cs.CG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12564", "abs": "https://arxiv.org/abs/2511.12564", "authors": ["David Denisov", "Shlomi Dolev", "Dan Felmdan", "Michael Segal"], "title": "Linear time small coresets for k-mean clustering of segments with applications", "comment": "First published in WALCOM 2026 by Springer Nature", "summary": "We study the $k$-means problem for a set $\\mathcal{S} \\subseteq \\mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \\subseteq \\mathbb{R}^d$ that minimize\n  $D(\\mathcal{S},X) := \\sum_{S \\in \\mathcal{S}} \\min_{x \\in X} D(S,x)$, where $D(S,x) := \\int_{p \\in S} |p - x| dp$\n  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\\varepsilon > 0$, an $\\varepsilon$-coreset is a weighted subset $C \\subseteq \\mathbb{R}^d$ that approximates $D(\\mathcal{S},X)$ within a factor of $1 \\pm \\varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\\varepsilon$, it produces a coreset of size $O(\\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u4efb\u610f\u8f93\u5165\u7ebf\u6bb5\u7684k-means\u805a\u7c7b\u6838\u96c6\u6784\u9020\u65b9\u6cd5\uff0c\u5bf9\u4e8e\u5e38\u6570k\u548c\u03b5\uff0c\u53ef\u751f\u6210\u5927\u5c0f\u4e3aO(log\u00b2n)\u7684\u6838\u96c6\uff0c\u8ba1\u7b97\u65f6\u95f4\u4e3aO(nd)\uff0c\u5728\u89c6\u9891\u8ddf\u8e2a\u7b49\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u4e14\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\u3002", "motivation": "\u7814\u7a76\u7ebf\u6bb5\u96c6\u5408\u7684k-means\u805a\u7c7b\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u7ebf\u6bb5\u6570\u636e\u5f00\u53d1\u9ad8\u6548\u7684\u6838\u96c6\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u6d41\u5f0f\u3001\u5206\u5e03\u5f0f\u548c\u5e76\u884c\u8ba1\u7b97\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u4efb\u610f\u8f93\u5165\u7ebf\u6bb5\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6838\u96c6\u6784\u9020\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u8f93\u5165\u7ebf\u6bb5\uff0c\u4f7f\u7528\u8ddd\u79bb\u51fd\u6570D(S,x) = \u222b_{p\u2208S}|p-x|dp\u6765\u5ea6\u91cf\u7ebf\u6bb5\u5230\u805a\u7c7b\u4e2d\u5fc3\u7684\u8ddd\u79bb\uff0c\u5e76\u652f\u6301\u5904\u7406\u5f02\u5e38\u503c\u3001\u4f7f\u7528M\u4f30\u8ba1\u91cf\u3001\u52a0\u6743\u8ddd\u79bb\u548c\u5f3a\u5236\u552f\u4e00\u805a\u7c7b\u5206\u914d\u7b49\u53d8\u4f53\u3002", "result": "\u5bf9\u4e8e\u5e38\u6570k\u548c\u03b5\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u5927\u5c0f\u4e3aO(log\u00b2n)\u7684\u6838\u96c6\uff0c\u8ba1\u7b97\u65f6\u95f4\u4e3aO(nd)\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u8ddf\u8e2a\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u4e14\u805a\u7c7b\u7cbe\u5ea6\u635f\u5931\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u5904\u7406\u4efb\u610f\u8f93\u5165\u7ebf\u6bb5\u7684k-means\u805a\u7c7b\u6838\u96c6\u6784\u9020\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4e3a\u7ebf\u6bb5\u6570\u636e\u7684\u6d41\u5f0f\u3001\u5206\u5e03\u5f0f\u548c\u5e76\u884c\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12740", "abs": "https://arxiv.org/abs/2511.12740", "authors": ["Amirhossein Hassanzadeh", "Bartosz Krawczyk", "Michael Saunders", "Rob Wible", "Keith Krause", "Dimah Dera", "Jan van Aardt"], "title": "Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4ece\u4f53\u7d20\u5316LiDAR\u70b9\u4e91\u6570\u636e\u63a8\u65ad\u4f53\u7d20\u5185\u76ee\u6807\u5360\u7528\u767e\u5206\u6bd4\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528KPConv\u548c\u591a\u76ee\u6807\u56de\u5f52\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u53d1\u73b0\u8f83\u5927\u4f53\u7d20\u5c3a\u5bf8\uff082\u7c73\uff09\u8bef\u5dee\u8f83\u4f4e\uff0c\u800c\u8f83\u5c0f\u4f53\u7d20\u5c3a\u5bf8\uff080.25-0.5\u7c73\uff09\u5728\u6811\u51a0\u533a\u57df\u8bef\u5dee\u8f83\u9ad8\u3002", "motivation": "\u4f53\u7d20\u5316\u867d\u7136\u80fd\u964d\u4f4eLiDAR\u6570\u636e\u5904\u7406\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u4f1a\u5bfc\u81f4\u7ec6\u5c3a\u5ea6\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u4ece\u9ad8\u7ea7\u4f53\u7d20\u5316LiDAR\u70b9\u4e91\u6570\u636e\u4e2d\u63a8\u65ad\u4f4e\u7ea7\u7684\u4f53\u7d20\u5185\u5bb9\u4fe1\u606f\uff08\u76ee\u6807\u5360\u7528\u767e\u5206\u6bd4\uff09\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6838\u70b9\u5377\u79ef\uff08KPConv\uff09\u7684\u591a\u76ee\u6807\u56de\u5f52\u65b9\u6cd5\uff0c\u91c7\u7528\u5bc6\u5ea6\u76f8\u5173\uff08DBR\uff09\u7684\u6210\u672c\u654f\u611f\u5b66\u4e60\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4f7f\u7528\u52a0\u6743\u5747\u65b9\u8bef\u5dee\u3001\u7126\u70b9\u56de\u5f52\u548c\u6b63\u5219\u5316\u6765\u4f18\u5316KPConv\u3002\u5bf9\u4f53\u7d20\u5c3a\u5bf8\uff080.25-2\u7c73\uff09\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u654f\u611f\u6027\u5206\u6790\u663e\u793a\uff0c\u8f83\u5927\u4f53\u7d20\u5c3a\u5bf8\uff08\u59822\u7c73\uff09\u7531\u4e8e\u53d8\u5f02\u6027\u964d\u4f4e\u800c\u8bef\u5dee\u8f83\u5c0f\uff0c\u800c\u8f83\u5c0f\u4f53\u7d20\u5c3a\u5bf8\uff08\u59820.25\u62160.5\u7c73\uff09\u8bef\u5dee\u8f83\u9ad8\uff0c\u7279\u522b\u662f\u5728\u6811\u51a0\u533a\u57df\u3002\u5bf9\u4e8e\u6811\u76ae\u548c\u6811\u53f6\u76ee\u6807\uff0c\u5c0f\u4f53\u7d20\u5c3a\u5bf8\u6570\u636e\u96c6\u7684\u8bef\u5dee\u503c\u663e\u8457\u9ad8\u4e8e\u5927\u4f53\u7d20\u5c3a\u5bf8\u6570\u636e\u96c6\u3002", "conclusion": "\u4f53\u7d20\u5c3a\u5bf8\u7684\u9009\u62e9\u53d6\u51b3\u4e8e\u5177\u4f53\u5e94\u7528\u3002\u672c\u7814\u7a76\u586b\u8865\u4e86\u6df1\u5ea6\u4e0d\u5e73\u8861\u5b66\u4e60\u6a21\u578b\u5728\u591a\u76ee\u6807\u56de\u5f52\u548c\u68ee\u67973D LiDAR\u70b9\u4e91\u6a21\u62df\u6570\u636e\u96c6\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2511.12568", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12568", "abs": "https://arxiv.org/abs/2511.12568", "authors": ["Mitul Goswami", "Romit Chatterjee"], "title": "Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data", "comment": "Published as Chapter 2 in Intelligent and Smart Computing: Applications to Engineering Problems, Cambridge Scholars Publishing (2025). ISBN: 978-1-0364-5886-7", "summary": "This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cf\u5316\u548c\u4f4d\u6df1\u5ea6\u4f18\u5316\u6280\u672f\u4f18\u5316\u590d\u6742\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u590d\u6742\u6027\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7387\u3002\u5728\u533b\u7597\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff0c\u5c06\u8f93\u5165\u6570\u636e\u4ecefloat64\u964d\u7ea7\u5230float32\u548cint32\uff0c\u7ed3\u679c\u663e\u793a\u65f6\u95f4\u590d\u6742\u6027\u663e\u8457\u964d\u4f4e\uff0c\u51c6\u786e\u7387\u4ec5\u6709\u5fae\u5c0f\u4e0b\u964d\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u6a21\u578b\u6267\u884c\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u6280\u672f\u51cf\u5c11\u65f6\u95f4\u590d\u6742\u6027\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7387\u3002", "method": "\u4f7f\u7528\u91cf\u5316\u548c\u4f4d\u6df1\u5ea6\u4f18\u5316\u7b56\u7565\uff0c\u5c06\u8f93\u5165\u6570\u636e\u4ecefloat64\u964d\u7ea7\u5230float32\u548cint32\uff0c\u5728\u4e24\u4e2a\u533b\u7597\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u903b\u8f91\u56de\u5f52\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u65f6\u95f4\u590d\u6742\u6027\u663e\u8457\u964d\u4f4e\uff0c\u6a21\u578b\u51c6\u786e\u7387\u5728\u4f18\u5316\u540e\u4ec5\u6709\u5fae\u5c0f\u4e0b\u964d\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u4f18\u5316\u6280\u672f\u7684\u5f71\u54cd\u53d6\u51b3\u4e8e\u4e00\u7ec4\u53c2\u6570\uff0c\u4f18\u5316\u6548\u679c\u56e0\u53c2\u6570\u8bbe\u7f6e\u800c\u5f02\u3002"}}
{"id": "2511.12581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12581", "abs": "https://arxiv.org/abs/2511.12581", "authors": ["Kai Ma", "Zhen Wang", "Hongquan He", "Qi Xu", "Tinghuan Chen", "Hao Geng"], "title": "LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction", "comment": null, "summary": "Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7f51\u8868\u53d8\u6362\u5668\uff08LNT\uff09\u9ad8\u6548\u5904\u7406SPICE\u6587\u4ef6\uff0c\u5c06\u7f51\u8868\u62d3\u6251\u8868\u793a\u4e3a3D\u70b9\u4e91\uff0c\u80fd\u591f\u5904\u7406\u6570\u5341\u4e07\u5230\u6570\u767e\u4e07\u8282\u70b9\u7684\u7f51\u8868\uff0c\u5b9e\u73b0\u9759\u6001\u7535\u538b\u964d\u9884\u6d4b\u3002", "motivation": "\u9759\u6001IR\u538b\u964d\u5206\u6790\u5728\u82af\u7247\u8bbe\u8ba1\u4e2d\u81f3\u5173\u91cd\u8981\u4f46\u8017\u65f6\uff0c\u89e3\u51b3IR\u538b\u964d\u8fdd\u89c4\u9700\u8981\u8fed\u4ee3\u5206\u6790\uff0c\u8ba1\u7b97\u8d1f\u62c5\u91cd\uff0c\u56e0\u6b64\u9700\u8981\u5feb\u901f\u51c6\u786e\u7684IR\u538b\u964d\u9884\u6d4b\u6765\u51cf\u5c11\u82af\u7247\u8bbe\u8ba1\u65f6\u95f4\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7f51\u8868\u53d8\u6362\u5668\u5904\u7406SPICE\u6587\u4ef6\uff0c\u5c06\u7f51\u8868\u62d3\u6251\u8868\u793a\u4e3a3D\u70b9\u4e91\uff0c\u5c06\u6240\u6709\u7c7b\u578b\u6570\u636e\uff08\u7f51\u8868\u6587\u4ef6\u548c\u56fe\u50cf\u6570\u636e\uff09\u7f16\u7801\u4e3a\u6f5c\u5728\u7a7a\u95f4\u7279\u5f81\u5e76\u8f93\u5165\u6a21\u578b\u8fdb\u884c\u9759\u6001\u7535\u538b\u964d\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728ICCAD 2023\u7ade\u8d5b\u83b7\u80dc\u56e2\u961f\u548c\u6700\u5148\u8fdb\u7b97\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73F1\u5206\u6570\u548c\u6700\u4f4eMAE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u7f51\u8868\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u5b9e\u73b0\u4e92\u8865\u9884\u6d4b\uff0c\u5728\u9759\u6001\u7535\u538b\u964d\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.12757", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12757", "abs": "https://arxiv.org/abs/2511.12757", "authors": ["Nicholas Karris", "Luke Durell", "Javier Flores", "Tegan Emerson"], "title": "Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion", "comment": null, "summary": "It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0Stable Diffusion\u5bf9CLIP\u5d4c\u5165\u77e9\u9635\u5177\u6709\u6392\u5217\u4e0d\u53d8\u6027\uff0c\u56e0\u6b64\u53ef\u5c06\u5d4c\u5165\u89c6\u4e3aWasserstein\u7a7a\u95f4\u4e2d\u7684\u70b9\u4e91\u800c\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u77e9\u9635\u3002\u901a\u8fc7\u5c06\u5d4c\u5165\u63d2\u503c\u95ee\u9898\u91cd\u6784\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u8ba1\u7b97\u5d4c\u5165\u4e4b\u95f4\u7684\u6700\u77ed\u8def\u5f84\uff0c\u4ece\u800c\u5728Stable Diffusion\u4e2d\u751f\u6210\u66f4\u5e73\u6ed1\u3001\u66f4\u8fde\u8d2f\u7684\u63d2\u503c\u56fe\u50cf\u3002", "motivation": "\u53d7Stable Diffusion\u5bf9CLIP\u5d4c\u5165\u77e9\u9635\u6392\u5217\u4e0d\u53d8\u6027\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5c06\u5d4c\u5165\u89e3\u91ca\u4e3aWasserstein\u7a7a\u95f4\u4e2d\u7684\u70b9\u4e91\u800c\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684\u77e9\u9635\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u5c06\u5d4c\u5165\u63d2\u503c\u95ee\u9898\u91cd\u6784\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u6c42\u89e3\u8be5\u95ee\u9898\u8ba1\u7b97\u5d4c\u5165\u4e4b\u95f4\u7684\u6700\u77ed\u8def\u5f84\uff08\u6d4b\u5730\u7ebf\uff09\uff0c\u5728Stable Diffusion\u751f\u6210\u6a21\u578b\u4e2d\u8fdb\u884c\u56fe\u50cf\u63d2\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u65b9\u6cd5\u76f8\u6bd4\u5176\u4ed6\u6807\u51c6\u63d2\u503c\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u56fe\u50cf\u63d2\u503c\u6548\u679c\u3002", "conclusion": "\u5c06\u5d4c\u5165\u89c6\u4e3a\u70b9\u4e91\u800c\u975e\u77e9\u9635\u80fd\u66f4\u597d\u5730\u53cd\u6620\u548c\u5229\u7528\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\uff0c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u63d2\u503c\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u81ea\u7136\u7684\u56fe\u50cf\u8fc7\u6e21\u6548\u679c\u3002"}}
{"id": "2511.12767", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12767", "abs": "https://arxiv.org/abs/2511.12767", "authors": ["C\u0103t\u0103lin-Alexandru R\u00eepanu", "Andrei-Theodor Hotnog", "Giulia-Stefania Imbrea", "Dumitru-Clementin Cercel"], "title": "RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition", "comment": "5 pages, 3 figures, 4 tables", "summary": "Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u7f57\u9a6c\u5c3c\u4e9a\u5b64\u7acb\u624b\u8bed\u8bc6\u522b(RoISLR)\u7684\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6RoCoISLR\uff0c\u5305\u542b9000\u591a\u4e2a\u89c6\u9891\u6837\u672c\u548c\u8fd16000\u4e2a\u6807\u51c6\u5316\u8bcd\u6c47\u3002\u901a\u8fc7\u8bc4\u4f307\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u9891\u8bc6\u522b\u6a21\u578b\uff0c\u53d1\u73b0\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u8868\u73b0\u6700\u4f73\uff0cSwin Transformer\u8fbe\u523034.1%\u7684Top-1\u51c6\u786e\u7387\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u624b\u8bed\u8bc6\u522b\u6570\u636e\u96c6\u4e13\u6ce8\u4e8e\u7f8e\u56fd\u624b\u8bed\uff0c\u800c\u7f57\u9a6c\u5c3c\u4e9a\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u51c6\u5316\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u6784\u5efaRoCoISLR\u6570\u636e\u96c6\uff0c\u5305\u542b9000\u591a\u4e2a\u89c6\u9891\u6837\u672c\u548c\u8fd16000\u4e2a\u6807\u51c6\u5316\u8bcd\u6c47\u3002\u5728\u4e00\u81f4\u7684\u5b9e\u9a8c\u8bbe\u7f6e\u4e0b\u8bc4\u4f307\u79cd\u89c6\u9891\u8bc6\u522b\u6a21\u578b\uff1aI3D\u3001SlowFast\u3001Swin Transformer\u3001TimeSformer\u3001Uniformer\u3001VideoMAE\u548cPoseConv3D\u3002", "result": "\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u4f18\u4e8e\u5377\u79ef\u57fa\u7ebf\u6a21\u578b\uff0cSwin Transformer\u83b7\u5f9734.1%\u7684Top-1\u51c6\u786e\u7387\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u4f4e\u8d44\u6e90\u624b\u8bed\u4e2d\u957f\u5c3e\u7c7b\u5206\u5e03\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "RoCoISLR\u4e3a\u7cfb\u7edf\u6027\u7684\u7f57\u9a6c\u5c3c\u4e9a\u5b64\u7acb\u624b\u8bed\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u521d\u6b65\u57fa\u7840\uff0c\u57fa\u51c6\u7ed3\u679c\u663e\u793a\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u548c\u673a\u9047\u3002"}}
{"id": "2511.12603", "categories": ["cs.LG", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.12603", "abs": "https://arxiv.org/abs/2511.12603", "authors": ["Hongyi Chen", "Jianhai Shu", "Jingtao Ding", "Yong Li", "Xiao-Ping Zhang"], "title": "PID-controlled Langevin Dynamics for Faster Sampling of Generative Models", "comment": "NeurIPS 2025 poster paper", "summary": "Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \\href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.", "AI": {"tldr": "PID\u63a7\u5236\u7684\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\uff08PIDLD\uff09\u662f\u4e00\u79cd\u65b0\u9896\u7684\u91c7\u6837\u52a0\u901f\u7b97\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u7406\u8bba\u539f\u7406\u91cd\u65b0\u89e3\u91ca\u91c7\u6837\u8fc7\u7a0b\uff0c\u5229\u7528\u5386\u53f2\u68af\u5ea6\u548c\u68af\u5ea6\u8d8b\u52bf\u6765\u9ad8\u6548\u7a7f\u8d8a\u80fd\u91cf\u666f\u89c2\u5e76\u81ea\u9002\u5e94\u7a33\u5b9a\uff0c\u663e\u8457\u51cf\u5c11\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u6240\u9700\u7684\u8fed\u4ee3\u6b21\u6570\u3002", "motivation": "\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u91c7\u6837\u5b58\u5728\u751f\u6210\u901f\u5ea6\u6781\u4f4e\u7684\u95ee\u9898\uff0c\u6839\u672c\u539f\u56e0\u662f\u9700\u8981\u5927\u91cf\u7ec6\u7c92\u5ea6\u8fed\u4ee3\u624d\u80fd\u6536\u655b\u5230\u76ee\u6807\u5206\u5e03\u3002", "method": "\u5c06\u80fd\u91cf\u68af\u5ea6\u89c6\u4e3a\u53cd\u9988\u4fe1\u53f7\uff0c\u7ed3\u5408\u5386\u53f2\u68af\u5ea6\uff08\u79ef\u5206\u9879\uff09\u548c\u68af\u5ea6\u8d8b\u52bf\uff08\u5fae\u5206\u9879\uff09\u6765\u9ad8\u6548\u904d\u5386\u80fd\u91cf\u666f\u89c2\u5e76\u81ea\u9002\u5e94\u7a33\u5b9a\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3001\u6570\u636e\u96c6\u6216\u5148\u9a8c\u4fe1\u606f\uff0c\u53ef\u4e0e\u4efb\u4f55\u57fa\u4e8e\u6717\u4e4b\u4e07\u7684\u65b9\u6cd5\u7acb\u5373\u96c6\u6210\u3002", "result": "\u5728\u56fe\u50cf\u751f\u6210\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPIDLD\u4ee5\u66f4\u5c11\u7684\u6b65\u9aa4\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8d28\u91cf\uff0c\u4f7f\u57fa\u4e8e\u6717\u4e4b\u4e07\u7684\u751f\u6210\u6a21\u578b\u5728\u6548\u7387\u5173\u952e\u5e94\u7528\u4e2d\u66f4\u52a0\u5b9e\u7528\u3002", "conclusion": "PIDLD\u901a\u8fc7\u63a7\u5236\u7406\u8bba\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u4e86\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u91c7\u6837\uff0c\u4e3a\u6548\u7387\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12785", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.12785", "abs": "https://arxiv.org/abs/2511.12785", "authors": ["Maria Larchenko", "Dmitry Guskov", "Alexander Lobashev", "Georgy Derevyanko"], "title": "Lightweight Optimal-Transport Harmonization on Edge Devices", "comment": "AAAI 2026, Oral", "summary": "Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u989c\u8272\u534f\u8c03\u65b9\u6cd5MKL-Harmonizer\uff0c\u7528\u4e8e\u589e\u5f3a\u73b0\u5b9e(AR)\u573a\u666f\u4e2d\u7684\u5b9e\u65f6\u989c\u8272\u534f\u8c03\uff0c\u901a\u8fc7\u8bad\u7ec3\u7d27\u51d1\u7f16\u7801\u5668\u9884\u6d4bMonge-Kantorovich\u4f20\u8f93\u6620\u5c04\u6765\u5b9e\u73b0\u8bbe\u5907\u7aef\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3AR\u7ba1\u9053\u4e2d\u7f3a\u4e4f\u5b9e\u65f6\u989c\u8272\u534f\u8c03\u7b97\u6cd5\u7684\u95ee\u9898\uff0c\u4f7f\u63d2\u5165\u5bf9\u8c61\u7684\u989c\u8272\u4e0e\u5468\u56f4\u56fe\u50cf\u611f\u77e5\u5339\u914d\uff0c\u5b9e\u73b0\u65e0\u7f1d\u5408\u6210\u3002", "method": "\u5229\u7528\u7ecf\u5178\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u8bad\u7ec3\u7d27\u51d1\u7f16\u7801\u5668\u9884\u6d4bMonge-Kantorovich\u4f20\u8f93\u6620\u5c04\uff0c\u652f\u6301\u8bbe\u5907\u7aef\u63a8\u7406\u3002", "result": "\u5728\u771f\u5b9eAR\u5408\u6210\u56fe\u50cf\u4e0a\uff0cMKL-Harmonizer\u65b9\u6cd5\u83b7\u5f97\u4e86\u6700\u4f73\u7efc\u5408\u5f97\u5206\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86AR\u573a\u666f\u7684\u5b9e\u65f6\u989c\u8272\u534f\u8c03\uff0c\u5e76\u53d1\u5e03\u4e86\u4e13\u95e8\u7684AR\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\u548c\u5de5\u5177\u5305\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.12628", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12628", "abs": "https://arxiv.org/abs/2511.12628", "authors": ["Ke Hu", "Liyao Xiang", "Peng Tang", "Weidong Qiu"], "title": "FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions", "comment": "coference", "summary": "Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.", "AI": {"tldr": "FedTopo\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u5f15\u5bfc\u7684\u5757\u7b5b\u9009\u548c\u62d3\u6251\u5d4c\u5165\u6765\u89e3\u51b3\u5f02\u6784\u6570\u636e\u4e0b\u7684\u8868\u793a\u6f02\u79fb\u95ee\u9898\uff0c\u4f7f\u7528\u62d3\u6251\u5bf9\u9f50\u635f\u5931\u6765\u4fdd\u6301\u8de8\u5ba2\u6237\u7aef\u8868\u793a\u7684\u62d3\u6251\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u5728\u5f02\u6784\uff08\u975eI.I.D.\uff09\u5ba2\u6237\u7aef\u6570\u636e\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u7279\u5f81\u8868\u793a\u53d1\u6563\uff0c\u50cf\u7d20\u6216\u8865\u4e01\u7ea7\u76ee\u6807\u65e0\u6cd5\u6355\u6349\u9ad8\u7ef4\u89c6\u89c9\u4efb\u52a1\u6240\u9700\u7684\u5168\u5c40\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u63d0\u51faFedTopo\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u62d3\u6251\u5f15\u5bfc\u5757\u7b5b\u9009\uff08TGBS\uff09\u81ea\u52a8\u9009\u62e9\u6700\u5177\u62d3\u6251\u4fe1\u606f\u7684\u5757\uff1b\u62d3\u6251\u5d4c\u5165\uff08TE\uff09\u91cf\u5316\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u62d3\u6251\u4fe1\u606f\uff1b\u62d3\u6251\u5bf9\u9f50\u635f\u5931\uff08TAL\uff09\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5bfc\u5ba2\u6237\u7aef\u4fdd\u6301\u4e0e\u5168\u5c40\u6a21\u578b\u7684\u62d3\u6251\u4e00\u81f4\u6027\u3002", "result": "\u5728Fashion-MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u7684\u56db\u79cd\u975eI.I.D.\u5206\u533a\u5b9e\u9a8c\u8868\u660e\uff0cFedTopo\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u52a0\u901f\u4e86\u6536\u655b\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u3002", "conclusion": "FedTopo\u901a\u8fc7\u5229\u7528\u62d3\u6251\u4fe1\u606f\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5f02\u6784\u6570\u636e\u5bfc\u81f4\u7684\u8868\u793a\u6f02\u79fb\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u5ba2\u6237\u7aef\u8868\u793a\u5bf9\u9f50\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.12659", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12659", "abs": "https://arxiv.org/abs/2511.12659", "authors": ["Alon Cohen", "Liad Erez", "Steve Hanneke", "Tomer Koren", "Yishay Mansour", "Shay Moran", "Qian Zhang"], "title": "Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back", "comment": null, "summary": "The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\\frac{DS^{1.5}}\u03b5 + \\frac{Nat}{\u03b5^2}$ where $\u03b5$ is the excess risk. This bound is tight up to a $\\sqrt{DS}$ factor in the first term, nearly matching known $Nat/\u03b5^2$ and $DS/\u03b5$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $\u03b5$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u591a\u7c7b\u5206\u7c7b\u7684PAC\u5b66\u4e60\u6837\u672c\u590d\u6742\u5ea6\u7531\u4e24\u4e2a\u7ef4\u5ea6\u5171\u540c\u63a7\u5236\uff1aDS\u7ef4\u5ea6\u548cNatarajan\u7ef4\u5ea6\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u754c\u4e3aDS^{1.5}/\u03b5 + Nat/\u03b5^2\uff0c\u8fd9\u53cd\u6620\u4e86\u591a\u7c7b\u5b66\u4e60\u4e0e\u4e8c\u5143\u5206\u7c7b\u7684\u672c\u8d28\u4e0d\u540c\u3002", "motivation": "\u591a\u7c7b\u5206\u7c7b\u7684PAC\u5b66\u4e60\u957f\u671f\u4ee5\u6765\u9762\u4e34\u6311\u6218\uff0c\u867d\u7136DS\u7ef4\u5ea6\u5df2\u88ab\u8bc1\u660e\u80fd\u591f\u523b\u753b\u591a\u7c7b\u53ef\u5b66\u4e60\u6027\uff0c\u4f46Natarajan\u7ef4\u5ea6\u4e0eDS\u7ef4\u5ea6\u53ef\u4ee5\u4efb\u610f\u53d1\u6563\uff0c\u8fd9\u8868\u660e\u591a\u7c7b\u5b66\u4e60\u53ef\u80fd\u7531\u591a\u4e2a\u7ed3\u6784\u53c2\u6570\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6280\u672f\u65b9\u6cd5\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u4e00\u81f4\u6536\u655b\u6216\u53ef\u5b66\u4e60\u60c5\u51b5\u7ea6\u7b80\u7684\u65b9\u6cd5\u3002\u5173\u952e\u521b\u65b0\u662f\u57fa\u4e8e\u81ea\u9002\u5e94\u4e58\u6027\u6743\u91cd\u7684\u5728\u7ebf\u8fc7\u7a0b\uff0c\u6267\u884c\u6807\u7b7e\u7a7a\u95f4\u7ea6\u7b80\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u7c7bPAC\u6837\u672c\u590d\u6742\u5ea6\u7684\u7d27\u81f4\u4e0a\u754c\u4e3aDS^{1.5}/\u03b5 + Nat/\u03b5^2\uff0c\u5176\u4e2d\u7b2c\u4e00\u9879\u53cd\u6620\u4e86DS\u63a7\u5236\u7684\u673a\u5236\uff0c\u7b2c\u4e8c\u9879\u8868\u660eNatarajan\u7ef4\u5ea6\u4ecd\u7136\u63a7\u5236\u5c0f\u03b5\u65f6\u7684\u6e10\u8fd1\u884c\u4e3a\u3002", "conclusion": "\u591a\u7c7b\u5b66\u4e60\u672c\u8d28\u4e0a\u6d89\u53ca\u4e24\u4e2a\u7ed3\u6784\u53c2\u6570\uff0c\u8fd9\u4e0e\u4e8c\u5143\u5206\u7c7b\u6216\u5728\u7ebf\u5206\u7c7b\uff08\u5206\u522b\u7531VC\u7ef4\u5ea6\u6216Littlestone\u7ef4\u5ea6\u63a7\u5236\uff09\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002"}}
{"id": "2511.12663", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12663", "abs": "https://arxiv.org/abs/2511.12663", "authors": ["Chen Gu", "Yingying Sun", "Yifan She", "Donghui Hu"], "title": "FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning", "comment": null, "summary": "Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.", "AI": {"tldr": "FLClear\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u8f6c\u7f6e\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u65e0\u78b0\u649e\u6c34\u5370\u805a\u5408\u3001\u589e\u5f3a\u6c34\u5370\u5b89\u5168\u6027\u548c\u53ef\u89c6\u5316\u6240\u6709\u6743\u9a8c\u8bc1\u3002", "motivation": "\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6a21\u578b\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u9632\u6b62\u4e2d\u592e\u670d\u52a1\u5668\u6076\u610f\u7be1\u6539\u5168\u5c40\u6a21\u578b\u6216\u865a\u5047\u58f0\u79f0\u6240\u6709\u6743\uff0c\u89e3\u51b3\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5b58\u5728\u7684\u6c34\u5370\u78b0\u649e\u3001\u5b89\u5168\u6027\u4e0d\u8db3\u548c\u9a8c\u8bc1\u673a\u5236\u4e0d\u76f4\u89c2\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faFLClear\u6846\u67b6\uff0c\u5f15\u5165\u8f6c\u7f6e\u6a21\u578b\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u8054\u5408\u4f18\u5316\uff0c\u96c6\u6210\u6c34\u5370\u548c\u4e3b\u8981\u4efb\u52a1\u76ee\u6807\u3002\u9a8c\u8bc1\u65f6\u4ece\u8f6c\u7f6e\u6a21\u578b\u91cd\u6784\u6c34\u5370\uff0c\u901a\u8fc7\u89c6\u89c9\u68c0\u67e5\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u3001\u805a\u5408\u65b9\u6848\u548c\u653b\u51fb\u573a\u666f\u4e0b\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cFLClear\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u5b66\u4e60\u6c34\u5370\u65b9\u6cd5\u3002", "conclusion": "FLClear\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u6c34\u5370\u805a\u5408\u3001\u589e\u5f3a\u5b89\u5168\u6027\u548c\u76f4\u89c2\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u3002"}}
{"id": "2511.12868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12868", "abs": "https://arxiv.org/abs/2511.12868", "authors": ["Ruiqi Yang", "Tian Yun", "Zihan Wang", "Ellie Pavlick"], "title": "Video Finetuning Improves Reasoning Between Frames", "comment": "Accepted at CogInterp @ NeurIPS 2025", "summary": "Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u601d\u7ef4\u94fe\uff08vCoT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u8fc7\u6e21\u4e8b\u4ef6\u63cf\u8ff0\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86\u4ec5\u56fe\u50cf\u6a21\u578b\u4e0e\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u65b9\u9762\u901a\u5e38\u53ea\u662f\u7b80\u5355\u62fc\u63a5\u5e27\u6807\u8bb0\uff0c\u7f3a\u4e4f\u5bf9\u5e27\u95f4\u8fc7\u6e21\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u89c6\u9891\u5fae\u8c03\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u5177\u4f53\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u601d\u7ef4\u94fe\uff08vCoT\uff09\u65b9\u6cd5\uff0c\u751f\u6210\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u8fc7\u6e21\u4e8b\u4ef6\u63cf\u8ff0\u4f5c\u4e3a\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u4ec5\u56fe\u50cf\u6a21\u578b\u4e0e\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u5728\u6709\u65e0\u8fc7\u6e21\u7ebf\u7d22\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u3002", "result": "vCoT\u663e\u8457\u63d0\u5347\u4e86\u4ec5\u56fe\u50cf\u6a21\u578b\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4f46\u5bf9\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u4ec5\u6709\u8fb9\u9645\u589e\u76ca\uff1b\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u80fd\u591f\u5c06\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u9759\u6001\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u89c6\u9891\u5fae\u8c03\u6a21\u578b\u5df2\u7ecf\u9690\u5f0f\u638c\u63e1\u4e86\u5e27\u95f4\u8fc7\u6e21\u63a8\u7406\u80fd\u529b\uff0c\u800cvCoT\u4e3a\u4ec5\u56fe\u50cf\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u663e\u5f0f\u63a8\u7406\u652f\u6301\uff0c\u89c6\u9891\u6a21\u578b\u8fd8\u5177\u5907\u5411\u9759\u6001\u4efb\u52a1\u8fc1\u79fb\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u7684\u7279\u70b9\u3002"}}
{"id": "2511.12682", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.12682", "abs": "https://arxiv.org/abs/2511.12682", "authors": ["Amirpasha Hedayat", "Karthik Duraisamy"], "title": "Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction", "comment": "13 pages, 7 figures, Preprint", "summary": "Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u964d\u9636\u5efa\u6a21\u6846\u67b6\u7528\u4e8e\u77ed\u671f\u5929\u6c14\u9884\u62a5\uff0c\u901a\u8fc7ResNet\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548c\u5757\u6ce8\u610f\u529b\u6a21\u5757\u964d\u4f4e\u9ad8\u7ef4\u5929\u6c14\u6570\u636e\u7684\u7ef4\u5ea6\uff0c\u5728\u5ef6\u8fdf\u5d4c\u5165\u7684\u6f5c\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7ebf\u6027\u7b97\u5b50\u6765\u6355\u6349\u52a8\u6001\u3002\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u6570\u636e\u671f\u95f4\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6cdb\u5316\u5230\u672a\u6765\u72b6\u6001\u65f6\u5b58\u5728\u9650\u5236\uff0c\u4e3b\u8981\u74f6\u9888\u662f\u6295\u5f71\u8bef\u5dee\u800c\u975e\u63a8\u7406\u8bef\u5dee\u3002", "motivation": "\u5929\u6c14\u9884\u62a5\u662f\u4e00\u4e2a\u590d\u6742\u7684\u975e\u7ebf\u6027\u6df7\u6c8c\u9ad8\u7ef4\u52a8\u529b\u7cfb\u7edf\u9884\u6d4b\u95ee\u9898\u3002\u4e0e\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u7684AI\u9a71\u52a8\u6a21\u578b\u4e0d\u540c\uff0c\u672c\u6587\u6846\u67b6\u4f18\u5148\u8003\u8651\u6548\u7387\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7cbe\u5ea6\uff0c\u65e8\u5728\u4e3a\u957f\u671f\u6c14\u5019\u5efa\u6a21\u7b49\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\u7684\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u57fa\u7ebf\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eResNet\u7684\u5377\u79ef\u81ea\u7f16\u7801\u5668\uff0c\u589e\u5f3a\u5757\u6ce8\u610f\u529b\u6a21\u5757\u6765\u964d\u4f4e\u9ad8\u7ef4\u5929\u6c14\u6570\u636e\u7684\u7ef4\u5ea6\uff1b\u5728\u5ef6\u8fdf\u5d4c\u5165\u7684\u6f5c\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7ebf\u6027\u7b97\u5b50\u6765\u9ad8\u6548\u6355\u6349\u52a8\u6001\uff1b\u4f7f\u7528ERA5\u518d\u5206\u6790\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u6570\u636e\u671f\u95f4\u80fd\u6709\u6548\u9884\u6d4b\u5929\u6c14\u6a21\u5f0f\uff0c\u4f46\u5728\u6cdb\u5316\u5230\u672a\u6765\u72b6\u6001\u65f6\u5b58\u5728\u91cd\u8981\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u7a97\u53e3\u4e4b\u5916\u7ef4\u6301\u9884\u6d4b\u7cbe\u5ea6\u65b9\u9762\uff1b\u53d1\u73b0\u5929\u6c14\u7cfb\u7edf\u8868\u73b0\u51fa\u5f3a\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u53ef\u901a\u8fc7\u9002\u5f53\u6784\u5efa\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u7ebf\u6027\u64cd\u4f5c\u6709\u6548\u6355\u6349\uff1b\u6295\u5f71\u8bef\u5dee\u800c\u975e\u63a8\u7406\u8bef\u5dee\u662f\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u6df7\u6c8c\u7cfb\u7edf\u964d\u9636\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u6df7\u5408\u65b9\u6cd5\u7684\u673a\u9047\uff1a\u5c06\u9ad8\u6548\u7684\u964d\u9636\u6a21\u578b\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u4e0e\u66f4\u590d\u6742\u7684AI\u67b6\u6784\u7ed3\u5408\uff0c\u7279\u522b\u662f\u5728\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\u7684\u957f\u671f\u6c14\u5019\u5efa\u6a21\u5e94\u7528\u4e2d\u3002"}}
{"id": "2511.12695", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12695", "abs": "https://arxiv.org/abs/2511.12695", "authors": ["Minghui Chen", "Hrad Ghoukasian", "Ruinan Jin", "Zehua Wang", "Sai Praneeth Karimireddy", "Xiaoxiao Li"], "title": "A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning", "comment": "33 pages, 6 figures, 8 tables", "summary": "Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u7ebf\u6027\u63a2\u6d4b\u540e\u5fae\u8c03(LP-FT)\u7b56\u7565\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\uff0c\u4ee5\u5e73\u8861\u4e2a\u6027\u5316\u4e0e\u6cdb\u5316\u6027\u80fd\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u7f13\u89e3\u8054\u90a6\u7279\u5f81\u626d\u66f2\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0b\u96be\u4ee5\u5e73\u8861\u5168\u5c40\u6cdb\u5316\u4e0e\u672c\u5730\u4e2a\u6027\u5316\uff0c\u4f20\u7edf\u4e2a\u6027\u5316\u5fae\u8c03\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u6216\u65e0\u6cd5\u9002\u5e94\u9886\u57df\u504f\u79fb\u3002", "method": "\u5c06\u96c6\u4e2d\u5f0f\u73af\u5883\u4e2d\u7684\u7ebf\u6027\u63a2\u6d4b\u540e\u5fae\u8c03\u7b56\u7565(LP-FT)\u9002\u914d\u5230\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u53c2\u6570\u66f4\u65b0\u6765\u7f13\u89e3\u7279\u5f81\u626d\u66f2\u95ee\u9898\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u548c6\u79cd\u4e2a\u6027\u5316\u5fae\u8c03\u53d8\u4f53\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\u8868\u660e\uff0cLP-FT\u5728\u5e73\u8861\u4e2a\u6027\u5316\u4e0e\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63ed\u793a\u4e86\u8054\u90a6\u7279\u5f81\u626d\u66f2\u73b0\u8c61\u3002", "conclusion": "LP-FT\u901a\u8fc7\u5206\u9636\u6bb5\u53c2\u6570\u66f4\u65b0\u6709\u6548\u7f13\u89e3\u8054\u90a6\u7279\u5f81\u626d\u66f2\uff0c\u5728\u90e8\u5206\u7279\u5f81\u91cd\u53e0\u548c\u534f\u53d8\u91cf-\u6982\u5ff5\u504f\u79fb\u7b49\u6761\u4ef6\u4e0b\u4f18\u4e8e\u6807\u51c6\u5fae\u8c03\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7a33\u5065\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2511.12878", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.12878", "abs": "https://arxiv.org/abs/2511.12878", "authors": ["Junyi Ma", "Wentao Bao", "Jingyi Xu", "Guanzhong Sun", "Yu Zheng", "Erhang Zhang", "Xieyuanli Chen", "Hesheng Wang"], "title": "Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views", "comment": "Extended journal version of MMTwin (IROS'25)", "summary": "Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., \"how to interact\"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., \"when to interact\") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEgoLoc\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5b9a\u4f4d\u624b\u4e0e\u7269\u4f53\u63a5\u89e6\u548c\u5206\u79bb\u7684\u65f6\u95f4\u6233\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u786e\u5b9a\u4f4d\u4ea4\u4e92\u5173\u952e\u65f6\u523b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ea4\u4e92\u52a8\u4f5c\u7684\u884c\u4e3a\u8303\u5f0f\uff08\"\u5982\u4f55\u4ea4\u4e92\"\uff09\uff0c\u4f46\u66f4\u6311\u6218\u6027\u7684\u7ec6\u7c92\u5ea6\u95ee\u9898\u2014\u2014\u6355\u6349\u624b\u4e0e\u76ee\u6807\u7269\u4f53\u63a5\u89e6\u548c\u5206\u79bb\u7684\u5173\u952e\u65f6\u523b\uff08\"\u4f55\u65f6\u4ea4\u4e92\"\uff09\u4ecd\u672a\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u5bf9\u6df7\u5408\u73b0\u5b9e\u4e2d\u7684\u6c89\u6d78\u5f0f\u4ea4\u4e92\u4f53\u9a8c\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faEgoLoc\u65b9\u6cd5\uff0c\u5f15\u5165\u624b\u52a8\u529b\u5b66\u5f15\u5bfc\u91c7\u6837\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u89c9\u63d0\u793a\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u63a5\u89e6/\u5206\u79bb\u5c5e\u6027\u3001\u5b9a\u4f4d\u7279\u5b9a\u65f6\u95f4\u6233\uff0c\u5e76\u63d0\u4f9b\u95ed\u73af\u53cd\u9988\u8fdb\u884c\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u7269\u4f53\u63a9\u7801\u548c\u52a8\u8bcd-\u540d\u8bcd\u5206\u7c7b\u6cd5\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cEgoLoc\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5b9e\u73b0\u4e86\u53ef\u4fe1\u7684\u65f6\u95f4\u4ea4\u4e92\u5b9a\u4f4d\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6709\u6548\u4fc3\u8fdb\u591a\u4e2a\u4e0b\u6e38\u5e94\u7528\u7684\u80fd\u529b\u3002", "conclusion": "EgoLoc\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u7269\u4f53\u63a9\u7801\u548c\u52a8\u8bcd-\u540d\u8bcd\u5206\u7c7b\u6cd5\u7684\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a8\u5e7f\u7684\u96f6\u6837\u672c\u5b9e\u73b0\uff0c\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4ea4\u4e92\u65f6\u523b\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12706", "abs": "https://arxiv.org/abs/2511.12706", "authors": ["Daniel Furelos-Blanco", "Charles Pert", "Frederik Kelbel", "Alex F. Spies", "Alessandra Russo", "Michael Dennis"], "title": "Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs", "comment": "Extended version of paper accepted for publication at the 40th AAAI Conference on Artificial Intelligence (AAAI)", "summary": "Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.", "AI": {"tldr": "ATLAS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u81ea\u52a8\u8bfe\u7a0b\u8bbe\u8ba1\u5728\u4efb\u52a1\u548c\u5173\u5361\u5c42\u9762\u751f\u6210\u53ef\u89e3\u51b3\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u7ec4\u5408\uff0c\u663e\u8457\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u3002", "motivation": "\u8bad\u7ec3\u901a\u7528\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u9075\u5faa\u590d\u6742\u6307\u4ee4\u5b58\u5728\u6311\u6218\uff0c\u968f\u673a\u91c7\u6837\u4efb\u52a1-\u5173\u5361\u5bf9\u7ecf\u5e38\u4ea7\u751f\u4e0d\u53ef\u89e3\u51b3\u7684\u7ec4\u5408\uff0c\u9700\u8981\u5171\u540c\u8bbe\u8ba1\u4efb\u52a1\u548c\u5173\u5361\u3002", "method": "\u57fa\u4e8e\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1(UED)\uff0cATLAS\u751f\u6210\u4efb\u52a1\u548c\u5173\u5361\u7684\u8054\u5408\u81ea\u52a8\u8bfe\u7a0b\uff0c\u5229\u7528\u4efb\u52a1\u548c\u5173\u5361\u7ed3\u6784\u7684\u7a81\u53d8\u6765\u52a0\u901f\u7b56\u7565\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eATLAS\u5728Minigrid\u73af\u5883\u4e2d\u5927\u5e45\u4f18\u4e8e\u968f\u673a\u91c7\u6837\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u53ef\u89e3\u51b3\u5bf9\u91c7\u6837\u6982\u7387\u8f83\u4f4e\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "ATLAS\u901a\u8fc7\u8054\u5408\u81ea\u52a8\u8bfe\u7a0b\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1-\u5173\u5361\u914d\u5bf9\u95ee\u9898\uff0c\u5229\u7528\u4efb\u52a1\u548c\u5173\u5361\u7ed3\u6784\u7684\u7a81\u53d8\u52a0\u901f\u4e86\u9ad8\u6027\u80fd\u7b56\u7565\u7684\u6536\u655b\u3002"}}
{"id": "2511.12880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12880", "abs": "https://arxiv.org/abs/2511.12880", "authors": ["Zihao Lin", "Zhenshan Shi", "Sasa Zhao", "Hanwei Zhu", "Lingyu Zhu", "Baoliang Chen", "Lei Mo"], "title": "Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings", "comment": null, "summary": "Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u81ea\u52a8\u53ef\u89e3\u91ca\u7ed8\u56fe\u521b\u9020\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u591a\u4efb\u52a1\u5b66\u4e60\u540c\u65f6\u9884\u6d4b\u521b\u9020\u529b\u5206\u6570\u3001\u5206\u7c7b\u5185\u5bb9\u7c7b\u578b\u548c\u63d0\u53d6\u98ce\u683c\u7279\u5f81\u3002", "motivation": "\u5f53\u524d\u521b\u9020\u529b\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u4e13\u5bb6\u4e3b\u89c2\u8bc4\u5206\uff0c\u65e2\u8017\u65f6\u53c8\u4e3b\u89c2\u3002\u53d7\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\uff0c\u5c06\u521b\u9020\u529b\u91cd\u65b0\u89e3\u91ca\u4e3a\u5185\u5bb9\uff08\u753b\u4ec0\u4e48\uff09\u548c\u98ce\u683c\uff08\u600e\u4e48\u753b\uff09\u4e24\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7684\u51fd\u6570\u3002", "method": "\u9996\u5148\u6269\u5c55\u73b0\u6709\u521b\u9020\u529b\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u589e\u52a0\u5185\u5bb9\u7c7b\u522b\u6807\u6ce8\uff1b\u7136\u540e\u63d0\u51fa\u591a\u6a21\u6001\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u5f15\u5165\u6761\u4ef6\u5b66\u4e60\u673a\u5236\uff0c\u6839\u636e\u7ed8\u56fe\u7684\u98ce\u683c\u548c\u8bed\u4e49\u7ebf\u7d22\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u7684\u53ef\u89e3\u91ca\u53ef\u89c6\u5316\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u4e14\u53ef\u89e3\u91ca\u5730\u8bc4\u4f30\u7ed8\u56fe\u521b\u9020\u529b\uff0c\u4ee3\u7801\u548c\u6807\u6ce8\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2511.12895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12895", "abs": "https://arxiv.org/abs/2511.12895", "authors": ["Kaixuan Zhang", "Minxian Li", "Mingwu Ren", "Jiankang Deng", "Xiatian Zhu"], "title": "Reconstructing 3D Scenes in Native High Dynamic Range", "comment": null, "summary": "High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NH-3DGS\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u76f4\u63a5\u4ece\u539f\u751fHDR\u89c2\u6d4b\u6570\u636e\u4e2d\u8fdb\u884c3D\u573a\u666f\u91cd\u5efa\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u6280\u672f\uff0c\u5728\u6574\u4e2a\u91cd\u5efa\u6d41\u7a0b\u4e2d\u4fdd\u6301\u5b8c\u6574\u52a8\u6001\u8303\u56f4\u3002", "motivation": "\u4e13\u4e1a\u6570\u5b57\u5a92\u4f53\u5236\u4f5c\u9700\u8981HDR\u6210\u50cf\uff0c\u4f46\u73b0\u67093D\u573a\u666f\u91cd\u5efa\u4e3b\u8981\u57fa\u4e8eLDR\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u66dd\u5149\u878d\u5408\u6216\u9006\u8272\u8c03\u6620\u5c04\uff0c\u589e\u52a0\u4e86\u6355\u83b7\u590d\u6742\u6027\u5e76\u4f9d\u8d56\u5408\u6210\u76d1\u7763\u3002", "method": "\u63d0\u51faNative High dynamic range 3D Gaussian Splatting (NH-3DGS)\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u9896\u7684\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u989c\u8272\u8868\u793a\uff0c\u76f4\u63a5\u4ece\u539f\u751fHDR\u76f8\u673a\u6570\u636e\u8fdb\u884c\u4f18\u5316\uff0c\u4fdd\u6301\u5b8c\u6574\u52a8\u6001\u8303\u56f4\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u591a\u89c6\u89d2HDR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNH-3DGS\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u52a8\u6001\u8303\u56f4\u4fdd\u6301\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u539f\u751fHDR\u6355\u83b7\u5b9e\u73b0\u4e13\u4e1a\u7ea73D\u91cd\u5efa\u3002", "conclusion": "NH-3DGS\u662f\u9996\u4e2a\u76f4\u63a5\u4ece\u539f\u751fHDR\u89c2\u6d4b\u6570\u636e\u5efa\u6a21\u76843D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u4eae\u5ea6-\u8272\u5ea6\u5206\u89e3\u6280\u672f\u5b9e\u73b0\u4e86\u4e13\u4e1a\u7ea7\u7684\u91cd\u5efa\u6548\u679c\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2511.12722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12722", "abs": "https://arxiv.org/abs/2511.12722", "authors": ["Nakshatra Gupta", "Sumanth Prabhu", "Supratik Chakraborty", "R Venkatesh"], "title": "On Robustness of Linear Classifiers to Targeted Data Poisoning", "comment": null, "summary": "Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6570\u636e\u96c6\u5bf9\u6807\u7b7e\u6270\u52a8\u578b\u6570\u636e\u6295\u6bd2\u653b\u51fb\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u8be5\u95ee\u9898\u662fNP\u5b8c\u5168\u7684\uff0c\u5e76\u5f00\u53d1\u4e86\u8ba1\u7b97\u9c81\u68d2\u6027\u4e0a\u4e0b\u754c\u7684\u6709\u6548\u6280\u672f\u3002", "motivation": "\u6570\u636e\u6295\u6bd2\u653b\u51fb\u5a01\u80c1\u6a21\u578b\u53ef\u4fe1\u5ea6\uff0c\u624b\u52a8\u68c0\u6d4b\u56f0\u96be\uff0c\u9700\u8981\u81ea\u52a8\u8bc4\u4f30\u6570\u636e\u96c6\u5bf9\u8fd9\u7c7b\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5728\u53ea\u80fd\u6270\u52a8\u8bad\u7ec3\u6570\u636e\u6807\u7b7e\u7684\u5a01\u80c1\u6a21\u578b\u4e0b\uff0c\u63d0\u51fa\u8ba1\u7b97\u9c81\u68d2\u6027\u4e0a\u4e0b\u754c\u7684\u6280\u672f\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9ad8\u6548\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8d85\u8fc7\u8bc6\u522b\u9c81\u68d2\u6027\u8fb9\u754c\u7684\u6295\u6bd2\u4f1a\u663e\u8457\u5f71\u54cd\u6d4b\u8bd5\u70b9\u5206\u7c7b\uff0c\u4e14\u8be5\u65b9\u6cd5\u5728\u66f4\u591a\u60c5\u51b5\u4e0b\u80fd\u6210\u529f\u8ba1\u7b97\u8fb9\u754c\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc4\u4f30\u6570\u636e\u96c6\u5bf9\u6807\u7b7e\u6270\u52a8\u578b\u6570\u636e\u6295\u6bd2\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2511.12899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12899", "abs": "https://arxiv.org/abs/2511.12899", "authors": ["Hao Li", "Zhenfeng Zhuang", "Jingyu Lin", "Yu Liu", "Yifei Chen", "Qiong Peng", "Lequan Yu", "Liansheng Wang"], "title": "FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI", "comment": "Accepted by AAAI2026", "summary": "Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u57df\u5206\u89e3\u9884\u5904\u7406\uff08FDP\uff09\u7684\u65e0\u76d1\u7763\u8111MRI\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u75c5\u7406\u7279\u5f81\u7684\u9891\u57df\u7279\u6027\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u540c\u65f6\u6291\u5236\u75c5\u7406\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u8111\u89e3\u5256\u7ed3\u6784\u7684\u591a\u6837\u6027\u548c\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u8111MRI\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u6a21\u62df\u566a\u58f0\u8bad\u7ec3\u751f\u6210\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f\u771f\u5b9e\u4e34\u5e8a\u75c5\u53d8\u7684\u751f\u7269\u7269\u7406\u4fdd\u771f\u5ea6\u548c\u5f62\u6001\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u9891\u7387\u5206\u89e3\u9884\u5904\u7406\uff08FDP\uff09\u6846\u67b6\uff0c\u9996\u6b21\u5229\u7528\u9891\u57df\u91cd\u5efa\u540c\u65f6\u5b9e\u73b0\u75c5\u7406\u6291\u5236\u548c\u89e3\u5256\u7ed3\u6784\u4fdd\u6301\u3002\u8be5\u65b9\u6cd5\u53ef\u4e0e\u73b0\u6709\u5f02\u5e38\u6a21\u62df\u6280\u672f\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eFDP\u80fd\u6301\u7eed\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u4e0eLDM\u7ed3\u5408\u65f6DICE\u5206\u6570\u63d0\u9ad8\u4e8617.63%\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u65b9\u6cd5\u4e0a\u90fd\u4fdd\u6301\u4e86\u7a33\u5065\u7684\u6539\u8fdb\u3002", "conclusion": "FDP\u901a\u8fc7\u9891\u57df\u5206\u6790\u6709\u6548\u63d0\u5347\u4e86\u8111MRI\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12908", "abs": "https://arxiv.org/abs/2511.12908", "authors": ["Junbo Zou", "Haotian Xia", "Zhen Ye", "Shengjie Zhang", "Christopher Lai", "Vicente Ordonez", "Weining Shen", "Hanjie Chen"], "title": "DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning", "comment": null, "summary": "Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.", "AI": {"tldr": "DeepSport\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u591a\u4efb\u52a1\u3001\u591a\u8fd0\u52a8\u89c6\u9891\u7406\u89e3MLLM\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u8fed\u4ee3\u63a8\u7406\u548c\u4e13\u7528\u5e27\u63d0\u53d6\u5de5\u5177\u5b9e\u73b0\"\u89c6\u9891\u601d\u8003\"\uff0c\u57286.7k\u95ee\u9898\u6d4b\u8bd5\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4f53\u80b2\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u5b58\u5728\u5355\u8fd0\u52a8\u4e2d\u5fc3\u5316\u3001\u4efb\u52a1\u5c40\u9650\u6216\u7f3a\u4e4f\u5b66\u4e60\u63a8\u7406\u8fc7\u7a0b\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u9ad8\u901f\u52a8\u6001\u611f\u77e5\u3001\u590d\u6742\u89c4\u5219\u7406\u89e3\u548c\u957f\u65f6\u5e8f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6570\u636e\u84b8\u998f\u7ba1\u9053\u4ece10\u4e2a\u6570\u636e\u6e90\u5408\u6210\u9ad8\u8d28\u91cf\u601d\u7ef4\u94fe\u8f68\u8ff9\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u76d1\u7763\u5fae\u8c03+\u5e26\u95e8\u63a7\u5de5\u5177\u4f7f\u7528\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u901a\u8fc7\u4e13\u7528\u5e27\u63d0\u53d6\u5de5\u5177\u5b9e\u73b0\u4e3b\u52a8\u8fed\u4ee3\u63a8\u7406\u3002", "result": "\u57286.7k\u95ee\u9898\u6d4b\u8bd5\u57fa\u51c6\u4e0a\uff0cDeepSport\u663e\u8457\u4f18\u4e8e\u4e13\u6709\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u9886\u57df\u7279\u5b9a\u89c6\u9891\u63a8\u7406\u5efa\u7acb\u4e86\u65b0\u57fa\u7840\uff0c\u80fd\u591f\u5e94\u5bf9\u591a\u6837\u5316\u4f53\u80b2\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2511.12725", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12725", "abs": "https://arxiv.org/abs/2511.12725", "authors": ["William Ward Armstrong"], "title": "Convolutional Model Trees", "comment": "9 pages. No figures. This paper gives an algorithm for creating a continuously differentiable approximation from sample data from the same type of function(in theory) using a forest of model trees (like CART trees with linear functions instead of constants)", "summary": "A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u6a21\u578b\u6811\u68ee\u6797\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u964d\u91c7\u6837\u56fe\u50cf\u3001\u786e\u5b9a\u6811\u8d85\u5e73\u9762\u3001\u5e94\u7528\u5377\u79ef\u5904\u7406\u8bad\u7ec3\u56fe\u50cf\u7684\u5c0f\u53d8\u5f62\uff0c\u4ee5\u53ca\u521b\u5efa\u6a21\u578b\u6811\u68ee\u6797\u6765\u63d0\u9ad8\u7cbe\u5ea6\u548c\u5e73\u6ed1\u62df\u5408\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u50cf\u7d20\u3001\u8d85\u5e73\u9762\u7cfb\u6570\u548c\u53f6\u51fd\u6570\u7cfb\u6570\u4e4b\u95f4\u76841\u5bf91\u5bf9\u5e94\u5173\u7cfb\uff0c\u80fd\u591f\u5904\u7406\u66f4\u5927\u7684\u53d8\u5f62\u5982\u4efb\u610f\u65cb\u8f6c\u6216\u89c6\u89d2\u53d8\u5316\u3002\u8fd8\u63cf\u8ff0\u4e86\u5e73\u6ed1\u68ee\u6797\u8f93\u51fa\u4ee5\u4ea7\u751f\u8fde\u7eed\u53ef\u5fae\u903c\u8fd1\u7684\u7406\u8bba\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6536\u655b\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5904\u7406\u56fe\u50cf\u51fd\u6570\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u8981\u5904\u7406\u56fe\u50cf\u7684\u5c0f\u53d8\u5f62\u548c\u66f4\u5927\u53d8\u5f62\uff08\u5982\u65cb\u8f6c\u548c\u89c6\u89d2\u53d8\u5316\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5e73\u6ed1\u62df\u5408\u3002", "method": "\u901a\u8fc7\u591a\u4e2a\u6b65\u9aa4\u6784\u5efa\u6a21\u578b\u6811\u68ee\u6797\uff1a\u964d\u91c7\u6837\u56fe\u50cf\u3001\u786e\u5b9a\u6811\u8d85\u5e73\u9762\u3001\u5bf9\u8d85\u5e73\u9762\u5e94\u7528\u5377\u79ef\u5904\u7406\u5c0f\u53d8\u5f62\u3001\u521b\u5efa\u6a21\u578b\u6811\u68ee\u6797\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002\u5229\u7528\u50cf\u7d20\u3001\u8d85\u5e73\u9762\u7cfb\u6570\u548c\u53f6\u51fd\u6570\u7cfb\u6570\u4e4b\u95f4\u76841\u5bf91\u5bf9\u5e94\u5173\u7cfb\u5904\u7406\u5927\u53d8\u5f62\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u56fe\u50cf\u7684\u5c0f\u53d8\u5f62\u548c\u66f4\u5927\u53d8\u5f62\uff08\u5982\u4efb\u610f\u65cb\u8f6c\u6216\u89c6\u89d2\u53d8\u5316\uff09\uff0c\u901a\u8fc7\u6a21\u578b\u6811\u68ee\u6797\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5e73\u6ed1\u62df\u5408\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u578b\u6811\u68ee\u6797\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u56fe\u50cf\u7684\u5404\u79cd\u53d8\u5f62\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u53ef\u5fae\u7684\u903c\u8fd1\uff0c\u5e76\u8bc1\u660e\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6536\u655b\u6027\uff0c\u4e3a\u56fe\u50cf\u51fd\u6570\u62df\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12909", "abs": "https://arxiv.org/abs/2511.12909", "authors": ["Yaohua Zha", "Xue Yuerong", "Chunlin Fan", "Yuansong Wang", "Tao Dai", "Ke Chen", "Shu-Tao Xia"], "title": "CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection", "comment": "Accepted to AAAI 2026", "summary": "Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66f2\u7387\u589e\u5f3a\u81ea\u76d1\u7763\u5b66\u4e60(CASL)\u76843D\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u66f2\u7387\u63d0\u793a\u5f15\u5bfc\u89e3\u7801\u5668\u91cd\u5efa\u70b9\u4e91\u5750\u6807\uff0c\u65e0\u9700\u7279\u5b9a\u5f02\u5e38\u68c0\u6d4b\u673a\u5236\u5373\u53ef\u5728\u7edf\u4e00\u5fae\u8c03\u8303\u5f0f\u4e0b\u5b9e\u73b0\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u6cdb\u5316\u6027\uff0c\u800c\u7ecf\u5178\u81ea\u76d1\u7763\u70b9\u4e91\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u5f00\u53d1\u4e00\u4e2a\u66f4\u901a\u7528\u76843D\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u4e14\u4e0d\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u3002", "method": "\u57fa\u4e8eU-Net\u67b6\u6784\uff0c\u5f15\u5165\u591a\u5c3a\u5ea6\u66f2\u7387\u63d0\u793a\u6765\u6307\u5bfc\u89e3\u7801\u5668\u9884\u6d4b\u6bcf\u4e2a\u70b9\u7684\u7a7a\u95f4\u5750\u6807\uff0c\u901a\u8fc7\u91cd\u5efa\u8303\u5f0f\u5b9e\u73b0\u66f2\u7387\u589e\u5f3a\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u4ec5\u4f7f\u7528\u70b9\u66f2\u7387\u4f5c\u4e3a\u5f02\u5e38\u8bc4\u5206\u5df2\u8d85\u8d8a\u591a\u4e2a\u7ecf\u5178\u81ea\u76d1\u7763\u548c\u4e13\u7528\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0cCASL\u6846\u67b6\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fbe\u5230\u9886\u5148\u6027\u80fd\uff0c\u4e14\u5b66\u5230\u7684\u8868\u793a\u5728\u6807\u51c63D\u7406\u89e3\u4efb\u52a1\u4e2d\u6cdb\u5316\u826f\u597d\u3002", "conclusion": "\u66f2\u7387\u57283D\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0cCASL\u6846\u67b6\u901a\u8fc7\u7b80\u5355\u5f02\u5e38\u5206\u7c7b\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u901a\u75283D\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12742", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12742", "abs": "https://arxiv.org/abs/2511.12742", "authors": ["Zhongteng Cai", "Yaxuan Wang", "Yang Liu", "Xueru Zhang"], "title": "Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering", "comment": "Accepted by AAAI-26", "summary": "As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop\" that can lead to training instability or \\textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \\textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6f5c\u5728\u7a7a\u95f4\u8fc7\u6ee4\uff08LSF\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u6ee4\u6df7\u5408\u6570\u636e\u96c6\u4e2d\u4e0d\u592a\u771f\u5b9e\u7684\u5408\u6210\u6570\u636e\u6765\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u800c\u65e0\u9700\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u6216\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u968f\u7740\u5408\u6210\u6570\u636e\u5728\u4e92\u8054\u7f51\u4e0a\u7684\u6269\u6563\uff0c\u5b83\u4eec\u7ecf\u5e38\u88ab\u7528\u6765\u8bad\u7ec3\u65b0\u4e00\u4ee3\u751f\u6210\u6a21\u578b\uff0c\u5f62\u6210'\u81ea\u6211\u6d88\u8017\u5faa\u73af'\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6216\u6a21\u578b\u5d29\u6e83\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u589e\u52a0\u8ba1\u7b97\u6210\u672c\uff0c\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3002", "method": "\u57fa\u4e8e\u5bf9\u81ea\u6d88\u8017\u6269\u6563\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u52a8\u6001\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u63d0\u51fa\u6f5c\u5728\u7a7a\u95f4\u8fc7\u6ee4\uff08LSF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc7\u6ee4\u6389\u6df7\u5408\u6570\u636e\u96c6\u4e2d\u4e0d\u592a\u771f\u5b9e\u7684\u5408\u6210\u6570\u636e\u6765\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLSF\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u7f13\u89e3\u6a21\u578b\u5d29\u6e83\uff0c\u4e14\u4e0d\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u6216\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "LSF\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u81ea\u6d88\u8017\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u6765\u8bc6\u522b\u548c\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u5408\u6210\u6570\u636e\u3002"}}
{"id": "2511.12917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12917", "abs": "https://arxiv.org/abs/2511.12917", "authors": ["Ruishu Zhu", "Sida Huang", "Ziheng Jiao", "Hongyuan Zhang"], "title": "Explore How to Inject Beneficial Noise in MLLMs", "comment": "Accepted by AAAI 2026", "summary": "Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\\sim2\\%$ additional parameters. The relevant code is uploaded in the supplementary.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6ce8\u5165\u6709\u76ca\u968f\u673a\u566a\u58f0\u7684\u65b0\u578b\u5fae\u8c03\u7b56\u7565MuNG\uff0c\u5728\u4ec5\u8c03\u65741-2%\u989d\u5916\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8d85\u8fc7\u5b8c\u6574\u5fae\u8c03\u548c\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u8de8\u6a21\u6001\u5f02\u8d28\u6027\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u4ece\u53d8\u5206\u63a8\u7406\u89d2\u5ea6\u91cd\u65b0\u5236\u5b9aMLLMs\u63a8\u7406\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001\u566a\u58f0\u751f\u6210\u5668\u52a8\u6001\u5206\u6790\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u8de8\u6a21\u6001\u5173\u7cfb\uff0c\u751f\u6210\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u6709\u76ca\u566a\u58f0\u3002", "result": "\u5728QwenVL\u548cLLaVA\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u5b8c\u6574\u53c2\u6570\u5fae\u8c03\u548c\u5176\u4ed6\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\uff0c\u540c\u65f6\u4ec5\u9700\u8c03\u6574\u7ea61-2%\u7684\u989d\u5916\u53c2\u6570\u3002", "conclusion": "\u6ce8\u5165\u5b9a\u5236\u5316\u566a\u58f0\u80fd\u6709\u6548\u6291\u5236\u4e0d\u76f8\u5173\u8bed\u4e49\u6210\u5206\uff0c\u663e\u8457\u6539\u5584\u8de8\u6a21\u6001\u8868\u793a\u5bf9\u9f50\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2511.12745", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.12745", "abs": "https://arxiv.org/abs/2511.12745", "authors": ["Vivek Chawla", "Boris Slautin", "Utkarsh Pratiush", "Dayakar Penumadu", "Sergei Kalinin"], "title": "DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes", "comment": "33 pages, 10 main figures, 7 additional in SI", "summary": "Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.", "AI": {"tldr": "DIVIDE\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u673a\u5236\u7279\u5b9a\u7684\u6df1\u5ea6\u7f16\u7801\u5668\u548c\u7ed3\u6784\u5316\u9ad8\u65af\u8fc7\u7a0b\u6765\u89e3\u8026\u79d1\u5b66\u6570\u636e\u96c6\u4e2d\u7684\u591a\u4e2a\u72ec\u7acb\u673a\u5236\u5f71\u54cd\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u673a\u5236\u611f\u77e5\u9884\u6d4b\u548c\u9ad8\u6548\u4e3b\u52a8\u5b66\u4e60\u3002", "motivation": "\u79d1\u5b66\u6570\u636e\u96c6\u901a\u5e38\u6765\u81ea\u591a\u4e2a\u72ec\u7acb\u673a\u5236\uff08\u5982\u7a7a\u95f4\u3001\u5206\u7c7b\u6216\u7ed3\u6784\u6548\u5e94\uff09\u7684\u7ec4\u5408\u5f71\u54cd\uff0c\u8fd9\u4e9b\u5f71\u54cd\u7684\u6df7\u5408\u63a9\u76d6\u4e86\u5404\u81ea\u7684\u8d21\u732e\uff0c\u9700\u8981\u89e3\u8026\u8fd9\u4e9b\u5f71\u54cd\u4ee5\u83b7\u5f97\u66f4\u6e05\u6670\u7684\u673a\u5236\u7406\u89e3\u3002", "method": "\u96c6\u6210\u673a\u5236\u7279\u5b9a\u7684\u6df1\u5ea6\u7f16\u7801\u5668\u4e0e\u7ed3\u6784\u5316\u9ad8\u65af\u8fc7\u7a0b\u5728\u8054\u5408\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u7f16\u7801\u5668\u9694\u79bb\u4e0d\u540c\u673a\u5236\uff0c\u9ad8\u65af\u8fc7\u7a0b\u6355\u83b7\u5b83\u4eec\u7684\u7ec4\u5408\u6548\u5e94\u5e76\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\uff0c\u652f\u6301\u7ed3\u6784\u5316\u5148\u9a8c\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u3001FerroSIM\u94c1\u7535\u6a21\u5f0f\u6a21\u62df\u548cPbTiO3\u8584\u819c\u5b9e\u9a8cPFM\u78c1\u6ede\u56de\u7ebf\u4e0a\uff0cDIVIDE\u6210\u529f\u5206\u79bb\u673a\u5236\uff0c\u91cd\u73b0\u52a0\u6027\u548c\u7f29\u653e\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u5728\u566a\u58f0\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "DIVIDE\u6846\u67b6\u6709\u6548\u89e3\u8026\u79d1\u5b66\u6570\u636e\u96c6\u4e2d\u7684\u72ec\u7acb\u673a\u5236\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u9884\u6d4b\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u53ef\u81ea\u7136\u6269\u5c55\u5230\u591a\u529f\u80fd\u6570\u636e\u96c6\uff08\u673a\u68b0\u3001\u7535\u78c1\u6216\u5149\u5b66\u54cd\u5e94\u5171\u5b58\uff09\u3002"}}
{"id": "2511.12919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12919", "abs": "https://arxiv.org/abs/2511.12919", "authors": ["Dexin Zuo", "Ang Li", "Wei Wang", "Wenxian Yu", "Danping Zou"], "title": "CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation", "comment": "7 pages, accepted by AAAI 2026 (oral)", "summary": "Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.", "AI": {"tldr": "CoordAR\u662f\u4e00\u4e2a\u7528\u4e8e\u5355\u53c2\u8003\u89c6\u56fe6D\u59ff\u6001\u4f30\u8ba1\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u5c063D-3D\u5bf9\u5e94\u5173\u7cfb\u5efa\u6a21\u4e3a\u79bb\u6563token\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5bf9\u79f0\u6027\u548c\u906e\u6321\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5355\u53c2\u8003\u89c6\u56fe\u76846D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u5168\u5c40\u4e00\u81f4\u6027\u4e0d\u8db3\u548c\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5bf9\u79f0\u6216\u906e\u6321\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCoordAR\u6846\u67b6\uff1a1\uff09\u5750\u6807\u56fetoken\u5316\u5b9e\u73b0\u6982\u7387\u9884\u6d4b\uff1b2\uff09\u6a21\u6001\u89e3\u8026\u7f16\u7801\u7b56\u7565\u5206\u522b\u5904\u7406RGB\u5916\u89c2\u548c\u5750\u6807\u7ebf\u7d22\uff1b3\uff09\u81ea\u56de\u5f52transformer\u89e3\u7801\u5668\u7ed3\u5408\u4f4d\u7f6e\u5bf9\u9f50\u67e5\u8be2\u7279\u5f81\u548c\u90e8\u5206\u751f\u6210token\u5e8f\u5217\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5bf9\u79f0\u6027\u3001\u906e\u6321\u7b49\u771f\u5b9e\u4e16\u754c\u6311\u6218\u4e2d\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "CoordAR\u901a\u8fc7\u81ea\u56de\u5f52\u548c\u6982\u7387\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5355\u53c2\u80036D\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u5bf9\u79f0\u6027\u548c\u906e\u6321\u95ee\u9898\uff0c\u4e3a\u65e03D\u6a21\u578b\u7684\u5bf9\u8c61\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.12921", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12921", "abs": "https://arxiv.org/abs/2511.12921", "authors": ["Huiqiang Sun", "Liao Shen", "Zhan Peng", "Kun Wang", "Size Wu", "Yuhang Zang", "Tianqi Liu", "Zihao Huang", "Xingyu Zeng", "Zhiguo Cao", "Wei Li", "Chen Change Loy"], "title": "Generative Photographic Control for Scene-Consistent Video Cinematic Editing", "comment": null, "summary": "Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.", "AI": {"tldr": "CineCtrl\u662f\u9996\u4e2a\u89c6\u9891\u7535\u5f71\u7f16\u8f91\u6846\u67b6\uff0c\u63d0\u4f9b\u5bf9\u4e13\u4e1a\u76f8\u673a\u53c2\u6570\uff08\u5982\u6563\u666f\u3001\u5feb\u95e8\u901f\u5ea6\uff09\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u901a\u8fc7\u89e3\u8026\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5206\u79bb\u76f8\u673a\u8fd0\u52a8\u548c\u6444\u5f71\u8f93\u5165\uff0c\u5e76\u5229\u7528\u7efc\u5408\u6570\u636e\u751f\u6210\u7b56\u7565\u6784\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u89c6\u9891\u6a21\u578b\u5927\u591a\u4ec5\u9650\u4e8e\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\uff0c\u96be\u4ee5\u63a7\u5236\u6444\u5f71\u5143\u7d20\uff08\u5982\u666f\u6df1\u3001\u66dd\u5149\uff09\u6765\u4f20\u8fbe\u60c5\u7eea\u548c\u521b\u9020\u7f8e\u5b66\u5438\u5f15\u529b\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5206\u79bb\u76f8\u673a\u8fd0\u52a8\u548c\u6444\u5f71\u8f93\u5165\uff0c\u5f00\u53d1\u7efc\u5408\u6570\u636e\u751f\u6210\u7b56\u7565\u5305\u62ec\u6a21\u62df\u6444\u5f71\u6548\u679c\u548c\u771f\u5b9e\u4e16\u754c\u6536\u96c6\u6d41\u7a0b\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u751f\u6210\u5177\u6709\u7cbe\u786e\u63a7\u5236\u7684\u7528\u6237\u6307\u5b9a\u6444\u5f71\u76f8\u673a\u6548\u679c\u7684\u9ad8\u4fdd\u771f\u89c6\u9891\u3002", "conclusion": "CineCtrl\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u4e13\u4e1a\u76f8\u673a\u53c2\u6570\u7684\u7cbe\u7ec6\u63a7\u5236\uff0c\u4e3a\u89c6\u9891\u7535\u5f71\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.12760", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12760", "abs": "https://arxiv.org/abs/2511.12760", "authors": ["Ben Gao", "Jordan Patracone", "St\u00e9phane Chr\u00e9tien", "Olivier Alata"], "title": "Conformal Online Learning of Deep Koopman Linear Embeddings", "comment": null, "summary": "We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.", "AI": {"tldr": "COLoKe\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u81ea\u9002\u5e94\u66f4\u65b0\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684Koopman\u4e0d\u53d8\u8868\u793a\uff0c\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u591a\u6b65\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u4f7f\u7528\u7b26\u5408\u6027\u673a\u5236\u52a8\u6001\u6821\u51c6\u66f4\u65b0\u9608\u503c\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u66f4\u65b0\u5e76\u907f\u514d\u8fc7\u62df\u5408\u3002", "motivation": "\u4f20\u7edfKoopman\u65b9\u6cd5\u96be\u4ee5\u4ece\u6d41\u6570\u636e\u4e2d\u81ea\u9002\u5e94\u66f4\u65b0\u8868\u793a\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9009\u62e9\u6027\u66f4\u65b0\u6a21\u578b\u5e76\u4fdd\u6301\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\u7684\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u63d0\u5347\u7a7a\u95f4\u4e2d\u7684\u591a\u6b65\u9884\u6d4b\u4e00\u81f4\u6027\uff0c\u4f7f\u7528\u7b26\u5408\u6027\u673a\u5236\u8bc4\u4f30\u5f53\u524dKoopman\u6a21\u578b\u7684\u4e00\u81f4\u6027\uff0c\u4ec5\u5f53\u9884\u6d4b\u8bef\u5dee\u8d85\u8fc7\u52a8\u6001\u6821\u51c6\u9608\u503c\u65f6\u89e6\u53d1\u66f4\u65b0\u3002", "result": "\u5728\u57fa\u51c6\u52a8\u529b\u7cfb\u7edf\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cCOLoKe\u80fd\u6709\u6548\u4fdd\u6301\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e0d\u5fc5\u8981\u66f4\u65b0\u5e76\u907f\u514d\u8fc7\u62df\u5408\u3002", "conclusion": "COLoKe\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5728\u7ebf\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u66f4\u65b0Koopman\u8868\u793a\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.12932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12932", "abs": "https://arxiv.org/abs/2511.12932", "authors": ["Feng Lv", "Haoxuan Feng", "Zilu Zhang", "Chunlong Xia", "Yanfeng Li"], "title": "Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes", "comment": null, "summary": "With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u7684\u6587\u672c\u9a71\u52a8\u4ea4\u901a\u573a\u666f\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u63a7\u63a9\u7801\u673a\u5236\u6574\u5408\u4e24\u9879\u4efb\u52a1\uff0c\u5229\u7528\u591a\u89c6\u89d2\u6570\u636e\u589e\u5f3a\u51e0\u4f55\u591a\u6837\u6027\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u7ec6\u8282\u8d28\u91cf\uff0c\u5f15\u5165\u63a9\u7801\u533a\u57df\u52a0\u6743\u635f\u5931\u6539\u5584\u5c0f\u5c3a\u5ea6\u4ea4\u901a\u5143\u7d20\u751f\u6210\u6548\u679c\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u6280\u672f\u5b58\u5728\u8bed\u4e49\u4e30\u5bcc\u5ea6\u4e0d\u8db3\u3001\u89c6\u89d2\u53d7\u9650\u3001\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f4e\u3001\u6587\u672c-\u5185\u5bb9\u5bf9\u9f50\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u7edf\u4e00\u6587\u672c\u9a71\u52a8\u6846\u67b6\u6574\u5408\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\uff1b2. \u53ef\u63a7\u63a9\u7801\u673a\u5236\u65e0\u7f1d\u8fde\u63a5\u4e24\u9879\u4efb\u52a1\uff1b3. \u8f66\u8f86\u4fa7\u548c\u8def\u4fa7\u591a\u89c6\u89d2\u6570\u636e\u589e\u5f3a\u51e0\u4f55\u591a\u6837\u6027\uff1b4. \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5927\u89c4\u6a21\u7c97\u7c92\u5ea6\u6587\u672c-\u56fe\u50cf\u6570\u636e\u6982\u5ff5\u5b66\u4e60 + \u7ec6\u7c92\u5ea6\u63cf\u8ff0\u6570\u636e\u5fae\u8c03\uff1b5. \u63a9\u7801\u533a\u57df\u52a0\u6743\u635f\u5931\u52a8\u6001\u5173\u6ce8\u5173\u952e\u5c0f\u533a\u57df\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4ea4\u901a\u573a\u666f\u7684\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u8fbe\u5230\u4e86\u9886\u5148\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u901a\u573a\u666f\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u5bf9\u9f50\u7cbe\u5ea6\u3002"}}
{"id": "2511.12764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12764", "abs": "https://arxiv.org/abs/2511.12764", "authors": ["Hao Wei", "Aleksandra Franz", "Bjoern List", "Nils Thuerey"], "title": "INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers", "comment": "Accepted at NeurIPS 2025. 35 pages, 10 figures", "summary": "When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\\(\\mathrm{INC}\\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \\(\\mathrm{INC}\\) reduces the error amplification on the order of \\(\u0394t^{-1} + L\\), where \\(\u0394t\\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \\(\\mathrm{INC}\\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\\(R^2\\)) by up to 158.7\\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u95f4\u63a5\u795e\u7ecf\u6821\u6b63\u5668\uff08INC\uff09\uff0c\u901a\u8fc7\u5c06\u5b66\u4e60\u5230\u7684\u4fee\u6b63\u96c6\u6210\u5230\u63a7\u5236\u65b9\u7a0b\u4e2d\u800c\u975e\u76f4\u63a5\u72b6\u6001\u66f4\u65b0\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u81ea\u56de\u5f52\u8bef\u5dee\uff0c\u5728\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684PDE\u4eff\u771f\u3002", "motivation": "\u4f20\u7edf\u6df7\u5408\u6c42\u89e3\u5668\u76f4\u63a5\u5c06\u5b66\u4e60\u5230\u7684\u4fee\u6b63\u5e94\u7528\u4e8e\u6c42\u89e3\u5668\u8f93\u51fa\uff0c\u5728\u957f\u671f\u63a8\u6f14\u4e2d\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u81ea\u56de\u5f52\u8bef\u5dee\uff0c\u7279\u522b\u662f\u5728\u6df7\u6c8c\u4f53\u7cfb\u4e2d\uff0c\u653e\u5927\u7684\u6270\u52a8\u4f1a\u4e0d\u65ad\u7d2f\u79ef\u3002", "method": "\u63d0\u51fa\u95f4\u63a5\u795e\u7ecf\u6821\u6b63\u5668\uff08INC\uff09\uff0c\u5c06\u5b66\u4e60\u5230\u7684\u4fee\u6b63\u96c6\u6210\u5230\u63a7\u5236\u65b9\u7a0b\u4e2d\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u8fdb\u884c\u72b6\u6001\u66f4\u65b0\uff0c\u4ece\u800c\u5c06\u8bef\u5dee\u653e\u5927\u964d\u4f4e\u5230\u0394t\u207b\u00b9 + L\u7684\u91cf\u7ea7\u3002", "result": "\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cINC\u5c06\u957f\u671f\u8f68\u8ff9\u6027\u80fd\uff08R\u00b2\uff09\u63d0\u5347\u9ad8\u8fbe158.7%\uff0c\u7a33\u5b9a\u4e86\u6fc0\u8fdb\u7c97\u5316\u4e0b\u7684\u7206\u70b8\u60c5\u51b5\uff0c\u5bf9\u590d\u67423D\u6e4d\u6d41\u6848\u4f8b\u5b9e\u73b0\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "conclusion": "INC\u5b9e\u73b0\u4e86\u5177\u6709\u5f62\u5f0f\u8bef\u5dee\u51cf\u5c11\u7684\u7a33\u5b9a\u9ad8\u6548PDE\u4eff\u771f\uff0c\u4e3a\u5177\u6709\u53ef\u9760\u7269\u7406\u4fdd\u8bc1\u7684\u66f4\u5feb\u79d1\u5b66\u548c\u5de5\u7a0b\u4eff\u771f\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.12939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12939", "abs": "https://arxiv.org/abs/2511.12939", "authors": ["Wei Jiang", "Jiahao Cui", "Yizheng Wu", "Zhan Peng", "Zhiyu Pan", "Zhiguo Cao"], "title": "Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking", "comment": "9 pages, 5 figures, accepted to AAAI 2026 (poster)", "summary": "Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534a\u76d1\u7763\u5b66\u4e60\u7684\u9ad8\u52a8\u6001\u8303\u56f4\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u751f\u6210\u4f2aHDR\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528\u4e0d\u786e\u5b9a\u6027\u63a9\u7801\u673a\u5236\u8fc7\u6ee4\u4e0d\u53ef\u9760\u533a\u57df\uff0c\u4ec5\u97006.7%\u7684HDR\u771f\u5b9e\u6807\u7b7e\u5373\u53ef\u8fbe\u5230\u5168\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684HDR\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u9700\u8981LDR-HDR\u56fe\u50cf\u5bf9\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5bf9\u96be\u4ee5\u83b7\u53d6\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u5728\u6709\u9650HDR\u771f\u5b9e\u6807\u7b7e\u4e0b\u5b9e\u73b0\u53ef\u6bd4\u8f83\u6027\u80fd\u7684\u6807\u6ce8\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u6559\u5e08\u6a21\u578b\u4e3a\u65e0\u6807\u7b7e\u7684LDR\u6837\u672c\u751f\u6210\u4f2aHDR\u6807\u7b7e\uff0c\u5b66\u751f\u6a21\u578b\u4ece\u4f2a\u6807\u7b7e\u5b66\u4e60\u3002\u63d0\u51fa\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u50cf\u7d20\u7ea7\u548c\u5757\u7ea7\u63a9\u7801\u8fc7\u7a0b\uff0c\u8fc7\u6ee4\u4f2a\u6807\u7b7e\u4e2d\u7684\u4e0d\u53ef\u9760\u90e8\u5206\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4f18\u4e8e\u5148\u524d\u7684\u6807\u6ce8\u9ad8\u6548\u7b97\u6cd5\uff0c\u800c\u4e14\u4ec5\u4f7f\u75286.7%\u7684HDR\u771f\u5b9e\u6807\u7b7e\u5c31\u80fd\u8fbe\u5230\u6700\u65b0\u5168\u76d1\u7763\u65b9\u6cd5\u7684\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u63a9\u7801\u673a\u5236\u6709\u6548\u7f13\u89e3\u4e86\u786e\u8ba4\u504f\u5dee\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u5728HDR\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6807\u6ce8\u9700\u6c42\u3002"}}
{"id": "2511.12791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12791", "abs": "https://arxiv.org/abs/2511.12791", "authors": ["Dahao Tang", "Nan Yang", "Yanli Li", "Zhiyu Zhu", "Zhibo Jin", "Dong Yuan"], "title": "Optimal Look-back Horizon for Time Series Forecasting in Federated Learning", "comment": "Accepted by AAAI-26 as Oral Presentation", "summary": "Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u90a6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u81ea\u9002\u5e94\u56de\u671b\u7a97\u53e3\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5728\u7a7a\u95f4\u8868\u793a\u6765\u89e3\u51b3\u6570\u636e\u5206\u6563\u3001\u5f02\u6784\u548c\u975e\u72ec\u7acb\u5206\u5e03\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5177\u6709\u5206\u6563\u6027\u3001\u5f02\u6784\u6027\u548c\u975e\u72ec\u7acb\u6027\u7279\u5f81\uff0c\u9009\u62e9\u5408\u9002\u7684\u56de\u671b\u7a97\u53e3\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u96c6\u4e2d\u5f0f\u548c\u72ec\u7acb\u5206\u5e03\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u9002\u5e94\u8054\u90a6\u73af\u5883\u3002", "method": "\u5f15\u5165\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u6355\u6349\u5ba2\u6237\u7aef\u6570\u636e\u7684\u65f6\u95f4\u7ed3\u6784\u7279\u5f81\uff0c\u5b9a\u4e49\u5c06\u65f6\u95f4\u5e8f\u5217\u7a97\u53e3\u6620\u5c04\u5230\u5185\u5728\u8868\u793a\u7a7a\u95f4\u7684\u53d8\u6362\uff0c\u63a8\u5bfc\u9884\u6d4b\u635f\u5931\u7684\u8d1d\u53f6\u65af\u9879\u548c\u8fd1\u4f3c\u9879\u5206\u89e3\u3002", "result": "\u5206\u6790\u8868\u660e\u589e\u52a0\u56de\u671b\u7a97\u53e3\u80fd\u6539\u5584\u786e\u5b9a\u6027\u6a21\u5f0f\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u4f46\u4f1a\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u548c\u964d\u4f4e\u6837\u672c\u6548\u7387\u5bfc\u81f4\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002\u9884\u6d4b\u603b\u635f\u5931\u5728\u4e0d\u53ef\u7ea6\u635f\u5931\u5f00\u59cb\u9971\u548c\u800c\u8fd1\u4f3c\u635f\u5931\u6301\u7eed\u4e0a\u5347\u7684\u6700\u5c0f\u7a97\u53e3\u5904\u6700\u5c0f\u5316\u3002", "conclusion": "\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u81ea\u9002\u5e94\u7a97\u53e3\u9009\u62e9\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.12956", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12956", "abs": "https://arxiv.org/abs/2511.12956", "authors": ["Chen Ma", "Ningfei Wang", "Junhao Zheng", "Qing Guo", "Qian Wang", "Qi Alfred Chen", "Chao Shen"], "title": "T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving", "comment": "16 pages, 12 figures", "summary": "Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.\n  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.", "AI": {"tldr": "DiffSign\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210CLIP\u635f\u5931\u548c\u63a9\u7801\u63d0\u793a\u6765\u63d0\u5347\u653b\u51fb\u6548\u679c\u548c\u53ef\u63a7\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u98ce\u683c\u5b9a\u5236\u65b9\u6cd5\u6765\u6539\u5584\u653b\u51fb\u9690\u853d\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u9690\u853d\u6027\u5dee\u3001\u6cdb\u5316\u80fd\u529b\u5f31\u3001\u8fc1\u79fb\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u7269\u7406\u4e16\u754c\u653b\u51fb\u6846\u67b6\u3002", "method": "\u63d0\u51faDiffSign\u6846\u67b6\uff0c\u96c6\u6210CLIP\u635f\u5931\u548c\u63a9\u7801\u63d0\u793a\u6765\u589e\u5f3a\u653b\u51fb\u805a\u7126\u548c\u53ef\u63a7\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u98ce\u683c\u5b9a\u5236\u65b9\u6cd5\u6307\u5bfc\u89c6\u89c9\u5916\u89c2\uff0c\u63d0\u9ad8\u8de8\u57df\u4ea4\u901a\u6807\u5fd7\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9690\u853d\u6027\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\uff08\u4e0d\u540c\u8ddd\u79bb\u3001\u89d2\u5ea6\u3001\u5149\u7167\u6761\u4ef6\u548c\u6807\u5fd7\u7c7b\u522b\uff09\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cDiffSign\u5b9e\u73b0\u4e8683.3%\u7684\u5e73\u5747\u7269\u7406\u4e16\u754c\u653b\u51fb\u6210\u529f\u7387\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u653b\u51fb\u8fc1\u79fb\u6027\u3002", "conclusion": "DiffSign\u6846\u67b6\u80fd\u591f\u751f\u6210\u7269\u7406\u9c81\u68d2\u3001\u9ad8\u6548\u3001\u53ef\u8fc1\u79fb\u3001\u5b9e\u7528\u4e14\u9690\u853d\u7684\u5916\u89c2\u653b\u51fb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2511.12797", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.12797", "abs": "https://arxiv.org/abs/2511.12797", "authors": ["Nathan Breslow", "Aayush Mishra", "Mahler Revsine", "Michael C. Schatz", "Anqi Liu", "Daniel Khashabi"], "title": "Genomic Next-Token Predictors are In-Context Learners", "comment": null, "summary": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?\n  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u56e0\u7ec4\u5e8f\u5217\u4e2d\u662f\u5426\u80fd\u591f\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u6d4b\u8bad\u7ec3\u81ea\u7136\u6d8c\u73b0\u51fa\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u53d1\u73b0\u57fa\u56e0\u7ec4\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7c7b\u4f3c\uff0c\u90fd\u8868\u73b0\u51fa\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\u662f\u5927\u89c4\u6a21\u9884\u6d4b\u5efa\u6a21\u7684\u666e\u904d\u73b0\u8c61\u3002", "motivation": "\u63a2\u7d22\u4e0a\u4e0b\u6587\u5b66\u4e60\u662f\u5426\u4ec5\u662f\u4eba\u7c7b\u8bed\u8a00\u7279\u6709\u7684\u73b0\u8c61\uff0c\u8fd8\u662f\u53ef\u4ee5\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u6d4b\u8bad\u7ec3\u5728\u5176\u4ed6\u7b26\u53f7\u5e8f\u5217\u9886\u57df\u81ea\u7136\u6d8c\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53d7\u63a7\u5b9e\u9a8c\u6846\u67b6\uff0c\u5728\u8bed\u8a00\u548c\u57fa\u56e0\u7ec4\u5f62\u5f0f\u4e2d\u5b9e\u4f8b\u5316\u7b26\u53f7\u63a8\u7406\u4efb\u52a1\uff0c\u76f4\u63a5\u6bd4\u8f83\u57fa\u56e0\u7ec4\u6a21\u578b\u548c\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u57fa\u56e0\u7ec4\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7c7b\u4f3c\uff0c\u968f\u7740\u4e0a\u4e0b\u6587\u6f14\u793a\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5728\u6a21\u5f0f\u5f52\u7eb3\u65b9\u9762\u8868\u73b0\u51fa\u5bf9\u6570\u7ebf\u6027\u589e\u76ca\uff0c\u9996\u6b21\u8bc1\u660e\u4e86\u57fa\u56e0\u7ec4\u5e8f\u5217\u4e2d\u81ea\u7136\u6d8c\u73b0\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u662f\u5927\u89c4\u6a21\u9884\u6d4b\u5efa\u6a21\u5728\u4e30\u5bcc\u6570\u636e\u4e0a\u7684\u7ed3\u679c\uff0c\u8fd9\u4e00\u53d1\u73b0\u5c06\u6d8c\u73b0\u7684\u5143\u5b66\u4e60\u6269\u5c55\u5230\u8bed\u8a00\u4e4b\u5916\uff0c\u6307\u5411\u4e86\u4e0e\u6a21\u6001\u65e0\u5173\u7684\u7edf\u4e00\u4e0a\u4e0b\u6587\u5b66\u4e60\u89c2\u70b9\u3002"}}
{"id": "2511.12962", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12962", "abs": "https://arxiv.org/abs/2511.12962", "authors": ["Daniel Cavadia"], "title": "EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics", "comment": null, "summary": "Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.", "AI": {"tldr": "EndoSight AI\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u624b\u672f\u4e2d\u7cbe\u786e\u5b9e\u65f6\u68c0\u6d4b\u80c3\u80a0\u9053\u606f\u8089\uff0c\u5728\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u63a8\u7406\u3002", "motivation": "\u5728\u5185\u7aa5\u955c\u624b\u672f\u4e2d\u7cbe\u786e\u5b9e\u65f6\u68c0\u6d4b\u80c3\u80a0\u9053\u606f\u8089\u5bf9\u4e8e\u7ed3\u76f4\u80a0\u764c\u7684\u65e9\u671f\u8bca\u65ad\u548c\u9884\u9632\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u516c\u5f00\u7684Hyper-Kvasir\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u4e34\u5e8a\u76f8\u5173\u6027\u80fd\u6307\u6807\u548c\u70ed\u611f\u77e5\u7a0b\u5e8f\u786e\u4fdd\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "result": "\u7cfb\u7edf\u5728\u606f\u8089\u68c0\u6d4b\u4e0a\u8fbe\u523088.3%\u7684\u5e73\u5747\u7cbe\u5ea6(mAP)\uff0c\u5206\u5272\u4efb\u52a1Dice\u7cfb\u6570\u8fbe69%\uff0cGPU\u786c\u4ef6\u4e0a\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u8d85\u8fc735\u5e27/\u79d2\u3002", "conclusion": "\u8be5\u96c6\u6210AI\u89e3\u51b3\u65b9\u6848\u8bbe\u8ba1\u7528\u4e8e\u65e0\u7f1d\u90e8\u7f72\u5230\u5185\u7aa5\u955c\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u6709\u671b\u63d0\u9ad8\u80c3\u80a0\u9053\u533b\u7597\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2511.12964", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12964", "abs": "https://arxiv.org/abs/2511.12964", "authors": ["Mehrab Mustafy Rahman", "Jayanth Mohan", "Tiberiu Sosea", "Cornelia Caragea"], "title": "CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models", "comment": null, "summary": "Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCalibrateMix\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684mixup\u7b56\u7565\u6539\u5584\u534a\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u6821\u51c6\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u7684\u540c\u65f6\u964d\u4f4e\u9884\u671f\u6821\u51c6\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6821\u51c6\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u6a21\u578b\u5f80\u5f80\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\u3002\u867d\u7136mixup\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\u6539\u5584\u4e86\u6821\u51c6\uff0c\u4f46\u5728\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7531\u4e8e\u4f2a\u6807\u7b7e\u7684\u4e0d\u53ef\u9760\u6027\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u5229\u7528\u8bad\u7ec3\u52a8\u6001\u8bc6\u522b'\u6613\u5b66'\u548c'\u96be\u5b66'\u6837\u672c\uff0c\u7136\u540e\u5bf9\u8fd9\u4e9b\u6837\u672c\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684mixup\u64cd\u4f5c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u4f4e\u7684\u9884\u671f\u6821\u51c6\u8bef\u5dee\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "CalibrateMix\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u6821\u51c6\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2511.12808", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.12808", "abs": "https://arxiv.org/abs/2511.12808", "authors": ["Omar Adalat", "Francesco Belardinelli"], "title": "Expressive Temporal Specifications for Reward Monitoring", "comment": null, "summary": "Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\\text{LTL}_f[\\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.", "AI": {"tldr": "\u4f7f\u7528\u5b9a\u91cf\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91(LTL_f[F])\u5408\u6210\u5956\u52b1\u76d1\u63a7\u5668\uff0c\u4e3a\u53ef\u89c2\u6d4b\u72b6\u6001\u8f68\u8ff9\u751f\u6210\u5bc6\u96c6\u5956\u52b1\u6d41\uff0c\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898", "motivation": "\u6307\u5b9a\u4fe1\u606f\u4e30\u5bcc\u4e14\u5bc6\u96c6\u7684\u5956\u52b1\u51fd\u6570\u662f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u76f4\u63a5\u5f71\u54cd\u667a\u80fd\u4f53\u8bad\u7ec3\u6548\u7387\u3002\u5f53\u524d\u6587\u732e\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u5e03\u5c14\u8bed\u4e49\u4f1a\u5bfc\u81f4\u957f\u89c6\u91ce\u51b3\u7b56\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u95ee\u9898", "method": "\u5229\u7528\u5b9a\u91cf\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u5728\u6709\u9650\u8f68\u8ff9\u4e0a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5408\u6210\u5956\u52b1\u76d1\u63a7\u5668\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u53ef\u89c2\u6d4b\u72b6\u6001\u8f68\u8ff9\u751f\u6210\u5bc6\u96c6\u5956\u52b1\u6d41\u3002\u8be5\u6846\u67b6\u662f\u7b97\u6cd5\u65e0\u5173\u7684\uff0c\u4ec5\u4f9d\u8d56\u72b6\u6001\u6807\u8bb0\u51fd\u6570\uff0c\u81ea\u7136\u652f\u6301\u975e\u9a6c\u5c14\u53ef\u592b\u5c5e\u6027\u7684\u6307\u5b9a", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b9a\u91cf\u76d1\u63a7\u5668\u59cb\u7ec8\u5305\u542b\u5e03\u5c14\u76d1\u63a7\u5668\uff0c\u5e76\u4e14\u6839\u636e\u73af\u5883\u7684\u4e0d\u540c\uff0c\u5728\u6700\u5927\u5316\u4efb\u52a1\u5b8c\u6210\u5ea6\u5b9a\u91cf\u6d4b\u91cf\u548c\u51cf\u5c11\u6536\u655b\u65f6\u95f4\u65b9\u9762\u4f18\u4e8e\u5e03\u5c14\u76d1\u63a7\u5668", "conclusion": "\u5b9a\u91cfLTL_f[F]\u5956\u52b1\u76d1\u63a7\u5668\u80fd\u591f\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u8bad\u7ec3\u53cd\u9988\uff0c\u6709\u6548\u7f13\u89e3\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u5728\u957f\u89c6\u91ce\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u5e03\u5c14\u76d1\u63a7\u5668"}}
{"id": "2511.12968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12968", "abs": "https://arxiv.org/abs/2511.12968", "authors": ["Ning Han", "Zhenyu Ge", "Feng Han", "Yuhua Sun", "Chengqing Li", "Jingjing Chen"], "title": "GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models", "comment": "10 pages, 6 figures", "summary": "Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fr\u00e9chet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.", "AI": {"tldr": "GrOCE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6982\u5ff5\u64e6\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u5f15\u5bfc\u7684\u8bed\u4e49\u63a8\u7406\u5b9e\u73b0\u7cbe\u786e\u3001\u81ea\u9002\u5e94\u7684\u6709\u5bb3\u5185\u5bb9\u79fb\u9664\uff0c\u540c\u65f6\u4fdd\u7559\u975e\u76ee\u6807\u8bed\u4e49\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u5fae\u8c03\uff0c\u8981\u4e48\u91c7\u7528\u7c97\u7c92\u5ea6\u8bed\u4e49\u5206\u79bb\uff0c\u5f80\u5f80\u4f1a\u9000\u5316\u65e0\u5173\u6982\u5ff5\u4e14\u7f3a\u4e4f\u5bf9\u52a8\u6001\u6982\u5ff5\u96c6\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u56fe\u5f15\u5bfc\u5728\u7ebf\u6982\u5ff5\u64e6\u9664\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u52a8\u6001\u62d3\u6251\u56fe\u6784\u5efa\u3001\u81ea\u9002\u5e94\u805a\u7c7b\u8bc6\u522b\u548c\u9009\u62e9\u6027\u8fb9\u5207\u65ad\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u5efa\u6a21\u6982\u5ff5\u95f4\u5173\u7cfb\u8fdb\u884c\u7cbe\u786e\u8bed\u4e49\u9694\u79bb\u3002", "result": "\u5728\u6982\u5ff5\u76f8\u4f3c\u5ea6\u548cFr\u00e9chet Inception\u8ddd\u79bb\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u7a33\u5b9a\u7684\u6982\u5ff5\u64e6\u9664\u3002", "conclusion": "GrOCE\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u6982\u5ff5\u64e6\u9664\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7cbe\u786e\u79fb\u9664\u76ee\u6807\u6982\u5ff5\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8bed\u4e49\u5b8c\u6574\u6027\u3002"}}
{"id": "2511.12817", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12817", "abs": "https://arxiv.org/abs/2511.12817", "authors": ["Shasha Zhou", "Mingyu Huang", "Jack Cole", "Charles Britton", "Ming Yin", "Jan Wolber", "Ke Li"], "title": "Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs", "comment": "Accepted as a conference paper at AAAI'26", "summary": "The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFAITH\u6846\u67b6\uff0c\u5229\u7528\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u8bc4\u4f30LLM\u751f\u6210\u54cd\u5e94\u7684\u771f\u5b9e\u6027\uff0c\u65e0\u9700\u53c2\u8003\u7b54\u6848\uff0c\u901a\u8fc7\u5206\u89e3\u54cd\u5e94\u3001\u94fe\u63a5\u77e5\u8bc6\u56fe\u8c31\u548c\u57fa\u4e8e\u8bc1\u636e\u8def\u5f84\u8bc4\u5206\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0e\u4e34\u5e8a\u533b\u751f\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\u4e14\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u80fd\u529b\u7684LLM\u3002", "motivation": "\u5728\u533b\u7597\u9886\u57df\u90e8\u7f72LLM\u9700\u8981\u4e25\u683c\u7684\u9a8c\u8bc1\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u63a2\u7d22\u57fa\u4e8e\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u4e8b\u5b9e\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFAITH\u6846\u67b6\uff1a\u5c06\u54cd\u5e94\u5206\u89e3\u4e3a\u539f\u5b50\u58f0\u660e\uff0c\u94fe\u63a5\u5230\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\uff0c\u57fa\u4e8e\u8bc1\u636e\u8def\u5f84\u8fdb\u884c\u8bc4\u5206\uff0c\u65e0\u9700\u53c2\u8003\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8bc4\u4f30\u4e0e\u4e34\u5e8a\u533b\u751f\u5224\u65ad\u76f8\u5173\u6027\u663e\u8457\u66f4\u9ad8\uff0c\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u80fd\u529b\u7684LLM\uff0c\u5bf9\u6587\u672c\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8bc4\u5206\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u9650\u5236\uff0c\u4f46\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u662f\u533b\u7597\u9886\u57df\u81ea\u52a8\u4e8b\u5b9e\u6027\u8bc4\u4f30\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2511.12969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12969", "abs": "https://arxiv.org/abs/2511.12969", "authors": ["Ziqiao Weng", "Yaoyu Fang", "Jiahe Qian", "Xinkun Wang", "Lee AD Cooper", "Weidong Cai", "Bo Zhou"], "title": "HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology", "comment": "Accepted to AAAI 2026. 7 pages (main text), 12 pages total including references and supplementary material. 6 figures", "summary": "Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.", "AI": {"tldr": "HiFusion\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5efa\u6a21\u548c\u8de8\u5c3a\u5ea6\u878d\u5408\u4eceH&E\u67d3\u8272\u75c5\u7406\u56fe\u50cf\u9884\u6d4b\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u57fa\u56e0\u8868\u8fbe\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6280\u672f\u9762\u4e34\u4e34\u5e8a\u5e94\u7528\u7684\u969c\u788d\uff0c\u5305\u62ec\u6280\u672f\u590d\u6742\u6027\u548c\u9ad8\u6602\u6210\u672c\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349spot\u5185\u7684\u751f\u7269\u5f02\u8d28\u6027\uff0c\u4e14\u5728\u6574\u5408\u5468\u56f4\u7ec4\u7ec7\u4e0a\u4e0b\u6587\u4fe1\u606f\u65f6\u6613\u53d7\u5f62\u6001\u5b66\u566a\u58f0\u5f71\u54cd\u3002", "method": "HiFusion\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a\u5206\u5c42spot\u5185\u5efa\u6a21\u6a21\u5757\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u5b50\u5757\u5206\u89e3\u63d0\u53d6\u7ec6\u7c92\u5ea6\u5f62\u6001\u8868\u5f81\uff1b\u4e0a\u4e0b\u6587\u611f\u77e5\u8de8\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u9009\u62e9\u6027\u5730\u6574\u5408\u751f\u7269\u76f8\u5173\u533a\u57df\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6ST\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHiFusion\u57282D\u5207\u7247\u4ea4\u53c9\u9a8c\u8bc1\u548c\u66f4\u5177\u6311\u6218\u6027\u76843D\u6837\u672c\u7279\u5b9a\u573a\u666f\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "HiFusion\u4f5c\u4e3a\u4e00\u4e2a\u7a33\u5065\u3001\u51c6\u786e\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u4ece\u5e38\u89c4\u75c5\u7406\u5b66\u63a8\u65ad\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12828", "abs": "https://arxiv.org/abs/2511.12828", "authors": ["Mohammad Marufur Rahman", "Guanchu Wang", "Kaixiong Zhou", "Minghan Chen", "Fan Yang"], "title": "Catastrophic Forgetting in Kolmogorov-Arnold Networks", "comment": "14 pages, 5 figures, accepted in the main technical track of AAAI 2026", "summary": "Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86KANs\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86KANs\u5728\u4f4e\u7ef4\u7b97\u6cd5\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u4f46\u5728\u9ad8\u7ef4\u9886\u57df\u4ecd\u6613\u9057\u5fd8\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86KAN-LoRA\u9002\u914d\u5668\u7528\u4e8e\u53c2\u6570\u9ad8\u6548\u7684\u6301\u7eed\u5fae\u8c03\u3002", "motivation": "KANs\u88ab\u8ba4\u4e3a\u901a\u8fc7\u5c40\u90e8\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u5177\u6709\u5185\u5728\u6297\u9057\u5fd8\u80fd\u529b\uff0c\u4f46\u5176\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u884c\u4e3a\u548c\u5c40\u9650\u6027\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u4ee5\u7406\u89e3\u5176\u4f18\u52bf\u548c\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u8fde\u63a5\u9057\u5fd8\u4e0e\u6fc0\u6d3b\u652f\u6301\u91cd\u53e0\u548c\u5185\u5728\u6570\u636e\u7ef4\u5ea6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5728\u5408\u6210\u548c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6d4b\u91cf\u4e0d\u540c\u6a21\u578b\u914d\u7f6e\u548c\u6570\u636e\u590d\u6742\u5ea6\u4e0b\u7684\u9057\u5fd8\u52a8\u6001\uff0c\u5e76\u63d0\u51fa\u4e86KAN-LoRA\u9002\u914d\u5668\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u5fae\u8c03\u3002", "result": "KANs\u5728\u4f4e\u7ef4\u7b97\u6cd5\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u6709\u524d\u666f\u7684\u8bb0\u5fc6\u4fdd\u6301\u80fd\u529b\uff0c\u4f46\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u8a00\u5efa\u6a21\u7b49\u9ad8\u7ef4\u9886\u57df\u4e2d\u4ecd\u7136\u5bb9\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u589e\u8fdb\u4e86\u5bf9KANs\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u7406\u89e3\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u8868\u660e\u867d\u7136KANs\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u5177\u6709\u6297\u9057\u5fd8\u7279\u6027\uff0c\u4f46\u5728\u590d\u6742\u9ad8\u7ef4\u4efb\u52a1\u4e2d\u4ecd\u9700\u4e13\u95e8\u7684\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2511.12829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12829", "abs": "https://arxiv.org/abs/2511.12829", "authors": ["Michael Chen", "Raghav Kansal", "Abhijith Gandrakota", "Zichun Hao", "Jennifer Ngadiuba", "Maria Spiropulu"], "title": "An Evaluation of Representation Learning Methods in Particle Physics Foundation Models", "comment": null, "summary": "We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u7c92\u5b50\u7269\u7406\u5b66\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u76ee\u6807\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e0b\u6bd4\u8f83\u4e86\u5bf9\u6bd4\u5b66\u4e60\u3001\u63a9\u7801\u7c92\u5b50\u5efa\u6a21\u548c\u751f\u6210\u91cd\u5efa\u7b49\u4e0d\u540c\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u76d1\u7763\u67b6\u6784\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4e3a\u7c92\u5b50\u7269\u7406\u5b66\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u4e0d\u540c\u5b66\u4e60\u76ee\u6807\u7684\u8d21\u732e\uff0c\u4e3a\u672a\u6765\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u53c2\u8003\u57fa\u51c6\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u7684\u57fa\u4e8etransformer\u7684\u7c92\u5b50\u4e91\u7f16\u7801\u5668\uff0c\u91c7\u7528\u6807\u51c6\u5316\u9884\u5904\u7406\u3001\u5339\u914d\u91c7\u6837\u548c\u4e00\u81f4\u8bc4\u4f30\u534f\u8bae\uff0c\u6bd4\u8f83\u5bf9\u6bd4\u5b66\u4e60\uff08\u76d1\u7763\u548c\u81ea\u76d1\u7763\uff09\u3001\u63a9\u7801\u7c92\u5b50\u5efa\u6a21\u548c\u751f\u6210\u91cd\u5efa\u76ee\u6807\u3002", "result": "\u5728\u53d7\u63a7\u6bd4\u8f83\u4e2d\u9694\u79bb\u4e86\u5b66\u4e60\u76ee\u6807\u7684\u8d21\u732e\uff0c\u7a81\u51fa\u4e86\u5404\u81ea\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u3002\u63d0\u51fa\u7684\u76d1\u7763\u67b6\u6784\u4fee\u6539\u5728\u57fa\u51c6\u8bc4\u4f30\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7c92\u5b50\u7269\u7406\u5b66\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u53c2\u8003\u70b9\uff0c\u4f7f\u6574\u4e2a\u793e\u533a\u80fd\u591f\u5b9e\u73b0\u66f4\u900f\u660e\u548c\u7a33\u5065\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.12977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12977", "abs": "https://arxiv.org/abs/2511.12977", "authors": ["Yixuan Yang", "Luyang Xie", "Zhen Luo", "Zixiang Zhao", "Mingqi Gao", "Feng Zheng"], "title": "ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes", "comment": null, "summary": "Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.", "AI": {"tldr": "ArtiWorld\u662f\u4e00\u4e2a\u573a\u666f\u611f\u77e5\u7ba1\u9053\uff0c\u80fd\u591f\u4ece\u6587\u672c\u573a\u666f\u63cf\u8ff0\u4e2d\u5b9a\u4f4d\u53ef\u5173\u8282\u5316\u5bf9\u8c61\uff0c\u5e76\u91cd\u5efa\u53ef\u6267\u884c\u7684URDF\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u51e0\u4f55\u5f62\u72b6\u3002\u6838\u5fc3\u7ec4\u4ef6Arti4URDF\u5229\u75283D\u70b9\u4e91\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u548cURDF\u5bfc\u5411\u63d0\u793a\u8bbe\u8ba1\uff0c\u5c06\u521a\u6027\u5bf9\u8c61\u5feb\u901f\u8f6c\u6362\u4e3a\u4ea4\u4e92\u5f0fURDF\u5173\u8282\u5bf9\u8c61\u3002", "motivation": "\u73b0\u6709\u6a21\u62df\u73af\u5883\u4e2d\u76843D\u8d44\u4ea7\u5927\u591a\u662f\u521a\u6027\u7684\uff0c\u624b\u52a8\u5c06\u5176\u8f6c\u6362\u4e3a\u5173\u8282\u5bf9\u8c61\u6781\u5176\u8017\u65f6\u8017\u529b\u3002\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u8bc6\u522b\u573a\u666f\u4e2d\u53ef\u5173\u8282\u5316\u5bf9\u8c61\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u5173\u8282\u8d44\u4ea7\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Arti4URDF\u7ec4\u4ef6\uff0c\u7ed3\u54083D\u70b9\u4e91\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u548cURDF\u5bfc\u5411\u63d0\u793a\u8bbe\u8ba1\uff0c\u4ece\u6587\u672c\u573a\u666f\u63cf\u8ff0\u4e2d\u5b9a\u4f4d\u5019\u9009\u53ef\u5173\u8282\u5316\u5bf9\u8c61\uff0c\u5e76\u91cd\u5efa\u53ef\u6267\u884c\u7684URDF\u6a21\u578b\u3002", "result": "\u57283D\u6a21\u62df\u5bf9\u8c61\u3001\u5b8c\u65743D\u6a21\u62df\u573a\u666f\u548c\u771f\u5b9e\u4e16\u754c\u626b\u63cf\u573a\u666f\u4e09\u4e2a\u5c42\u9762\u8fdb\u884c\u8bc4\u4f30\uff0c\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u8c61\u51e0\u4f55\u5f62\u72b6\u5e76\u6b63\u786e\u6355\u6349\u5bf9\u8c61\u4ea4\u4e92\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u76f4\u63a5\u4ece\u73b0\u67093D\u8d44\u4ea7\u6784\u5efa\u4ea4\u4e92\u5f0f\u3001\u673a\u5668\u4eba\u5c31\u7eea\u7684\u6a21\u62df\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.12978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12978", "abs": "https://arxiv.org/abs/2511.12978", "authors": ["Aishwarya Agarwal", "Srikrishna Karanam", "Vineet Gandhi"], "title": "Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach", "comment": "25 pages, 21 figures", "summary": "Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CCI\u65b9\u6cd5\uff0c\u5229\u7528CLIP\u7684\u8865\u4e01\u5d4c\u5165\u5bf9\u7a7a\u95f4\u8865\u4e01\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\u548c\u63a9\u7801\uff0c\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u7684\u76f8\u5bf9\u53d8\u5316\uff0c\u5728\u5fe0\u5b9e\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SOTA\u3002\u8fd8\u63d0\u51fa\u4e86COVAR\u57fa\u51c6\u6765\u7cfb\u7edf\u5206\u79bb\u524d\u666f\u548c\u80cc\u666f\u5f71\u54cd\uff0c\u5bf918\u4e2aCLIP\u53d8\u4f53\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002", "motivation": "\u5bf9\u6bd4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982CLIP\u5728\u96f6\u6837\u672c\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u865a\u5047\u76f8\u5173\u6027\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5bf9\u80cc\u666f\u7684\u8fc7\u5ea6\u4f9d\u8d56\u95ee\u9898\u3002\u73b0\u6709\u57fa\u51c6\u5982CounterAnimals\u4ec5\u4f9d\u8d56\u51c6\u786e\u6027\uff0c\u9690\u542b\u5730\u5c06\u6240\u6709\u6027\u80fd\u4e0b\u964d\u5f52\u56e0\u4e8e\u80cc\u666f\u76f8\u5173\u6027\uff0c\u8fd9\u79cd\u5047\u8bbe\u662f\u4e0d\u5b8c\u6574\u7684\u3002", "method": "\u63d0\u51fa\u4e86CCI\u65b9\u6cd5\uff1a\u4f7f\u7528CLIP\u81ea\u8eab\u7684\u8865\u4e01\u5d4c\u5165\u5c06\u7a7a\u95f4\u8865\u4e01\u5206\u7ec4\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u7c07\uff0c\u63a9\u7801\u8fd9\u4e9b\u7c07\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u7684\u76f8\u5bf9\u53d8\u5316\u3002\u7ed3\u5408GroundedSAM\u81ea\u52a8\u5c06\u9884\u6d4b\u5206\u7c7b\u4e3a\u524d\u666f\u9a71\u52a8\u6216\u80cc\u666f\u9a71\u52a8\u3002\u8fd8\u63d0\u51fa\u4e86COVAR\u57fa\u51c6\u6765\u7cfb\u7edf\u53d8\u5316\u5bf9\u8c61\u524d\u666f\u548c\u80cc\u666f\u3002", "result": "CCI\u5728\u5fe0\u5b9e\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u521b\u9020\u4e86\u65b0SOTA\uff0c\u5728MS COCO\u68c0\u7d22\u7684\u5220\u9664AUC\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7\u4e24\u500d\u7684\u6539\u8fdb\u3002\u901a\u8fc7CCI\u4e0eCOVAR\u7684\u7ed3\u5408\uff0c\u5bf918\u4e2aCLIP\u53d8\u4f53\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u9664\u4e86\u80cc\u666f\u76f8\u5173\u6027\u5916\uff0c\u89c6\u89d2\u53d8\u5316\u3001\u5c3a\u5ea6\u504f\u79fb\u548c\u7ec6\u7c92\u5ea6\u5bf9\u8c61\u6df7\u6dc6\u4e5f\u662f\u9519\u8bef\u7684\u91cd\u8981\u6765\u6e90\u3002", "conclusion": "CCI\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u8bca\u65ad\u80fd\u529b\uff0cCOVAR\u57fa\u51c6\u80fd\u591f\u7cfb\u7edf\u5206\u79bb\u4e0d\u540c\u5f71\u54cd\u56e0\u7d20\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5b66\u8fdb\u5c55\u548c\u5b9e\u8bc1\u8bc1\u636e\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.12846", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12846", "abs": "https://arxiv.org/abs/2511.12846", "authors": ["Zelin Zhu", "Yancheng Huang", "Kai Yang"], "title": "RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees", "comment": null, "summary": "Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.", "AI": {"tldr": "RoS-Guard\u662f\u4e00\u79cd\u9488\u5bf9\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7ebf\u6027\u7cfb\u7edf\u7684\u9c81\u68d2\u6700\u4f18\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u5c55\u5f00\u5b9e\u73b0GPU\u52a0\u901f\u7684\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u7cbe\u786e\u7684\u7cfb\u7edf\u77e5\u8bc6\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u4e0d\u5207\u5b9e\u9645\uff0c\u4e14\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u901a\u8fc7\u5bf9OCD\u4f18\u5316\u95ee\u9898\u8fdb\u884c\u7d27\u5bc6\u677e\u5f1b\u548c\u91cd\u6784\uff0c\u91c7\u7528\u795e\u7ecf\u5c55\u5f00\u6280\u672f\u5b9e\u73b0GPU\u52a0\u901f\u7684\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u6027\u80fd\u4fdd\u8bc1\uff0c\u5305\u62ec\u9884\u671f\u8bef\u62a5\u7387\u548c\u6700\u574f\u60c5\u51b5\u5e73\u5747\u68c0\u6d4b\u5ef6\u8fdf\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u5e76\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u663e\u8457\u8ba1\u7b97\u52a0\u901f\u3002", "conclusion": "RoS-Guard\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u5728\u7ebf\u53d8\u5316\u68c0\u6d4b\u7b97\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u3002"}}
{"id": "2511.12988", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12988", "abs": "https://arxiv.org/abs/2511.12988", "authors": ["Furui Xu", "Shaobo Wang", "Jiajun Zhang", "Chenghao Sun", "Haixiang Tang", "Linfeng Zhang"], "title": "UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective", "comment": "AAAI 2026, 13 pages, 9 figures, 5 tables", "summary": "The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\\%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUNSEEN\u6846\u67b6\uff0c\u4ece\u6cdb\u5316\u89d2\u5ea6\u8fdb\u884c\u6570\u636e\u96c6\u526a\u679d\uff0c\u901a\u8fc7\u4f7f\u7528\u672a\u5728\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u7684\u6a21\u578b\u6765\u8bc4\u5206\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u6837\u672c\u8bc4\u5206\u5bc6\u96c6\u5206\u5e03\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6b65\u573a\u666f\u4e2d\u4f18\u5316\u6838\u5fc3\u96c6\u8d28\u91cf\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6570\u636e\u96c6\u89c4\u6a21\u589e\u957f\u5e26\u6765\u8ba1\u7b97\u6311\u6218\uff0c\u4f20\u7edf\u6570\u636e\u96c6\u526a\u679d\u65b9\u6cd5\u57fa\u4e8e\u8bad\u7ec3\u9636\u6bb5\u6a21\u578b\u6027\u80fd\u8bc4\u5206\uff0c\u5bfc\u81f4\u6837\u672c\u8bc4\u5206\u5bc6\u96c6\u5206\u5e03\uff0c\u96be\u4ee5\u6709\u6548\u533a\u5206\u6837\u672c\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faUNSEEN\u6846\u67b6\uff0c\u4ece\u6cdb\u5316\u89d2\u5ea6\u8bc4\u5206\u6837\u672c\uff0c\u4f7f\u7528\u672a\u5728\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u7684\u6a21\u578b\uff1b\u6269\u5c55\u5230\u591a\u6b65\u573a\u666f\uff0c\u901a\u8fc7\u5728\u4e0d\u540c\u6838\u5fc3\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u589e\u91cf\u9009\u62e9\uff0c\u52a8\u6001\u4f18\u5316\u6838\u5fc3\u96c6\u8d28\u91cf\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet-1K\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u5728ImageNet-1K\u4e0a\u51cf\u5c1130%\u8bad\u7ec3\u6570\u636e\u7684\u540c\u65f6\u5b9e\u73b0\u65e0\u635f\u6027\u80fd\u3002", "conclusion": "\u4ece\u6cdb\u5316\u89d2\u5ea6\u8fdb\u884c\u6570\u636e\u96c6\u526a\u679d\u80fd\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0cUNSEEN\u6846\u67b6\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12992", "abs": "https://arxiv.org/abs/2511.12992", "authors": ["Lintong Zhang", "Kang Yin", "Seong-Whan Lee"], "title": "Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection", "comment": "31page, 7 figures", "summary": "In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWSAE-Net\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u6743\u8bed\u4e49\u56fe\u548c\u81ea\u9002\u5e94\u5019\u9009\u7f16\u8f91\u5e8f\u5217\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e2d\u8bed\u4e49\u76f8\u5173\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u7f16\u8f91\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u975e\u751f\u6210\u5f0f\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5728\u66ff\u6362\u56fe\u50cf\u533a\u57df\u65f6\u5ffd\u7565\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u635f\u5bb3\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5e76\u963b\u788d\u7f16\u8f91\u6d41\u7a0b\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faWSAE-Net\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u751f\u6210\u52a0\u6743\u8bed\u4e49\u56fe\u4ee5\u51cf\u5c11\u975e\u8bed\u4e49\u7279\u5f81\u5355\u5143\u8ba1\u7b97\uff1b2\uff09\u8bbe\u8ba1\u81ea\u9002\u5e94\u5019\u9009\u7f16\u8f91\u5e8f\u5217\u786e\u5b9a\u6700\u4f18\u8ba1\u7b97\u987a\u5e8f\u3002", "result": "\u901a\u8fc7\u5168\u9762\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6709\u52a9\u4e8e\u66f4\u6e05\u6670\u6df1\u5165\u5730\u7406\u89e3\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "conclusion": "WSAE-Net\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u89c9\u53cd\u4e8b\u5b9e\u89e3\u91ca\u4e2d\u7684\u8bed\u4e49\u76f8\u5173\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.12998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12998", "abs": "https://arxiv.org/abs/2511.12998", "authors": ["Zewei Chang", "Zheng-Peng Duan", "Jianxing Zhang", "Chun-Le Guo", "Siyu Liu", "Hyungju Chun", "Hyunhee Park", "Zikun Liu", "Chongyi Li"], "title": "PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching", "comment": "To appear at AAAI 2026", "summary": "Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.", "AI": {"tldr": "PerTouch\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7edf\u4e00\u56fe\u50cf\u6da6\u8272\u6846\u67b6\uff0c\u652f\u6301\u8bed\u4e49\u7ea7\u56fe\u50cf\u6da6\u8272\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u7f8e\u5b66\u3002\u901a\u8fc7\u53c2\u6570\u6620\u5c04\u3001\u8bed\u4e49\u611f\u77e5\u8bad\u7ec3\u673a\u5236\u548cVLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u6da6\u8272\u4e2d\u53ef\u63a7\u6027\u4e0e\u4e3b\u89c2\u6027\u5e73\u8861\u7684\u6311\u6218\uff0c\u6ee1\u8db3\u7528\u6237\u4e2a\u6027\u5316\u5ba1\u7f8e\u504f\u597d\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5305\u542b\u8bed\u4e49\u533a\u57df\u5c5e\u6027\u503c\u7684\u53c2\u6570\u6620\u5c04\u4f5c\u4e3a\u8f93\u5165\uff0c\u6784\u5efa\u663e\u5f0f\u53c2\u6570\u5230\u56fe\u50cf\u7684\u6620\u5c04\uff1b\u5f15\u5165\u8bed\u4e49\u66ff\u6362\u548c\u53c2\u6570\u6270\u52a8\u673a\u5236\u63d0\u5347\u8bed\u4e49\u8fb9\u754c\u611f\u77e5\uff1b\u5f00\u53d1VLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5904\u7406\u5f3a\u5f31\u7528\u6237\u6307\u4ee4\uff0c\u914d\u5907\u53cd\u9988\u9a71\u52a8\u91cd\u601d\u8003\u548c\u573a\u666f\u611f\u77e5\u8bb0\u5fc6\u673a\u5236\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027\uff0cPerTouch\u5728\u4e2a\u6027\u5316\u56fe\u50cf\u6da6\u8272\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "PerTouch\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8bed\u4e49\u7ea7\u56fe\u50cf\u6da6\u8272\u4e0e\u4e2a\u6027\u5316\u504f\u597d\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u4e3a\u56fe\u50cf\u6da6\u8272\u63d0\u4f9b\u4e86\u53ef\u63a7\u4e14\u4e3b\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12869", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.IT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12869", "abs": "https://arxiv.org/abs/2511.12869", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Zeeshan Memon", "Muhammad Ibtsaam Qadir", "Sagnik Bhattacharya", "Hassan Rizwan", "Abhiram R. Gorle", "Maahe Zehra Kazmi", "Ayesha Mohsin", "Muhammad Usman Rafique", "Zihao He", "Pulkit Mehta", "Muhammad Ali Jamshed", "John M. Cioffi"], "title": "On the Fundamental Limits of LLMs at Scale", "comment": "Submitted to TMLR 2025", "summary": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ece\u8ba1\u7b97\u7406\u8bba\u3001\u4fe1\u606f\u8bba\u548c\u7edf\u8ba1\u5b66\u89d2\u5ea6\u5f62\u5f0f\u5316\u5206\u6790\u4e86LLM\u6269\u5c55\u7684\u4e94\u4e2a\u57fa\u672c\u9650\u5236\uff1a\u5e7b\u89c9\u3001\u4e0a\u4e0b\u6587\u538b\u7f29\u3001\u63a8\u7406\u9000\u5316\u3001\u68c0\u7d22\u8106\u5f31\u6027\u548c\u591a\u6a21\u6001\u4e0d\u5bf9\u9f50\uff0c\u5e76\u6307\u51fa\u4e86\u8fd9\u4e9b\u9650\u5236\u7684\u7406\u8bba\u4e0a\u9650\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u4ece\u7ecf\u9a8c\u89d2\u5ea6\u63cf\u8ff0LLM\u6269\u5c55\u7684\u9650\u5236\u73b0\u8c61\uff0c\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u73b0\u8c61\u4e0e\u8ba1\u7b97\u3001\u4fe1\u606f\u548c\u5b66\u4e60\u7684\u57fa\u7840\u7406\u8bba\u9650\u5236\u76f8\u8054\u7cfb\u7684\u4e25\u683c\u7406\u8bba\u7efc\u5408\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bc1\u660e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u8ba1\u7b97\u7406\u8bba\uff08\u53ef\u8ba1\u7b97\u6027\u4e0e\u4e0d\u53ef\u8ba1\u7b97\u6027\uff09\u3001\u4fe1\u606f\u8bba\uff08\u6709\u9650\u63cf\u8ff0\u957f\u5ea6\uff09\u548c\u7edf\u8ba1\u5b66\uff08\u6837\u672c\u590d\u6742\u5ea6\uff09\u7684\u7406\u8bba\u5206\u6790\uff0c\u5e76\u8f85\u4ee5\u5b9e\u8bc1\u8bc1\u636e\u3002", "result": "\u8bc1\u660e\u4e86LLM\u6269\u5c55\u5b58\u5728\u56fa\u6709\u7684\u7406\u8bba\u5929\u82b1\u677f\uff1a\u4e0d\u53ef\u8ba1\u7b97\u6027\u5bfc\u81f4\u4e0d\u53ef\u6d88\u9664\u7684\u9519\u8bef\u6b8b\u7559\uff1b\u4fe1\u606f\u8bba\u7ea6\u675f\u9650\u5236\u4e86\u53ef\u8fbe\u5230\u7684\u51c6\u786e\u5ea6\uff1b\u51e0\u4f55\u548c\u8ba1\u7b97\u6548\u5e94\u4f7f\u957f\u4e0a\u4e0b\u6587\u538b\u7f29\u8fdc\u4f4e\u4e8e\u540d\u4e49\u5927\u5c0f\uff1b\u57fa\u4e8e\u4f3c\u7136\u7684\u8bad\u7ec3\u504f\u597d\u6a21\u5f0f\u5b8c\u6210\u800c\u975e\u63a8\u7406\u3002", "conclusion": "LLM\u6269\u5c55\u5728\u67d0\u4e9b\u65b9\u9762\u6709\u5e2e\u52a9\uff0c\u4f46\u5728\u67d0\u4e9b\u65b9\u9762\u4f1a\u9971\u548c\uff0c\u5728\u67d0\u4e9b\u65b9\u9762\u65e0\u6cd5\u8fdb\u5c55\u3002\u6587\u7ae0\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u7f13\u89e3\u8def\u5f84\uff0c\u5982\u6709\u754c\u9884\u8a00\u68c0\u7d22\u3001\u4f4d\u7f6e\u8bfe\u7a0b\u548c\u7a00\u758f\u6216\u5206\u5c42\u6ce8\u610f\u529b\u3002"}}
{"id": "2511.13001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13001", "abs": "https://arxiv.org/abs/2511.13001", "authors": ["Pengcheng Shi", "Jiawei Chen", "Jiaqi Liu", "Xinglin Zhang", "Tao Chen", "Lei Li"], "title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation", "comment": "Accepted by CVPR 2025 Workshop MedSegFM", "summary": "We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.", "AI": {"tldr": "Medal S\u662f\u4e00\u4e2a\u533b\u5b66\u5206\u5272\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u539f\u751f\u5206\u8fa8\u7387\u7a7a\u95f4\u548c\u6587\u672c\u63d0\u793a\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u5728BiomedSegFM\u6570\u636e\u96c6\u4e0a\u652f\u6301\u591a\u8fbe243\u4e2a\u7c7b\u522b\uff0c\u5728\u4e94\u6a21\u6001\u9a8c\u8bc1\u96c6\u4e0aDSC\u8fbe\u523075.44\uff0c\u76f8\u6bd4SAT\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u65b9\u6cd5\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u901a\u9053\u5bf9\u9f50\u673a\u5236\u7f13\u89e3\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u5e26\u6765\u7684\u4e0d\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5e76\u884c\u591a\u7c7b\u522b\u5206\u5272\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea73D\u5377\u79ef\u6a21\u5757\u8fdb\u884c\u4f53\u7d20\u7a7a\u95f4\u7ec6\u5316\uff0c\u652f\u6301\u6587\u672c\u63d0\u793a\u548c\u7a7a\u95f4\u63d0\u793a\u4e24\u79cd\u6a21\u5f0f\uff0c\u63d0\u51fa\u52a8\u6001\u91cd\u91c7\u6837\u5904\u7406\u76ee\u6807-\u8865\u4e01\u6bd4\u4f8b\u4e0d\u5e73\u8861\uff0c\u5e76\u5f00\u53d1\u4e86\u4f18\u5316\u7684\u6587\u672c\u9884\u5904\u7406\u3001\u4e24\u9636\u6bb5\u63a8\u7406\u7b56\u7565\u548c\u540e\u5904\u7406\u6280\u672f\u3002", "result": "\u572824\u7c7b\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u5e76\u884c\u7a7a\u95f4\u63d0\u793a\u6bd4\u987a\u5e8f\u63d0\u793a\u51cf\u5c1190%\u4ee5\u4e0a\u7684\u63a8\u7406\u65f6\u95f4\uff1b\u5728\u4e94\u6a21\u6001\u9a8c\u8bc1\u96c6\u4e0a\uff0cDSC\u4e3a75.44\uff08vs SAT 69.83\uff09\uff0cNSD\u4e3a77.34\uff08vs 71.06\uff09\uff0cF1\u4e3a38.24\uff08vs 24.88\uff09\uff0cDSC TP\u4e3a65.46\uff08vs 46.97\uff09\u3002", "conclusion": "Medal S\u901a\u8fc7\u534f\u8c03\u7a7a\u95f4\u7cbe\u5ea6\u4e0e\u8bed\u4e49\u6587\u672c\u6307\u5bfc\uff0c\u5728\u591a\u7c7b\u522b\u533b\u5b66\u5206\u5272\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u57fa\u4e8e\u987a\u5e8f\u63d0\u793a\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.12881", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.12881", "abs": "https://arxiv.org/abs/2511.12881", "authors": ["Cheongjae Jang", "Jonghyun Won", "Soyeon Jun", "Chun Kee Chung", "Keehyoung Joo", "Yung-Kyun Noh"], "title": "On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples", "comment": "Extended version of paper accepted to AAAI 2026. 18 pages, 12 figures", "summary": "Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u652f\u6301\u96c6\u663e\u8457\u91cd\u53e0\u4f46\u5bc6\u5ea6\u51fd\u6570\u5b58\u5728\u663e\u8457\u70b9\u5dee\u5f02\u7684\u60c5\u51b5\u4e0b\uff0c\u4e00\u7ef4Wasserstein\u8ddd\u79bb\u5982\u4f55\u51c6\u786e\u8bc6\u522b\u5bc6\u5ea6\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u6709\u9650\u6837\u672c\u8bbe\u7f6e\u4e2d\u7684\u89e3\u6790\u7279\u6027\u3002", "motivation": "\u5f53\u4e24\u4e2a\u5bc6\u5ea6\u51fd\u6570\u7684\u652f\u6301\u96c6\u663e\u8457\u91cd\u53e0\u4f46\u5bc6\u5ea6\u503c\u5b58\u5728\u663e\u8457\u70b9\u5dee\u5f02\u65f6\uff0c\u4f20\u7edfWasserstein\u8ddd\u79bb\u4e3b\u8981\u5173\u6ce8\u652f\u6301\u96c6\u5dee\u5f02\uff0c\u800c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5bc6\u5ea6\u5dee\u5f02\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0bWasserstein\u8ddd\u79bb\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u8fd9\u4e9b\u5bc6\u5ea6\u5dee\u5f02\u3002", "method": "\u5229\u7528\u6cca\u677e\u8fc7\u7a0b\u548c\u5206\u79bb\u901f\u7387\u56e0\u5b50\uff0c\u5206\u6790\u4e00\u7ef4Wasserstein\u8ddd\u79bb\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u4fe1\u606f\u5904\u7406\u80fd\u529b\uff0c\u7814\u7a76\u5176\u5982\u4f55\u6355\u6349\u70b9\u5bc6\u5ea6\u5dee\u5f02\u4ee5\u53ca\u8fd9\u4e9b\u4fe1\u606f\u5982\u4f55\u4e0e\u652f\u6301\u96c6\u5dee\u5f02\u534f\u8c03\u3002", "result": "\u901a\u8fc7\u795e\u7ecf\u8109\u51b2\u5e8f\u5217\u89e3\u7801\u548c\u6c28\u57fa\u9178\u63a5\u89e6\u9891\u7387\u6570\u636e\u7684\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u4e00\u7ef4Wasserstein\u8ddd\u79bb\u80fd\u591f\u7a81\u51fa\u4e0e\u901f\u7387\u548c\u652f\u6301\u96c6\u76f8\u5173\u7684\u6709\u610f\u4e49\u7684\u5bc6\u5ea6\u5dee\u5f02\u3002", "conclusion": "\u4e00\u7ef4Wasserstein\u8ddd\u79bb\u80fd\u591f\u6709\u6548\u6355\u6349\u70b9\u5bc6\u5ea6\u5dee\u5f02\uff0c\u8fd9\u4e9b\u4fe1\u606f\u4e0e\u652f\u6301\u96c6\u5dee\u5f02\u76f8\u534f\u8c03\uff0c\u4e3a\u5728\u652f\u6301\u96c6\u91cd\u53e0\u4f46\u5bc6\u5ea6\u5dee\u5f02\u663e\u8457\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528Wasserstein\u8ddd\u79bb\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2511.13002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13002", "abs": "https://arxiv.org/abs/2511.13002", "authors": ["Jihun Park", "Kyoungmin Lee", "Jongmin Gim", "Hyeonseo Jo", "Minseok Oh", "Wonhyeok Choi", "Kyumin Hwang", "Jaeyeul Kim", "Minwoo Choi", "Sunghoon Im"], "title": "Infinite-Story: A Training-Free Consistent Text-to-Image Generation", "comment": "18pages, 13 figures, AAAI 2026 Oral", "summary": "We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.", "AI": {"tldr": "Infinite-Story\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u5c3a\u5ea6\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u591a\u63d0\u793a\u8bcd\u8bb2\u6545\u4e8b\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u548c\u98ce\u683c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u5feb\u6a21\u578b\u5feb6\u500d\u4ee5\u4e0a\u3002", "motivation": "\u89e3\u51b3\u591a\u63d0\u793a\u8bcd\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8eab\u4efd\u4e0d\u4e00\u81f4\u548c\u98ce\u683c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\u6216\u63a8\u7406\u901f\u5ea6\u6162\u7684\u7f3a\u70b9\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u4e92\u8865\u6280\u672f\uff1a\u8eab\u4efd\u63d0\u793a\u8bcd\u66ff\u6362\u6765\u51cf\u8f7b\u6587\u672c\u7f16\u7801\u5668\u7684\u4e0a\u4e0b\u6587\u504f\u5dee\uff1b\u7edf\u4e00\u6ce8\u610f\u529b\u5f15\u5bfc\u673a\u5236\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u98ce\u683c\u6ce8\u5165\u548c\u540c\u6b65\u5f15\u5bfc\u9002\u914d\uff0c\u5171\u540c\u5f3a\u5236\u6267\u884c\u5168\u5c40\u98ce\u683c\u548c\u8eab\u4efd\u5916\u89c2\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u751f\u6210\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe\u5230\u6bcf\u5f20\u56fe\u50cf1.72\u79d2\uff0c\u6bd4\u73b0\u6709\u6700\u5feb\u7684\u4e00\u81f4\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5feb6\u500d\u4ee5\u4e0a\u3002", "conclusion": "Infinite-Story\u6846\u67b6\u5728\u4fdd\u6301\u63d0\u793a\u8bcd\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8eab\u4efd\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u89c6\u89c9\u8bb2\u6545\u4e8b\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2511.12890", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12890", "abs": "https://arxiv.org/abs/2511.12890", "authors": ["Arth Sojitra", "Omer San"], "title": "Method of Manufactured Learning for Solver-free Training of Neural Operators", "comment": null, "summary": "Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5236\u9020\u5b66\u4e60\u65b9\u6cd5\uff08MML\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u4e0d\u4f9d\u8d56\u6570\u503c\u6c42\u89e3\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6784\u9020\u7269\u7406\u4e00\u81f4\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u795e\u7ecf\u7b97\u5b50\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6602\u8d35\u6570\u503c\u6a21\u62df\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u7531\u6570\u503c\u6c42\u89e3\u5668\u751f\u6210\u7684\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u7269\u7406\u7cfb\u7edf\u63a2\u7d22\u3002MML\u65e8\u5728\u6446\u8131\u5bf9\u6c42\u89e3\u5668\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u53d7\u5236\u9020\u89e3\u65b9\u6cd5\u7684\u542f\u53d1\uff0cMML\u4ece\u53d7\u63a7\u5206\u6790\u7a7a\u95f4\u91c7\u6837\u5e73\u6ed1\u5019\u9009\u89e3\uff0c\u901a\u8fc7\u76f4\u63a5\u5e94\u7528\u63a7\u5236\u5fae\u5206\u7b97\u5b50\u63a8\u5bfc\u76f8\u5e94\u7684\u5f3a\u8feb\u573a\u3002\u5728\u63a8\u7406\u65f6\uff0c\u5c06\u8fd9\u4e9b\u5f3a\u8feb\u9879\u8bbe\u4e3a\u96f6\u4ee5\u6062\u590d\u539f\u59cb\u63a7\u5236\u65b9\u7a0b\u3002", "result": "\u5728\u70ed\u4f20\u5bfc\u3001\u5e73\u6d41\u3001Burgers\u548c\u6269\u6563\u53cd\u5e94\u7b49\u7ecf\u5178\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMML\u5b9e\u73b0\u4e86\u9ad8\u8c31\u7cbe\u5ea6\u3001\u4f4e\u6b8b\u5dee\u8bef\u5dee\uff0c\u5e76\u5bf9\u672a\u89c1\u6761\u4ef6\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MML\u901a\u8fc7\u5c06\u6570\u636e\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u6790\u5408\u6210\u8fc7\u7a0b\uff0c\u4e3a\u6784\u5efa\u7269\u7406\u57fa\u7840\u7684\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u3001\u6c42\u89e3\u5668\u65e0\u5173\u7684\u8def\u5f84\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u6570\u503c\u6a21\u62df\u6216\u5b9e\u9a8c\u6570\u636e\u3002"}}
{"id": "2511.13005", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13005", "abs": "https://arxiv.org/abs/2511.13005", "authors": ["Wenqian Ye", "Di Wang", "Guangtao Zheng", "Bohan Liu", "Aidong Zhang"], "title": "SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias", "comment": "Accepted at AAAI 2026", "summary": "Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSAGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u63d0\u793a\u9009\u62e9\u6765\u7f13\u89e3CLIP\u6a21\u578b\u4e2d\u7684\u591a\u6a21\u6001\u4f2a\u504f\u5dee\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u63d0\u5347\u96f6\u6837\u672c\u5206\u7c7b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "CLIP\u7b49\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u591a\u6a21\u6001\u4f2a\u504f\u5dee\u95ee\u9898\uff0c\u5373\u6a21\u578b\u503e\u5411\u4e8e\u4f9d\u8d56\u865a\u5047\u7279\u5f81\uff08\u5982\u80cc\u666f\uff09\u800c\u975e\u6838\u5fc3\u7279\u5f81\u8fdb\u884c\u63a8\u65ad\uff0c\u8fd9\u4e25\u91cd\u5f71\u54cd\u4e86\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e0b\u6e38\u6570\u636e\u5fae\u8c03\u6216\u5148\u9a8c\u504f\u5dee\u77e5\u8bc6\uff0c\u9650\u5236\u4e86CLIP\u7684\u5f00\u7bb1\u5373\u7528\u6027\u3002", "method": "\u63d0\u51faSAGE\u65b9\u6cd5\uff0c\u9996\u5148\u7406\u8bba\u5206\u6790\u591a\u6a21\u6001\u4f2a\u504f\u5dee\u5bf9\u96f6\u6837\u672c\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u7136\u540e\u901a\u8fc7\u5f15\u5bfc\u63d0\u793a\u9009\u62e9\u6765\u7f13\u89e3\u504f\u5dee\u3002SAGE\u65e0\u9700\u8bad\u7ec3\u3001\u5fae\u8c03\u6216\u5916\u90e8\u6807\u6ce8\uff0c\u901a\u8fc7\u63a2\u7d22\u63d0\u793a\u6a21\u677f\u7a7a\u95f4\u5e76\u9009\u62e9\u80fd\u8bf1\u5bfc\u6700\u5927\u7c7b\u522b\u8bed\u4e49\u5206\u79bb\u7684\u63d0\u793a\uff0c\u4ece\u800c\u6539\u5584\u6700\u5dee\u7ec4\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u6d41\u884c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u80fd\u6301\u7eed\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u5148\u524d\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u6216\u6a21\u578b\u66f4\u65b0\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "conclusion": "SAGE\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u8bad\u7ec3\u6216\u5916\u90e8\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u7f13\u89e3CLIP\u6a21\u578b\u7684\u591a\u6a21\u6001\u4f2a\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2511.12898", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12898", "abs": "https://arxiv.org/abs/2511.12898", "authors": ["Zhiqi Li", "Yuchen Sun", "Greg Turk", "Bo Zhu"], "title": "Functional Mean Flow in Hilbert Space", "comment": "29 pages, 13 figures", "summary": "We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.", "AI": {"tldr": "\u63d0\u51faFunctional Mean Flow (FMF)\u4f5c\u4e3a\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u4e00\u6b65\u751f\u6210\u6a21\u578b\uff0c\u5c06Mean Flow\u6846\u67b6\u6269\u5c55\u5230\u51fd\u6570\u57df\uff0c\u63d0\u4f9b\u7406\u8bba\u516c\u5f0f\u548c\u5b9e\u7528\u5b9e\u73b0\u3002", "motivation": "\u5c06\u4e00\u6b65\u751f\u6210\u6a21\u578b\u6269\u5c55\u5230\u51fd\u6570\u57df\uff0c\u4ee5\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u3001\u56fe\u50cf\u3001PDE\u548c3D\u51e0\u4f55\u7b49\u51fd\u6570\u6570\u636e\u751f\u6210\u4efb\u52a1\u3002", "method": "\u5728\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5b9a\u4e49Functional Flow Matching\uff0c\u63d0\u51fax1\u9884\u6d4b\u53d8\u4f53\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u63d0\u4f9b\u9ad8\u6548\u8bad\u7ec3\u548c\u91c7\u6837\u7684\u5b9e\u7528\u5b9e\u73b0\u3002", "result": "\u5f00\u53d1\u51fa\u4e00\u4e2a\u5b9e\u7528\u7684\u5355\u6b65Flow Matching\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u51fd\u6570\u6570\u636e\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "FMF\u662f\u4e00\u4e2a\u5728\u51fd\u6570\u57df\u4e2d\u6709\u6548\u7684\u4e00\u6b65\u751f\u6210\u6a21\u578b\uff0c\u4e3a\u5404\u79cd\u51fd\u6570\u6570\u636e\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13013", "abs": "https://arxiv.org/abs/2511.13013", "authors": ["Guoyi Zhang", "Guangsheng Xu", "Siyang Chen", "Han Wang", "Xiaohu Zhang"], "title": "You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection", "comment": null, "summary": "Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.", "AI": {"tldr": "\u63d0\u51faBP-FPN\uff0c\u4e00\u79cd\u57fa\u4e8e\u53cd\u5411\u4f20\u64ad\u7684\u7279\u5f81\u91d1\u5b57\u5854\u67b6\u6784\uff0c\u901a\u8fc7\u68af\u5ea6\u9694\u79bb\u4f4e\u5c42\u6377\u5f84\u548c\u65b9\u5411\u68af\u5ea6\u6b63\u5219\u5316\u6765\u6539\u8fdb\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u65f6\u7a7a\u7279\u5f81\u805a\u5408\uff0c\u4f46\u589e\u76ca\u6709\u9650\uff0c\u8868\u660e\u6839\u672c\u74f6\u9888\u5728\u4e8e\u6a21\u7cca\u7684\u5355\u5e27\u7279\u5f81\u8868\u793a\u800c\u975e\u65f6\u7a7a\u5efa\u6a21\u672c\u8eab\u3002", "method": "\u63d0\u51faBP-FPN\u67b6\u6784\uff0c\u5305\u542b\u68af\u5ea6\u9694\u79bb\u4f4e\u5c42\u6377\u5f84(GILS)\u6765\u6709\u6548\u6574\u5408\u7ec6\u7c92\u5ea6\u76ee\u6807\u7ec6\u8282\u800c\u4e0d\u5f15\u53d1\u6377\u5f84\u5b66\u4e60\uff0c\u4ee5\u53ca\u65b9\u5411\u68af\u5ea6\u6b63\u5219\u5316(DGR)\u6765\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\u5f3a\u5236\u5c42\u6b21\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBP-FPN\u59cb\u7ec8\u5efa\u7acb\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5b8c\u5168\u4ece\u53cd\u5411\u4f20\u64ad\u89d2\u5ea6\u4e3a\u8be5\u4efb\u52a1\u8bbe\u8ba1\u7684FPN\uff0c\u8bbe\u8ba1\u5177\u6709\u7406\u8bba\u4f9d\u636e\uff0c\u5f15\u5165\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6846\u67b6\u4e2d\u3002"}}
{"id": "2511.12905", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12905", "abs": "https://arxiv.org/abs/2511.12905", "authors": ["Tania-Amanda Fredrick Eneye", "Ashlesha Malla", "Pawan Paudel"], "title": "LinkedIn Profile Characteristics and Professional Success Indicators", "comment": null, "summary": "This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8LinkedIn\u4e2a\u4eba\u8d44\u6599\u7279\u5f81\u4e0e\u804c\u4e1a\u6210\u529f\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u67906.2\u4e07\u4efd\u533f\u540d\u8d44\u6599\uff0c\u53d1\u73b0\u664b\u5347\u53ef\u9ad8\u5ea6\u9884\u6d4b\uff0c\u800c\u7c89\u4e1d\u589e\u957f\u66f4\u4e3a\u590d\u6742\u3002", "motivation": "\u63a2\u7d22LinkedIn\u4e2a\u4eba\u8d44\u6599\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u804c\u4e1a\u6210\u529f\u6307\u6807\uff08\u664b\u5347\u3001\u7c89\u4e1d\u6570\u3001\u804c\u4e1a\u53d1\u5c55\u901f\u5ea6\uff09\uff0c\u4e3a\u4e13\u4e1a\u4eba\u58eb\u4f18\u5316LinkedIn\u5f62\u8c61\u548c\u804c\u4e1a\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u5bf9\u8d85\u8fc762,000\u4efd\u533f\u540dLinkedIn\u4e2a\u4eba\u8d44\u6599\u6570\u636e\u96c6\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\uff0c\u8bc6\u522b\u5f71\u54cd\u804c\u4e1a\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002", "result": "\u664b\u5347\u5177\u6709\u9ad8\u5ea6\u53ef\u9884\u6d4b\u6027\uff0c\u800c\u7c89\u4e1d\u589e\u957f\u8868\u73b0\u51fa\u66f4\u5927\u7684\u590d\u6742\u6027\uff0c\u6a21\u578b\u6210\u529f\u8bc6\u522b\u4e86\u9a71\u52a8\u804c\u4e1a\u6210\u529f\u7684\u6700\u6709\u5f71\u54cd\u529b\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e13\u4e1a\u4eba\u58eb\u4f18\u5316LinkedIn\u5b58\u5728\u548c\u804c\u4e1a\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u4e0d\u540c\u804c\u4e1a\u6210\u529f\u6307\u6807\u7684\u53ef\u9884\u6d4b\u6027\u5dee\u5f02\u3002"}}
{"id": "2511.13019", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13019", "abs": "https://arxiv.org/abs/2511.13019", "authors": ["Zheyuan Hu", "Chieh-Hsin Lai", "Ge Wu", "Yuki Mitsufuji", "Stefano Ermon"], "title": "MeanFlow Transformers with Representation Autoencoders", "comment": "Code is available at https://github.com/sony/mf-rae", "summary": "MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8868\u793a\u81ea\u7f16\u7801\u5668\uff08RAE\uff09\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3MeanFlow\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u4e2d\u671f\u8bad\u7ec3\u548c\u4e24\u9636\u6bb5\u65b9\u6848\u89e3\u51b3\u68af\u5ea6\u7206\u70b8\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u91c7\u6837\u6210\u672c\uff0c\u5e76\u5728ImageNet\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "MeanFlow\uff08MF\uff09\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u5b9e\u73b0\u9ad8\u6548\u5c11\u6b65\u751f\u6210\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5927\u3001\u4e0d\u7a33\u5b9a\u3001\u63a8\u7406\u6210\u672c\u9ad8\u4ee5\u53ca\u9700\u8981\u590d\u6742\u5f15\u5bfc\u53c2\u6570\u7b49\u95ee\u9898\u3002", "method": "\u5728RAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8bad\u7ec3MF\uff0c\u91c7\u7528\u4e00\u81f4\u6027\u4e2d\u671f\u8bad\u7ec3\u8fdb\u884c\u8f68\u8ff9\u611f\u77e5\u521d\u59cb\u5316\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u65b9\u6848\uff1a\u5148\u901a\u8fc7\u9884\u8bad\u7ec3\u6d41\u5339\u914d\u6559\u5e08\u8fdb\u884c\u84b8\u998f\u4ee5\u52a0\u901f\u6536\u655b\u548c\u51cf\u5c11\u65b9\u5dee\uff0c\u7136\u540e\u4f7f\u7528\u5355\u70b9\u901f\u5ea6\u4f30\u8ba1\u5668\u8fdb\u884c\u53ef\u9009\u7684\u81ea\u4e3e\u9636\u6bb5\u4ee5\u51cf\u5c11\u4e0e\u7406\u60f3\u5e73\u5747\u6d41\u7684\u504f\u5dee\u3002", "result": "\u5728ImageNet 256\u4e0a\u5b9e\u73b0\u4e861\u6b65FID\u4e3a2.03\uff0c\u4f18\u4e8e\u539f\u59cbMF\u76843.43\uff0c\u540c\u65f6\u91c7\u6837GFLOPS\u51cf\u5c1138%\uff0c\u603b\u8bad\u7ec3\u6210\u672c\u964d\u4f4e83%\u3002\u5728ImageNet 512\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u76841\u6b65FID 3.23\uff0c\u5728\u6240\u6709\u57fa\u7ebf\u4e2dGFLOPS\u6700\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u5bf9\u5f15\u5bfc\u7684\u9700\u6c42\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u914d\u7f6e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u91c7\u6837\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f18\u5f02\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u6548\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13020", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13020", "abs": "https://arxiv.org/abs/2511.13020", "authors": ["Yufei Wen", "Yuting Zhang", "Jingdan Kang", "Hao Ren", "Weibin Cheng", "Jintai Chen", "Kaishun Wu"], "title": "SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction", "comment": null, "summary": "Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.", "AI": {"tldr": "\u63d0\u51fa\u4e86SpectralAdapt\u534a\u76d1\u7763\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5149\u8c31\u5bc6\u5ea6\u63a9\u853d\u548c\u5149\u8c31\u7aef\u5143\u8868\u793a\u5bf9\u9f50\u6280\u672f\uff0c\u89e3\u51b3\u533b\u5b66HSI\u91cd\u5efa\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u57df\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5e94\u7528\u4e2dHSI\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u57df\u9002\u5e94\u65b9\u6cd5\u5229\u7528\u4e30\u5bcc\u7684\u901a\u7528\u9886\u57dfHSI\u6570\u636e\u6765\u63d0\u5347\u533b\u5b66HSI\u91cd\u5efa\u6027\u80fd\u3002", "method": "\u63d0\u51faSpectralAdapt\u6846\u67b6\uff0c\u5305\u542b\u5149\u8c31\u5bc6\u5ea6\u63a9\u853d\uff08\u81ea\u9002\u5e94\u63a9\u853dRGB\u901a\u9053\uff09\u548c\u5149\u8c31\u7aef\u5143\u8868\u793a\u5bf9\u9f50\uff08\u4f7f\u7528\u7269\u7406\u53ef\u89e3\u91ca\u7aef\u5143\u4f5c\u4e3a\u57df\u4e0d\u53d8\u951a\u70b9\uff09\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u5149\u8c31\u4fdd\u771f\u5ea6\u3001\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u6709\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "SSDA\u4e3a\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u9ad8\u5149\u8c31\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u57df\u504f\u79fb\u3001\u5149\u8c31\u9000\u5316\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2511.13026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13026", "abs": "https://arxiv.org/abs/2511.13026", "authors": ["Jiaze Li", "Hao Yin", "Wenhui Tan", "Jingyang Chen", "Boshen Xu", "Yuxun Qu", "Yijing Chen", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan"], "title": "REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding", "comment": null, "summary": "Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86REVISOR\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u53cd\u601d\u673a\u5236\u589e\u5f3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u5fae\u8c03\u6216\u5916\u90e8\u6a21\u578b\u5373\u53ef\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u7eaf\u6587\u672c\u7684\u53cd\u601d\u673a\u5236\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a1\uff09\u957f\u89c6\u9891\u5305\u542b\u66f4\u4e30\u5bcc\u52a8\u6001\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u4ec5\u53cd\u601d\u6587\u672c\u4fe1\u606f\u4e0d\u8db3\uff1b2\uff09\u7eaf\u6587\u672c\u53cd\u601d\u7f3a\u4e4f\u8de8\u6a21\u6001\u4ea4\u4e92\u80fd\u529b\uff0c\u65e0\u6cd5\u5145\u5206\u6574\u5408\u89c6\u89c9\u4fe1\u606f\u3002", "method": "\u63d0\u51faREVISOR\u6846\u67b6\uff0c\u652f\u6301\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u534f\u4f5c\u53cd\u601d\u8fc7\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53cc\u5f52\u56e0\u89e3\u8026\u5956\u52b1\u673a\u5236\uff0c\u96c6\u6210\u5230GRPO\u8bad\u7ec3\u7b56\u7565\u4e2d\uff0c\u786e\u4fdd\u6a21\u578b\u63a8\u7406\u4e0e\u6240\u9009\u89c6\u9891\u8bc1\u636e\u4e4b\u95f4\u7684\u56e0\u679c\u5bf9\u9f50\u3002", "result": "\u5728VideoMME\u3001LongVideoBench\u3001MLVU\u548cLVBench\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "REVISOR\u6846\u67b6\u901a\u8fc7\u8de8\u6a21\u6001\u53cd\u601d\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u6574\u5408\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12955", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12955", "abs": "https://arxiv.org/abs/2511.12955", "authors": ["Onur Vural", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series", "comment": "This work has been accepted at the 2025 IEEE International Conference on Big Data (IEEE BigData 2025) on October 23, 2025", "summary": "Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5168\u5c40\u8de8\u65f6\u95f4\u6ce8\u610f\u529b\u878d\u5408(GCTAF)\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u592a\u9633\u8000\u6591\u9884\u6d4b\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u548c\u957f\u7a0b\u65f6\u95f4\u5efa\u6a21\u7684\u6311\u6218\u3002", "motivation": "\u592a\u9633\u8000\u6591\u9884\u6d4b\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8000\u6591\u4e8b\u4ef6\u7684\u9ad8\u5ea6\u4e0d\u5e73\u8861\u6027\uff08\u5f3a\u70c8\u8000\u6591\u7a00\u5c11\uff09\u548c\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u7a0b\u65f6\u95f4\u5efa\u6a21\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faGCTAF\u67b6\u6784\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u8de8\u6ce8\u610f\u529b\u5168\u5c40\u4ee4\u724c\uff0c\u8fd9\u4e9b\u4ee4\u724c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u4e0e\u8f93\u5165\u5e8f\u5217\u4ea4\u4e92\uff0c\u603b\u7ed3\u6574\u4e2a\u5e8f\u5217\u4e2d\u7684\u91cd\u8981\u65f6\u95f4\u6a21\u5f0f\uff0c\u7136\u540e\u878d\u5408\u56de\u65f6\u95f4\u8868\u793a\u4e2d\u3002", "result": "\u5728\u57fa\u51c6\u592a\u9633\u8000\u6591\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cGCTAF\u80fd\u6709\u6548\u68c0\u6d4b\u5f3a\u70c8\u8000\u6591\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u6539\u8fdb\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u4e3a\u592a\u9633\u8000\u6591\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6f5c\u529b\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.13031", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13031", "abs": "https://arxiv.org/abs/2511.13031", "authors": ["Weihua Wang", "Yubo Cui", "Xiangru Lin", "Zhiheng Li", "Zheng Fang"], "title": "Towards 3D Object-Centric Feature Learning for Semantic Scene Completion", "comment": "Accept by AAAI-2026", "summary": "Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.", "AI": {"tldr": "Ocean\u662f\u4e00\u4e2a\u9762\u5411\u5bf9\u8c61\u76843D\u8bed\u4e49\u573a\u666f\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u573a\u666f\u5206\u89e3\u4e3a\u72ec\u7acb\u5bf9\u8c61\u5b9e\u4f8b\u6765\u63d0\u9ad8\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u5728\u6574\u4e2a\u573a\u666f\u4e2d\u805a\u5408\u548c\u6269\u6563\u7279\u5f81\uff0c\u4f46\u5ffd\u7565\u4e86\u7ec6\u7c92\u5ea6\u7684\u5bf9\u8c61\u7ea7\u7ec6\u8282\uff0c\u5bfc\u81f4\u590d\u6742\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u6a21\u7cca\u3002", "method": "\u9996\u5148\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5206\u5272\u6a21\u578bMobileSAM\u4ece\u8f93\u5165\u56fe\u50cf\u4e2d\u63d0\u53d6\u5b9e\u4f8b\u63a9\u7801\uff1b\u7136\u540e\u5f15\u51653D\u8bed\u4e49\u7ec4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5229\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u57283D\u7a7a\u95f4\u4e2d\u805a\u5408\u9762\u5411\u5bf9\u8c61\u7684\u7279\u5f81\uff1b\u8bbe\u8ba1\u5168\u5c40\u76f8\u4f3c\u6027\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\u5904\u7406\u5206\u5272\u9519\u8bef\u548c\u7f3a\u5931\u5b9e\u4f8b\uff1b\u6700\u540e\u63d0\u51fa\u5b9e\u4f8b\u611f\u77e5\u5c40\u90e8\u6269\u6563\u6a21\u5757\uff0c\u901a\u8fc7\u751f\u6210\u8fc7\u7a0b\u6539\u8fdb\u5b9e\u4f8b\u7279\u5f81\u5e76\u5728BEV\u7a7a\u95f4\u4e2d\u7ec6\u5316\u573a\u666f\u8868\u793a\u3002", "result": "\u5728SemanticKITTI\u548cSSCBench-KITTI360\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOcean\u5206\u522b\u5b9e\u73b0\u4e8617.40\u548c20.28\u7684mIoU\u5206\u6570\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Ocean\u901a\u8fc7\u9762\u5411\u5bf9\u8c61\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D\u8bed\u4e49\u573a\u666f\u8865\u5168\u4e2d\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u6a21\u7cca\u95ee\u9898\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.12979", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.12979", "abs": "https://arxiv.org/abs/2511.12979", "authors": ["Zhengchao Wang", "Yitao Hu", "Jianing Ye", "Zhuxuan Chang", "Jiazheng Yu", "Youpeng Deng", "Keqiu Li"], "title": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RAGPulse\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u5927\u89c4\u6a21RAG\u5de5\u4f5c\u8d1f\u8f7d\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6536\u96c6\u81ea\u670d\u52a1\u8d85\u8fc740,000\u540d\u5e08\u751f\u7684\u5927\u89c4\u6a21\u95ee\u7b54\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709LLM\u63a8\u7406\u8ddf\u8e2a\u65e0\u6cd5\u6355\u6349RAG\u7279\u5b9a\u52a8\u6001\u7684\u95ee\u9898\u3002", "motivation": "RAG\u7cfb\u7edf\u7684\u591a\u9636\u6bb5\u6d41\u6c34\u7ebf\u548c\u72ec\u7279\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\uff08\u5982\u77e5\u8bc6\u4f9d\u8d56\uff09\u7ed9\u670d\u52a1\u6027\u80fd\u4f18\u5316\u5e26\u6765\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u901a\u7528LLM\u63a8\u7406\u8ddf\u8e2a\u65e0\u6cd5\u6355\u6349\u8fd9\u4e9bRAG\u7279\u5b9a\u52a8\u6001\uff0c\u5bfc\u81f4\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u81ea2024\u5e744\u6708\u8d77\u670d\u52a1\u8d85\u8fc740,000\u540d\u5e08\u751f\u7684\u5927\u5b66\u8303\u56f4\u95ee\u7b54\u7cfb\u7edf\u6536\u96c6RAG\u5de5\u4f5c\u8d1f\u8f7d\u6570\u636e\uff0c\u91c7\u7528\u9690\u79c1\u4fdd\u62a4\u7684\u57fa\u4e8e\u54c8\u5e0c\u7684\u6570\u636e\u683c\u5f0f\uff0c\u5e76\u8fdb\u884c\u6df1\u5165\u7684\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u5206\u6790\u663e\u793a\u771f\u5b9e\u4e16\u754c\u7684RAG\u5de5\u4f5c\u8d1f\u8f7d\u8868\u73b0\u51fa\u663e\u8457\u7684\u65f6\u95f4\u5c40\u90e8\u6027\u548c\u9ad8\u5ea6\u503e\u659c\u7684\u70ed\u95e8\u6587\u6863\u8bbf\u95ee\u6a21\u5f0f\u3002RAGPulse\u4e3a\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u5185\u5bb9\u611f\u77e5\u6279\u5904\u7406\u548c\u68c0\u7d22\u7f13\u5b58\u7b49\u4f18\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u57fa\u7840\u3002", "conclusion": "RAGPulse\u586b\u8865\u4e86RAG\u7cfb\u7edf\u6027\u80fd\u4f18\u5316\u7814\u7a76\u7684\u6570\u636e\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u65b0\u9896\u4f18\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\uff0c\u6700\u7ec8\u5c06\u63d0\u5347RAG\u670d\u52a1\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.13036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13036", "abs": "https://arxiv.org/abs/2511.13036", "authors": ["Dahyun Chung", "Donghyun Shin", "Yujin Sung", "Seunggi Moon", "Jinwoo Jeon", "Byung-Jun Lee"], "title": "uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data", "comment": "Our project page can be found at https://dinyudin203.github.io/uCLIP-project/", "summary": "Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u6846\u67b6\uff0c\u65e0\u9700\u56fe\u50cf-\u6587\u672c\u5bf9\u6216\u6587\u672c-\u6587\u672c\u5bf9\uff0c\u4ec5\u8bad\u7ec3\u4e00\u4e2a170\u4e07\u53c2\u6570\u7684\u6295\u5f71\u6a21\u5757\uff0c\u4f7f\u7528\u82f1\u8bed\u8868\u793a\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6377\u514b\u8bed\u3001\u82ac\u5170\u8bed\u3001\u514b\u7f57\u5730\u4e9a\u8bed\u3001\u5308\u7259\u5229\u8bed\u548c\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7b49\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u4e0a\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u82f1\u8bed\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6269\u5c55\u53d7\u9650\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u591a\u8bed\u8a00\u56fe\u50cf-\u6587\u672c\u6570\u636e\u3002\u73b0\u6709\u591a\u8bed\u8a00\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728XM3600\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5bf9\u6377\u514b\u8bed\u3001\u82ac\u5170\u8bed\u3001\u514b\u7f57\u5730\u4e9a\u8bed\u3001\u5308\u7259\u5229\u8bed\u548c\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7b49\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u7684\u68c0\u7d22\u6027\u80fd\u666e\u904d\u8f83\u4f4e\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6570\u636e\u9ad8\u6548\u6846\u67b6\uff0c\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u7f16\u7801\u5668\u548c\u591a\u8bed\u8a00\u6587\u672c\u7f16\u7801\u5668\uff0c\u4ec5\u8bad\u7ec3\u4e00\u4e2a\u7d27\u51d1\u7684170\u4e07\u53c2\u6570\u6295\u5f71\u6a21\u5757\uff0c\u4f7f\u7528\u82f1\u8bed\u8868\u793a\u4f5c\u4e3a\u8bed\u4e49\u951a\u70b9\u8fdb\u884c\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\uff0c\u65e0\u9700\u56fe\u50cf-\u6587\u672c\u5bf9\u6216\u6587\u672c-\u6587\u672c\u5bf9\u3002", "result": "\u5728\u591a\u4e2a\u591a\u8bed\u8a00\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u4e94\u4e2a\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u4e0a\u663e\u793a\u51fa\u663e\u8457\u63d0\u5347\uff0c\u8fd9\u4e9b\u8bed\u8a00\u6b63\u662f\u73b0\u6709\u6a21\u578b\u901a\u5e38\u8868\u73b0\u4e0d\u4f73\u7684\u5730\u65b9\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u57fa\u4e8e\u67a2\u8f74\u3001\u53c2\u6570\u9ad8\u6548\u7684\u5bf9\u9f50\u7b56\u7565\u5728\u5305\u5bb9\u6027\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u591a\u8bed\u8a00\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12986", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.12986", "abs": "https://arxiv.org/abs/2511.12986", "authors": ["Abdelouahed Ben Mhamed", "Assia Kamal-Idrissi", "Amal El Fallah Seghrouchni"], "title": "Learning Branching Policies for MILPs with Proximal Policy Optimization", "comment": "11 pages, 3 figures, AAAI conference", "summary": "Branch-and-Bound (B\\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTGPPO\u6846\u67b6\uff0c\u4f7f\u7528PPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\u5206\u652f\u7b56\u7565\uff0c\u65e8\u5728\u63d0\u9ad8\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u4e2d\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u5728\u5f02\u6784\u5b9e\u4f8b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u5206\u652f\u7b56\u7565\u5bb9\u6613\u8fc7\u62df\u5408\u4e13\u5bb6\u6f14\u793a\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u7ed3\u6784\u591a\u6837\u6216\u672a\u89c1\u8fc7\u7684\u5b9e\u4f8b\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u5206\u652f\u7b56\u7565\u3002", "method": "\u63d0\u51faTree-Gate PPO\u6846\u67b6\uff0c\u6784\u5efa\u53c2\u6570\u5316\u72b6\u6001\u7a7a\u95f4\u8868\u793a\uff0c\u52a8\u6001\u6355\u6349\u641c\u7d22\u6811\u7684\u6f14\u5316\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u8bad\u7ec3\u5206\u652f\u7b56\u7565\u3002", "result": "TGPPO\u5728\u51cf\u5c11\u63a2\u7d22\u8282\u70b9\u6570\u548c\u6539\u8fdbp-\u539f\u59cb\u5bf9\u5076\u79ef\u5206\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u5b9e\u4f8b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5177\u6709\u5f00\u53d1\u9c81\u68d2\u4e14\u81ea\u9002\u5e94MILP\u6c42\u89e3\u5668\u5206\u652f\u7b56\u7565\u7684\u6f5c\u529b\uff0cTGPPO\u5c55\u793a\u4e86RL\u5728\u63d0\u5347\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.13047", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13047", "abs": "https://arxiv.org/abs/2511.13047", "authors": ["Yan Gong", "Jianli Lu", "Yongsheng Gao", "Jie Zhao", "Xiaojuan Zhang", "Susanto Rahardja"], "title": "DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation", "comment": "11 pages, 5 figures, 5 tables", "summary": "Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.", "AI": {"tldr": "DiffPixelFormer\u662f\u4e00\u79cd\u7528\u4e8eRGB-D\u5ba4\u5185\u573a\u666f\u8bed\u4e49\u5206\u5272\u7684\u5dee\u5206\u50cf\u7d20\u611f\u77e5Transformer\uff0c\u901a\u8fc7\u589e\u5f3a\u6a21\u6001\u5185\u8868\u793a\u548c\u5efa\u6a21\u6a21\u6001\u95f4\u4ea4\u4e92\u6765\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709RGB-D\u878d\u5408\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u7684\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bf9\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7279\u5f81\u5173\u7cfb\u5efa\u6a21\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7279\u5f81\u5bf9\u9f50\u4e0d\u7cbe\u786e\u548c\u5224\u522b\u6027\u8868\u793a\u53d7\u9650\u3002", "method": "\u63d0\u51faIntra-Inter Modal Interaction Block (IIMIB)\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u6355\u83b7\u6a21\u6001\u5185\u957f\u7a0b\u4f9d\u8d56\uff0c\u4f7f\u7528Differential-Shared Inter-Modal (DSIM)\u6a21\u5757\u5efa\u6a21\u6a21\u6001\u95f4\u4ea4\u4e92\uff0c\u5206\u79bb\u6a21\u6001\u7279\u5b9a\u548c\u5171\u4eab\u7ebf\u7d22\uff0c\u5b9e\u73b0\u50cf\u7d20\u7ea7\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u91c7\u7528\u52a8\u6001\u878d\u5408\u7b56\u7565\u5e73\u8861\u6a21\u6001\u8d21\u732e\u3002", "result": "\u5728SUN RGB-D\u548cNYUDv2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiffPixelFormer-L\u5206\u522b\u8fbe\u523054.28%\u548c59.95%\u7684mIoU\uff0c\u6bd4DFormer-L\u5206\u522b\u63d0\u53471.78%\u548c2.75%\u3002", "conclusion": "DiffPixelFormer\u901a\u8fc7\u6709\u6548\u7684\u6a21\u6001\u5185\u8868\u793a\u589e\u5f3a\u548c\u6a21\u6001\u95f4\u4ea4\u4e92\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB-D\u5ba4\u5185\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2511.13054", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13054", "abs": "https://arxiv.org/abs/2511.13054", "authors": ["Bo Fang", "Yuxin Song", "Qiangqiang Wu", "Haoyuan Sun", "Wenhao Wu", "Antoni B. Chan"], "title": "ViSS-R1: Self-Supervised Reinforcement Video Reasoning", "comment": "Our paper was initially titled \"Video-SSR1: Self-Supervised Reinforcement Video Reasoning.\" Upon noticing its close resemblance to the title of a recently released paper, we have decided to rename our work as \"ViSS-R1.\"", "summary": "Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Pretext-GRPO\u7b97\u6cd5\u548cViSS-R1\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u89c6\u89c9\u4e2d\u5fc3\u7406\u89e3\u80fd\u529b\uff0c\u907f\u514d\u5bf9\u6587\u672c\u4fe1\u606f\u7684\u8fc7\u5ea6\u4f9d\u8d56\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eR1\u7684\u65b9\u6cd5\u5728\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u5f80\u5f80\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e30\u5bcc\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u5bb9\u6613\u5bfc\u81f4\u6377\u5f84\u5b66\u4e60\u548c\u5e7b\u89c9\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u89c6\u89c9\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5Pretext-GRPO\uff0c\u901a\u8fc7\u5bf9\u53d8\u6362\u540e\u7684\u89c6\u89c9\u8f93\u5165\u6b63\u786e\u89e3\u51b3\u524d\u7f6e\u4efb\u52a1\u6765\u5206\u914d\u6b63\u5956\u52b1\uff1b2. \u63d0\u51faViSS-R1\u6846\u67b6\uff0c\u5c06\u524d\u7f6e\u4efb\u52a1\u81ea\u76d1\u7763\u5b66\u4e60\u76f4\u63a5\u96c6\u6210\u5230MLLM\u7684R1\u540e\u8bad\u7ec3\u8303\u5f0f\u4e2d\uff0c\u540c\u65f6\u5904\u7406\u524d\u7f6e\u95ee\u9898\u548c\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u89c6\u9891\u63a8\u7406\u548c\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86Pretext-GRPO\u548cViSS-R1\u5728\u590d\u6742\u89c6\u9891\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5f3a\u5236\u6a21\u578b\u5bf9\u53d8\u6362\u540e\u7684\u89c6\u89c9\u8f93\u5165\u8fdb\u884c\u63a8\u7406\uff0c\u901a\u8fc7\u8bc6\u522b\u5e94\u7528\u7684\u53d8\u6362\u548c\u91cd\u5efa\u539f\u59cb\u89c6\u9891\u6765\u5236\u5b9a\u51c6\u786e\u7684\u6700\u7ec8\u7b54\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2511.13055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13055", "abs": "https://arxiv.org/abs/2511.13055", "authors": ["Ruixin Liu", "Zejian Yuan"], "title": "Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries", "comment": null, "summary": "Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.", "AI": {"tldr": "MonoUnc\u662f\u4e00\u4e2a\u65e0\u9700\u9e1f\u77b0\u56fe\u7684\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5c40\u90e8\u8f66\u9053\u7ed3\u6784\u5efa\u6a21\u6765\u663e\u5f0f\u5904\u7406\u89c2\u6d4b\u566a\u58f0\u5f15\u8d77\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7b80\u5316\u7684\u51e0\u4f55\u5047\u8bbe\uff0c\u5982\u72ec\u7acb\u70b9\u9884\u6d4b\u6216\u5168\u5c40\u5e73\u9762\u5efa\u6a21\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u7ed3\u6784\u53d8\u5316\u548c\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5c063D\u8f66\u9053\u7ebf\u6295\u5f71\u5230\u524d\u89c6\u56fe\u7a7a\u95f4\u5e76\u7528\u53c2\u6570\u5316\u66f2\u7ebf\u8fd1\u4f3c\uff0c\u57fa\u4e8e\u66f2\u7ebf\u9884\u6d4b\u52a8\u6001\u751f\u6210\u66f2\u7ebf\u70b9\u67e5\u8be2\u5d4c\u5165\uff0c\u5c06\u76f8\u90bb\u70b9\u5f62\u6210\u7684\u7ebf\u6bb5\u5efa\u6a21\u4e3a3D\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u76843D\u9ad8\u65af\u5339\u914d\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728ONCE-3DLanes\u548cOpenLane\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMonoUnc\u5728\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6807\u51c6\u4e0b\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "MonoUnc\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5c40\u90e8\u8f66\u9053\u7ed3\u6784\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13022", "abs": "https://arxiv.org/abs/2511.13022", "authors": ["Eshani Patel", "Yisong Yue", "Geeling Chau"], "title": "Learning Time-Scale Invariant Population-Level Neural Representations", "comment": "10 pages, 5 figures, NeurIPS 2025 Foundation Models for the Brain and Body", "summary": "General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e2d\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u5339\u914d\u5bf9\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u65f6\u95f4\u5c3a\u5ea6\u589e\u5f3a\u9884\u8bad\u7ec3(TSAP)\u65b9\u6cd5\u6765\u63d0\u9ad8\u6a21\u578b\u5bf9\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u901a\u7528\u795e\u7ecf\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5bf9\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\u548c\u8111\u673a\u63a5\u53e3\u5e94\u7528\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5bf9\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u4efb\u52a1\u95f4\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u5339\u914d\u654f\u611f\uff0c\u7f3a\u4e4f\u4e0d\u53d8\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u65f6\u95f4\u5c3a\u5ea6\u589e\u5f3a\u9884\u8bad\u7ec3(TSAP)\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u65f6\u95f4\u5c3a\u5ea6\u589e\u5f3a\u6765\u6784\u5efa\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u4e0d\u53d8\u6027\u3002", "result": "TSAP\u65b9\u6cd5\u5728\u4e0d\u540c\u89e3\u7801\u4efb\u52a1\u4e2d\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u5bf9\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u5efa\u7acb\u4e86\u4e0d\u53d8\u6027\u3002", "conclusion": "\u5904\u7406\u9884\u5904\u7406\u591a\u6837\u6027\u662f\u6784\u5efa\u53ef\u6cdb\u5316\u795e\u7ecf\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\uff0cTSAP\u4e3a\u89e3\u51b3\u65f6\u95f4\u5c3a\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2511.13023", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13023", "abs": "https://arxiv.org/abs/2511.13023", "authors": ["Jiacheng Wang", "Yejun Zeng", "Jinyang Guo", "Yuqing Ma", "Aishan Liu", "Xianglong Liu"], "title": "SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment", "comment": null, "summary": "Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.", "AI": {"tldr": "SLMQuant\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30LLM\u538b\u7f29\u6280\u672f\u5728SLMs\u4e0a\u5e94\u7528\u7684\u57fa\u51c6\uff0c\u53d1\u73b0SLMs\u548cLLMs\u5728\u91cf\u5316\u654f\u611f\u6027\u4e0a\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u76f4\u63a5\u8fc1\u79fbLLM\u4f18\u5316\u6280\u672f\u4f1a\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8d44\u6e90\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u6a21\u578b\u538b\u7f29\u4e2d\u7684\u6548\u7387\u5dee\u8ddd\u5c1a\u672a\u89e3\u51b3\u3002\u91cf\u5316\u5bf9LLMs\u6709\u6548\uff0c\u4f46\u5bf9SLMs\u7684\u9002\u7528\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8de8\u4e0d\u540c\u67b6\u6784\u548c\u4efb\u52a1\u7684\u5168\u9762\u591a\u8f68\u9053\u8bc4\u4f30\uff0c\u5206\u6790\u6700\u5148\u8fdb\u7684\u91cf\u5316\u65b9\u6cd5\u5728SLMs\u4e0a\u7684\u8868\u73b0\u3002\u8bc6\u522b\u5f71\u54cdSLM\u6709\u6548\u91cf\u5316\u7684\u5173\u952e\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86SLMs\u548cLLMs\u5728\u91cf\u5316\u654f\u611f\u6027\u4e0a\u7684\u6839\u672c\u5dee\u5f02\uff0c\u8bc1\u660e\u7531\u4e8eSLMs\u72ec\u7279\u7684\u67b6\u6784\u7279\u5f81\u548c\u8bad\u7ec3\u52a8\u6001\uff0c\u76f4\u63a5\u8fc1\u79fbLLM\u4f18\u5316\u6280\u672f\u4f1a\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u3002", "conclusion": "SLMQuant\u4e3a\u5728\u8fb9\u7f18\u5e94\u7528\u4e2d\u63a8\u8fdb\u9ad8\u6548SLM\u90e8\u7f72\u5efa\u7acb\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u5e76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u90e8\u7f72\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2511.13065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13065", "abs": "https://arxiv.org/abs/2511.13065", "authors": ["Reeshoon Sayera", "Akash Kumar", "Sirshapan Mitra", "Prudvi Kamtam", "Yogesh S Rawat"], "title": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition", "comment": "IEEE WACV'26 Main Conference", "summary": "Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86RobustGait\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u5916\u89c2\u7684\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u5e72\u6270\u548c\u8f6e\u5ed3\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u6db5\u76d6\u56db\u79cd\u6270\u52a8\u7c7b\u578b\u3001\u591a\u79cd\u8f6e\u5ed3\u63d0\u53d6\u65b9\u6cd5\u548c\u6a21\u578b\u67b6\u6784\uff0c\u53d1\u73b0\u4e86\u566a\u58f0\u4f20\u64ad\u3001\u8f6e\u5ed3\u63d0\u53d6\u5668\u504f\u5dee\u7b49\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\u7b49\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5916\u89c2\u7684\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u5728\u53d7\u63a7\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e72\u6270\u548c\u8f6e\u5ed3\u53d8\u5316\u4e0b\u9c81\u68d2\u6027\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1RobustGait\u6846\u67b6\uff0c\u5728\u56db\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\uff1a\u6270\u52a8\u7c7b\u578b\uff08\u6570\u5b57\u3001\u73af\u5883\u3001\u65f6\u95f4\u3001\u906e\u6321\uff09\u3001\u8f6e\u5ed3\u63d0\u53d6\u65b9\u6cd5\u3001\u6b65\u6001\u8bc6\u522b\u6a21\u578b\u67b6\u6784\u5bb9\u91cf\u548c\u90e8\u7f72\u573a\u666f\uff0c\u5f15\u516515\u79cd\u5e72\u6270\u7c7b\u578b\u548c5\u4e2a\u4e25\u91cd\u7ea7\u522b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f306\u4e2a\u6700\u5148\u8fdb\u7684\u6b65\u6001\u7cfb\u7edf\u3002", "result": "\u53d1\u73b0RGB\u7ea7\u522b\u566a\u58f0\u80fd\u66f4\u597d\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u9000\u5316\uff1b\u6b65\u6001\u7cbe\u5ea6\u5bf9\u8f6e\u5ed3\u63d0\u53d6\u5668\u504f\u5dee\u9ad8\u5ea6\u654f\u611f\uff1b\u9c81\u68d2\u6027\u65e2\u53d6\u51b3\u4e8e\u6270\u52a8\u7c7b\u578b\u4e5f\u53d6\u51b3\u4e8e\u67b6\u6784\u8bbe\u8ba1\uff1b\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u63ed\u793a\u4e86\u8f6e\u5ed3\u63d0\u53d6\u5668\u504f\u5dee\u8fd9\u4e00\u88ab\u5ffd\u89c6\u7684\u57fa\u51c6\u504f\u5dee\u6765\u6e90\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u8bad\u7ec3\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u7cfb\u7edf\u5411\u53ef\u90e8\u7f72\u72b6\u6001\u53d1\u5c55\u3002"}}
{"id": "2511.13079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13079", "abs": "https://arxiv.org/abs/2511.13079", "authors": ["Jiacheng Tang", "Mingyue Feng", "Jiachao Liu", "Yaonong Wang", "Jian Pu"], "title": "Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving", "comment": "11 pages, 8 figures", "summary": "Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdaptiveAD\u67b6\u6784\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u7ed3\u6784\u89e3\u8026\u573a\u666f\u611f\u77e5\u548c\u81ea\u8f66\u72b6\u6001\uff0c\u89e3\u51b3\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u81ea\u8f66\u72b6\u6001\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u573a\u666f\u7406\u89e3\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u5757\u5316\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u81ea\u8f66\u72b6\u6001\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u548c\u573a\u666f\u7406\u89e3\u4e0d\u9c81\u68d2\u3002\u6839\u672c\u539f\u56e0\u5728\u4e8e\u67b6\u6784\u8bbe\u8ba1\u4e2d\u81ea\u8f66\u72b6\u6001\u8fc7\u65e9\u878d\u5408\u5230BEV\u7f16\u7801\u5668\u4e2d\uff0c\u5f62\u6210\u4fe1\u606f\u6377\u5f84\u3002", "method": "\u63d0\u51faAdaptiveAD\u67b6\u6784\uff1a1) \u53cc\u5206\u652f\u7ed3\u6784\u5206\u522b\u8fdb\u884c\u573a\u666f\u9a71\u52a8\u63a8\u7406\uff08\u591a\u4efb\u52a1\u5b66\u4e60\u4f46\u6392\u9664\u81ea\u8f66\u72b6\u6001\uff09\u548c\u81ea\u8f66\u9a71\u52a8\u63a8\u7406\uff1b2) \u573a\u666f\u611f\u77e5\u878d\u5408\u6a21\u5757\u81ea\u9002\u5e94\u6574\u5408\u4e24\u5206\u652f\u51b3\u7b56\uff1b3) \u8def\u5f84\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u81ea\u8f66-BEV\u4ea4\u4e92\uff1b4) BEV\u5355\u5411\u84b8\u998f\u548c\u81ea\u56de\u5f52\u5728\u7ebf\u6620\u5c04\u8f85\u52a9\u4efb\u52a1\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cAdaptiveAD\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u73af\u89c4\u5212\u6027\u80fd\uff0c\u663e\u8457\u51cf\u8f7b\u4e86\u5bf9\u81ea\u8f66\u72b6\u6001\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AdaptiveAD\u901a\u8fc7\u67b6\u6784\u5c42\u9762\u7684\u591a\u4e0a\u4e0b\u6587\u878d\u5408\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u81ea\u8f66\u72b6\u6001\u7684\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u6cdb\u5316\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.13044", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13044", "abs": "https://arxiv.org/abs/2511.13044", "authors": ["Rosario Napoli", "Giovanni Lonia", "Antonio Celesti", "Massimo Villari", "Maria Fazio"], "title": "Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data", "comment": "Accepted at the 14th International Joint Conference on Knowledge Graphs (IJCKG) 2025", "summary": "Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.", "AI": {"tldr": "Bi-View\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408Node2Vec\u548cGraphSAGE\u4e24\u79cd\u56fe\u5d4c\u5165\u6280\u672f\uff0c\u589e\u5f3a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u8282\u70b9\u7279\u5f81\u7684\u4fe1\u606f\u5185\u5bb9\uff0c\u4ece\u800c\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u5408\u6210\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u56fe\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u624d\u80fd\u8868\u73b0\u826f\u597d\uff0c\u9650\u5236\u4e86\u5728\u7a00\u758f\u6216\u4e0d\u5b8c\u6574\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u56fe\u673a\u5668\u5b66\u4e60\u867d\u7136\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728\u5904\u7406\u77e5\u8bc6\u56fe\u8c31\u65f6\u4ecd\u9762\u4e34\u4fe1\u606f\u9690\u85cf\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u7684\u56fe\u5d4c\u5165\u6280\u672f\uff1aNode2Vec\uff08\u901a\u8fc7\u65e0\u76d1\u7763\u968f\u673a\u6e38\u8d70\u6355\u83b7\u7ed3\u6784\u6a21\u5f0f\uff09\u548cGraphSAGE\uff08\u4ee5\u76d1\u7763\u65b9\u5f0f\u805a\u5408\u90bb\u57df\u4fe1\u606f\uff09\u3002\u9996\u5148\u8ba1\u7b97Node2Vec\u5d4c\u5165\u8868\u793a\u56fe\u62d3\u6251\uff0c\u7136\u540e\u7528\u57fa\u4e8e\u4e2d\u5fc3\u6027\u7684\u6307\u6807\u4e30\u5bcc\u8282\u70b9\u7279\u5f81\u4f5c\u4e3aGraphSAGE\u7684\u8f93\u5165\uff0c\u6700\u540e\u901a\u8fc7\u878d\u5408\u5c42\u7ed3\u5408\u4e24\u79cd\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u521d\u59cb\u7279\u5f81\u8f83\u5dee\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u66f4\u51c6\u786e\u548c\u7cbe\u786e\u7684KG\u589e\u5f3a\u56fe\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "Bi-View\u65b9\u6cd5\u80fd\u591f\u6355\u83b7\u56fe\u7684\u62d3\u6251\u548c\u8bed\u4e49\u5c5e\u6027\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u4f46\u672a\u660e\u786e\u8868\u793a\u7684\u4fe1\u606f\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u56fe\u8c31\u4e2d\u4fe1\u606f\u9690\u85cf\u7684\u95ee\u9898\u3002"}}
{"id": "2511.13081", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13081", "abs": "https://arxiv.org/abs/2511.13081", "authors": ["Yehonatan Elisha", "Seffi Cohen", "Oren Barkan", "Noam Koenigstein"], "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations", "comment": null, "summary": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations.Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RFxG\u5206\u7c7b\u6cd5\uff0c\u4ece\u53c2\u8003\u6846\u67b6\u548c\u7c92\u5ea6\u4e24\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u663e\u8457\u6027\u89e3\u91ca\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u6765\u7cfb\u7edf\u8bc4\u4f30\u89e3\u91ca\u65b9\u6cd5\u7684\u8d28\u91cf\u3002", "motivation": "\u663e\u8457\u6027\u56fe\u5728\u6df1\u5ea6\u5b66\u4e60\u89c6\u89c9\u89e3\u91ca\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u5176\u9884\u671f\u76ee\u7684\u548c\u4e0e\u4e0d\u540c\u7528\u6237\u67e5\u8be2\u4e00\u81f4\u6027\u7684\u5171\u8bc6\uff0c\u8fd9\u79cd\u6a21\u7cca\u6027\u963b\u788d\u4e86\u89e3\u91ca\u65b9\u6cd5\u7684\u6709\u6548\u8bc4\u4f30\u548c\u5b9e\u9645\u6548\u7528\u3002", "method": "\u5f15\u5165Reference-Frame \u00d7 Granularity (RFxG)\u5206\u7c7b\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u539f\u5219\u6027\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5c06\u663e\u8457\u6027\u89e3\u91ca\u7ec4\u7ec7\u5728\u4e24\u4e2a\u57fa\u672c\u8f74\u4e0a\uff1a\u53c2\u8003\u6846\u67b6\uff08\u70b9\u5bf9\u70b9vs\u5bf9\u6bd4\uff09\u548c\u7c92\u5ea6\uff08\u7ec6\u7c92\u5ea6\u7c7b\u7ea7vs\u7c97\u7c92\u5ea6\u7ec4\u7ea7\uff09\u3002\u63d0\u51fa\u4e86\u56db\u4e2a\u65b0\u7684\u5fe0\u5b9e\u6027\u6307\u6807\u6765\u7cfb\u7edf\u8bc4\u4f30\u89e3\u91ca\u8d28\u91cf\u3002", "result": "\u901a\u8fc7RFxG\u89c6\u89d2\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u6307\u6807\u8fc7\u5ea6\u4f18\u5148\u8003\u8651\u70b9\u5bf9\u70b9\u5fe0\u5b9e\u6027\u800c\u5ffd\u89c6\u5bf9\u6bd4\u63a8\u7406\u548c\u8bed\u4e49\u7c92\u5ea6\u3002\u5bf9\u5341\u4e2a\u6700\u5148\u8fdb\u7684\u663e\u8457\u6027\u65b9\u6cd5\u3001\u56db\u4e2a\u6a21\u578b\u67b6\u6784\u548c\u4e09\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7\u5021\u5bfc\u5411\u7528\u6237\u610f\u56fe\u9a71\u52a8\u7684\u8bc4\u4f30\u8f6c\u53d8\uff0c\u672c\u7814\u7a76\u4e3a\u5f00\u53d1\u4e0d\u4ec5\u5fe0\u5b9e\u4e8e\u5e95\u5c42\u6a21\u578b\u884c\u4e3a\uff0c\u800c\u4e14\u4e0e\u4eba\u7c7b\u7406\u89e3\u548c\u67e5\u8be2\u590d\u6742\u6027\u6709\u610f\u4e49\u7684\u89c6\u89c9\u89e3\u91ca\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u548c\u5b9e\u8df5\u5de5\u5177\u3002"}}
{"id": "2511.13049", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13049", "abs": "https://arxiv.org/abs/2511.13049", "authors": ["Antoine Ledent", "Mun Chong Soo", "Nong Minh Hieu"], "title": "Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information", "comment": "Accepted at AAAI 2026", "summary": "We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \\textit{share a common subspace}. We assume that a large amount $M$ of \\textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\\widetilde{O}\\left(\\sqrt{\\frac{nd}{M}}\\right)$ and $\\widetilde{O}\\left(\\sqrt{\\frac{dr}{N}}\\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u77e9\u9635\u8865\u5168\u95ee\u9898\uff0c\u5176\u4e2d\u771f\u5b9e\u77e9\u9635R\u548c\u672a\u77e5\u91c7\u6837\u5206\u5e03P\u90fd\u662f\u4f4e\u79e9\u77e9\u9635\u4e14\u5171\u4eab\u5171\u540c\u5b50\u7a7a\u95f4\u3002\u5229\u7528\u5927\u91cf\u65e0\u6807\u7b7e\u6570\u636e\uff08\u6765\u81eaP\uff09\u548c\u5c11\u91cf\u5e26\u6807\u7b7e\u6570\u636e\uff0c\u7ed3\u5408\u4f4e\u79e9\u5b50\u7a7a\u95f4\u6062\u590d\u7406\u8bba\u548c\u77e9\u9635\u8865\u5168\u6cdb\u5316\u8fb9\u754c\uff0c\u63d0\u51fa\u4e86\u8bef\u5dee\u754c\u9650\u5206\u6790\u3002", "motivation": "\u53d7\u63a8\u8350\u7cfb\u7edf\u573a\u666f\u542f\u53d1\uff0c\u65e0\u6807\u7b7e\u6570\u636e\u5bf9\u5e94'\u9690\u5f0f\u53cd\u9988'\uff08\u5982\u8d2d\u4e70\u3001\u70b9\u51fb\u7b49\uff09\uff0c\u5e26\u6807\u7b7e\u6570\u636e\u5bf9\u5e94'\u663e\u5f0f\u53cd\u9988'\uff08\u7528\u6237\u660e\u786e\u8bc4\u5206\uff09\u3002\u7814\u7a76\u5982\u4f55\u6709\u6548\u7ed3\u5408\u8fd9\u4e24\u79cd\u53cd\u9988\u7c7b\u578b\u6765\u6539\u8fdb\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5229\u7528\u4f4e\u79e9\u5b50\u7a7a\u95f4\u6062\u590d\u7406\u8bba\u548c\u77e9\u9635\u8865\u5168\u6a21\u578b\u7684\u7ecf\u5178\u6cdb\u5316\u8fb9\u754c\uff0c\u63a8\u5bfc\u51fa\u7531\u4e24\u4e2a\u8bef\u5dee\u9879\u7ec4\u6210\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5206\u522b\u4e0e\u65e0\u6807\u7b7e\u6570\u636e\u548c\u5e26\u6807\u7b7e\u6570\u636e\u7684\u6570\u91cf\u76f8\u5173\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u8bef\u5dee\u754c\u9650\u5305\u542b\u4e24\u4e2a\u9879\uff1aO(\u221a(nd/M))\u548cO(\u221a(dr/N))\uff0c\u5176\u4e2dd\u662fP\u7684\u79e9\uff0cr\u662fR\u7684\u79e9\u3002\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6cdb\u5316\u8bef\u5dee\u81ea\u7136\u5206\u89e3\u4e3aP\u548cR\u7684\u4f30\u8ba1\u8bef\u5dee\u3002\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u663e\u5f0f\u8bc4\u5206\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5206\u6790\u63a8\u8350\u7cfb\u7edf\u4e2d\u663e\u5f0f\u548c\u9690\u5f0f\u53cd\u9988\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u7ed3\u5408\u4e24\u79cd\u53cd\u9988\u7c7b\u578b\u53ef\u4ee5\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.13052", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13052", "abs": "https://arxiv.org/abs/2511.13052", "authors": ["Yunhun Nam", "Jaehyung Kim", "Jongheon Jeong"], "title": "Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting", "comment": "17 pages; AAAI 2026; Code is available at https://github.com/yunpal/LfU", "summary": "Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to \"undesirable\" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.", "AI": {"tldr": "\u63d0\u51fa\u4e86LfU\u65b9\u6cd5\uff0c\u4e00\u79cd\u9488\u5bf9\u76d1\u7763\u5fae\u8c03\u7684\u7b80\u5355\u6709\u6548\u6b63\u5219\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u62b5\u6297\u4e0d\u826f\u6a21\u578b\u66f4\u65b0\u6765\u7f13\u89e3\u6709\u9650\u6570\u636e\u4e0b\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u5728\u6709\u9650\u6570\u636e\u4e0b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u65f6\uff0c\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u4f9d\u8d56\u865a\u5047\u6a21\u5f0f\u6216\u635f\u5bb3\u5176\u4ed6\u6709\u7528\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5b66\u4e60\u81ea\u4e0d\u826f\u65b9\u6cd5(LfU)\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u6b63\u5219\u5316\u76f4\u63a5\u5bf9\u9f50\u6a21\u578b\u5185\u90e8\u8868\u793a\u4e0e\u4e0d\u826f\u66f4\u65b0\u540e\u7684\u8868\u793a\uff0c\u5229\u7528\u8868\u793a\u7ea7\u6570\u636e\u589e\u5f3a\u4fc3\u8fdb\u6cdb\u5316\u3002", "result": "\u5728\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0cLfU\u76f8\u6bd4\u666e\u901aSFT\u5728\u6570\u5b66\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u534716.8%\uff0c\u4e14\u5bf9\u63d0\u793a\u53d8\u5316\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\uff0c\u8f93\u51fa\u6027\u80fd\u6807\u51c6\u5dee\u964d\u4f4e92.1%\u3002", "conclusion": "LfU\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u5148\u9a8c\uff0c\u5728\u6709\u9650\u6570\u636e\u4e0b\u65e2\u80fd\u589e\u5f3a\u9002\u5e94\u6027\u53c8\u80fd\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2511.13105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13105", "abs": "https://arxiv.org/abs/2511.13105", "authors": ["Seungjae Kim", "SeungJoon Lee", "MyeongAh Cho"], "title": "PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking", "comment": "AAAI 2026. Code: https://github.com/VisualScienceLab-KHU/PlugTrack", "summary": "Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.", "AI": {"tldr": "PlugTrack\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548c\u6570\u636e\u9a71\u52a8\u7684\u8fd0\u52a8\u9884\u6d4b\u5668\u6765\u89e3\u51b3\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u5f0f\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6570\u636e\u9a71\u52a8\u7684\u8fd0\u52a8\u9884\u6d4b\u5668\u867d\u7136\u80fd\u6355\u6349\u590d\u6742\u975e\u7ebf\u6027\u52a8\u6001\uff0c\u4f46\u5b58\u5728\u9886\u57df\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\u73b0\u5b9e\u8ddf\u8e2a\u573a\u666f\u540c\u65f6\u5305\u542b\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u5f0f\u3002", "method": "\u63d0\u51faPlugTrack\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u611f\u77e5\u8fd0\u52a8\u7406\u89e3\u81ea\u9002\u5e94\u878d\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548c\u6570\u636e\u9a71\u52a8\u8fd0\u52a8\u9884\u6d4b\u5668\uff0c\u4f7f\u7528\u591a\u611f\u77e5\u8fd0\u52a8\u5206\u6790\u751f\u6210\u81ea\u9002\u5e94\u6df7\u5408\u56e0\u5b50\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u8fd0\u52a8\u9884\u6d4b\u5668\u3002", "result": "\u5728MOT17/MOT20\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5728DanceTrack\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u975e\u7ebf\u6027\u8fd0\u52a8\u4e3b\u5bfc\u7684\u6570\u636e\u96c6\u4e2d\u4ecd\u80fd\u572834%\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u5668\u3002", "conclusion": "PlugTrack\u662f\u9996\u4e2a\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u6865\u63a5\u7ecf\u5178\u4e0e\u73b0\u4ee3\u8fd0\u52a8\u9884\u6d4b\u8303\u5f0f\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u6709\u6548\u5229\u7528\u4e86\u4e0d\u540c\u9884\u6d4b\u5668\u7684\u4e92\u8865\u4f18\u52bf\u3002"}}
{"id": "2511.13060", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13060", "abs": "https://arxiv.org/abs/2511.13060", "authors": ["Duo Yi"], "title": "Latency and Ordering Effects in Online Decisions", "comment": null, "summary": "Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_\u03a6$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \\ge L_{\\mathrm{ideal}} + g_1(\u03bb) + g_2(\\varepsilon_\\star) + g_{12}(\u03bb,\\varepsilon_\\star) - D_{\\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\\mathrm{ncx}}\\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5ef6\u8fdf\u53cd\u9988\u548c\u987a\u5e8f\u654f\u611f\u52a8\u6001\u7684\u5728\u7ebf\u51b3\u7b56\u7cfb\u7edf\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7Bregman\u6563\u5ea6\u4f5c\u4e3a\u635f\u5931\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u8d85\u989d\u57fa\u51c6\u635f\u5931\u7684\u7ed3\u6784\u5316\u4e0b\u754c\uff0c\u5305\u542b\u5ef6\u8fdf\u60e9\u7f5a\u3001\u987a\u5e8f\u654f\u611f\u6027\u60e9\u7f5a\u53ca\u5176\u51e0\u4f55\u4ea4\u4e92\u9879\uff0c\u5e76\u6269\u5c55\u5230\u975e\u51f8\u8bbe\u7f6e\u3002", "motivation": "\u5728\u7ebf\u51b3\u7b56\u7cfb\u7edf\u7ecf\u5e38\u5728\u5ef6\u8fdf\u53cd\u9988\u548c\u987a\u5e8f\u654f\u611f\u52a8\u6001\u4e0b\u8fd0\u884c\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u6548\u5e94\uff0c\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u91cf\u5316\u8fd9\u4e9b\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Bregman\u6563\u5ea6\u4f5c\u4e3a\u635f\u5931\u57fa\u51c6\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u4e0b\u754c\u7406\u8bba\uff0c\u5305\u542b\u5ef6\u8fdf\u60e9\u7f5a\u3001\u987a\u5e8f\u654f\u611f\u6027\u60e9\u7f5a\u53ca\u5176\u4ea4\u4e92\u9879\uff0c\u5e76\u6269\u5c55\u5230\u8fd1\u4f3c\u6b63\u5219\u548c\u5f31\u51f8\u8bbe\u7f6e\u3002", "result": "\u8bc1\u660e\u4e86\u8d85\u989d\u57fa\u51c6\u635f\u5931\u7684\u4e0b\u754c\u7ed3\u6784\uff0c\u63d0\u4f9b\u4e86\u901a\u8fc72\u00d72\u968f\u673a\u5316\u5b9e\u9a8c\u548c\u6d41\u5f0f\u8bca\u65ad\u6765\u4f30\u8ba1\u548c\u76d1\u63a7\u5404\u9879\u53c2\u6570\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u5f02\u6784\u5ef6\u8fdf\u3001\u975e\u4ea4\u6362\u6027\u548c\u5b9e\u73b0\u5dee\u8ddd\u6548\u5e94\u6253\u5305\u6210\u5355\u4e00\u53ef\u89e3\u91ca\u7684\u4e0b\u754c\u9648\u8ff0\uff0c\u53ef\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u548c\u8c03\u4f18\u3002"}}
{"id": "2511.13106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13106", "abs": "https://arxiv.org/abs/2511.13106", "authors": ["Fengzhi Xu", "Ziyuan Yang", "Mengyu Sun", "Joey Tianyi Zhou", "Yi Zhang"], "title": "Low-Level Dataset Distillation for Medical Image Enhancement", "comment": null, "summary": "Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u7684\u4f4e\u7ea7\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u89e3\u5256\u5148\u9a8c\u548c\u7ed3\u6784\u4fdd\u6301\u4e2a\u6027\u5316\u751f\u6210\u6a21\u5757\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5b66\u4e60\u590d\u6742\u50cf\u7d20\u7ea7\u6620\u5c04\uff0c\u4f46\u8bad\u7ec3\u548c\u5b58\u50a8\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9ad8\u7ea7\u4efb\u52a1\uff0c\u800c\u4f4e\u7ea7\u4efb\u52a1\u7684\u50cf\u7d20\u7ea7\u4fdd\u771f\u5ea6\u8981\u6c42\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u4e0d\u9002\u7528\u3002", "method": "\u5229\u7528\u60a3\u8005\u95f4\u7684\u89e3\u5256\u76f8\u4f3c\u6027\u6784\u5efa\u5171\u4eab\u89e3\u5256\u5148\u9a8c\u4f5c\u4e3a\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u7ed3\u6784\u4fdd\u6301\u4e2a\u6027\u5316\u751f\u6210\u6a21\u5757\u5c06\u60a3\u8005\u7279\u5b9a\u89e3\u5256\u4fe1\u606f\u6574\u5408\u5230\u84b8\u998f\u6570\u636e\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u50cf\u7d20\u7ea7\u4fdd\u771f\u5ea6\u3002\u901a\u8fc7\u68af\u5ea6\u5bf9\u9f50\u5c06\u60a3\u8005\u7279\u5b9a\u77e5\u8bc6\u6ce8\u5165\u84b8\u998f\u6570\u636e\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u5305\u542b\u62bd\u8c61\u8bad\u7ec3\u4fe1\u606f\u7684\u84b8\u998f\u6570\u636e\u96c6\uff0c\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u60a3\u8005\u6570\u636e\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u9ad8\u6548\u6570\u636e\u84b8\u998f\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f4e\u7ea7\u4efb\u52a1\u4e2d\u50cf\u7d20\u7ea7\u4fdd\u771f\u5ea6\u8981\u6c42\u7684\u6311\u6218\u3002"}}
{"id": "2511.13061", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.13061", "abs": "https://arxiv.org/abs/2511.13061", "authors": ["Vladim\u00edr Macko", "Vladim\u00edr Bo\u017ea"], "title": "MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity", "comment": "8 pages + 7 pages appendix, 11 figures, Code available at https://github.com/vlejd/macko_spmv", "summary": "Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.", "AI": {"tldr": "MACKO-SpMV\u662f\u4e00\u79cdGPU\u4f18\u5316\u7684\u7a00\u758f\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u683c\u5f0f\u548c\u5185\u6838\uff0c\u4e13\u95e8\u9488\u5bf9\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\u8bbe\u8ba1\uff0c\u572850%\u7a00\u758f\u5ea6\u4e0b\u5b9e\u73b01.5\u500d\u5185\u5b58\u51cf\u5c11\u548c1.2-1.5\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709SpMV\u65b9\u6cd5\u5728\u7a00\u758fLLMs\u4e2d\u5e38\u89c1\u7684\u4f4e\u4e14\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0830-90%\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u975e\u7ed3\u6784\u5316\u526a\u679d\u53ea\u80fd\u63d0\u4f9b\u6709\u9650\u7684\u5185\u5b58\u51cf\u5c11\u548c\u52a0\u901f\u6548\u679c\u3002", "method": "\u63d0\u51faMACKO-SpMV\uff0c\u8fd9\u662f\u4e00\u79cdGPU\u4f18\u5316\u7684\u683c\u5f0f\u548c\u5185\u6838\u534f\u540c\u8bbe\u8ba1\uff0c\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u4e0eGPU\u6267\u884c\u6a21\u578b\u7684\u517c\u5bb9\u6027\uff0c\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u5355\u5143\u6216\u683c\u5f0f\u7279\u5b9a\u7684\u9884\u8ba1\u7b97\u3002", "result": "\u572850%\u7a00\u758f\u5ea6\u4e0b\uff0cMACKO\u662f\u9996\u4e2a\u5b9e\u73b0\u663e\u84571.5\u500d\u5185\u5b58\u51cf\u5c11\u548c1.2-1.5\u500d\u52a0\u901f\u7684\u65b9\u6cd5\uff1b\u76f8\u6bd4\u5176\u4ed6SpMV\u57fa\u7ebf\uff1a\u6bd4cuSPARSE\u5feb2.8-13.0\u500d\uff0c\u6bd4Sputnik\u5feb1.9-2.6\u500d\uff0c\u6bd4DASP\u5feb2.2-2.5\u500d\uff1b\u5e94\u7528\u4e8eLlama2-7B\u6a21\u578b\uff0c\u5728fp16\u7cbe\u5ea6\u4e0b\u5b9e\u73b01.5\u500d\u5185\u5b58\u51cf\u5c11\u548c1.5\u500d\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "\u5f97\u76ca\u4e8eMACKO\uff0c50%\u7a00\u758f\u5ea6\u7684\u975e\u7ed3\u6784\u5316\u526a\u679d\u73b0\u5728\u53ef\u4ee5\u5728\u5b9e\u9645LLM\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u5408\u7406\u5e94\u7528\u3002"}}
{"id": "2511.13062", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13062", "abs": "https://arxiv.org/abs/2511.13062", "authors": ["Mohit Meena", "Yash Punjabi", "Abhishek A", "Vishal Sharma", "Mahesh Chandran"], "title": "Self-Adaptive Graph Mixture of Models", "comment": "17 pages, 5 figures", "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86SAGMM\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u548c\u7ec4\u5408\u591a\u79cdGNN\u67b6\u6784\u6765\u89e3\u51b3\u6a21\u578b\u9009\u62e9\u56f0\u96be\u95ee\u9898\uff0c\u572816\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dGNN\u6027\u80fd\u589e\u957f\u8d8b\u4e8e\u5e73\u7f13\uff0c\u590d\u6742\u6a21\u578b\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff0c\u4e14\u4e3a\u7279\u5b9a\u56fe\u4efb\u52a1\u9009\u62e9\u5408\u9002\u6a21\u578b\u5b58\u5728\u56f0\u96be\u3002", "method": "SAGMM\u91c7\u7528\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5229\u7528\u62d3\u6251\u611f\u77e5\u6ce8\u610f\u529b\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u5206\u914d\u4e13\u5bb6\u6a21\u578b\uff0c\u5305\u542b\u526a\u679d\u673a\u5236\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u63a2\u7d22\u9884\u8bad\u7ec3\u4e13\u5bb6\u6a21\u578b\u7684\u9ad8\u6548\u53d8\u4f53\u3002", "result": "\u5728\u8282\u70b9\u5206\u7c7b\u3001\u56fe\u5206\u7c7b\u3001\u56de\u5f52\u548c\u94fe\u63a5\u9884\u6d4b\u7b4916\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cSAGMM\u59cb\u7ec8\u4f18\u4e8e\u6216\u5339\u914d\u9886\u5148\u7684GNN\u57fa\u7ebf\u548c\u73b0\u6709\u6df7\u5408\u65b9\u6cd5\u3002", "conclusion": "SAGMM\u4e3a\u73b0\u5b9e\u4e16\u754c\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u52a8\u9009\u62e9\u6700\u9002\u5408\u7684GNN\u6a21\u578b\u7ec4\u5408\u3002"}}
{"id": "2511.13110", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13110", "abs": "https://arxiv.org/abs/2511.13110", "authors": ["Shuaibin Fan", "Senming Zhong", "Wenchao Yan", "Minglong Xue"], "title": "Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing", "comment": null, "summary": "Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u9000\u5316\u8868\u793a\u7684\u65e0\u76d1\u7763\u53bb\u96fe\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u9053\u72ec\u7acb\u548c\u901a\u9053\u4f9d\u8d56\u673a\u5236\u6765\u589e\u5f3a\u975e\u7ebf\u6027\u4f9d\u8d56\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5c06\u96fe\u973e\u9000\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u51fd\u6570\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u573a\u666f\u65f6\u96be\u4ee5\u5e73\u8861\u975e\u5747\u5300\u96fe\u973e\u5206\u5e03\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u8868\u793a\u4e0e\u5168\u5c40\u4e00\u81f4\u6027\u5efa\u6a21\uff0c\u4e14\u9700\u8981\u66f4\u597d\u5730\u5b66\u4e60\u96fe\u973e\u5728\u7a7a\u95f4\u53d8\u5316\u4e2d\u7684\u5171\u540c\u9000\u5316\u8868\u793a\u3002", "method": "1. \u57fa\u4e8eKolmogorov-Arnold\u8868\u793a\u5b9a\u7406\uff0c\u63d0\u51fa\u901a\u9053\u72ec\u7acb\u4e0e\u901a\u9053\u4f9d\u8d56\u673a\u5236\u7ed3\u5408\u7684\u65b9\u6cd5\uff1b2. \u8bbe\u8ba1\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5c06\u96fe\u973e\u9000\u5316\u5efa\u6a21\u4e3a\u8fde\u7eed\u51fd\u6570\uff1b3. \u8bbe\u8ba1\u5bc6\u96c6\u6b8b\u5dee\u589e\u5f3a\u6a21\u5757\u6765\u5b66\u4e60\u96fe\u973e\u7279\u5f81\u7684\u9690\u5f0f\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u516c\u5171\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u53bb\u96fe\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9690\u5f0f\u795e\u7ecf\u9000\u5316\u8868\u793a\u548c\u5bc6\u96c6\u6b8b\u5dee\u589e\u5f3a\u6a21\u5757\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u56fe\u50cf\u53bb\u96fe\uff0c\u5177\u6709\u826f\u597d\u7684\u89c6\u89c9\u611f\u77e5\u6548\u679c\u3002"}}
{"id": "2511.13078", "categories": ["cs.LG", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.13078", "abs": "https://arxiv.org/abs/2511.13078", "authors": ["Liuyi Jin", "Pasan Gunawardena", "Amran Haroon", "Runzhi Wang", "Sangwoo Lee", "Radu Stoleru", "Michael Middleton", "Zepeng Huo", "Jeeeun Kim", "Jason Moats"], "title": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning", "comment": null, "summary": "Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.", "AI": {"tldr": "EMSGlass\u662f\u4e00\u4e2a\u57fa\u4e8eEMSNet\u548cEMSServe\u7684\u667a\u80fd\u773c\u955c\u7cfb\u7edf\uff0c\u4e13\u4e3a\u6025\u6551\u533b\u7597\u670d\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u6210\u63d0\u5347\u6025\u6551\u4eba\u5458\u7684\u5b9e\u65f6\u60c5\u5883\u611f\u77e5\u548c\u51b3\u7b56\u6548\u7387\u3002", "motivation": "\u6025\u6551\u533b\u7597\u6280\u672f\u4eba\u5458\u5728\u9ad8\u538b\u73af\u5883\u4e0b\u5de5\u4f5c\uff0c\u9700\u8981\u5728\u91cd\u8ba4\u77e5\u8d1f\u8377\u4e0b\u5feb\u901f\u505a\u51fa\u5173\u952e\u51b3\u7b56\uff0c\u73b0\u6709\u7cfb\u7edf\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u6a21\u6001\u5b9e\u65f6\u652f\u6301\u3002", "method": "\u5f00\u53d1EMSNet\u591a\u6a21\u6001\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u96c6\u6210\u6587\u672c\u3001\u751f\u547d\u4f53\u5f81\u548c\u573a\u666f\u56fe\u50cf\u6570\u636e\uff1b\u6784\u5efaEMSServe\u4f4e\u5ef6\u8fdf\u670d\u52a1\u6846\u67b6\uff0c\u5305\u542b\u6a21\u6001\u611f\u77e5\u6a21\u578b\u5206\u5272\u5668\u548c\u7279\u5f81\u7f13\u5b58\u673a\u5236\u3002", "result": "EMSNet\u57285\u4e2a\u5173\u952eEMS\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf\uff1bEMSServe\u6bd4\u76f4\u63a5PyTorch\u591a\u6a21\u6001\u63a8\u7406\u5feb1.9-11.7\u500d\uff1b\u7528\u6237\u7814\u7a76\u663e\u793aEMSGlass\u663e\u8457\u63d0\u5347\u60c5\u5883\u611f\u77e5\u548c\u51b3\u7b56\u6548\u7387\u3002", "conclusion": "EMSGlass\u6210\u529f\u5c06\u591a\u6a21\u6001\u667a\u80fd\u4e0e\u73b0\u5b9e\u4e16\u754c\u6025\u6551\u54cd\u5e94\u5de5\u4f5c\u6d41\u7a0b\u76f8\u7ed3\u5408\uff0c\u4e3a\u4e0b\u4e00\u4ee3AI\u652f\u6301\u7684EMS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2511.13121", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13121", "abs": "https://arxiv.org/abs/2511.13121", "authors": ["Yuqi Zhang", "Guanying Chen", "Jiaxing Chen", "Chuanyu Fu", "Chuan Huang", "Shuguang Cui"], "title": "CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model", "comment": "Project Link: https://zyqz97.github.io/CloseUpShot/", "summary": "Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.", "AI": {"tldr": "CloseUpShot\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u8f93\u5165\u89c6\u56fe\u5408\u6210\u8fd1\u8ddd\u79bb\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u70b9\u6761\u4ef6\u89c6\u9891\u6269\u6563\u89e3\u51b3\u8fd1\u8ddd\u79bb\u573a\u666f\u4e2d\u7ec6\u8282\u91cd\u5efa\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9002\u5ea6\u7684\u89c6\u89d2\u53d8\u5316\uff0c\u5728\u8fd1\u8ddd\u79bb\u573a\u666f\u4e2d\u7531\u4e8e\u8f93\u5165\u4fe1\u606f\u4e25\u91cd\u53d7\u9650\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u3002\u89c6\u9891\u6269\u6563\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u8fd1\u8ddd\u79bb\u8bbe\u7f6e\u4e0b\u50cf\u7d20\u626d\u66f2\u6761\u4ef6\u5b58\u5728\u4e25\u91cd\u7a00\u758f\u6027\u548c\u80cc\u666f\u6cc4\u6f0f\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u5c42\u626d\u66f2\u548c\u906e\u6321\u611f\u77e5\u566a\u58f0\u6291\u5236\uff0c\u63d0\u9ad8\u89c6\u9891\u6269\u6563\u6a21\u578b\u6761\u4ef6\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u5b8c\u6574\u6027\uff1b\u5f15\u5165\u5168\u5c40\u7ed3\u6784\u6307\u5bfc\uff0c\u5229\u7528\u5bc6\u96c6\u878d\u5408\u70b9\u4e91\u4e3a\u6269\u6563\u8fc7\u7a0b\u63d0\u4f9b\u4e00\u81f4\u7684\u51e0\u4f55\u4e0a\u4e0b\u6587\uff0c\u8865\u507f\u7a00\u758f\u6761\u4ef6\u8f93\u5165\u4e2d\u7f3a\u4e4f\u5168\u5c40\u4e00\u81f43D\u7ea6\u675f\u7684\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd1\u8ddd\u79bb\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "CloseUpShot\u901a\u8fc7\u6539\u8fdb\u7684\u6761\u4ef6\u673a\u5236\u548c\u5168\u5c40\u51e0\u4f55\u7ea6\u675f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u8f93\u5165\u4e0b\u8fd1\u8ddd\u79bb\u65b0\u89c6\u89d2\u5408\u6210\u7684\u6311\u6218\uff0c\u5728\u7ec6\u8282\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2511.13124", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2511.13124", "abs": "https://arxiv.org/abs/2511.13124", "authors": ["Changxi Chi", "Yufei Huang", "Jun Xia", "Jiangbin Zheng", "Yunfan Liu", "Zelin Zang", "Stan Z. Li"], "title": "Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schr\u00f6dinger Bridges", "comment": null, "summary": "Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schr\u00f6dinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u859b\u5b9a\u8c14\u6865\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u76f4\u63a5\u5bf9\u9f50\u63a7\u5236\u7ec4\u548c\u6270\u52a8\u7ec4\u7684\u5355\u7ec6\u80de\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u5355\u7ec6\u80de\u6270\u52a8\u6570\u636e\u65e0\u914d\u5bf9\u7684\u95ee\u9898\uff0c\u5e76\u5728\u9057\u4f20\u548c\u836f\u7269\u6270\u52a8\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5355\u7ec6\u80de\u6270\u52a8\u9884\u6d4b\u662f\u57fa\u56e0\u529f\u80fd\u5206\u6790\u548c\u836f\u7269\u5019\u9009\u9009\u62e9\u7684\u5173\u952e\uff0c\u4f46\u7531\u4e8e\u6d4b\u5e8f\u7684\u7834\u574f\u6027\uff0c\u5355\u7ec6\u80de\u6570\u636e\u65e0\u6cd5\u914d\u5bf9\u89c2\u6d4b\u3002\u73b0\u6709\u795e\u7ecf\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u6761\u4ef6\u5316\u6216\u4f9d\u8d56\u5148\u9a8c\u7a7a\u95f4\u8fdb\u884c\u95f4\u63a5\u5206\u5e03\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u7cbe\u786e\u7684\u6270\u52a8\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u8fd1\u4f3c\u859b\u5b9a\u8c14\u6865\u6765\u76f4\u63a5\u5bf9\u9f50\u4e0d\u540c\u6270\u52a8\u6761\u4ef6\u4e0b\u63a7\u5236\u7ec4\u548c\u6270\u52a8\u7ec4\u7684\u5355\u7ec6\u80de\u5206\u5e03\uff0c\u5229\u7528Minibatch-OT\u914d\u5bf9\u907f\u514d\u53cc\u5411\u63a8\u7406\uff0c\u76f4\u63a5\u6307\u5bfc\u6865\u5b66\u4e60\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684SB\u8fd1\u4f3c\u3002\u6784\u5efa\u4e86\u4e24\u4e2aSB\u6a21\u578b\uff1a\u4e00\u4e2a\u5efa\u6a21\u79bb\u6563\u57fa\u56e0\u6fc0\u6d3b\u72b6\u6001\uff0c\u53e6\u4e00\u4e2a\u5efa\u6a21\u8fde\u7eed\u8868\u8fbe\u5206\u5e03\u3002", "result": "\u5728\u516c\u5171\u9057\u4f20\u548c\u836f\u7269\u6270\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u6709\u6548\u6355\u6349\u4e86\u5f02\u8d28\u6027\u5355\u7ec6\u80de\u54cd\u5e94\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u5206\u5e03\u5bf9\u9f50\u548c\u8054\u5408\u8bad\u7ec3\uff0c\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u5355\u7ec6\u80de\u6270\u52a8\u5e76\u6355\u6349\u7ec6\u80de\u5f02\u8d28\u6027\uff0c\u4e3a\u5355\u7ec6\u80de\u6270\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13127", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.13127", "abs": "https://arxiv.org/abs/2511.13127", "authors": ["Zonghao Ying", "Moyang Chen", "Nizhang Li", "Zhiqiang Wang", "Wenxin Zhang", "Quanchen Zou", "Zonglei Jing", "Aishan Liu", "Xianglong Liu"], "title": "VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language", "comment": null, "summary": "Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.", "AI": {"tldr": "VEIL\u6846\u67b6\u901a\u8fc7\u4e2d\u6027\u573a\u666f\u951a\u70b9\u3001\u6f5c\u5728\u542c\u89c9\u89e6\u53d1\u5668\u548c\u98ce\u683c\u8c03\u5236\u5668\uff0c\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u8de8\u6a21\u6001\u5173\u8054\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9690\u853d\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u4f7f\u6a21\u578b\u751f\u6210\u8fdd\u53cd\u5b89\u5168\u7b56\u7565\u4f46\u4fdd\u7559\u539f\u59cb\u610f\u56fe\u7684\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\u901a\u5e38\u901a\u8fc7\u6dfb\u52a0\u660e\u663e\u4e0d\u5b89\u5168\u7684\u5bf9\u6297\u6027\u6270\u52a8\u6765\u5b9e\u73b0\uff0c\u5bb9\u6613\u88ab\u68c0\u6d4b\u548c\u9632\u5fa1\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u770b\u4f3c\u826f\u6027\u7684\u63d0\u793a\u5982\u4f55\u901a\u8fc7\u4e30\u5bcc\u7684\u9690\u542b\u7ebf\u7d22\u8bf1\u5bfc\u6a21\u578b\u751f\u6210\u8bed\u4e49\u4e0d\u5b89\u5168\u7684\u89c6\u9891\u3002", "method": "\u63d0\u51faVEIL\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u63d0\u793a\u8bbe\u8ba1\uff1a\u4e2d\u6027\u573a\u666f\u951a\u70b9\u63d0\u4f9b\u8868\u9762\u573a\u666f\u63cf\u8ff0\uff1b\u6f5c\u5728\u542c\u89c9\u89e6\u53d1\u5668\u5229\u7528\u97f3\u9891-\u89c6\u89c9\u5171\u73b0\u5148\u9a8c\uff1b\u98ce\u683c\u8c03\u5236\u5668\u901a\u8fc7\u7535\u5f71\u6307\u4ee4\u589e\u5f3a\u89e6\u53d1\u6548\u679c\u3002\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u548c\u5f15\u5bfc\u641c\u7d22\u5e73\u8861\u9690\u853d\u6027\u548c\u6709\u6548\u6027\u3002", "result": "\u57287\u4e2a\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u653b\u51fb\u65b9\u6cd5\u6709\u6548\uff0c\u5728\u5546\u4e1a\u6a21\u578b\u4e2d\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad8\u4e8623%\u3002", "conclusion": "VEIL\u6846\u67b6\u63ed\u793a\u4e86\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u5b89\u5168\u76f2\u70b9\uff0c\u8868\u660e\u770b\u4f3c\u826f\u6027\u7684\u63d0\u793a\u53ef\u4ee5\u901a\u8fc7\u8de8\u6a21\u6001\u5173\u8054\u6a21\u5f0f\u6210\u529f\u5b9e\u73b0\u8d8a\u72f1\u653b\u51fb\uff0c\u5bf9\u6a21\u578b\u5b89\u5168\u9632\u62a4\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\u3002"}}
{"id": "2511.13133", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13133", "abs": "https://arxiv.org/abs/2511.13133", "authors": ["Shudong Wang", "Xinfei Wang", "Chenhao Zhang", "Shanchen Pang", "Haiyuan Gui", "Wenhao Ji", "Xiaojian Liao"], "title": "Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning", "comment": null, "summary": "Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.\n  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.", "AI": {"tldr": "SoCo-DT\u662f\u4e00\u79cd\u57fa\u4e8e\u53c2\u6570\u91cd\u8981\u6027\u7684\u8f6f\u51b2\u7a81\u89e3\u51b3\u65b9\u6cd5\uff0c\u901a\u8fc7Fisher\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u63a9\u7801\u503c\uff0c\u4fdd\u7559\u91cd\u8981\u53c2\u6570\u5e76\u6291\u5236\u51b2\u7a81\u53c2\u6570\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u56db\u5206\u4f4d\u8ddd\u7684\u52a8\u6001\u7a00\u758f\u5ea6\u8c03\u6574\u7b56\u7565\uff0c\u5728Meta-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b58\u5728\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u63a9\u7801\u7684\u65b9\u6cd5\u4f7f\u7528\u7c97\u7c92\u5ea6\u4e8c\u5143\u63a9\u7801\u4f1a\u8fc7\u5ea6\u6291\u5236\u5173\u952e\u51b2\u7a81\u53c2\u6570\uff0c\u963b\u788d\u4efb\u52a1\u95f4\u77e5\u8bc6\u5171\u4eab\uff0c\u4e14\u4e0d\u540c\u4efb\u52a1\u51b2\u7a81\u7a0b\u5ea6\u4e0d\u540c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7a00\u758f\u5ea6\u7b56\u7565\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51faSoCo-DT\u65b9\u6cd5\uff1a1\uff09\u5229\u7528Fisher\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u63a9\u7801\u503c\uff0c\u4fdd\u7559\u91cd\u8981\u53c2\u6570\u540c\u65f6\u6291\u5236\u51b2\u7a81\u53c2\u6570\uff1b2\uff09\u57fa\u4e8e\u56db\u5206\u4f4d\u8ddd\u5f15\u5165\u52a8\u6001\u7a00\u758f\u5ea6\u8c03\u6574\u7b56\u7565\uff0c\u6839\u636e\u51b2\u7a81\u4e0e\u548c\u8c10\u5206\u6570\u7684\u5206\u5e03\u6784\u5efa\u4efb\u52a1\u7279\u5b9a\u7684\u9608\u503c\u65b9\u6848\uff1b3\uff09\u91c7\u7528\u975e\u5bf9\u79f0\u4f59\u5f26\u9000\u706b\u8c03\u5ea6\u6301\u7eed\u66f4\u65b0\u9608\u503c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u7a00\u758f\u5ea6\u6f14\u5316\u3002", "result": "\u5728Meta-World\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSoCo-DT\u5728MT50\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd57.6%\uff0c\u5728\u6b21\u4f18\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e10.5%\uff0c\u8bc1\u660e\u5176\u5728\u7f13\u89e3\u68af\u5ea6\u51b2\u7a81\u548c\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SoCo-DT\u901a\u8fc7\u8f6f\u51b2\u7a81\u89e3\u51b3\u548c\u52a8\u6001\u7a00\u758f\u5ea6\u8c03\u6574\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13144", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13144", "abs": "https://arxiv.org/abs/2511.13144", "authors": ["Jiacheng Cheng", "Xu Zhang", "Guanghui Qiu", "Yifang Zhang", "Yinchuan Li", "Kaiyuan Feng"], "title": "Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching", "comment": "Accepted in AAAI 2026", "summary": "Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.", "AI": {"tldr": "pFed1BS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u6bd4\u7279\u968f\u673a\u8349\u56fe\u5b9e\u73b0\u6781\u7aef\u901a\u4fe1\u538b\u7f29\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u53cc\u5411\u901a\u4fe1\u5f00\u9500\u548c\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u53cc\u5411\u901a\u4fe1\u5f00\u9500\u548c\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u5728\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u3002", "method": "\u63d0\u51fapFed1BS\u6846\u67b6\uff0c\u5ba2\u6237\u7aef\u4f20\u8f93\u9ad8\u5ea6\u538b\u7f29\u7684\u4e00\u6bd4\u7279\u8349\u56fe\uff0c\u670d\u52a1\u5668\u805a\u5408\u5e76\u5e7f\u64ad\u5168\u5c40\u4e00\u6bd4\u7279\u5171\u8bc6\uff1b\u5f15\u5165\u57fa\u4e8e\u7b26\u53f7\u7684\u6b63\u5219\u5316\u5668\u6307\u5bfc\u672c\u5730\u6a21\u578b\u4e0e\u5168\u5c40\u5171\u8bc6\u5bf9\u9f50\uff1b\u4f7f\u7528\u5feb\u901f\u54c8\u8fbe\u739b\u53d8\u6362\u8fdb\u884c\u9ad8\u6548\u6295\u5f71\u3002", "result": "\u7406\u8bba\u5206\u6790\u4fdd\u8bc1\u7b97\u6cd5\u6536\u655b\u5230\u5168\u5c40\u52bf\u51fd\u6570\u7684\u7a33\u5b9a\u90bb\u57df\uff1b\u6570\u503c\u6a21\u62df\u663e\u793apFed1BS\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u540c\u65f6\u4e0e\u5148\u8fdb\u7684\u901a\u4fe1\u9ad8\u6548\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "pFed1BS\u901a\u8fc7\u4e00\u6bd4\u7279\u968f\u673a\u8349\u56fe\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2511.13135", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13135", "abs": "https://arxiv.org/abs/2511.13135", "authors": ["Junjie Yang", "Yuhao Yan", "Gang Wu", "Yuxuan Wang", "Ruoyu Liang", "Xinjie Jiang", "Xiang Wan", "Fenglei Fan", "Yongquan Zhang", "Feiwei Qin", "Changmiao Wan"], "title": "MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation", "comment": "CVPR 2026 Under Review", "summary": "As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \\textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.", "AI": {"tldr": "MedGEN-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u533b\u5b66\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u5305\u542b6,422\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u6db5\u76d66\u79cd\u6210\u50cf\u6a21\u6001\u300116\u4e2a\u4e34\u5e8a\u4efb\u52a1\u548c28\u4e2a\u5b50\u4efb\u52a1\uff0c\u652f\u6301\u89c6\u89c9\u95ee\u7b54\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u751f\u6210\u4e09\u79cd\u683c\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u4e34\u5e8a\u533b\u751f\u671f\u671bAI\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u751f\u6210\u6587\u672c\u8bca\u65ad\uff0c\u8fd8\u80fd\u751f\u6210\u4e0e\u771f\u5b9e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u65e0\u7f1d\u96c6\u6210\u7684\u533b\u5b66\u56fe\u50cf\u3002\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u57fa\u51c6\u5b58\u5728\u67e5\u8be2\u6a21\u7cca\u3001\u8bca\u65ad\u63a8\u7406\u7b80\u5316\u8fc7\u5ea6\u4ee5\u53ca\u5ffd\u89c6\u56fe\u50cf\u751f\u6210\u80fd\u529b\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86MedGEN-Bench\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u4e2a\u683c\u5f0f\uff1a\u89c6\u89c9\u95ee\u7b54\u3001\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u751f\u6210\u3002\u91c7\u7528\u4e09\u5c42\u8bc4\u4f30\u6846\u67b6\uff1a\u50cf\u7d20\u7ea7\u6307\u6807\u3001\u8bed\u4e49\u6587\u672c\u5206\u6790\u548c\u4e13\u5bb6\u6307\u5bfc\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u8bc4\u5206\uff0c\u8bc4\u4f30\u4e8610\u4e2a\u7ec4\u5408\u6846\u67b6\u30013\u4e2a\u7edf\u4e00\u6a21\u578b\u548c5\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5728\u533b\u5b66\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8bc6\u522b\u4e86\u9700\u8981\u6539\u8fdb\u7684\u9886\u57df\u3002", "conclusion": "MedGEN-Bench\u901a\u8fc7\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4ea4\u7ec7\u7684\u6307\u4ee4\u548c\u5f00\u653e\u5f0f\u751f\u6210\u8f93\u51fa\uff0c\u8d85\u8d8a\u4e86\u591a\u9879\u9009\u62e9\u683c\u5f0f\u7684\u9650\u5236\uff0c\u63a8\u52a8\u4e86\u533b\u5b66AI\u7814\u7a76\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5bf9\u9700\u8981\u590d\u6742\u8de8\u6a21\u6001\u63a8\u7406\u7684\u4efb\u52a1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.13147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13147", "abs": "https://arxiv.org/abs/2511.13147", "authors": ["Shaoyuan Chen", "Zhixuan Chen", "Dawei Yang", "Zhihang Yuan", "Qiang Wu"], "title": "OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs", "comment": null, "summary": "Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.", "AI": {"tldr": "OTARo\u662f\u4e00\u79cd\u65b0\u9896\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u4f7f\u8bbe\u5907\u7aefLLM\u80fd\u591f\u7075\u6d3b\u5207\u6362\u91cf\u5316\u7cbe\u5ea6\uff0c\u901a\u8fc7\u4e00\u6b21\u5fae\u8c03\u4fdd\u6301\u6027\u80fd\u9c81\u68d2\u6027\uff0c\u4f7f\u7528\u5171\u4eab\u6307\u6570\u6d6e\u70b9\u673a\u5236\u5b9e\u73b0\u4e0d\u540c\u4f4d\u5bbd\u7684\u7b80\u5355\u622a\u65ad\u3002", "motivation": "\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u7ed3\u6784\u9650\u5236\uff0c\u65e0\u6cd5\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u652f\u6301\u8bbe\u5907\u7aef\u7cbe\u5ea6\u5207\u6362\uff0c\u56e0\u4e3a\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u91cf\u5316\u7cbe\u5ea6\uff08\u5982\u7406\u89e3\u4efb\u52a1\u6bd4\u751f\u6210\u4efb\u52a1\u5bf9\u7cbe\u5ea6\u964d\u4f4e\u66f4\u5bb9\u5fcd\uff09\u3002", "method": "\u63d0\u51faOTARo\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165\u5171\u4eab\u6307\u6570\u6d6e\u70b9\u91cf\u5316\u673a\u5236\uff0c\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u7684\u5c3e\u6570\u622a\u65ad\u4ea7\u751f\u4e0d\u540c\u4f4d\u5bbd\uff1b2\uff09\u91c7\u7528\u5229\u7528-\u63a2\u7d22\u4f4d\u5bbd\u8def\u5f84\u641c\u7d22\u7b56\u7565\u8fed\u4ee3\u66f4\u65b0\u641c\u7d22\u8def\u5f84\uff1b3\uff09\u4f7f\u7528\u4f4e\u7cbe\u5ea6\u5f02\u6b65\u7d2f\u79ef\u8fdb\u884c\u5f02\u6b65\u68af\u5ea6\u7d2f\u79ef\u548c\u5ef6\u8fdf\u66f4\u65b0\u3002", "result": "\u5728LLaMA3.2-1B\u3001LLaMA3-8B\u7b49\u6d41\u884cLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOTARo\u5728\u6240\u6709\u7cbe\u5ea6\u4e0b\u90fd\u80fd\u5b9e\u73b0\u4e00\u81f4\u5f3a\u5927\u4e14\u9c81\u68d2\u7684\u6027\u80fd\u3002", "conclusion": "OTARo\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u5728\u8bbe\u5907\u7aef\u7cbe\u5ea6\u5207\u6362\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7075\u6d3b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13145", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13145", "abs": "https://arxiv.org/abs/2511.13145", "authors": ["Cesar Portocarrero Rodriguez", "Laura Vandeweyen", "Yosuke Yamamoto"], "title": "Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks", "comment": null, "summary": "The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u8fdb\u884c\u9053\u8def\u635f\u574f\u5206\u5272\uff0c\u8bc4\u4f30GAN\u751f\u6210\u5408\u6210\u6570\u636e\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u6709\u7528\u6027\uff0c\u6bd4\u8f83CNN\u548cMaskFormer\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0GAN\u6570\u636e\u80fd\u63d0\u5347\u6a21\u578b\u8868\u73b0\u4e14MaskFormer\u5728mAP50\u548cIoU\u6307\u6807\u4e0a\u4f18\u4e8eCNN\u3002", "motivation": "\u7f8e\u56fd\u57fa\u7840\u8bbe\u65bd\u72b6\u51b5\u4e0d\u4f73\uff0c\u9053\u8def\u7cfb\u7edf\u8bc4\u7ea7\u4e3aD\uff0c\u4f20\u7edf\u9053\u8def\u68c0\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u6210\u672c\u9ad8\u6602\u3002\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b9e\u65f6\u89c6\u89c9\u6570\u636e\u7684\u53ef\u7528\u6027\u589e\u52a0\uff0c\u6709\u673a\u4f1a\u5e94\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u8fdb\u884c\u5148\u8fdb\u9053\u8def\u76d1\u63a7\u3002", "method": "\u9996\u5148\u8bc4\u4f30\u4f7f\u7528GAN\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u6709\u7528\u6027\uff0c\u7136\u540e\u5e94\u7528CNN\u8fdb\u884c\u9053\u8def\u635f\u574f\u5206\u5272\uff0c\u6700\u540e\u7814\u7a76\u57fa\u4e8etransformer\u7684MaskFormer\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u663e\u793aGAN\u751f\u6210\u7684\u6570\u636e\u80fd\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0cMaskFormer\u5728mAP50\u548cIoU\u4e24\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8eCNN\u6a21\u578b\u3002", "conclusion": "\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u7279\u522b\u662fMaskFormer\u6a21\u578b\u7ed3\u5408GAN\u751f\u6210\u6570\u636e\uff0c\u4e3a\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u76d1\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6307\u5bfc\u57fa\u7840\u8bbe\u65bd\u4fee\u590d\u5de5\u4f5c\u3002"}}
{"id": "2511.13150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13150", "abs": "https://arxiv.org/abs/2511.13150", "authors": ["Rifen Lin", "Alex Jinpeng Wang", "Jiawei Mo", "Min Li"], "title": "Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification", "comment": null, "summary": "Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.", "AI": {"tldr": "CSIP-ReID\u662f\u4e00\u79cd\u57fa\u4e8e\u9aa8\u67b6\u9a71\u52a8\u7684\u89c6\u9891\u884c\u4eba\u91cd\u8bc6\u522b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u9aa8\u67b6\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u878d\u5408\u8fd0\u52a8\u4e0e\u5916\u89c2\u7ebf\u7d22\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u89c6\u9891\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a\u7f3a\u4e4f\u771f\u6b63\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\uff0c\u4ee5\u53ca\u6587\u672c\u65e0\u6cd5\u6355\u6349\u7ec6\u7c92\u5ea6\u65f6\u95f4\u8fd0\u52a8\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u5728\u5e8f\u5217\u7ea7\u522b\u5bf9\u9f50\u9aa8\u67b6\u548c\u89c6\u89c9\u7279\u5f81\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u52a8\u6001\u539f\u578b\u878d\u5408\u66f4\u65b0\u5668\u6765\u7cbe\u70bc\u591a\u6a21\u6001\u8eab\u4efd\u539f\u578b\uff0c\u5e76\u63d0\u51fa\u9aa8\u67b6\u5f15\u5bfc\u7684\u65f6\u95f4\u5efa\u6a21\u6a21\u5757\u3002", "result": "\u5728\u6807\u51c6\u89c6\u9891ReID\u57fa\u51c6\u6d4b\u8bd5\uff08MARS\u3001LS-VID\u3001iLIDS-VID\uff09\u4e2d\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5e76\u5728\u7eaf\u9aa8\u67b6ReID\u4efb\u52a1\uff08BIWI\u3001IAS\uff09\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CSIP-ReID\u5f00\u521b\u4e86\u65e0\u6807\u6ce8\u548c\u8fd0\u52a8\u611f\u77e5\u7684ReID\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u524d\u6cbf\u3002"}}
{"id": "2511.13185", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13185", "abs": "https://arxiv.org/abs/2511.13185", "authors": ["Aishwarya Venkataramanan", "Sai Karthikeya Vemuri", "Adithya Ashok Chalain Valapil", "Joachim Denzler"], "title": "Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction", "comment": "EurIPS DiffSys workshop 2025", "summary": "Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u5728CARS\u5230\u62c9\u66fc\u4fe1\u53f7\u91cd\u5efa\u4e2d\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u5e76\u8bc1\u660e\u5c06\u7269\u7406\u77e5\u8bc6\u7ea6\u675f\u878d\u5165\u6a21\u578b\u53ef\u4ee5\u6539\u5584\u6821\u51c6\uff0c\u4e3a\u66f4\u53ef\u9760\u7684CARS\u6570\u636e\u5206\u6790\u63d0\u4f9b\u8def\u5f84\u3002", "motivation": "CARS\u5149\u8c31\u5b66\u5728\u533b\u5b66\u548c\u6750\u6599\u79d1\u5b66\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u975e\u5171\u632f\u80cc\u666f\u5e72\u6270\u4e86\u771f\u5b9e\u62c9\u66fc\u4fe1\u53f7\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u8fd9\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e86CARS\u5230\u62c9\u66fc\u4fe1\u53f7\u91cd\u5efa\u4e2d\u7684\u5404\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u5e76\u5c06\u7269\u7406\u77e5\u8bc6\u7ea6\u675f\uff08\u5982Kramers-Kronig\u5173\u7cfb\u548c\u5149\u6ed1\u6027\u7ea6\u675f\uff09\u6574\u5408\u5230\u6a21\u578b\u4e2d\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u7269\u7406\u77e5\u8bc6\u7ea6\u675f\u878d\u5165\u6a21\u578b\u53ef\u4ee5\u6539\u5584\u5176\u6821\u51c6\u6027\u80fd\uff0c\u4e3a\u66f4\u53ef\u9760\u7684CARS\u6570\u636e\u5206\u6790\u63d0\u4f9b\u6709\u524d\u666f\u7684\u8def\u5f84\u3002", "conclusion": "\u901a\u8fc7\u96c6\u6210\u7269\u7406\u77e5\u8bc6\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u66f4\u503c\u5f97\u4fe1\u8d56\u7684CARS\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u9ad8\u98ce\u9669\u79d1\u5b66\u548c\u751f\u7269\u533b\u5b66\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.13168", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13168", "abs": "https://arxiv.org/abs/2511.13168", "authors": ["Haodong Wang", "Tao Zhuo", "Xiuwei Zhang", "Hanlin Yin", "Wencong Wu", "Yanning Zhang"], "title": "SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration", "comment": null, "summary": "Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.", "AI": {"tldr": "SOMA\u662f\u4e00\u4e2a\u5c06\u7ed3\u6784\u68af\u5ea6\u5148\u9a8c\u96c6\u6210\u5230\u6df1\u5ea6\u7279\u5f81\u4e2d\u7684SAR-\u5149\u5b66\u56fe\u50cf\u5bc6\u96c6\u914d\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u5339\u914d\u7b56\u7565\u63d0\u5347\u914d\u51c6\u7cbe\u5ea6\u3002", "motivation": "SAR\u548c\u5149\u5b66\u56fe\u50cf\u7531\u4e8e\u6210\u50cf\u673a\u5236\u548c\u89c6\u89c9\u7279\u5f81\u4e0d\u540c\uff0c\u50cf\u7d20\u7ea7\u914d\u51c6\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\u5728\u8bb8\u591a\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5728SAR-\u5149\u5b66\u914d\u51c6\u4efb\u52a1\u4e2d\u8868\u73b0\u4ecd\u4e0d\u7406\u60f3\uff0c\u68af\u5ea6\u4fe1\u606f\u5728\u4f20\u7edf\u624b\u5de5\u63cf\u8ff0\u7b26\u4e2d\u5f88\u5173\u952e\u4f46\u672a\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u6709\u6548\u5229\u7528\u3002", "method": "\u63d0\u51faSOMA\u6846\u67b6\uff1a1\uff09\u7279\u5f81\u68af\u5ea6\u589e\u5f3a\u5668\uff08FGE\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u548c\u91cd\u5efa\u673a\u5236\u5c06\u591a\u5c3a\u5ea6\u3001\u591a\u65b9\u5411\u68af\u5ea6\u6ee4\u6ce2\u5668\u5d4c\u5165\u7279\u5f81\u7a7a\u95f4\uff1b2\uff09\u5168\u5c40-\u5c40\u90e8\u4eff\u5c04\u6d41\u5339\u914d\u5668\uff08GLAM\uff09\uff0c\u5728\u7c97\u5230\u7ec6\u67b6\u6784\u4e2d\u7ed3\u5408\u4eff\u5c04\u53d8\u6362\u548c\u57fa\u4e8e\u6d41\u7684\u7ec6\u5316\u3002", "result": "\u5728SEN1-2\u6570\u636e\u96c6\u4e0aCMR@1px\u63d0\u9ad812.29%\uff0c\u5728GFGE_SO\u6570\u636e\u96c6\u4e0a\u63d0\u9ad818.50%\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u8de8\u573a\u666f\u3001\u5206\u8fa8\u7387\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SOMA\u901a\u8fc7\u6709\u6548\u5229\u7528\u68af\u5ea6\u5148\u9a8c\u548c\u6df7\u5408\u5339\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86SAR-\u5149\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13186", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.13186", "abs": "https://arxiv.org/abs/2511.13186", "authors": ["Akash Karthikeyan", "Yash Vardhan Pant"], "title": "DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play", "comment": "Initial results presented at the IJCAI 2025 Workshop on User-Aligned Assessment of Adaptive AI Systems. Project page: https://aku02.github.io/projects/difffp/", "summary": "Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $\u03b5$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\\times$ faster convergence and 30$\\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations", "AI": {"tldr": "DiffFP\u662f\u4e00\u4e2a\u865a\u6784\u535a\u5f08\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u7b56\u7565\u6765\u4f30\u8ba1\u5bf9\u672a\u89c1\u5bf9\u624b\u7684\u6700\u4f73\u54cd\u5e94\uff0c\u5728\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u591a\u6a21\u6001\u884c\u4e3a\u7b56\u7565\uff0c\u5728\u96f6\u548c\u6e38\u620f\u4e2d\u6536\u655b\u5230\u03b5-\u7eb3\u4ec0\u5747\u8861\u3002", "motivation": "\u89e3\u51b3\u8fde\u7eed\u51b3\u7b56\u7a7a\u95f4\u4e2d\u81ea\u6211\u535a\u5f08\u5f3a\u5316\u5b66\u4e60\u7684\u6311\u6218\uff0c\u786e\u4fdd\u5728\u52a8\u6001\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u907f\u514d\u6536\u655b\u7f13\u6162\u6216\u65e0\u6cd5\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDiffFP\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u7b56\u7565\u901a\u8fc7\u751f\u6210\u5efa\u6a21\u6765\u5b66\u4e60\u81ea\u9002\u5e94\u548c\u591a\u6837\u5316\u7684\u7b56\u7565\uff0c\u4f30\u8ba1\u5bf9\u672a\u89c1\u5bf9\u624b\u7684\u6700\u4f73\u54cd\u5e94\u3002", "result": "\u5728\u8d5b\u8f66\u548c\u591a\u7c92\u5b50\u96f6\u548c\u6e38\u620f\u7b49\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5bf9\u591a\u6837\u5316\u5bf9\u624b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u6bd4\u57fa\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8868\u73b0\u66f4\u597d\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u9ad83\u500d\uff0c\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad830\u500d\u3002", "conclusion": "DiffFP\u6846\u67b6\u5728\u8fde\u7eed\u7a7a\u95f4\u96f6\u548c\u6e38\u620f\u4e2d\u6709\u6548\u6536\u655b\u5230\u03b5-\u7eb3\u4ec0\u5747\u8861\uff0c\u5c55\u793a\u4e86\u5bf9\u624b\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u8bad\u7ec3\u8fed\u4ee3\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.13170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13170", "abs": "https://arxiv.org/abs/2511.13170", "authors": ["Zahra Tabatabaei", "Jon Sporring"], "title": "THIR: Topological Histopathological Image Retrieval", "comment": null, "summary": "According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.\n  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.", "AI": {"tldr": "THIR\u662f\u4e00\u4e2a\u57fa\u4e8e\u62d3\u6251\u6570\u636e\u5206\u6790\u7684\u65e0\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u6846\u67b6\uff0c\u5229\u7528\u6301\u4e45\u540c\u8c03\u4e2d\u7684Betti\u6570\u6765\u8868\u5f81\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u7684\u5185\u5728\u7ed3\u6784\u6a21\u5f0f\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u68c0\u7d22\u76f8\u4f3c\u56fe\u50cf\u3002", "motivation": "\u4e73\u817a\u764c\u662f\u5168\u7403\u5973\u6027\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u65e9\u671f\u8bca\u65ad\u548c\u51c6\u786e\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548cGPU\u8d44\u6e90\uff0c\u800cTHIR\u65e8\u5728\u63d0\u4f9b\u65e0\u9700\u76d1\u7763\u7684\u5feb\u901f\u3001\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7acb\u65b9\u6301\u4e45\u6027\u4eceRGB\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e2d\u63d0\u53d6\u62d3\u6251\u6307\u7eb9\uff0c\u901a\u8fc7Betti\u6570\u7f16\u7801\u73af\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u751f\u6210\u7d27\u51d1\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u5411\u91cf\uff0c\u7136\u540e\u8ba1\u7b97\u8fd9\u4e9b\u62d3\u6251\u63cf\u8ff0\u7b26\u4e4b\u95f4\u7684\u8ddd\u79bb\u8fdb\u884c\u76f8\u4f3c\u6027\u68c0\u7d22\u3002", "result": "\u5728BreaKHis\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTHIR\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u548c\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u6574\u4e2a\u6570\u636e\u96c6\u5904\u7406\u65f6\u95f4\u4e0d\u523020\u5206\u949f\uff08\u6807\u51c6CPU\uff09\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u53ef\u6269\u5c55\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u4e34\u5e8a\u56fe\u50cf\u68c0\u7d22\u3002", "conclusion": "THIR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u76d1\u7763\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u533b\u5b66\u56fe\u50cf\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6807\u51c6CPU\u4e0a\u5c31\u80fd\u5feb\u901f\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6709\u671b\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2511.13198", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13198", "abs": "https://arxiv.org/abs/2511.13198", "authors": ["Zhixin Ou", "Peng Liang", "Jianchen Han", "Baihui Liu", "Linbo Qiao"], "title": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer", "comment": null, "summary": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.", "AI": {"tldr": "ParaDySe\u662f\u4e00\u4e2a\u7528\u4e8e\u52a8\u6001\u5e8f\u5217\u7684\u81ea\u9002\u5e94\u5e76\u884c\u7b56\u7565\u5207\u6362\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u77ed\u5e8f\u5217\u901a\u4fe1\u5e76\u884c\u5316\u53d6\u6d88\u548c\u957f\u5e8f\u5217\u5185\u5b58\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u6846\u67b6\u5bf9\u52a8\u6001\u5e8f\u5217\u91c7\u7528\u9884\u5b9a\u4e49\u7684\u9759\u6001\u5e76\u884c\u7b56\u7565\uff0c\u5bfc\u81f4\u77ed\u5e8f\u5217\u901a\u4fe1\u5e76\u884c\u5316\u53d6\u6d88\u548c\u957f\u5e8f\u5217\u5185\u5b58\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5b9e\u73b0\u5e76\u884c\u7b56\u7565\u7684\u6a21\u5757\u5316\u51fd\u6570\u5e93\uff0c\u6784\u5efa\u5e8f\u5217\u611f\u77e5\u7684\u5185\u5b58\u548c\u65f6\u95f4\u6210\u672c\u6a21\u578b\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u7b97\u6cd5\u9009\u62e9\u6700\u4f18\u5c42\u95f4\u7b56\u7565\uff0c\u5b9e\u73b0\u65e0\u7f1d\u70ed\u5207\u6362\u3002", "result": "\u5728\u5e8f\u5217\u957f\u5ea6\u8fbe624K\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cParaDySe\u89e3\u51b3\u4e86OOM\u548cCPC\u74f6\u9888\uff0c\u7cfb\u7edf\u5730\u5c06\u957f\u5e8f\u5217\u4f18\u5316\u4e0e\u73b0\u6709\u6846\u67b6\u96c6\u6210\u3002", "conclusion": "ParaDySe\u901a\u8fc7\u81ea\u9002\u5e94\u5e76\u884c\u7b56\u7565\u5207\u6362\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u52a8\u6001\u5e8f\u5217\u7684\u901a\u4fe1\u548c\u5185\u5b58\u95ee\u9898\u3002"}}
{"id": "2511.13175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13175", "abs": "https://arxiv.org/abs/2511.13175", "authors": ["Chao Yang", "Boqian Zhang", "Jinghao Xu", "Guang Jiang"], "title": "HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution", "comment": null, "summary": "Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.", "AI": {"tldr": "\u63d0\u51faHDW-SR\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6ce2\u5206\u89e3\u548c\u6b8b\u5dee\u6269\u6563\u4e13\u6ce8\u4e8e\u9ad8\u9891\u4fe1\u606f\u6062\u590d\uff0c\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u6709\u6548\u63d0\u5347\u7ec6\u8282\u6062\u590d\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u9ad8\u9891\u57df\u6307\u5bfc\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6062\u590d\u7684\u7ec6\u8282\u6a21\u7cca\uff0c\u9700\u8981\u6539\u8fdb\u9ad8\u9891\u4fe1\u606f\u6062\u590d\u673a\u5236", "method": "\u4f7f\u7528\u5c0f\u6ce2\u5206\u89e3\u66ff\u4ee3\u4f20\u7edfCNN\u4e0b\u91c7\u6837\uff0c\u5728\u6b8b\u5dee\u56fe\u4e0a\u8fdb\u884c\u6269\u6563\uff0c\u5f15\u5165\u7a00\u758f\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u9608\u503c\u5757\u6765\u6307\u5bfc\u9ad8\u9891\u4fe1\u606f\u6062\u590d", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHDW-SR\u5728\u8d85\u5206\u8fa8\u7387\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u7279\u522b\u64c5\u957f\u6062\u590d\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7ec6\u8282", "conclusion": "HDW-SR\u901a\u8fc7\u5c0f\u6ce2\u5206\u89e3\u548c\u6b8b\u5dee\u6269\u6563\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u9891\u7ec6\u8282\u7684\u6062\u590d\u80fd\u529b\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2511.13223", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13223", "abs": "https://arxiv.org/abs/2511.13223", "authors": ["Yuxiang Zhang", "Zhengxu Yu", "Weihang Pan", "Zhongming Jin", "Qiang Fu", "Deng Cai", "Binbin Lin", "Jieping Ye"], "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs", "comment": "Accepted to NeurIPS 2025", "summary": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.", "AI": {"tldr": "TokenSqueeze\u662f\u4e00\u79cd\u65b0\u7684Long2Short\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6df1\u5ea6\u548c\u5206\u5e03\u5bf9\u9f50\u7684\u8bed\u8a00\u7cbe\u70bc\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406LLMs\u7684token\u4f7f\u7528\u91cf\u3002", "motivation": "\u73b0\u6709\u63a8\u7406LLMs\u751f\u6210\u7684\u957f\u94fe\u5f0f\u601d\u7ef4\u5bfc\u81f4token\u4f7f\u7528\u91cf\u589e\u52a0\uff0c\u5e26\u6765\u66f4\u9ad8\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\uff0c\u9700\u8981\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51faTokenSqueeze\u65b9\u6cd5\uff1a1\uff09\u81ea\u9002\u5e94\u9009\u62e9\u4e0e\u95ee\u9898\u590d\u6742\u5ea6\u5339\u914d\u7684\u63a8\u7406\u6df1\u5ea6\u6837\u672c\uff1b2\uff09\u5206\u5e03\u5bf9\u9f50\u7684\u8bed\u8a00\u7cbe\u70bc\u65b9\u6cd5\uff0c\u5728\u4e0d\u6539\u53d8\u63a8\u7406\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u8bed\u8a00\u8868\u8fbe\u3002", "result": "\u5728MATH500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u5fae\u8c03\u7684DeepSeek-R1-Distill-Qwen-7B\u5b9e\u73b0\u4e8650%\u7684\u5e73\u5747token\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "TokenSqueeze\u4ec5\u5229\u7528\u6a21\u578b\u81ea\u751f\u6210\u6570\u636e\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6574\u7406\u7684\u77ed\u7b54\u6848\u6570\u636e\u96c6\uff0c\u5c31\u80fd\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f\u7684\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u5e94\u7528\u3002"}}
{"id": "2511.13183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13183", "abs": "https://arxiv.org/abs/2511.13183", "authors": ["Alec Sargood", "Lemuel Puglisi", "Elinor Thompson", "Mirco Musolesi", "Daniel C. Alexander"], "title": "GenTract: Generative Global Tractography", "comment": null, "summary": "Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.", "AI": {"tldr": "GenTract\u662f\u9996\u4e2a\u7528\u4e8e\u5168\u5c40\u7ea4\u7ef4\u675f\u6210\u50cf\u7684\u751f\u6210\u6a21\u578b\uff0c\u5c06\u7ea4\u7ef4\u675f\u6210\u50cf\u6784\u5efa\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u76f4\u63a5\u4ecedMRI\u6620\u5c04\u5230\u5b8c\u6574\u3001\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u7684\u6d41\u7ebf\u3002\u5728\u4f4e\u5206\u8fa8\u7387\u548c\u566a\u58f0\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7cbe\u5ea6\u6bd4\u6b21\u4f18\u65b9\u6cd5\u9ad82.1\u500d\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5c40\u90e8\u7ea4\u7ef4\u675f\u6210\u50cf\u65b9\u6cd5\u5bb9\u6613\u7d2f\u79ef\u8bef\u5dee\u548c\u9ad8\u5047\u9633\u6027\u7387\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5168\u5c40\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGenTract\u751f\u6210\u6a21\u578b\uff0c\u5c06\u7ea4\u7ef4\u675f\u6210\u50cf\u6784\u5efa\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u5b66\u4e60\u4ecedMRI\u5230\u5b8c\u6574\u6d41\u7ebf\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u6269\u6563\u548c\u6d41\u5339\u914d\u7684\u8303\u5f0f\u3002", "result": "GenTract\u7cbe\u5ea6\u6bd4\u6b21\u4f18\u65b9\u6cd5TractOracle\u9ad82.1\u500d\uff0c\u5728\u4f4e\u5206\u8fa8\u7387\u548c\u566a\u58f0\u8bbe\u7f6e\u4e0b\u4f18\u52bf\u66f4\u660e\u663e\uff0c\u6bd4\u6700\u63a5\u8fd1\u7684\u7ade\u4e89\u8005\u9ad8\u51fa\u6570\u91cf\u7ea7\u3002", "conclusion": "GenTract\u5728\u7814\u7a76\u7ea7\u6570\u636e\u4e0a\u4ea7\u751f\u9ad8\u7cbe\u5ea6\u7ea4\u7ef4\u675f\u56fe\uff0c\u540c\u65f6\u5728\u4f4e\u8d28\u91cf\u6570\u636e\u4e0a\u4fdd\u6301\u53ef\u9760\u6027\uff0c\u662f\u5168\u5c40\u7ea4\u7ef4\u675f\u6210\u50cf\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13229", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13229", "abs": "https://arxiv.org/abs/2511.13229", "authors": ["Mary Chriselda Antony Oliver", "Michael Roberts", "Carola-Bibiane Sch\u00f6nlieb", "Matthew Thorpe"], "title": "Laplace Learning in Wasserstein Space", "comment": "46 page, 5 figures", "summary": "The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning\n  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical\n  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to\n  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator\n  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through\n  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u56fe\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u62c9\u666e\u62c9\u65af\u5b66\u4e60\u6269\u5c55\u5230Wasserstein\u7a7a\u95f4\uff0c\u4ece\u6709\u9650\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u63a8\u5e7f\u5230\u65e0\u9650\u7ef4\u8bbe\u7f6e\u3002\u901a\u8fc7\u8bc1\u660e\u79bb\u6563\u56fep-Dirichlet\u80fd\u91cf\u5230\u5176\u8fde\u7eed\u5bf9\u5e94\u7269\u7684\u53d8\u5206\u6536\u655b\uff0c\u5e76\u523b\u753bWasserstein\u7a7a\u95f4\u5b50\u6d41\u5f62\u4e0a\u7684\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\uff0c\u6700\u7ec8\u901a\u8fc7\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u6d41\u5f62\u5047\u8bf4\u8ba4\u4e3a\u9ad8\u7ef4\u6570\u636e\u901a\u5e38\u5b58\u5728\u4e8e\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\u3002\u672c\u6587\u57fa\u4e8e\u8fd9\u4e00\u5047\u8bf4\u7814\u7a76\u57fa\u4e8e\u56fe\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5c06\u62c9\u666e\u62c9\u65af\u5b66\u4e60\u6269\u5c55\u5230Wasserstein\u7a7a\u95f4\uff0c\u4ee5\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u5206\u7c7b\u95ee\u9898\u3002", "method": "\u5c06\u7ecf\u5178\u7684\u57fa\u4e8e\u56fe\u534a\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u4ece\u6709\u9650\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u6269\u5c55\u5230\u65e0\u9650\u7ef4Wasserstein\u7a7a\u95f4\u3002\u901a\u8fc7\u8bc1\u660e\u79bb\u6563\u56fep-Dirichlet\u80fd\u91cf\u5230\u5176\u8fde\u7eed\u5bf9\u5e94\u7269\u7684\u53d8\u5206\u6536\u655b\uff0c\u5e76\u523b\u753bWasserstein\u7a7a\u95f4\u5b50\u6d41\u5f62\u4e0a\u7684\u62c9\u666e\u62c9\u65af-\u8d1d\u5c14\u7279\u62c9\u7c73\u7b97\u5b50\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u4ece\u79bb\u6563\u5230\u8fde\u7eed\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e0b\u5206\u7c7b\u6027\u80fd\u5177\u6709\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u6210\u529f\u5730\u5c06\u57fa\u4e8e\u56fe\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55\u5230Wasserstein\u7a7a\u95f4\uff0c\u4e3a\u5904\u7406\u9ad8\u7ef4\u6570\u636e\u5206\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\u548c\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2511.13234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13234", "abs": "https://arxiv.org/abs/2511.13234", "authors": ["Boris Kriuk"], "title": "MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing", "comment": "8 pages, 5 figures", "summary": "Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (\u03c3=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.", "AI": {"tldr": "MorphBoost\u662f\u4e00\u79cd\u65b0\u578b\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u7ec4\u7ec7\u6811\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u5206\u88c2\u884c\u4e3a\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u88c2\u51fd\u6570\u548c\u95ee\u9898\u6307\u7eb9\u8bc6\u522b\u6280\u672f\uff0c\u572810\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u4f18\u4e8eXGBoost 0.84%\uff0c\u83b7\u5f9740%\u7684\u80dc\u7387\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6\u63d0\u5347\u7b97\u6cd5\u4f7f\u7528\u9759\u6001\u6811\u7ed3\u6784\uff0c\u5206\u88c2\u6807\u51c6\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e0d\u53d8\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u4e0d\u540c\u5b66\u4e60\u9636\u6bb5\u68af\u5ea6\u5206\u5e03\u53d8\u5316\u548c\u95ee\u9898\u7279\u5b9a\u7279\u5f81\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u81ea\u7ec4\u7ec7\u6811\u7ed3\u6784\uff0c\u5305\u542b\uff1a(1)\u7ed3\u5408\u68af\u5ea6\u5f97\u5206\u548c\u4fe1\u606f\u8bba\u5ea6\u91cf\u7684\u53d8\u5f62\u5206\u88c2\u51c6\u5219\uff1b(2)\u81ea\u52a8\u95ee\u9898\u6307\u7eb9\u8bc6\u522b\uff1b(3)\u5411\u91cf\u5316\u6811\u9884\u6d4b\uff1b(4)\u4ea4\u4e92\u611f\u77e5\u7279\u5f81\u91cd\u8981\u6027\uff1b(5)\u5feb\u901f\u6a21\u5f0f\u4f18\u5316\u3002", "result": "\u572810\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cMorphBoost\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e73\u5747\u4f18\u4e8eXGBoost 0.84%\uff0c\u83b7\u5f974/10\u6570\u636e\u96c6\u80dc\u5229\uff0840%\u80dc\u7387\uff09\u548c6/30\u524d\u4e09\u540d\u5b8c\u6210\uff0820%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f4e\u65b9\u5dee\uff08\u03c3=0.0948\uff09\u548c\u6700\u9ad8\u6700\u5c0f\u51c6\u786e\u7387\u3002", "conclusion": "MorphBoost\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u7684\u6811\u7ed3\u6784\u5728\u4fdd\u6301\u9ad8\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u7684\u540c\u65f6\uff0c\u5728\u590d\u6742\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ea7\u95ee\u9898\u4e0a\u7531\u4e8e\u66f4\u9ad8\u7684\u9002\u5e94\u6c34\u5e73\u800c\u83b7\u5f97\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2511.13190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13190", "abs": "https://arxiv.org/abs/2511.13190", "authors": ["Haoran Tang", "Meng Cao", "Ruyang Liu", "Xiaoxi Liang", "Linglong Li", "Ge Li", "Xiaodan Liang"], "title": "Video Spatial Reasoning with Object-Centric 3D Rollout", "comment": null, "summary": "Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).", "AI": {"tldr": "\u63d0\u51faObject-Centric 3D Rollout (OCR)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u6270\u52a8\u6765\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5b58\u5728\u7684\u67e5\u8be2\u9501\u5b9a\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u8868\u73b0\u4e3a\u67e5\u8be2\u9501\u5b9a\u63a8\u7406\u2014\u2014\u4ec5\u5173\u6ce8\u63d0\u793a\u4e2d\u660e\u786e\u63d0\u5230\u7684\u5bf9\u8c61\u800c\u5ffd\u7565\u5173\u952e\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3002", "method": "\u63d0\u51faOCR\u7b56\u7565\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5bf9\u9009\u5b9a\u5bf9\u8c61\u76843D\u51e0\u4f55\u7ed3\u6784\u5f15\u5165\u7ed3\u6784\u5316\u6270\u52a8\uff0c\u901a\u8fc7\u964d\u7ea7\u5bf9\u8c61\u7279\u5b9a\u89c6\u89c9\u7ebf\u7d22\u5e76\u5c06\u6539\u53d8\u7684\u51e0\u4f55\u7ed3\u6784\u6295\u5f71\u52302D\u7a7a\u95f4\uff0c\u8feb\u4f7f\u6a21\u578b\u5bf9\u6574\u4e2a\u573a\u666f\u8fdb\u884c\u6574\u4f53\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3B\u53c2\u6570\u6a21\u578b\u5728VSI-Bench\u4e0a\u8fbe\u523047.5%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u591a\u4e2a7B\u57fa\u7ebf\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9eOCR\u4f18\u4e8e\u5148\u524d\u7684\u5c55\u5f00\u7b56\u7565\u3002", "conclusion": "OCR\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u6270\u52a8\u548c\u5c55\u5f00\u5f0f\u8bad\u7ec3\u7ba1\u9053\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002"}}
{"id": "2511.13237", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13237", "abs": "https://arxiv.org/abs/2511.13237", "authors": ["Alan G. Paredes Cetina", "Kaouther Benguessoum", "Raoni Louren\u00e7o", "Sylvain Kubler"], "title": "Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification", "comment": "Accepted in AAAI 2026 Technical Main Track", "summary": "Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\\geq10\\%$ higher confidence while improving sparsity in $\\geq40\\%$.", "AI": {"tldr": "CONFETTI\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u76ee\u6807\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u901a\u8fc7\u5e73\u8861\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3001\u90bb\u8fd1\u6027\u548c\u7a00\u758f\u6027\u6765\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u548c\u56de\u5f52\u4e2d\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u800c\u73b0\u6709\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u4f20\u8fbe\u5b8c\u6574\u7684\u51b3\u7b56\u7a7a\u95f4\uff0c\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u53ea\u4f18\u5148\u8003\u8651\u51c6\u786e\u6027\u3001\u90bb\u8fd1\u6027\u6216\u7a00\u758f\u6027\u4e2d\u7684\u5355\u4e00\u76ee\u6807\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u4ef7\u503c\u3002", "method": "CONFETTI\u901a\u8fc7\u8bc6\u522b\u5173\u952eMTS\u5b50\u5e8f\u5217\u3001\u5b9a\u4f4d\u53cd\u4e8b\u5b9e\u76ee\u6807\uff0c\u5e76\u4f18\u5316\u4fee\u6539\u65f6\u95f4\u5e8f\u5217\u6765\u5e73\u8861\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3001\u90bb\u8fd1\u6027\u548c\u7a00\u758f\u6027\u8fd9\u4e09\u4e2a\u76ee\u6807\u3002", "result": "\u5728UEA\u6863\u6848\u4e2d\u76847\u4e2aMTS\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCONFETTI\u5728\u4f18\u5316\u76ee\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\uff0c\u5728\u6587\u732e\u4e2d\u7684\u5176\u4ed66\u4e2a\u6307\u6807\u4e0a\u4e5f\u8868\u73b0\u66f4\u597d\uff0c\u5b9e\u73b0\u4e86\u226510%\u7684\u66f4\u9ad8\u7f6e\u4fe1\u5ea6\uff0c\u540c\u65f6\u5728\u226540%\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u4e86\u7a00\u758f\u6027\u3002", "conclusion": "CONFETTI\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5177\u6709\u6700\u5c0f\u53d8\u5316\u7684\u9ad8\u8d28\u91cf\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u652f\u6301\u80fd\u529b\u3002"}}
{"id": "2511.13191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13191", "abs": "https://arxiv.org/abs/2511.13191", "authors": ["Ying Jiang", "Jiayin Lu", "Yunuo Chen", "Yumeng He", "Kui Wu", "Yin Yang", "Chenfanfu Jiang"], "title": "Birth of a Painting: Differentiable Brushstroke Reconstruction", "comment": "13 pages", "summary": "Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u7b14\u89e6\u91cd\u5efa\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u7ed8\u753b\u3001\u98ce\u683c\u5316\u7eb9\u7406\u548c\u6d82\u62b9\u64cd\u4f5c\uff0c\u80fd\u591f\u771f\u5b9e\u518d\u73b0\u4eba\u7c7b\u7ed8\u753b-\u6d82\u62b9\u5faa\u73af\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u56fe\u50cf\u751f\u6210\u6216\u57fa\u4e8e\u8865\u4e01\u7684\u8fc7\u7a0b\u6a21\u62df\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u7b14\u89e6\u7ed3\u6784\uff0c\u65e0\u6cd5\u4ea7\u751f\u5e73\u6ed1\u903c\u771f\u7684\u9634\u5f71\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5e76\u884c\u53ef\u5fae\u5206\u7ed8\u753b\u6e32\u67d3\u5668\u4f18\u5316\u5355\u8272\u548c\u53cc\u8272\u8d1d\u585e\u5c14\u7b14\u89e6\uff0c\u7ed3\u5408\u98ce\u683c\u751f\u6210\u6a21\u5757\u5408\u6210\u51e0\u4f55\u6761\u4ef6\u7eb9\u7406\uff0c\u5e76\u5f15\u5165\u53ef\u5fae\u5206\u6d82\u62b9\u7b97\u5b50\u5b9e\u73b0\u81ea\u7136\u8272\u5f69\u6df7\u5408\u548c\u9634\u5f71\u6548\u679c\u3002", "result": "\u5728\u6cb9\u753b\u3001\u6c34\u5f69\u3001\u6c34\u58a8\u548c\u6570\u5b57\u7ed8\u753b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ea7\u751f\u903c\u771f\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u7b14\u89e6\u91cd\u5efa\u3001\u5e73\u6ed1\u7684\u8272\u8c03\u8fc7\u6e21\u548c\u4e30\u5bcc\u7684\u98ce\u683c\u5316\u5916\u89c2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8868\u8fbe\u6027\u6570\u5b57\u7ed8\u753b\u521b\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\uff0c\u80fd\u591f\u5fe0\u5b9e\u518d\u73b0\u4eba\u7c7b\u7ed8\u753b\u8fc7\u7a0b\u3002"}}
{"id": "2511.13195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13195", "abs": "https://arxiv.org/abs/2511.13195", "authors": ["Soyul Lee", "Seungmin Baek", "Dongbo Min"], "title": "Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection", "comment": "AAAI 2026 accepted", "summary": "Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.", "AI": {"tldr": "MonoDLGD\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u96be\u5ea6\u611f\u77e5\u6807\u7b7e\u5f15\u5bfc\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6270\u52a8\u548c\u91cd\u5efa\u771f\u5b9e\u6807\u7b7e\u6765\u89e3\u51b3\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u7531\u4e8e\u6df1\u5ea6\u7ebf\u7d22\u7684\u56fa\u6709\u6a21\u7cca\u6027\u800c\u5b58\u5728\u6839\u672c\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u4e14\u5ffd\u7565\u4e86\u5b9e\u4f8b\u7ea7\u68c0\u6d4b\u96be\u5ea6\uff08\u5982\u906e\u6321\u3001\u8ddd\u79bb\u548c\u622a\u65ad\uff09\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMonoDLGD\u6846\u67b6\uff0c\u57fa\u4e8e\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u81ea\u9002\u5e94\u6270\u52a8\u548c\u91cd\u5efa\u771f\u5b9e\u6807\u7b7e\uff1a\u5bf9\u7b80\u5355\u5b9e\u4f8b\u5e94\u7528\u66f4\u5f3a\u7684\u6270\u52a8\uff0c\u5bf9\u56f0\u96be\u5b9e\u4f8b\u5e94\u7528\u66f4\u5f31\u7684\u6270\u52a8\uff0c\u7136\u540e\u91cd\u5efa\u5b83\u4eec\u4ee5\u63d0\u4f9b\u663e\u5f0f\u51e0\u4f55\u76d1\u7763\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6807\u7b7e\u91cd\u5efa\u548c3D\u76ee\u6807\u68c0\u6d4b\uff0c\u4fc3\u8fdb\u51e0\u4f55\u611f\u77e5\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMonoDLGD\u5728\u6240\u6709\u96be\u5ea6\u7ea7\u522b\u4e0a\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MonoDLGD\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u7684\u6807\u7b7e\u5f15\u5bfc\u53bb\u566a\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6df1\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5bf9\u4e0d\u540c\u590d\u6742\u5ea6\u76ee\u6807\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13240", "abs": "https://arxiv.org/abs/2511.13240", "authors": ["Arka Pal", "Teo Kitanovski", "Arthur Liang", "Akilesh Potti", "Micah Goldblum"], "title": "Incoherent Beliefs & Inconsistent Actions in Large Language Models", "comment": null, "summary": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u4fe1\u5ff5\u66f4\u65b0\u4e0d\u4e00\u81f4\u548c\u884c\u52a8\u4e0e\u4fe1\u5ff5\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u9ad8\u7cbe\u5ea6\u6216\u826f\u597d\u6821\u51c6\u7684\u6a21\u578b\u4e2d\u4e5f\u5b58\u5728\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e0e\u9759\u6001\u6570\u636e\u96c6\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u987a\u5e8f\u4ea4\u4e92\u3001\u6839\u636e\u65b0\u8bc1\u636e\u8fde\u8d2f\u66f4\u65b0\u4fe1\u5ff5\u5e76\u505a\u51fa\u76f8\u5e94\u51b3\u7b56\u3002\u9884\u6d4bLLM\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\u5f88\u91cd\u8981\uff0c\u4f46\u96be\u4ee5\u4ece\u9759\u6001\u8bbe\u7f6e\u4e2d\u786e\u5b9a\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u68c0\u9a8cLLM\u7684\u4e24\u4e2a\u5173\u952e\u80fd\u529b\uff1a\u4fe1\u5ff5\u7684\u8fde\u8d2f\u66f4\u65b0\u80fd\u529b\uff0c\u4ee5\u53ca\u884c\u52a8\u4e0e\u4fe1\u5ff5\u7684\u4e00\u81f4\u6027\u7a0b\u5ea6\u3002\u5177\u4f53\u5305\u62ec\u76f4\u63a5\u5f15\u51fa\u540e\u9a8c\u4e0e\u6b63\u786e\u66f4\u65b0\u5148\u9a8c\u7684\u5bf9\u6bd4\u3001\u6295\u6ce8\u5e02\u573a\u4e2d\u7684\u884c\u52a8\u4e00\u81f4\u6027\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u5bf9\u7528\u6237\u8d28\u7591\u7684\u81ea\u6211\u4e00\u81f4\u6027\u8bc4\u4f30\u3002", "result": "1. LLM\u5728\u4fe1\u5ff5\u66f4\u65b0\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\u6027\uff0c\u76f4\u63a5\u5f15\u51fa\u7684\u540e\u9a8c\u4e0e\u6b63\u786e\u66f4\u65b0\u7684\u5148\u9a8c\u4e4b\u95f4\u5b58\u5728\u9ad8\u8fbe30%\u7684\u5e73\u5747\u5dee\u5f02\uff1b2. LLM\u7684\u884c\u52a8\u7ecf\u5e38\u4e0e\u5176\u6301\u6709\u7684\u4fe1\u5ff5\u4e0d\u4e00\u81f4\uff0c\u5728\u6295\u6ce8\u5e02\u573a\u4e2d\u751a\u81f3\u4e0d\u6309\u5185\u5728\u4fe1\u5ff5\u65b9\u5411\u4e0b\u6ce8\uff1b3. \u6a21\u578b\u5bf9\u7528\u6237\u8d28\u7591\u7684\u56de\u5e94\u5b58\u5728\u4e2d\u7b49\u7a0b\u5ea6\u7684\u81ea\u6211\u4e0d\u4e00\u81f4\u6027\uff1b4. \u8fd9\u4e9b\u95ee\u9898\u5373\u4f7f\u5728\u83b7\u5f97\u9ad8\u7cbe\u5ea6\u6216\u826f\u597d\u6821\u51c6\u7684\u5f3a\u6a21\u578b\u4e2d\u4f9d\u7136\u5b58\u5728\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5728\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u9884\u6d4bLLM\u884c\u4e3a\u7684\u56f0\u96be\uff0c\u8868\u660e\u5373\u4f7f\u8868\u73b0\u826f\u597d\u7684\u6a21\u578b\u5728\u52a8\u6001\u4ea4\u4e92\u4efb\u52a1\u4e2d\u4e5f\u53ef\u80fd\u5b58\u5728\u4fe1\u5ff5\u548c\u884c\u52a8\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\u3002"}}
{"id": "2511.13197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13197", "abs": "https://arxiv.org/abs/2511.13197", "authors": ["Alberto Gomez", "Jorge Oliveira", "Ramon Casero", "Agis Chartsias"], "title": "Self-Supervised Ultrasound Screen Detection", "comment": "Submitted to ISBI 2026", "summary": "Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u7ba1\u9053\uff0c\u4ece\u8d85\u58f0\u8bbe\u5907\u663e\u793a\u5668\u7167\u7247\u4e2d\u63d0\u53d6\u8d85\u58f0\u56fe\u50cf\uff0c\u7ed5\u8fc7DICOM\u4f20\u8f93\u74f6\u9888\uff0c\u5b9e\u73b0\u5feb\u901f\u7b97\u6cd5\u6d4b\u8bd5\u548c\u539f\u578b\u5f00\u53d1\u3002\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u6821\u6b63\u540e\u7684\u56fe\u50cf\u4fdd\u6301\u4e86\u8db3\u591f\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5728\u5fc3\u810f\u89c6\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u52300.79\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "motivation": "\u8d85\u58f0\u8bbe\u5907\u5185\u7f6e\u663e\u793a\u5668\u663e\u793a\u56fe\u50cf\uff0c\u4f46\u5e38\u89c4\u4f20\u8f93\u5230\u533b\u9662\u7cfb\u7edf\u4f9d\u8d56DICOM\u683c\u5f0f\u3002\u8fd9\u79cd\u4f9d\u8d56\u5f62\u6210\u4e86\u4f20\u8f93\u74f6\u9888\uff0c\u9650\u5236\u4e86\u65b0\u7b97\u6cd5\u7684\u5feb\u901f\u6d4b\u8bd5\u548c\u539f\u578b\u5f00\u53d1\u3002", "method": "\u5f00\u53d1\u81ea\u76d1\u7763\u7ba1\u9053\uff0c\u901a\u8fc7\u62cd\u6444\u8d85\u58f0\u8bbe\u5907\u663e\u793a\u5668\u7167\u7247\uff0c\u4ece\u4e2d\u63d0\u53d6\u548c\u6821\u6b63\u8d85\u58f0\u56fe\u50cf\uff0c\u907f\u514d\u5bf9DICOM\u4f20\u8f93\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u4e2d\uff0c\u6821\u6b63\u540e\u7684\u56fe\u50cf\u4fdd\u6301\u4e86\u8db3\u591f\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5728\u5fc3\u810f\u89c6\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u76f8\u5bf9\u4e8e\u539f\u59cbDICOM\u56fe\u50cf\u8fbe\u5230\u4e860.79\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed5\u8fc7\u4e86DICOM\u4f20\u8f93\u74f6\u9888\uff0c\u4e3a\u5feb\u901f\u6d4b\u8bd5\u548c\u539f\u578b\u5f00\u53d1\u65b0\u7b97\u6cd5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6821\u6b63\u56fe\u50cf\u5728\u4e34\u5e8a\u5e94\u7528\u573a\u666f\u4e2d\u4fdd\u6301\u4e86\u8db3\u591f\u7684\u8bca\u65ad\u4ef7\u503c\u3002"}}
{"id": "2511.13243", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13243", "abs": "https://arxiv.org/abs/2511.13243", "authors": ["Xiaoqi Han", "Ru Li", "Ran Yi", "Hongye Tan", "Zhuomin Liang", "V\u00edctor Guti\u00e9rrez-Basulto", "Jeff Z. Pan"], "title": "Uncovering and Mitigating Transient Blindness in Multimodal Model Editing", "comment": "Accepted at AAAI'26", "summary": "Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.13204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13204", "abs": "https://arxiv.org/abs/2511.13204", "authors": ["Junhee Lee", "ChaeBeen Bang", "MyoungChul Kim", "MyeongAh Cho"], "title": "RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection", "comment": "Accepted to AAAI 2026", "summary": "Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both \"how\" motion evolves and \"what\" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.", "AI": {"tldr": "RefineVAD\u662f\u4e00\u4e2a\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u8fd0\u52a8\u6a21\u5f0f\u548c\u8bed\u4e49\u7ed3\u6784\u6765\u8bc6\u522b\u591a\u79cd\u5f02\u5e38\u7c7b\u578b\uff0c\u5305\u542b\u8fd0\u52a8\u611f\u77e5\u65f6\u95f4\u6ce8\u610f\u529b\u91cd\u6821\u51c6\u548c\u7c7b\u522b\u5bfc\u5411\u7ec6\u5316\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5f02\u5e38\u4e8b\u4ef6\u89c6\u4e3a\u5355\u4e00\u7c7b\u522b\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u5f02\u5e38\u7684\u591a\u6837\u8bed\u4e49\u548c\u65f6\u95f4\u7279\u6027\uff0c\u9700\u8981\u6a21\u4eff\u4eba\u7c7b\u611f\u77e5\u5f02\u5e38\u7684\u53cc\u8fc7\u7a0b\u63a8\u7406\u65b9\u5f0f\u3002", "method": "\u63d0\u51faRefineVAD\u6846\u67b6\uff1a1) MoTAR\u6a21\u5757\u4f30\u8ba1\u8fd0\u52a8\u663e\u8457\u6027\u5e76\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u6ce8\u610f\u529b\uff1b2) CORE\u6a21\u5757\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u5c06\u7247\u6bb5\u7279\u5f81\u4e0e\u53ef\u5b66\u4e60\u7c7b\u522b\u539f\u578b\u5bf9\u9f50\uff0c\u6ce8\u5165\u8f6f\u5f02\u5e38\u7c7b\u522b\u5148\u9a8c\u3002", "result": "\u5728WVAD\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RefineVAD\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u6574\u5408\u8bed\u4e49\u4e0a\u4e0b\u6587\u5bf9\u5f15\u5bfc\u7279\u5f81\u7ec6\u5316\u81f3\u5f02\u5e38\u76f8\u5173\u6a21\u5f0f\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5229\u7528\u65f6\u95f4\u52a8\u6001\u548c\u8bed\u4e49\u7ed3\u6784\uff0cRefineVAD\u80fd\u591f\u663e\u5f0f\u5efa\u6a21\u8fd0\u52a8\u6f14\u5316\u65b9\u5f0f\u548c\u8bed\u4e49\u7c7b\u522b\u76f8\u4f3c\u6027\uff0c\u63d0\u5347\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.13244", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13244", "abs": "https://arxiv.org/abs/2511.13244", "authors": ["Nadav Bojan Sellam", "Meital Bojan", "Paul Schanda", "Alex Bronstein"], "title": "Seek and You Shall Fold", "comment": null, "summary": "Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u86cb\u767d\u8d28\u751f\u6210\u6a21\u578b\u7684\u4e0d\u53ef\u5fae\u5206\u6307\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u5c06\u8fde\u7eed\u6269\u6563\u751f\u6210\u5668\u4e0e\u4efb\u610f\u9ed1\u76d2\u76ee\u6807\u8026\u5408\uff0c\u89e3\u51b3\u4e86\u5b9e\u9a8c\u6570\u636e\u96be\u4ee5\u6574\u5408\u5230\u86cb\u767d\u8d28\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u51c6\u786e\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u5bf9\u4e8e\u7406\u89e3\u751f\u7269\u529f\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c06\u5b9e\u9a8c\u6570\u636e\u7eb3\u5165\u86cb\u767d\u8d28\u751f\u6210\u6a21\u578b\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u5927\u591a\u6570\u5b9e\u9a8c\u53ef\u89c2\u6d4b\u503c\u7684\u9884\u6d4b\u5668\u662f\u4e0d\u53ef\u5fae\u5206\u7684\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u6761\u4ef6\u91c7\u6837\u4e0d\u517c\u5bb9\uff0c\u7279\u522b\u662f\u5728\u6838\u78c1\u5171\u632f\u9886\u57df\uff0c\u5316\u5b66\u4f4d\u79fb\u7b49\u4e30\u5bcc\u6570\u636e\u96be\u4ee5\u76f4\u63a5\u6574\u5408\u5230\u751f\u6210\u5efa\u6a21\u4e2d\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7528\u4e8e\u86cb\u767d\u8d28\u751f\u6210\u6a21\u578b\u7684\u4e0d\u53ef\u5fae\u5206\u6307\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u5236\u7684\u9057\u4f20\u7b97\u6cd5\u5c06\u8fde\u7eed\u6269\u6563\u751f\u6210\u5668\u4e0e\u4efb\u4f55\u9ed1\u76d2\u76ee\u6807\u8026\u5408\u3002", "result": "\u5728\u4e09\u79cd\u6a21\u5f0f\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff1a\u6210\u5bf9\u8ddd\u79bb\u7ea6\u675f\u3001\u6838\u5965\u5f17\u8c6a\u6cfd\u6548\u5e94\u7ea6\u675f\uff0c\u4ee5\u53ca\u9996\u6b21\u5b9e\u73b0\u7684\u5316\u5b66\u4f4d\u79fb\u6307\u5bfc\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u5316\u5b66\u4f4d\u79fb\u6307\u5bfc\u7684\u7ed3\u6784\u751f\u6210\u662f\u53ef\u884c\u7684\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9884\u6d4b\u5668\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5e76\u5c55\u793a\u4e86\u4e00\u79cd\u6574\u5408\u591a\u6837\u5316\u5b9e\u9a8c\u4fe1\u53f7\u7684\u901a\u7528\u7b56\u7565\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6307\u5411\u4e86\u8d85\u8d8a\u53ef\u5fae\u5206\u6027\u9650\u5236\u7684\u81ea\u52a8\u5316\u3001\u6570\u636e\u6761\u4ef6\u5316\u7684\u86cb\u767d\u8d28\u5efa\u6a21\u65b9\u5411\u3002"}}
{"id": "2511.13208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13208", "abs": "https://arxiv.org/abs/2511.13208", "authors": ["Yonghui Yu", "Jiahang Cai", "Xun Wang", "Wenwu Yang"], "title": "End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer", "comment": null, "summary": "Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \\textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u591a\u5e272D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5PAVE-Net\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u4e2d\u7684\u542f\u53d1\u5f0f\u64cd\u4f5c\uff0c\u901a\u8fc7\u59ff\u6001\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8de8\u5e27\u4e2a\u4f53\u5173\u8054\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u591a\u4eba\u89c6\u9891\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff08\u68c0\u6d4b+\u65f6\u5e8f\u5efa\u6a21\uff09\uff0c\u4f9d\u8d56\u68c0\u6d4b\u3001RoI\u88c1\u526a\u548cNMS\u7b49\u542f\u53d1\u5f0f\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u9700\u8981\u5f00\u53d1\u5b8c\u5168\u7aef\u5230\u7aef\u7684\u6846\u67b6\u6765\u6d88\u9664\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51faPAVE-Net\u6846\u67b6\uff0c\u5305\u542b\u7a7a\u95f4\u7f16\u7801\u5668\u5efa\u6a21\u5e27\u5185\u5173\u7cfb\u548c\u65f6\u7a7a\u59ff\u6001\u89e3\u7801\u5668\u6355\u83b7\u8de8\u5e27\u5168\u5c40\u4f9d\u8d56\u3002\u6838\u5fc3\u521b\u65b0\u662f\u59ff\u6001\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f7f\u6bcf\u4e2a\u59ff\u6001\u67e5\u8be2\u80fd\u591f\u9009\u62e9\u6027\u805a\u5408\u8de8\u8fde\u7eed\u5e27\u4e2d\u76f8\u540c\u4e2a\u4f53\u7684\u7279\u5f81\uff0c\u5e76\u663e\u5f0f\u5efa\u6a21\u59ff\u6001\u5173\u952e\u70b9\u95f4\u7684\u65f6\u7a7a\u4f9d\u8d56\u3002", "result": "\u5728PoseTrack2017\u4e0a\u6bd4\u57fa\u4e8e\u56fe\u50cf\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u63d0\u53476.0 mAP\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u4e24\u9636\u6bb5\u89c6\u9891\u65b9\u6cd5\u7cbe\u5ea6\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002", "conclusion": "PAVE-Net\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u591a\u5e272D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u590d\u6742\u91cd\u53e0\u65f6\u5e8f\u8f68\u8ff9\u4e0b\u7684\u4e2a\u4f53\u5173\u8054\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2511.13250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13250", "abs": "https://arxiv.org/abs/2511.13250", "authors": ["Aleksandar Stankovi\u0107", "Dejan Lisica"], "title": "Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs", "comment": "8 pages, 3 figures, 5 tables. Code and artifacts: https://github.com/SV25-22/ECHO-Proteins", "summary": "We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.", "AI": {"tldr": "\u672c\u6587\u4e3aogbn-proteins\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u8fb9\u7f18\u611f\u77e5\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7814\u7a76\u4e86\u8fb9\u7f18\u7279\u5f81\u805a\u5408\u548c\u6d88\u606f\u4f20\u9012\u4e2d\u7684\u5173\u952e\u7cfb\u7edf\u9009\u62e9\uff0c\u53d1\u73b0\u57fa\u4e8esum\u7684\u8fb9\u7f18\u5230\u8282\u70b9\u7279\u5f81\u805a\u5408\u6548\u679c\u6700\u4f73\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u5f52\u4e00\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3aogbn-proteins\u6570\u636e\u96c6\u5efa\u7acb\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u7814\u7a76\u8fb9\u7f18\u7279\u5f81\u5982\u4f55\u805a\u5408\u5230\u8282\u70b9\u8f93\u5165\u4ee5\u53ca\u5982\u4f55\u5728\u6d88\u606f\u4f20\u9012\u4e2d\u4f7f\u7528\u8fb9\u7f18\uff0c\u4e3a\u5b9e\u8df5\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528PyTorch Geometric\u5b9e\u73b0GraphSAGE\u6a21\u578b\uff0c\u6bd4\u8f83sum\u3001mean\u3001max\u4e09\u79cd\u8fb9\u7f18\u7279\u5f81\u805a\u5408\u65b9\u5f0f\uff0c\u4ee5\u53caLayerNorm\u3001BatchNorm\u548cConditional LayerNorm\u4e09\u79cd\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u540e\u5904\u7406\u4f18\u5316\u3002", "result": "sum\u805a\u5408\u65b9\u5f0f\u4f18\u4e8emean\u548cmax\uff1bBatchNorm\u83b7\u5f97\u6700\u4f73AUC\uff0cConditional LayerNorm\u5728AUC\u524d\u6cbf\u5339\u914d\u4e14\u5177\u6709\u66f4\u597d\u7684\u9608\u503cF1\uff1b\u540e\u5904\u7406\u6280\u672f\u663e\u8457\u63d0\u5347micro-F1\u548c\u6821\u51c6\u8bef\u5dee\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684ogbn-proteins\u57fa\u7ebf\u65b9\u6cd5\uff0csum\u8fb9\u7f18\u7279\u5f81\u805a\u5408\u548cBatchNorm\u662f\u6700\u4f73\u5b9e\u8df5\u9009\u62e9\uff0c\u540e\u5904\u7406\u6280\u672f\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u800c\u51e0\u4e4e\u4e0d\u5f71\u54cdAUC\u3002"}}
{"id": "2511.13211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13211", "abs": "https://arxiv.org/abs/2511.13211", "authors": ["Yijia Fan", "Jusheng Zhang", "Kaitong Cai", "Jing Yang", "Jian Wang", "Keze Wang"], "title": "3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale", "comment": null, "summary": "Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.", "AI": {"tldr": "3DAlign-DAER\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6587\u672c-3D\u51e0\u4f55\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6ce8\u610f\u529b\u7b56\u7565\u548c\u9ad8\u6548\u68c0\u7d22\u7b56\u7565\u89e3\u51b3\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u5927\u89c4\u6a213D\u6570\u636e\u5e93\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5c06\u7ec6\u7c92\u5ea6\u6587\u672c\u8bed\u4e49\u4e0e\u8be6\u7ec6\u51e0\u4f55\u7ed3\u6784\u5bf9\u9f50\uff0c\u4e14\u5728\u5927\u89c4\u6a213D\u6570\u636e\u5e93\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6ce8\u610f\u529b\u7b56\u7565\uff08DAP\uff09\u4f7f\u7528\u5206\u5c42\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u8868\u793a\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u52a8\u6001\u6821\u51c6\u6ce8\u610f\u529b\u6743\u91cd\uff1b\u63a8\u7406\u65f6\u91c7\u7528\u9ad8\u6548\u68c0\u7d22\u7b56\u7565\uff08ERS\uff09\u8fdb\u884c\u5206\u5c42\u641c\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u6784\u5efa\u4e86\u5305\u542b200\u4e07\u6587\u672c-3D\u5bf9\u7684Align3D-2M\u6570\u636e\u96c6\u3002", "conclusion": "3DAlign-DAER\u5728\u6587\u672c-3D\u5bf9\u9f50\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\u548c\u5927\u89c4\u6a21\u68c0\u7d22\u65b9\u9762\u3002"}}
{"id": "2511.13222", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13222", "abs": "https://arxiv.org/abs/2511.13222", "authors": ["Qida Tan", "Hongyu Yang", "Wenchao Du"], "title": "Hybrid-Domain Adaptative Representation Learning for Gaze Estimation", "comment": "AAAI2026", "summary": "Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\\textbf{5.02}^{\\circ}$ and $\\textbf{3.36}^{\\circ}$, and $\\textbf{9.26}^{\\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u57df\u81ea\u9002\u5e94\u8868\u793a\u5b66\u4e60\u6846\u67b6HARL\uff0c\u901a\u8fc7\u5229\u7528\u591a\u6e90\u6df7\u5408\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u7684\u76ee\u5149\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u5916\u89c2\u7684\u76ee\u5149\u4f30\u8ba1\u5728\u8de8\u57df\u8bc4\u4f30\u4e2d\u56e0\u8868\u60c5\u3001\u4f69\u6234\u7269\u548c\u56fe\u50cf\u8d28\u91cf\u7b49\u65e0\u5173\u56e0\u7d20\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5916\u89c2\u7684\u76ee\u5149\u4f30\u8ba1\u65b9\u6cd5\u5728\u8de8\u57df\u8bc4\u4f30\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u53d7\u5230\u8868\u60c5\u3001\u4f69\u6234\u7269\u548c\u56fe\u50cf\u8d28\u91cf\u7b49\u4e0e\u76ee\u5149\u65e0\u5173\u56e0\u7d20\u7684\u5e72\u6270\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ece\u4f4e\u8d28\u91cf\u9762\u90e8\u56fe\u50cf\u4e2d\u5206\u79bb\u51fa\u76ee\u5149\u76f8\u5173\u8868\u793a\u7684\u9c81\u68d2\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u6df7\u5408\u57df\u81ea\u9002\u5e94\u8868\u793a\u5b66\u4e60\u6846\u67b6HARL\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u5f0f\u5bf9\u9f50\u9ad8\u8d28\u91cf\u8fd1\u773c\u56fe\u50cf\u7279\u5f81\u6765\u89e3\u8026\u76ee\u5149\u76f8\u5173\u8868\u793a\uff1b\u540c\u65f6\u8bbe\u8ba1\u4e86\u7a00\u758f\u56fe\u878d\u5408\u6a21\u5757\u6765\u63a2\u7d22\u76ee\u5149\u65b9\u5411\u4e0e\u5934\u90e8\u59ff\u6001\u4e4b\u95f4\u7684\u51e0\u4f55\u7ea6\u675f\u5173\u7cfb\u3002", "result": "\u5728EyeDiap\u3001MPIIFaceGaze\u548cGaze360\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u8fbe\u5230\u4e865.02\u00b0\u30013.36\u00b0\u548c9.26\u00b0\u7684\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u5e76\u5728\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u4f18\u52bf\u3002", "conclusion": "HARL\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u57df\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u51e0\u4f55\u7ea6\u675f\u5efa\u6a21\uff0c\u80fd\u591f\u6709\u6548\u5b66\u4e60\u9c81\u68d2\u7684\u76ee\u5149\u8868\u793a\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u8de8\u57df\u76ee\u5149\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2511.13322", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13322", "abs": "https://arxiv.org/abs/2511.13322", "authors": ["Senne Deproost", "Dennis Steckelmacher", "Ann Now\u00e9"], "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning", "comment": "Accepted for BNAIC/BeNeLearn 2025", "summary": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7Voronoi\u5206\u533a\u5c06\u72b6\u6001\u7a7a\u95f4\u5212\u5206\u4e3a\u533a\u57df\uff0c\u5728\u6bcf\u4e2a\u533a\u57df\u4e2d\u4f7f\u7528\u7b80\u5316\u7684\u7ebf\u6027\u6a21\u578b\u6765\u66ff\u4ee3\u590d\u6742\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u96be\u4ee5\u6ee1\u8db3\u76d1\u7ba1\u8981\u6c42\u548c\u5efa\u7acb\u4fe1\u4efb\uff0c\u9700\u8981\u5c06\u5b66\u4e60\u5230\u7684\u884c\u4e3a\u8f6c\u79fb\u5230\u4eba\u7c7b\u53ef\u8bfb\u7684\u6a21\u578b\u4e2d\u3002", "method": "\u4f7f\u7528Voronoi\u5206\u533a\u5c06\u72b6\u6001\u7a7a\u95f4\u5212\u5206\u4e3a\u533a\u57df\uff0c\u5728\u6bcf\u4e2a\u533a\u57df\u5185\u4f7f\u7528\u7ebf\u6027\u6a21\u578b\u6765\u8fd1\u4f3c\u539f\u59cb\u63a7\u5236\u5668\u7684\u884c\u4e3a\uff0c\u5b9e\u73b0\u5c40\u90e8\u4e13\u4e1a\u5316\u3002", "result": "\u5728\u7f51\u683c\u4e16\u754c\u73af\u5883\u548c\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u84b8\u998f\u5f97\u5230\u7684\u5c40\u90e8\u4e13\u4e1a\u5316\u7ebf\u6027\u6a21\u578b\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u7b56\u7565\uff0c\u6027\u80fd\u4e0e\u539f\u9ed1\u76d2\u7b56\u7565\u76f8\u5f53\u6216\u7565\u6709\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5c06\u590d\u6742\u63a7\u5236\u5668\u84b8\u998f\u4e3a\u53ef\u89e3\u91ca\u7684\u5c40\u90e8\u7ebf\u6027\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u900f\u660e\u5ea6\u3002"}}
{"id": "2511.13232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13232", "abs": "https://arxiv.org/abs/2511.13232", "authors": ["Malek Al Abed", "Sebiha Demir", "Anne Groteklaes", "Elodie Germani", "Shahrooz Faghihroohi", "Hemmen Sabir", "Shadi Albarqouni"], "title": "MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI", "comment": "5 pages, 4 figures", "summary": "Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.", "AI": {"tldr": "MRIQT\u662f\u4e00\u4e2a3D\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u4fbf\u643a\u5f0f\u8d85\u4f4e\u573aMRI\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u5230\u9ad8\u573aMRI\u6c34\u5e73\uff0c\u901a\u8fc7\u7269\u7406\u4e00\u81f4\u7684K\u7a7a\u95f4\u964d\u7ea7\u3001v-prediction\u548cSNR\u52a0\u67433D\u611f\u77e5\u635f\u5931\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u8f6c\u6362\u3002", "motivation": "\u4fbf\u643a\u5f0f\u8d85\u4f4e\u573aMRI\u5728\u65b0\u751f\u513f\u62a4\u7406\u4e2d\u5177\u6709\u53ef\u53ca\u6027\u4f18\u52bf\uff0c\u4f46\u5176\u4f4e\u4fe1\u566a\u6bd4\u548c\u8bca\u65ad\u8d28\u91cf\u5dee\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\uff0c\u9700\u8981\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u4ee5\u8fbe\u5230\u9ad8\u573aMRI\u7684\u8bca\u65ad\u6807\u51c6\u3002", "method": "\u7ed3\u5408\u7269\u7406\u4e00\u81f4\u7684K\u7a7a\u95f4\u964d\u7ea7\u6a21\u62dfuLF MRI\u3001\u4f7f\u7528v-prediction\u548c\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\u7684\u7a33\u5b9a\u56fe\u50cf\u751f\u6210\u3001SNR\u52a0\u6743\u76843D\u611f\u77e5\u635f\u5931\u4fdd\u6301\u89e3\u5256\u4fdd\u771f\u5ea6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6ce8\u610f\u529bUNet\u67b6\u6784\u7684\u4f53\u7d20\u7ea7\u53bb\u566a\u8f6c\u6362\u3002", "result": "\u5728\u65b0\u751f\u513f\u961f\u5217\u6d4b\u8bd5\u4e2d\uff0cMRIQT\u5728PSNR\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u53471.78%\uff0c\u603b\u4f53\u63d0\u534715.3%\uff0c85%\u7684\u8f93\u51fa\u88ab\u533b\u751f\u8bc4\u4e3a\u826f\u597d\u8d28\u91cf\u4e14\u75c5\u7406\u6e05\u6670\u53ef\u89c1\u3002", "conclusion": "MRIQT\u80fd\u591f\u5b9e\u73b0\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\u8d28\u91cf\u8f6c\u6362\uff0c\u4e3a\u4fbf\u643a\u5f0f\u8d85\u4f4e\u573aMRI\u63d0\u4f9b\u53ef\u9760\u7684\u8111\u90e8\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2511.13338", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13338", "abs": "https://arxiv.org/abs/2511.13338", "authors": ["Yunze Leng", "Rohan Ghosh", "Mehul Motani"], "title": "Tab-PET: Graph-Based Positional Encodings for Tabular Transformers", "comment": null, "summary": "Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\u4f4d\u7f6e\u7f16\u7801\uff08PEs\uff09\u53ef\u4ee5\u63d0\u5347\u8868\u683c\u6570\u636e\u4e0aTransformer\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u901a\u8fc7\u964d\u4f4e\u7279\u5f81\u7684\u6709\u6548\u79e9\u6765\u7b80\u5316\u4efb\u52a1\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86Tab-PET\u6846\u67b6\u6765\u4f30\u8ba1\u548c\u6ce8\u5165\u57fa\u4e8e\u56fe\u7684\u4f4d\u7f6e\u7f16\u7801\u3002", "motivation": "\u8868\u683c\u6570\u636e\u7f3a\u4e4f\u5185\u5728\u4f4d\u7f6e\u7ed3\u6784\uff0c\u963b\u788d\u4e86\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u6709\u6548\u6027\u3002\u73b0\u6709\u7684\u8868\u683cTransformer\u6a21\u578b\u901a\u5e38\u4e0d\u4f7f\u7528\u4f4d\u7f6e\u7f16\u7801\uff0c\u56e0\u4e3a\u6ca1\u6709\u5148\u9a8c\u7684\u7ed3\u6784\u4fe1\u606f\u53ef\u7528\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u7ed3\u6784\u7ebf\u7d22\uff08\u7279\u522b\u662f\u4f4d\u7f6e\u7f16\u7801\uff09\u5982\u4f55\u6539\u5584\u8868\u683cTransformer\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86Tab-PET\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u4f30\u8ba1\u548c\u6ce8\u5165\u4f4d\u7f6e\u7f16\u7801\u3002\u63a2\u7d22\u4e86\u4e24\u79cd\u56fe\u4f30\u8ba1\u8303\u5f0f\uff1a\u57fa\u4e8e\u5173\u8054\u7684\u56fe\u548c\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u56fe\uff0c\u4ece\u56fe\u62d3\u6251\u4e2d\u63a8\u5bfc\u4f4d\u7f6e\u7f16\u7801\u3002", "result": "\u572850\u4e2a\u5206\u7c7b\u548c\u56de\u5f52\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u56fe\u63a8\u5bfc\u7684\u4f4d\u7f6e\u7f16\u7801\u663e\u8457\u63d0\u5347\u4e863T\u6a21\u578b\u7684\u6027\u80fd\u3002\u57fa\u4e8e\u5173\u8054\u7684\u56fe\u6bd4\u56e0\u679c\u5173\u7cfb\u9a71\u52a8\u7684\u56fe\u4ea7\u751f\u66f4\u7a33\u5b9a\u548c\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u4f4d\u7f6e\u7f16\u7801\u5728\u8868\u683cTransformer\u4e2d\u626e\u6f14\u4e86\u610f\u60f3\u4e0d\u5230\u7684\u89d2\u8272\uff0c\u53ef\u4ee5\u901a\u8fc7\u964d\u4f4e\u7279\u5f81\u7684\u6709\u6548\u79e9\u6765\u6539\u5584\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u8868\u683c\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2511.13242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13242", "abs": "https://arxiv.org/abs/2511.13242", "authors": ["Junjie Wu", "Guohong Fu"], "title": "MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection", "comment": null, "summary": "Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.", "AI": {"tldr": "MMD-Thinker\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u7ef4\u601d\u7ef4\u89e3\u51b3\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u4e2d\u5b58\u5728\u7684\u63a8\u7406\u4e0d\u8db3\u548c\u63a8\u7406\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u968f\u7740AIGC\u65f6\u4ee3\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u6cdb\u6ee5\uff0c\u73b0\u6709\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u4e2d\u5b58\u5728\u63a8\u7406\u4e0d\u8db3\u548c\u63a8\u7406\u504f\u5dee\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u5feb\u901f\u6f14\u53d8\u7684\u590d\u6742\u865a\u5047\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u8bbe\u8ba1\u9488\u5bf9\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u5b9a\u5236\u5316\u601d\u7ef4\u6a21\u5f0f\uff1b2) \u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u6307\u4ee4\u8c03\u4f18\u5c06\u5b9a\u5236\u601d\u7ef4\u6ce8\u5165\u901a\u7528\u6a21\u578b\uff1b3) \u4f7f\u7528\u6df7\u5408\u4f18\u52bf\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002\u540c\u65f6\u6784\u5efa\u4e86\u5305\u542b8K+\u56fe\u50cf-\u6587\u672c\u5bf9\u7684MMR\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMMD-Thinker\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7075\u6d3b\u7684\u63a8\u7406\u548ctoken\u4f7f\u7528\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u591a\u7ef4\u601d\u7ef4\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u5e94\u5bf9AIGC\u65f6\u4ee3\u865a\u5047\u4fe1\u606f\u5a01\u80c1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13339", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13339", "abs": "https://arxiv.org/abs/2511.13339", "authors": ["Han Meng", "Gang Mei", "Hong Tian", "Nengxiong Xu", "Jianbing Peng"], "title": "Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model", "comment": null, "summary": "Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8868\u683c\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5ca9\u77f3\u4e0d\u8fde\u7eed\u9762\u7edf\u8ba1\u51c6\u786e\u751f\u6210\u9884\u6d4b\u7684\u7b80\u5355\u800c\u7a33\u5065\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u6761\u4ef6\u4e0b\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u5206\u5e03\u6a21\u5f0f\u3002", "motivation": "\u5ca9\u77f3\u4e0d\u8fde\u7eed\u9762\u5bf9\u5ca9\u4f53\u529b\u5b66\u884c\u4e3a\u548c\u7a33\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5185\u90e8\u5206\u5e03\u901a\u5e38\u4e0d\u53ef\u89c2\u6d4b\uff0c\u53ea\u80fd\u901a\u8fc7\u8868\u9762\u66b4\u9732\u7684\u4e0d\u8fde\u7eed\u9762\u8fdb\u884c\u751f\u6210\u9884\u6d4b\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u6355\u6349\u590d\u6742\u5206\u5e03\u6a21\u5f0f\uff0c\u8981\u4e48\u5728\u6570\u636e\u7a00\u758f\u6761\u4ef6\u4e0b\u7f3a\u4e4f\u7a33\u5065\u6027\u3002", "method": "\u5229\u7528\u4e13\u95e8\u4e3a\u5c0f\u6570\u636e\u8bbe\u8ba1\u7684\u8868\u683c\u57fa\u7840\u6a21\u578b\uff0c\u53d1\u6325\u5176\u5f3a\u5927\u7684\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u5728\u6709\u9650\u7684\u6d4b\u91cf\u4e0d\u8fde\u7eed\u9762\u4e2d\u6709\u6548\u6355\u6349\u6f5c\u5728\u7684\u590d\u6742\u5206\u5e03\u6a21\u5f0f\u3002", "result": "\u5728\u5341\u4e2a\u5177\u6709\u4e0d\u540c\u89c4\u6a21\u548c\u5206\u5e03\u6a21\u5f0f\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u548c\u6df1\u5ea6\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u5ca9\u4f53\u7ed3\u6784\u7684\u5b9a\u91cf\u8868\u5f81\uff0c\u652f\u6301\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u6570\u636e\u9a71\u52a8\u5ca9\u571f\u5de5\u7a0b\u8bbe\u8ba1\u3002"}}
{"id": "2511.13351", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13351", "abs": "https://arxiv.org/abs/2511.13351", "authors": ["Xinlan Wu", "Bin Zhu", "Feng Han", "Pengkun Jiao", "Jingjing Chen"], "title": "Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning", "comment": null, "summary": "Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u98df\u54c1\u5b66\u4e60\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ccLoRA\u67b6\u6784\u548c\u8d28\u91cf\u589e\u5f3a\u4f2a\u56de\u653e\u89e3\u51b3\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u98df\u54c1\u5206\u6790\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u98df\u54c1\u5206\u6790\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9700\u8981\u6602\u8d35\u7684\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u53ccLoRA\u67b6\u6784\uff1a\u4e13\u95e8LoRA\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u5e76\u4fdd\u6301\u4e0e\u5148\u524d\u4efb\u52a1\u5b50\u7a7a\u95f4\u7684\u6b63\u4ea4\u7ea6\u675f\uff0c\u534f\u4f5cLoRA\u901a\u8fc7\u4f2a\u56de\u653e\u6574\u5408\u8de8\u4efb\u52a1\u5171\u4eab\u77e5\u8bc6\u3002\u8d28\u91cf\u589e\u5f3a\u4f2a\u56de\u653e\u7b56\u7565\u5229\u7528\u81ea\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u51cf\u5c11\u751f\u6210\u6837\u672c\u4e2d\u7684\u5e7b\u89c9\u3002", "result": "\u5728\u7efc\u5408Uni-Food\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u7f13\u89e3\u9057\u5fd8\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u662f\u9996\u4e2a\u9488\u5bf9\u590d\u6742\u98df\u54c1\u4efb\u52a1\u7684\u6709\u6548\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u98df\u54c1\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u8425\u517b\u548c\u6162\u6027\u75c5\u9884\u9632\u7b49\u5065\u5eb7\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13259", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13259", "abs": "https://arxiv.org/abs/2511.13259", "authors": ["Yushuo Zheng", "Jiangyong Ying", "Huiyu Duan", "Chunyi Li", "Zicheng Zhang", "Jing Liu", "Xiaohong Liu", "Guangtao Zhai"], "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models", "comment": null, "summary": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.", "AI": {"tldr": "GeoX-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u548c\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b10,859\u4e2a\u5168\u666f-\u536b\u661f\u56fe\u50cf\u5bf9\u548c755,976\u4e2a\u95ee\u7b54\u5bf9\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u548c\u59ff\u6001\u4f30\u8ba1\u9886\u57df\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u800c\u8fd9\u4e9b\u80fd\u529b\u5bf9\u5bfc\u822a\u3001\u81ea\u52a8\u9a7e\u9a76\u3001\u6237\u5916\u673a\u5668\u4eba\u7b49\u5e94\u7528\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b128\u4e2a\u57ce\u5e02\u300149\u4e2a\u56fd\u5bb6\u768410,859\u4e2a\u5168\u666f-\u536b\u661f\u56fe\u50cf\u5bf9\u6570\u636e\u96c6\uff0c\u5e76\u521b\u5efa\u4e86755,976\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5176\u4e2d42,900\u4e2a\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002\u8bc4\u4f30\u4e8625\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u6548\u679c\u3002", "result": "\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u66f4\u590d\u6742\u7684\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u4e0b\u964d\u3002\u901a\u8fc7GeoX-Bench\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u8de8\u89c6\u89d2\u5730\u7406\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "GeoX-Bench\u63ed\u793a\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u8de8\u89c6\u89d2\u5730\u7406\u5b9a\u4f4d\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u7684\u80fd\u529b\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u9700\u8981\u672a\u6765\u6539\u8fdb\uff0c\u6307\u4ee4\u8c03\u4f18\u662f\u63d0\u5347\u8fd9\u4e9b\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.13373", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13373", "abs": "https://arxiv.org/abs/2511.13373", "authors": ["Prakrit Timilsina", "Anuj Nepal", "Rajan Kadel", "Robin Doss"], "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs", "comment": null, "summary": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u53c2\u6570\u7a7a\u95f4\u5408\u5e76\u6280\u672f\u5e94\u7528\u4e8e\u4e24\u4e2a\u57fa\u4e8eMistral-7B\u7684\u533b\u7597LLM\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u52a0\u6743\u63d2\u503c\u7684\u65b0\u65b9\u6cd5\uff0c\u53d1\u73b0\u5bf9\u4e8e\u67b6\u6784\u517c\u5bb9\u7684\u6a21\u578b\uff0c\u7b80\u5355\u5e73\u5747\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u533b\u7597\u4e2dLLM\u9762\u4e34\u7684\u6311\u6218\uff1a\u6574\u5408\u8de8\u673a\u6784\u4e13\u4e1a\u77e5\u8bc6\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3001\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3001\u9632\u6b62\u6a21\u578b\u66f4\u65b0\u65f6\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u8bc4\u4f30\u516d\u79cd\u53c2\u6570\u5408\u5e76\u6280\u672f\uff08\u4efb\u52a1\u7b97\u672f\u3001\u7ebf\u6027\u5e73\u5747\u3001DARE-TIES\u3001DELLA\u3001Breadcrumbs\u548c\u63d0\u51fa\u7684\u5206\u5c42\u65b9\u6cd5\uff09\uff0c\u65b0\u65b9\u6cd5\u7ed3\u5408\u9009\u62e9\u6027\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u6ce8\u610f\u529b\u5c42\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u52a0\u6743\u63d2\u503c\u3002", "result": "\u67b6\u6784\u517c\u5bb9\u6a21\u578b\u4ece\u7b80\u5355\u5e73\u5747\u65b9\u6cd5\u4e2d\u83b7\u76ca\u663e\u8457\uff0c\u4efb\u52a1\u7b97\u672f\u5728MedQA\u4e0a\u8fbe\u523045.80%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u590d\u6742\u7684\u526a\u679d\u65b9\u6cd5\u3002", "conclusion": "\u5bf9\u4e8e\u67b6\u6784\u517c\u5bb9\u6a21\u578b\uff0c\u7b80\u5355\u5e73\u5747\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u77e5\u8bc6\u6574\u5408\u57fa\u7ebf\uff0c\u4e3a\u53ef\u6269\u5c55\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2511.13261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13261", "abs": "https://arxiv.org/abs/2511.13261", "authors": ["Junlong Li", "Huaiyuan Xu", "Sijie Cheng", "Kejun Wu", "Kim-Hui Yap", "Lap-Pui Chau", "Yi Wang"], "title": "Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges", "comment": "26 pages, 8 figures, 8 tables, Under peer-review", "summary": "Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u7a0b\u5e8fAI\u52a9\u624b\uff08EgoProceAssist\uff09\u6982\u5ff5\uff0c\u65e8\u5728\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u9010\u6b65\u652f\u6301\u65e5\u5e38\u7a0b\u5e8f\u6027\u4efb\u52a1\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e09\u4e2a\u6838\u5fc3\u4efb\u52a1\uff1a\u81ea\u6211\u4e2d\u5fc3\u7a0b\u5e8f\u9519\u8bef\u68c0\u6d4b\u3001\u81ea\u6211\u4e2d\u5fc3\u7a0b\u5e8f\u5b66\u4e60\u548c\u81ea\u6211\u4e2d\u5fc3\u7a0b\u5e8f\u95ee\u7b54\u3002", "motivation": "\u53d7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u6211\u4e2d\u5fc3\u611f\u77e5\u7814\u7a76\u7684\u8fdb\u5c55\u63a8\u52a8\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u65e5\u5e38\u7a0b\u5e8f\u6027\u4efb\u52a1\u7684AI\u52a9\u624b\u3002", "method": "\u901a\u8fc7\u5168\u9762\u56de\u987e\u5f53\u524d\u6280\u672f\u3001\u76f8\u5173\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5efa\u7acb\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf9\u4ee3\u8868\u6027VLM\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u8bc6\u522b\u4e86\u73b0\u6709VLM\u52a9\u624b\u4e0eEgoProceAssist\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6280\u672f\u5206\u6790\u548c\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u672a\u6765\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u5efa\u7acb\u4e86\u6301\u7eed\u66f4\u65b0\u7684\u516c\u5f00\u8d44\u6e90\u5e93\u6765\u6536\u96c6\u6700\u65b0\u7814\u7a76\u6210\u679c\u3002"}}
{"id": "2511.13391", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13391", "abs": "https://arxiv.org/abs/2511.13391", "authors": ["Chengdong Ma", "Th\u00e9o Tao Zhaowei", "Pengyu Li", "Minghao Liu", "Haojun Chen", "Zihao Mao", "Yuan Cheng", "Yuan Qi", "Yaodong Yang"], "title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning", "comment": null, "summary": "Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPackingStar\u7684AI\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u4eb2\u543b\u6570\u95ee\u9898\u5efa\u6a21\u4e3a\u53cc\u73a9\u5bb6\u77e9\u9635\u5b8c\u6210\u6e38\u620f\uff0c\u4f7f\u7528\u535a\u5f08\u8bba\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u9ad8\u7ef4\u7a7a\u95f4\uff0c\u572825-31\u7ef4\u5ea6\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u5df2\u77e5\u8bb0\u5f55\uff0c\u5e76\u53d1\u73b0\u4e866000\u591a\u4e2a\u65b0\u7ed3\u6784\u3002", "motivation": "\u4eb2\u543b\u6570\u95ee\u9898\u81ea1694\u5e74\u725b\u987f\u7814\u7a76\u4ee5\u6765\u4e00\u76f4\u662f\u57fa\u7840\u6027\u6311\u6218\uff0c\u4ee3\u8868\u4e86\u5e0c\u5c14\u4f2f\u7279\u7b2c18\u95ee\u9898\u7684\u5c40\u90e8\u7248\u672c\u3002\u9ad8\u7ef4\u51e0\u4f55\u7684\u4e0d\u89c4\u5219\u6027\u548c\u6307\u6570\u7ea7\u589e\u957f\u7684\u7ec4\u5408\u590d\u6742\u6027\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5c06\u4eb2\u543b\u6570\u95ee\u9898\u5efa\u6a21\u4e3a\u53cc\u73a9\u5bb6\u77e9\u9635\u5b8c\u6210\u6e38\u620f\uff1a\u4e00\u4e2a\u73a9\u5bb6\u586b\u5145\u77e9\u9635\u6761\u76ee\uff08\u8868\u793a\u7403\u5fc3\u5411\u91cf\u5bf9\u4f59\u5f26\uff09\uff0c\u53e6\u4e00\u4e2a\u73a9\u5bb6\u4fee\u6b63\u6b21\u4f18\u6761\u76ee\uff0c\u5171\u540c\u6700\u5927\u5316\u77e9\u9635\u5927\u5c0f\uff08\u5bf9\u5e94\u4eb2\u543b\u6570\uff09\u3002\u4f7f\u7528\u535a\u5f08\u8bba\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edfPackingStar\u9ad8\u6548\u63a2\u7d22\u9ad8\u7ef4\u7a7a\u95f4\u3002", "result": "PackingStar\u91cd\u73b0\u4e86\u5148\u524d\u914d\u7f6e\uff0c\u572825-31\u7ef4\u5ea6\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u4eba\u7c7b\u5df2\u77e5\u8bb0\u5f55\uff0c\u5176\u4e2d25\u7ef4\u914d\u7f6e\u51e0\u4f55\u5bf9\u5e94Leech\u683c\u5e76\u6697\u793a\u53ef\u80fd\u7684\u6700\u4f18\u6027\u3002\u572813\u7ef4\u5ea6\u4e0a\u9996\u6b21\u7a81\u78341971\u5e74\u4ee5\u6765\u7684\u6709\u7406\u7ed3\u6784\uff0c\u572814\u7ef4\u548c\u5176\u4ed6\u7ef4\u5ea6\u53d1\u73b0\u4e866000\u591a\u4e2a\u65b0\u7ed3\u6784\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5c55\u793a\u4e86AI\u63a2\u7d22\u8d85\u8d8a\u4eba\u7c7b\u76f4\u89c9\u7684\u9ad8\u7ef4\u7a7a\u95f4\u7684\u80fd\u529b\uff0c\u4e3a\u4eb2\u543b\u6570\u95ee\u9898\u548c\u66f4\u5e7f\u6cdb\u7684\u51e0\u4f55\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.13264", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.13264", "abs": "https://arxiv.org/abs/2511.13264", "authors": ["Keshav Gupta", "Akshat Sanghvi", "Shreyas Reddy Palley", "Astitva Srivastava", "Charu Sharma", "Avinash Sharma"], "title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression", "comment": "Project Page: https://symgs.github.io/", "summary": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \\textbf{\\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \\textbf{\\color{cyan}{symgs.github.io}}", "AI": {"tldr": "SymGS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u79f0\u611f\u77e5\u76843D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u955c\u50cf\u6765\u6d88\u9664\u53cd\u5c04\u5197\u4f59\uff0c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u6e32\u67d3\u901f\u5ea6\u548c\u771f\u5b9e\u611f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5185\u5b58\u5360\u7528\u968f\u573a\u666f\u590d\u6742\u5ea6\u5feb\u901f\u589e\u957f\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u539f\u59cb\u7ea7\u522b\u7684\u5197\u4f59\u8fdb\u884c\u538b\u7f29\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u63d0\u51faSymGS\u6846\u67b6\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u955c\u50cf\u6765\u6d88\u9664\u5c40\u90e8\u548c\u5168\u5c40\u7684\u53cd\u5c04\u5197\u4f59\uff0c\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u589e\u5f3a\u6a21\u5757\u4e0e\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\uff08\u5982HAC\uff09\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u76f8\u6bd4HAC\u65b9\u6cd5\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b01.66\u500d\u538b\u7f29\uff08\u5927\u89c4\u6a21\u573a\u666f\u53ef\u8fbe3\u500d\uff09\uff0c\u5e73\u5747\u5b9e\u73b0108\u500d\u76843DGS\u573a\u666f\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "SymGS\u901a\u8fc7\u5bf9\u79f0\u611f\u77e5\u6280\u672f\u6210\u529f\u7a81\u7834\u4e86\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u4e3a3D\u9ad8\u65af\u6cfc\u6e85\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13394", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13394", "abs": "https://arxiv.org/abs/2511.13394", "authors": ["Vasilis Gkolemis", "Christos Diou", "Michael Gutmann"], "title": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo", "comment": null, "summary": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53ef\u5fae\u5206\u6a21\u62df\u5668\u7684\u8d1d\u53f6\u65af\u53c2\u6570\u63a8\u65ad\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u968f\u673a\u6a21\u62df\u8f6c\u5316\u4e3a\u786e\u5b9a\u6027\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u68af\u5ea6\u65b9\u6cd5\u9ad8\u6548\u5bfc\u822a\u540e\u9a8c\u9ad8\u5bc6\u5ea6\u533a\u57df\uff0c\u663e\u8457\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "\u590d\u6742\u968f\u673a\u6a21\u62df\u5668\u7684\u8d1d\u53f6\u65af\u53c2\u6570\u63a8\u65ad\u9762\u4e34\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u5904\u7406\u7684\u95ee\u9898\uff0c\u73b0\u6709\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u65ad\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6a21\u62df\uff0c\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u6216\u90e8\u5206\u65e0\u4fe1\u606f\u8f93\u51fa\u95ee\u9898\u4e2d\u6210\u672c\u9ad8\u6602\u3002", "method": "\u57fa\u4e8e\u4f18\u5316\u8499\u7279\u5361\u6d1b\u6846\u67b6\uff0c\u5c06\u968f\u673a\u6a21\u62df\u91cd\u65b0\u8868\u8ff0\u4e3a\u786e\u5b9a\u6027\u4f18\u5316\u95ee\u9898\uff0c\u5e94\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u9ad8\u6548\u5bfc\u822a\u540e\u9a8c\u9ad8\u5bc6\u5ea6\u533a\u57df\uff0c\u907f\u514d\u5728\u4f4e\u6982\u7387\u533a\u57df\u6d6a\u8d39\u6a21\u62df\uff0c\u5e76\u4f7f\u7528JAX\u5b9e\u73b0\u5173\u952e\u7ec4\u4ef6\u7684\u5411\u91cf\u5316\u3002", "result": "\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u3001\u65e0\u4fe1\u606f\u8f93\u51fa\u3001\u591a\u89c2\u6d4b\u548c\u591a\u5cf0\u540e\u9a8c\u7b49\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u5339\u914d\u5e76\u7ecf\u5e38\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u5fae\u5206\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u8d1d\u53f6\u65af\u53c2\u6570\u63a8\u65ad\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.13269", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13269", "abs": "https://arxiv.org/abs/2511.13269", "authors": ["Lingfeng Zhang", "Yuchen Zhang", "Hongsheng Li", "Haoxiang Fu", "Yingbo Tang", "Hangjun Ye", "Long Chen", "Xiaojun Liang", "Xiaoshuai Hao", "Wenbo Ding"], "title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation", "comment": null, "summary": "Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SpatialSky-Bench\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86Sky-VLM\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u548c\u89e3\u91ca\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86SpatialSky-Bench\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b\u73af\u5883\u611f\u77e5\u548c\u573a\u666f\u7406\u89e3\u4e24\u4e2a\u7c7b\u522b13\u4e2a\u5b50\u7c7b\u522b\uff09\uff0c\u521b\u5efa\u4e86\u5305\u542b100\u4e07\u6837\u672c\u7684SpatialSky-Dataset\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e13\u95e8\u7528\u4e8e\u65e0\u4eba\u673a\u7a7a\u95f4\u63a8\u7406\u7684Sky-VLM\u6a21\u578b\u3002", "result": "\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u65e0\u4eba\u673a\u5bfc\u822a\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u7406\u60f3\uff0c\u800cSky-VLM\u5728\u6240\u6709\u57fa\u51c6\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "Sky-VLM\u4e3a\u5f00\u53d1\u9002\u7528\u4e8e\u65e0\u4eba\u673a\u573a\u666f\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2511.13419", "categories": ["cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2511.13419", "abs": "https://arxiv.org/abs/2511.13419", "authors": ["Shaheen Mohammed Saleh Ahmed", "Hakan Hakan Guneyli"], "title": "MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction", "comment": null, "summary": "Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data", "AI": {"tldr": "\u63d0\u51faMMWSTM-ADRAN+\u53cc\u6d41\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7ed3\u5408\u5929\u6c14\u72b6\u6001\u8f6c\u6362\u6a21\u578b\u548c\u5f02\u5e38\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u6781\u7aef\u6c14\u6e29\u4e8b\u4ef6\u9884\u6d4b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u6781\u7aef\u6c14\u6e29\u4e8b\u4ef6\u5728\u6c14\u5019\u98ce\u9669\u7ba1\u7406\u4e2d\u5177\u6709\u6839\u672c\u6027\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u53cc\u6d41\u67b6\u6784\uff1aMMWSTM\u6d41\u7ed3\u5408\u53cc\u5411LSTM\u548c\u53ef\u5b66\u4e60\u9a6c\u5c14\u53ef\u592b\u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u6355\u83b7\u5929\u6c14\u72b6\u6001\u53d8\u5316\uff1bADRAN\u6d41\u96c6\u6210\u53cc\u5411GRU\u3001\u591a\u5934\u81ea\u6ce8\u610f\u529b\u548c\u5f02\u5e38\u653e\u5927\u5c42\u589e\u5f3a\u5bf9\u4f4e\u6982\u7387\u4fe1\u53f7\u7684\u654f\u611f\u6027\uff1b\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u95e8\u81ea\u9002\u5e94\u786e\u5b9a\u5404\u6d41\u8d21\u732e\u3002", "result": "\u6a21\u578b\u91c7\u7528\u5b9a\u5236\u5316\u7684ExtremeWeatherLoss\u51fd\u6570\u5bf9\u6e29\u5ea6\u5206\u5e03\u4e0a\u4e0b5%\u7684\u8bef\u5dee\u8fdb\u884c\u52a0\u6743\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u589e\u5f3a\u5c06\u8bad\u7ec3\u6570\u636e\u6709\u6548\u6269\u5145\u56db\u500d\u3002", "conclusion": "\u8be5\u67b6\u6784\u901a\u8fc7\u8026\u5408\u5929\u6c14\u72b6\u6001\u52a8\u6001\u6a21\u578b\u548c\u5f02\u5e38\u805a\u7126\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3a\u6781\u7aef\u6c14\u6e29\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13278", "abs": "https://arxiv.org/abs/2511.13278", "authors": ["Zihan Li", "Tengfei Wang", "Wentian Gan", "Hao Zhan", "Xin Wang", "Zongqian Zhan"], "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting", "comment": null, "summary": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/", "AI": {"tldr": "SF-Recon\u662f\u4e00\u79cd\u76f4\u63a5\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u8f7b\u91cf\u7ea7\u5efa\u7b51\u8868\u9762\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u540e\u5904\u7406\u7f51\u683c\u7b80\u5316\uff0c\u901a\u8fc73D\u9ad8\u65af\u6e85\u5c04\u548c\u7ed3\u6784\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u591a\u89c6\u89d2\u51e0\u4f55\u6d41\u7a0b\u4f9d\u8d56\u5bc6\u96c6\u91cd\u5efa\u3001\u7f51\u683c\u5316\u548c\u540e\u7eed\u7b80\u5316\uff0c\u8fc7\u7a0b\u7e41\u7410\u4e14\u8d28\u91cf\u654f\u611f\uff0c\u9700\u8981\u76f4\u63a5\u91cd\u5efa\u8f7b\u91cf\u7ea7\u5efa\u7b51\u8868\u9762\u6a21\u578b\u3002", "method": "\u9996\u5148\u8bad\u7ec33D\u9ad8\u65af\u6e85\u5c04\u573a\u83b7\u5f97\u89c6\u56fe\u4e00\u81f4\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u6cd5\u5411\u68af\u5ea6\u5f15\u5bfc\u7684\u9ad8\u65af\u4f18\u5316\u9009\u62e9\u4e0e\u5c4b\u9876\u548c\u5899\u58c1\u8fb9\u754c\u5bf9\u9f50\u7684\u57fa\u5143\uff0c\u518d\u901a\u8fc7\u591a\u89c6\u89d2\u8fb9\u7f18\u4e00\u81f4\u6027\u4fee\u526a\u589e\u5f3a\u7ed3\u6784\u9510\u5ea6\uff0c\u6700\u540e\u7528\u591a\u89c6\u89d2\u6df1\u5ea6\u7ea6\u675f\u7684Delaunay\u4e09\u89d2\u5316\u5c06\u7ed3\u6784\u5316\u9ad8\u65af\u573a\u8f6c\u6362\u4e3a\u8f7b\u91cf\u7ea7\u5efa\u7b51\u7f51\u683c\u3002", "result": "\u5728\u63d0\u51fa\u7684SF\u6570\u636e\u96c6\u4e0a\uff0cSF-Recon\u80fd\u591f\u76f4\u63a5\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u8f7b\u91cf\u7ea7\u5efa\u7b51\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u9762\u548c\u9876\u70b9\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "SF-Recon\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u8f7b\u91cf\u7ea7\u3001\u7ed3\u6784\u4fdd\u771f\u7684\u5efa\u7b51\u7f51\u683c\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6d41\u7a0b\u4e2d\u7684\u7e41\u7410\u540e\u5904\u7406\u6b65\u9aa4\u3002"}}
{"id": "2511.13421", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13421", "abs": "https://arxiv.org/abs/2511.13421", "authors": ["Tingkai Yan", "Haodong Wen", "Binghui Li", "Kairong Luo", "Wenguang Chen", "Kaifeng Lyu"], "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression", "comment": null, "summary": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($\u0398(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6570\u636e\u6709\u9650\u4e14\u91cd\u590d\u8bad\u7ec3\u591a\u4e2aepoch\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u3002\u901a\u8fc7\u7ebf\u6027\u56de\u5f52\u7684\u7406\u8bba\u5206\u6790\uff0c\u5b9a\u4e49\u4e86\"\u6709\u6548\u91cd\u7528\u7387\"E(K,N)\u6765\u8861\u91cf\u591aepoch\u8bad\u7ec3\u4e0e\u5355\u6b21\u8bad\u7ec3\u7684\u6027\u80fd\u7b49\u4ef7\u5173\u7cfb\uff0c\u53d1\u73b0\u5f53K\u8f83\u5c0f\u65f6E(K,N)\u2248K\uff0c\u4f46\u968f\u7740K\u589e\u52a0\u4f1a\u8fbe\u5230\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u5e73\u53f0\u671f\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u7f29\u653e\u5b9a\u5f8b\u4e3b\u8981\u7814\u7a76\u4e00\u6b21\u6027\u4f7f\u7528\u6d77\u91cf\u6570\u636e\u7684\u60c5\u51b5\uff0c\u4f46\u5728\u5b9e\u9645\u8bad\u7ec3\u4e2d\u7ecf\u5e38\u9700\u8981\u91cd\u590d\u4f7f\u7528\u6709\u9650\u6570\u636e\u8fdb\u884c\u591a\u8f6e\u8bad\u7ec3\u3002\u8fd9\u79cd\u6570\u636e\u91cd\u7528\u5bf9\u7f29\u653e\u5b9a\u5f8b\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u5728\u5f3a\u51f8\u6027\u6216Zipf\u5206\u5e03\u6570\u636e\u4e0b\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u7814\u7a76SGD\u8bad\u7ec3\u4e2d\u6570\u636e\u6709\u6548\u91cd\u7528\u7387E(K,N)\u7684\u7f29\u653e\u884c\u4e3a\u3002", "result": "\u5f53K\u8f83\u5c0f\u65f6\uff0cE(K,N)\u2248K\uff0c\u6bcf\u4e2a\u65b0epoch\u5e26\u6765\u7ebf\u6027\u589e\u76ca\uff1b\u968f\u7740K\u589e\u52a0\uff0cE(K,N)\u4f1a\u8fbe\u5230\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u5e73\u53f0\u671f\uff0c\u8be5\u5e73\u53f0\u503c\u968fN\u589e\u957f\uff08\u5f3a\u51f8\u60c5\u51b5\u4e0b\u4e3a\u0398(log N)\uff09\u3002", "conclusion": "\u6570\u636e\u91cd\u7528\u7684\u8fb9\u9645\u6548\u76ca\u53d6\u51b3\u4e8e\u6570\u636e\u89c4\u6a21\u548c\u6570\u636e\u5206\u5e03\uff0c\u9700\u8981\u660e\u786e\u5efa\u6a21\u8fd9\u4e24\u4e2a\u56e0\u7d20\u6765\u7814\u7a76\u6570\u636e\u91cd\u7528\u4e0b\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u8fd9\u5bf9\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u6548\u7387\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.13444", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13444", "abs": "https://arxiv.org/abs/2511.13444", "authors": ["Zhipeng Ma", "Bo N\u00f8rregaard J\u00f8rgensen", "Zheng Grace Ma"], "title": "Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes", "comment": null, "summary": "Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5377\u79ef\u805a\u7c7b\u7684\u65e0\u76d1\u7763\u5de5\u4e1a\u65f6\u5e8f\u6570\u636e\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65f6\u5e8f\u6570\u636e\u8f6c\u6362\u4e3a\u7070\u5ea6\u56fe\u50cf\u8868\u793a\uff0c\u7ed3\u5408\u8f6f\u786c\u805a\u7c7b\u548c\u590d\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u6709\u6548\u53d1\u73b0\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u7684\u64cd\u4f5c\u6a21\u5f0f\u3002", "motivation": "\u5de5\u4e1a\u8fc7\u7a0b\u76d1\u63a7\u4e2d\u7684\u4f20\u611f\u5668\u65f6\u5e8f\u6570\u636e\u7f3a\u4e4f\u6807\u7b7e\u3001\u53d8\u5316\u5927\u4e14\u566a\u58f0\u591a\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6a21\u5f0f\u3002\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u56fa\u5b9a\u8ddd\u79bb\u5ea6\u91cf\uff0c\u8981\u4e48\u662f\u4e3a\u9759\u6001\u6570\u636e\u8bbe\u8ba1\u7684\u6df1\u5ea6\u6a21\u578b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u3001\u975e\u7ed3\u6784\u5316\u7684\u5de5\u4e1a\u5e8f\u5217\u3002", "method": "\u5c06\u539f\u59cb\u65f6\u5e8f\u5e8f\u5217\u901a\u8fc7\u91cd\u53e0\u6ed1\u52a8\u7a97\u53e3\u8f6c\u6362\u4e3a\u7070\u5ea6\u77e9\u9635\u8868\u793a\uff0c\u4f7f\u7528\u6df1\u5ea6\u5377\u79ef\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff1b\u96c6\u6210\u8f6f\u786c\u805a\u7c7b\u8f93\u51fa\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u8fdb\u884c\u7ec6\u5316\uff1b\u4f7f\u7528\u65b0\u5f00\u53d1\u7684\u590d\u5408\u8bc4\u5206S_eva\uff08\u7ed3\u5408\u5f52\u4e00\u5316\u8f6e\u5ed3\u7cfb\u6570\u3001Calinski-Harabasz\u548cDavies-Bouldin\u6307\u6570\uff09\u5ba2\u89c2\u8bc4\u4f30\u805a\u7c7b\u6027\u80fd\u3002", "result": "\u5728\u6765\u81ea\u5317\u6b27\u94f8\u9020\u5382\u76843900\u591a\u4e2a\u7194\u7089\u64cd\u4f5c\u6570\u636e\u4e0a\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u8bc6\u522b\u51fa7\u4e2a\u53ef\u89e3\u91ca\u7684\u64cd\u4f5c\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u80fd\u8017\u3001\u70ed\u52a8\u529b\u5b66\u548c\u751f\u4ea7\u6301\u7eed\u65f6\u95f4\u7684\u663e\u8457\u5dee\u5f02\u3002\u4e0e\u7ecf\u5178\u548c\u6df1\u5ea6\u805a\u7c7b\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6574\u4f53\u6027\u80fd\u3001\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9886\u57df\u5bf9\u9f50\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u65f6\u5e8f\u5206\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u5e8f\u5217\u4e0d\u89c4\u5219\u6027\u3001\u6a21\u5f0f\u91cd\u53e0\u548c\u5ea6\u91cf\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u8bca\u65ad\u548c\u80fd\u6e90\u4f18\u5316\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13283", "abs": "https://arxiv.org/abs/2511.13283", "authors": ["Jongha Kim", "Minseong Bae", "Sanghyeok Lee", "Jinsung Yoon", "Hyunwoo J. Kim"], "title": "TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing", "comment": "AAAI 2026 (Main Technical Track)", "summary": "Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.", "AI": {"tldr": "TabFlash\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u95ee\u9898\u6761\u4ef6\u5316\u3001\u526a\u679d\u7b56\u7565\u548ctoken\u805a\u7126\u8bad\u7ec3\u6765\u89e3\u51b3\u8868\u683c\u56fe\u50cf\u7406\u89e3\u4e2d\u7684\u5197\u4f59\u548c\u6548\u7387\u95ee\u9898\uff0c\u5728\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u8868\u683c\u56fe\u50cf\u7406\u89e3\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u9700\u8981\u95ee\u9898\u7279\u5b9a\u7684\u5173\u6ce8\u70b9\uff0c\u4ee5\u53ca\u5b58\u5728\u5197\u4f59\u80cc\u666f\u533a\u57df\u3002\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u8fd9\u4e9b\u7279\u6027\uff0c\u5bfc\u81f4\u89c6\u89c9\u8868\u793a\u4e0d\u591f\u4fe1\u606f\u4e30\u5bcc\u4e14\u5197\u4f59\u3002", "method": "1. \u6e10\u8fdb\u5f0f\u95ee\u9898\u6761\u4ef6\u5316\uff1a\u5c06\u95ee\u9898\u4fe1\u606f\u4ee5\u9010\u6e10\u589e\u52a0\u7684\u9891\u7387\u6ce8\u5165Vision Transformer\u5404\u5c42\uff1b2. \u526a\u679d\u7b56\u7565\uff1a\u4e22\u5f03\u80cc\u666ftoken\u4ee5\u63d0\u9ad8\u6548\u7387\uff1b3. Token\u805a\u7126\uff1a\u8bad\u7ec3\u7b56\u7565\u4fc3\u4f7f\u6a21\u578b\u5c06\u5173\u952e\u4fe1\u606f\u96c6\u4e2d\u5728\u4fdd\u7559\u7684token\u4e2d\u3002", "result": "TabFlash\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u5f00\u6e90\u548c\u4e13\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u76f8\u6bd4\u7b2c\u4e8c\u597d\u7684\u6a21\u578b\u51cf\u5c11\u4e8627%\u7684FLOPs\u548c30%\u7684\u5185\u5b58\u4f7f\u7528\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6e10\u8fdb\u5f0f\u95ee\u9898\u6761\u4ef6\u5316\u3001\u526a\u679d\u548ctoken\u805a\u7126\u8bad\u7ec3\uff0cTabFlash\u80fd\u591f\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u4e14\u7d27\u51d1\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8868\u683c\u7406\u89e3\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2511.13453", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.13453", "abs": "https://arxiv.org/abs/2511.13453", "authors": ["Iulius Gherasim", "Carlos Garc\u00eda S\u00e1nchez"], "title": "Hardware optimization on Android for inference of AI models", "comment": "8 pages", "summary": "The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76Android\u7cfb\u7edf\u4e0aAI\u6a21\u578b\u7684\u6700\u4f18\u6267\u884c\u914d\u7f6e\uff0c\u91cd\u70b9\u5173\u6ce8\u76ee\u6807\u68c0\u6d4b(YOLO\u7cfb\u5217)\u548c\u56fe\u50cf\u5206\u7c7b(ResNet)\u4efb\u52a1\uff0c\u8bc4\u4f30\u6a21\u578b\u91cf\u5316\u65b9\u6848\u548c\u8bbe\u5907\u52a0\u901f\u5668(GPU/NPU)\u7684\u4f7f\u7528\uff0c\u65e8\u5728\u5b9e\u73b0\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\u5316\u548c\u63a8\u7406\u901f\u5ea6\u6700\u5927\u5316\u7684\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u5728\u79fb\u52a8\u8ba1\u7b97\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f18\u5316\u79fb\u52a8\u7528\u6237\u4f53\u9a8c\u9700\u8981\u6700\u5c0f\u5ef6\u8fdf\u548c\u9ad8\u54cd\u5e94\u6027\u3002\u9762\u4e34\u7684\u6311\u6218\u5305\u62ec\u5145\u5206\u5229\u7528\u5b9e\u65f6\u7ea6\u675f\u7684\u6267\u884c\u7b56\u7565\u548c\u5f02\u6784\u786c\u4ef6\u67b6\u6784\u7684\u5229\u7528\u3002", "method": "\u7814\u7a76\u5e76\u63d0\u51fa\u4e86Android\u7cfb\u7edf\u4e0aAI\u6a21\u578b\u7684\u6700\u4f18\u6267\u884c\u914d\u7f6e\uff0c\u8bc4\u4f30\u4e86\u5404\u79cd\u6a21\u578b\u91cf\u5316\u65b9\u6848\u4ee5\u53caGPU\u548cNPU\u8bbe\u5907\u52a0\u901f\u5668\u7684\u4f7f\u7528\uff0c\u91cd\u70b9\u5173\u6ce8YOLO\u7cfb\u5217\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u548cResNet\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u786e\u5b9a\u4e86\u5728\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931\u548c\u6700\u5927\u63a8\u7406\u52a0\u901f\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u7684\u7ec4\u5408\u914d\u7f6e\u3002", "conclusion": "\u627e\u5230\u4e86\u5728Android\u7cfb\u7edf\u4e0a\u90e8\u7f72AI\u6a21\u578b\u65f6\uff0c\u6a21\u578b\u91cf\u5316\u4e0e\u786c\u4ef6\u52a0\u901f\u5668\u4f7f\u7528\u7684\u6700\u4f73\u914d\u7f6e\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u4e0e\u6027\u80fd\u7684\u826f\u597d\u6743\u8861\u3002"}}
{"id": "2511.13285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13285", "abs": "https://arxiv.org/abs/2511.13285", "authors": ["Yunjie Yu", "Jingchen Wu", "Junchen Zhu", "Chunze Lin", "Guibin Chen"], "title": "SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design", "comment": null, "summary": "Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.", "AI": {"tldr": "SkyReels-Text\u662f\u4e00\u4e2a\u65e0\u9700\u5b57\u4f53\u6807\u7b7e\u6216\u5fae\u8c03\u7684\u5b57\u4f53\u53ef\u63a7\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u6d77\u62a5\u6587\u672c\u7f16\u8f91\uff0c\u652f\u6301\u540c\u65f6\u7f16\u8f91\u591a\u4e2a\u6587\u672c\u533a\u57df\u5e76\u4fdd\u6301\u975e\u7f16\u8f91\u533a\u57df\u7684\u89c6\u89c9\u5916\u89c2\u3002", "motivation": "\u89e3\u51b3\u4e13\u4e1a\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u73b0\u4ee3\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u3001\u5b57\u4f53\u611f\u77e5\u6587\u672c\u64cd\u4f5c\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u4fdd\u6301\u89c6\u89c9\u548c\u8c10\u548c\u6392\u7248\u610f\u56fe\u7684\u540c\u65f6\u5feb\u901f\u7cbe\u786e\u4fee\u6539\u6587\u672c\u5185\u5bb9\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b57\u4f53\u53ef\u63a7\u6846\u67b6\uff0c\u7528\u6237\u53ea\u9700\u63d0\u4f9b\u6240\u9700\u5b57\u4f53\u7684\u88c1\u526a\u5b57\u5f62\u8865\u4e01\uff0c\u65e0\u9700\u5b57\u4f53\u6807\u7b7e\u6216\u63a8\u7406\u65f6\u5fae\u8c03\uff0c\u5373\u53ef\u540c\u65f6\u7f16\u8f91\u591a\u4e2a\u4e0d\u540c\u6392\u7248\u98ce\u683c\u7684\u6587\u672c\u533a\u57df\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5305\u62ec\u624b\u5199\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSkyReels-Text\u5728\u6587\u672c\u4fdd\u771f\u5ea6\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5f25\u5408\u4e86\u901a\u7528\u56fe\u50cf\u7f16\u8f91\u4e0e\u4e13\u4e1a\u7ea7\u6392\u7248\u8bbe\u8ba1\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5b57\u4f53\u65cf\u548c\u98ce\u683c\u7ec6\u5fae\u5dee\u522b\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2511.13457", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13457", "abs": "https://arxiv.org/abs/2511.13457", "authors": ["Bin Liu", "Qinghao Zhao", "Yuxi Zhou", "Zhejun Sun", "Kaijie Lei", "Deyun Zhang", "Shijia Geng", "Shenda Hong"], "title": "Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure", "comment": "19 pages, 5 figures", "summary": "Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u80ba\u6d3b\u91cf\u56fe\u65f6\u95f4\u5e8f\u5217\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u65e9\u671f\u68c0\u6d4b\u53f3\u5fc3\u8870\u7aed\uff0c\u5728UK Biobank\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5728\u4e34\u5e8a\u9ad8\u5371\u4eba\u7fa4\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u53f3\u5fc3\u8870\u7aed\u5177\u6709\u9ad8\u53d1\u75c5\u7387\u548c\u6b7b\u4ea1\u7387\uff0c\u80ba\u90e8\u75be\u75c5\u5e38\u5bfc\u81f4\u53f3\u5fc3\u5ba4\u8d1f\u8377\u589e\u52a0\u5f15\u53d1\u53f3\u5fc3\u8870\u7aed\uff0c\u9700\u8981\u4ece\u80ba\u90e8\u75be\u75c5\u60a3\u8005\u4e2d\u7b5b\u67e5\u51fa\u53ef\u80fd\u53d1\u5c55\u4e3a\u53f3\u5fc3\u8870\u7aed\u7684\u60a3\u8005\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ece\u6570\u636e\u589e\u5f3a\u7684\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u5b66\u4e60\u80ba\u6d3b\u91cf\u56fe\u65f6\u95f4\u5e8f\u5217\u7684\u7a33\u5065\u4f4e\u7ef4\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5c06\u8be5\u8868\u793a\u4e0e\u4eba\u53e3\u7edf\u8ba1\u5b66\u4fe1\u606f\u878d\u5408\uff0c\u8f93\u5165CatBoost\u5206\u7c7b\u5668\u8fdb\u884c\u53f3\u5fc3\u8870\u7aed\u9884\u6d4b\u3002", "result": "\u5728UK Biobank\u768426,617\u540d\u4e2a\u4f53\u4e0aAUROC\u4e3a0.7501\uff1b\u5728\u6162\u6027\u80be\u75c574\u540d\u60a3\u8005\u6d4b\u8bd5\u96c6\u4e0aAUROC\u4e3a0.8194\uff1b\u5728\u74e3\u819c\u6027\u5fc3\u810f\u75c564\u540d\u60a3\u8005\u6d4b\u8bd5\u96c6\u4e0aAUROC\u4e3a0.8413\u3002", "conclusion": "\u8be5\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u80ba\u6d3b\u91cf\u56fe\u65f6\u95f4\u5e8f\u5217\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\uff0c\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5177\u6709\u65e9\u671f\u68c0\u6d4b\u53f3\u5fc3\u8870\u7aed\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13297", "abs": "https://arxiv.org/abs/2511.13297", "authors": ["Enhui Ma", "Lijun Zhou", "Tao Tang", "Jiahuan Zhang", "Junpeng Jiang", "Zhan Zhang", "Dong Han", "Kun Zhan", "Xueyang Zhang", "XianPeng Lang", "Haiyang Sun", "Xia Zhou", "Di Lin", "Kaicheng Yu"], "title": "CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving", "comment": null, "summary": "End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.", "AI": {"tldr": "\u63d0\u51faCorrectAD\u7cfb\u7edf\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0e3D\u5e03\u5c40\u5bf9\u9f50\u7684\u9ad8\u4fdd\u771f\u89c6\u9891\u6570\u636e\uff0c\u81ea\u52a8\u7ea0\u6b63\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u7684\u957f\u5c3e\u6545\u969c\u6848\u4f8b", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\u56e0\u957f\u5c3e\u95ee\u9898\uff08\u7f55\u89c1\u4f46\u5b89\u5168\u5173\u952e\u7684\u6545\u969c\u6848\u4f8b\uff09\u5bfc\u81f4\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u95ee\u9898", "method": "1) PM-Agent\u6a21\u62df\u4ea7\u54c1\u7ecf\u7406\u5236\u5b9a\u6570\u636e\u9700\u6c42\uff1b2) DriveSora\u751f\u6210\u4e0e3D\u5e03\u5c40\u5bf9\u9f50\u7684\u65f6\u7a7a\u4e00\u81f4\u89c6\u9891\uff1b3) \u6784\u5efa\u7aef\u5230\u7aef\u6a21\u578b\u65e0\u5173\u7684\u81ea\u7ea0\u6b63\u7cfb\u7edfCorrectAD", "result": "\u5728nuScenes\u548c\u5185\u90e8\u6570\u636e\u96c6\u4e0a\uff0cCorrectAD\u5206\u522b\u7ea0\u6b63\u4e8662.5%\u548c49.8%\u7684\u6545\u969c\u6848\u4f8b\uff0c\u78b0\u649e\u7387\u5206\u522b\u964d\u4f4e39%\u548c27%", "conclusion": "CorrectAD\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u81ea\u52a8\u7ea0\u6b63\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u7684\u6545\u969c\u6848\u4f8b\uff0c\u63d0\u9ad8\u7cfb\u7edf\u9c81\u68d2\u6027"}}
{"id": "2511.13309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13309", "abs": "https://arxiv.org/abs/2511.13309", "authors": ["Kaiwen Cai", "Xinze Liu", "Xia Zhou", "Hengtong Hu", "Jie Xiang", "Luyao Zhang", "Xueyang Zhang", "Kun Zhan", "Yifei Zhan", "Xianpeng Lang"], "title": "DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving", "comment": "AAAI2026", "summary": "The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.", "AI": {"tldr": "DriveLiDAR4D\u662f\u4e00\u4e2a\u65b0\u9896\u7684LiDAR\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5305\u542b\u591a\u6a21\u6001\u6761\u4ef6\u548c\u987a\u5e8f\u566a\u58f0\u9884\u6d4b\u6a21\u578bLiDAR4DNet\uff0c\u80fd\u591f\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684LiDAR\u573a\u666f\uff0c\u5177\u6709\u9ad8\u5ea6\u53ef\u63a7\u7684\u524d\u666f\u7269\u4f53\u548c\u771f\u5b9e\u80cc\u666f\u3002", "motivation": "\u73b0\u6709\u76843D LiDAR\u70b9\u4e91\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u7f3a\u4e4f\u987a\u5e8f\u751f\u6210\u80fd\u529b\u3001\u65e0\u6cd5\u4ea7\u751f\u7cbe\u786e\u5b9a\u4f4d\u7684\u524d\u666f\u7269\u4f53\u548c\u771f\u5b9e\u80cc\u666f\u7b49\u9650\u5236\uff0c\u963b\u788d\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86DriveLiDAR4D\u6d41\u6c34\u7ebf\uff0c\u5305\u542b\u591a\u6a21\u6001\u6761\u4ef6\u548cLiDAR4DNet\u987a\u5e8f\u566a\u58f0\u9884\u6d4b\u6a21\u578b\uff0c\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u5b9e\u73b0LiDAR\u573a\u666f\u7684\u987a\u5e8f\u751f\u6210\u548c\u5b8c\u6574\u573a\u666f\u64cd\u4f5c\u80fd\u529b\u3002", "result": "\u5728nuScenes\u548cKITTI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u83b7\u5f97FRD\u5206\u6570743.13\u548cFVD\u5206\u657016.96\uff0c\u76f8\u6bd4\u5f53\u524dSOTA\u65b9\u6cd5UniScene\uff0cFRD\u6027\u80fd\u63d0\u534737.2%\uff0cFVD\u63d0\u534724.1%\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u89e3\u51b3\u5177\u6709\u5b8c\u6574\u573a\u666f\u64cd\u4f5c\u80fd\u529b\u7684LiDAR\u573a\u666f\u987a\u5e8f\u751f\u6210\u7684\u5de5\u4f5c\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2511.13465", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13465", "abs": "https://arxiv.org/abs/2511.13465", "authors": ["Meng Zhu", "Quan Xiao", "Weidong Min"], "title": "AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate", "comment": "25 pages, 6 figures, 12 tables", "summary": "Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdamX\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u578b\u4e8c\u9636\u77e9\u4f30\u8ba1\u6307\u6570\u8870\u51cf\u7387\uff0c\u5728\u8bad\u7ec3\u540e\u671f\u51cf\u5f31\u5b66\u4e60\u6b65\u957f\u4fee\u6b63\u5f3a\u5ea6\u5e76\u9000\u5316\u4e3aSGD\uff0c\u63d0\u5347\u7a33\u5b9a\u671f\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "Adam\u4f18\u5316\u5668\u5728\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\u4ecd\u662f\u4e3b\u6d41\uff0c\u4f46\u76f8\u6bd4SGD\u66f4\u5bb9\u6613\u6536\u655b\u5230\u975e\u5e73\u5766\u6700\u5c0f\u503c\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faAdamX\u7b97\u6cd5\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u65b0\u578b\u4e8c\u9636\u77e9\u4f30\u8ba1\u6307\u6570\u8870\u51cf\u7387\uff0c\u968f\u7740\u8bad\u7ec3\u8fdb\u884c\u9010\u6e10\u51cf\u5f31\u5b66\u4e60\u6b65\u957f\u4fee\u6b63\u5f3a\u5ea6\uff0c\u5728\u7a33\u5b9a\u8bad\u7ec3\u671f\u9000\u5316\u4e3aSGD\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b0\u578b\u4e8c\u9636\u77e9\u4f30\u8ba1\u6307\u6570\u8870\u51cf\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cAdamX\u5728\u6027\u80fd\u4e0a\u7a33\u5b9a\u4f18\u4e8eAdam\u53ca\u5176\u53d8\u4f53\u3002", "conclusion": "AdamX\u901a\u8fc7\u6539\u8fdb\u4e8c\u9636\u77e9\u4f30\u8ba1\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4f18\u5316\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2511.13387", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13387", "abs": "https://arxiv.org/abs/2511.13387", "authors": ["Fei Kong"], "title": "Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model", "comment": "in Chinese language", "summary": "Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5e7f\u4e49\u53bb\u566a\u6269\u6563\u538b\u7f29\u6a21\u578b(gDDCM)\uff0c\u5c06DDCM\u6269\u5c55\u5230\u4e3b\u6d41\u6269\u6563\u6a21\u578b\u53ca\u5176\u53d8\u4f53\uff0c\u5305\u62ecDDPM\u3001\u57fa\u4e8e\u5206\u6570\u7684\u6a21\u578b\u3001\u4e00\u81f4\u6027\u6a21\u578b\u548c\u6574\u6d41\u6d41\uff0c\u5728CIFAR-10\u548cLSUN Bedroom\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "DDCM\u867d\u7136\u5229\u7528DDPM\u548c\u7279\u5b9a\u566a\u58f0\u96c6\u5b9e\u73b0\u4e86\u56fe\u50cf\u538b\u7f29\uff0c\u4f46\u65e0\u6cd5\u5e94\u7528\u4e8eDDPM\u4e4b\u5916\u7684\u5176\u4ed6\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u66f4\u901a\u7528\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fagDDCM\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53cd\u5411\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u566a\u58f0\u66ff\u6362\u4e3a\u6309\u9884\u5b9a\u89c4\u5219\u4ece\u7279\u5b9a\u96c6\u5408\u91c7\u6837\u7684\u566a\u58f0\uff0c\u5c06DDCM\u6269\u5c55\u5230\u591a\u79cd\u6269\u6563\u6a21\u578b\u53d8\u4f53\u3002", "result": "\u5728CIFAR-10\u548cLSUN Bedroom\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5c06DDCM\u63a8\u5e7f\u5230\u4e0a\u8ff0\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u4e86\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "gDDCM\u6709\u6548\u6269\u5c55\u4e86DDCM\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u591a\u79cd\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u901a\u7528\u7684\u56fe\u50cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13344", "abs": "https://arxiv.org/abs/2511.13344", "authors": ["Ori Meiraz", "Sharon Shalev", "Avishai Weizman"], "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection", "comment": "1 figure, 1 table", "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv9-T\u7684\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8def\u7531\u5b9e\u73b0\u52a8\u6001\u7279\u5f81\u4e13\u4e1a\u5316\uff0c\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u76f8\u6bd4\u5355\u4e00YOLOv9-T\u6a21\u578b\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684mAP\u548cAR\u6307\u6807", "motivation": "\u4e3a\u4e86\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u8ba9\u6a21\u578b\u80fd\u591f\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7279\u5f81\u6a21\u5f0f\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u5316\u7684\u7279\u5f81\u5904\u7406", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u96c6\u6210\u591a\u4e2aYOLOv9-T\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\u5728\u4e0d\u540c\u4e13\u5bb6\u4e4b\u95f4\u8fdb\u884c\u52a8\u6001\u9009\u62e9", "result": "\u76f8\u6bd4\u5355\u4e00YOLOv9-T\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u5e73\u5747\u7cbe\u5ea6(mAP)\u548c\u5e73\u5747\u53ec\u56de\u7387(AR)", "conclusion": "\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u81ea\u9002\u5e94\u8def\u7531\u673a\u5236\u5b9e\u73b0\u4e86\u7279\u5f81\u7684\u52a8\u6001\u4e13\u4e1a\u5316\u5904\u7406"}}
{"id": "2511.13497", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.13497", "abs": "https://arxiv.org/abs/2511.13497", "authors": ["Liudmila A. Zhukas", "Vivian Ni Zhang", "Qiang Miao", "Qingfeng Wang", "Marko Cetina", "Jungsang Kim", "Lawrence Carin", "Christopher Monroe"], "title": "Quantum Machine Learning via Contrastive Training", "comment": "7 figures, 20 pages total", "summary": "Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u53ef\u7f16\u7a0b\u79bb\u5b50\u9631\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u7f16\u7801\u56fe\u50cf\u4e3a\u91cf\u5b50\u6001\u5e76\u8fdb\u884c\u5bf9\u6bd4\u9884\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u6280\u672f\u548c\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u6027\u589e\u52a0\u65f6\u3002", "method": "\u5728\u53ef\u7f16\u7a0b\u79bb\u5b50\u9631\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b0\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u5c06\u56fe\u50cf\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4ece\u672a\u6807\u6ce8\u6837\u672c\u4e2d\u5b66\u4e60\u4e0d\u53d8\u6027\uff0c\u6240\u6709\u8bad\u7ec3\u548c\u5206\u7c7b\u9636\u6bb5\u90fd\u5728\u786c\u4ef6\u4e0a\u6267\u884c\u3002", "result": "\u9884\u8bad\u7ec3\u5f97\u5230\u7684\u8868\u793a\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u6bd4\u968f\u673a\u521d\u59cb\u5316\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u7684\u8fd0\u884c\u95f4\u53d8\u5f02\u6027\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u91cf\u5b50\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u9014\u5f84\uff0c\u5bf9\u91cf\u5b50\u539f\u751f\u6570\u636e\u96c6\u5177\u6709\u76f4\u63a5\u76f8\u5173\u6027\uff0c\u5e76\u4e3a\u5904\u7406\u66f4\u5927\u89c4\u6a21\u7ecf\u5178\u8f93\u5165\u63d0\u4f9b\u4e86\u6e05\u6670\u8def\u5f84\u3002"}}
{"id": "2511.13397", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13397", "abs": "https://arxiv.org/abs/2511.13397", "authors": ["Nikos Theodoridis", "Tim Brophy", "Reenu Mohandas", "Ganesh Sistu", "Fiachra Collins", "Anthony Scanlan", "Ciaran Eising"], "title": "Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)", "comment": null, "summary": "The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.", "AI": {"tldr": "DTPQA\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u901a\u573a\u666f\u4e2d\u611f\u77e5\u80fd\u529b\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u4e24\u90e8\u5206\uff0c\u5177\u6709\u8ddd\u79bb\u6807\u6ce8\u529f\u80fd\u3002", "motivation": "\u7531\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u9700\u6c42\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u8fdc\u8ddd\u79bb\u7269\u4f53\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u521b\u5efa\u4e86DTPQA\u57fa\u51c6\uff0c\u5305\u542b\u5408\u6210\u6570\u636e\u96c6\uff08\u4f7f\u7528\u6a21\u62df\u5668\u751f\u6210\uff09\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08\u57fa\u4e8e\u73b0\u6709\u771f\u5b9e\u4ea4\u901a\u573a\u666f\u56fe\u50cf\uff09\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u56fe\u50cf\u3001\u95ee\u9898\u3001\u771f\u5b9e\u7b54\u6848\u548c\u7269\u4f53\u8ddd\u79bb\u4fe1\u606f\u3002", "result": "\u8be5\u57fa\u51c6\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30VLM\u5728\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u611f\u77e5\u6027\u80fd\uff0c\u7279\u522b\u662f\u5206\u6790\u6a21\u578b\u6027\u80fd\u968f\u7269\u4f53\u8ddd\u79bb\u589e\u52a0\u800c\u4e0b\u964d\u7684\u60c5\u51b5\u3002", "conclusion": "DTPQA\u4e3a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u611f\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u6d4b\u8bd5\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.13399", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13399", "abs": "https://arxiv.org/abs/2511.13399", "authors": ["Yuchen Bao", "Yiting Wang", "Wenjian Huang", "Haowei Wang", "Shen Chen", "Taiping Yao", "Shouhong Ding", "Jianguo Zhang"], "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing", "comment": "Accepted by AAAI2026", "summary": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS", "AI": {"tldr": "TripleFDS\u662f\u4e00\u4e2a\u7528\u4e8e\u573a\u666f\u6587\u672c\u7f16\u8f91\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6587\u672c\u6837\u5f0f\u3001\u5185\u5bb9\u548c\u80cc\u666f\u4e09\u4e2a\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u548c\u4e00\u81f4\u7684\u6587\u672c\u7f16\u8f91\u3002\u8be5\u65b9\u6cd5\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u6587\u672c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u573a\u666f\u6587\u672c\u7f16\u8f91\u65b9\u6cd5\u5728\u53ef\u7f16\u8f91\u5c5e\u6027\u89e3\u8026\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u901a\u5e38\u53ea\u80fd\u5904\u7406\u5355\u4e00\u65b9\u9762\u7684\u7f16\u8f91\uff08\u5982\u6587\u672c\u5185\u5bb9\uff09\uff0c\u9650\u5236\u4e86\u53ef\u63a7\u6027\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86TripleFDS\u6846\u67b6\u548cSCB Synthesis\u6570\u636e\u96c6\uff0c\u4f7f\u7528SCB Group\u4f5c\u4e3a\u57fa\u672c\u8bad\u7ec3\u5355\u5143\u8fdb\u884c\u4e09\u91cd\u7279\u5f81\u89e3\u8026\uff0c\u901a\u8fc7\u7ec4\u95f4\u5bf9\u6bd4\u6b63\u5219\u5316\u548c\u7ec4\u5185\u591a\u7279\u5f81\u6b63\u4ea4\u6027\u786e\u4fdd\u8bed\u4e49\u51c6\u786e\u6027\u548c\u51cf\u5c11\u5197\u4f59\u3002\u5728\u5408\u6210\u9636\u6bb5\u8fdb\u884c\u7279\u5f81\u91cd\u6620\u5c04\u4ee5\u9632\u6b62\u91cd\u5efa\u4e2d\u7684\"\u6377\u5f84\"\u73b0\u8c61\u3002", "result": "\u5728125,000\u4e2aSCB Groups\u4e0a\u8bad\u7ec3\u540e\uff0cTripleFDS\u5728\u4e3b\u6d41STE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e8644.54\u7684SSIM\u548c93.58%\u7684ACC\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TripleFDS\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u652f\u6301\u66f4\u7075\u6d3b\u7684\u7f16\u8f91\u64cd\u4f5c\uff0c\u5982\u6837\u5f0f\u66ff\u6362\u548c\u80cc\u666f\u8f6c\u79fb\uff0c\u4e3a\u573a\u666f\u6587\u672c\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13527", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13527", "abs": "https://arxiv.org/abs/2511.13527", "authors": ["Ihab Asaad", "Maha Shadaydeh", "Joachim Denzler"], "title": "Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images", "comment": "Accepted at EurIPS 2025 Workshop: Unifying Perspectives on Learning Biases (UPLB)", "summary": "Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001\u975e\u7ebf\u6027\u663e\u5fae\u955c\u56fe\u50cf\u4e2d\u7684\u80bf\u7624\u68c0\u6d4b\u95ee\u9898\uff0c\u91c7\u7528\u57fa\u4e8epatch\u7684\u4e8c\u5143\u5206\u7c7b\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u8be5\u65b9\u6cd5\u4f1a\u5f15\u5165\u865a\u5047\u76f8\u5173\u6027\uff08\u80bf\u7624patch\u901a\u5e38\u5305\u542b\u8f83\u5927\u7ec4\u7ec7\u533a\u57df\uff0c\u800c\u975e\u80bf\u7624patch\u591a\u4e3a\u80cc\u666f\uff09\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528GERNE\u53bb\u504f\u7b56\u7565\u6765\u63d0\u9ad8\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u3002", "motivation": "\u57fa\u4e8epatch\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u6790\u4e2d\u6bd4\u50cf\u7d20\u7ea7\u5206\u5272\u66f4\u9ad8\u6548\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3001\u7b80\u5316\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u5141\u8bb8\u7075\u6d3b\u7684patch\u5927\u5c0f\u8c03\u6574\u3002\u4f46\u8be5\u7b80\u5316\u65b9\u6cd5\u4f1a\u5f15\u5165patch\u7ec4\u6210\u4e0e\u6807\u7b7e\u4e4b\u95f4\u7684\u865a\u5047\u76f8\u5173\u6027\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528patch-wise\u4e8c\u5143\u5206\u7c7b\u65b9\u6cd5\u68c0\u6d4b\u80bf\u7624\uff0c\u8bc6\u522b\u51fa\u80bf\u7624patch\u4e0e\u975e\u80bf\u7624patch\u5728\u7ec4\u7ec7\u533a\u57df\u5927\u5c0f\u4e0a\u7684\u865a\u5047\u76f8\u5173\u6027\u3002\u4f7f\u7528GERNE\u53bb\u504f\u7b56\u7565\u6765\u6700\u5927\u5316\u6700\u5dee\u7ec4\u51c6\u786e\u7387\uff0c\u7f13\u89e3\u865a\u5047\u76f8\u5173\u6027\u5e26\u6765\u7684\u504f\u5dee\u3002", "result": "\u4e0eERM\u76f8\u6bd4\uff0cGERNE\u53bb\u504f\u7b56\u7565\u5c06\u6700\u5dee\u7ec4\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u7ea67%\uff08\u9488\u5bf9\u4e24\u79cd\u4e0d\u540c\u7684\u865a\u5047\u7279\u5f81\u4e8c\u503c\u5316\u9608\u503c\uff09\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5728\u5173\u952e\u5c11\u6570\u60c5\u51b5\u4e0b\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5982\u5c0f\u7ec4\u7ec7\u533a\u57df\u7684\u80bf\u7624patch\u548c\u5927\u7ec4\u7ec7\u533a\u57df\u7684\u975e\u80bf\u7624patch\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5728\u57fa\u4e8epatch\u7684\u5206\u7c7b\u95ee\u9898\u4e2d\uff0c\u865a\u5047\u76f8\u5173\u6027\u611f\u77e5\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002GERNE\u53bb\u504f\u7b56\u7565\u80fd\u6709\u6548\u7f13\u89e3\u865a\u5047\u76f8\u5173\u6027\u5e26\u6765\u7684\u504f\u5dee\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u5173\u952e\u5c11\u6570\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.13400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13400", "abs": "https://arxiv.org/abs/2511.13400", "authors": ["Jinkun Zhao", "Lei Huang", "Wenjun Wu"], "title": "What Color Is It? A Text-Interference Multimodal Hallucination Benchmark", "comment": null, "summary": "With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the \"What Color Is It\" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\"What Color Is It\"\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u989c\u8272\u611f\u77e5\u65b9\u9762\u7684\u89c6\u89c9\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u5176\u539f\u56e0\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u5bb9\u6613\u53d7\u5230\u4fe1\u606f\u5e72\u6270\uff0c\u7279\u522b\u662f\u5728\u989c\u8272\u611f\u77e5\u65b9\u9762\u5b58\u5728\u5e7b\u89c9\u98ce\u9669\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u5e76\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u6784\u5efa\"What Color Is It\"\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7b80\u5355\u65b9\u6cd5\u89e6\u53d1\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u7684\u5355\u6a21\u6001\u89c6\u89c9\u5e7b\u89c9\uff0c\u5e76\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u5206\u6790\u89c6\u89c9\u5e7b\u89c9\u7684\u6839\u672c\u539f\u56e0\u3002", "result": "\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u989c\u8272\u611f\u77e5\u65b9\u9762\u786e\u5b9e\u5b58\u5728\u89c6\u89c9\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u8bc6\u522b\u4e86\u5bfc\u81f4\u8fd9\u4e9b\u5e7b\u89c9\u7684\u6f5c\u5728\u673a\u5236\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u89c6\u89c9\u6a21\u6001\u4e2d\u5b58\u5728\u989c\u8272\u611f\u77e5\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u65b9\u6848\u6765\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.13541", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13541", "abs": "https://arxiv.org/abs/2511.13541", "authors": ["Yue Hou", "Ruomei Liu", "Yingke Su", "Junran Wu", "Ke Xu"], "title": "Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries", "comment": "Accepted by AAAI 2026 (The 40th Annual AAAI Conference on Artificial Intelligence)", "summary": "A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u56feOOD\u68c0\u6d4b\u65b9\u6cd5BaCa\uff0c\u901a\u8fc7\u53cc\u52a8\u6001\u66f4\u65b0\u5b57\u5178\u6821\u51c6OOD\u5206\u6570\uff0c\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u56feOOD\u68c0\u6d4b\u4e2d\u7f3a\u4e4f\u771f\u5b9eOOD\u6837\u672c\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f18\u5316ID\u6570\u636e\u7279\u5f81\uff0c\u96be\u4ee5\u51c6\u786e\u8868\u793a\u5206\u5e03\u8fb9\u754c\uff0c\u4e14\u56fe\u6570\u636e\u7684\u6f5c\u5728\u591a\u56e0\u7d20\u7ed3\u6784\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "BaCa\u65b9\u6cd5\u4f30\u8ba1\u56fe\u8bba\u5e76\u5e94\u7528\u6df7\u5408\u7b56\u7565\u751f\u6210\u8fb9\u754c\u611f\u77e5\u5224\u522b\u62d3\u6251\uff0c\u6784\u5efa\u53cc\u52a8\u6001\u5b57\u5178\u901a\u8fc7\u4f18\u5148\u7ea7\u961f\u5217\u548c\u6ce8\u610f\u529b\u673a\u5236\u81ea\u9002\u5e94\u6355\u83b7\u6f5c\u5728ID\u548cOOD\u8868\u793a\uff0c\u7528\u4e8e\u8fb9\u754c\u611f\u77e5OOD\u5206\u6570\u6821\u51c6\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cBaCa\u5728OOD\u68c0\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "BaCa\u65b9\u6cd5\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u7684OOD\u5206\u6570\u6821\u51c6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56feOOD\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u65e0\u9700\u8f85\u52a9\u6570\u636e\u96c6\u6216\u6a21\u578b\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u4f18\u5f02\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.13442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13442", "abs": "https://arxiv.org/abs/2511.13442", "authors": ["Rui Zuo", "Qinyue Tong", "Zhe-Ming Lu", "Ziqian Lu"], "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline", "comment": null, "summary": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.", "AI": {"tldr": "Foresee\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u50cf\u4f2a\u9020\u5206\u6790\u6d41\u7a0b\uff0c\u901a\u8fc7\u7c7b\u578b\u5148\u9a8c\u9a71\u52a8\u7b56\u7565\u548c\u7075\u6d3b\u7279\u5f81\u68c0\u6d4b\u5668\u6a21\u5757\uff0c\u6709\u6548\u91ca\u653e\u4e86\u539f\u59cbMLLM\u5728\u53d6\u8bc1\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5728\u7be1\u6539\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u6587\u672c\u89e3\u91ca\u4e30\u5bcc\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u6570\u636e\u96c6\u4e14\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3002\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u89c4\u6a21\u8bad\u7ec3\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u539f\u59cbMLLM\u7684\u5185\u5728\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684Foresee\u6d41\u7a0b\uff0c\u91c7\u7528\u7c7b\u578b\u5148\u9a8c\u9a71\u52a8\u7b56\u7565\u548c\u7075\u6d3b\u7279\u5f81\u68c0\u6d4b\u5668\u6a21\u5757\u4e13\u95e8\u5904\u7406\u590d\u5236\u79fb\u52a8\u7be1\u6539\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u63a8\u7406\u3002", "result": "\u5728\u591a\u79cd\u7be1\u6539\u7c7b\u578b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709IFDL\u65b9\u6cd5\uff0c\u5305\u62ec\u590d\u5236\u79fb\u52a8\u3001\u62fc\u63a5\u3001\u79fb\u9664\u3001\u5c40\u90e8\u589e\u5f3a\u3001\u6df1\u5ea6\u4f2a\u9020\u548cAIGC\u7f16\u8f91\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u6587\u672c\u89e3\u91ca\u3002", "conclusion": "Foresee\u6210\u529f\u91ca\u653e\u4e86\u539f\u59cbMLLM\u5728\u53d6\u8bc1\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u56fe\u50cf\u4f2a\u9020\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13561", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13561", "abs": "https://arxiv.org/abs/2511.13561", "authors": ["Shihao Dong", "Yue Liu", "Xiaotong Zhou", "Yuhui Zheng", "Huiying Xu", "Xinzhong Zhu"], "title": "RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise", "comment": null, "summary": "Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9760\u6027\u611f\u77e5\u7684\u5bf9\u6bd4\u6df1\u5ea6\u591a\u89c6\u56fe\u805a\u7c7b\u6846\u67b6RAC-DMVC\uff0c\u7528\u4e8e\u5904\u7406\u5305\u542b\u7f3a\u5931\u566a\u58f0\u548c\u89c2\u6d4b\u566a\u58f0\u7684\u591a\u6e90\u566a\u58f0\u73af\u5883\u4e0b\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u5e38\u9762\u4e34\u591a\u6e90\u566a\u58f0\uff08\u5305\u62ec\u7f3a\u5931\u566a\u58f0\u548c\u89c2\u6d4b\u566a\u58f0\uff09\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6b64\u5904\u7406\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faRAC-DMVC\u6846\u67b6\uff1a1\uff09\u8de8\u89c6\u56fe\u91cd\u6784\u5904\u7406\u89c2\u6d4b\u566a\u58f0\uff1b2\uff09\u53ef\u9760\u6027\u611f\u77e5\u566a\u58f0\u5bf9\u6bd4\u5b66\u4e60\u51cf\u5c11\u566a\u58f0\u8868\u793a\u5e26\u6765\u7684\u504f\u5dee\uff1b3\uff09\u53cc\u6ce8\u610f\u529b\u63d2\u8865\u5904\u7406\u7f3a\u5931\u566a\u58f0\uff1b4\uff09\u81ea\u76d1\u7763\u805a\u7c7b\u84b8\u998f\u6a21\u5757\u4f18\u5316\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRAC-DMVC\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u566a\u58f0\u6bd4\u4f8b\u4e0b\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "RAC-DMVC\u901a\u8fc7\u53ef\u9760\u6027\u56fe\u6307\u5bfc\u7684\u9c81\u68d2\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6e90\u566a\u58f0\u73af\u5883\u4e0b\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u95ee\u9898\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.13420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13420", "abs": "https://arxiv.org/abs/2511.13420", "authors": ["Xingming Long", "Jie Zhang", "Shiguang Shan", "Xilin Chen"], "title": "VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task", "comment": "8 pages", "summary": "Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VOPE\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u5b58\u5728\u6027\u8bc4\u4f30\u6765\u5224\u65ad\u6a21\u578b\u662f\u5426\u5728\u751f\u6210\u54cd\u5e94\u65f6\u4ea7\u751f\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7981\u6b62\u8f93\u51fa\u56fe\u50cf\u4e2d\u4e0d\u5b58\u5728\u5185\u5bb9\u7684\u4e8b\u5b9e\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u800c\u5ffd\u89c6\u4e86\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\uff08\u5982\u6545\u4e8b\u5199\u4f5c\uff09\u4e2d\u7684\u5e7b\u89c9\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4e2d\u6a21\u578b\u88ab\u671f\u671b\u751f\u6210\u8d85\u8d8a\u7ed9\u5b9a\u56fe\u50cf\u7684\u65b0\u5185\u5bb9\u3002", "method": "\u5f15\u5165VOPE\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u51fa\u57fa\u4e8e\u91cd\u65b0\u68c0\u67e5\u7684\u95ee\u9898\u6765\u8bc4\u4f30LVLM\u5982\u4f55\u89e3\u91ca\u5176\u54cd\u5e94\u4e2d\u60f3\u8c61\u5bf9\u8c61\u7684\u5b58\u5728\u6027\uff0c\u7136\u540e\u6839\u636e\u6a21\u578b\u89e3\u91ca\u4e0e\u56fe\u50cf\u4e2d\u5bf9\u8c61\u5b58\u5728\u6027\u7684\u4e00\u81f4\u6027\u6765\u5224\u65ad\u662f\u5426\u4ea7\u751f\u5e7b\u89c9\u3002", "result": "\u5e94\u7528VOPE\u5230\u591a\u4e2a\u4e3b\u6d41LVLM\u548c\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\uff0c\u53d1\u73b0\uff1a(1)\u5927\u591a\u6570LVLM\u5728\u81ea\u613f\u60f3\u8c61\u4e2d\u4e25\u91cd\u5e7b\u89c9\uff0c\u5728\u60f3\u8c61\u5bf9\u8c61\u4e0a\u7684\u5b58\u5728\u6027\u8bc4\u4f30\u8868\u73b0\u663e\u8457\u8f83\u5dee\uff1b(2)\u73b0\u6709\u5e7b\u89c9\u7f13\u89e3\u65b9\u6cd5\u5728\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u81ea\u613f\u60f3\u8c61\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u6b64\u7c7b\u4efb\u52a1\u7684\u9002\u7528\u6027\u6709\u9650\u3002"}}
{"id": "2511.13431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13431", "abs": "https://arxiv.org/abs/2511.13431", "authors": ["Lorenzo Olearo", "Giulio Vigan\u00f2", "Daniele Baieri", "Filippo Maggioli", "Simone Melzi"], "title": "FUSE: A Flow-based Mapping Between Shapes", "comment": "11 pages, 9 figures", "summary": "We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\u6a21\u578b\u76843D\u5f62\u72b6\u6620\u5c04\u65b0\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u652f\u6301\u8de8\u8868\u793a\u5f62\u72b6\u5339\u914d\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u6570\u636e\u9a71\u52a8\u8fc7\u7a0b\u3002", "motivation": "\u5f00\u53d1\u8ba1\u7b97\u9ad8\u6548\u3001\u652f\u6301\u8de8\u8868\u793a\u5f62\u72b6\u5339\u914d\u7684\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u6570\u636e\u9a71\u52a8\u8fc7\u7a0b\u7684\u9650\u5236\u3002", "method": "\u5c063D\u5f62\u72b6\u8868\u793a\u4e3a\u4ece\u56fa\u5b9a\u951a\u5206\u5e03\u901a\u8fc7\u8fde\u7eed\u53ef\u9006\u6d41\u6620\u5c04\u8bf1\u5bfc\u7684\u6982\u7387\u5206\u5e03\uff0c\u901a\u8fc7\u6e90\u5f62\u72b6\u5230\u951a\u7684\u9006\u6d41\u4e0e\u951a\u5230\u76ee\u6807\u5f62\u72b6\u7684\u6b63\u5411\u6d41\u7ec4\u5408\u5b9e\u73b0\u70b9\u5bf9\u70b9\u6620\u5c04\uff0c\u5e76\u4f7f\u7528\u70b9\u7ea7\u4efb\u52a1\u5b9a\u5236\u5d4c\u5165\u7f16\u7801\u5f62\u72b6\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u548c\u6311\u6218\u6027\u8bbe\u7f6e\u4e2d\u4e00\u81f4\u5b9e\u73b0\u9ad8\u8986\u76d6\u7387\u548c\u51c6\u786e\u6027\uff0c\u5728\u5f62\u72b6\u5339\u914d\u3001UV\u6620\u5c04\u548c\u4eba\u4f53\u539f\u59cb\u70b9\u4e91\u626b\u63cf\u914d\u51c6\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u9006\u4e14\u6a21\u6001\u65e0\u5173\u7684\u5f62\u72b6\u6620\u5c04\u8868\u793a\uff0c\u652f\u6301\u70b9\u4e91\u3001\u7f51\u683c\u3001SDF\u548c\u4f53\u6570\u636e\u7b49\u591a\u79cd\u8868\u793a\u5f62\u5f0f\uff0c\u5728\u591a\u4e2a3D\u5f62\u72b6\u5904\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2511.13625", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13625", "abs": "https://arxiv.org/abs/2511.13625", "authors": ["Kaichi Irie", "Shuhei Watanabe", "Masaki Onishi"], "title": "Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization", "comment": "Accepted to 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)", "summary": "Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u7a0b\u89e3\u8026\u62df\u725b\u987f\u66f4\u65b0\uff0c\u5728\u4fdd\u6301\u6279\u91cf\u8c03\u7528\u91c7\u96c6\u51fd\u6570\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u9006Hessian\u77e9\u9635\u8fd1\u4f3c\u8bef\u5dee\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7684BoTorch\u5e93\u5728\u4f18\u5316\u591a\u70b9\u91c7\u96c6\u51fd\u6570\u65f6\uff0c\u867d\u7136\u901a\u8fc7PyTorch\u6279\u5904\u7406\u52a0\u901f\u4e86\u591a\u8d77\u70b9\u4f18\u5316\uff0c\u4f46\u7531\u4e8e\u62df\u725b\u987f\u65b9\u6cd5\u4e2d\u9006Hessian\u77e9\u9635\u7684\u975e\u5bf9\u89d2\u7ebf\u8fd1\u4f3c\u8bef\u5dee\uff0c\u5bfc\u81f4\u6536\u655b\u901f\u5ea6\u53d8\u6162\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u534f\u7a0b\u89e3\u8026\u62df\u725b\u987f\u66f4\u65b0\uff0c\u540c\u65f6\u4fdd\u6301\u91c7\u96c6\u51fd\u6570\u7684\u6279\u91cf\u8c03\u7528\uff0c\u8fd9\u6837\u65e2\u83b7\u5f97\u4e86\u4e0e\u987a\u5e8f\u591a\u8d77\u70b9\u4f18\u5316\u76f8\u540c\u7684\u7406\u8bba\u6536\u655b\u6027\uff0c\u53c8\u5927\u5e45\u51cf\u5c11\u4e86\u5b9e\u9645\u8fd0\u884c\u65f6\u95f4\u3002", "result": "\u65b0\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4e0e\u987a\u5e8f\u591a\u8d77\u70b9\u4f18\u5316\u76f8\u540c\u7684\u7406\u8bba\u6536\u655b\u6027\uff0c\u800c\u4e14\u76f8\u6bd4\u4e4b\u524d\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5b9e\u9645\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u534f\u7a0b\u89e3\u8026\u62df\u725b\u987f\u66f4\u65b0\u540c\u65f6\u6279\u91cf\u8c03\u7528\u91c7\u96c6\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u91c7\u96c6\u51fd\u6570\u4f18\u5316\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u6536\u655b\u6027\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.13478", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13478", "abs": "https://arxiv.org/abs/2511.13478", "authors": ["Adam Hazimeh", "Ke Wang", "Mark Collier", "Gilles Baechler", "Efi Kokiopoulou", "Pascal Frossard"], "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling", "comment": null, "summary": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.", "AI": {"tldr": "SliDer\u662f\u4e00\u4e2a\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u5e7b\u706f\u7247\u56fe\u50cf\u8f6c\u6362\u4e3a\u53ef\u7f16\u8f91SVG\u683c\u5f0f\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u51e0\u4f55\u5149\u6805-\u77e2\u91cf\u8f6c\u6362\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u7559\u6587\u6863\u9ad8\u7ea7\u7ed3\u6784\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u5a92\u4f53\u6587\u6863\u901a\u5e38\u4ee5\u9759\u6001\u5149\u6805\u683c\u5f0f\u5206\u53d1\uff0c\u9650\u5236\u4e86\u7f16\u8f91\u548c\u5b9a\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u7559\u590d\u6742\u6587\u6863\u7684\u9ad8\u5c42\u7ed3\u6784\uff0c\u5bfc\u81f4\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u548c\u63d0\u53d6\u5149\u6805\u8f93\u5165\u4e2d\u7684\u56fe\u50cf\u548c\u6587\u672c\u5143\u7d20\u5c5e\u6027\uff0c\u5c06\u5176\u7ec4\u7ec7\u6210\u8fde\u8d2f\u7684SVG\u683c\u5f0f\uff0c\u5e76\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\u3002", "result": "SliDer\u5b9e\u73b0\u4e860.069\u7684\u91cd\u5efaLPIPS\uff0c\u572882.9%\u7684\u60c5\u51b5\u4e0b\u88ab\u4eba\u7c7b\u8bc4\u4f30\u8005\u8ba4\u4e3a\u4f18\u4e8e\u6700\u5f3a\u7684\u96f6\u6837\u672cVLM\u57fa\u7ebf\u3002", "conclusion": "SliDer\u80fd\u591f\u6709\u6548\u5c06\u5e7b\u706f\u7247\u56fe\u50cf\u8f6c\u6362\u4e3a\u7d27\u51d1\u4e14\u53ef\u7f16\u8f91\u7684SVG\u8868\u793a\uff0c\u540c\u65f6\u5f15\u5165\u4e86Slide2SVG\u6570\u636e\u96c6\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2511.13640", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13640", "abs": "https://arxiv.org/abs/2511.13640", "authors": ["Haohui Wang", "Jingyuan Qi", "Jianpeng Chen", "Jun Wu", "Lifu Huang", "Lecheng Zheng", "Kevin Choi", "Balaji Veeramani", "Edward Bowen", "Alison Hu", "Tyler Cody", "Dawei Zhou"], "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures", "comment": null, "summary": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u4f1a\u5bfc\u81f4\u5206\u5e03\u504f\u5dee\uff0c\u7279\u522b\u662f\u957f\u5c3e\u77e5\u8bc6\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u4f5c\u8005\u8bc6\u522b\u4e86\u4e09\u79cd\u9636\u6bb5\u7684\u7f29\u653e\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u6df7\u5408\u6570\u636e\u7684\u6cdb\u5316\u8fb9\u754c\u7406\u8bba\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u6570\u636e\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u4f7f\u7528\u65e5\u76ca\u666e\u904d\uff0c\u4f46\u5408\u6210\u6570\u636e\u5b58\u5728\u7cfb\u7edf\u6027\u5206\u5e03\u504f\u5dee\uff0c\u7279\u522b\u662f\u957f\u5c3e\u77e5\u8bc6\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fd9\u7ed9\u6570\u636e\u6548\u7528\u8bc4\u4f30\u5e26\u6765\u4e86\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u8bc6\u522b\u4e86\u4e09\u79cd\u9636\u6bb5\u7684\u7f29\u653e\u884c\u4e3a\u7279\u5f81\uff0c\u63a8\u5bfc\u4e86\u9002\u7528\u4e8e\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u6df7\u5408\u7684LLM\u6cdb\u5316\u8fb9\u754c\u7406\u8bba\uff0c\u5e76\u57fa\u4e8e\u7406\u8bba\u53d1\u73b0\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u5230\u5927\u89c4\u6570\u636e\u96c6\u7684\u9ad8\u6548\u6570\u636e\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u60c5\u611f\u5206\u7c7b\u3001\u6307\u4ee4\u8ddf\u968f\u548c\u590d\u6742\u63a8\u7406\u56db\u4e2a\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u8bc4\u4f30\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u6548\u7528\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u6570\u636e\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2511.13488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13488", "abs": "https://arxiv.org/abs/2511.13488", "authors": ["Lipeng Wang", "Hongxing Fan", "Haohua Chen", "Zehuan Huang", "Lu Sheng"], "title": "InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE", "comment": "Accepted to AAAI-26. Codes: https://github.com/Lighten001/InterMoE", "summary": "Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.", "AI": {"tldr": "InterMoE\u662f\u4e00\u4e2a\u57fa\u4e8e\u52a8\u6001\u65f6\u95f4\u9009\u62e9\u6027\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u4ea4\u4e92\u52a8\u4f5c\uff0c\u5728\u4fdd\u6301\u4e2a\u4f53\u7279\u5f81\u548c\u6587\u672c\u8bed\u4e49\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u4eba\u7c7b\u4ea4\u4e92\u52a8\u4f5c\u65f6\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u72ec\u7279\u7684\u4e2a\u4f53\u7279\u5f81\u6216\u5b8c\u5168\u9075\u5faa\u6587\u672c\u63cf\u8ff0\uff0c\u8fd9\u9650\u5236\u4e86\u5728\u865a\u62df\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u4ef7\u503c\u3002", "method": "InterMoE\u91c7\u7528\u52a8\u6001\u65f6\u95f4\u9009\u62e9\u6027\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u7531\u673a\u5236\u534f\u540c\u4f7f\u7528\u9ad8\u7ea7\u6587\u672c\u8bed\u4e49\u548c\u4f4e\u7ea7\u52a8\u4f5c\u4e0a\u4e0b\u6587\uff0c\u5c06\u65f6\u95f4\u52a8\u4f5c\u7279\u5f81\u5206\u914d\u7ed9\u4e13\u95e8\u7684\u4e13\u5bb6\uff0c\u4e13\u5bb6\u52a8\u6001\u786e\u5b9a\u9009\u62e9\u80fd\u529b\u5e76\u5173\u6ce8\u5173\u952e\u65f6\u95f4\u7279\u5f81\u3002", "result": "\u5728InterHuman\u6570\u636e\u96c6\u4e0aFID\u5206\u6570\u964d\u4f4e9%\uff0c\u5728InterX\u6570\u636e\u96c6\u4e0a\u964d\u4f4e22%\uff0c\u5b9e\u73b0\u4e86\u4e2a\u4f53\u7279\u5b9a\u9ad8\u4fdd\u771f3D\u4eba\u7c7b\u4ea4\u4e92\u751f\u6210\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "InterMoE\u6846\u67b6\u80fd\u591f\u6709\u6548\u4fdd\u6301\u7279\u5b9a\u4e2a\u4f53\u7279\u5f81\u8eab\u4efd\uff0c\u540c\u65f6\u786e\u4fdd\u9ad8\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u5728\u4eba\u7c7b\u4ea4\u4e92\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.13645", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13645", "abs": "https://arxiv.org/abs/2511.13645", "authors": ["Aleksandar Stankovi\u0107"], "title": "FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs", "comment": "15 pages. Code and reproducibility scripts: https://github.com/SV25-22/FuseSampleAgg", "summary": "We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.", "AI": {"tldr": "FuseSampleAgg\u662f\u4e00\u4e2aCUDA\u7b97\u5b50\uff0c\u5c06\u90bb\u5c45\u91c7\u6837\u548c\u5747\u503c\u805a\u5408\u878d\u5408\u4e3a\u5355\u6b21\u64cd\u4f5c\uff0c\u7528\u4e8e\u4e00\u9636\u548c\u4e8c\u9636GraphSAGE\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u7684GraphSAGE\u5b9e\u73b0\u9700\u8981\u591a\u6b21\u5185\u6838\u542f\u52a8\u548c\u5757\u7269\u5316\uff0c\u5bfc\u81f4\u5185\u5b58\u6d41\u91cf\u548c\u5f00\u9500\u8f83\u5927\uff0c\u9700\u8981\u4f18\u5316\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u901a\u8fc7\u4fdd\u5b58\u7d22\u5f15\u91cd\u653e\u6765\u6d88\u9664\u5757\u7269\u5316\u548c\u989d\u5916\u5185\u6838\u542f\u52a8\uff0c\u5c06\u90bb\u5c45\u91c7\u6837\u548c\u5747\u503c\u805a\u5408\u878d\u5408\u5230\u5355\u6b21\u64cd\u4f5c\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301GraphSAGE\u5747\u503c\u8bed\u4e49\u3002", "result": "\u5728Reddit\u3001ogbn-arxiv\u548cogbn-products\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6b65\u957f\u65f6\u95f4\u52a0\u901f\u6700\u9ad8\u8fbe51\u500d\uff0cGPU\u5185\u5b58\u5cf0\u503c\u51cf\u5c11\u6700\u9ad8\u8fbe100\u500d\uff0c\u4e14\u7b97\u5b50\u5177\u6709\u786e\u5b9a\u6027\u3002", "conclusion": "FuseSampleAgg\u901a\u8fc7\u64cd\u4f5c\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86GraphSAGE\u7684\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6PyTorch\u4f18\u5316\u5668\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2511.13494", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13494", "abs": "https://arxiv.org/abs/2511.13494", "authors": ["Jae Joong Lee"], "title": "Language-Guided Invariance Probing of Vision-Language Models", "comment": null, "summary": "Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.\n  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8bed\u8a00\u5f15\u5bfc\u4e0d\u53d8\u6027\u63a2\u6d4b(LGIP)\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5bf9\u8bed\u4e49\u4fdd\u6301\u7684\u6539\u5199\u548c\u8bed\u4e49\u6539\u53d8\u7684\u7ffb\u8f6c\u7684\u54cd\u5e94\u53ef\u9760\u6027\u3002\u901a\u8fc7\u57284\u4e07\u5f20MS COCO\u56fe\u50cf\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u53d1\u73b0EVA02-CLIP\u548c\u5927\u578bOpenCLIP\u53d8\u4f53\u5728\u4e0d\u53d8\u6027\u548c\u654f\u611f\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800cSigLIP\u6a21\u578b\u5219\u5b58\u5728\u8f83\u5927\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(\u5982CLIP\u3001OpenCLIP\u7b49)\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5176\u5bf9\u53d7\u63a7\u8bed\u8a00\u6270\u52a8\u7684\u54cd\u5e94\u53ef\u9760\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u8bed\u8a00\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faLGIP\u57fa\u51c6\uff0c\u4f7f\u75284\u4e07\u5f20MS COCO\u56fe\u50cf\u53ca\u5176\u4e94\u4e2a\u4eba\u5de5\u6807\u6ce8\u63cf\u8ff0\uff0c\u81ea\u52a8\u751f\u6210\u8bed\u4e49\u4fdd\u6301\u7684\u6539\u5199\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u8bed\u4e49\u7ffb\u8f6c(\u6539\u53d8\u5bf9\u8c61\u7c7b\u522b\u3001\u989c\u8272\u6216\u6570\u91cf)\uff0c\u901a\u8fc7\u4e0d\u53d8\u6027\u8bef\u5dee\u3001\u8bed\u4e49\u654f\u611f\u6027\u5dee\u8ddd\u548c\u6b63\u7387\u7edf\u8ba1\u6765\u603b\u7ed3\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5728\u4e5d\u4e2aVLM\u6a21\u578b\u4e2d\uff0cEVA02-CLIP\u548c\u5927\u578bOpenCLIP\u53d8\u4f53\u5904\u4e8e\u6709\u5229\u7684\u4e0d\u53d8\u6027-\u654f\u611f\u6027\u8fb9\u754c\uff0c\u5177\u6709\u8f83\u4f4e\u7684\u6539\u5199\u8bf1\u5bfc\u65b9\u5dee\uff0c\u4e14\u5bf9\u539f\u59cb\u63cf\u8ff0\u7684\u8bc4\u5206\u59cb\u7ec8\u9ad8\u4e8e\u7ffb\u8f6c\u7248\u672c\u3002\u800cSigLIP\u548cSigLIP2\u8868\u73b0\u51fa\u8f83\u5927\u7684\u4e0d\u53d8\u6027\u8bef\u5dee\uff0c\u7ecf\u5e38\u504f\u597d\u7ffb\u8f6c\u63cf\u8ff0\u800c\u975e\u4eba\u7c7b\u63cf\u8ff0\uff0c\u7279\u522b\u662f\u5728\u5bf9\u8c61\u548c\u989c\u8272\u7f16\u8f91\u65b9\u9762\u3002", "conclusion": "LGIP\u4e3aVLM\u7684\u8bed\u8a00\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6a21\u578b\u65e0\u5173\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u8fd9\u4e9b\u5931\u8d25\u5728\u6807\u51c6\u68c0\u7d22\u6307\u6807\u4e2d\u57fa\u672c\u4e0d\u53ef\u89c1\uff0c\u8868\u660e\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u51c6\u786e\u5ea6\u5206\u6570\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2511.13653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13653", "abs": "https://arxiv.org/abs/2511.13653", "authors": ["Leo Gao", "Achyuta Rajaram", "Jacob Coxon", "Soham V. Govande", "Bowen Baker", "Dan Mossing"], "title": "Weight-sparse transformers have interpretable circuits", "comment": null, "summary": "Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u8bad\u7ec3\u6743\u91cd\u7a00\u758f\u7684\u8bed\u8a00\u6a21\u578b\u6765\u53d1\u73b0\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7535\u8def\u7ed3\u6784\uff0c\u901a\u8fc7\u7ea6\u675f\u5927\u90e8\u5206\u6743\u91cd\u4e3a\u96f6\u6765\u4f7f\u6bcf\u4e2a\u795e\u7ecf\u5143\u53ea\u6709\u5c11\u91cf\u8fde\u63a5\uff0c\u4ece\u800c\u83b7\u5f97\u53ef\u89e3\u91ca\u7684\u7535\u8def\u3002", "motivation": "\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u627e\u5230\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7535\u8def\u662f\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u9886\u57df\u7684\u6838\u5fc3\u76ee\u6807\uff0c\u5f53\u524d\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8bad\u7ec3\u6743\u91cd\u7a00\u758f\u7684\u6a21\u578b\uff0c\u7ea6\u675f\u5927\u90e8\u5206\u6743\u91cd\u4e3a\u96f6\uff0c\u4f7f\u6bcf\u4e2a\u795e\u7ecf\u5143\u53ea\u6709\u5c11\u91cf\u8fde\u63a5\uff1b\u901a\u8fc7\u526a\u679d\u6280\u672f\u9694\u79bb\u7279\u5b9a\u4efb\u52a1\u5bf9\u5e94\u7684\u7535\u8def\u90e8\u5206\u3002", "result": "\u83b7\u5f97\u7684\u7535\u8def\u5305\u542b\u5bf9\u5e94\u81ea\u7136\u6982\u5ff5\u7684\u795e\u7ecf\u5143\u548c\u6b8b\u5dee\u901a\u9053\uff0c\u5177\u6709\u5c11\u91cf\u76f4\u63a5\u53ef\u89e3\u91ca\u7684\u8fde\u63a5\uff1b\u7a00\u758f\u5316\u5728\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u6743\u8861\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u80fd\u6539\u5584\u8fd9\u4e00\u6743\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u524d\u6240\u672a\u6709\u7684\u53ef\u7406\u89e3\u7535\u8def\uff0c\u5e76\u7ecf\u8fc7\u4e25\u683c\u9a8c\u8bc1\uff1b\u4f46\u5c06\u7a00\u758f\u6a21\u578b\u6269\u5c55\u5230\u6570\u5343\u4e07\u975e\u96f6\u53c2\u6570\u4ee5\u4e0a\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u4ecd\u5177\u6311\u6218\u6027\uff1b\u8be5\u65b9\u6cd5\u4e5f\u53ef\u7528\u4e8e\u89e3\u91ca\u73b0\u6709\u7684\u5bc6\u96c6\u6a21\u578b\u3002"}}
{"id": "2511.13545", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13545", "abs": "https://arxiv.org/abs/2511.13545", "authors": ["Md. Iqbal Hossain", "Afia Sajeeda", "Neeresh Kumar Perla", "Ming Shao"], "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks", "comment": null, "summary": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\u9632\u5fa1\u540e\u95e8\u653b\u51fb\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u5272\"\u9884\u8a00\u673a\"\u6765\u8bc6\u522b\u540e\u95e8\u89e6\u53d1\u5668\u3001\u53d7\u5bb3\u6837\u672c\u548c\u6807\u7b7e\uff0c\u5e76\u5f00\u53d1\u7b97\u6cd5\u4fee\u590d\u4e2d\u6bd2\u7684CLIP\u6a21\u578b\u3002", "motivation": "\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982CLIP\uff09\u5bb9\u6613\u53d7\u5230\u540e\u95e8\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u9700\u8981\u4ece\u5934\u8bad\u7ec3\u6216\u4f7f\u7528\u5927\u91cf\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u4e14\u65e0\u6cd5\u7cbe\u786e\u5b9a\u4f4d\u53d7\u5f71\u54cd\u7684\u6807\u7b7e\u3002", "method": "\u5f15\u5165\u56fe\u50cf\u5206\u5272\"\u9884\u8a00\u673a\"\u4f5c\u4e3a\u76d1\u7763\u5668\uff0c\u5f00\u53d1\u4e24\u79cd\u7b97\u6cd5\uff1a1\uff09\u533a\u5206CLIP\u548c\u9884\u8a00\u673a\u7684\u77e5\u8bc6\u6765\u8bc6\u522b\u6f5c\u5728\u89e6\u53d1\u5668\uff1b2\uff09\u7cbe\u786e\u5b9a\u4f4d\u53d7\u5f71\u54cd\u6807\u7b7e\u548c\u53d7\u5bb3\u6837\u672c\uff0c\u5e76\u6784\u5efa\u7d27\u51d1\u7684\u5fae\u8c03\u6570\u636e\u96c6\u3002", "result": "\u5728\u89c6\u89c9\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u5728\u57fa\u4e8eCLIP\u7684\u540e\u95e8\u9632\u5fa1\u4e2d\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u540e\u95e8\u89e6\u53d1\u5668\u3001\u53d7\u5bb3\u6837\u672c\u548c\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u6784\u5efa\u7684\u5fae\u8c03\u6570\u636e\u96c6\u6210\u529f\u4fee\u590d\u4e2d\u6bd2\u7684CLIP\u6a21\u578b\uff0c\u6d88\u9664\u540e\u95e8\u5f71\u54cd\u3002"}}
{"id": "2511.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13507", "abs": "https://arxiv.org/abs/2511.13507", "authors": ["Wenyu Zhang", "Yao Tong", "Yiqiu Liu", "Rui Cao"], "title": "Mapping the Vanishing and Transformation of Urban Villages in China", "comment": "Appendix A. Supplementary data at https://ars.els-cdn.com/content/image/1-s2.0-S2210670725008418-mmc1.docx", "summary": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\u76d1\u6d4b\u4e2d\u56fd\u57ce\u4e2d\u6751\u65f6\u7a7a\u53d8\u5316\uff0c\u53d1\u73b0\u57ce\u4e2d\u6751\u6539\u9020\u8fc7\u7a0b\u901a\u5e38\u6f2b\u957f\uff0c\u4e3b\u8981\u53d1\u751f\u5728\u57ce\u5e02\u5916\u56f4\u533a\u57df\uff0c\u63ed\u793a\u4e86\u4e09\u79cd\u65f6\u7a7a\u8f6c\u578b\u8def\u5f84\uff0c\u5f3a\u8c03\u9700\u8981\u5206\u5c42\u548c\u56e0\u5730\u5236\u5b9c\u7684\u89c4\u5212\u7b56\u7565\u3002", "motivation": "\u4e2d\u56fd\u57ce\u4e2d\u6751\u7ecf\u5386\u4e86\u5927\u89c4\u6a21\u62c6\u8fc1\u6539\u9020\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u62c6\u8fc1\u540e\u571f\u5730\u5229\u7528\u6709\u6548\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u8981\u8bc4\u4f30\u5f53\u524d\u6539\u9020\u5b9e\u8df5\u7684\u6210\u6548\u548c\u53ef\u6301\u7eed\u6027\u3002", "method": "\u4f7f\u7528\u591a\u65f6\u76f8\u9065\u611f\u5f71\u50cf\u7684\u8bed\u4e49\u5206\u5272\u6765\u7ed8\u5236\u57ce\u4e2d\u6751\u8fb9\u754c\u6f14\u53d8\uff0c\u7136\u540e\u5c06\u62c6\u8fc1\u540e\u571f\u5730\u5229\u7528\u5206\u4e3a\u516d\u7c7b\uff1a\u672a\u5b8c\u6210\u62c6\u8fc1\u3001\u95f2\u7f6e\u571f\u5730\u3001\u5efa\u7b51\u5de5\u5730\u3001\u5efa\u7b51\u7269\u3001\u7eff\u5730\u548c\u5176\u4ed6\u3002", "result": "1) \u57ce\u4e2d\u6751\u6539\u9020\u8fc7\u7a0b\u7ecf\u5e38\u5ef6\u957f\uff1b2) \u6539\u9020\u4e3b\u8981\u53d1\u751f\u5728\u57ce\u5e02\u5916\u56f4\u533a\u57df\uff0c\u800c\u57ce\u5e02\u6838\u5fc3\u533a\u76f8\u5bf9\u7a33\u5b9a\uff1b3) \u63ed\u793a\u4e86\u4e09\u79cd\u65f6\u7a7a\u8f6c\u578b\u8def\u5f84\uff1a\u540c\u6b65\u6539\u9020\u3001\u5ef6\u8fdf\u6539\u9020\u548c\u6e10\u8fdb\u4f18\u5316\u3002", "conclusion": "\u57ce\u4e2d\u6751\u6539\u9020\u5177\u6709\u788e\u7247\u5316\u3001\u590d\u6742\u548c\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u9700\u8981\u5206\u5c42\u548c\u56e0\u5730\u5236\u5b9c\u7684\u89c4\u5212\u7b56\u7565\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u652f\u6301\u66f4\u5305\u5bb9\u3001\u9ad8\u6548\u548c\u53ef\u6301\u7eed\u7684\u57ce\u5e02\u66f4\u65b0\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002"}}
{"id": "2511.13654", "categories": ["cs.LG", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13654", "abs": "https://arxiv.org/abs/2511.13654", "authors": ["Pascal Zimmer", "Ghassan Karame"], "title": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning", "comment": "To appear in the Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) 2026", "summary": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u4f18\u5316\u8d85\u53c2\u6570\uff08\u5b66\u4e60\u7387\u3001\u6743\u91cd\u8870\u51cf\u3001\u52a8\u91cf\u3001\u6279\u5927\u5c0f\uff09\u5bf9\u8fc1\u79fb\u653b\u51fb\u548c\u67e5\u8be2\u653b\u51fb\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u4e0d\u540c\u653b\u51fb\u7c7b\u578b\u4e0b\u5b66\u4e60\u7387\u7684\u5f71\u54cd\u5448\u73b0\u76f8\u53cd\u8d8b\u52bf\uff0c\u5e76\u63a2\u7d22\u4e86\u540c\u65f6\u589e\u5f3a\u5bf9\u4e24\u79cd\u653b\u51fb\u9c81\u68d2\u6027\u7684\u8d85\u53c2\u6570\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u7814\u7a76\u4f18\u5316\u8d85\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u5bf9\u8fc1\u79fb\u653b\u51fb\u548c\u67e5\u8be2\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u96c6\u4e2d\u8bad\u7ec3\u3001\u96c6\u6210\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u7b49\u591a\u79cd\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e0b\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4e0d\u540c\u4f18\u5316\u8d85\u53c2\u6570\u5bf9\u653b\u51fb\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5b66\u4e60\u7387\u5bf9\u4e24\u79cd\u653b\u51fb\u7684\u5f71\u54cd\u5448\u73b0\u4e8c\u5206\u73b0\u8c61\uff1a\u964d\u4f4e\u5b66\u4e60\u7387\u53ef\u63d0\u5347\u8fc1\u79fb\u653b\u51fb\u9c81\u68d2\u6027\u8fbe64%\uff0c\u800c\u589e\u52a0\u5b66\u4e60\u7387\u53ef\u63d0\u5347\u67e5\u8be2\u653b\u51fb\u9c81\u68d2\u6027\u8fbe28%\u3002\u5206\u5e03\u5f0f\u6a21\u578b\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u80fd\u6700\u6709\u6548\u5730\u540c\u65f6\u7f13\u89e3\u4e24\u79cd\u653b\u51fb\u3002", "conclusion": "\u4f18\u5316\u8d85\u53c2\u6570\u8bbe\u8ba1\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5206\u5e03\u5f0f\u8bad\u7ec3\u6a21\u578b\u901a\u8fc7\u9002\u5f53\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u80fd\u591f\u5b9e\u73b0\u5bf9\u6297\u8fc1\u79fb\u653b\u51fb\u548c\u67e5\u8be2\u653b\u51fb\u7684\u6700\u4f73\u6743\u8861\u6548\u679c\u3002"}}
{"id": "2511.13575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13575", "abs": "https://arxiv.org/abs/2511.13575", "authors": ["Linhan Zhou", "Shuang Li", "Neng Dong", "Yonghang Tai", "Yafei Zhang", "Huafeng Li"], "title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification", "comment": "9 pages, 4 figures, accepted by AAAI 2026", "summary": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6HPL\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u63d0\u793a\u5efa\u6a21\u8054\u5408\u4f18\u5316\u56fe\u50cf\u5230\u56fe\u50cf\uff08I2I\uff09\u548c\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u7684\u4eba\u5458\u91cd\u8bc6\u522b\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06I2I\u548cT2I\u4efb\u52a1\u5206\u5f00\u5904\u7406\uff0c\u53ef\u80fd\u5bfc\u81f4\u8868\u793a\u7ea0\u7f20\u548c\u6027\u80fd\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u540c\u65f6\u4f18\u5316\u8fd9\u4e24\u4e2a\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u63d0\u793a\u5b66\u4e60\uff08HPL\uff09\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u4efb\u52a1\u8def\u7531Transformer\uff0c\u5728\u5171\u4eab\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u53cc\u5206\u7c7b\u4ee4\u724c\uff1b2\uff09\u5206\u5c42\u63d0\u793a\u751f\u6210\u65b9\u6848\uff0c\u7ed3\u5408\u8eab\u4efd\u7ea7\u53ef\u5b66\u4e60\u4ee4\u724c\u548c\u5b9e\u4f8b\u7ea7\u4f2a\u6587\u672c\u4ee4\u724c\uff1b3\uff09\u8de8\u6a21\u6001\u63d0\u793a\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5728\u63d0\u793a\u4ee4\u724c\u7a7a\u95f4\u5f3a\u5236\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2aReID\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728I2I\u548cT2I\u4efb\u52a1\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HPL\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u63d0\u793a\u5efa\u6a21\u6210\u529f\u7edf\u4e00\u4e86I2I\u548cT2I\u4eba\u5458\u91cd\u8bc6\u522b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2511.13533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13533", "abs": "https://arxiv.org/abs/2511.13533", "authors": ["Jeffrey Wen", "Rizwan Ahmad", "Philip Schniter"], "title": "Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems", "comment": null, "summary": "In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fd1\u6781\u5c0f\u6781\u5927\u65b9\u6cd5\u7528\u4e8e\u591a\u76ee\u6807\u5171\u5f62\u9884\u6d4b\uff0c\u4e3a\u6210\u50cf\u9006\u95ee\u9898\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u7d27\u5bc6\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u540c\u65f6\u786e\u4fdd\u8054\u5408\u8fb9\u9645\u8986\u76d6\u3002", "motivation": "\u5728\u75c5\u6001\u6210\u50cf\u9006\u95ee\u9898\u4e2d\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u6807\u91cf\u4f30\u8ba1\u76ee\u6807\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u901a\u5e38\u6d89\u53ca\u591a\u4e2a\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u6e10\u8fd1\u6781\u5c0f\u6781\u5927\u591a\u76ee\u6807\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u786e\u4fdd\u8054\u5408\u8fb9\u9645\u8986\u76d6\u7684\u540c\u65f6\u63d0\u4f9b\u7d27\u5bc6\u7684\u9884\u6d4b\u533a\u95f4\u3002\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u591a\u6307\u6807\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3001\u591a\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u591a\u8f6e\u6d4b\u91cf\u91c7\u96c6\u3002", "result": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u78c1\u5171\u632f\u6210\u50cf\u6570\u636e\u7684\u6570\u503c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u73b0\u6709\u7684\u591a\u76ee\u6807\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6781\u5c0f\u6781\u5927\u65b9\u6cd5\u4e3a\u591a\u76ee\u6807\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.13675", "categories": ["cs.LG", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13675", "abs": "https://arxiv.org/abs/2511.13675", "authors": ["Minh Vu", "Andrey Lokhov"], "title": "Scientific Data Compression and Super-Resolution Sampling", "comment": null, "summary": "Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6307\u6570\u65cf\u5b66\u4e60\u7684\u65b0\u578b\u79d1\u5b66\u6570\u636e\u538b\u7f29\u548c\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u80fd\u591f\u5728\u538b\u7f29\u6570\u636e\u7684\u540c\u65f6\u4fdd\u6301\u7269\u7406\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u652f\u6301\u538b\u7f29\u6bd4\u4e0e\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u7075\u6d3b\u6743\u8861\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u6a21\u62df\u3001\u89c2\u6d4b\u548c\u5927\u89c4\u6a21\u5b9e\u9a8c\u4ea7\u751f\u7684\u6570\u636e\u91cf\u5f80\u5f80\u8d85\u8fc7\u5b58\u50a8\u3001\u5904\u7406\u548c\u5206\u6790\u7684\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7ba1\u7406\u6d77\u91cf\u6570\u636e\u96c6\u540c\u65f6\u4fdd\u7559\u57fa\u672c\u7269\u7406\u7279\u5f81\u7684\u6570\u636e\u7f29\u51cf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u68c0\u67e5\u70b9\u548c\u91cd\u542f\u7b49\u9700\u8981\u4ece\u538b\u7f29\u8868\u793a\u4e2d\u6062\u590d\u6570\u636e\u7684\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "method": "\u57fa\u4e8e\u8fd1\u671f\u6307\u6570\u65cf\u5b66\u4e60\u7684\u8fdb\u5c55\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u79d1\u5b66\u6570\u636e\u538b\u7f29\u548c\u8d85\u5206\u8fa8\u7387\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u7269\u7406\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u538b\u7f29\u6bd4\u4e0e\u91cd\u5efa\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u7075\u6d3b\u6743\u8861\uff0c\u80fd\u591f\u4ece\u538b\u7f29\u8868\u793a\u4e2d\u6062\u590d\u6570\u636e\u5e76\u4fdd\u8bc1\u5173\u952e\u7269\u7406\u7279\u6027\u7684\u4fdd\u5b58\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79d1\u5b66\u6570\u636e\u538b\u7f29\u548c\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6570\u636e\u6062\u590d\u4fdd\u8bc1\u7684\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5982\u68c0\u67e5\u70b9\u548c\u91cd\u542f\u64cd\u4f5c\u3002"}}
{"id": "2511.13587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13587", "abs": "https://arxiv.org/abs/2511.13587", "authors": ["Haotian Dong", "Ye Li", "Rongwei Lu", "Chen Tang", "Shu-Tao Xia", "Zhi Wang"], "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping", "comment": null, "summary": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVVS\u7684\u65b0\u9896\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u9a8c\u8bc1\u8df3\u8fc7\u6765\u52a0\u901f\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u751f\u6210\uff0c\u5c06\u76ee\u6807\u6a21\u578b\u524d\u5411\u4f20\u9012\u6b21\u6570\u51cf\u5c112.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u89c9\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u867d\u7136\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0c\u4f46\u5176\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u8303\u5f0f\u5f15\u5165\u4e86\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u91c7\u7528\"\u8d77\u8349\u4e00\u6b65\uff0c\u9a8c\u8bc1\u4e00\u6b65\"\u7684\u6a21\u5f0f\uff0c\u65e0\u6cd5\u76f4\u63a5\u51cf\u5c11\u524d\u5411\u4f20\u9012\u6b21\u6570\uff0c\u9650\u5236\u4e86\u52a0\u901f\u6f5c\u529b\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u4ee4\u724c\u53ef\u4e92\u6362\u6027\u7684\u89c2\u5bdf\uff0c\u63d0\u51faVVS\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u6a21\u5757\uff1a(1)\u5e26\u52a8\u6001\u622a\u65ad\u7684\u514d\u9a8c\u8bc1\u4ee4\u724c\u9009\u62e9\u5668\uff0c(2)\u4ee4\u724c\u7ea7\u7279\u5f81\u7f13\u5b58\u548c\u91cd\u7528\uff0c(3)\u7ec6\u7c92\u5ea6\u7684\u8df3\u8fc7\u6b65\u9aa4\u8c03\u5ea6\u3002", "result": "VVS\u5c06\u76ee\u6807\u6a21\u578b\u524d\u5411\u4f20\u9012\u6b21\u6570\u76f8\u5bf9\u4e8e\u539f\u59cb\u81ea\u56de\u5f52\u89e3\u7801\u51cf\u5c11\u4e862.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5728\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u4e0a\u4f18\u4e8e\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\u3002", "conclusion": "VVS\u901a\u8fc7\u90e8\u5206\u9a8c\u8bc1\u8df3\u8fc7\u6765\u52a0\u901f\u89c6\u89c9\u81ea\u56de\u5f52\u751f\u6210\uff0c\u5c55\u793a\u4e86\u91cd\u5851\u63a8\u6d4b\u89e3\u7801\u8303\u5f0f\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u4e3a\u89c6\u89c9AR\u6a21\u578b\u63d0\u4f9b\u4e86\u4f18\u8d8a\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13535", "abs": "https://arxiv.org/abs/2511.13535", "authors": ["Farhin Farhad Riya", "Shahinul Hoque", "Jinyuan Stella Sun", "Olivera Kotevska"], "title": "Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew", "comment": null, "summary": "As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u7c7b\u578b\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5fae\u5c0f\u989c\u8272\u6270\u52a8\u6765\u7834\u574f\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u800c\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\uff0c\u6311\u6218\u4e86\u6a21\u578b\u5ba1\u8ba1\u4e2d\u6b63\u786e\u9884\u6d4b\u5373\u610f\u5473\u7740\u5fe0\u5b9e\u89e3\u91ca\u7684\u5047\u8bbe\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u53ef\u89c6\u5316\u89e3\u91ca\u6280\u672f\u5bf9\u652f\u6301\u900f\u660e\u5ea6\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u4e00\u79cd\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u7834\u574f\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u578b\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e86\u540d\u4e3a\u8272\u5ea6\u6270\u52a8\u6a21\u5757\u7684\u663e\u8457\u6027\u611f\u77e5\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u524d\u666f\u548c\u80cc\u666f\u4e4b\u95f4\u7684\u989c\u8272\u5bf9\u6bd4\u5ea6\u6765\u5236\u4f5c\u5bf9\u6297\u6837\u672c\uff0c\u4ece\u800c\u7834\u574f\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002\u8fd9\u4e9b\u6270\u52a8\u5728\u8bad\u7ec3\u8f6e\u6b21\u4e2d\u7d2f\u79ef\uff0c\u4ee5\u9690\u853d\u548c\u6301\u4e45\u7684\u65b9\u5f0f\u6bd2\u5316\u5168\u5c40\u6a21\u578b\u7684\u5185\u90e8\u7279\u5f81\u5f52\u56e0\u3002", "result": "\u653b\u51fb\u5c06Grad-CAM\u89e3\u91ca\u4e2d\u7684\u5cf0\u503c\u6fc0\u6d3b\u91cd\u53e0\u51cf\u5c11\u4e86\u9ad8\u8fbe35%\uff0c\u540c\u65f6\u5728\u6240\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc796%\u3002\u6807\u51c6\u8bad\u7ec3\u6d41\u7a0b\u4e0d\u8db3\u4ee5\u68c0\u6d4b\u6216\u7f13\u89e3\u89e3\u91ca\u9000\u5316\uff0c\u7279\u522b\u662f\u5728\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\uff0c\u7ec6\u5fae\u7684\u989c\u8272\u6270\u52a8\u66f4\u96be\u5bdf\u89c9\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u53ef\u89e3\u91ca\u6027\u672c\u8eab\u53ef\u4ee5\u6210\u4e3a\u653b\u51fb\u9762\uff0c\u6311\u6218\u4e86\u6a21\u578b\u5ba1\u8ba1\u4e2d\u6b63\u786e\u9884\u6d4b\u5373\u610f\u5473\u7740\u5fe0\u5b9e\u89e3\u91ca\u7684\u5e38\u89c1\u5047\u8bbe\u3002\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u7684\u7ec6\u5fae\u989c\u8272\u6270\u52a8\u5bf9\u6a21\u578b\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002"}}
{"id": "2511.13680", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.13680", "abs": "https://arxiv.org/abs/2511.13680", "authors": ["Leopoldo Agorio", "Juan Cervi\u00f1o", "Miguel Calvo-Fullana", "Alejandro Ribeiro", "Juan Andr\u00e9s Bazerque"], "title": "Cross-Learning from Scarce Data via Multi-Task Constrained Optimization", "comment": "13 pages, 11 figures", "summary": "A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \\emph{cross-learning} framework to overcome data scarcity by jointly estimating \\emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u4ea4\u53c9\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u591a\u4e2a\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u786e\u5b9a\u6027\u53c2\u6570\u6765\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u6570\u636e\u4e30\u5bcc\u7684\u4efb\u52a1\u5411\u6570\u636e\u7a00\u7f3a\u4efb\u52a1\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u4efb\u52a1\u9700\u8981\u5927\u91cf\u4ee3\u8868\u6027\u6570\u636e\uff0c\u5f53\u6570\u636e\u6709\u9650\u65f6\uff0c\u5b66\u4e60\u6a21\u578b\u65e0\u6cd5\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6848\u4f8b\u3002\u672c\u6587\u65e8\u5728\u514b\u670d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5728\u53c2\u6570\u63a8\u65ad\u5bf9\u6709\u9650\u6570\u636e\u81f3\u5173\u91cd\u8981\u7684\u573a\u666f\u4e2d\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u8054\u5408\u4f30\u8ba1\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u7ea6\u675f\u6761\u4ef6\u63a7\u5236\u4e0d\u540c\u6a21\u578b\u53c2\u6570\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5141\u8bb8\u53c2\u6570\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u540c\u65f6\u7ed3\u5408\u6765\u81ea\u591a\u4e2a\u6570\u636e\u6e90\u7684\u4fe1\u606f\u3002", "result": "\u5728\u63a7\u5236\u6846\u67b6\u4e0b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u4f20\u67d3\u75c5\u4f20\u64ad\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u4ea4\u53c9\u5b66\u4e60\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u53ef\u9760\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "conclusion": "\u4ea4\u53c9\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u4ece\u6570\u636e\u4e30\u5bcc\u7684\u4efb\u52a1\u5411\u6570\u636e\u7a00\u7f3a\u7684\u4efb\u52a1\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e3a\u6709\u9650\u6570\u636e\u4e0b\u7684\u53c2\u6570\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.13539", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13539", "abs": "https://arxiv.org/abs/2511.13539", "authors": ["Yuanchao Wang", "Tian Qin", "Eduardo Valle", "Bruno Abrahao"], "title": "BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse", "comment": "8 pages", "summary": "Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.", "AI": {"tldr": "BootOOD\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u76d1\u7763\u7684OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4eceID\u6570\u636e\u4e2d\u5408\u6210\u4f2aOOD\u7279\u5f81\uff0c\u5229\u7528\u795e\u7ecf\u5d29\u6e83\u73b0\u8c61\uff0c\u4f7f\u7528\u57fa\u4e8e\u7279\u5f81\u8303\u6570\u7684\u8f7b\u91cf\u7ea7\u8f85\u52a9\u5934\u8fdb\u884cOOD\u68c0\u6d4b\uff0c\u5728\u8bed\u4e49\u76f8\u4f3c\u7684OOD\u6837\u672c\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684OOD\u68c0\u6d4b\u5668\u5728\u5904\u7406\u4e0eID\u7c7b\u522b\u8bed\u4e49\u76f8\u4f3c\u7684OOD\u6837\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e13\u95e8\u5904\u7406\u8fd9\u79cd\u8bed\u4e49\u6311\u6218\u6027OOD\u6837\u672c\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7ID\u8868\u793a\u7684\u7b80\u5355\u53d8\u6362\u5408\u6210\u4f2aOOD\u7279\u5f81\uff0c\u5229\u7528\u795e\u7ecf\u5d29\u6e83\u73b0\u8c61\uff0c\u5f15\u5165\u57fa\u4e8e\u7279\u5f81\u8303\u6570\u7684\u8f7b\u91cf\u7ea7\u8f85\u52a9\u5934\u8fdb\u884c\u534a\u5f84\u5206\u7c7b\uff0c\u5c06OOD\u68c0\u6d4b\u4e0e\u4e3b\u5206\u7c7b\u5668\u89e3\u8026\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet-200\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBootOOD\u4f18\u4e8e\u5148\u9a8c\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u5728\u6ca1\u6709\u5f02\u5e38\u66b4\u9732\u7684\u60c5\u51b5\u4e0b\u8d85\u8d8a\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u5f02\u5e38\u66b4\u9732\u65b9\u6cd5\u7ade\u4e89\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8ID\u51c6\u786e\u7387\u3002", "conclusion": "BootOOD\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763OOD\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u8bed\u4e49\u76f8\u4f3c\u7684OOD\u6837\u672c\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.13621", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13621", "abs": "https://arxiv.org/abs/2511.13621", "authors": ["Dimitrios Koutsianos", "Ladislav Mosner", "Yannis Panagakis", "Themos Stafylakis"], "title": "Alpha Divergence Losses for Biometric Verification", "comment": null, "summary": "Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $\u03b1$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $\u03b1>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $\u03b1$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u03b1-\u6563\u5ea6\u7684\u8fb9\u9645\u635f\u5931\u51fd\u6570\uff1aQ-Margin\uff08\u5728\u53c2\u8003\u5ea6\u91cf\u4e2d\u5f15\u5165\u8fb9\u9645\uff09\u548cA3M\uff08\u5728\u5bf9\u6570\u4e2d\u5f15\u5165\u8fb9\u9645\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u4eba\u8138\u548c\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8bef\u63a5\u53d7\u7387\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8fb9\u9645\u7684softmax\u635f\u5931\uff08\u5982CosFace\u548cArcFace\uff09\u5728\u4eba\u8138\u548c\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u03b1-\u6563\u5ea6\u635f\u5931\u51fd\u6570\u80fd\u591f\u8bf1\u5bfc\u7a00\u758f\u89e3\uff0c\u5c06\u5176\u4e0e\u89d2\u5ea6\u8fb9\u9645\u7ed3\u5408\u4ee5\u63d0\u5347\u9a8c\u8bc1\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u5c06\u89d2\u5ea6\u8fb9\u9645\u96c6\u6210\u5230\u03b1-\u6563\u5ea6\u635f\u5931\u4e2d\uff1a\u5728\u53c2\u8003\u5ea6\u91cf\u4e2d\u5f15\u5165\u8fb9\u9645\uff08Q-Margin\uff09\u548c\u5728\u5bf9\u6570\u4e2d\u5f15\u5165\u8fb9\u9645\uff08A3M\uff09\uff0c\u5e76\u9488\u5bf9A3M\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u63d0\u51fa\u4e86\u539f\u578b\u91cd\u65b0\u521d\u59cb\u5316\u7b56\u7565\u3002", "result": "\u5728IJB-B\u548cIJB-C\u4eba\u8138\u9a8c\u8bc1\u57fa\u51c6\u4ee5\u53caVoxCeleb\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8bef\u63a5\u53d7\u7387\u4e0b\u660e\u663e\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684Q-Margin\u548cA3M\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u03b1-\u6563\u5ea6\u7684\u7a00\u758f\u6027\u548c\u89d2\u5ea6\u8fb9\u9645\u7684\u5224\u522b\u80fd\u529b\uff0c\u4e3a\u9ad8\u5b89\u5168\u6027\u5e94\u7528\uff08\u5982\u94f6\u884c\u8ba4\u8bc1\uff09\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13699", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.13699", "abs": "https://arxiv.org/abs/2511.13699", "authors": ["Parikshit Gopalan", "Konstantinos Stavropoulos", "Kunal Talwar", "Pranay Tankala"], "title": "Efficient Calibration for Decision Making", "comment": "50 pages, 3 figures", "summary": "A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.\n  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u540e\u5904\u7406\u51fd\u6570\u65cf\u7684\u6821\u51c6\u51b3\u7b56\u635f\u5931\u5ea6\u91cf\u65b9\u6cd5CDL_K\uff0c\u4ee5\u89e3\u51b3\u539f\u59cbCDL\u5ea6\u91cf\u7684\u8ba1\u7b97\u56f0\u96be\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u5176\u4fe1\u606f\u8bba\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u539f\u59cb\u6821\u51c6\u51b3\u7b56\u635f\u5931(CDL)\u5728\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u96be\u4ee5\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u56f0\u96be\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u9650\u5236\u540e\u5904\u7406\u51fd\u6570\u7684\u7ed3\u6784\u6765\u83b7\u5f97\u53ef\u8ba1\u7b97\u7684\u6821\u51c6\u5ea6\u91cf\u3002", "method": "\u5b9a\u4e49\u4e86\u76f8\u5bf9\u4e8e\u7ed3\u6784\u5316\u540e\u5904\u7406\u51fd\u6570\u65cfK\u7684\u6821\u51c6\u51b3\u7b56\u635f\u5931CDL_K\uff0c\u8003\u8651\u6240\u6709\u9002\u5f53\u635f\u5931\u4f46\u5c06\u540e\u5904\u7406\u9650\u5236\u5728\u7ed3\u6784\u5316\u65cfK\u4e2d\u3002\u5f00\u53d1\u4e86CDL_K\u4fe1\u606f\u8bba\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u53ef\u5904\u7406\u6027\u7684\u7efc\u5408\u7406\u8bba\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u81ea\u7136\u51fd\u6570\u7c7bK\u7684CDL_K\u4e0a\u4e0b\u754c\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u91cd\u65b0\u6821\u51c6\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7ed3\u6784\u5316\u540e\u5904\u7406\u51fd\u6570\u65cf\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u539f\u59cbCDL\u5ea6\u91cf\u7684\u8ba1\u7b97\u56f0\u96be\u95ee\u9898\uff0c\u4e3a\u51b3\u7b56\u6821\u51c6\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9a\u4e49\u548c\u7b97\u6cd5\u6280\u672f\u3002"}}
{"id": "2511.13571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13571", "abs": "https://arxiv.org/abs/2511.13571", "authors": ["Ziyang Huang", "Jiagang Chen", "Jin Liu", "Shunping Ji"], "title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation", "comment": "Accepted at AAAI 2026 as a Conference Paper", "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.", "AI": {"tldr": "Opt3DGS\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\u6765\u89e3\u51b33D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\u7684\u5c40\u90e8\u6700\u4f18\u9677\u9631\u548c\u6536\u655b\u8d28\u91cf\u4e0d\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a2\u7d22\u548c\u66f2\u7387\u5f15\u5bfc\u5f00\u53d1\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "3DGS\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6838\u5fc3\u4f18\u5316\u95ee\u9898\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u9677\u5165\u5c40\u90e8\u6700\u4f18\u548c\u6536\u655b\u8d28\u91cf\u4e0d\u8db3\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u8fc7\u7a0b\uff1a\u63a2\u7d22\u9636\u6bb5\u4f7f\u7528\u81ea\u9002\u5e94\u52a0\u6743\u968f\u673a\u68af\u5ea6Langevin\u52a8\u529b\u5b66\uff08SGLD\uff09\u589e\u5f3a\u5168\u5c40\u641c\u7d22\uff1b\u5f00\u53d1\u9636\u6bb5\u4f7f\u7528\u5c40\u90e8\u62df\u725b\u987f\u65b9\u5411\u5f15\u5bfc\u7684Adam\u4f18\u5316\u5668\u5229\u7528\u66f2\u7387\u4fe1\u606f\u8fdb\u884c\u7cbe\u786e\u6536\u655b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cOpt3DGS\u5728\u4e0d\u6539\u53d83DGS\u5e95\u5c42\u8868\u793a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "Opt3DGS\u901a\u8fc7\u6539\u8fdb3DGS\u7684\u4f18\u5316\u8fc7\u7a0b\u6709\u6548\u89e3\u51b3\u4e86\u5176\u4f18\u5316\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u6027\u80fd\u3002"}}
{"id": "2511.13702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13702", "abs": "https://arxiv.org/abs/2511.13702", "authors": ["Luyao Niu", "Nuoxian Huang"], "title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification", "comment": null, "summary": "Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.", "AI": {"tldr": "\u63d0\u51faST-ProC\u6846\u67b6\u89e3\u51b3GPS\u8f68\u8ff9\u51fa\u884c\u6a21\u5f0f\u8bc6\u522b\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u539f\u578b\u591a\u76ee\u6807\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u7ed3\u5408\u56fe\u6b63\u5219\u5316\u3001\u539f\u578b\u951a\u5b9a\u548c\u8fb9\u754c\u611f\u77e5\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "GPS\u8f68\u8ff9\u51fa\u884c\u6a21\u5f0f\u8bc6\u522b\u56e0\u6807\u6ce8\u6210\u672c\u9ad8\u5bfc\u81f4\u6807\u7b7e\u7a00\u7f3a\uff0c\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u786e\u8ba4\u504f\u5dee\u95ee\u9898\u4e14\u5ffd\u7565\u6570\u636e\u6d41\u5f62\u7ed3\u6784\u3002", "method": "ST-ProC\u6846\u67b6\u7ed3\u5408\u56fe\u539f\u578b\u6838\u5fc3\u4e0e\u57fa\u7840\u534a\u76d1\u7763\u5b66\u4e60\u652f\u6301\uff0c\u5305\u62ec\u56fe\u6b63\u5219\u5316\u3001\u539f\u578b\u951a\u5b9a\u3001\u8fb9\u754c\u611f\u77e5\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u4ee5\u53ca\u5bf9\u6bd4\u5b66\u4e60\u548c\u5e08\u751f\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "ST-ProC\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7a00\u758f\u6807\u7b7e\u8bbe\u7f6e\u4e2d\u6bd4FixMatch\u7b49\u6700\u5148\u8fdb\u65b9\u6cd5\u6027\u80fd\u63d0\u534721.5%\u3002", "conclusion": "ST-ProC\u901a\u8fc7\u6709\u6548\u5229\u7528\u6570\u636e\u6d41\u5f62\u548c\u566a\u58f0\u6291\u5236\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u51fa\u884c\u6a21\u5f0f\u8bc6\u522b\u4e2d\u7684\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u4e3a\u57ce\u5e02\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13705", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.13705", "abs": "https://arxiv.org/abs/2511.13705", "authors": ["Alaa Mezghiche"], "title": "Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering", "comment": "16 pages", "summary": "Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI \"Gene Expression Cancer RNA-Seq\" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u548c\u805a\u7c7b\u7a33\u5b9a\u6027\u5206\u6790\u5728RNA-seq\u6570\u636e\u4e2d\u5bfb\u627e\u7f55\u89c1\u4f46\u53ef\u91cd\u73b0\u7684\u57fa\u56e0\u7ec4\u4e9a\u578b\u3002\u5728\u6cdb\u764c\u5206\u6790\u4e2d\uff0c\u805a\u7c7b\u4e3b\u8981\u6309\u7ec4\u7ec7\u6765\u6e90\u5212\u5206\uff0c\u800c\u5728\u80be\u900f\u660e\u7ec6\u80de\u764c(KIRC)\u5185\u90e8\u5206\u6790\u53d1\u73b0\u4e86\u4e00\u4e2a\u7f55\u89c1\u4e14\u7a33\u5b9a\u7684\u4e9a\u578b\u3002", "motivation": "\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u5728\u9ad8\u7ef4RNA-seq\u6570\u636e\u4e2d\u53d1\u73b0\u6807\u51c6\u6807\u7b7e\u4e4b\u5916\u7684\u5206\u5b50\u4e9a\u578b\uff0c\u7279\u522b\u662f\u5bfb\u627e\u7f55\u89c1\u4f46\u53ef\u91cd\u73b0\u7684\u57fa\u56e0\u7ec4\u4e9a\u578b\u3002", "method": "\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u8868\u793a\u4e0e\u805a\u7c7b\u548c\u7a33\u5b9a\u6027\u5206\u6790\uff1a\u9009\u62e9\u9ad8\u53d8\u5f02\u57fa\u56e0\uff0c\u8bad\u7ec3\u524d\u9988\u81ea\u7f16\u7801\u5668\uff08128\u7ef4\u6f5c\u7a7a\u95f4\uff09\uff0c\u8fd0\u884ck-means\u805a\u7c7b\uff08k=2-10\uff09\uff0c\u4f7f\u7528\u7a33\u5b9a\u6027\u5206\u6790\uff08Jaccard\u6307\u6570\u22650.60\uff09\u548c\u7f55\u89c1\u6027\u6807\u51c6\uff08<10%\uff09\u6765\u8bc6\u522b\u4e9a\u578b\u3002", "result": "\u6cdb\u764c\u5206\u6790\u663e\u793a\u805a\u7c7b\u4e0e\u7ec4\u7ec7\u6765\u6e90\u9ad8\u5ea6\u4e00\u81f4\uff08Cramer's V=0.887\uff09\u3002\u5728KIRC\u5185\u90e8\u53d1\u73b0k=5\u65f6\u5b58\u5728\u4e00\u4e2a\u7f55\u89c1\u7c07C0\uff086.85%\u60a3\u8005\uff09\uff0c\u5177\u6709\u9ad8\u7a33\u5b9a\u6027\uff08Jaccard=0.787\uff09\uff0c\u901a\u8fc7\u5dee\u5f02\u8868\u8fbe\u5206\u6790\u8bc6\u522b\u51fa\u8fde\u8d2f\u7684\u6807\u8bb0\u57fa\u56e0\u3002", "conclusion": "\u6cdb\u764c\u805a\u7c7b\u4e3b\u8981\u53d7\u7ec4\u7ec7\u6765\u6e90\u4e3b\u5bfc\uff0c\u800c\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u5355\u764c\u79cd\u5206\u6790\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u7f55\u89c1\u4e14\u53ef\u91cd\u73b0\u7684KIRC\u4e9a\u578b\u3002"}}
{"id": "2511.13586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13586", "abs": "https://arxiv.org/abs/2511.13586", "authors": ["Yinuo Xu", "Yan Cui", "Mingyao Li", "Zhi Huang"], "title": "Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images", "comment": null, "summary": "Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.\n  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.\n  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.", "AI": {"tldr": "NuClass\u662f\u4e00\u4e2a\u75c5\u7406\u5b66\u5bb6\u5de5\u4f5c\u6d41\u7a0b\u542f\u53d1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7ec6\u80de\u7ea7\u522b\u7684\u591a\u5c3a\u5ea6\u6574\u5408\u6838\u5f62\u6001\u548c\u5fae\u73af\u5883\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u95e8\u63a7\u6a21\u5757\u5e73\u8861\u5c40\u90e8\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u5e76\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u76ee\u6807\u4fc3\u8fdb\u4e92\u8865\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u5757\u7684\u6a21\u578b\u80fd\u591f\u6355\u6349\u8be6\u7ec6\u7684\u6838\u5f62\u6001\u4f46\u5f80\u5f80\u65e0\u6cd5\u878d\u5165\u5f71\u54cd\u7ec6\u80de\u529f\u80fd\u548c\u8eab\u4efd\u7684\u66f4\u5e7f\u6cdb\u7ec4\u7ec7\u4e0a\u4e0b\u6587\uff0c\u4e14\u53ef\u7528\u7684\u4eba\u7c7b\u6807\u6ce8\u901a\u5e38\u662f\u7c97\u7c92\u5ea6\u7684\uff0c\u96be\u4ee5\u83b7\u5f97\u7ec6\u7c92\u5ea6\u7684\u4e9a\u578b\u7ea7\u76d1\u7763\u3002", "method": "NuClass\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1aPath local\uff08\u5173\u6ce8224\u00d7224\u50cf\u7d20\u88c1\u526a\u7684\u6838\u5f62\u6001\uff09\u548cPath global\uff08\u5efa\u6a21\u5468\u56f41024\u00d71024\u50cf\u7d20\u90bb\u57df\uff09\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u95e8\u63a7\u6a21\u5757\u81ea\u9002\u5e94\u5e73\u8861\u5c40\u90e8\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u5e76\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u76ee\u6807\u3002", "result": "\u5728\u4e09\u4e2a\u5b8c\u5168\u4fdd\u7559\u7684\u961f\u5217\u4e0a\u8bc4\u4f30\uff0cNuClass\u5728\u5176\u6700\u4f73\u8868\u73b0\u7c7b\u522b\u4e0a\u8fbe\u523096%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u591a\u5c3a\u5ea6\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u878d\u5408\u53ef\u4ee5\u5f25\u5408\u5207\u7247\u7ea7\u75c5\u7406\u57fa\u7840\u6a21\u578b\u4e0e\u53ef\u9760\u7ec6\u80de\u7ea7\u8868\u578b\u9884\u6d4b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.13712", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13712", "abs": "https://arxiv.org/abs/2511.13712", "authors": ["Kiana Vu", "\u0130smet Sel\u00e7uk \u00d6zer", "Phung Lai", "Zheng Wu", "Thilanka Munasinghe", "Jennifer Wei"], "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness", "comment": null, "summary": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u53ef\u89e3\u91caAI\u5728\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u91ce\u706b\u9884\u6d4b\u4e3a\u4f8b\uff0c\u8bc4\u4f30AI\u6a21\u578b\u5e76\u4f7f\u7528SHAP\u89e3\u91ca\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u51b3\u7b56\u652f\u6301\u80fd\u529b\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u6781\u7aef\u4e8b\u4ef6\u7684\u9891\u7387\u548c\u4e25\u91cd\u6027\uff0c\u9700\u8981\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u64cd\u4f5c\u7684\u9884\u6d4b\u3002\u867d\u7136AI\u6a21\u578b\u5728\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u9650\u5236\u4e86\u5728\u5b9e\u9645\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u91ce\u706b\u9884\u6d4b\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u8bc4\u4f30\u591a\u79cdAI\u6a21\u578b\uff0c\u5e76\u91c7\u7528SHAP\u65b9\u6cd5\u6765\u63ed\u793a\u5173\u952e\u7279\u5f81\u3001\u51b3\u7b56\u8def\u5f84\u548c\u6f5c\u5728\u504f\u89c1\uff0c\u63d0\u4f9b\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u7684\u53ef\u89c6\u5316\u3002", "result": "\u5206\u6790\u8868\u660eXAI\u4e0d\u4ec5\u80fd\u6f84\u6e05\u6a21\u578b\u63a8\u7406\uff0c\u8fd8\u80fd\u652f\u6301\u9886\u57df\u4e13\u5bb6\u548c\u54cd\u5e94\u56e2\u961f\u7684\u5173\u952e\u51b3\u7b56\uff0c\u901a\u8fc7\u60c5\u5883\u5316\u7279\u5f81\u91cd\u8981\u6027\u548c\u65f6\u7a7a\u6a21\u5f0f\u6765\u589e\u5f3aAI\u89e3\u91ca\u7684\u53ef\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0AI\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u6027\uff0c\u8fd8\u9700\u8981\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u4fe1\u6027\uff0c\u8fd9\u5bf9\u4e8e\u707e\u5bb3\u51c6\u5907\u3001\u98ce\u9669\u7f13\u89e3\u548c\u6c14\u5019\u97e7\u6027\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.13714", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13714", "abs": "https://arxiv.org/abs/2511.13714", "authors": ["Junwei Yu", "Trevor Darrell", "XuDong Wang"], "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity", "comment": null, "summary": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.", "AI": {"tldr": "UnSAMv2\u662f\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5c31\u80fd\u5b9e\u73b0\u4efb\u610f\u7c92\u5ea6\u5206\u5272\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u53d1\u73b0\u4e30\u5bcc\u7684\u63a9\u7801-\u7c92\u5ea6\u5bf9\u548c\u5f15\u5165\u7c92\u5ea6\u63a7\u5236\u5d4c\u5165\uff0c\u5728\u4ec5\u4f7f\u75286K\u672a\u6807\u6ce8\u56fe\u50cf\u548c0.02%\u989d\u5916\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86SAM-2\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SAM\u6a21\u578b\u5728\u63a7\u5236\u5206\u5272\u7c92\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7528\u6237\u901a\u5e38\u9700\u8981\u624b\u52a8\u7ec6\u5316\u7ed3\u679c\u6765\u8fbe\u5230\u6240\u9700\u7ec6\u8282\u6c34\u5e73\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u65e2\u6a21\u7cca\u53c8\u6602\u8d35\uff0c\u4f7f\u5f97\u76d1\u7763\u89e3\u51b3\u65b9\u6848\u4e0d\u53ef\u884c\u3002", "method": "\u6269\u5c55UnSAM\u7684\u5206\u6cbb\u7b56\u7565\uff0c\u53d1\u73b0\u4e30\u5bcc\u7684\u63a9\u7801-\u7c92\u5ea6\u5bf9\uff0c\u5e76\u5f15\u5165\u65b0\u9896\u7684\u7c92\u5ea6\u63a7\u5236\u5d4c\u5165\uff0c\u5b9e\u73b0\u5bf9\u5206\u5272\u5c3a\u5ea6\u7684\u7cbe\u786e\u8fde\u7eed\u63a7\u5236\u3002", "result": "\u5728\u8d85\u8fc711\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUnSAMv2\u663e\u8457\u63d0\u5347\u4e86SAM-2\u7684\u6027\u80fd\uff1aNoC90\u4ece5.69\u964d\u81f34.75\uff0c1-IoU\u4ece58.0\u63d0\u5347\u81f373.1\uff0cAR1000\u4ece49.6\u63d0\u5347\u81f368.3\u3002", "conclusion": "\u5c11\u91cf\u672a\u6807\u6ce8\u6570\u636e\u7ed3\u5408\u7c92\u5ea6\u611f\u77e5\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u91ca\u653e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13719", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13719", "abs": "https://arxiv.org/abs/2511.13719", "authors": ["Zhongang Cai", "Ruisi Wang", "Chenyang Gu", "Fanyi Pu", "Junxiang Xu", "Yubo Wang", "Wanqi Yin", "Zhitao Yang", "Chen Wei", "Qingping Sun", "Tongxi Zhou", "Jiaqi Li", "Hui En Pang", "Oscar Qian", "Yukun Wei", "Zhiqian Lin", "Xuanke Shi", "Kewang Deng", "Xiaoyang Han", "Zukai Chen", "Xiangyu Fan", "Hanming Deng", "Lewei Lu", "Liang Pan", "Bo Li", "Ziwei Liu", "Quan Wang", "Dahua Lin", "Lei Yang"], "title": "Scaling Spatial Intelligence with Multimodal Foundation Models", "comment": "Model: https://huggingface.co/collections/sensenova/sensenova-si; Code: https://github.com/OpenSenseNova/SenseNova-SI", "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.", "AI": {"tldr": "SenseNova-SI\u9879\u76ee\u901a\u8fc7\u6784\u5efa800\u4e07\u591a\u6837\u5316\u7684\u7a7a\u95f4\u667a\u80fd\u6570\u636e\u96c6\uff0c\u5728\u73b0\u6709\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0a\u63d0\u5347\u4e86\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u6269\u5c55\u3001\u6cdb\u5316\u80fd\u529b\u7b49\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u63d0\u5347\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8eQwen3-VL\u3001InternVL3\u548cBagel\u7b49\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u7cfb\u7edf\u6784\u5efa\u5305\u542b800\u4e07\u591a\u6837\u5316\u6837\u672c\u7684SenseNova-SI-8M\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e25\u683c\u7684\u7a7a\u95f4\u80fd\u529b\u5206\u7c7b\u6cd5\u8fdb\u884c\u6570\u636e\u6574\u7406\u3002", "result": "\u5728\u591a\u4e2a\u7a7a\u95f4\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aVSI-Bench 68.7%\u3001MMSI 43.3%\u3001MindCube 85.6%\u3001ViewSpatial 54.6%\u3001SITE 50.1%\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u901a\u7528\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff08MMBench-En 84.9%\uff09\u3002", "conclusion": "SenseNova-SI\u9879\u76ee\u9a8c\u8bc1\u4e86\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u667a\u80fd\u80fd\u529b\uff0c\u53d1\u73b0\u4e86\u65e9\u671f\u6d8c\u73b0\u7684\u6cdb\u5316\u80fd\u529b\u8ff9\u8c61\uff0c\u5e76\u516c\u5f00\u4e86\u6240\u6709\u65b0\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.13615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13615", "abs": "https://arxiv.org/abs/2511.13615", "authors": ["Kesi Xu", "Eleni Chiou", "Ali Varamesh", "Laura Acqualagna", "Nasir Rajpoot"], "title": "Tissue Aware Nuclei Detection and Classification Model for Histopathology Images", "comment": "5 pages, 3 figures. Under review", "summary": "Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.", "AI": {"tldr": "TAND\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u7ec4\u7ec7\u611f\u77e5\u7ec6\u80de\u6838\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u70b9\u7ea7\u76d1\u7763\u548c\u7ec4\u7ec7\u63a9\u7801\u6761\u4ef6\u5316\uff0c\u5b9e\u73b0\u4e86\u7ec6\u80de\u6838\u7684\u8054\u5408\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u5728PUMA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5bf9\u8be6\u7ec6\u4e13\u5bb6\u6807\u6ce8\u7684\u4f9d\u8d56\u548c\u7ec4\u7ec7\u4e0a\u4e0b\u6587\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7ec6\u80de\u6838\u68c0\u6d4b\u548c\u5206\u7c7b\u65b9\u6cd5\u4ee5\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\u3002", "method": "TAND\u5c06ConvNeXt\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4e0e\u51bb\u7ed3\u7684Virchow-2\u7ec4\u7ec7\u5206\u5272\u5206\u652f\u8026\u5408\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u7a7a\u95f4\u7279\u5f81\u7ebf\u6027\u8c03\u5236\u6280\u672f\uff0c\u5229\u7528\u8bed\u4e49\u7ec4\u7ec7\u6982\u7387\u9009\u62e9\u6027\u5730\u8c03\u8282\u5206\u7c7b\u6d41\u3002", "result": "\u5728PUMA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTAND\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u7ec4\u7ec7\u65e0\u5173\u57fa\u7ebf\u548c\u63a9\u7801\u76d1\u7763\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4e0a\u76ae\u7ec6\u80de\u3001\u5185\u76ae\u7ec6\u80de\u548c\u57fa\u8d28\u7ec6\u80de\u7b49\u7ec4\u7ec7\u4f9d\u8d56\u6027\u7ec6\u80de\u7c7b\u578b\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7ec4\u7ec7\u63a9\u7801\u8fdb\u884c\u5355\u7ec6\u80de\u5206\u7c7b\u6761\u4ef6\u5316\u7684\u65b9\u6cd5\uff0c\u4e3a\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2511.13644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13644", "abs": "https://arxiv.org/abs/2511.13644", "authors": ["Shrenik Patel", "Daivik Patel"], "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding", "comment": null, "summary": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.", "AI": {"tldr": "CacheFlow\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u95ee\u7b54\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u4ee4\u724c\u4e22\u5f03\u548c\u538b\u7f29\u957f\u671f\u8bb0\u5fc6\u6765\u51cf\u5c11\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u7b54\u6848\u51c6\u786e\u6027\u7684\u540c\u65f6\u5904\u7406\u66f4\u5c11\u7684\u4ee4\u724c\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u95ee\u7b54\u65f6\u9762\u4e34\u6ce8\u610f\u529b\u673a\u5236\u548cKV\u7f13\u5b58\u968f\u8fd0\u884c\u65f6\u95f4\u589e\u957f\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u9ad8\u6602\u6216\u53ea\u80fd\u4f7f\u7528\u77ed\u89c6\u7684\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u4ee4\u724c\u4e22\u5f03\u548c\u538b\u7f29\u957f\u671f\u8bb0\u5fc6\uff1aDTD\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5728\u7ebf\u4fee\u526a\u6bcf\u4e2a\u8865\u4e01\u7684\u4ee4\u724c\uff0c\u5e78\u5b58\u4ee4\u724c\u6253\u5305\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u5757\uff1b\u4f7f\u7528\u5c0f\u578b\u5faa\u73af\u7f16\u7801\u5668\u6784\u5efa\u68c0\u7d22\u7d22\u5f15\uff0c\u5728\u63a8\u7406\u65f6\u68c0\u7d22\u6700\u76f8\u5173\u7684Top-K\u5757\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5728\u79bb\u7ebf\u548c\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCacheFlow\u4f18\u4e8e\u5f53\u524d\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u5904\u7406\u4ee4\u724c\u6570\u91cf\u51cf\u5c11\u9ad8\u8fbe87%\u3002", "conclusion": "CacheFlow\u4f7f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65e2\u9ad8\u6548\u53c8\u5177\u5907\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u5b9e\u7528\u7684\u957f\u89c6\u9891\u7406\u89e3\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2511.13647", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13647", "abs": "https://arxiv.org/abs/2511.13647", "authors": ["Chunshi Wang", "Junliang Ye", "Yunhan Yang", "Yang Li", "Zizhuo Lin", "Jun Zhu", "Zhuo Chen", "Yawei Luo", "Chunchao Guo"], "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model", "comment": null, "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/", "AI": {"tldr": "Part-X-MLLM\u662f\u4e00\u4e2a\u539f\u751f3D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u591a\u68373D\u4efb\u52a1\u7edf\u4e00\u4e3a\u7ed3\u6784\u5316\u53ef\u6267\u884c\u8bed\u6cd5\u4e2d\u7684\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u7edf\u4e00\u76843D\u7406\u89e3\u548c\u751f\u6210\u63a5\u53e3\u3002", "motivation": "\u73b0\u6709\u76843D\u591a\u6a21\u6001\u6a21\u578b\u5f80\u5f80\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u63a5\u53e3\u6765\u5904\u7406\u591a\u6837\u5316\u76843D\u4efb\u52a1\uff0c\u5982\u90e8\u4ef6\u7ea7\u68c0\u6d4b\u3001\u8bed\u4e49\u63cf\u8ff0\u548c\u7f16\u8f91\u547d\u4ee4\u3002", "method": "\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5c06\u7ed3\u6784\u4fe1\u606f\u4e0e\u8bed\u4e49\u4fe1\u606f\u89e3\u8026\uff0c\u5728RGB\u70b9\u4e91\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e0b\u81ea\u56de\u5f52\u751f\u6210\u5305\u542b\u90e8\u4ef6\u7ea7\u8fb9\u754c\u6846\u3001\u8bed\u4e49\u63cf\u8ff0\u548c\u7f16\u8f91\u547d\u4ee4\u7684\u7ed3\u6784\u5316\u4ee4\u724c\u5e8f\u5217\u3002", "result": "\u6a21\u578b\u5728\u7ed3\u6784\u5316\u89c4\u5212\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5728\u57fa\u4e8e\u90e8\u4ef6\u7684\u95ee\u7b54\u3001\u7ec4\u5408\u751f\u6210\u548c\u5c40\u90e8\u5316\u7f16\u8f91\u7b49\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Part-X-MLLM\u901a\u8fc7\u5c06\u7b26\u53f7\u89c4\u5212\u4e0e\u51e0\u4f55\u5408\u6210\u89e3\u8026\uff0c\u4e3a\u4efb\u4f55\u517c\u5bb9\u7684\u51e0\u4f55\u5f15\u64ce\u63d0\u4f9b\u4e86\u5355\u4e00\u7684\u8bed\u8a00\u539f\u751f\u524d\u7aef\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00\u76843D\u591a\u6a21\u6001\u63a5\u53e3\u3002"}}
{"id": "2511.13648", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.13648", "abs": "https://arxiv.org/abs/2511.13648", "authors": ["Ziang Cao", "Fangzhou Hong", "Zhaoxi Chen", "Liang Pan", "Ziwei Liu"], "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image", "comment": "Project page: https://physx-anything.github.io/", "summary": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.", "AI": {"tldr": "PhysX-Anything\u662f\u9996\u4e2a\u9762\u5411\u4eff\u771f\u7684\u7269\u74063D\u751f\u6210\u6846\u67b6\uff0c\u80fd\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5177\u6709\u660e\u786e\u51e0\u4f55\u7ed3\u6784\u3001\u5173\u8282\u8fde\u63a5\u548c\u7269\u7406\u5c5e\u6027\u7684\u9ad8\u8d28\u91cf\u4eff\u771f\u5c31\u7eea3D\u8d44\u4ea7\u3002", "motivation": "\u73b0\u67093D\u751f\u6210\u65b9\u6cd5\u5927\u591a\u5ffd\u7565\u4e86\u5173\u952e\u7684\u7269\u7406\u548c\u5173\u8282\u5c5e\u6027\uff0c\u9650\u5236\u4e86\u5728\u5177\u8eabAI\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u5c063D\u5efa\u6a21\u4ece\u9759\u6001\u89c6\u89c9\u8868\u793a\u8f6c\u5411\u53ef\u76f4\u63a5\u7528\u4e8e\u4eff\u771f\u548c\u4ea4\u4e92\u7684\u7269\u7406\u5316\u3001\u53ef\u5173\u8282\u5316\u7684\u8d44\u4ea7\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8eVLM\u7684\u7269\u74063D\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u76843D\u8868\u793a\u65b9\u6cd5\u5c06\u51e0\u4f55\u7ed3\u6784\u9ad8\u6548\u5206\u8bcd\uff0ctoken\u6570\u91cf\u51cf\u5c11193\u500d\uff0c\u65e0\u9700\u5728\u5fae\u8c03\u65f6\u5f15\u5165\u7279\u6b8atoken\u3002\u6784\u5efa\u4e86PhysX-Mobility\u6570\u636e\u96c6\uff0c\u5305\u542b2000\u591a\u4e2a\u5e38\u89c1\u771f\u5b9e\u4e16\u754c\u7269\u4f53\uff0c\u7269\u7406\u6807\u6ce8\u4e30\u5bcc\u3002", "result": "\u5728PhysX-Mobility\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPhysX-Anything\u5177\u6709\u5f3a\u5927\u7684\u751f\u6210\u6027\u80fd\u548c\u7a33\u5065\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728MuJoCo\u98ce\u683c\u73af\u5883\u4e2d\u7684\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u751f\u6210\u7684\u4eff\u771f\u5c31\u7eea\u8d44\u4ea7\u53ef\u76f4\u63a5\u7528\u4e8e\u63a5\u89e6\u5bc6\u96c6\u7684\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "PhysX-Anything\u80fd\u591f\u663e\u8457\u8d4b\u80fd\u4e0b\u6e38\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u5177\u8eabAI\u548c\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u9886\u57df\uff0c\u4e3a3D\u751f\u6210\u5411\u7269\u7406\u5316\u3001\u53ef\u4ea4\u4e92\u5316\u65b9\u5411\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u63a8\u52a8\u3002"}}
{"id": "2511.13649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13649", "abs": "https://arxiv.org/abs/2511.13649", "authors": ["Dengyang Jiang", "Dongyang Liu", "Zanyi Wang", "Qilong Wu", "Xin Jin", "David Liu", "Zhen Li", "Mengmeng Wang", "Peng Gao", "Harry Yang"], "title": "Distribution Matching Distillation Meets Reinforcement Learning", "comment": "The synergy of reinforcement learning and distribution matching distillation. See more: https://github.com/vvvvvjdy/dmdr", "summary": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.", "AI": {"tldr": "DMDR\u662f\u4e00\u4e2a\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u84b8\u998f\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u591a\u6b65\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u5c11\u6b65\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u751a\u81f3\u80fd\u8d85\u8d8a\u539f\u59cb\u6559\u5e08\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7684\u5206\u5e03\u5339\u914d\u84b8\u998f\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u4f46\u5c11\u6b65\u6a21\u578b\u7684\u6027\u80fd\u5f80\u5f80\u53d7\u9650\u4e8e\u591a\u6b65\u6559\u5e08\u6a21\u578b\u3002\u4e3a\u4e86\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u6765\u91ca\u653e\u5c11\u6b65\u751f\u6210\u5668\u7684\u6f5c\u529b\u3002", "method": "\u5c06\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u878d\u5165\u84b8\u998f\u8fc7\u7a0b\uff0c\u4f7f\u7528DMD\u635f\u5931\u4f5c\u4e3a\u66f4\u6709\u6548\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff1b\u8bbe\u8ba1\u52a8\u6001\u5206\u5e03\u5f15\u5bfc\u548c\u52a8\u6001\u91cd\u566a\u58f0\u91c7\u6837\u8bad\u7ec3\u7b56\u7565\u6765\u6539\u8fdb\u521d\u59cb\u84b8\u998f\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDMDR\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u63d0\u793a\u4e00\u81f4\u6027\u65b9\u9762\u5728\u5c11\u6b65\u65b9\u6cd5\u4e2d\u8868\u73b0\u9886\u5148\uff0c\u751a\u81f3\u5c55\u73b0\u51fa\u8d85\u8d8a\u591a\u6b65\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "DMDR\u901a\u8fc7\u540c\u65f6\u8fdb\u884c\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6210\u529f\u91ca\u653e\u4e86\u5c11\u6b65\u751f\u6210\u5668\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7a81\u7834\uff0c\u4e3a\u9ad8\u6548\u6269\u6563\u6a21\u578b\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.13655", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13655", "abs": "https://arxiv.org/abs/2511.13655", "authors": ["Henry Herzog", "Favyen Bastani", "Yawen Zhang", "Gabriel Tseng", "Joseph Redmon", "Hadrien Sablon", "Ryan Park", "Jacob Morrison", "Alexandra Buraczynski", "Karen Farley", "Joshua Hansen", "Andrew Howe", "Patrick Alan Johnson", "Mark Otterlee", "Ted Schmitt", "Hunter Pitelka", "Stephen Daspit", "Rachel Ratner", "Christopher Wilhelm", "Sebastian Wood", "Mike Jacobi", "Hannah Kerner", "Evan Shelhamer", "Ali Farhadi", "Ranjay Krishna", "Patrick Beukema"], "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation", "comment": null, "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.", "AI": {"tldr": "OlmoEarth\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u95e8\u4e3a\u5730\u7403\u89c2\u6d4b\u6570\u636e\u8bbe\u8ba1\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u6570\u636e\u5177\u6709\u7a7a\u95f4\u6027\uff08\u5982\u56fe\u50cf\uff09\u3001\u5e8f\u5217\u6027\uff08\u5982\u89c6\u9891\u6216\u6587\u672c\uff09\u548c\u591a\u6a21\u6001\u7279\u6027\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6a21\u578b\u6765\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7279\u5f81\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u3001\u63a9\u7801\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u4e13\u95e8\u9488\u5bf9\u5730\u7403\u89c2\u6d4b\u9886\u57df\u8bbe\u8ba1\u3002", "result": "\u572824\u4e2a\u4efb\u52a1\u4e2d\u768415\u4e2a\u4efb\u52a1\u4e0a\u83b7\u5f97\u6700\u4f73\u5d4c\u5165\u6027\u80fd\uff0c\u572829\u4e2a\u4efb\u52a1\u4e2d\u768419\u4e2a\u4efb\u52a1\u4e0a\u83b7\u5f97\u6700\u4f73\u5fae\u8c03\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed612\u4e2a\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "OlmoEarth\u4f5c\u4e3a\u7aef\u5230\u7aef\u5e73\u53f0\u7684\u6838\u5fc3\uff0c\u4e3a\u975e\u8425\u5229\u7ec4\u7ec7\u548cNGO\u63d0\u4f9b\u524d\u6cbf\u57fa\u7840\u6a21\u578b\u548c\u5f3a\u5927\u6570\u636e\u7ba1\u7406\u5de5\u5177\uff0c\u7528\u4e8e\u89e3\u51b3\u5168\u7403\u91cd\u5927\u95ee\u9898\u3002"}}
{"id": "2511.13684", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13684", "abs": "https://arxiv.org/abs/2511.13684", "authors": ["Jiangnan Ye", "Jiedong Zhuang", "Lianrui Mu", "Wenjie Zheng", "Jiaqi Hu", "Xingze Zou", "Jing Wang", "Haoji Hu"], "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting", "comment": "Submitting for Neurocomputing", "summary": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.", "AI": {"tldr": "GS-Light\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7684\u9ad8\u6548\u6587\u672c\u5f15\u5bfc3D\u573a\u666f\u91cd\u5149\u7167\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u514d\u8d39\u7684\u6269\u6563\u6a21\u578b\u6269\u5c55\u5904\u7406\u591a\u89c6\u56fe\u8f93\u5165\uff0c\u5229\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u5149\u7167\u5148\u9a8c\uff0c\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u751f\u6210\u9ad8\u8d28\u91cf\u91cd\u5149\u7167\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u76843D\u573a\u666f\u91cd\u5149\u7167\u65b9\u6cd5\u5728\u5904\u7406\u7528\u6237\u6307\u5b9a\u7684\u5149\u7167\u65b9\u5411\u3001\u989c\u8272\u3001\u5f3a\u5ea6\u7b49\u590d\u6742\u6587\u672c\u6307\u4ee4\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u7406\u89e3\u7528\u6237\u610f\u56fe\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u91cd\u5149\u7167\u7ed3\u679c\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LVLM\u89e3\u6790\u6587\u672c\u63d0\u793a\u4e3a\u5149\u7167\u5148\u9a8c\uff0c\u7ed3\u5408\u6df1\u5ea6\u3001\u6cd5\u7ebf\u548c\u8bed\u4e49\u5206\u5272\u7b49\u51e0\u4f55\u7ea6\u675f\u8ba1\u7b97\u5149\u7167\u56fe\uff0c\u751f\u6210\u521d\u59cb\u6f5c\u7801\u6307\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u6e32\u67d3\u56fe\u50cf\u548c\u521d\u59cb\u6f5c\u7801\u8f93\u5165\u591a\u89c6\u56fe\u91cd\u5149\u7167\u6a21\u578b\uff0c\u6700\u540e\u5fae\u8c033DGS\u573a\u666f\u3002", "result": "\u5728\u5ba4\u5185\u5916\u573a\u666f\u8bc4\u4f30\u4e2d\uff0cGS-Light\u5728\u5b9a\u91cf\u6307\u6807\uff08\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u3001\u56fe\u50cf\u8d28\u91cf\u3001\u7f8e\u5b66\u8bc4\u5206\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7b49\uff09\u548c\u5b9a\u6027\u8bc4\u4f30\uff08\u7528\u6237\u7814\u7a76\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GS-Light\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6587\u672c\u5f15\u5bfc3D\u573a\u666f\u91cd\u5149\u7167\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u51c6\u786e\u53cd\u6620\u7528\u6237\u671f\u671b\u7684\u5149\u7167\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u5149\u7167\u65b9\u5411\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.13704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13704", "abs": "https://arxiv.org/abs/2511.13704", "authors": ["Harold Haodong Chen", "Disen Lan", "Wen-Jie Shu", "Qingyang Liu", "Zihan Wang", "Sirui Chen", "Wenkai Cheng", "Kanghao Chen", "Hongfei Zhang", "Zixin Zhang", "Rongjin Guo", "Yu Cheng", "Ying-Cong Chen"], "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models", "comment": "Project: https://haroldchen19.github.io/TiViBench-Page/", "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.", "AI": {"tldr": "\u63d0\u51fa\u4e86TiViBench\u57fa\u51c6\u6765\u8bc4\u4f30\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6db5\u76d64\u4e2a\u7ef4\u5ea6\u768424\u4e2a\u4efb\u52a1\u573a\u666f\uff0c\u5e76\u5f00\u53d1\u4e86VideoTPO\u6d4b\u8bd5\u65f6\u4f18\u5316\u7b56\u7565\u6765\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u7f3a\u4e4f\u5bf9\u9ad8\u9636\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u8981\u4e13\u95e8\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u5206\u5c42\u57fa\u51c6TiViBench\uff0c\u7cfb\u7edf\u8bc4\u4f30\u7ed3\u6784\u63a8\u7406\u3001\u7a7a\u95f4\u89c6\u89c9\u6a21\u5f0f\u63a8\u7406\u3001\u7b26\u53f7\u903b\u8f91\u63a8\u7406\u3001\u52a8\u4f5c\u89c4\u5212\u56db\u4e2a\u7ef4\u5ea6\u7684\u63a8\u7406\u80fd\u529b\uff1b\u63d0\u51faVideoTPO\u6d4b\u8bd5\u65f6\u7b56\u7565\uff0c\u901a\u8fc7LLM\u81ea\u5206\u6790\u751f\u6210\u5019\u9009\u6765\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002", "result": "\u5546\u4e1a\u6a21\u578b\uff08\u5982Sora 2\u3001Veo 3.1\uff09\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u63a8\u7406\u6f5c\u529b\uff0c\u5f00\u6e90\u6a21\u578b\u56e0\u8bad\u7ec3\u89c4\u6a21\u548c\u6570\u636e\u591a\u6837\u6027\u9650\u5236\u800c\u6f5c\u529b\u672a\u5145\u5206\u5f00\u53d1\uff1bVideoTPO\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "TiViBench\u548cVideoTPO\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdb\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u8be5\u65b0\u5174\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u8bbe\u5b9a\u4e86\u6846\u67b6\u3002"}}
{"id": "2511.13713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13713", "abs": "https://arxiv.org/abs/2511.13713", "authors": ["Xincheng Shuai", "Zhenyuan Qin", "Henghui Ding", "Dacheng Tao"], "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine", "comment": "AAAI 2026, Project Page: https://henghuiding.com/FFSE/", "summary": "Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.", "AI": {"tldr": "FFSE\u662f\u4e00\u4e2a3D\u611f\u77e5\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u5b9e\u73b0\u76f4\u89c2\u3001\u7269\u7406\u4e00\u81f4\u7684\u5bf9\u8c61\u7f16\u8f91\uff0c\u901a\u8fc7\u5efa\u6a21\u4e3a3D\u53d8\u6362\u5e8f\u5217\u6765\u652f\u6301\u5e73\u79fb\u3001\u7f29\u653e\u3001\u65cb\u8f6c\u7b49\u64cd\u4f5c\uff0c\u540c\u65f6\u4fdd\u6301\u80cc\u666f\u6548\u679c\u548c\u573a\u666f\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b03D\u611f\u77e5\u7684\u5bf9\u8c61\u64cd\u4f5c\uff0c\u8981\u4e48\u5728\u56fe\u50cf\u7a7a\u95f4\u64cd\u4f5c\uff0c\u8981\u4e48\u9700\u8981\u7f13\u6162\u4e14\u5bb9\u6613\u51fa\u9519\u76843D\u91cd\u5efa\u3002", "method": "\u63d0\u51faFFSE\u6846\u67b6\uff0c\u5c06\u7f16\u8f91\u5efa\u6a21\u4e3a\u5b66\u4e60\u76843D\u53d8\u6362\u5e8f\u5217\uff0c\u5e76\u5f15\u51653DObjectEditor\u6df7\u5408\u6570\u636e\u96c6\uff0c\u4ece\u6a21\u62df\u7f16\u8f91\u5e8f\u5217\u4e2d\u6784\u5efa\uff0c\u652f\u6301\u591a\u8f6e\u548c\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u8bad\u7ec3\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFFSE\u5728\u5355\u8f6e\u548c\u591a\u8f6e3D\u611f\u77e5\u7f16\u8f91\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FFSE\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u76f4\u89c2\u3001\u7269\u7406\u4e00\u81f4\u76843D\u611f\u77e5\u5bf9\u8c61\u7f16\u8f91\uff0c\u5728\u4fdd\u6301\u80cc\u666f\u6548\u679c\u548c\u573a\u666f\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u652f\u6301\u591a\u8f6e\u7f16\u8f91\u64cd\u4f5c\u3002"}}
{"id": "2511.13715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13715", "abs": "https://arxiv.org/abs/2511.13715", "authors": ["Hengrui Hu", "Kaining Ying", "Henghui Ding"], "title": "Segment Anything Across Shots: A Method and Benchmark", "comment": "AAAI 2026, Project Page: https://henghuiding.com/SAAS/", "summary": "This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SAAS\u6a21\u578b\u548cTMA\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u955c\u5934\u534a\u76d1\u7763\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u62df\u955c\u5934\u8f6c\u6362\u6765\u63d0\u5347\u8de8\u955c\u5934\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u57fa\u51c6Cut-VOS\u4e0a\u53d6\u5f97SOTA\u6548\u679c\u3002", "motivation": "\u73b0\u6709VOS\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u955c\u5934\u89c6\u9891\uff0c\u96be\u4ee5\u5904\u7406\u955c\u5934\u95f4\u7684\u4e0d\u8fde\u7eed\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u89e3\u51b3\u591a\u955c\u5934\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u95ee\u9898\u3002", "method": "\u63d0\u51faTMA\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4f7f\u7528\u5355\u955c\u5934\u6570\u636e\u5b9e\u73b0\u8de8\u955c\u5934\u6cdb\u5316\uff1b\u5f00\u53d1SAAS\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u7406\u89e3\u955c\u5934\u8f6c\u6362\uff1b\u5efa\u7acbCut-VOS\u591a\u955c\u5934\u5206\u5272\u57fa\u51c6\u3002", "result": "\u5728YouMVOS\u548cCut-VOS\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAAS\u6a21\u578b\u901a\u8fc7\u6709\u6548\u6a21\u62df\u3001\u7406\u89e3\u548c\u5206\u5272\u590d\u6742\u8f6c\u6362\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SAAS\u6a21\u578b\u548cTMA\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u591a\u955c\u5934\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u95ee\u9898\uff0c\u4e3aMVOS\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u57fa\u51c6\u3002"}}
