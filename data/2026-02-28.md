<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 15]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.LG](#cs.LG) [Total: 55]
- [cs.AI](#cs.AI) [Total: 32]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [TWICE: An LLM Agent Framework for Simulating Personalized User Tweeting Behavior with Long-term Temporal Features](https://arxiv.org/abs/2602.22222)
*Bingrui Jin,Kunyao Lan,Mengyue Wu*

Main category: cs.IR

TL;DR: TWICE是一个基于LLM的框架，利用社交媒体数据的长期时间和个性化特征来模拟个性化用户推文行为，有效捕捉长期时间特性。


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟方法主要关注集体行为或交互系统，难以处理需要建模时间特性的任务，特别是在社交媒体中捕捉个性化用户的长期时间特征。

Method: 提出TWICE框架，整合个性化用户画像、事件驱动记忆模块和个性化风格重写工作流，模拟个性化用户推文行为并捕捉长期时间特征。

Result: 实验结果表明，该框架通过有效整合时间动态改进了个性化用户模拟，为长期行为跟踪提供了稳健解决方案。

Conclusion: TWICE框架能够有效模拟个性化用户行为并捕捉长期时间特征，解决了现有方法在建模时间特性方面的局限性。

Abstract: User simulators are often used to generate large amounts of data for various tasks such as generation, training, and evaluation. However, existing approaches concentrate on collective behaviors or interactive systems, struggling with tasks that require modeling temporal characteristics. To address this limitation, we propose TWICE, an LLM-based framework that leverages the long-term temporal and personalized features of social media data. This framework integrates personalized user profiling, an event-driven memory module, and a workflow for personalized style rewriting, enabling simulation of personalized user tweeting behavior while capturing long-term temporal characteristics. In addition, we conduct a comprehensive evaluation with a focus on analyzing tweeting style and event-based changes in behavior. Experiment results demonstrate that our framework improves personalized user simulation by effectively incorporating temporal dynamics, providing a robust solution for long-term behavior tracking.

</details>


### [2] [SEGB: Self-Evolved Generative Bidding with Local Autoregressive Diffusion](https://arxiv.org/abs/2602.22226)
*Yulong Gao,Wan Jiang,Mingzhe Cao,Xuepu Wang,Zeyu Pan,Haonan Yang,Ye Liu,Xin Yang*

Main category: cs.IR

TL;DR: SEGB是一个自我进化的生成式竞价框架，通过合成短期未来状态进行前瞻性规划，并通过价值引导的策略精炼在离线状态下自我改进，显著提升了在线广告自动竞价的性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线训练的生成式竞价策略缺乏对动态市场的短期预见能力，通常依赖模拟器或外部专家进行后训练改进，存在关键局限性。

Method: 提出SEGB框架：1）合成合理的短期未来状态来指导每次出价，提供动态预见能力；2）执行价值引导的策略精炼，无需外部干预即可迭代发现更优策略。

Result: 在AuctionNet基准测试和大规模A/B测试中，SEGB显著优于最先进的基线方法。在大规模在线部署中，实现了目标成本+10.19%的增长，创造了实质性商业价值。

Conclusion: SEGB通过先进的规划和自我进化范式，仅从静态数据就能实现稳健的策略改进，为在线广告自动竞价提供了有效的解决方案。

Abstract: In the realm of online advertising, automated bidding has become a pivotal tool, enabling advertisers to efficiently capture impression opportunities in real-time. Recently, generative auto-bidding has shown significant promise, offering innovative solutions for effective ad optimization. However, existing offline-trained generative policies lack the near-term foresight required for dynamic markets and usually depend on simulators or external experts for post-training improvement. To overcome these critical limitations, we propose Self-Evolved Generative Bidding (SEGB), a framework that plans proactively and refines itself entirely offline. SEGB first synthesizes plausible short-horizon future states to guide each bid, providing the agent with crucial, dynamic foresight. Crucially, it then performs value-guided policy refinement to iteratively discover superior strategies without any external intervention. This self-contained approach uniquely enables robust policy improvement from static data alone. Experiments on the AuctionNet benchmark and a large-scale A/B test validate our approach, demonstrating that SEGB significantly outperforms state-of-the-art baselines. In a large-scale online deployment, it delivered substantial business value, achieving a +10.19% increase in target cost, proving the effectiveness of our advanced planning and evolution paradigm.

</details>


### [3] [RETLLM: Training and Data-Free MLLMs for Multimodal Information Retrieval](https://arxiv.org/abs/2602.22278)
*Dawei Su,Dongsheng Wang*

Main category: cs.IR

TL;DR: RetLLM是一个无需训练和数据的多模态信息检索框架，通过直接提示MLLMs预测检索分数，采用粗筛-精评的两阶段流程，在多个基准测试中超越了微调模型。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态信息检索方法虽然利用MLLMs提升了性能，但存在预训练不一致性和需要大量数据集的问题。作者希望开发一个无需训练和数据的框架，直接利用MLLMs的内在多模态推理能力进行检索。

Method: 提出RetLLM框架：1) 将MMIR定义为相似度分数生成任务；2) 采用粗筛-精评两阶段流程：粗筛阶段使用top-k过滤策略构建高质量候选池，精评阶段将查询和候选同时输入MLLMs预测检索分数；3) 引入视觉增强模块，在推理过程中帮助MLLMs重新拾取被遗忘的视觉信息。

Result: 在多个多模态信息检索基准测试中，RetLLM超越了经过微调的模型，证明了无需训练的MLLMs也能实现强大的MMIR性能。消融研究验证了每个组件的有效性。

Conclusion: MLLMs无需任何训练就能在多模态信息检索中表现出色，展示了其固有的多模态推理能力。RetLLM提供了一个简单、可扩展的框架，为MMIR研究提供了新的方向。

Abstract: Multimodal information retrieval (MMIR) has gained attention for its flexibility in handling text, images, or mixed queries and candidates. Recent breakthroughs in multimodal large language models (MLLMs) boost MMIR performance by incorporating MLLM knowledge under the contrastive finetuning framework. However, they suffer from pre-training inconsistency and require large datasets. In this work, we introduce a novel framework, RetLLM, designed to query MLLMs for MMIR in a training- and data-free manner. Specifically, we formulate MMIR as a similarity score generation task and prompt MLLMs to directly predict retrieval scores in a coarse-then-fine pipeline. At the coarse stage, a top-k filtering strategy builds a small yet high-quality candidate pool for each query, enabling MLLMs to focus on semantically relevant candidates. Subsequently, the retrieval score is predicted by feeding both the query and candidate into MLLMs at the fine stage. Importantly, we propose a visual enhancement module during reasoning to help MLLMs re-pick forgotten visuals, improving retrieval. Extensive experiments on MMIR benchmarks show that RetLLM outperforms fine-tuned models. Ablation studies further verify each component. Our work demonstrates that MLLMs can achieve strong MMIR performance without any training, highlighting their inherent multimodal reasoning ability in a simple, scalable framework. We release our code at: https://github.com/alivecat05/RETLLM

</details>


### [4] [TFPS: A Temporal Filtration-enhanced Positive Sample Set Construction Method for Implicit Collaborative Filtering](https://arxiv.org/abs/2602.22521)
*Jiayi Wu,Zhengyu Wu,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.IR

TL;DR: 提出TFPS方法，通过时间过滤增强构建高质量正样本集，提升隐式反馈推荐效果


<details>
  <summary>Details</summary>
Motivation: 现有负采样策略主要优化负样本而忽视正样本探索，且现有去噪方法忽略时间信息，现有序列方法忽视时间间隔信息，难以准确捕捉用户当前偏好

Method: 1) 基于交互时间间隔设计时间衰减模型，将原始图转换为加权用户-物品二部图；2) 基于预定义过滤操作对加权图进行分层；3) 设计层增强策略为分层子图构建高质量正样本集

Result: 在三个真实数据集上实验证明TFPS能有效提升Recall@k和NDCG@k指标，并提供理论分析解释其有效性

Conclusion: TFPS方法能从数据角度构建高质量正样本集，可与多种隐式CF推荐器或负采样方法结合提升性能

Abstract: The negative sampling strategy can effectively train collaborative filtering (CF) recommendation models based on implicit feedback by constructing positive and negative samples. However, existing methods primarily optimize the negative sampling process while neglecting the exploration of positive samples. Some denoising recommendation methods can be applied to denoise positive samples within negative sampling strategies, but they ignore temporal information. Existing work integrates sequential information during model aggregation but neglects time interval information, hindering accurate capture of users' current preferences. To address this problem, from a data perspective, we propose a novel temporal filtration-enhanced approach to construct a high-quality positive sample set. First, we design a time decay model based on interaction time intervals, transforming the original graph into a weighted user-item bipartite graph. Then, based on predefined filtering operations, the weighted user-item bipartite graph is layered. Finally, we design a layer-enhancement strategy to construct a high-quality positive sample set for the layered subgraphs. We provide theoretical insights into why TFPS can improve Recall@k and NDCG@k, and extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed method. Additionally, TFPS can be integrated with various implicit CF recommenders or negative sampling methods to enhance its performance.

</details>


### [5] [Generative Agents Navigating Digital Libraries](https://arxiv.org/abs/2602.22529)
*Saber Zerhoudi,Michael Granitzer*

Main category: cs.IR

TL;DR: Agent4DL是一个专门为数字图书馆设计的用户搜索行为模拟器，利用大语言模型生成逼真的用户档案和动态搜索会话，解决了数字图书馆研究中因隐私问题导致用户搜索数据稀缺的挑战。


<details>
  <summary>Details</summary>
Motivation: 数字图书馆研究面临一个长期挑战：由于隐私问题，公开可用的用户搜索模式数据集稀缺。大语言模型的发展为模拟用户行为提供了新的可能性，需要开发专门针对数字图书馆环境的用户搜索行为模拟器。

Method: 开发了Agent4DL用户搜索行为模拟器，能够生成逼真的用户档案和动态搜索会话。该模拟器模拟实际的搜索策略，包括针对特定用户档案定制的查询、点击和停止行为。

Result: 通过与真实用户数据的比较验证了Agent4DL在复制真实用户交互方面的准确性。与现有用户搜索模拟器（如SimIIR 2.0）相比，Agent4DL表现出竞争性性能，特别是在生成更多样化和上下文感知的用户行为方面。

Conclusion: Agent4DL是一个有效的数字图书馆用户搜索行为模拟器，能够生成逼真的用户交互数据，解决了数字图书馆研究中用户数据稀缺的问题，为相关研究提供了有价值的工具。

Abstract: In the rapidly evolving field of digital libraries, the development of large language models (LLMs) has opened up new possibilities for simulating user behavior. This innovation addresses the longstanding challenge in digital library research: the scarcity of publicly available datasets on user search patterns due to privacy concerns. In this context, we introduce Agent4DL, a user search behavior simulator specifically designed for digital library environments. Agent4DL generates realistic user profiles and dynamic search sessions that closely mimic actual search strategies, including querying, clicking, and stopping behaviors tailored to specific user profiles. Our simulator's accuracy in replicating real user interactions has been validated through comparisons with real user data. Notably, Agent4DL demonstrates competitive performance compared to existing user search simulators such as SimIIR 2.0, particularly in its ability to generate more diverse and context-aware user behaviors.

</details>


### [6] [Towards Dynamic Dense Retrieval with Routing Strategy](https://arxiv.org/abs/2602.22547)
*Zhan Su,Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Bingbing Wen,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 提出动态稠密检索（DDR）方法，通过前缀调优模块和动态路由策略，实现仅需2%训练参数的灵活领域适应，解决传统稠密检索模型在新领域适应困难和频繁更新成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统稠密检索（DR）方法存在两个主要问题：1）在训练数据有限时难以适应新领域；2）模型过时需要从头训练，在频繁更新场景下成本过高。需要一种更灵活、高效的领域适应方法。

Method: 提出动态稠密检索（DDR）方法，使用前缀调优（prefix tuning）作为特定领域的模块，这些模块可以通过动态路由策略组合使用，实现检索部分的灵活领域适应。

Result: 在6个零样本下游任务上的广泛评估表明，该方法在仅使用2%训练参数的情况下超越了传统DR方法，为信息检索中实现更灵活的稠密检索铺平了道路。

Conclusion: DDR是一种有前景的未来方向，能够将稠密检索灵活应用于各种任务，解决了传统方法的领域适应困难和更新成本问题。

Abstract: The \textit{de facto} paradigm for applying dense retrieval (DR) to new tasks involves fine-tuning a pre-trained model for a specific task. However, this paradigm has two significant limitations: (1) It is difficult adapt the DR to a new domain if the training dataset is limited.
  (2) Old DR models are simply replaced by newer models that are trained from scratch when the former are no longer up to date. Especially for scenarios where the model needs to be updated frequently, this paradigm is prohibitively expensive. To address these challenges, we propose a novel dense retrieval approach, termed \textit{dynamic dense retrieval} (DDR). DDR uses \textit{prefix tuning} as a \textit{module} specialized for a specific domain. These modules can then be compositional combined with a dynamic routing strategy, enabling highly flexible domain adaptation in the retrieval part. Extensive evaluation on six zero-shot downstream tasks demonstrates that this approach can surpass DR while utilizing only 2\% of the training parameters, paving the way to achieve more flexible dense retrieval in IR. We see it as a promising future direction for applying dense retrieval to various tasks.

</details>


### [7] [Where Relevance Emerges: A Layer-Wise Study of Internal Attention for Zero-Shot Re-Ranking](https://arxiv.org/abs/2602.22591)
*Haodong Chen,Shengyao Zhuang,Zheng Yao,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.IR

TL;DR: 本文提出Selective-ICR方法，通过选择性利用transformer层中的注意力信号，在保持效果的同时将推理延迟降低30%-50%，小模型性能超越传统生成式方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的文档重排方法主要依赖生成式评分或输出logits，存在推理延迟和结果一致性的瓶颈。虽然最近提出的ICR方法通过提取内部注意力信号避免了文本生成开销，但现有方法简单聚合所有层信号，未探索层间贡献差异及其在不同架构中的一致性，也缺乏对内部注意力与传统生成/似然机制的统一比较。

Method: 1. 对生成、似然和内部注意力机制进行正交评估；2. 发现transformer层中相关性信号的"钟形曲线"分布规律；3. 提出Selective-ICR策略，选择性利用关键层的注意力信号而非简单聚合所有层；4. 在BRIGHT推理密集型基准上进行评估。

Result: 1. Selective-ICR将推理延迟降低30%-50%且不损害效果；2. 零样本8B模型性能匹配14B强化学习重排器；3. 0.6B模型超越最先进的生成式方法；4. 高质量上下文注意力信号可减少模型缩放和强化学习需求。

Conclusion: 研究重新定义了基于LLM重排的效率-效果边界，揭示了内部信号在复杂推理排序任务中的潜在价值。Selective-ICR通过选择性利用注意力信号实现了显著的效率提升，为轻量级重排系统提供了新方向。

Abstract: Zero-shot document re-ranking with Large Language Models (LLMs) has evolved from Pointwise methods to Listwise and Setwise approaches that optimize computational efficiency. Despite their success, these methods predominantly rely on generative scoring or output logits, which face bottlenecks in inference latency and result consistency. In-Context Re-ranking (ICR) has recently been proposed as an $O(1)$ alternative method. ICR extracts internal attention signals directly, avoiding the overhead of text generation. However, existing ICR methods simply aggregate signals across all layers; layer-wise contributions and their consistency across architectures have been left unexplored. Furthermore, no unified study has compared internal attention with traditional generative and likelihood-based mechanisms across diverse ranking frameworks under consistent conditions.
  In this paper, we conduct an orthogonal evaluation of generation, likelihood, and internal attention mechanisms across multiple ranking frameworks. We further identify a universal "bell-curve" distribution of relevance signals across transformer layers, which motivates the proposed Selective-ICR strategy that reduces inference latency by 30%-50% without compromising effectiveness. Finally, evaluation on the reasoning-intensive BRIGHT benchmark shows that precisely capturing high-quality in-context attention signals fundamentally reduces the need for model scaling and reinforcement learning: a zero-shot 8B model matches the performance of 14B reinforcement-learned re-rankers, while even a 0.6B model outperforms state-of-the-art generation-based approaches. These findings redefine the efficiency-effectiveness frontier for LLM-based re-ranking and highlight the latent potential of internal signals for complex reasoning ranking tasks. Our code and results are publicly available at https://github.com/ielab/Selective-ICR.

</details>


### [8] [Fine-grained Semantics Integration for Large Language Model-based Recommendation](https://arxiv.org/abs/2602.22632)
*Jiawen Feng,Xiaoyu Kong,Leheng Sheng,Bin Wu,Chao Yi,Feifang Yang,Xiang-Rong Sheng,Han Zhu,Xiang Wang,Jiancan Wu,Xiangnan He*

Main category: cs.IR

TL;DR: TS-Rec提出了一种将细粒度语义信息集成到基于LLM的生成式推荐系统中的方法，通过语义感知的嵌入初始化和令牌级语义对齐来解决语义标识符空间建模的两个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的生成式推荐系统在语义标识符（SID）空间建模面临两个基本挑战：1）语义无意义的初始化 - SID令牌随机初始化，切断了SID空间与预训练语言空间之间的语义联系；2）粗粒度对齐 - 现有的基于SFT的对齐任务主要关注项目级优化，而忽略了SID序列中单个令牌的语义。

Method: TS-Rec包含两个关键组件：1）语义感知嵌入初始化（SA-Init）：通过应用教师模型提取的关键词的预训练嵌入的平均池化来初始化SID令牌嵌入；2）令牌级语义对齐（TS-Align）：将SID序列中的单个令牌与相应项目簇的共享语义对齐。

Result: 在两个真实世界基准测试上的广泛实验表明，TS-Rec在所有标准指标上始终优于传统和生成式基线方法。结果证明，集成细粒度语义信息显著提升了基于LLM的生成式推荐系统的性能。

Conclusion: TS-Rec通过解决语义标识符空间建模的两个关键挑战，成功地将细粒度语义信息集成到基于LLM的生成式推荐系统中，显著提升了推荐性能，为LLM在推荐系统中的应用提供了新的思路。

Abstract: Recent advances in Large Language Models (LLMs) have shifted in recommendation systems from the discriminative paradigm to the LLM-based generative paradigm, where the recommender autoregressively generates sequences of semantic identifiers (SIDs) for target items conditioned on historical interaction. While prevalent LLM-based recommenders have demonstrated performance gains by aligning pretrained LLMs between the language space and the SID space, modeling the SID space still faces two fundamental challenges: (1) Semantically Meaningless Initialization: SID tokens are randomly initialized, severing the semantic linkage between the SID space and the pretrained language space at start point, and (2) Coarse-grained Alignment: existing SFT-based alignment tasks primarily focus on item-level optimization, while overlooking the semantics of individual tokens within SID sequences.To address these challenges, we propose TS-Rec, which can integrate Token-level Semantics into LLM-based Recommenders. Specifically, TS-Rec comprises two key components: (1) Semantic-Aware embedding Initialization (SA-Init), which initializes SID token embeddings by applying mean pooling to the pretrained embeddings of keywords extracted by a teacher model; and (2) Token-level Semantic Alignment (TS-Align), which aligns individual tokens within the SID sequence with the shared semantics of the corresponding item clusters. Extensive experiments on two real-world benchmarks demonstrate that TS-Rec consistently outperforms traditional and generative baselines across all standard metrics. The results demonstrate that integrating fine-grained semantic information significantly enhances the performance of LLM-based generative recommenders.

</details>


### [9] [Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators](https://arxiv.org/abs/2602.22647)
*Zhengyang Su,Isay Katsman,Yueqi Wang,Ruining He,Lukasz Heldt,Raghunandan Keshavan,Shao-Chuan Wang,Xinyang Yi,Mingyan Gao,Onkar Dalal,Lichan Hong,Ed Chi,Ningren Han*

Main category: cs.IR

TL;DR: STATIC是一种针对TPU/GPU优化的高效约束解码技术，通过将前缀树扁平化为稀疏矩阵，实现硬件加速的生成式检索，在工业推荐系统中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统需要将输出空间限制在特定项目子集（如内容新鲜度或产品类别），但标准自回归解码无法原生支持这种约束，而现有的前缀树约束解码方法在硬件加速器上存在严重的延迟问题。

Method: 将前缀树扁平化为静态压缩稀疏行(CSR)矩阵，将不规则的树遍历转换为完全向量化的稀疏矩阵操作，从而在TPU/GPU上实现大规模效率提升。

Result: 在大型工业视频推荐平台部署STATIC，产生显著的产品指标影响，延迟开销极低（每步0.033ms，占推理时间的0.25%），比CPU前缀树实现快948倍，比硬件加速的二分搜索基线快47-1033倍。在学术基准测试中也显著改善了生成式检索的冷启动性能。

Conclusion: STATIC实现了首个生产规模的严格约束生成式检索部署，通过稀疏矩阵转换实现了硬件加速器上的高效约束解码，为工业推荐系统提供了实用的解决方案。

Abstract: Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.

</details>


### [10] [Generative Recommendation for Large-Scale Advertising](https://arxiv.org/abs/2602.22732)
*Ben Xue,Dan Liu,Lixiang Wang,Mingjie Sun,Peng Wang,Pengfei Zhang,Shaoyun Shi,Tianyu Xu,Yunhao Sha,Zhiqiang Liu,Bo Kong,Bo Wang,Hang Yang,Jieting Xue,Junhao Wang,Shengyu Wang,Shuping Hui,Wencai Ye,Xiao Lin,Yongzhi Li,Yuhang Chen,Zhihui Yin,Quan Chen,Shiyang Wen,Wenjin Wu,Han Li,Guorui Zhou,Changcheng Li,Peng Jiang*

Main category: cs.IR

TL;DR: GR4AD是一个面向广告生成式推荐的生产系统，通过统一广告语义ID、惰性自回归解码器、价值感知学习和排名引导优化等技术，在快手广告系统中实现了4.2%的收入提升。


<details>
  <summary>Details</summary>
Motivation: 在大规模广告系统中部署实时生成式推荐面临挑战，需要超越传统大语言模型训练和服务方案的设计，以在固定服务预算下实现规模化，并优化业务价值对齐。

Method: 1) 提出UA-SID统一广告语义ID捕获复杂业务信息；2) 设计LazyAR惰性自回归解码器，放松层间依赖以降低推理成本；3) 采用VSL价值感知监督学习和RSPO排名引导软最大偏好优化算法；4) 在线推理使用动态束搜索服务，根据生成层级和在线负载自适应调整束宽度。

Result: 大规模在线A/B测试显示，相比现有DLRM基线，广告收入提升最高达4.2%，在模型缩放和推理时缩放方面均获得一致增益。系统已在快手广告系统全面部署，服务超过4亿用户，实现高吞吐实时服务。

Conclusion: GR4AD通过架构、学习和服务的协同设计，成功实现了生产级生成式推荐系统，在保持效果的同时降低了推理成本，为大规模广告场景提供了可行的生成式推荐解决方案。

Abstract: Generative recommendation has recently attracted widespread attention in industry due to its potential for scaling and stronger model capacity. However, deploying real-time generative recommendation in large-scale advertising requires designs beyond large-language-model (LLM)-style training and serving recipes. We present a production-oriented generative recommender co-designed across architecture, learning, and serving, named GR4AD (Generative Recommendation for ADdvertising). As for tokenization, GR4AD proposes UA-SID (Unified Advertisement Semantic ID) to capture complicated business information. Furthermore, GR4AD introduces LazyAR, a lazy autoregressive decoder that relaxes layer-wise dependencies for short, multi-candidate generation, preserving effectiveness while reducing inference cost, which facilitates scaling under fixed serving budgets. To align optimization with business value, GR4AD employs VSL (Value-Aware Supervised Learning) and proposes RSPO (Ranking-Guided Softmax Preference Optimization), a ranking-aware, list-wise reinforcement learning algorithm that optimizes value-based rewards under list-level metrics for continual online updates. For online inference, we further propose dynamic beam serving, which adapts beam width across generation levels and online load to control compute. Large-scale online A/B tests show up to 4.2% ad revenue improvement over an existing DLRM-based stack, with consistent gains from both model scaling and inference-time scaling. GR4AD has been fully deployed in Kuaishou advertising system with over 400 million users and achieves high-throughput real-time serving.

</details>


### [11] [Sequential Regression for Continuous Value Prediction using Residual Quantization](https://arxiv.org/abs/2602.23012)
*Runpeng Cui,Zhipeng Sun,Chi Lu,Peng Jiang*

Main category: cs.IR

TL;DR: 提出基于残差量化的序列学习框架，用于推荐系统中的连续值预测任务，通过从粗到细的递归量化编码预测来建模复杂数据分布。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的连续值预测（如观看时长、GMV等）面临数据分布复杂且长尾的挑战。现有生成方法依赖刚性参数分布假设，当假设与现实数据不匹配时性能受限：简化形式无法充分建模现实复杂性，而复杂假设则存在可扩展性和泛化性问题。

Method: 提出残差量化（RQ）序列学习框架：将目标连续值表示为有序量化编码之和，从粗到细粒度递归预测，量化误差逐渐减小。引入表示学习目标，将RQ编码嵌入空间与目标值的序数结构对齐，使模型能够捕获量化编码的连续表示，进一步提高预测准确性。

Result: 在LTV和观看时长预测的公共基准测试以及工业短视频推荐平台的大规模在线GMV预测实验中进行了广泛评估。结果显示该方法始终优于最先进方法，并在推荐系统多样连续值预测任务中表现出强大的泛化能力。

Conclusion: 提出的残差量化序列学习框架有效解决了推荐系统中连续值预测的挑战，通过从粗到细的递归量化编码预测和表示学习对齐，在多个任务上取得了优于现有方法的性能，并展现出良好的泛化能力。

Abstract: Continuous value prediction plays a crucial role in industrial-scale recommendation systems, including tasks such as predicting users' watch-time and estimating the gross merchandise value (GMV) in e-commerce transactions. However, it remains challenging due to the highly complex and long-tailed nature of the data distributions. Existing generative approaches rely on rigid parametric distribution assumptions, which fundamentally limits their performance when such assumptions misalign with real-world data. Overly simplified forms cannot adequately model real-world complexities, while more intricate assumptions often suffer from poor scalability and generalization.
  To address these challenges, we propose a residual quantization (RQ)-based sequence learning framework that represents target continuous values as a sum of ordered quantization codes, predicted recursively from coarse to fine granularity with diminishing quantization errors. We introduce a representation learning objective that aligns RQ code embedding space with the ordinal structure of target values, allowing the model to capture continuous representations for quantization codes and further improving prediction accuracy. We perform extensive evaluations on public benchmarks for lifetime value (LTV) and watch-time prediction, alongside a large-scale online experiment for GMV prediction on an industrial short-video recommendation platform. The results consistently show that our approach outperforms state-of-the-art methods, while demonstrating strong generalization across diverse continuous value prediction tasks in recommendation systems.

</details>


### [12] [MoDora: Tree-Based Semi-Structured Document Analysis System](https://arxiv.org/abs/2602.23061)
*Bangrui Xu,Qihang Yao,Zirui Tang,Xuanhe Zhou,Yeye He,Shihan Yu,Qianqian Xu,Bin Wang,Guoliang Li,Conghui He,Fan Wu*

Main category: cs.IR

TL;DR: MoDora是一个基于大语言模型的半结构化文档分析系统，通过布局感知组件构建、层次化组件组织和问题类型感知检索，显著提升了半结构化文档问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 半结构化文档（包含表格、图表、层次化段落等元素）在现实世界中广泛存在，但现有方法在处理这类文档的自然语言问答时面临三大挑战：OCR提取的元素碎片化且缺乏语义上下文；缺乏有效的层次结构表示；难以检索和整合分散在多区域的信息。

Method: 1. 采用局部对齐聚合策略将OCR解析元素转换为布局感知组件，并对具有层次标题或非文本元素的组件进行类型特定信息提取。2. 设计组件关联树（CCTree）层次化组织组件，通过自底向上的级联摘要过程显式建模组件间关系和布局差异。3. 提出问题类型感知检索策略，支持基于布局的网格划分检索和基于LLM引导剪枝的语义检索。

Result: 实验表明，MoDora在准确性上比基线方法提升了5.97%-61.07%，显著优于现有方法。

Conclusion: MoDora通过创新的组件表示、层次化组织和智能检索策略，有效解决了半结构化文档问答的技术挑战，为复杂文档分析提供了实用解决方案。

Abstract: Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document.
  To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.

</details>


### [13] [MaRI: Accelerating Ranking Model Inference via Structural Re-parameterization in Large Scale Recommendation System](https://arxiv.org/abs/2602.23105)
*Yusheng Huang,Pengbo Xu,Shen Wang,Changxin Lao,Jiangxia Cao,Shuang Wen,Shuang Yang,Zhaojie Liu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: 提出MaRI框架，通过矩阵重参数化技术实现推荐排序模型的无损加速，解决现有加速方法导致精度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有排序模型加速方法（结构轻量化或知识蒸馏）通常会导致精度下降，而通过优化特征融合矩阵乘法实现无损加速的研究不足。观察到用户侧计算在特征融合矩阵乘法中存在冗余。

Method: 提出MaRI（矩阵重参数化推理框架），采用结构重参数化思想来减少用户侧计算的冗余，作为现有技术的补充方法。

Result: 框架能够加速排序模型推理而不损失任何精度，解决了现有加速方法精度下降的问题。

Conclusion: MaRI为大规模推荐系统中的排序模型提供了一种无损加速的新方法，通过矩阵重参数化优化特征融合计算，是现有加速技术的有效补充。

Abstract: Ranking models, i.e., coarse-ranking and fine-ranking models, serve as core components in large-scale recommendation systems, responsible for scoring massive item candidates based on user preferences. To meet the stringent latency requirements of online serving, structural lightweighting or knowledge distillation techniques are commonly employed for ranking model acceleration. However, these approaches typically lead to a non-negligible drop in accuracy. Notably, the angle of lossless acceleration by optimizing feature fusion matrix multiplication, particularly through structural reparameterization, remains underexplored. In this paper, we propose MaRI, a novel Matrix Re-parameterized Inference framework, which serves as a complementary approach to existing techniques while accelerating ranking model inference without any accuracy loss. MaRI is motivated by the observation that user-side computation is redundant in feature fusion matrix multiplication, and we therefore adopt the philosophy of structural reparameterization to alleviate such redundancy.

</details>


### [14] [From Agnostic to Specific: Latent Preference Diffusion for Multi-Behavior Sequential Recommendation](https://arxiv.org/abs/2602.23132)
*Ruochen Yang,Xiaodong Li,Jiawei Sheng,Jiangxia Cao,Xinkui Lin,Shen Wang,Shuang Yang,Zhaojie Liu,Tingwen Liu*

Main category: cs.IR

TL;DR: FatsMB是一个基于扩散模型的框架，通过从行为无关到行为特定的潜在空间偏好生成，实现多样化和准确的多行为序列推荐。


<details>
  <summary>Details</summary>
Motivation: 现有多行为序列推荐方法存在两个主要问题：1）忽略了用户潜在偏好对决策的影响；2）基于偏好评分的判别式范式无法有效处理从低熵行为到高熵项目的不确定性，导致推荐效果不佳。

Method: 提出FatsMB框架：1）设计多行为自动编码器（MBAE）构建统一的用户潜在偏好空间；2）使用行为感知RoPE（BaRoPE）进行多信息融合；3）在潜在空间中进行目标行为特定偏好转移；4）引入多条件引导层归一化（MCGLN）进行去噪。

Result: 在真实世界数据集上的广泛实验证明了该模型的有效性。

Conclusion: FatsMB通过从行为无关到行为特定的潜在空间偏好生成，能够实现多样化和准确的多行为序列推荐，解决了现有方法忽略用户潜在偏好和处理不确定性的问题。

Abstract: Multi-behavior sequential recommendation (MBSR) aims to learn the dynamic and heterogeneous interactions of users' multi-behavior sequences, so as to capture user preferences under target behavior for the next interacted item prediction. Unlike previous methods that adopt unidirectional modeling by mapping auxiliary behaviors to target behavior, recent concerns are shifting from behavior-fixed to behavior-specific recommendation. However, these methods still ignore the user's latent preference that underlying decision-making, leading to suboptimal solutions. Meanwhile, due to the asymmetric deterministic between items and behaviors, discriminative paradigm based on preference scoring is unsuitable to capture the uncertainty from low-entropy behaviors to high-entropy items, failing to provide efficient and diverse recommendation. To address these challenges, we propose \textbf{FatsMB}, a framework based diffusion model that guides preference generation \textit{\textbf{F}rom Behavior-\textbf{A}gnostic \textbf{T}o Behavior-\textbf{S}pecific} in latent spaces, enabling diverse and accurate \textit{\textbf{M}ulti-\textbf{B}ehavior Sequential Recommendation}. Specifically, we design a Multi-Behavior AutoEncoder (MBAE) to construct a unified user latent preference space, facilitating interaction and collaboration across Behaviors, within Behavior-aware RoPE (BaRoPE) employed for multiple information fusion. Subsequently, we conduct target behavior-specific preference transfer in the latent space, enriching with informative priors. A Multi-Condition Guided Layer Normalization (MCGLN) is introduced for the denoising. Extensive experiments on real-world datasets demonstrate the effectiveness of our model.

</details>


### [15] [Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments](https://arxiv.org/abs/2602.23234)
*Evangelia Christakopoulou,Vivekkumar Patel,Hemanth Velaga,Sandip Gaikwad*

Main category: cs.IR

TL;DR: 论文通过优化LLM配置生成大量文本相关性标签，解决了行为相关性标签丰富但文本相关性标签稀缺的问题，将生成的标签加入生产排序器后，在离线评估和线上A/B测试中都取得了显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 商业搜索系统需要同时优化行为相关性（用户点击/下载倾向）和文本相关性（结果与查询的语义匹配），但文本相关性标签由专家提供，数量远少于行为相关性标签，存在数据稀缺问题。

Method: 系统评估不同LLM配置，发现经过专门微调的模型在提供高质量文本相关性标签方面显著优于更大的预训练模型；使用最优模型作为"力量倍增器"生成数百万文本相关性标签；将这些标签加入生产排序器中。

Result: 离线评估显示NDCG在行为相关性和文本相关性上同时提升，Pareto前沿向外移动；全球A/B测试显示转化率显著提升0.24%，在尾部查询中效果最明显，因为新标签在没有可靠行为相关性标签时提供了稳健信号。

Conclusion: 通过优化LLM生成大量高质量文本相关性标签，可以有效解决标签稀缺问题，显著提升搜索系统的相关性排序效果，特别是在行为信号不足的尾部查询中效果尤为突出。

Abstract: Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Automating the Detection of Requirement Dependencies Using Large Language Models](https://arxiv.org/abs/2602.22456)
*Ikram Darif,Feifei Niu,Manel Abdellatif,Lionel C. Briand,Ramesh S.,Arun Adiththan*

Main category: cs.SE

TL;DR: LEREDD：基于大语言模型的需求依赖关系自动检测方法，利用检索增强生成和上下文学习技术，在需求依赖分类任务中表现出色，准确率达0.93，F1分数0.84。


<details>
  <summary>Details</summary>
Motivation: 需求之间存在多种依赖关系，识别这些依赖对软件开发至关重要。然而，由于现代软件系统中需求数量庞大、复杂度高、自然语言需求存在模糊性且频繁变更，依赖检测往往被忽视或只能手动进行。大语言模型在自然语言处理方面表现出强大能力，为需求相关任务提供了新途径。

Method: 提出LEREDD方法，基于大语言模型自动检测需求依赖关系，采用检索增强生成（RAG）和上下文学习（ICL）技术，直接从自然语言需求中识别多种依赖类型。

Result: LEREDD在依赖和非依赖需求分类中准确率达到0.93，F1分数0.84（非依赖情况平均0.96）。在细粒度依赖类型检测上表现优异，特别是Requires依赖类型的F1分数相比基线方法分别提升了94.87%和105.41%。

Conclusion: LEREDD方法有效解决了需求依赖检测的自动化问题，显著优于零样本大语言模型和现有基线方法。研究还提供了包含813个需求对的标注数据集，支持可重复性和未来研究。

Abstract: Requirements are inherently interconnected through various types of dependencies. Identifying these dependencies is essential, as they underpin critical decisions and influence a range of activities throughout software development. However, this task is challenging, particularly in modern software systems, given the high volume of complex, coupled requirements. These challenges are further exacerbated by the ambiguity of Natural Language (NL) requirements and their constant change. Consequently, requirement dependency detection is often overlooked or performed manually. Large Language Models (LLMs) exhibit strong capabilities in NL processing, presenting a promising avenue for requirement-related tasks. While they have shown to enhance various requirements engineering tasks, their effectiveness in identifying requirement dependencies remains unexplored. In this paper, we introduce LEREDD, an LLM-based approach for automated detection of requirement dependencies that leverages Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL). It is designed to identify diverse dependency types directly from NL requirements. We empirically evaluate LEREDD against two state-of-the-art baselines. The results show that LEREDD provides highly accurate classification of dependent and non-dependent requirements, achieving an accuracy of 0.93, and an F1 score of 0.84, with the latter averaging 0.96 for non-dependent cases. LEREDD outperforms zero-shot LLMs and baselines, particularly in detecting fine-grained dependency types, where it yields average relative gains of 94.87% and 105.41% in F1 scores for the Requires dependency over the baselines. We also provide an annotated dataset of requirement dependencies encompassing 813 requirement pairs across three distinct systems to support reproducibility and future research.

</details>


### [17] [RandSet: Randomized Corpus Reduction for Fuzzing Seed Scheduling](https://arxiv.org/abs/2602.22729)
*Yuchong Xie,Kaikai Zhang,Yu Liu,Rundong Yang,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.SE

TL;DR: RandSet是一种新颖的随机化语料库缩减技术，通过将语料库缩减问题建模为集合覆盖问题，计算覆盖所有特征的随机子集，从而在降低语料库大小的同时保持种子多样性，有效缓解模糊测试中的种子爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 模糊测试中的种子爆炸问题导致模糊器维护庞大的种子语料库却无法选择有前景的种子。现有工作主要关注种子优先级排序，但语料库规模仍然庞大。需要从语料库缩减的新视角来解决这一问题，但现有技术如cull_queue、AFL-Cmin和MinSet存在多样性差或开销大的问题。

Method: 将语料库缩减问题建模为集合覆盖问题，计算覆盖整个语料库所有特征的随机子集。通过引入随机性获得随机化算法的两个优势：随机输出（多样化种子选择）和低运行时成本。然后从这个小的随机子集中调度种子，而不是从整个语料库中调度。

Result: 在AFL++、LibAFL和Centipede三个流行模糊器上实现RandSet，并在独立程序、FuzzBench和Magma上评估。结果显示：在独立程序和FuzzBench程序上平均子集比例分别为4.03%和5.99%；在独立程序上实现16.58%的覆盖率提升，在FuzzBench上最高提升3.57%；在Magma上比最先进技术多触发7个真实漏洞；仅引入1.17%-3.93%的开销。

Conclusion: RandSet通过随机化语料库缩减技术，在显著降低语料库规模的同时保持种子多样性，有效缓解种子爆炸问题，以最小开销实现了更好的覆盖率提升和漏洞发现能力。

Abstract: Seed explosion is a fundamental problem in fuzzing seed scheduling, where a fuzzer maintains a huge corpus and fails to choose promising seeds. Existing works focus on seed prioritization but still suffer from seed explosion since corpus size remains huge. We tackle this from a new perspective: corpus reduction, i.e., computing a seed corpus subset. However, corpus reduction could lead to poor seed diversity and large runtime overhead. Prior techniques like cull_queue, AFL-Cmin, and MinSet suffer from poor diversity or prohibitive overhead, making them unsuitable for high-frequency seed scheduling.
  We propose RandSet, a novel randomized corpus reduction technique that reduces corpus size and yields diverse seed selection simultaneously with minimal overhead. Our key insight is introducing randomness into corpus reduction to enjoy two benefits of a randomized algorithm: randomized output (diverse seed selection) and low runtime cost. Specifically, we formulate corpus reduction as a set cover problem and compute a randomized subset covering all features of the entire corpus. We then schedule seeds from this small, randomized subset rather than the entire corpus, effectively mitigating seed explosion.
  We implement RandSet on three popular fuzzers: AFL++, LibAFL, and Centipede, and evaluate it on standalone programs, FuzzBench, and Magma. Results show RandSet achieves significantly more diverse seed selection than other reduction techniques, with average subset ratios of 4.03% and 5.99% on standalone and FuzzBench programs. RandSet achieves a 16.58% coverage gain on standalone programs and up to 3.57% on FuzzBench in AFL++, triggers up to 7 more ground-truth bugs than the state-of-the-art on Magma, while introducing only 1.17%-3.93% overhead.

</details>


### [18] [Productivity and Collaboration in Hybrid Agile Teams: An Interview Study](https://arxiv.org/abs/2602.22835)
*Elisabeth Mo,Jefferson Seide Molléri,Asle Fagerstrøm*

Main category: cs.SE

TL;DR: 混合工作模式影响敏捷团队的生产力与协作，研究发现减少非正式互动、参与不均、工具依赖增加，敏捷仪式成为协调锚点


<details>
  <summary>Details</summary>
Motivation: 疫情后混合工作成为常态，研究旨在了解混合工作环境如何影响敏捷团队的价值交付、协作和适应能力

Method: 通过对三个挪威敏捷团队进行九次访谈的定性研究，探讨混合工作环境的影响

Result: 混合工作减少非正式互动、造成参与不均、增加对数字工具的依赖；敏捷仪式成为团队协调的锚点；信任、沟通和工具支持是团队效能的中介因素

Conclusion: 混合敏捷工作是一个不断发展的领域，需要定制化结构来支持包容性、团队凝聚力和可持续绩效

Abstract: Hybrid work has become a reality post-pandemic, transforming how Agile teams deliver value, collaborate, and adapt. This study investigate how hybrid settings influence productivity and collaboration through nine interviews with three Norwegian Agile teams. Our findings show that hybrid work reduces informal interaction, creates uneven participation, and increases reliance on digital tools. Agile ceremonies became alignment anchors, while trust, communication, and tool support mediate team effectiveness. Hybrid Agile work is an evolving field that requires tailored structures to support inclusion, team cohesion, and sustainable performance.

</details>


### [19] [CL4SE: A Context Learning Benchmark For Software Engineering Tasks](https://arxiv.org/abs/2602.23047)
*Haichuan Hu,Ye Shang,Guoqing Xie,Congqing He,Quanjun Zhang*

Main category: cs.SE

TL;DR: CL4SE是首个针对软件工程上下文学习的标准化评估框架，提出了四种SE专用上下文类型的细粒度分类法，构建了包含13,000多个样本的高质量数据集，实验表明上下文学习平均提升性能24.7%。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统性的软件工程专用上下文类型分类法，也没有专门的基准来量化不同上下文在核心SE工作流程中的异质效应，这限制了上下文学习在软件工程任务中的潜力发挥。

Method: 提出CL4SE基准，包含四种SE导向的上下文类型分类：可解释示例、项目特定上下文、程序化决策上下文、正负上下文，每种映射到代表性任务（代码生成、代码摘要、代码审查、补丁正确性评估），构建了来自30多个开源项目的13,000多个样本的高质量数据集，评估了五个主流LLM在九个指标上的表现。

Result: 上下文学习在所有任务上平均性能提升24.7%：程序化上下文将代码审查性能提升高达33%（Qwen3-Max），混合正负上下文将补丁评估提升30%（DeepSeek-V3），项目特定上下文将代码摘要BLEU提升14.78%（GPT-Oss-120B），可解释示例将代码生成PASS@1提升5.72%（DeepSeek-V3）。

Conclusion: CL4SE建立了首个SE上下文学习的标准化评估框架，提供了任务特定上下文设计的可操作经验见解，并发布了大规模数据集以促进该领域的可重复研究。

Abstract: Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive & negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain.

</details>


### [20] [LLM-Powered Silent Bug Fuzzing in Deep Learning Libraries via Versatile and Controlled Bug Transfer](https://arxiv.org/abs/2602.23065)
*Kunpeng Zhang,Dongwei Xiao,Daoyuan Wu,Jiali Zhao,Yuanyi Lin,Tongtong Xu,Shaohua Wang,Shuai Wang*

Main category: cs.SE

TL;DR: TransFuzz：利用LLM从历史bug报告中提取模式，通过语义匹配将bug上下文和测试用例迁移到功能相似的API上，以检测深度学习库中的静默bug。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模糊测试技术主要检测崩溃，但难以发现静默bug（无崩溃但行为错误的bug）。历史bug报告包含丰富的静默bug信息但未充分利用，需要新方法来主动检测这类严重bug。

Method: 1) 使用LLM从历史问题中提取上下文感知的bug模式；2) 基于功能嵌入匹配语义相关的API；3) 合成带有定制化测试预言（oracle）的测试用例；4) 引入LLM驱动的自验证模块确保bug迁移可靠性；5) 实现为TransFuzz工具。

Result: 在PyTorch、TensorFlow和MindSpore三个主流DL库中发现了79个先前未知的bug（其中12个被确认为CVE），涵盖10种bug类型，证明了方法的有效性和泛化能力。

Conclusion: TransFuzz通过LLM驱动的上下文感知bug迁移，成功解决了DL库静默bug检测的挑战，为深度学习库的可靠性提供了有效工具，并能将bug发现能力迁移到不同库和API。

Abstract: Deep learning (DL) libraries are widely used in critical applications, where even subtle silent bugs can lead to serious consequences. While existing DL fuzzing techniques have made progress in detecting crashes, they inherently struggle to detect silent bugs due to the lack of effective test programs and corresponding oracles.
  Building on the observation that historical bug reports contain rich, underutilized information about silent bugs, we leverage large language models (LLMs) to perform versatile yet controlled bug transfer for silent bug fuzzing. Specifically, our approach uses LLMs to extract context-aware bug patterns from historical issues, match semantically related Application Programming Interfaces (APIs) using functionality-based embeddings, and synthesize test cases with customized oracles. This enables proactive detection of silent bugs by transferring high-risk contexts and oracle designs from known buggy APIs to functionally similar target APIs. To ensure the reliability of our context-aware bug transfer, we introduce an LLM-powered self-validation module that systematically evaluates the validity of each transferred bug instance. We implement this methodology in a tool named TransFuzz and evaluate it on three mainstream DL libraries: PyTorch, TensorFlow, and MindSpore. TransFuzz successfully discovers 79 previously unknown bugs (12 confirmed as Common Vulnerabilities and Exposures (CVEs)) in 10 bug types, demonstrating its effectiveness and generalizability in migrating DL library bug discovery capabilities.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Prior Knowledge-enhanced Spatio-temporal Epidemic Forecasting](https://arxiv.org/abs/2602.22270)
*Sijie Ruan,Jinyu Li,Jia Wei,Zenghao Xu,Jie Bao,Junshi Xu,Junyang Qiu,Hanning Yuan,Xiaoxiao Wang,Shuliang Wang*

Main category: cs.LG

TL;DR: STOEP是一个结合隐式时空先验和显式专家先验的混合框架，通过动态调整区域依赖、增强弱信号和正则化参数来解决流行病预测中的关键挑战，在COVID-19和流感数据集上表现优于基线11.1%。


<details>
  <summary>Details</summary>
Motivation: 现有时空流行病预测方法存在三个主要问题：对弱流行信号不敏感、空间关系过于简化、参数估计不稳定，这些限制了预测的准确性和实用性。

Method: STOEP包含三个核心组件：1) 病例感知邻接学习(CAL)：基于历史感染模式动态调整移动性区域依赖；2) 空间信息参数估计(SPE)：使用可学习的空间先验增强弱流行信号；3) 基于滤波的机制预测(FMF)：采用专家引导的自适应阈值策略正则化流行病参数。

Result: 在真实世界的COVID-19和流感数据集上，STOEP在RMSE指标上优于最佳基线方法11.1%，系统已在中国某省级疾控中心部署以支持下游应用。

Conclusion: STOEP通过整合隐式时空先验和显式专家先验，有效解决了流行病预测中的关键挑战，提高了预测准确性，并已在实际公共卫生管理中验证了其实用价值。

Abstract: Spatio-temporal epidemic forecasting is critical for public health management, yet existing methods often struggle with insensitivity to weak epidemic signals, over-simplified spatial relations, and unstable parameter estimation. To address these challenges, we propose the Spatio-Temporal priOr-aware Epidemic Predictor (STOEP), a novel hybrid framework that integrates implicit spatio-temporal priors and explicit expert priors. STOEP consists of three key components: (1) Case-aware Adjacency Learning (CAL), which dynamically adjusts mobility-based regional dependencies using historical infection patterns; (2) Space-informed Parameter Estimating (SPE), which employs learnable spatial priors to amplify weak epidemic signals; and (3) Filter-based Mechanistic Forecasting (FMF), which uses an expert-guided adaptive thresholding strategy to regularize epidemic parameters. Extensive experiments on real-world COVID-19 and influenza datasets demonstrate that STOEP outperforms the best baseline by 11.1% in RMSE. The system has been deployed at one provincial CDC in China to facilitate downstream applications.

</details>


### [22] [Support Tokens, Stability Margins, and a New Foundation for Robust LLMs](https://arxiv.org/abs/2602.22271)
*Deepak Agarwal,Dhyey Dharmendrakumar Mavani,Suyash Gupta,Karthik Sethuraman,Tejas Dharamsi*

Main category: cs.LG

TL;DR: 该论文将因果自注意力Transformer重新解释为概率框架，揭示了参数约束导致的token空间结构化几何，提出了类似支持向量机的边界解释，并开发了只需最小修改的贝叶斯训练方法。


<details>
  <summary>Details</summary>
Motivation: 重新解释现代基础模型的核心组件——因果自注意力Transformer，将其置于概率框架中，类似于经典PCA扩展到概率PCA的方式，以揭示其深层结构特性。

Method: 1. 将因果自注意力Transformer重新表述为概率框架；2. 揭示由于变量变换现象产生的自注意力参数约束；3. 提出类似支持向量机的边界解释和支持token概念；4. 将LLM解释为token空间幂集上的随机过程；5. 提出贝叶斯框架和MAP估计目标，在标准交叉熵损失中添加平滑对数障碍惩罚。

Result: 1. 发现了自注意力参数约束导致的token空间高度结构化几何；2. 揭示了注意力变得病态的边界，提供了类似支持向量机的边际解释；3. 提出了支持token的概念；4. 开发了只需最小修改的训练方法，在保持样本外准确性的同时提供更鲁棒的模型。

Conclusion: 该研究为Transformer架构提供了新的概率理论框架，揭示了其内在的结构约束和几何特性，并提出了实用的贝叶斯训练方法，为理解LLM解码动态和提升模型鲁棒性提供了理论洞见。

Abstract: Self-attention is usually described as a flexible, content-adaptive way to mix a token with information from its past. We re-interpret causal self-attention transformers, the backbone of modern foundation models, within a probabilistic framework, much like how classical PCA is extended to probabilistic PCA. However, this re-formulation reveals a surprising and deeper structural insight: due to a change-of-variables phenomenon, a barrier constraint emerges on the self-attention parameters. This induces a highly structured geometry on the token space, providing theoretical insights into the dynamics of LLM decoding. This reveals a boundary where attention becomes ill-conditioned, leading to a margin interpretation similar to classical support vector machines. Just like support vectors, this naturally gives rise to the concept of support tokens.
  Furthermore, we show that LLMs can be interpreted as a stochastic process over the power set of the token space, providing a rigorous probabilistic framework for sequence modeling. We propose a Bayesian framework and derive a MAP estimation objective that requires only a minimal modification to standard LLM training: the addition of a smooth log-barrier penalty to the usual cross-entropy loss. We demonstrate that this provides more robust models without sacrificing out-of-sample accuracy and that it is straightforward to incorporate in practice.

</details>


### [23] [X-REFINE: XAI-based RElevance input-Filtering and archItecture fiNe-tuning for channel Estimation](https://arxiv.org/abs/2602.22277)
*Abdul Karim Gizzini,Yahia Medjahdi*

Main category: cs.LG

TL;DR: X-REFINE是一个基于XAI的框架，通过联合输入滤波和架构微调，优化6G无线通信中的深度学习模型部署，在保持性能的同时降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 6G无线通信中AI原生架构至关重要，但深度学习模型的黑盒特性和高复杂度限制了其在关键应用（如信道估计）中的实际部署。现有的基于扰动的XAI解决方案通常只关注输入滤波，而忽略了内部结构优化。

Method: 提出X-REFINE框架，采用基于分解的、符号稳定的LRP epsilon规则，通过反向传播预测来推导子载波和隐藏神经元的高分辨率相关性分数，实现联合输入滤波和架构微调的全方位优化。

Result: 仿真结果表明，X-REFINE在可解释性-性能-复杂度权衡方面表现优异，显著降低了计算复杂度，同时在不同场景下保持了稳健的误码率性能。

Conclusion: X-REFINE为6G无线通信中的AI模型部署提供了一种有效的解决方案，通过XAI技术实现了模型优化和复杂度降低的平衡。

Abstract: AI-native architectures are vital for 6G wireless communications. The black-box nature and high complexity of deep learning models employed in critical applications, such as channel estimation, limit their practical deployment. While perturbation-based XAI solutions offer input filtering, they often neglect internal structural optimization. We propose X-REFINE, an XAI-based framework for joint input-filtering and architecture fine-tuning. By utilizing a decomposition-based, sign-stabilized LRP epsilon rule, X-REFINE backpropagates predictions to derive high-resolution relevance scores for both subcarriers and hidden neurons. This enables a holistic optimization that identifies the most faithful model components. Simulation results demonstrate that X-REFINE achieves a superior interpretability-performance-complexity trade-off, significantly reducing computational complexity while maintaining robust bit error rate (BER) performance across different scenarios.

</details>


### [24] [Integrating Machine Learning Ensembles and Large Language Models for Heart Disease Prediction Using Voting Fusion](https://arxiv.org/abs/2602.22280)
*Md. Tahsin Amin,Tanim Ahmmod,Zannatul Ferdus,Talukder Naemul Hasan Naem,Ehsanul Ferdous,Arpita Bhattacharjee,Ishmam Ahmed Solaiman,Nahiyan Bin Noor*

Main category: cs.LG

TL;DR: 该研究比较了传统机器学习模型与大型语言模型在心血管疾病预测中的表现，发现机器学习集成方法（95.78%准确率）优于LLMs（78.9%准确率），而两者融合的混合方法（96.62%准确率）达到最佳效果。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要早期识别、精确风险分类和可靠的决策支持技术。虽然机器学习算法在处理复杂非线性患者数据方面表现出色，但大型语言模型的出现提供了新的零样本和少样本推理能力，研究旨在探索两者在心血管疾病预测中的表现和融合潜力。

Method: 使用包含1,190名患者记录的合并数据集，比较传统机器学习模型（包括Random Forest、XGBoost、LightGBM、CatBoost等集成方法）与开源大型语言模型（通过OpenRouter API访问）。最终提出混合融合方法，将ML集成与LLM推理结合，使用Gemini 2.5 Flash模型。

Result: ML集成方法获得最高性能（95.78%准确率，ROC-AUC 0.96）；LLMs在零样本设置下表现中等（78.9%准确率），少样本设置下略差（72.6%准确率）；混合融合方法达到最佳结果（96.62%准确率，0.97 AUC）。

Conclusion: LLMs在单独使用时表现不如传统ML模型，但与ML模型结合使用时效果最佳。混合ML-LLM系统可以在不确定情况下增强预测能力，为更可靠的临床决策支持工具开辟道路，尽管提升幅度相对较小。

Abstract: Cardiovascular disease is the primary cause of death globally, necessitating early identification, precise risk classification, and dependable decision-support technologies. The advent of large language models (LLMs) provides new zero-shot and few-shot reasoning capabilities, even though machine learning (ML) algorithms, especially ensemble approaches like Random Forest, XGBoost, LightGBM, and CatBoost, are excellent at modeling complex, non-linear patient data and routinely beat logistic regression. This research predicts cardiovascular disease using a merged dataset of 1,190 patient records, comparing traditional machine learning models (95.78% accuracy, ROC-AUC 0.96) with open-source large language models via OpenRouter APIs. Finally, a hybrid fusion of the ML ensemble and LLM reasoning under Gemini 2.5 Flash achieved the best results (96.62% accuracy, 0.97 AUC), showing that LLMs (78.9 % accuracy) work best when combined with ML models rather than used alone. Results show that ML ensembles achieved the highest performance (95.78% accuracy, ROC-AUC 0.96), while LLMs performed moderately in zero-shot (78.9%) and slightly better in few-shot (72.6%) settings. The proposed hybrid method enhanced the strength in uncertain situations, illustrating that ensemble ML is considered the best structured tabular prediction case, but it can be integrated with hybrid ML-LLM systems to provide a minor increase and open the way to more reliable clinical decision-support tools.

</details>


### [25] [Early Risk Stratification of Dosing Errors in Clinical Trials Using Machine Learning](https://arxiv.org/abs/2602.22285)
*Félicien Hêche,Sohrab Ferdowsi,Anthony Yazdani,Sara Sansaloni-Pastor,Douglas Teodoro*

Main category: cs.LG

TL;DR: 开发基于机器学习的框架，利用试验启动前信息对临床试验进行早期风险分层，预测高剂量错误率风险


<details>
  <summary>Details</summary>
Motivation: 临床试验中的剂量错误是重要的安全问题，需要早期识别高风险试验以进行主动质量管理

Method: 从ClinicalTrials.gov构建42,112个试验数据集，提取结构化、半结构化和非结构化文本数据，使用XGBoost处理结构化特征，ClinicalModernBERT处理文本数据，采用简单晚期融合模型，并进行概率校准

Result: 晚期融合模型获得最高AUC-ROC（0.862），校准输出能够将试验可靠地分层到预定义风险类别中，高风险组中标记为剂量错误率过高的试验比例单调增加

Conclusion: 研究提出了可重复、可扩展的机器学习框架，用于临床试验剂量错误风险的早期分层，支持基于风险的主动质量管理

Abstract: Objective: The objective of this study is to develop a machine learning (ML)-based framework for early risk stratification of clinical trials (CTs) according to their likelihood of exhibiting a high rate of dosing errors, using information available prior to trial initiation. Materials and Methods: We constructed a dataset from ClinicalTrials.gov comprising 42,112 CTs. Structured, semi-structured trial data, and unstructured protocol-related free-text data were extracted. CTs were assigned binary labels indicating elevated dosing error rate, derived from adverse event reports, MedDRA terminology, and Wilson confidence intervals. We evaluated an XGBoost model trained on structured features, a ClinicalModernBERT model using textual data, and a simple late-fusion model combining both modalities. Post-hoc probability calibration was applied to enable interpretable, trial-level risk stratification. Results: The late-fusion model achieved the highest AUC-ROC (0.862). Beyond discrimination, calibrated outputs enabled robust stratification of CTs into predefined risk categories. The proportion of trials labeled as having an excessively high dosing error rate increased monotonically across higher predicted risk groups and aligned with the corresponding predicted probability ranges. Discussion: These findings indicate that dosing error risk can be anticipated at the trial level using pre-initiation information. Probability calibration was essential for translating model outputs into reliable and interpretable risk categories, while simple multimodal integration yielded performance gains without requiring complex architectures. Conclusion: This study introduces a reproducible and scalable ML framework for early, trial-level risk stratification of CTs at risk of high dosing error rates, supporting proactive, risk-based quality management in clinical research.

</details>


### [26] [Reliable XAI Explanations in Sudden Cardiac Death Prediction for Chagas Cardiomyopathy](https://arxiv.org/abs/2602.22288)
*Vinícius P. Chagas,Luiz H. T. Viana,Mac M. da S. Carlos,João P. V. Madeiro,Roberto C. Pedrosa,Thiago Alves Rocha,Carlos H. L. Cavalcante*

Main category: cs.LG

TL;DR: 该研究将具有正确性保证的逻辑可解释性方法应用于恰加斯心肌病患者的猝死预测，解决了AI模型黑箱问题，实现了高精度预测和100%解释保真度。


<details>
  <summary>Details</summary>
Motivation: 恰加斯心肌病患者的猝死预测具有挑战性，尤其是对非高风险患者。现有AI模型虽然能改善风险分层，但缺乏透明度，被视为黑箱模型，且一些启发式解释方法缺乏正确性保证，可能导致决策错误。

Method: 应用具有正确性保证的逻辑可解释性方法到猝死预测问题。该方法应用于一个准确率和召回率超过95%的AI分类器，确保解释的保真度和可靠性。

Result: 该方法展示了强大的预测性能，实现了100%的解释保真度。与最先进的启发式方法相比，表现出更优的一致性和鲁棒性。

Conclusion: 逻辑可解释性方法增强了临床信任，促进了AI驱动工具在实践中的整合，特别是在最需要的流行地区，有助于大规模部署。

Abstract: Sudden cardiac death (SCD) is unpredictable, and its prediction in Chagas cardiomyopathy (CC) remains a significant challenge, especially in patients not classified as high risk. While AI and machine learning models improve risk stratification, their adoption is hindered by a lack of transparency, as they are often perceived as \textit{black boxes} with unclear decision-making processes. Some approaches apply heuristic explanations without correctness guarantees, leading to mistakes in the decision-making process. To address this, we apply a logic-based explainability method with correctness guarantees to the problem of SCD prediction in CC. This explainability method, applied to an AI classifier with over 95\% accuracy and recall, demonstrated strong predictive performance and 100\% explanation fidelity. When compared to state-of-the-art heuristic methods, it showed superior consistency and robustness. This approach enhances clinical trust, facilitates the integration of AI-driven tools into practice, and promotes large-scale deployment, particularly in endemic regions where it is most needed.

</details>


### [27] [Manifold of Failure: Behavioral Attraction Basins in Language Models](https://arxiv.org/abs/2602.22291)
*Sarthak Munshi,Manish Bhatt,Vineeth Sai Narajala,Idan Habler,AmmarnAl-Kahfah,Ken Huang,Blake Gatto*

Main category: cs.LG

TL;DR: 该论文提出了一个系统映射大语言模型失败流形（Manifold of Failure）的框架，将漏洞搜索重新定义为质量多样性问题，使用MAP-Elites算法揭示失败区域的连续拓扑结构，并展示了不同LLM模型的安全景观差异。


<details>
  <summary>Details</summary>
Motivation: 先前工作主要关注将对抗性示例投影回自然数据流形以恢复安全性，但作者认为全面理解AI安全需要表征不安全区域本身。该研究旨在系统性地映射大语言模型的失败流形，从寻找离散故障转向理解其底层结构。

Method: 将漏洞搜索重新定义为质量多样性问题，使用MAP-Elites算法来揭示失败区域的连续拓扑结构（称为行为吸引盆地）。采用对齐偏差（Alignment Deviation）作为质量度量，引导搜索朝向模型行为与预期对齐差异最大的区域。在三个LLM模型（Llama-3-8B、GPT-OSS-20B、GPT-5-Mini）上进行实验。

Result: MAP-Elites实现了高达63%的行为覆盖率，发现了多达370个不同的漏洞生态位。不同模型展现出显著不同的拓扑特征：Llama-3-8B表现出近乎普遍的脆弱性平台（平均对齐偏差0.93），GPT-OSS-20B显示碎片化景观和空间集中的盆地（平均0.73），GPT-5-Mini表现出强大的鲁棒性，上限为0.50。该方法生成了现有攻击方法（GCG、PAIR、TAP）无法提供的可解释全局安全地图。

Conclusion: 该研究引入了一个系统映射大语言模型失败流形的框架，将AI安全研究范式从寻找离散故障转向理解其底层结构。通过揭示不同模型特有的拓扑特征，为模型安全评估和比较提供了新的视角，并生成了可解释的全局安全景观地图。

Abstract: While prior work has focused on projecting adversarial examples back onto the manifold of natural data to restore safety, we argue that a comprehensive understanding of AI safety requires characterizing the unsafe regions themselves. This paper introduces a framework for systematically mapping the Manifold of Failure in Large Language Models (LLMs). We reframe the search for vulnerabilities as a quality diversity problem, using MAP-Elites to illuminate the continuous topology of these failure regions, which we term behavioral attraction basins. Our quality metric, Alignment Deviation, guides the search towards areas where the model's behavior diverges most from its intended alignment. Across three LLMs: Llama-3-8B, GPT-OSS-20B, and GPT-5-Mini, we show that MAP-Elites achieves up to 63% behavioral coverage, discovers up to 370 distinct vulnerability niches, and reveals dramatically different model-specific topological signatures: Llama-3-8B exhibits a near-universal vulnerability plateau (mean Alignment Deviation 0.93), GPT-OSS-20B shows a fragmented landscape with spatially concentrated basins (mean 0.73), and GPT-5-Mini demonstrates strong robustness with a ceiling at 0.50. Our approach produces interpretable, global maps of each model's safety landscape that no existing attack method (GCG, PAIR, or TAP) can provide, shifting the paradigm from finding discrete failures to understanding their underlying structure.

</details>


### [28] [When Should a Model Change Its Mind? An Energy-Based Theory and Regularizer for Concept Drift in Electrocardiogram (ECG) Signals](https://arxiv.org/abs/2602.22294)
*Timothy Oladunni,Blessing Ojeme,Kyndal Maclin,Clyde Baidoo*

Main category: cs.LG

TL;DR: 提出PECT理论框架，通过能量守恒原理区分生理信号中的良性变化与真实概念漂移，开发ECRL正则化方法提升模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有概念漂移框架主要基于分布变化，无法区分生理信号中良性的能量波动与真正的概念变化，导致深度学习模型对幅度、速率或形态的无害变化产生不稳定预测。

Method: 提出生理能量守恒理论(PECT)，认为在虚拟漂移下，归一化潜在位移应与归一化信号能量变化成比例。通过能量约束表示学习(ECRL)实现这一原则，作为轻量级正则化器惩罚能量不一致的潜在移动。

Result: 在七种单模态和混合模型上的实验表明，最强三模态混合模型中，干净准确率基本保持(96.0%到94.1%)，扰动准确率显著提升(72.6%到85.5%)，融合表示漂移减少超过45%。

Conclusion: PECT作为能量-漂移定律，能够有效管理连续生理信号中的概念稳定性，ECRL正则化在不改变编码器架构或增加推理成本的情况下显著提升模型鲁棒性。

Abstract: Models operating on dynamic physiologic signals must distinguish benign, label-preserving variability from true concept change. Existing concept-drift frameworks are largely distributional and provide no principled guidance on how much a model's internal representation may move when the underlying signal undergoes physiologically plausible fluctuations in energy. As a result, deep models often misinterpret harmless changes in amplitude, rate, or morphology as concept drift, yielding unstable predictions, particularly in multimodal fusion settings.
  This study introduces Physiologic Energy Conservation Theory (PECT), an energy-based framework for concept stability in dynamic signals. PECT posits that under virtual drift, normalized latent displacement should scale proportionally with normalized signal energy change, while persistent violations of this proportionality indicate real concept drift. We operationalize this principle through Energy-Constrained Representation Learning (ECRL), a lightweight regularizer that penalizes energy-inconsistent latent movement without modifying encoder architectures or adding inference-time cost.
  Although PECT is formulated for dynamic signals in general, we instantiate and evaluate it on multimodal ECG across seven unimodal and hybrid models. Experiments show that in the strongest trimodal hybrid (1D+2D+Transformer), clean accuracy is largely preserved (96.0% to 94.1%), while perturbed accuracy improves substantially (72.6% to 85.5%) and fused representation drift decreases by over 45%. Similar trends are observed across all architectures, providing empirical evidence that PECT functions as an energy-drift law governing concept stability in continuous physiologic signals.

</details>


### [29] [Learning Rewards, Not Labels: Adversarial Inverse Reinforcement Learning for Machinery Fault Detection](https://arxiv.org/abs/2602.22297)
*Dhiraj Neupane,Richard Dazeley,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: 该研究将机械故障检测重新定义为离线逆强化学习问题，通过从健康操作序列中学习奖励动态，无需人工设计奖励函数或故障标签，实现了基于异常评分的早期故障检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的机械故障检测方法未能充分利用强化学习的序列决策优势，通常将故障检测简化为上下文赌博机问题。需要将强化学习的序列推理能力与机械故障检测的时间结构对齐。

Method: 采用对抗性逆强化学习框架，训练一个判别器来区分正常（专家）样本和策略生成的转移。判别器学习到的奖励作为异常评分，用于检测偏离正常操作行为的故障。

Result: 在三个运行至故障基准数据集（HUMS2023、IMS和XJTU-SY）上评估，模型始终对正常样本分配低异常评分，对故障样本分配高异常评分，实现了早期且鲁棒的故障检测。

Conclusion: 通过将强化学习的序列推理与机械故障检测的时间结构对齐，这项工作为数据驱动的工业环境中基于强化学习的诊断开辟了新路径。

Abstract: Reinforcement learning (RL) offers significant promise for machinery fault detection (MFD). However, most existing RL-based MFD approaches do not fully exploit RL's sequential decision-making strengths, often treating MFD as a simple guessing game (Contextual Bandits). To bridge this gap, we formulate MFD as an offline inverse reinforcement learning problem, where the agent learns the reward dynamics directly from healthy operational sequences, thereby bypassing the need for manual reward engineering and fault labels. Our framework employs Adversarial Inverse Reinforcement Learning to train a discriminator that distinguishes between normal (expert) and policy-generated transitions. The discriminator's learned reward serves as an anomaly score, indicating deviations from normal operating behaviour. When evaluated on three run-to-failure benchmark datasets (HUMS2023, IMS, and XJTU-SY), the model consistently assigns low anomaly scores to normal samples and high scores to faulty ones, enabling early and robust fault detection. By aligning RL's sequential reasoning with MFD's temporal structure, this work opens a path toward RL-based diagnostics in data-driven industrial settings.

</details>


### [30] [Disentangling Shared and Target-Enriched Topics via Background-Contrastive Non-negative Matrix Factorization](https://arxiv.org/abs/2602.22387)
*Yixuan Li,Archer Y. Yang,Yue Li*

Main category: cs.LG

TL;DR: 背景对比非负矩阵分解（BC-NMF）方法，通过联合分解目标数据集和匹配背景数据，抑制背景表达结构，提取目标富集的潜在主题，解决高维数据中生物信号被共享变异掩盖的问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据中感兴趣的生物信号常常被跨条件共享的主导变异所掩盖，这些变异来自基线生物结构或技术效应，阻碍标准降维方法解析条件特异性结构。现有背景校正方法要么无法扩展到高维度，要么缺乏可解释性。

Method: 提出背景对比非负矩阵分解（BC-NMF），通过联合分解目标数据集和匹配背景数据，使用共享非负基，在对比目标下抑制背景表达结构。该方法采用高效的乘法更新算法，支持GPU加速和小批量训练，可扩展到大数据。

Result: 在模拟和多种生物数据集中，BC-NMF揭示了传统方法无法检测的信号，包括抑郁症死后脑单细胞RNA-seq中的疾病相关程序、小鼠中基因型相关的蛋白表达模式、白血病中治疗特异性转录变化，以及癌症细胞系中TP53依赖的药物反应。

Conclusion: BC-NMF是一种可扩展且可解释的方法，能够有效分离目标特异性变异，揭示被传统方法掩盖的生物信号，在多种生物应用场景中表现出优越性能。

Abstract: Biological signals of interest in high-dimensional data are often masked by dominant variation shared across conditions. This variation, arising from baseline biological structure or technical effects, can prevent standard dimensionality reduction methods from resolving condition-specific structure. The challenge is that these confounding topics are often unknown and mixed with biological signals. Existing background correction methods are either unscalable to high dimensions or not interpretable. We introduce background contrastive Non-negative Matrix Factorization (\model), which extracts target-enriched latent topics by jointly factorizing a target dataset and a matched background using shared non-negative bases under a contrastive objective that suppresses background-expressed structure. This approach yields non-negative components that are directly interpretable at the feature level, and explicitly isolates target-specific variation. \model is learned by an efficient multiplicative update algorithm via matrix multiplication such that it is highly efficient on GPU hardware and scalable to big data via minibatch training akin to deep learning approach. Across simulations and diverse biological datasets, \model reveals signals obscured by conventional methods, including disease-associated programs in postmortem depressive brain single-cell RNA-seq, genotype-linked protein expression patterns in mice, treatment-specific transcriptional changes in leukemia, and TP53-dependent drug responses in cancer cell lines.

</details>


### [31] [Predicting Multi-Drug Resistance in Bacterial Isolates Through Performance Comparison and LIME-based Interpretation of Classification Models](https://arxiv.org/abs/2602.22400)
*Santanam Wishal,Riad Sahara*

Main category: cs.LG

TL;DR: 提出可解释机器学习框架预测细菌多重耐药性，使用临床特征和抗生素敏感性模式，评估五种分类模型，XGBoost和LightGBM表现最佳，结合LIME提供局部解释，识别关键抗生素家族贡献因素。


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性特别是多重耐药性的上升对临床决策构成严峻挑战，传统药敏试验耗时且治疗选择有限，需要开发能够早期识别MDR并提供可解释预测的机器学习方法。

Method: 提出可解释机器学习框架，评估五种分类模型（逻辑回归、随机森林、AdaBoost、XGBoost、LightGBM），使用9,714个分离株的临床数据集，在抗生素家族水平编码耐药性以捕捉交叉耐药模式，应用LIME进行局部可解释性分析。

Result: 集成模型特别是XGBoost和LightGBM在所有评估指标（准确率、F1分数、AUC-ROC、马修斯相关系数）上表现最佳。LIME分析识别出喹诺酮类、复方新诺明、多粘菌素、氨基糖苷类和呋喃类耐药是MDR预测的最强贡献因素，与已知生物学机制一致。

Conclusion: 结合高性能模型与局部可解释性方法，既能提供准确的MDR预测，又能产生可操作的临床见解，支持早期MDR识别并增强机器学习辅助临床决策的信任度，有助于抗菌药物管理。

Abstract: The rise of Antimicrobial Resistance, particularly Multi-Drug Resistance (MDR), presents a critical challenge for clinical decision-making due to limited treatment options and delays in conventional susceptibility testing. This study proposes an interpretable machine learning framework to predict MDR in bacterial isolates using clinical features and antibiotic susceptibility patterns. Five classification models were evaluated, including Logistic Regression, Random Forest, AdaBoost, XGBoost, and LightGBM. The models were trained on a curated dataset of 9,714 isolates, with resistance encoded at the antibiotic family level to capture cross-class resistance patterns consistent with MDR definitions. Performance assessment included accuracy, F1-score, AUC-ROC, and Matthews Correlation Coefficient. Ensemble models, particularly XGBoost and LightGBM, demonstrated superior predictive capability across all metrics. To address the clinical transparency gap, Local Interpretable Model-agnostic Explanations (LIME) was applied to generate instance-level explanations. LIME identified resistance to quinolones, Co-trimoxazole, Colistin, aminoglycosides, and Furanes as the strongest contributors to MDR predictions, aligning with known biological mechanisms. The results show that combining high-performing models with local interpretability provides both accuracy and actionable insights for antimicrobial stewardship. This framework supports earlier MDR identification and enhances trust in machine learning-assisted clinical decision support.

</details>


### [32] [MolFM-Lite: Multi-Modal Molecular Property Prediction with Conformer Ensemble Attention and Cross-Modal Fusion](https://arxiv.org/abs/2602.22405)
*Syed Omer Shah,Mohammed Maqsood Ahmed,Danish Mohiuddin Mohammed,Shahnawaz Alam,Mohd Vahaj ur Rahman*

Main category: cs.LG

TL;DR: MolFM-Lite是一个多模态分子性质预测模型，通过交叉注意力融合SELFIES序列(1D)、分子图(2D)和构象集合(3D)表示，并使用FiLM条件化实验上下文，在多个基准测试中显著优于单模态基线。


<details>
  <summary>Details</summary>
Motivation: 现有分子性质预测模型通常依赖单一分子表示（序列、图或3D结构）并将分子几何视为静态，无法充分利用多模态信息和分子构象的动态分布。

Method: 1) 构象集合注意力机制：结合可学习注意力与Boltzmann加权先验，捕捉多个RDKit生成构象的热力学分布；2) 交叉模态融合层：各模态可相互关注，实现互补信息共享；3) 使用FiLM条件化实验上下文；4) 在ZINC250K上进行跨模态对比和掩码原子预训练。

Result: 在四个MoleculeNet支架分割基准测试中，三模态融合相比单模态基线提供7-11% AUC提升，构象集合相比单构象变体增加约2%改进。消融研究确认所有架构组件独立贡献。

Conclusion: MolFM-Lite通过多模态融合和构象集合建模有效提升分子性质预测性能，证明了整合1D、2D、3D表示和考虑分子构象动态分布的重要性。

Abstract: Most machine learning models for molecular property prediction rely on a single molecular representation (either a sequence, a graph, or a 3D structure) and treat molecular geometry as static. We present MolFM-Lite, a multi-modal model that jointly encodes SELFIES sequences (1D), molecular graphs (2D), and conformer ensembles (3D) through cross-attention fusion, while conditioning predictions on experimental context via Feature-wise Linear Modulation (FiLM). Our main methodological contributions are: (1) a conformer ensemble attention mechanism that combines learnable attention with Boltzmann-weighted priors over multiple RDKit-generated conformers, capturing the thermodynamic distribution of molecular shapes; and (2) a cross-modal fusion layer where each modality can attend to others, enabling complementary information sharing. We evaluate on four MoleculeNet scaffold-split benchmarks using our model's own splits, and report all baselines re-evaluated under the same protocol. Comprehensive ablation studies across all four datasets confirm that each architectural component contributes independently, with tri-modal fusion providing 7-11% AUC improvement over single-modality baselines and conformer ensembles adding approximately 2% over single-conformer variants. Pre-training on ZINC250K (~250K molecules) using cross-modal contrastive and masked-atom objectives enables effective weight initialization at modest compute cost. We release all code, trained models, and data splits to support reproducibility.

</details>


### [33] [Reinforcement-aware Knowledge Distillation for LLM Reasoning](https://arxiv.org/abs/2602.22495)
*Zhaoyang Zhang,Shuli Jiang,Yantao Shen,Yuting Zhang,Dhananjay Ram,Shuo Yang,Zhuowen Tu,Wei Xia,Stefano Soatto*

Main category: cs.LG

TL;DR: 提出RLAD方法解决强化学习后训练中知识蒸馏的分布不匹配和目标干扰问题，通过选择性模仿和信任区域比率蒸馏实现更有效的学生模型训练


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要针对监督微调设计，当与强化学习结合时存在分布不匹配和目标干扰问题：教师监督可能与学生演化中的rollout分布不一致，KL正则化器可能与奖励最大化冲突且需要精细的损失平衡

Method: 提出RL-aware distillation (RLAD)方法，在强化学习过程中进行选择性模仿，核心组件是Trust Region Ratio Distillation (TRRD)，用PPO/GRPO风格的可能性比率目标替代教师-学生KL正则化器，基于教师-旧策略混合进行锚定，实现优势感知、信任区域有界的蒸馏

Result: 在多样化的逻辑推理和数学基准测试中，RLAD持续优于离线蒸馏、标准GRPO和基于KL的在线教师-学生知识蒸馏方法

Conclusion: RLAD通过选择性模仿和信任区域比率蒸馏有效解决了强化学习后训练中知识蒸馏的分布不匹配和目标干扰问题，为将大型语言模型的推理能力蒸馏到更小模型提供了更有效的方法

Abstract: Reinforcement learning (RL) post-training has recently driven major gains in long chain-of-thought reasoning large language models (LLMs), but the high inference cost of such models motivates distillation into smaller students. Most existing knowledge distillation (KD) methods are designed for supervised fine-tuning (SFT), relying on fixed teacher traces or teacher-student Kullback-Leibler (KL) divergence-based regularization. When combined with RL, these approaches often suffer from distribution mismatch and objective interference: teacher supervision may not align with the student's evolving rollout distribution, and the KL regularizer can compete with reward maximization and require careful loss balancing. To address these issues, we propose RL-aware distillation (RLAD), which performs selective imitation during RL -- guiding the student toward the teacher only when it improves the current policy update. Our core component, Trust Region Ratio Distillation (TRRD), replaces the teacher-student KL regularizer with a PPO/GRPO-style likelihood-ratio objective anchored to a teacher--old-policy mixture, yielding advantage-aware, trust-region-bounded distillation on student rollouts and naturally balancing exploration, exploitation, and imitation. Across diverse logic reasoning and math benchmarks, RLAD consistently outperforms offline distillation, standard GRPO, and KL-based on-policy teacher-student knowledge distillation.

</details>


### [34] [TEFL: Prediction-Residual-Guided Rolling Forecasting for Multi-Horizon Time Series](https://arxiv.org/abs/2602.22520)
*Xiannan Huang,Shen Fang,Shuhan Qiu,Chengcheng Yu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: TEFL是一个通过历史预测残差反馈来提升时间序列预测性能的统一学习框架，在多种数据集和骨干架构上平均降低MAE 5-10%


<details>
  <summary>Details</summary>
Motivation: 现有深度预测模型通常只最小化逐点预测损失，忽略了滚动预测中历史残差所包含的丰富信息，这些残差反映了持续偏差、未建模模式或演化动态

Method: 提出TEFL框架，在训练和评估中显式纳入历史残差。解决三个关键挑战：1)在滚动预测部分可观测性下选择可观测的多步残差；2)通过轻量级低秩适配器集成残差以保持效率防止过拟合；3)设计两阶段训练程序联合优化基础预测器和误差模块

Result: 在10个真实世界数据集和5种骨干架构上的广泛实验表明，TEFL持续提升准确性，平均降低MAE 5-10%。在突变和分布偏移下表现出强鲁棒性，在挑战性场景中误差降低超过10%（最高达19.5%）

Conclusion: 通过将基于残差的反馈直接嵌入学习过程，TEFL为现代深度预测系统提供了一个简单、通用且有效的增强方法

Abstract: Time series forecasting plays a critical role in domains such as transportation, energy, and meteorology. Despite their success, modern deep forecasting models are typically trained to minimize point-wise prediction loss without leveraging the rich information contained in past prediction residuals from rolling forecasts - residuals that reflect persistent biases, unmodeled patterns, or evolving dynamics. We propose TEFL (Temporal Error Feedback Learning), a unified learning framework that explicitly incorporates these historical residuals into the forecasting pipeline during both training and evaluation. To make this practical in deep multi-step settings, we address three key challenges: (1) selecting observable multi-step residuals under the partial observability of rolling forecasts, (2) integrating them through a lightweight low-rank adapter to preserve efficiency and prevent overfitting, and (3) designing a two-stage training procedure that jointly optimizes the base forecaster and error module. Extensive experiments across 10 real-world datasets and 5 backbone architectures show that TEFL consistently improves accuracy, reducing MAE by 5-10% on average. Moreover, it demonstrates strong robustness under abrupt changes and distribution shifts, with error reductions exceeding 10% (up to 19.5%) in challenging scenarios. By embedding residual-based feedback directly into the learning process, TEFL offers a simple, general, and effective enhancement to modern deep forecasting systems.

</details>


### [35] [Predicting Tennis Serve directions with Machine Learning](https://arxiv.org/abs/2602.22527)
*Ying Zhu,Ruthuparna Naikar*

Main category: cs.LG

TL;DR: 开发了一种机器学习方法来预测职业网球选手的一发方向，男性选手平均预测准确率约49%，女性约44%，分析显示顶级选手采用混合策略且疲劳可能影响发球方向选择


<details>
  <summary>Details</summary>
Motivation: 职业网球中发球（尤其是一发）非常重要，发球者需要战略性地选择发球方向以最大化获胜机会并保持不可预测性，而接发球者则试图预测发球方向以做出良好回击。这种心理博弈是职业网球比赛中决策的重要组成部分，需要理解选手的发球决策模式。

Method: 通过特征工程开发了一种机器学习方法来预测职业网球选手的一发方向。该方法利用各种特征来分析选手的发球决策模式。

Result: 该方法对男性选手的平均预测准确率达到约49%，对女性选手达到约44%。分析提供了证据表明顶级职业选手在发球决策中使用混合策略模型，疲劳可能是选择发球方向的一个因素。此外，分析还表明情境信息对接发球者的预期反应可能比先前认为的更重要。

Conclusion: 开发的机器学习方法能够有效预测职业网球选手的发球方向，揭示了顶级选手采用混合策略、疲劳影响发球决策等重要发现，同时强调了情境信息在接发球预期中的重要性。

Abstract: Serves, especially first serves, are very important in professional tennis. Servers choose their serve directions strategically to maximize their winning chances while trying to be unpredictable. On the other hand, returners try to predict serve directions to make good returns. The mind game between servers and returners is an important part of decision-making in professional tennis matches. To help understand the players' serve decisions, we have developed a machine learning method for predicting professional tennis players' first serve directions. Through feature engineering, our method achieves an average prediction accuracy of around 49\% for male players and 44\% for female players. Our analysis provides some evidence that top professional players use a mixed-strategy model in serving decisions and that fatigue might be a factor in choosing serve directions. Our analysis also suggests that contextual information is perhaps more important for returners' anticipatory reactions than previously thought.

</details>


### [36] [Persistent Nonnegative Matrix Factorization via Multi-Scale Graph Regularization](https://arxiv.org/abs/2602.22536)
*Jichao Zhang,Ran Miao,Limin Li*

Main category: cs.LG

TL;DR: 提出了一种基于持久同调的持久非负矩阵分解(pNMF)方法，通过尺度参数化产生序列化嵌入而非单一分解，能捕捉跨分辨率的结构演化。


<details>
  <summary>Details</summary>
Motivation: 现有NMF方法是单尺度的，无法捕捉连接结构在不同分辨率下的演化过程，需要一种能处理多尺度数据表示的方法。

Method: 提出持久非负矩阵分解(pNMF)，利用持久同调识别底层连接发生质变的规范最小充分尺度集，构建尺度参数化的图拉普拉斯序列，采用耦合NMF公式化，包含尺度几何正则化和跨尺度一致性约束。

Result: 建立了沿尺度参数的嵌入结构性质分析，证明了连续尺度间增量的边界，开发了保证收敛的顺序交替优化算法，在合成和单细胞RNA测序数据集上验证了多尺度低秩嵌入的有效性。

Conclusion: pNMF定义了跨尺度的非平凡解路径而非单一分解，解决了多尺度数据表示的计算挑战，为多尺度低秩嵌入提供了有效框架。

Abstract: Matrix factorization techniques, especially Nonnegative Matrix Factorization (NMF), have been widely used for dimensionality reduction and interpretable data representation. However, existing NMF-based methods are inherently single-scale and fail to capture the evolution of connectivity structures across resolutions. In this work, we propose persistent nonnegative matrix factorization (pNMF), a scale-parameterized family of NMF problems, that produces a sequence of persistence-aligned embeddings rather than a single one. By leveraging persistent homology, we identify a canonical minimal sufficient scale set at which the underlying connectivity undergoes qualitative changes. These canonical scales induce a sequence of graph Laplacians, leading to a coupled NMF formulation with scale-wise geometric regularization and explicit cross-scale consistency constraint. We analyze the structural properties of the embeddings along the scale parameter and establish bounds on their increments between consecutive scales. The resulting model defines a nontrivial solution path across scales, rather than a single factorization, which poses new computational challenges. We develop a sequential alternating optimization algorithm with guaranteed convergence. Numerical experiments on synthetic and single-cell RNA sequencing datasets demonstrate the effectiveness of the proposed approach in multi-scale low-rank embeddings.

</details>


### [37] [LUMOS: Democratizing SciML Workflows with L0-Regularized Learning for Unified Feature and Parameter Adaptation](https://arxiv.org/abs/2602.22537)
*Shouwei Gao,Xu Zheng,Dongsheng Luo,Sheng Di,Wenqian Dong*

Main category: cs.LG

TL;DR: LUMOS是一个基于L0正则化学习的端到端框架，通过统一特征选择和模型剪枝来简化科学机器学习模型设计，在13个SciML任务中平均减少71.45%参数并实现6.4倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习模型设计需要大量先验知识和人工专业知识，特别是在确定输入特征和模型规模方面，这限制了SciML的普及和应用。

Method: 提出LUMOS框架，基于L0正则化学习，采用半随机门控和重参数化技术，在训练过程中动态选择信息特征并剪枝冗余参数。

Result: 在13个SciML任务（包括宇宙学和分子科学）上评估，平均实现71.45%参数减少和6.4倍推理加速，分布式数据并行训练在8个GPU上验证了可扩展性。

Conclusion: LUMOS通过自动化特征选择和模型剪枝，减少了对人工调优的依赖，同时保持预测准确性，有助于民主化SciML模型设计。

Abstract: The rapid growth of scientific machine learning (SciML) has accelerated discovery across diverse domains, yet designing effective SciML models remains a challenging task. In practice, building such models often requires substantial prior knowledge and manual expertise, particularly in determining which input features to use and how large the model should be. We introduce LUMOS, an end-to-end framework based on L0-regularized learning that unifies feature selection and model pruning to democratize SciML model design. By employing semi-stochastic gating and reparameterization techniques, LUMOS dynamically selects informative features and prunes redundant parameters during training, reducing the reliance on manual tuning while maintaining predictive accuracy. We evaluate LUMOS across 13 diverse SciML workloads, including cosmology and molecular sciences, and demonstrate its effectiveness and generalizability. Experiments on 13 SciML models show that LUMOS achieves 71.45% parameter reduction and a 6.4x inference speedup on average. Furthermore, Distributed Data Parallel (DDP) training on up to eight GPUs confirms the scalability of

</details>


### [38] [RAIN-Merging: A Gradient-Free Method to Enhance Instruction Following in Large Reasoning Models with Preserved Thinking Format](https://arxiv.org/abs/2602.22538)
*Zhehao Huang,Yuhang Liu,Baijiong Lin,Yixin Lou,Zhengbao He,Hanling Tian,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: RAIN-Merging是一种无需梯度的方法，通过将指令调优模型的任务向量投影到推理模型思考标记的前向特征零空间，并利用指令注意力进行模块特定缩放，从而在保持推理能力的同时显著提升指令遵循性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）擅长长链推理，但在遵循输出格式、约束或特定要求等指令方面表现不佳。研究探索是否可以通过将指令调优模型（ITM）集成到LRM中来弥补这一差距。

Method: 提出RAIN-Merging方法：1）使用小型推理校准集，将ITM任务向量投影到思考特殊标记前向特征的零空间，以保持LRM的结构化推理机制；2）使用小型指令校准集，估计指令注意力以推导模块特定缩放，放大指令相关组件并抑制泄漏。

Result: 在四个指令遵循基准测试和九个推理与通用能力基准测试中，RAIN-Merging显著提高了指令遵循能力，同时保持了推理质量。这种改进在不同模型规模和架构中保持一致，并在智能体设置中转化为性能提升。

Conclusion: 通过分析LRM和ITM在参数空间中的差异，发现它们的主子空间在关键模块中几乎正交，这为轻量级合并提供了可能。RAIN-Merging成功解决了朴素合并的脆弱性问题，实现了推理能力与指令遵循能力的有效融合。

Abstract: Large reasoning models (LRMs) excel at a long chain of reasoning but often fail to faithfully follow instructions regarding output format, constraints, or specific requirements. We investigate whether this gap can be closed by integrating an instruction-tuned model (ITM) into an LRM. Analyzing their differences in parameter space, namely task vectors, we find that their principal subspaces are nearly orthogonal across key modules, suggesting a lightweight merging with minimal interference. However, we also demonstrate that naive merges are fragile because they overlook the output format mismatch between LRMs (with explicit thinking and response segments) and ITMs (answers-only). We introduce RAIN-Merging (Reasoning-Aware Instruction-attention guided Null-space projection Merging), a gradient-free method that integrates instruction following while preserving thinking format and reasoning performance. First, with a small reasoning calibration set, we project the ITM task vector onto the null space of forward features at thinking special tokens, which preserves the LRM's structured reasoning mechanisms. Second, using a small instruction calibration set, we estimate instruction attention to derive module-specific scaling that amplifies instruction-relevant components and suppresses leakage. Across four instruction-following benchmarks and nine reasoning & general capability benchmarks, RAIN-Merging substantially improves instruction adherence while maintaining reasoning quality. The gains are consistent across model scales and architectures, translating to improved performance in agent settings.

</details>


### [39] [Relatron: Automating Relational Machine Learning over Relational Databases](https://arxiv.org/abs/2602.22552)
*Zhikai Chen,Han Xie,Jian Zhang,Jiliang Tang,Xiang Song,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 该研究系统比较了关系深度学习(RDL)与深度特征合成(DFS)在关系数据库预测建模中的表现，发现RDL并不总是优于DFS，性能高度依赖任务特性。研究提出了基于任务嵌入的元选择器Relatron，能有效选择RDL或DFS并优化架构搜索。


<details>
  <summary>Details</summary>
Motivation: 关系数据库预测建模面临两大挑战：捕捉跨表依赖关系和复杂特征交互。虽然关系深度学习(RDL)通过消息传递自动特征工程，而传统方法如深度特征合成(DFS)依赖预定义聚合器，但RDL相对于DFS的优势以及有效架构选择原则仍不明确。

Method: 研究将RDL和DFS统一到共享设计空间，进行架构中心搜索。通过分析构建模型性能库，识别影响RDL-DFS性能差距的两个任务信号：关系数据库任务同质性和捕捉大小、路径、特征及时间结构的亲和嵌入。基于此提出Relatron元选择器，结合轻量级损失景观度量避免脆弱检查点。

Result: 研究发现：(1)RDL并不总是优于DFS，性能高度任务依赖；(2)没有单一架构在所有任务中占优；(3)验证准确率不是可靠的架构选择指南。Relatron解决了"更多调参，更差性能"效应，在联合超参数-架构优化中比强基线提升18.5%，成本比基于Fisher信息的替代方案低10倍。

Conclusion: 关系数据库预测建模需要任务感知的模型选择。Relatron通过任务嵌入信号实现RDL与DFS的智能选择，结合损失景观平坦度优化，为关系数据库机器学习提供了高效、可解释的架构选择框架。

Abstract: Predictive modeling over relational databases (RDBs) powers applications, yet remains challenging due to capturing both cross-table dependencies and complex feature interactions. Relational Deep Learning (RDL) methods automate feature engineering via message passing, while classical approaches like Deep Feature Synthesis (DFS) rely on predefined non-parametric aggregators. Despite performance gains, the comparative advantages of RDL over DFS and the design principles for selecting effective architectures remain poorly understood. We present a comprehensive study that unifies RDL and DFS in a shared design space and conducts architecture-centric searches across diverse RDB tasks. Our analysis yields three key findings: (1) RDL does not consistently outperform DFS, with performance being highly task-dependent; (2) no single architecture dominates across tasks, underscoring the need for task-aware model selection; and (3) validation accuracy is an unreliable guide for architecture choice. This search yields a model performance bank that links architecture configurations to their performance; leveraging this bank, we analyze the drivers of the RDL-DFS performance gap and introduce two task signals -- RDB task homophily and an affinity embedding that captures size, path, feature, and temporal structure -- whose correlation with the gap enables principled routing. Guided by these signals, we propose Relatron, a task embedding-based meta-selector that chooses between RDL and DFS and prunes the within-family search. Lightweight loss-landscape metrics further guard against brittle checkpoints by preferring flatter optima. In experiments, Relatron resolves the "more tuning, worse performance" effect and, in joint hyperparameter-architecture optimization, achieves up to 18.5% improvement over strong baselines with 10x lower cost than Fisher information-based alternatives.

</details>


### [40] [Multilingual Safety Alignment Via Sparse Weight Editing](https://arxiv.org/abs/2602.22554)
*Jiaming Liang,Zhaoxin Wang,Handing Wang*

Main category: cs.LG

TL;DR: 提出了一种基于稀疏权重编辑的无训练对齐框架，通过将低资源语言的有害表示映射到高资源语言的安全子空间，解决LLM跨语言安全差异问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不同语言间存在显著的安全差异，低资源语言往往能绕过为高资源语言（如英语）建立的安全防护。现有解决方案（如多语言监督微调或RLHF）计算成本高且依赖稀缺的多语言安全数据。

Method: 基于稀疏权重编辑的无训练对齐框架。识别出安全能力集中在稀疏的安全神经元中，将跨语言对齐问题表述为约束线性变换。推导出闭式解，将低资源语言的有害表示最优映射到高资源语言的鲁棒安全子空间，同时通过零空间投影约束保留一般效用。

Result: 在8种语言和多个模型家族（Llama-3、Qwen-2.5）上的广泛实验表明，该方法显著降低了低资源语言的攻击成功率，对一般推理能力影响可忽略，且只需一次数据高效的计算。

Conclusion: 提出了一种计算高效、无需训练的方法来解决LLM跨语言安全对齐问题，通过稀疏权重编辑实现低资源语言到高资源语言安全子空间的映射，在保持模型效用的同时显著提升低资源语言的安全性。

Abstract: Large Language Models (LLMs) exhibit significant safety disparities across languages, with low-resource languages (LRLs) often bypassing safety guardrails established for high-resource languages (HRLs) like English. Existing solutions, such as multilingual supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), are computationally expensive and dependent on scarce multilingual safety data. In this work, we propose a novel, training-free alignment framework based on Sparse Weight Editing. Identifying that safety capabilities are localized within a sparse set of safety neurons, we formulate the cross-lingual alignment problem as a constrained linear transformation. We derive a closed-form solution to optimally map the harmful representations of LRLs to the robust safety subspaces of HRLs, while preserving general utility via a null-space projection constraint. Extensive experiments across 8 languages and multiple model families (Llama-3, Qwen-2.5) demonstrate that our method substantially reduces Attack Success Rate (ASR) in LRLs with negligible impact on general reasoning capabilities, all achieved with a single, data-efficient calculation.

</details>


### [41] [Autoregressive Visual Decoding from EEG Signals](https://arxiv.org/abs/2602.22555)
*Sicheng Dai,Hongwang Xiao,Shan Yu,Qiwei Ye*

Main category: cs.LG

TL;DR: AVDE是一个轻量级高效的EEG视觉解码框架，通过对比学习对齐EEG与图像表征，采用自回归生成框架进行多尺度图像重建，仅用10%参数即超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前EEG视觉解码方法面临模态鸿沟、多阶段复杂适配、计算开销大等问题，限制了其在真实BCI应用中的实用性。

Method: 1) 使用预训练EEG模型LaBraM，通过对比学习微调以对齐EEG与图像表征；2) 采用基于"下一尺度预测"策略的自回归生成框架，将图像编码为多尺度token图，用transformer从EEG嵌入开始自回归预测更精细尺度token。

Result: 在两个数据集上，AVDE在图像检索和重建任务中均优于现有方法，仅使用10%参数；中间输出可视化显示其生成过程反映了人类视觉感知的层次性。

Conclusion: AVDE展示了自回归模型作为高效可解释工具在实用BCI应用中的潜力，实现了轻量级、高效的EEG视觉解码。

Abstract: Electroencephalogram (EEG) signals have become a popular medium for decoding visual information due to their cost-effectiveness and high temporal resolution. However, current approaches face significant challenges in bridging the modality gap between EEG and image data. These methods typically rely on complex adaptation processes involving multiple stages, making it hard to maintain consistency and manage compounding errors. Furthermore, the computational overhead imposed by large-scale diffusion models limit their practicality in real-world brain-computer interface (BCI) applications. In this work, we present AVDE, a lightweight and efficient framework for visual decoding from EEG signals. First, we leverage LaBraM, a pre-trained EEG model, and fine-tune it via contrastive learning to align EEG and image representations. Second, we adopt an autoregressive generative framework based on a "next-scale prediction" strategy: images are encoded into multi-scale token maps using a pre-trained VQ-VAE, and a transformer is trained to autoregressively predict finer-scale tokens starting from EEG embeddings as the coarsest representation. This design enables coherent generation while preserving a direct connection between the input EEG signals and the reconstructed images. Experiments on two datasets show that AVDE outperforms previous state-of-the-art methods in both image retrieval and reconstruction tasks, while using only 10% of the parameters. In addition, visualization of intermediate outputs shows that the generative process of AVDE reflects the hierarchical nature of human visual perception. These results highlight the potential of autoregressive models as efficient and interpretable tools for practical BCI applications.

</details>


### [42] [Stable Adaptive Thinking via Advantage Shaping and Length-Aware Gradient Regulation](https://arxiv.org/abs/2602.22556)
*Zihang Xu,Haozhi Xie,Ziqi Miao,Wuxuan Gong,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: 本文提出一个两阶段框架解决大推理模型在简单问题上过度思考的问题，通过混合微调和自适应强化学习，在提升准确率的同时显著减少生成token数量。


<details>
  <summary>Details</summary>
Motivation: 大推理模型在处理低复杂度查询时经常表现出过度思考行为，现有方法在准确率-效率权衡上不稳定，且对异构推理行为的鲁棒性较差。

Method: 提出两阶段框架：1) 混合微调让模型同时接触思考和不思考行为，建立良好初始化；2) 自适应强化学习，使用正确性保持优势塑形避免抑制正确长链推理，以及长度感知梯度调节来稳定优化。

Result: 在Qwen2.5-1.5B和7B模型上实验显示，相比强基线准确率提升最高+3.7/+3.6个百分点，同时生成token减少40.6%/43.9%。在不同难度问题和分布外任务上验证了方法的鲁棒性和泛化能力。

Conclusion: 该两阶段框架有效解决了大推理模型的过度思考问题，实现了稳定的准确率-效率权衡，并在异构推理场景下表现出良好的鲁棒性和泛化性。

Abstract: Large reasoning models (LRMs) achieve strong performance through extended reasoning traces, but they often exhibit overthinking behavior for low-complexity queries. Existing efforts to mitigate this issue are fundamentally limited by unstable accuracy-efficiency trade-offs and poor robustness to heterogeneous reasoning behaviors. To address these challenges, we propose a two-stage framework for stable adaptive thinking in LRMs. The framework first applies Hybrid Fine-Tuning to expose the model to both thinking and no-thinking behaviors, establishing well-conditioned initialization. It then performs adaptive reinforcement learning with Correctness-Preserving Advantage Shaping (CPAS) to avoid suppressing correct long-chain reasoning, and Length-Aware Gradient Regulation (LAGR) to stabilize optimization under severe reasoning-length heterogeneity. Extensive experiments on Qwen2.5-1.5B and 7B show consistent improvements over strong baselines, achieving up to +3.7/+3.6 accuracy points while reducing generated tokens by 40.6%/43.9%. Further analyses across varying problem difficulties and out-of-distribution tasks confirm the robustness and generalization of our approach.

</details>


### [43] [Operationalizing Fairness: Post-Hoc Threshold Optimization Under Hard Resource Limits](https://arxiv.org/abs/2602.22560)
*Moirangthem Tiken Singh,Amit Kalita,Sapam Jitu Singh*

Main category: cs.LG

TL;DR: 提出一个后处理、模型无关的阈值优化框架，在严格容量约束下联合平衡安全性、效率和公平性，使用单一全局决策阈值确保法律合规性。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署机器学习时，需要在预测安全性和算法公平性之间取得平衡。现有公平性干预措施通常假设无约束资源，并使用违反反歧视法规的群体特定决策阈值。

Method: 引入后处理、模型无关的阈值优化框架，通过参数化伦理损失函数和有界决策规则，在严格容量约束下联合平衡安全性、效率和公平性，强制使用单一全局决策阈值以确保法律合规。

Result: 容量约束主导伦理优先级，在超过80%的测试配置中，严格资源限制决定了最终部署的阈值。在25%的严格容量限制下，该框架成功保持高风险识别（召回率0.409-0.702），而标准无约束公平启发式方法则崩溃到接近零效用。

Conclusion: 理论公平目标必须明确服从于运营容量限制才能实际部署。通过将预测评分与政策评估解耦并严格限制干预率，该框架为利益相关者在资源受限环境中导航不可避免的伦理权衡提供了实用且法律合规的机制。

Abstract: The deployment of machine learning in high-stakes domains requires a balance between predictive safety and algorithmic fairness. However, existing fairness interventions often as- sume unconstrained resources and employ group-specific decision thresholds that violate anti- discrimination regulations. We introduce a post-hoc, model-agnostic threshold optimization framework that jointly balances safety, efficiency, and equity under strict and hard capacity constraints. To ensure legal compliance, the framework enforces a single, global decision thresh- old. We formulated a parameterized ethical loss function coupled with a bounded decision rule that mathematically prevents intervention volumes from exceeding the available resources. An- alytically, we prove the key properties of the deployed threshold, including local monotonicity with respect to ethical weighting and the formal identification of critical capacity regimes. We conducted extensive experimental evaluations on diverse high-stakes datasets. The principal re- sults demonstrate that capacity constraints dominate ethical priorities; the strict resource limit determines the final deployed threshold in over 80% of the tested configurations. Furthermore, under a restrictive 25% capacity limit, the proposed framework successfully maintains high risk identification (recall ranging from 0.409 to 0.702), whereas standard unconstrained fairness heuristics collapse to a near-zero utility. We conclude that theoretical fairness objectives must be explicitly subordinated to operational capacity limits to remain in deployment. By decou- pling predictive scoring from policy evaluation and strictly bounding intervention rates, this framework provides a practical and legally compliant mechanism for stakeholders to navigate unavoidable ethical trade-offs in resource-constrained environments.

</details>


### [44] [S2O: Early Stopping for Sparse Attention via Online Permutation](https://arxiv.org/abs/2602.22575)
*Yu Zhang,Songwei Liu,Chenqian Yan,Sheng Lin,Beichen Ning,Fangmin Chen,Xing Wang*

Main category: cs.LG

TL;DR: S2O通过在线排列和早期停止机制提升稀疏注意力效率，突破现有块粒度稀疏化的稀疏度上限，显著减少长上下文推理的计算开销。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的计算复杂度随序列长度呈二次方增长，限制了长上下文推理。现有的块粒度稀疏化方法存在固有的稀疏度上限，即使经过精心设计也难以进一步提升性能。

Method: S2O采用在线排列和早期停止机制：1) 借鉴内存系统中的虚拟到物理地址映射，重新分解FlashAttention执行，使推理能够加载非连续token；2) 基于注意力热图的细粒度结构，将显式排列转化为在线、索引引导的离散加载策略；3) 引入早期停止规则，按重要性从高到低计算，当块分数低于阈值时提前终止。

Result: 在Llama-3.1-8B的128K上下文上：1) 在相同稀疏度下将单算子MSE降低3.82倍；2) 在相同MSE下将预填充计算密度降低3.31倍；3) 保持端到端精度，实现7.51倍注意力加速和3.81倍端到端加速。

Conclusion: S2O通过在线排列和早期停止机制显著提高了稀疏注意力的实际稀疏度上限，有效减少了长上下文推理的计算开销，同时保持了模型精度。

Abstract: Attention scales quadratically with sequence length, fundamentally limiting long-context inference. Existing block-granularity sparsification can reduce latency, but coarse blocks impose an intrinsic sparsity ceiling, making further improvements difficult even with carefully engineered designs. We present S2O, which performs early stopping for sparse attention via online permutation. Inspired by virtual-to-physical address mapping in memory systems, S2O revisits and factorizes FlashAttention execution, enabling inference to load non-contiguous tokens rather than a contiguous span in the original order. Motivated by fine-grained structures in attention heatmaps, we transform explicit permutation into an online, index-guided, discrete loading policy; with extremely lightweight preprocessing and index-remapping overhead, it concentrates importance on a small set of high-priority blocks. Building on this importance-guided online permutation for loading, S2O further introduces an early-stopping rule: computation proceeds from high to low importance; once the current block score falls below a threshold, S2O terminates early and skips the remaining low-contribution blocks, thereby increasing effective sparsity and reducing computation under a controlled error budget.
  As a result, S2O substantially raises the practical sparsity ceiling. On Llama-3.1-8B under a 128K context, S2O reduces single-operator MSE by 3.82$\times$ at matched sparsity, and reduces prefill compute density by 3.31$\times$ at matched MSE; meanwhile, it preserves end-to-end accuracy and achieves 7.51$\times$ attention and 3.81$\times$ end-to-end speedups.

</details>


### [45] [IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck](https://arxiv.org/abs/2602.22581)
*Tian Bian,Yifan Niu,Chaohao Yuan,Chengzhi Piao,Bingzhe Wu,Long-Kai Huang,Yu Rong,Tingyang Xu,Hong Cheng,Jia Li*

Main category: cs.LG

TL;DR: 提出IBCircuit方法，基于信息瓶颈原理进行端到端的整体电路发现，无需为不同任务设计特定的损坏激活，在IOI和Greater-Than任务中识别出更忠实、更精简的电路


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法大多忽视电路的整体性，且需要为不同任务设计特定的损坏激活，这种方法既不准确也不高效

Method: 基于信息瓶颈原理的端到端方法IBCircuit，这是一个用于整体电路发现的优化框架，可应用于任何给定任务而无需繁琐的损坏激活设计

Result: 在间接宾语识别(IOI)和大于关系(Greater-Than)任务中，IBCircuit在关键节点组件和边组件方面识别出比近期相关工作更忠实、更精简的电路

Conclusion: IBCircuit提供了一种无需任务特定损坏激活设计的整体电路发现方法，在多个任务中表现出更好的性能

Abstract: Circuit discovery has recently attracted attention as a potential research direction to explain the non-trivial behaviors of language models. It aims to find the computational subgraphs, also known as circuits, within the model that are responsible for solving specific tasks. However, most existing studies overlook the holistic nature of these circuits and require designing specific corrupted activations for different tasks, which is inaccurate and inefficient. In this work, we propose an end-to-end approach based on the principle of Information Bottleneck, called IBCircuit, to identify informative circuits holistically. IBCircuit is an optimization framework for holistic circuit discovery and can be applied to any given task without tediously corrupted activation design. In both the Indirect Object Identification (IOI) and Greater-Than tasks, IBCircuit identifies more faithful and minimal circuits in terms of critical node components and edge components compared to recent related work.

</details>


### [46] [TabDLM: Free-Form Tabular Data Generation via Joint Numerical-Language Diffusion](https://arxiv.org/abs/2602.22586)
*Donghong Cai,Jiarui Feng,Yanbo Wang,Da Zheng,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: TabDLM：基于掩码扩散语言模型的统一框架，用于生成包含自由文本字段的异构表格数据，通过联合数值-语言扩散模型解决现有方法在文本质量和数值精度上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据越来越多地包含自由文本字段（如评论或临床笔记）与结构化数值和分类属性并存。生成这种异构表格并联合建模不同模态具有挑战性。现有方法分为扩散模型和LLM方法两类：扩散模型能捕捉数值和分类特征的复杂依赖关系，但扩展到开放文本困难且文本质量下降；LLM方法能自然生成流畅文本，但其离散标记化会扭曲精确或宽范围数值，阻碍对数字和语言的准确建模。

Method: 提出TabDLM框架，基于掩码扩散语言模型构建联合数值-语言扩散模型。通过掩码扩散建模文本和分类特征，同时通过学习的专用数值标记嵌入用连续扩散过程建模数值特征；双向注意力在单个模型内捕捉跨模态交互。

Result: 在多样化基准测试上的广泛实验表明，TabDLM相比强大的扩散模型和LLM基线方法具有有效性。

Conclusion: TabDLM提供了一个统一的框架，能够有效生成包含自由文本字段的异构表格数据，克服了现有方法在文本质量和数值精度方面的局限性，实现了对表格数据中不同模态的联合建模。

Abstract: Synthetic tabular data generation has attracted growing attention due to its importance for data augmentation, foundation models, and privacy. However, real-world tabular datasets increasingly contain free-form text fields (e.g., reviews or clinical notes) alongside structured numerical and categorical attributes. Generating such heterogeneous tables with joint modeling of different modalities remains challenging. Existing approaches broadly fall into two categories: diffusion-based methods and LLM-based methods. Diffusion models can capture complex dependencies over numerical and categorical features in continuous or discrete spaces, but extending them to open-ended text is nontrivial and often leads to degraded text quality. In contrast, LLM-based generators naturally produce fluent text, yet their discrete tokenization can distort precise or wide-range numerical values, hindering accurate modeling of both numbers and language. In this work, we propose TabDLM, a unified framework for free-form tabular data generation via a joint numerical--language diffusion model built on masked diffusion language models (MDLMs). TabDLM models textual and categorical features through masked diffusion, while modeling numerical features with a continuous diffusion process through learned specialized numeric tokens embedding; bidirectional attention then captures cross-modality interactions within a single model. Extensive experiments on diverse benchmarks demonstrate the effectiveness of TabDLM compared to strong diffusion- and LLM-based baselines.

</details>


### [47] [pQuant: Towards Effective Low-Bit Language Models via Decoupled Linear Quantization-Aware Training](https://arxiv.org/abs/2602.22592)
*Wenzheng Zhang,Bingzheng Liu,Yang Hu,Xiaoying Bai,Wentao Zhang,Bin Cui*

Main category: cs.LG

TL;DR: pQuant提出了一种新的量化感知训练方法，通过将线性层拆分为1位主导分支和高精度分支来解决参数民主化效应，在极低位量化（sub 2-bit）中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有极低位量化方法在准确性和可扩展性方面仍不理想，主要瓶颈是参数民主化效应——所有参数的敏感性变得同质化，严重限制了模型的表达能力。

Method: pQuant将线性层拆分为两个专门分支：1位主导分支用于高效计算，紧凑的高精度分支专门保留最敏感参数。通过特征缩放引导模型将敏感参数分配到高精度分支，并将该分支扩展为多个稀疏激活的专家以实现高效容量扩展。

Result: 大量实验表明pQuant在极低位量化中实现了最先进的性能，解决了参数民主化效应问题，提升了量化模型的准确性和可扩展性。

Conclusion: pQuant通过参数解耦和专门化分支设计，有效解决了极低位量化中的参数民主化效应，为边缘部署提供了高效的大语言模型量化解决方案。

Abstract: Quantization-Aware Training from scratch has emerged as a promising approach for building efficient large language models (LLMs) with extremely low-bit weights (sub 2-bit), which can offer substantial advantages for edge deployment. However, existing methods still fail to achieve satisfactory accuracy and scalability. In this work, we identify a parameter democratization effect as a key bottleneck: the sensitivity of all parameters becomes homogenized, severely limiting expressivity. To address this, we propose pQuant, a method that decouples parameters by splitting linear layers into two specialized branches: a dominant 1-bit branch for efficient computation and a compact high-precision branch dedicated to preserving the most sensitive parameters. Through tailored feature scaling, we explicitly guide the model to allocate sensitive parameters to the high-precision branch. Furthermore, we extend this branch into multiple, sparsely-activated experts, enabling efficient capacity scaling. Extensive experiments indicate our pQuant achieves state-of-the-art performance in extremely low-bit quantization.

</details>


### [48] [Transformers converge to invariant algorithmic cores](https://arxiv.org/abs/2602.22600)
*Joshua S. Schiffman*

Main category: cs.LG

TL;DR: 该研究提出"算法核心"概念，发现不同训练运行的Transformer模型会收敛到相同的紧凑子空间结构，这些核心结构是任务性能的必要且充分条件。


<details>
  <summary>Details</summary>
Motivation: 大语言模型表现出复杂能力，但理解其内部工作机制仍是核心挑战。主要障碍在于训练选择的是行为而非电路，因此许多权重配置可以实现相同功能。哪些内部结构反映计算本质，哪些只是特定训练运行的偶然产物？

Method: 提取"算法核心"：任务性能所必需且充分的紧凑子空间。通过分析独立训练的Transformer模型、马尔可夫链Transformer、模块化加法Transformer和GPT-2语言模型，研究不同训练运行中的结构不变性。

Result: 1) 独立训练的Transformer学习不同权重但收敛到相同核心；2) 马尔可夫链Transformer在近正交子空间中嵌入3D核心，但恢复相同的转移谱；3) 模块化加法Transformer在"顿悟"时发现紧凑循环算子，随后膨胀；4) GPT-2通过单一轴控制主谓一致，翻转该轴可在生成过程中反转语法数。

Conclusion: Transformer计算围绕紧凑、共享的算法结构组织，这些低维不变量在不同训练运行和规模中持续存在。机制可解释性应针对这些计算本质（算法核心），而非实现特定的细节。

Abstract: Large language models exhibit sophisticated capabilities, yet understanding how they work internally remains a central challenge. A fundamental obstacle is that training selects for behavior, not circuitry, so many weight configurations can implement the same function. Which internal structures reflect the computation, and which are accidents of a particular training run? This work extracts algorithmic cores: compact subspaces necessary and sufficient for task performance. Independently trained transformers learn different weights but converge to the same cores. Markov-chain transformers embed 3D cores in nearly orthogonal subspaces yet recover identical transition spectra. Modular-addition transformers discover compact cyclic operators at grokking that later inflate, yielding a predictive model of the memorization-to-generalization transition. GPT-2 language models govern subject-verb agreement through a single axis that, when flipped, inverts grammatical number throughout generation across scales. These results reveal low-dimensional invariants that persist across training runs and scales, suggesting that transformer computations are organized around compact, shared algorithmic structures. Mechanistic interpretability could benefit from targeting such invariants -- the computational essence -- rather than implementation-specific details.

</details>


### [49] [$φ$-DPO: Fairness Direct Preference Optimization Approach to Continual Learning in Large Multimodal Models](https://arxiv.org/abs/2602.22601)
*Thanh-Dat Truong,Huu-Thien Tran,Jackson Cothren,Bhiksha Raj,Khoa Luu*

Main category: cs.LG

TL;DR: 本文提出了一种名为FaiDPO（φ-DPO）的新框架，用于解决大型多模态模型持续学习中的公平性问题，特别是在数据分布不平衡的情况下。该方法通过改进直接偏好优化（DPO）来同时缓解灾难性遗忘和数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）持续学习中的公平性是一个新兴但尚未充分探索的挑战。现有研究主要关注灾难性遗忘问题，但在数据分布不平衡情况下导致的模型更新偏见和跨任务性能次优问题仍未得到充分解决。

Method: 首先提出基于直接偏好优化（DPO）的持续学习新范式，通过配对偏好信号对齐学习来缓解灾难性遗忘。然后识别传统DPO在数据不平衡情况下的局限性，提出新的φ-DPO损失函数，明确解决分布偏见问题。同时为现有持续学习基准构建了配对偏好标注。

Result: 广泛的实验和消融研究表明，提出的φ-DPO在多个基准测试中达到了最先进的性能，优于先前的大型多模态模型持续学习方法。理论分析也证明了该方法能同时解决遗忘和数据不平衡问题。

Conclusion: 本文提出的FaiDPO（φ-DPO）框架有效地解决了大型多模态模型持续学习中的公平性问题，特别是在数据不平衡情况下，为这一新兴挑战提供了有效的解决方案。

Abstract: Fairness in Continual Learning for Large Multimodal Models (LMMs) is an emerging yet underexplored challenge, particularly in the presence of imbalanced data distributions that can lead to biased model updates and suboptimal performance across tasks. While recent continual learning studies have made progress in addressing catastrophic forgetting, the problem of fairness caused the imbalanced data remains largely underexplored. This paper presents a novel Fairness Direct Preference Optimization (FaiDPO or $φ$-DPO) framework for continual learning in LMMs. In particular, we first propose a new continual learning paradigm based on Direct Preference Optimization (DPO) to mitigate catastrophic forgetting by aligning learning with pairwise preference signals. Then, we identify the limitations of conventional DPO in imbalanced data and present a new $φ$-DPO loss that explicitly addresses distributional biases. We provide a comprehensive theoretical analysis demonstrating that our approach addresses both forgetting and data imbalance. Additionally, to enable $φ$-DPO-based continual learning, we construct pairwise preference annotations for existing benchmarks in the context of continual learning. Extensive experiments and ablation studies show the proposed $φ$-DPO achieves State-of-the-Art performance across multiple benchmarks, outperforming prior continual learning methods of LMMs.

</details>


### [50] [Mitigating Membership Inference in Intermediate Representations via Layer-wise MIA-risk-aware DP-SGD](https://arxiv.org/abs/2602.22611)
*Jiayang Meng,Tao Huang,Chen Hou,Guolong Zheng,Hong Chen*

Main category: cs.LG

TL;DR: 本文提出LM-DP-SGD方法，针对嵌入即接口场景中不同层对成员推理攻击的异质性脆弱性，通过分层风险评估自适应分配隐私保护资源，在相同隐私预算下实现更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 在嵌入即接口场景中，预训练模型的中间表示会泄露训练集成员信息，不同层对成员推理攻击的脆弱性存在异质性。现有的差分隐私随机梯度下降方法采用统一的噪声乘子和逐样本梯度裁剪，忽略了这种分层脆弱性差异，导致隐私保护资源分配不合理。

Method: 提出分层MIA风险感知的DP-SGD方法：1）在公共影子数据集上训练影子模型；2）从训练/测试分割中提取每层中间表示；3）拟合分层特定的MIA攻击器，使用攻击错误率作为MIA风险评估；4）利用MIA的跨数据集可迁移性，在私有训练中根据风险权重重新调整每层对全局裁剪梯度的贡献，在固定噪声幅度下提供分层适当的保护。

Result: 在相同隐私预算下，LM-DP-SGD显著降低了峰值中间表示级别的MIA风险，同时保持了模型效用，实现了更优的隐私-效用权衡。实验验证了方法的有效性。

Conclusion: LM-DP-SGD通过分层风险评估自适应分配隐私保护资源，解决了传统DP-SGD忽略分层MIA脆弱性异质性的问题，在固定隐私预算下提供了更精细的隐私保护，实现了更好的隐私-效用平衡。

Abstract: In Embedding-as-an-Interface (EaaI) settings, pre-trained models are queried for Intermediate Representations (IRs). The distributional properties of IRs can leak training-set membership signals, enabling Membership Inference Attacks (MIAs) whose strength varies across layers. Although Differentially Private Stochastic Gradient Descent (DP-SGD) mitigates such leakage, existing implementations employ per-example gradient clipping and a uniform, layer-agnostic noise multiplier, ignoring heterogeneous layer-wise MIA vulnerability. This paper introduces Layer-wise MIA-risk-aware DP-SGD (LM-DP-SGD), which adaptively allocates privacy protection across layers in proportion to their MIA risk. Specifically, LM-DP-SGD trains a shadow model on a public shadow dataset, extracts per-layer IRs from its train/test splits, and fits layer-specific MIA adversaries, using their attack error rates as MIA-risk estimates. Leveraging the cross-dataset transferability of MIAs, these estimates are then used to reweight each layer's contribution to the globally clipped gradient during private training, providing layer-appropriate protection under a fixed noise magnitude. We further establish theoretical guarantees on both privacy and convergence of LM-DP-SGD. Extensive experiments show that, under the same privacy budget, LM-DP-SGD reduces the peak IR-level MIA risk while preserving utility, yielding a superior privacy-utility trade-off.

</details>


### [51] [Semantic Tube Prediction: Beating LLM Data Efficiency with JEPA](https://arxiv.org/abs/2602.22617)
*Hai Huang,Yann LeCun,Randall Balestriero*

Main category: cs.LG

TL;DR: 该论文提出了一种基于测地线假设的语义管预测任务，通过几何先验约束隐藏状态轨迹，使LLM仅用1/16的训练数据就能达到基线准确率，突破了Chinchilla式缩放定律的数据效率限制。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的缩放定律虽然能预测损失随计算、数据和参数的变化，但这些定律是描述性的而非规定性的，主要刻画典型训练而非最优训练。很少有工作能成功挑战这些定律所隐含的数据效率边界，这是本文的主要关注点。

Method: 提出测地线假设，认为词元序列在平滑语义流形上追踪测地线，因此具有局部线性特性。基于此原理，提出语义管预测任务，这是一种JEPA风格的规范化器，将隐藏状态轨迹限制在测地线的管状邻域内。STP将JEPA推广到语言领域，无需显式的多视图增强。

Result: STP约束提高了信噪比，并在推理过程中通过防止轨迹碰撞来保持多样性。在NL-RX-SYNTH数据集上，STP使LLM仅用1/16的训练数据就能匹配基线准确率，直接违反了Chinchilla式缩放定律的数据项。

Conclusion: 研究表明，基于几何原理的先验可以超越暴力缩放方法，通过测地线假设和语义管预测任务，能够显著提高语言模型的数据效率，突破传统缩放定律的限制。

Abstract: Large Language Models (LLMs) obey consistent scaling laws -- empirical power-law fits that predict how loss decreases with compute, data, and parameters. While predictive, these laws are descriptive rather than prescriptive: they characterize typical training, not optimal training. Surprisingly few works have successfully challenged the data-efficiency bounds implied by these laws -- which is our primary focus. To that end, we introduce the Geodesic Hypothesis, positing that token sequences trace geodesics on a smooth semantic manifold and are therefore locally linear. Building on this principle, we propose a novel Semantic Tube Prediction (STP) task, a JEPA-style regularizer that confines hidden-state trajectories to a tubular neighborhood of the geodesic. STP generalizes JEPA to language without requiring explicit multi-view augmentations. We show this constraint improves signal-to-noise ratio, and consequently preserves diversity by preventing trajectory collisions during inference. Empirically, STP allows LLMs to match baseline accuracy with 16$\times$ less training data on the NL-RX-SYNTH dataset, directly violating the data term of Chinchilla-style scaling laws and demonstrating that principled geometric priors can surpass brute-force scaling. Code is available at https://github.com/galilai-group/llm-jepa#stp.

</details>


### [52] [ContextRL: Enhancing MLLM's Knowledge Discovery Efficiency with Context-Augmented RL](https://arxiv.org/abs/2602.22623)
*Xingyu Lu,Jinpeng Wang,YiFan Zhang,Shijie Ma,Xiao Hu,Tianke Zhang,Haonan fan,Kaiyu Jiang,Changyi Liu,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Chun Yuan*

Main category: cs.LG

TL;DR: ContextRL是一个利用上下文增强来改进强化学习与验证的新框架，通过提供完整参考解决方案作为上下文来增强可识别性，并引入多轮采样策略来改善可达性，显著提高了知识发现效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决强化学习与验证中的瓶颈问题，特别是可识别性（区分正确答案但低质量推理过程）和可达性（从全负样本组中恢复正确响应）的挑战。

Method: 1. 为奖励模型提供完整参考解决方案作为上下文，实现细粒度过程验证；2. 引入多轮采样策略，奖励模型为失败尝试生成错误报告，指导策略从先前全负样本组中"恢复"正确响应。

Result: 在11个感知和推理基准测试中，ContextRL显著提高了知识发现效率。Qwen3-VL-8B模型实现了与32B模型相当的性能，大幅优于标准RLVR基线，同时有效缓解了奖励黑客攻击。

Conclusion: 上下文信息对于提高奖励模型准确性具有显著潜力，研究记录了奖励黑客攻击的普遍发生，为未来RLVR研究提供了有价值的见解。

Abstract: We propose ContextRL, a novel framework that leverages context augmentation to overcome these bottlenecks. Specifically, to enhance Identifiability, we provide the reward model with full reference solutions as context, enabling fine-grained process verification to filter out false positives (samples with the right answer but low-quality reasoning process). To improve Reachability, we introduce a multi-turn sampling strategy where the reward model generates mistake reports for failed attempts, guiding the policy to "recover" correct responses from previously all-negative groups. Experimental results on 11 perception and reasoning benchmarks show that ContextRL significantly improves knowledge discovery efficiency. Notably, ContextRL enables the Qwen3-VL-8B model to achieve performance comparable to the 32B model, outperforming standard RLVR baselines by a large margin while effectively mitigating reward hacking. Our in-depth analysis reveals the significant potential of contextual information for improving reward model accuracy and document the widespread occurrence of reward hacking, offering valuable insights for future RLVR research.

</details>


### [53] [Tackling Privacy Heterogeneity in Differentially Private Federated Learning](https://arxiv.org/abs/2602.22633)
*Ruichen Xu,Ying-Jun Angela Zhang,Jianwei Huang*

Main category: cs.LG

TL;DR: 该论文研究了差分隐私联邦学习中的隐私异质性挑战，提出了首个隐私感知的客户端选择策略，通过优化选择概率最小化训练误差，在CIFAR-10上相比基线方法提升了10%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私联邦学习方法假设所有客户端共享统一的隐私预算，但现实场景中隐私需求差异很大。这种隐私异质性带来了挑战：传统基于数据量的客户端选择策略无法区分提供高质量更新的客户端和因严格隐私约束引入大量噪声的客户端。

Method: 1. 建立理论基础：推导收敛分析，量化隐私异质性对训练误差的影响；2. 提出隐私感知客户端选择策略：将其表述为凸优化问题，自适应调整选择概率以最小化训练误差。

Result: 在基准数据集上的大量实验表明，该方法在异构隐私预算下，相比现有基线方法在CIFAR-10上实现了高达10%的测试准确率提升。

Conclusion: 该研究强调了将隐私异质性纳入客户端选择对于实现实用有效的联邦学习的重要性，提出的隐私感知选择策略能显著提升模型性能。

Abstract: Differentially private federated learning (DP-FL) enables clients to collaboratively train machine learning models while preserving the privacy of their local data. However, most existing DP-FL approaches assume that all clients share a uniform privacy budget, an assumption that does not hold in real-world scenarios where privacy requirements vary widely. This privacy heterogeneity poses a significant challenge: conventional client selection strategies, which typically rely on data quantity, cannot distinguish between clients providing high-quality updates and those introducing substantial noise due to strict privacy constraints. To address this gap, we present the first systematic study of privacy-aware client selection in DP-FL. We establish a theoretical foundation by deriving a convergence analysis that quantifies the impact of privacy heterogeneity on training error. Building on this analysis, we propose a privacy-aware client selection strategy, formulated as a convex optimization problem, that adaptively adjusts selection probabilities to minimize training error. Extensive experiments on benchmark datasets demonstrate that our approach achieves up to a 10% improvement in test accuracy on CIFAR-10 compared to existing baselines under heterogeneous privacy budgets. These results highlight the importance of incorporating privacy heterogeneity into client selection for practical and effective federated learning.

</details>


### [54] [Compress the Easy, Explore the Hard: Difficulty-Aware Entropy Regularization for Efficient LLM Reasoning](https://arxiv.org/abs/2602.22642)
*Qin-Wen Luo,Sheng Ren,Xiang Chen,Rui Liu,Jun Fang,Naiqiang Tan,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: CEEH方法通过难度感知的熵正则化，在保持推理能力的同时压缩CoT推理步骤，解决现有方法因过度压缩导致熵崩溃的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CoT压缩方法（如自训练、带长度约束的强化学习）在追求简短推理步骤时往往牺牲推理能力，主要问题是显式优化短轨迹会触发快速熵崩溃，过早缩小探索空间，阻碍发现有效推理路径，特别是对于需要大量推导的难题。

Method: 提出CEEH（Compress responses for Easy questions and Explore Hard ones）方法：1）动态评估实例难度，应用选择性熵正则化：对难题保持多样化搜索空间以确保鲁棒性，对易题允许激进压缩；2）引入基于历史最短正确答案的动态最优长度惩罚，有效抵消熵引起的长度膨胀并稳定奖励信号。

Result: 在六个推理基准测试中，CEEH持续减少响应长度，同时保持与基础模型相当的准确性，并相对于仅优化长度的方法提高了Pass@k指标。

Conclusion: CEEH通过难度感知的熵正则化和动态长度惩罚，在保持推理能力的同时有效压缩CoT推理步骤，解决了现有压缩方法中熵崩溃的问题，实现了高效推理。

Abstract: Chain-of-Thought (CoT) has substantially empowered Large Language Models (LLMs) to tackle complex reasoning tasks, yet the verbose nature of explicit reasoning steps incurs prohibitive inference latency and computational costs, limiting real-world deployment. While existing compression methods - ranging from self-training to Reinforcement Learning (RL) with length constraints - attempt to mitigate this, they often sacrifice reasoning capability for brevity. We identify a critical failure mode in these approaches: explicitly optimizing for shorter trajectories triggers rapid entropy collapse, which prematurely shrinks the exploration space and stifles the discovery of valid reasoning paths, particularly for challenging questions requiring extensive deduction. To address this issue, we propose Compress responses for Easy questions and Explore Hard ones (CEEH), a difficulty-aware approach to RL-based efficient reasoning. CEEH dynamically assesses instance difficulty to apply selective entropy regularization: it preserves a diverse search space for currently hard questions to ensure robustness, while permitting aggressive compression on easier instances where the reasoning path is well-established. In addition, we introduce a dynamic optimal-length penalty anchored to the historically shortest correct response, which effectively counteracts entropy-induced length inflation and stabilizes the reward signal. Across six reasoning benchmarks, CEEH consistently reduces response length while maintaining accuracy comparable to the base model, and improves Pass@k relative to length-only optimization.

</details>


### [55] [MUG: Meta-path-aware Universal Heterogeneous Graph Pre-Training](https://arxiv.org/abs/2602.22645)
*Lianze Shan,Jitao Zhao,Dongxiao He,Yongqi Huang,Zhiyong Feng,Weixiong Zhang*

Main category: cs.LG

TL;DR: MUG提出了一种针对异质图的通用预训练方法，通过输入统一模块和维度感知编码器解决异质图类型多样性和元路径差异的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前通用图预训练主要关注同质图，而异质图具有更复杂的结构和语义，面临两大挑战：1) 多样类型和数据集特定语义阻碍统一表示空间构建；2) 元路径数量和语义跨数据集变化，使编码和聚合模式难以迁移。

Method: 提出元路径感知的通用异质图预训练方法MUG：1) 输入统一模块将异质图中多节点和关系类型信息整合为统一表示；2) 维度感知编码器将表示投影到共享空间；3) 共享编码器捕获跨元路径视图的一致结构模式；4) 全局目标增强可区分性并减少数据集特定偏差。

Result: 在多个真实数据集上的广泛实验证明了MUG的有效性。

Conclusion: MUG成功解决了异质图通用预训练的关键挑战，为异质图表示学习提供了有效的预训练框架。

Abstract: Universal graph pre-training has emerged as a key paradigm in graph representation learning, offering a promising way to train encoders to learn transferable representations from unlabeled graphs and to effectively generalize across a wide range of downstream tasks. However, recent explorations in universal graph pre-training primarily focus on homogeneous graphs and it remains unexplored for heterogeneous graphs, which exhibit greater structural and semantic complexity. This heterogeneity makes it highly challenging to train a universal encoder for diverse heterogeneous graphs: (i) the diverse types with dataset-specific semantics hinder the construction of a unified representation space; (ii) the number and semantics of meta-paths vary across datasets, making encoding and aggregation patterns learned from one dataset difficult to apply to others. To address these challenges, we propose a novel Meta-path-aware Universal heterogeneous Graph pre-training (MUG) approach. Specifically, for challenge (i), MUG introduces a input unification module that integrates information from multiple node and relation types within each heterogeneous graph into a unified representation.This representation is then projected into a shared space by a dimension-aware encoder, enabling alignment across graphs with diverse schemas.Furthermore, for challenge (ii), MUG trains a shared encoder to capture consistent structural patterns across diverse meta-path views rather than relying on dataset-specific aggregation strategies, while a global objective encourages discriminability and reduces dataset-specific biases. Extensive experiments demonstrate the effectiveness of MUG on some real datasets.

</details>


### [56] [LEDA: Latent Semantic Distribution Alignment for Multi-domain Graph Pre-training](https://arxiv.org/abs/2602.22660)
*Lianze Shan,Jitao Zhao,Dongxiao He,Siqi Liu,Jiaxu Cui,Weixiong Zhang*

Main category: cs.LG

TL;DR: LEDA是一种用于通用图预训练的新方法，通过潜在语义分布对齐解决现有方法在跨域图学习中的语义对齐和训练指导不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有通用图预训练方法面临两个主要挑战：1）简单数据对齐无法处理高度多样化的图数据，导致语义不对齐；2）将域内预训练范式任意应用于跨域场景，难以从多图中捕获有效知识。

Method: 提出LEDA模型：1）维度投影单元自适应地将不同域特征对齐到共享语义空间；2）变分语义推断模块获取共享潜在分布；3）用该分布指导域投影，实现跨域语义对齐和学习。

Result: LEDA在广泛的图和下游任务中表现优异，特别是在少样本跨域设置中，显著优于域内基线和先进的通用预训练模型。

Conclusion: LEDA通过潜在语义分布对齐有效解决了通用图预训练中的语义对齐和跨域知识学习问题，为图表示学习提供了新的有效方法。

Abstract: Recent advances in generic large models, such as GPT and DeepSeek, have motivated the introduction of universality to graph pre-training, aiming to learn rich and generalizable knowledge across diverse domains using graph representations to improve performance in various downstream applications. However, most existing methods face challenges in learning effective knowledge from generic graphs, primarily due to simplistic data alignment and limited training guidance. The issue of simplistic data alignment arises from the use of a straightforward unification for highly diverse graph data, which fails to align semantics and misleads pre-training models. The problem with limited training guidance lies in the arbitrary application of in-domain pre-training paradigms to cross-domain scenarios. While it is effective in enhancing discriminative representation in one data space, it struggles to capture effective knowledge from many graphs. To address these challenges, we propose a novel Latent sEmantic Distribution Alignment (LEDA) model for universal graph pre-training. Specifically, we first introduce a dimension projection unit to adaptively align diverse domain features into a shared semantic space with minimal information loss. Furthermore, we design a variational semantic inference module to obtain the shared latent distribution. The distribution is then adopted to guide the domain projection, aligning it with shared semantics across domains and ensuring cross-domain semantic learning. LEDA exhibits strong performance across a broad range of graphs and downstream tasks. Remarkably, in few-shot cross-domain settings, it significantly outperforms in-domain baselines and advanced universal pre-training models.

</details>


### [57] [Switch-Hurdle: A MoE Encoder with AR Hurdle Decoder for Intermittent Demand Forecasting](https://arxiv.org/abs/2602.22685)
*Fabian Muşat,Simona Căbuz*

Main category: cs.LG

TL;DR: Switch-Hurdle是一个用于间歇性需求预测的新框架，结合了混合专家编码器和基于Hurdle的概率解码器，将预测任务分解为销售概率估计和条件数量预测两个部分，在M5基准和零售数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 间歇性需求（长时间零销售与零星非零值交替出现）在零售和供应链预测中是一个持续挑战。传统方法（如ARIMA、指数平滑、Croston变体）和现代神经架构（如DeepAR和Transformer模型）在处理这类数据时表现不佳，因为它们将需求视为单一连续过程，或者在处理大量稀疏序列时计算成本过高。

Method: Switch-Hurdle框架包含：1）混合专家编码器，在前向传播中使用稀疏Top-1专家路由，但通过直通估计器在反向传播中实现近似密集计算；2）基于交叉注意力自回归设计的Hurdle概率解码器，包含共享的hurdle头，将预测任务明确分解为：a）估计销售概率的二元分类组件，b）给定销售条件下的数量预测回归组件。

Result: 在M5基准和大型专有零售数据集上的实证结果表明，Switch-Hurdle实现了最先进的预测性能，同时保持了可扩展性。

Conclusion: Switch-Hurdle通过将间歇性需求预测结构化地分解为发生概率和数量预测两个组件，有效捕捉了间歇性需求的内在特征，在保持可扩展性的同时实现了优异的预测性能。

Abstract: Intermittent demand, a pattern characterized by long sequences of zero sales punctuated by sporadic, non-zero values, poses a persistent challenge in retail and supply chain forecasting. Both traditional methods, such as ARIMA, exponential smoothing, or Croston variants, as well as modern neural architectures such as DeepAR and Transformer-based models often underperform on such data, as they treat demand as a single continuous process or become computationally expensive when scaled across many sparse series. To address these limitations, we introduce Switch-Hurdle: a new framework that integrates a Mixture-of-Experts (MoE) encoder with a Hurdle-based probabilistic decoder. The encoder uses a sparse Top-1 expert routing during the forward pass yet approximately dense in the backward pass via a straight-through estimator (STE). The decoder follows a cross-attention autoregressive design with a shared hurdle head that explicitly separates the forecasting task into two components: a binary classification component estimating the probability of a sale, and a conditional regression component, predicting the quantity given a sale. This structured separation enables the model to capture both occurrence and magnitude processes inherent to intermittent demand. Empirical results on the M5 benchmark and a large proprietary retail dataset show that Switch-Hurdle achieves state-of-the-art prediction performance while maintaining scalability.

</details>


### [58] [Enhancing Geometric Perception in VLMs via Translator-Guided Reinforcement Learning](https://arxiv.org/abs/2602.22703)
*Hao Yu,Shuning Jia,Guanghao Li,Wenhao Jiang,Chun Yuan*

Main category: cs.LG

TL;DR: GeoPerceive是一个几何感知基准测试，包含图表实例和领域特定语言表示，用于评估视觉语言模型的几何感知能力。GeoDPO是一个基于翻译器引导的强化学习框架，通过NL-to-DSL翻译器生成细粒度奖励信号，显著提升了模型的几何感知和推理性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在几何推理方面存在困难，主要原因是它们对基本图表元素的感知能力有限。现有方法难以将几何感知与推理能力分开评估，需要专门的基准测试来独立评估几何感知能力。

Method: 1) 提出GeoPerceive基准测试，包含图表实例和领域特定语言表示，配备高效自动数据生成管道；2) 提出GeoDPO框架，使用在GeoPerceive合成数据上训练的NL-to-DSL翻译器，将自然语言转换为DSL，生成细粒度DSL级分数作为强化学习的奖励信号。

Result: 实验结果显示：监督微调在域外场景中改进有限甚至可能损害性能，而GeoDPO在域内数据上提升26.5%，域外数据上提升8.0%，下游推理任务上提升39.0%。这表明GeoDPO具有优异的性能和泛化能力。

Conclusion: GeoPerceive基准测试能够独立评估几何感知能力，而GeoDPO框架通过翻译器引导的强化学习显著提升了视觉语言模型的几何感知和推理性能，优于传统的监督微调方法，具有良好的泛化能力。

Abstract: Vision-language models (VLMs) often struggle with geometric reasoning due to their limited perception of fundamental diagram elements. To tackle this challenge, we introduce GeoPerceive, a benchmark comprising diagram instances paired with domain-specific language (DSL) representations, along with an efficient automatic data generation pipeline. This design enables the isolated evaluation of geometric perception independently from reasoning. To exploit the data provided by GeoPerceive for enhancing the geometric perception capabilities of VLMs, we propose GeoDPO, a translator-guided reinforcement learning (RL) framework. GeoDPO employs an NL-to-DSL translator, which is trained on synthetic pairs generated by the data engine of GeoPerceive, to bridge natural language and DSL. This translator facilitates the computation of fine-grained, DSL-level scores, which serve as reward signals in reinforcement learning. We assess GeoDPO on both in-domain and out-of-domain datasets, spanning tasks in geometric perception as well as downstream reasoning. Experimental results demonstrate that, while supervised fine-tuning (SFT) offers only marginal improvements and may even impair performance in out-of-domain scenarios, GeoDPO achieves substantial gains: $+26.5\%$ on in-domain data, $+8.0\%$ on out-of-domain data, and $+39.0\%$ on downstream reasoning tasks. These findings underscore the superior performance and generalization ability of GeoDPO over SFT. All codes are released at https://github.com/Longin-Yu/GeoPerceive
  to ensure reproducibility.

</details>


### [59] [Multi-agent imitation learning with function approximation: Linear Markov games and beyond](https://arxiv.org/abs/2602.22810)
*Luca Viano,Till Freihaut,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: 该论文首次对线性马尔可夫博弈中的多智能体模仿学习进行理论分析，提出特征级集中系数替代状态-动作级系数，并设计了首个计算高效的交互式MAIL算法，在Tic-Tac-Toe和Connect4游戏中超越行为克隆。


<details>
  <summary>Details</summary>
Motivation: 多智能体模仿学习（MAIL）在理论分析方面存在空白，特别是在线性马尔可夫博弈中。现有方法依赖状态-动作级的"所有策略偏差集中系数"，该系数可能过大，限制了算法的实际应用效果。

Method: 1. 利用线性马尔可夫博弈的结构（转移动态和奖励函数在给定特征下线性），将状态-动作级集中系数替换为特征级集中系数；2. 设计首个计算高效的交互式MAIL算法，避免对集中系数的依赖；3. 基于理论发现提出深度MAIL交互算法。

Result: 1. 特征级集中系数比状态-动作级系数小得多（当特征能有效表示状态相似性时）；2. 交互式MAIL算法的样本复杂度仅依赖于特征映射维度d；3. 深度MAIL交互算法在Tic-Tac-Toe和Connect4游戏中明显优于行为克隆（BC）。

Conclusion: 该研究为线性马尔可夫博弈中的多智能体模仿学习提供了首个理论分析框架，通过特征级集中系数和交互式算法设计，显著提升了学习效率和性能，为MAIL的实际应用奠定了理论基础。

Abstract: In this work, we present the first theoretical analysis of multi-agent imitation learning (MAIL) in linear Markov games where both the transition dynamics and each agent's reward function are linear in some given features. We demonstrate that by leveraging this structure, it is possible to replace the state-action level "all policy deviation concentrability coefficient" (Freihaut et al., arXiv:2510.09325) with a concentrability coefficient defined at the feature level which can be much smaller than the state-action analog when the features are informative about states' similarity. Furthermore, to circumvent the need for any concentrability coefficient, we turn to the interactive setting. We provide the first, computationally efficient, interactive MAIL algorithm for linear Markov games and show that its sample complexity depends only on the dimension of the feature map $d$. Building on these theoretical findings, we propose a deep MAIL interactive algorithm which clearly outperforms BC on games such as Tic-Tac-Toe and Connect4.

</details>


### [60] [Hierarchy-of-Groups Policy Optimization for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2602.22817)
*Shuo He,Lang Feng,Qi Wei,Xin Cheng,Lei Feng,Bo An*

Main category: cs.LG

TL;DR: HGPO提出了一种分层组策略优化方法，解决了基于组的强化学习中步级相对优势估计的上下文不一致问题，在长视野智能体任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的强化学习方法在步级策略优化中存在上下文不一致问题，即同一组内的步骤可能具有不同的历史上下文，这会导致优势估计偏差，从而显著降低策略优化效果。

Method: HGPO将每个步骤分配到多个分层组中，根据历史上下文的一致性进行分组。然后为每个步骤计算不同组内的优势，并通过自适应加权方案进行聚合，从而实现优势估计的偏差-方差权衡优化。

Result: 在ALFWorld和WebShop两个具有挑战性的智能体任务上，使用Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型进行评估，HGPO在相同计算约束下显著优于现有的智能体强化学习方法。

Conclusion: HGPO通过分层分组和自适应优势聚合有效解决了步级优势估计中的上下文不一致问题，无需额外模型或rollout，在长视野智能体任务上取得了显著改进。

Abstract: Group-based reinforcement learning (RL), such as GRPO, has advanced the capabilities of large language models on long-horizon agentic tasks. To enable more fine-grained policy updates, recent research has increasingly shifted toward stepwise group-based policy optimization, which treats each step in a rollout trajectory independently while using a memory module to retain historical context. However, we find a key issue in estimating stepwise relative advantages, namely context inconsistency, where steps within the same group may differ in their historical contexts. Empirically, we reveal that this issue can lead to severely biased advantage estimation, thereby degrading policy optimization significantly. To address the issue, in this paper, we propose Hierarchy-of-Groups Policy Optimization (HGPO) for long-horizon agentic tasks. Specifically, within a group of rollout trajectories, HGPO assigns each step to multiple hierarchical groups according to the consistency of historical contexts. Then, for each step, HGPO computes distinct advantages within each group and aggregates them with an adaptive weighting scheme. In this way, HGPO can achieve a favorable bias-variance trade-off in stepwise advantage estimation, without extra models or rollouts. Evaluations on two challenging agentic tasks, ALFWorld and WebShop with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct, show that HGPO significantly outperforms existing agentic RL methods under the same computational constraints. Code is available at https://github.com/langfengQ/verl-agent/tree/master/recipe/hgpo.

</details>


### [61] [Moral Preferences of LLMs Under Directed Contextual Influence](https://arxiv.org/abs/2602.22831)
*Phil Blandfort,Tushar Karayil,Urja Pawar,Robert Graham,Alex McKenzie,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 该研究探讨了上下文提示如何影响LLM在道德困境中的决策，发现即使表面相关的上下文也能显著改变模型选择，基线偏好无法预测可操控性，影响可能适得其反，推理会降低平均敏感性但放大偏见示例的影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM道德基准测试通常使用无上下文提示，假设模型有稳定偏好。但在实际部署中，提示常包含用户请求、社会规范线索等上下文信号，这些可能影响决策。研究者希望了解定向上下文影响如何重塑道德困境中的决策。

Method: 引入了一个针对电车问题式道德困境的定向上下文影响评估框架。对每个人口统计因素，应用匹配的、方向翻转的上下文影响（仅区别偏向哪个群体），从而系统测量定向响应。

Result: 发现：(1)上下文影响常显著改变决策，即使仅表面相关；(2)基线偏好无法预测定向可操控性，模型可能基线中立但在影响下表现出系统性可操控不对称；(3)影响可能适得其反：模型可能声称中立或忽略上下文线索，但选择仍会改变，有时甚至反向；(4)推理降低平均敏感性，但放大偏见少样本示例的影响。

Conclusion: 研究结果表明需要通过受控的、方向翻转的上下文操作来扩展道德评估，以更好地表征模型行为。基线测试不足以揭示模型在上下文影响下的实际决策模式。

Abstract: Moral benchmarks for LLMs typically use context-free prompts, implicitly assuming stable preferences. In deployment, however, prompts routinely include contextual signals such as user requests, cues on social norms, etc. that may steer decisions. We study how directed contextual influences reshape decisions in trolley-problem-style moral triage settings. We introduce a pilot evaluation harness for directed contextual influence in trolley-problem-style moral triage: for each demographic factor, we apply matched, direction-flipped contextual influences that differ only in which group they favor, enabling systematic measurement of directional response. We find that: (i) contextual influences often significantly shift decisions, even when only superficially relevant; (ii) baseline preferences are a poor predictor of directional steerability, as models can appear baseline-neutral yet exhibit systematic steerability asymmetry under influence; (iii) influences can backfire: models may explicitly claim neutrality or discount the contextual cue, yet their choices still shift, sometimes in the opposite direction; and (iv) reasoning reduces average sensitivity, but amplifies the effect of biased few-shot examples. Our findings motivate extending moral evaluations with controlled, direction-flipped context manipulations to better characterize model behavior.

</details>


### [62] [MEDNA-DFM: A Dual-View FiLM-MoE Model for Explainable DNA Methylation Prediction](https://arxiv.org/abs/2602.22850)
*Yi He,Yina Cao,Jixiu Zhai,Di Wang,Junxiao Kong,Tianchi Lu*

Main category: cs.LG

TL;DR: MEDNA-DFM是一个高性能的DNA甲基化预测模型，结合机制启发的信号净化算法，不仅能准确预测甲基化，还能提取可靠的保守基序，并提出了"序列-结构协同"假说。


<details>
  <summary>Details</summary>
Motivation: 深度学习在DNA甲基化识别方面表现出色，但其"黑盒"特性阻碍了生物学洞察。需要开发既能准确预测又能提供生物学解释的方法。

Method: 提出MEDNA-DFM高性能模型和机制启发的信号净化算法。通过外部独立数据集验证模型泛化能力，应用算法提取可靠基序，并通过果蝇6mA案例研究提出"序列-结构协同"假说，使用计算机模拟突变验证。

Result: MEDNA-DFM能有效捕捉保守的甲基化模式，在不同物种间实现稳健区分。模型泛化由保守的内在基序驱动而非系统发育接近性。提取的基序可靠性显著高于先前研究。果蝇6mA案例验证了GAGG核心基序和上游A-tract元件的协同作用。

Conclusion: 这项工作提供了强大的甲基化预测工具，展示了可解释深度学习如何推动方法学创新和生物学假说生成，为理解表观遗传调控提供了新视角。

Abstract: Accurate computational identification of DNA methylation is essential for understanding epigenetic regulation. Although deep learning excels in this binary classification task, its "black-box" nature impedes biological insight. We address this by introducing a high-performance model MEDNA-DFM, alongside mechanism-inspired signal purification algorithms. Our investigation demonstrates that MEDNA-DFM effectively captures conserved methylation patterns, achieving robust distinction across diverse species. Validation on external independent datasets confirms that the model's generalization is driven by conserved intrinsic motifs (e.g., GC content) rather than phylogenetic proximity. Furthermore, applying our developed algorithms extracted motifs with significantly higher reliability than prior studies. Finally, empirical evidence from a Drosophila 6mA case study prompted us to propose a "sequence-structure synergy" hypothesis, suggesting that the GAGG core motif and an upstream A-tract element function cooperatively. We further validated this hypothesis via in silico mutagenesis, confirming that the ablation of either or both elements significantly degrades the model's recognition capabilities. This work provides a powerful tool for methylation prediction and demonstrates how explainable deep learning can drive both methodological innovation and the generation of biological hypotheses.

</details>


### [63] [Fair feature attribution for multi-output prediction: a Shapley-based perspective](https://arxiv.org/abs/2602.22882)
*Umberto Biccari,Alain Ibáñez de Opakua,José María Mato,Óscar Millet,Roberto Morales,Enrique Zuazua*

Main category: cs.LG

TL;DR: 该论文为多输出预测器的特征归因提供了Shapley框架下的公理化特征，证明了任何满足效率、对称性、虚拟玩家和可加性公理的归因规则必须按输出分量分解，揭示了Shapley解释在多输出学习中的结构约束。


<details>
  <summary>Details</summary>
Motivation: 目前SHAP解释通常独立计算每个输出坐标，但这种做法的理论必要性尚不明确。论文旨在通过扩展经典Shapley公理到向量值合作博弈，澄清多输出预测器中特征归因的理论基础。

Method: 将经典Shapley公理扩展到向量值合作博弈，建立刚性定理证明任何满足效率、对称性、虚拟玩家和可加性公理的归因规则必须按分量分解。通过数值实验在生物医学基准上验证多输出模型的优势。

Result: 证明了任何满足四个经典Shapley公理的联合输出归因规则必须按输出分量分解，因此任何联合输出归因规则必须至少放松一个经典公理。这揭示了Shapley解释在多输出学习中的结构约束。

Conclusion: 该研究形式化了Shapley解释在多输出学习中的结构约束，澄清了公平一致解释的确切范围。多输出模型在训练和部署中可带来计算节省，同时产生的SHAP解释仍完全符合Shapley公理强加的分量结构。

Abstract: In this article, we provide an axiomatic characterization of feature attribution for multi-output predictors within the Shapley framework. While SHAP explanations are routinely computed independently for each output coordinate, the theoretical necessity of this practice has remained unclear. By extending the classical Shapley axioms to vector-valued cooperative games, we establish a rigidity theorem showing that any attribution rule satisfying efficiency, symmetry, dummy player, and additivity must necessarily decompose component-wise across outputs. Consequently, any joint-output attribution rule must relax at least one of the classical Shapley axioms. This result identifies a previously unformalized structural constraint in Shapley-based interpretability, clarifying the precise scope of fairness-consistent explanations in multi-output learning. Numerical experiments on a biomedical benchmark illustrate that multi-output models can yield computational savings in training and deployment, while producing SHAP explanations that remain fully consistent with the component-wise structure imposed by the Shapley axioms.

</details>


### [64] [NoRA: Breaking the Linear Ceiling of Low-Rank Adaptation via Manifold Expansion](https://arxiv.org/abs/2602.22911)
*Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: NoRA通过引入非线性激活和结构化dropout，突破了LoRA在复杂推理任务中的线性天花板限制，在更低秩的情况下实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调中占主导地位，但在复杂推理任务中面临"线性天花板"问题：单纯增加秩会因内在线性约束而收益递减。

Method: 提出NoRA（非线性秩适应），这是一种权重级并行适配器，通过注入SiLU门控和结构化dropout来诱导流形扩展。

Result: 在SlimOrca基准测试中，NoRA在秩64时（PPL 3.89）优于LoRA在秩512时的表现（PPL 3.90）。在数学推理任务中，NoRA在MathInstruct上达到困惑度1.97，显著优于LoRA的饱和点2.07。

Conclusion: NoRA通过激活奇异值谱的休眠尾部，有效防止了线性方法中观察到的秩崩溃，在参数效率方面具有显著优势。

Abstract: Low-Rank Adaptation (LoRA) dominates parameter-efficient fine-tuning (PEFT). However, it faces a critical ``linear ceiling'' in complex reasoning tasks: simply increasing the rank yields diminishing returns due to intrinsic linear constraints. We introduce NoRA (Non-linear Rank Adaptation), a weight-level parallel adapter that injects SiLU gating and structural dropout to induce manifold expansion. On the SlimOrca benchmark, NoRA breaks this linear barrier: NoRA remarkably at rank 64 (PPL 3.89) outperforms LoRA at rank 512 (PPL 3.90), demonstrating superior spectral efficiency. This advantage generalizes to mathematical reasoning, where NoRA achieves a perplexity of 1.97 on MathInstruct, significantly surpassing LoRA's saturation point of 2.07. Mechanism analysis via Singular Value Decomposition (SVD) confirms that NoRA activates the dormant tail of the singular value spectrum, effectively preventing the rank collapse observed in linear methods.

</details>


### [65] [Scaling Laws of Global Weather Models](https://arxiv.org/abs/2602.22962)
*Yuejiang Yu,Langwen Huang,Alexandru Calotoiu,Torsten Hoefler*

Main category: cs.LG

TL;DR: 该论文分析了天气预测数据驱动模型的缩放规律，发现Aurora模型数据缩放能力最强，GraphCast参数效率最高，且天气预测模型更倾向于增加宽度而非深度。


<details>
  <summary>Details</summary>
Motivation: 随着数据驱动模型在天气预测领域的革命性应用，需要优化训练效率和模型性能。本文旨在通过分析该领域的经验缩放规律，为模型设计提供指导。

Method: 研究模型性能（验证损失）与三个关键因素之间的关系：模型大小(N)、数据集大小(D)和计算预算(C)。分析不同模型的缩放行为，包括数据缩放、参数效率和计算最优分配。

Result: Aurora模型表现出最强的数据缩放行为：训练数据集增加10倍可使验证损失减少达3.2倍。GraphCast具有最高的参数效率但硬件利用率有限。在固定计算预算下，分配资源到更长的训练时间比增加模型大小能获得更大的性能提升。天气预测模型与语言模型的缩放行为不同，始终倾向于增加宽度而非深度。

Conclusion: 未来的天气预测模型应优先考虑更宽的架构和更大的有效训练数据集，以最大化预测性能。这些发现为天气预测模型的设计和资源分配提供了重要指导。

Abstract: Data-driven models are revolutionizing weather forecasting. To optimize training efficiency and model performance, this paper analyzes empirical scaling laws within this domain. We investigate the relationship between model performance (validation loss) and three key factors: model size ($N$), dataset size ($D$), and compute budget ($C$). Across a range of models, we find that Aurora exhibits the strongest data-scaling behavior: increasing the training dataset by 10x reduces validation loss by up to 3.2x. GraphCast demonstrates the highest parameter efficiency, yet suffers from limited hardware utilization. Our compute-optimal analysis indicates that, under fixed compute budgets, allocating resources to longer training durations yields greater performance gains than increasing model size. Furthermore, we analyze model shape and uncover scaling behaviors that differ fundamentally from those observed in language models: weather forecasting models consistently favor increased width over depth. These findings suggest that future weather models should prioritize wider architectures and larger effective training datasets to maximize predictive performance.

</details>


### [66] [Learning Disease-Sensitive Latent Interaction Graphs From Noisy Cardiac Flow Measurements](https://arxiv.org/abs/2602.23035)
*Viraj Patel,Marko Grujic,Philipp Aigner,Theodor Abart,Marcus Granegger,Deblina Bhattacharjee,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出了一种物理信息化的潜在关系框架，将心脏涡流建模为图中的交互节点，用于量化疾病严重程度和干预效果。


<details>
  <summary>Details</summary>
Motivation: 当前成像和计算方法无法捕捉心脏血流模式中相干流动特征的底层关系结构，而这些模式包含丰富的疾病严重程度和临床干预信息。

Method: 结合神经关系推理架构与物理启发的交互能量和生死动力学，将心脏涡流建模为图中的交互节点，构建对疾病严重程度和干预水平敏感的潜在图。

Result: 在主动脉缩窄的CFD模拟中，随着主动脉半径变窄，涡流相互作用变得更强更频繁，图熵与缩窄严重程度呈单调相关（R²=0.78，Spearman |ρ|=0.96）。在左心室超声数据集中，该方法成功捕捉到相干涡流结构的减弱，展示了跨模态泛化能力。

Conclusion: 潜在交互图和熵可作为心脏疾病和干预的稳健且可解释的生物标志物，该方法能够跨模态捕捉心脏血流模式的底层关系结构。

Abstract: Cardiac blood flow patterns contain rich information about disease severity and clinical interventions, yet current imaging and computational methods fail to capture underlying relational structures of coherent flow features. We propose a physics-informed, latent relational framework to model cardiac vortices as interacting nodes in a graph. Our model combines a neural relational inference architecture with physics-inspired interaction energy and birth-death dynamics, yielding a latent graph sensitive to disease severity and intervention level. We first apply this to computational fluid dynamics simulations of aortic coarctation. Learned latent graphs reveal that as the aortic radius narrows, vortex interactions become stronger and more frequent. This leads to a higher graph entropy, correlating monotonically with coarctation severity ($R^2=0.78$, Spearman $|ρ|=0.96$). We then extend this method to ultrasound datasets of left ventricles under varying levels of left ventricular assist device support. Again the latent graph representation captures the weakening of coherent vortical structures, thereby demonstrating cross-modal generalisation. Results show latent interaction graphs and entropy serve as robust and interpretable markers of cardiac disease and intervention.

</details>


### [67] [RhythmBERT: A Self-Supervised Language Model Based on Latent Representations of ECG Waveforms for Heart Disease Detection](https://arxiv.org/abs/2602.23060)
*Xin Wang,Burcu Ozek,Aruna Mohan,Amirhossein Ravari,Or Zilbershot,Fatemeh Afghah*

Main category: cs.LG

TL;DR: RhythmBERT：将心电图视为结构化语言，通过编码P、QRS、T段为符号token，结合连续嵌入保留形态细节，在单导联上实现与12导联基线相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法将ECG视为通用时间序列，忽略了生理语义和节律级结构。对比学习方法使用扭曲形态的数据增强，而生成方法采用固定窗口分割导致心脏周期不对齐。

Method: 提出RhythmBERT生成式ECG语言模型，将ECG视为语言范式：通过自编码器潜在表示将P、QRS、T段编码为符号token，同时使用连续嵌入保留细粒度形态，在约80万未标记ECG记录上进行掩码预测预训练。

Result: 尽管仅使用单导联，RhythmBERT在多种心脏疾病诊断上达到或优于强12导联基线性能，包括房颤、细微ST-T异常和心肌梗死等临床挑战性病例。

Conclusion: 将ECG视为结构化语言为心脏分析提供了可扩展且生理对齐的途径，能够有效学习上下文表示并实现标签高效学习。

Abstract: Electrocardiogram (ECG) analysis is crucial for diagnosing heart disease, but most self-supervised learning methods treat ECG as a generic time series, overlooking physiologic semantics and rhythm-level structure. Existing contrastive methods utilize augmentations that distort morphology, whereas generative approaches employ fixed-window segmentation, which misaligns cardiac cycles. To address these limitations, we propose RhythmBERT, a generative ECG language model that considers ECG as a language paradigm by encoding P, QRS, and T segments into symbolic tokens via autoencoder-based latent representations. These discrete tokens capture rhythm semantics, while complementary continuous embeddings retain fine-grained morphology, enabling a unified view of waveform structure and rhythm. RhythmBERT is pretrained on approximately 800,000 unlabeled ECG recordings with a masked prediction objective, allowing it to learn contextual representations in a label-efficient manner. Evaluations show that despite using only a single lead, RhythmBERT achieves comparable or superior performance to strong 12-lead baselines. This generalization extends from prevalent conditions such as atrial fibrillation to clinically challenging cases such as subtle ST-T abnormalities and myocardial infarction. Our results suggest that considering ECG as structured language offers a scalable and physiologically aligned pathway for advancing cardiac analysis.

</details>


### [68] [PRAC: Principal-Random Subspace for LLM Activation Compression and Memory-Efficient Training](https://arxiv.org/abs/2602.23111)
*Yanyi Li,Yimu Zhang,Cong Fang*

Main category: cs.LG

TL;DR: PRAC是一种新的LLM激活压缩方法，通过主成分子空间和随机子空间分解，实现无偏梯度估计和最小方差，显著减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 在大批量LLM训练中，激活已成为主要内存瓶颈。现有压缩方法未能充分利用激活的谱结构，导致收敛缓慢或压缩效果有限。

Method: 提出PRAC方法，将激活分解为两个组件：通过SVD捕获的主成分子空间保留主要信息，以及从正交补空间中采样的随机子空间近似尾部信息。通过引入精确的缩放因子，确保无偏梯度估计和最小方差。

Result: 在预训练和微调任务上的广泛实验表明，PRAC实现了高达36%的总内存减少，性能下降可忽略不计，计算成本最小。

Conclusion: PRAC通过有效利用激活的谱结构，提供了一种高效的内存压缩解决方案，在保持模型性能的同时显著降低训练内存需求。

Abstract: Activations have become the primary memory bottleneck in large-batch LLM training. However, existing compression methods fail to exploit the spectral structure of activations, resulting in slow convergence or limited compression. To address this, we bridge the relationship between the algorithm's fast convergence and the requirements for subspace projection, and show that an effective compression should yield an unbiased estimate of the original activation with low variance. We propose Principal-Random Subspace for LLM Activation Compression (PRAC), which novelly decomposes activations into two components: a principal subspace captured via SVD to retain dominant information, and a random subspace sampled from the orthogonal complement to approximate the tail. By introducing a precise scaling factor, we prove that PRAC yields an unbiased gradient estimator with minimum variance under certain conditions. Extensive experiments on pre-training and fine-tuning tasks demonstrate that PRAC achieves up to 36% total memory reduction with negligible performance degradation and minimal computational cost.

</details>


### [69] [DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding](https://arxiv.org/abs/2602.23135)
*Tyler Bonnet,Marek Rei*

Main category: cs.LG

TL;DR: DyGnROLE：一种基于Transformer的动态图架构，通过分离源节点和目标节点的表示，结合角色语义位置编码，在低标签场景下使用时间对比链接预测进行自监督预训练，显著提升未来边分类性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的动态图通常是有向的，源节点和目标节点表现出不对称的行为模式和时序动态。现有动态图架构大多依赖共享参数处理源节点和目标节点，缺乏系统性的角色感知建模。

Method: 提出DyGnROLE架构：1）使用独立的嵌入词汇表分离源节点和目标节点表示；2）采用角色语义位置编码捕捉每个角色特有的结构和时序上下文；3）引入时间对比链接预测（TCLP）自监督预训练目标，利用未标记的交互历史编码信息性结构偏置。

Result: 在未来边分类任务评估中，DyGnROLE显著优于多种最先进的基线方法，证明了角色感知建模在动态图学习中的有效性。

Conclusion: 角色感知建模是动态图学习的有效策略，通过显式解耦源节点和目标节点表示，结合专门的自监督预训练，能够在低标签场景下学习到更具信息量的角色特定表示。

Abstract: Real-world dynamic graphs are often directed, with source and destination nodes exhibiting asymmetrical behavioral patterns and temporal dynamics. However, existing dynamic graph architectures largely rely on shared parameters for processing source and destination nodes, with limited or no systematic role-aware modeling. We propose DyGnROLE (Dynamic Graph Node-Role-Oriented Latent Encoding), a transformer-based architecture that explicitly disentangles source and destination representations. By using separate embedding vocabularies and role-semantic positional encodings, the model captures the distinct structural and temporal contexts unique to each role. Critical to the effectiveness of these specialized embeddings in low-label regimes is a self-supervised pretraining objective we introduce: Temporal Contrastive Link Prediction (TCLP). The pretraining uses the full unlabeled interaction history to encode informative structural biases, enabling the model to learn role-specific representations without requiring annotated data. Evaluation on future edge classification demonstrates that DyGnROLE substantially outperforms a diverse set of state-of-the-art baselines, establishing role-aware modeling as an effective strategy for dynamic graph learning.

</details>


### [70] [Prediction of Diffusion Coefficients in Mixtures with Tensor Completion](https://arxiv.org/abs/2602.23142)
*Zeno Romero,Kerstin Münnemann,Hans Hasse,Fabian Jirasek*

Main category: cs.LG

TL;DR: 该研究提出了一种混合张量补全方法，用于预测二元混合物中无限稀释扩散系数的温度依赖性，结合了实验数据、半经验模型和主动学习策略。


<details>
  <summary>Details</summary>
Motivation: 混合物中扩散系数的预测在许多应用中至关重要，但实验数据稀缺。现有的矩阵补全方法仅限于单一温度预测，且准确性高度依赖于每个温度下高质量实验数据的可用性。

Method: 提出了一种混合张量补全方法，采用Tucker分解，在298K、313K和333K的实验数据上联合训练。将半经验SEGWE模型的预测作为贝叶斯训练框架的先验知识。该方法可在268K至378K之间进行线性外推。通过主动学习策略扩展实验数据库，使用脉冲场梯度核磁共振测量了19个溶质+溶剂体系的扩散系数。

Result: 与现有模型相比，该方法在所有研究温度下都显著提高了预测准确性。通过主动学习获得的额外实验数据进一步大幅提升了张量补全方法的预测精度。

Conclusion: 研究结果表明，将数据高效的机器学习方法与自适应实验相结合，具有推进传输性质预测建模的潜力。

Abstract: Predicting diffusion coefficients in mixtures is crucial for many applications, as experimental data remain scarce, and machine learning (ML) offers promising alternatives to established semi-empirical models. Among ML models, matrix completion methods (MCMs) have proven effective in predicting thermophysical properties, including diffusion coefficients in binary mixtures. However, MCMs are restricted to single-temperature predictions, and their accuracy depends strongly on the availability of high-quality experimental data for each temperature of interest. In this work, we address this challenge by presenting a hybrid tensor completion method (TCM) for predicting temperature-dependent diffusion coefficients at infinite dilution in binary mixtures. The TCM employs a Tucker decomposition and is jointly trained on experimental data for diffusion coefficients at infinite dilution in binary systems at 298 K, 313 K, and 333 K. Predictions from the semi-empirical SEGWE model serve as prior knowledge within a Bayesian training framework. The TCM then extrapolates linearly to any temperature between 268 K and 378 K, achieving markedly improved prediction accuracy compared to established models across all studied temperatures. To further enhance predictive performance, the experimental database was expanded using active learning (AL) strategies for targeted acquisition of new diffusion data by pulsed-field gradient (PFG) NMR measurements. Diffusion coefficients at infinite dilution in 19 solute + solvent systems were measured at 298 K, 313 K, and 333 K. Incorporating these results yields a substantial improvement in the TCM's predictive accuracy. These findings highlight the potential of combining data-efficient ML methods with adaptive experimentation to advance predictive modeling of transport properties.

</details>


### [71] [Partial recovery of meter-scale surface weather](https://arxiv.org/abs/2602.23146)
*Jonathan Giezendanner,Qidong Yang,Eric Schmitt,Anirban Chandra,Daniel Salles Civitarese,Johannes Jakubik,Jeremy Vila,Detlef Hohl,Campbell Watson,Sherrie Wang*

Main category: cs.LG

TL;DR: 该研究提出了一种方法，通过结合稀疏地面站观测和高分辨率地球观测数据，从现有观测中统计推断出10米分辨率的近地表气象场，显著改进了传统天气分析的精度。


<details>
  <summary>Details</summary>
Motivation: 当前天气分析和预报缺乏米尺度的近地表气象变异性信息，这种变异性由地表覆盖和地形引起，但尚不清楚这种变异性是否包含可从地表特征和大尺度大气强迫中预测的成分。

Method: 通过将粗分辨率大气状态与稀疏地面站测量和高分辨率地球观测数据相结合，推断出美国本土连续空间上10米分辨率的近地表风、温度和湿度场。

Result: 相比ERA5再分析数据，推断出的场将风误差降低了29%，温度和露点误差降低了6%，并在固定时间步长上解释了更多的空间方差。结果显示出可物理解释的结构，如城市热岛、蒸散驱动的湿度对比和不同土地覆盖类型间的风速差异。

Conclusion: 研究表明米尺度近地表天气存在可统计恢复的物理相干成分，扩展了天气建模的前沿，展示了一种计算可行的洲际尺度米分辨率推断方法，并说明通过将粗分辨率动力模型与静态精细尺度特征结合可以揭示地球系统中先前未解析的成分。

Abstract: Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.

</details>


### [72] [Benchmarking Temporal Web3 Intelligence: Lessons from the FinSurvival 2025 Challenge](https://arxiv.org/abs/2602.23159)
*Oshani Seneviratne,Fernando Spadea,Adrien Pavao,Aaron Micah Green,Kristin P. Bennett*

Main category: cs.LG

TL;DR: FinSurvival Challenge 2025作为时间Web3智能的基准测试案例研究，使用Aave v3协议的2180万笔交易记录，构建了16个生存预测任务来建模用户行为转换，展示了领域感知时间特征构建显著优于通用建模方法。


<details>
  <summary>Details</summary>
Motivation: 时间Web分析越来越依赖大规模纵向数据来理解用户、内容和系统随时间演变。尽管时间Web3（去中心化平台）提供了丰富的不可变时间戳事件流数据，但该领域缺乏能够捕捉真实世界时间动态（特别是审查和非平稳性）的共享可复现基准，这阻碍了方法学进展并限制了Web3与更广泛Web领域之间的技术转移。

Method: 使用Aave v3协议的2180万笔交易记录，设计了16个生存预测任务来建模用户行为转换，创建了时间Web3智能的基准测试。重点展示了领域感知时间特征构建方法。

Result: 领域感知时间特征构建方法在生存预测任务中显著优于通用建模方法。该基准测试为时间Web3智能提供了有效的评估框架，并展示了Web3系统作为研究时间挑战（如流失、风险和演化）的高保真沙盒的价值。

Conclusion: Web3系统为研究时间挑战提供了高保真沙盒，这些挑战对更广泛的Web领域至关重要。FinSurvival Challenge 2025展示了如何通过领域感知的时间特征构建来提升预测性能，并为下一代时间基准测试提供了重要经验。

Abstract: Temporal Web analytics increasingly relies on large-scale, longitudinal data to understand how users, content, and systems evolve over time. A rapidly growing frontier is the \emph{Temporal Web3}: decentralized platforms whose behavior is recorded as immutable, time-stamped event streams. Despite the richness of this data, the field lacks shared, reproducible benchmarks that capture real-world temporal dynamics, specifically censoring and non-stationarity, across extended horizons. This absence slows methodological progress and limits the transfer of techniques between Web3 and broader Web domains. In this paper, we present the \textit{FinSurvival Challenge 2025} as a case study in benchmarking \emph{temporal Web3 intelligence}. Using 21.8 million transaction records from the Aave v3 protocol, the challenge operationalized 16 survival prediction tasks to model user behavior transitions.We detail the benchmark design and the winning solutions, highlighting how domain-aware temporal feature construction significantly outperformed generic modeling approaches. Furthermore, we distill lessons for next-generation temporal benchmarks, arguing that Web3 systems provide a high-fidelity sandbox for studying temporal challenges, such as churn, risk, and evolution that are fundamental to the wider Web.

</details>


### [73] [Induction Meets Biology: Mechanisms of Repeat Detection in Protein Language Models](https://arxiv.org/abs/2602.23179)
*Gal Kesten-Pomeranz,Yaniv Nikankin,Anja Reusch,Tomer Tsaban,Ora Schueler-Furman,Yonatan Belinkov*

Main category: cs.LG

TL;DR: PLMs通过结合语言模式匹配和生物学专业知识来识别蛋白质序列中的精确和近似重复片段


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列中存在大量重复片段（精确和近似重复），这些重复对蛋白质结构和功能很重要，需要理解PLMs如何检测这些重复的内部机制

Method: 研究PLMs在掩码标记预测中的行为，分析其内部机制如何检测精确和近似重复，揭示两个主要阶段：特征表示构建和诱导头对齐

Result: 发现PLMs检测近似重复的机制功能上包含了检测精确重复的机制；PLMs通过通用位置注意力头和生物学专门组件构建特征表示，然后诱导头在重复片段间对齐标记

Conclusion: PLMs通过结合语言模式匹配和生物学专业知识来解决这一生物任务，为研究PLMs中更复杂的进化过程奠定了基础

Abstract: Protein sequences are abundant in repeating segments, both as exact copies and as approximate segments with mutations. These repeats are important for protein structure and function, motivating decades of algorithmic work on repeat identification. Recent work has shown that protein language models (PLMs) identify repeats, by examining their behavior in masked-token prediction. To elucidate their internal mechanisms, we investigate how PLMs detect both exact and approximate repeats. We find that the mechanism for approximate repeats functionally subsumes that of exact repeats. We then characterize this mechanism, revealing two main stages: PLMs first build feature representations using both general positional attention heads and biologically specialized components, such as neurons that encode amino-acid similarity. Then, induction heads attend to aligned tokens across repeated segments, promoting the correct answer. Our results reveal how PLMs solve this biological task by combining language-based pattern matching with specialized biological knowledge, thereby establishing a basis for studying more complex evolutionary processes in PLMs.

</details>


### [74] [Physics Informed Viscous Value Representations](https://arxiv.org/abs/2602.23280)
*Hrishikesh Viswanath,Juanwu Lu,S. Talha Bukhari,Damon Conover,Ziran Wang,Aniket Bera*

Main category: cs.LG

TL;DR: 本文提出了一种基于Hamilton-Jacobi-Bellman方程粘性解的物理信息正则化方法，用于改进离线目标条件强化学习中的价值估计问题。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习面临状态-动作空间覆盖有限导致价值估计不准确的问题。现有的基于一阶偏微分方程（如Eikonal方程）的物理信息方法在复杂高维环境中往往不适定。

Method: 提出基于Hamilton-Jacobi-Bellman方程粘性解的物理信息正则化方法，通过最优控制理论为学习过程提供物理先验，并利用Feynman-Kac定理将偏微分方程解重新表述为期望，实现可处理的蒙特卡洛估计。

Result: 实验表明该方法提高了几何一致性，可广泛应用于导航和高维复杂操作任务。开源代码已发布。

Conclusion: 通过将物理信息正则化与Hamilton-Jacobi-Bellman方程粘性解相结合，为离线目标条件强化学习提供了更稳定和准确的价值估计方法。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.

</details>


### [75] [A Proper Scoring Rule for Virtual Staining](https://arxiv.org/abs/2602.23305)
*Samuel Tonks,Steve Hood,Ryan Musso,Ceridwen Hopely,Steve Titus,Minh Doan,Iain Styles,Alexander Krull*

Main category: cs.LG

TL;DR: 提出信息增益(IG)作为生成虚拟染色模型的细胞级评估框架，直接评估预测后验分布，弥补现有方法只评估数据集边际分布的不足


<details>
  <summary>Details</summary>
Motivation: 生成虚拟染色模型能为高通量筛选提供生物特征的预测后验分布，但现有评估方法只能检查数据集边际分布的准确性，无法直接评估预测后验分布的质量

Method: 引入信息增益(IG)作为严格适当评分规则，提供理论动机支持可解释性，允许跨模型和特征比较结果

Result: 在广泛的高通量筛选数据集上评估基于扩散和GAN的模型，IG能揭示其他指标无法检测的显著性能差异

Conclusion: IG为生成虚拟染色模型提供了有效的细胞级后验分布评估框架，具有理论严谨性和实际应用价值

Abstract: Generative virtual staining (VS) models for high-throughput screening (HTS) can provide an estimated posterior distribution of possible biological feature values for each input and cell. However, when evaluating a VS model, the true posterior is unavailable. Existing evaluation protocols only check the accuracy of the marginal distribution over the dataset rather than the predicted posteriors. We introduce information gain (IG) as a cell-wise evaluation framework that enables direct assessment of predicted posteriors. IG is a strictly proper scoring rule and comes with a sound theoretical motivation allowing for interpretability, and for comparing results across models and features. We evaluate diffusion- and GAN-based models on an extensive HTS dataset using IG and other metrics and show that IG can reveal substantial performance differences other metrics cannot.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [76] [Cognitive Models and AI Algorithms Provide Templates for Designing Language Agents](https://arxiv.org/abs/2602.22523)
*Ryan Liu,Dilip Arumugam,Cedegao E. Zhang,Sean Escola,Xaq Pitkow,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 该论文提出从认知模型和AI算法中寻找设计模块化语言智能体的蓝图，通过定义智能体模板来指导多个LLM的组合与协作。


<details>
  <summary>Details</summary>
Motivation: 当前单个大型语言模型仍难以解决许多复杂问题，需要探索如何将多个LLM作为组件组合成更强大的整体。现有研究在如何有效组合多个LLM方面仍存在不确定性。

Method: 提出智能体模板的概念，用于定义个体LLM的角色及其功能组合方式。通过文献调研，分析现有语言智能体，揭示其底层模板直接来源于认知模型或AI算法。

Result: 展示了多种现有语言智能体的设计模式，这些模式都源于认知科学和AI算法。通过形式化智能体模板，为开发模块化语言智能体提供了系统化框架。

Conclusion: 认知科学和AI算法启发的智能体模板是开发高效、可解释语言智能体的有力工具，应更多关注这类设计模式的研究与应用。

Abstract: While contemporary large language models (LLMs) are increasingly capable in isolation, there are still many difficult problems that lie beyond the abilities of a single LLM. For such tasks, there is still uncertainty about how best to take many LLMs as parts and combine them into a greater whole. This position paper argues that potential blueprints for designing such modular language agents can be found in the existing literature on cognitive models and artificial intelligence (AI) algorithms. To make this point clear, we formalize the idea of an agent template that specifies roles for individual LLMs and how their functionalities should be composed. We then survey a variety of existing language agents in the literature and highlight their underlying templates derived directly from cognitive models or AI algorithms. By highlighting these designs, we aim to call attention to agent templates inspired by cognitive science and AI as a powerful tool for developing effective, interpretable language agents.

</details>


### [77] [Agentic AI for Intent-driven Optimization in Cell-free O-RAN](https://arxiv.org/abs/2602.22539)
*Mohammad Hossein Shokouhi,Vincent W. S. Wong*

Main category: cs.AI

TL;DR: 提出一个基于智能体的AI框架，用于无蜂窝O-RAN中的意图翻译和优化，通过多个LLM智能体协作处理复杂意图，实现节能和资源管理。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注由独立智能体处理的简单意图，而需要多个智能体协调的复杂意图尚未得到充分探索。O-RAN架构为部署和协调此类智能体提供了可能，但缺乏处理复杂意图的有效框架。

Method: 提出一个多智能体AI框架：1) 监督智能体将运营商意图翻译为优化目标和最低速率要求；2) 用户权重智能体从记忆模块检索先验经验确定用户优先级权重；3) 节能模式下激活O-RU管理智能体，使用DRL算法确定活跃O-RU集合；4) 监控智能体测量用户数据速率并协调其他智能体保证最低速率要求；5) 采用参数高效微调(PEFT)方法使同一底层LLM适用于不同智能体。

Result: 仿真结果显示：1) 在节能模式下，相比三种基线方案，所提框架将活跃O-RU数量减少41.93%；2) 使用PEFT方法相比部署单独的LLM智能体，内存使用减少92%。

Conclusion: 该智能体AI框架能够有效处理无蜂窝O-RAN中的复杂意图，通过多智能体协作实现节能优化，同时PEFT方法显著提高了系统的可扩展性。

Abstract: Agentic artificial intelligence (AI) is emerging as a key enabler for autonomous radio access networks (RANs), where multiple large language model (LLM)-based agents reason and collaborate to achieve operator-defined intents. The open RAN (O-RAN) architecture enables the deployment and coordination of such agents. However, most existing works consider simple intents handled by independent agents, while complex intents that require coordination among agents remain unexplored. In this paper, we propose an agentic AI framework for intent translation and optimization in cell-free O-RAN. A supervisor agent translates the operator intents into an optimization objective and minimum rate requirements. Based on this information, a user weighting agent retrieves relevant prior experience from a memory module to determine the user priority weights for precoding. If the intent includes an energy-saving objective, then an open radio unit (O-RU) management agent will also be activated to determine the set of active O-RUs by using a deep reinforcement learning (DRL) algorithm. A monitoring agent measures and monitors the user data rates and coordinates with other agents to guarantee the minimum rate requirements are satisfied. To enhance scalability, we adopt a parameter-efficient fine-tuning (PEFT) method that enables the same underlying LLM to be used for different agents. Simulation results show that the proposed agentic AI framework reduces the number of active O-RUs by 41.93% when compared with three baseline schemes in energy-saving mode. Using the PEFT method, the proposed framework reduces the memory usage by 92% when compared with deploying separate LLM agents.

</details>


### [78] [Requesting Expert Reasoning: Augmenting LLM Agents with Learned Collaborative Intervention](https://arxiv.org/abs/2602.22546)
*Zhiming Wang,Jinwei He,Feng Lu*

Main category: cs.AI

TL;DR: AHCE框架通过主动学习如何向人类专家请求推理，而非简单求助，显著提升LLM智能体在专业领域的任务成功率


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在通用推理方面表现出色，但在需要长尾知识的专业领域往往失败。虽然人类专家可以提供这些缺失知识，但他们的指导通常是非结构化和不可靠的，难以直接整合到智能体的规划中。

Method: 提出了AHCE（主动人类增强挑战参与）框架，用于按需进行人机协作。其核心是人类反馈模块（HFM），通过学习策略将人类专家视为交互式推理工具。

Result: 在Minecraft中的大量实验表明该框架非常有效：在普通难度任务上任务成功率提高了32%，在极高难度任务上提高了近70%，且只需最少的人类干预。

Conclusion: 成功增强智能体需要学习如何请求专家推理，而不仅仅是简单的求助请求。AHCE框架通过主动学习与人类专家的交互方式，显著提升了专业领域任务的成功率。

Abstract: Large Language Model (LLM) based agents excel at general reasoning but often fail in specialized domains where success hinges on long-tail knowledge absent from their training data. While human experts can provide this missing knowledge, their guidance is often unstructured and unreliable, making its direct integration into an agent's plan problematic. To address this, we introduce AHCE (Active Human-Augmented Challenge Engagement), a framework for on-demand Human-AI collaboration. At its core, the Human Feedback Module (HFM) employs a learned policy to treat the human expert as an interactive reasoning tool. Extensive experiments in Minecraft demonstrate the framework's effectiveness, increasing task success rates by 32% on normal difficulty tasks and nearly 70% on highly difficult tasks, all with minimal human intervention. Our work demonstrates that successfully augmenting agents requires learning how to request expert reasoning, moving beyond simple requests for help.

</details>


### [79] [CourtGuard: A Model-Agnostic Framework for Zero-Shot Policy Adaptation in LLM Safety](https://arxiv.org/abs/2602.22557)
*Umid Suleymanov,Rufiz Bayramov,Suad Gafarli,Seljan Musayeva,Taghi Mammadov,Aynur Akhundlu,Murat Kantarcioglu*

Main category: cs.AI

TL;DR: CourtGuard是一个基于检索增强的多智能体框架，将安全评估重新构想为证据辩论，通过对抗性辩论实现零样本适应性，无需微调即可达到SOTA安全性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全机制主要依赖静态微调分类器，存在适应性僵化问题，无法在不进行昂贵重新训练的情况下强制执行新的治理规则。

Method: 引入CourtGuard框架，采用检索增强的多智能体架构，将安全评估重新构想为基于外部政策文件的证据辩论，通过对抗性辩论来评估模型安全性。

Result: 在7个安全基准测试中达到最先进性能，超越专用政策遵循基线；在零样本适应性测试中，通过替换参考政策在维基百科破坏检测任务上达到90%准确率；能够自动策划和审计9个新颖的对抗攻击数据集。

Conclusion: 将安全逻辑与模型权重解耦为AI治理提供了一条稳健、可解释且适应性强的路径，能够满足当前和未来的监管要求。

Abstract: Current safety mechanisms for Large Language Models (LLMs) rely heavily on static, fine-tuned classifiers that suffer from adaptation rigidity, the inability to enforce new governance rules without expensive retraining. To address this, we introduce CourtGuard, a retrieval-augmented multi-agent framework that reimagines safety evaluation as Evidentiary Debate. By orchestrating an adversarial debate grounded in external policy documents, CourtGuard achieves state-of-the-art performance across 7 safety benchmarks, outperforming dedicated policy-following baselines without fine-tuning. Beyond standard metrics, we highlight two critical capabilities: (1) Zero-Shot Adaptability, where our framework successfully generalized to an out-of-domain Wikipedia Vandalism task (achieving 90\% accuracy) by swapping the reference policy; and (2) Automated Data Curation and Auditing, where we leveraged CourtGuard to curate and audit nine novel datasets of sophisticated adversarial attacks. Our results demonstrate that decoupling safety logic from model weights offers a robust, interpretable, and adaptable path for meeting current and future regulatory requirements in AI governance.

</details>


### [80] [Strategy Executability in Mathematical Reasoning: Leveraging Human-Model Differences for Effective Guidance](https://arxiv.org/abs/2602.22583)
*Weida Liang,Yiyou Sun,Shuyuan Nan,Chuang Li,Dawn Song,Kenji Kawaguchi*

Main category: cs.AI

TL;DR: 论文发现示例引导在数学推理中的效果不稳定源于策略使用率与策略可执行性之间的差距，提出了选择性策略检索框架来提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 示例引导在数学推理中广泛使用，但其效果在不同问题和模型间极不稳定，即使引导正确且与问题相关。这种不稳定性源于策略使用率（策略是否出现在成功解中）与策略可执行性（策略作为引导时是否仍有效）之间未被充分探索的差距。

Method: 通过对比分析人工编写和模型生成的解决方案，识别使用率与可执行性之间的系统性差异。在此基础上提出选择性策略检索框架，通过经验性、多路径、源感知信号来选择性检索和组合策略，显式建模可执行性。

Result: 在多个数学推理基准测试中，选择性策略检索框架相比直接求解、上下文学习和单源引导，实现了可靠且一致的改进。在AIME25上准确率提升高达13个百分点，在Apex上提升5个百分点，尤其对紧凑推理模型效果显著。

Conclusion: 策略使用率与可执行性之间的差距是示例引导效果不稳定的关键原因，选择性策略检索框架通过显式建模可执行性，能够有效提升数学推理性能，为推理引导提供了更可靠的方法。

Abstract: Example-based guidance is widely used to improve mathematical reasoning at inference time, yet its effectiveness is highly unstable across problems and models-even when the guidance is correct and problem-relevant. We show that this instability arises from a previously underexplored gap between strategy usage-whether a reasoning strategy appears in successful solutions-and strategy executability-whether the strategy remains effective when instantiated as guidance for a target model. Through a controlled analysis of paired human-written and model-generated solutions, we identify a systematic dissociation between usage and executability: human- and model-derived strategies differ in structured, domain-dependent ways, leading to complementary strengths and consistent source-dependent reversals under guidance. Building on this diagnosis, we propose Selective Strategy Retrieval (SSR), a test-time framework that explicitly models executability by selectively retrieving and combining strategies using empirical, multi-route, source-aware signals. Across multiple mathematical reasoning benchmarks, SSR yields reliable and consistent improvements over direct solving, in-context learning, and single-source guidance, improving accuracy by up to $+13$ points on AIME25 and $+5$ points on Apex for compact reasoning models. Code and benchmark are publicly available at: https://github.com/lwd17/strategy-execute-pipeline.

</details>


### [81] [Correcting Human Labels for Rater Effects in AI Evaluation: An Item Response Theory Approach](https://arxiv.org/abs/2602.22585)
*Jodi M. Casabianca,Maggie Beiting-Parrish*

Main category: cs.AI

TL;DR: 该论文提出将心理测量学中的评分者模型整合到AI评估流程中，通过项目反应理论（特别是多面Rasch模型）分离真实输出质量与评分者行为偏差，从而提高人类评估数据的可靠性和有效性。


<details>
  <summary>Details</summary>
Motivation: 人类评估在AI模型的训练和评估中起着核心作用，但这些数据很少被视为存在系统误差的测量。当前AI评估中的人类评分数据存在评分者效应（如严格性和中心性偏差），这些偏差扭曲了观察到的评分，导致基于原始评分的决策可能不准确。

Method: 采用心理测量学中的项目反应理论评分者模型，特别是多面Rasch模型，将真实输出质量与评分者行为分离。使用OpenAI摘要数据集作为实证案例，展示如何通过调整评分者严格性来校正摘要质量估计，并提供评分者表现的诊断性洞察。

Result: 调整评分者严格性后产生了校正后的摘要质量估计，提供了对评分者表现的诊断性洞察。这种方法使得开发者能够基于调整后的分数而非原始、易错的评分做出决策。

Conclusion: 将心理测量建模整合到人机协同评估中，能够更原则性、更透明地使用人类数据。这一视角为AI开发和评估提供了更稳健、可解释且与构念对齐的实践路径，提高了人类评估数据的可靠性和有效性。

Abstract: Human evaluations play a central role in training and assessing AI models, yet these data are rarely treated as measurements subject to systematic error. This paper integrates psychometric rater models into the AI pipeline to improve the reliability and validity of conclusions drawn from human judgments. The paper reviews common rater effects, severity and centrality, that distort observed ratings, and demonstrates how item response theory rater models, particularly the multi-faceted Rasch model, can separate true output quality from rater behavior. Using the OpenAI summarization dataset as an empirical example, we show how adjusting for rater severity produces corrected estimates of summary quality and provides diagnostic insight into rater performance. Incorporating psychometric modeling into human-in-the-loop evaluation offers more principled and transparent use of human data, enabling developers to make decisions based on adjusted scores rather than raw, error-prone ratings. This perspective highlights a path toward more robust, interpretable, and construct-aligned practices for AI development and evaluation.

</details>


### [82] [AHBid: An Adaptable Hierarchical Bidding Framework for Cross-Channel Advertising](https://arxiv.org/abs/2602.22650)
*Xinxin Yang,Yangyang Tang,Yikun Zhou,Yaolei Liu,Yun Li,Bo Yang*

Main category: cs.AI

TL;DR: AHBid是一个用于在线广告多渠道自动出价的分层框架，结合生成式规划和实时控制，相比现有基线提升13.57%的整体回报


<details>
  <summary>Details</summary>
Motivation: 在线广告环境复杂动态，多渠道场景下预算和约束分配困难。现有优化方法缺乏动态适应性，强化学习方法难以捕捉历史依赖和观察模式

Method: 提出AHBid分层出价框架：高层使用扩散模型生成规划器动态分配预算和约束，包含约束执行机制和轨迹精炼机制；底层采用基于控制的出价算法结合历史知识和实时信息

Result: 在大规模离线数据集和在线A/B测试中验证有效性，相比现有基线实现13.57%的整体回报提升

Conclusion: AHBid框架通过结合生成式规划和实时控制，有效解决了多渠道自动出价中的动态适应性和历史依赖捕捉问题，显著提升了广告投放效果

Abstract: In online advertising, the inherent complexity and dynamic nature of advertising environments necessitate the use of auto-bidding services to assist advertisers in bid optimization. This complexity is further compounded in multi-channel scenarios, where effective allocation of budgets and constraints across channels with distinct behavioral patterns becomes critical for optimizing return on investment. Current approaches predominantly rely on either optimization-based strategies or reinforcement learning techniques. However, optimization-based methods lack flexibility in adapting to dynamic market conditions, while reinforcement learning approaches often struggle to capture essential historical dependencies and observational patterns within the constraints of Markov Decision Process frameworks. To address these limitations, we propose AHBid, an Adaptable Hierarchical Bidding framework that integrates generative planning with real-time control. The framework employs a high-level generative planner based on diffusion models to dynamically allocate budgets and constraints by effectively capturing historical context and temporal patterns. We introduce a constraint enforcement mechanism to ensure compliance with specified constraints, along with a trajectory refinement mechanism that enhances adaptability to environmental changes through the utilization of historical data. The system further incorporates a control-based bidding algorithm that synergistically combines historical knowledge with real-time information, significantly improving both adaptability and operational efficacy. Extensive experiments conducted on large-scale offline datasets and through online A/B tests demonstrate the effectiveness of AHBid, yielding a 13.57% increase in overall return compared to existing baselines.

</details>


### [83] [Toward Personalized LLM-Powered Agents: Foundations, Evaluation, and Future Directions](https://arxiv.org/abs/2602.22680)
*Yue Xu,Qian Chen,Zizhan Ma,Dongrui Liu,Wenxuan Wang,Xiting Wang,Li Xiong,Wenjie Wang*

Main category: cs.AI

TL;DR: 这篇论文是关于个性化LLM智能体的综述，围绕四个核心组件（用户画像建模、记忆、规划、行动执行）系统梳理了相关方法、评估指标和应用场景，为构建更用户对齐、自适应、可部署的智能体系统提供结构化框架。


<details>
  <summary>Details</summary>
Motivation: 随着LLM智能体在长期交互中需要适应用户个体差异并保持连续性，个性化成为关键需求。现有研究缺乏对个性化LLM智能体能力的系统性梳理，需要建立结构化框架来理解各组件如何协同工作，推动从原型到实际部署的进展。

Method: 采用能力导向的综述方法，围绕四个相互依赖的组件组织文献：1) 用户画像建模（如何表示用户特征），2) 记忆（如何存储和检索用户相关信息），3) 规划（如何基于用户信息制定决策），4) 行动执行（如何将个性化决策转化为具体行动）。分析用户信号在各组件间的表示、传播和利用方式。

Result: 建立了个性化LLM智能体的结构化分类框架，识别了跨组件交互模式和常见设计权衡，总结了专门针对个性化智能体的评估指标和基准测试，梳理了从通用助手到专业领域的应用场景，为系统设计和研究提供了清晰路线图。

Conclusion: 个性化LLM智能体需要贯穿整个决策流程的系统性设计，而不仅仅是表层生成。该综述提供的框架将加速从原型个性化到可扩展实际助手的进展，推动构建更用户对齐、自适应、鲁棒且可部署的智能体系统。

Abstract: Large language models have enabled agents that reason, plan, and interact with tools and environments to accomplish complex tasks. As these agents operate over extended interaction horizons, their effectiveness increasingly depends on adapting behavior to individual users and maintaining continuity across time, giving rise to personalized LLM-powered agents. In such long-term, user-dependent settings, personalization permeates the entire decision pipeline rather than remaining confined to surface-level generation. This survey provides a capability-oriented review of personalized LLM-powered agents. We organize the literature around four interdependent components: profile modeling, memory, planning, and action execution. Using this taxonomy, we synthesize representative methods and analyze how user signals are represented, propagated, and utilized, highlighting cross-component interactions and recurring design trade-offs. We further examine evaluation metrics and benchmarks tailored to personalized agents, summarize application scenarios spanning general assistance to specialized domains, and outline future directions for research and deployment. By offering a structured framework for understanding and designing personalized LLM-powered agents, this survey charts a roadmap toward more user-aligned, adaptive, robust, and deployable agentic systems, accelerating progress from prototype personalization to scalable real-world assistants.

</details>


### [84] [RLHFless: Serverless Computing for Efficient RLHF](https://arxiv.org/abs/2602.22718)
*Rui Wei,Hanfei Yu,Shubham Jain,Yogarajan Sivakumar,Devesh Tiwari,Jian Li,Seung-Jong Park,Hao Wang*

Main category: cs.AI

TL;DR: RLHFless：首个基于无服务器计算的可扩展同步RLHF训练框架，通过动态资源适配、前缀预计算和成本感知的actor扩展策略，实现1.35倍加速和44.8%成本降低


<details>
  <summary>Details</summary>
Motivation: 传统RLHF框架基于服务器基础设施，难以应对RL训练中动态变化的资源需求，导致组件间空闲时间和资源浪费问题

Method: 1) 基于无服务器计算环境构建；2) 适应RLHF流水线的动态资源需求；3) 预计算共享前缀避免重复计算；4) 成本感知的actor扩展策略考虑响应长度变化；5) 高效分配工作负载减少函数内不平衡和空闲时间

Result: 在物理测试平台和大规模模拟集群上的实验显示，相比最先进的基线方法，RLHFless实现了最高1.35倍的加速和44.8%的成本降低

Conclusion: RLHFless通过无服务器计算架构有效解决了同步RLHF训练中的资源利用效率问题，为大规模语言模型对齐提供了更高效、成本更低的训练框架

Abstract: Reinforcement Learning from Human Feedback (RLHF) has been widely applied to Large Language Model (LLM) post-training to align model outputs with human preferences. Recent models, such as DeepSeek-R1, have also shown RLHF's potential to improve LLM reasoning on complex tasks. In RL, inference and training co-exist, creating dynamic resource demands throughout the workflow. Compared to traditional RL, RLHF further challenges training efficiency due to expanding model sizes and resource consumption. Several RLHF frameworks aim to balance flexible abstraction and efficient execution. However, they rely on serverful infrastructures, which struggle with fine-grained resource variability. As a result, during synchronous RLHF training, idle time between or within RL components often causes overhead and resource wastage.
  To address these issues, we present RLHFless, the first scalable training framework for synchronous RLHF, built on serverless computing environments. RLHFless adapts to dynamic resource demands throughout the RLHF pipeline, pre-computes shared prefixes to avoid repeated computation, and uses a cost-aware actor scaling strategy that accounts for response length variation to find sweet spots with lower cost and higher speed. In addition, RLHFless assigns workloads efficiently to reduce intra-function imbalance and idle time. Experiments on both physical testbeds and a large-scale simulated cluster show that RLHFless achieves up to 1.35x speedup and 44.8% cost reduction compared to the state-of-the-art baseline.

</details>


### [85] [Generative Data Transformation: From Mixed to Unified Data](https://arxiv.org/abs/2602.22743)
*Jiaqing Zhang,Mingjia Yin,Hao Wang,Yuxin Tian,Yuyang Ye,Yawen Li,Wei Guo,Yong Liu,Enhong Chen*

Main category: cs.AI

TL;DR: Taesar是一个数据中心的跨域序列推荐框架，通过对比解码机制将跨域上下文编码到目标域序列中，解决数据稀疏和冷启动问题，避免负迁移。


<details>
  <summary>Details</summary>
Motivation: 传统模型中心方法依赖复杂的定制架构，难以捕捉跨域的细微非结构化序列依赖，导致泛化能力差且计算资源需求高。跨域数据存在领域差异，可能导致负迁移和模型性能下降。

Method: 提出Taesar框架，采用对比解码机制自适应地将跨域上下文编码到目标域序列中，生成增强的数据集，使标准模型能够学习复杂依赖而无需复杂融合架构。

Result: 实验表明Taesar优于模型中心解决方案，能够泛化到各种序列模型，有效结合了数据中心和模型中心范式的优势。

Conclusion: Taesar通过数据中心的序列再生方法解决了跨域推荐中的负迁移问题，为推荐系统提供了一种更高效、泛化能力更强的解决方案。

Abstract: Recommendation model performance is intrinsically tied to the quality, volume, and relevance of their training data. To address common challenges like data sparsity and cold start, recent researchs have leveraged data from multiple auxiliary domains to enrich information within the target domain. However, inherent domain gaps can degrade the quality of mixed-domain data, leading to negative transfer and diminished model performance. Existing prevailing \emph{model-centric} paradigm -- which relies on complex, customized architectures -- struggles to capture the subtle, non-structural sequence dependencies across domains, leading to poor generalization and high demands on computational resources. To address these shortcomings, we propose \textsc{Taesar}, a \emph{data-centric} framework for \textbf{t}arget-\textbf{a}lign\textbf{e}d \textbf{s}equenti\textbf{a}l \textbf{r}egeneration, which employs a contrastive decoding mechanism to adaptively encode cross-domain context into target-domain sequences. It employs contrastive decoding to encode cross-domain context into target sequences, enabling standard models to learn intricate dependencies without complex fusion architectures. Experiments show \textsc{Taesar} outperforms model-centric solutions and generalizes to various sequential models. By generating enriched datasets, \textsc{Taesar} effectively combines the strengths of data- and model-centric paradigms. The code accompanying this paper is available at~ \textcolor{blue}{https://github.com/USTC-StarTeam/Taesar}.

</details>


### [86] [Decomposing Physician Disagreement in HealthBench](https://arxiv.org/abs/2602.22758)
*Satya Borgohain,Roy Mariathas*

Main category: cs.AI

TL;DR: 研究分析了HealthBench医疗AI评估数据集中医生意见分歧的来源，发现大部分分歧（81.8%）来自病例层面的残差，无法通过现有元数据、专业领域或表面特征解释，但可减少的不确定性（如信息缺失）会显著增加分歧概率。


<details>
  <summary>Details</summary>
Motivation: 理解医疗AI评估中医生意见分歧的来源，识别哪些因素可以解释分歧，哪些是结构性限制，从而为改进医疗AI评估设计提供依据。

Method: 通过分解HealthBench数据集中的医生分歧，分析评估标准、医生身份、病例特征、元数据标签、医学专业、表面特征、嵌入表示等因素对分歧的影响，并使用统计方法（如方差分析、AUC、比值比）量化各因素的贡献。

Result: 1. 病例层面残差占分歧方差的81.8%，无法通过现有解释变量显著减少；2. 评估标准仅解释3.6-6.9%的分歧方差；3. 医生身份仅占2.4%；4. 可减少的不确定性（缺失上下文、模糊表述）使分歧几率增加2.55倍；5. 不可减少的不确定性（真正的医学模糊性）无显著影响；6. 分歧与完成质量呈倒U型关系（AUC=0.689）。

Conclusion: 医疗AI评估中的一致性上限主要是结构性的，但可减少与不可减少不确定性的分离表明，通过填补评估场景中的信息空白可以降低分歧（当不存在固有的临床模糊性时），这为改进评估设计提供了可操作的途径。

Abstract: We decompose physician disagreement in the HealthBench medical AI evaluation dataset to understand where variance resides and what observable features can explain it. Rubric identity accounts for 15.8% of met/not-met label variance but only 3.6-6.9% of disagreement variance; physician identity accounts for just 2.4%. The dominant 81.8% case-level residual is not reduced by HealthBench's metadata labels (z = -0.22, p = 0.83), normative rubric language (pseudo R^2 = 1.2%), medical specialty (0/300 Tukey pairs significant), surface-feature triage (AUC = 0.58), or embeddings (AUC = 0.485). Disagreement follows an inverted-U with completion quality (AUC = 0.689), confirming physicians agree on clearly good or bad outputs but split on borderline cases. Physician-validated uncertainty categories reveal that reducible uncertainty (missing context, ambiguous phrasing) more than doubles disagreement odds (OR = 2.55, p < 10^(-24)), while irreducible uncertainty (genuine medical ambiguity) has no effect (OR = 1.01, p = 0.90), though even the former explains only ~3% of total variance. The agreement ceiling in medical AI evaluation is thus largely structural, but the reducible/irreducible dissociation suggests that closing information gaps in evaluation scenarios could lower disagreement where inherent clinical ambiguity does not, pointing toward actionable evaluation design improvements.

</details>


### [87] [AMA-Bench: Evaluating Long-Horizon Memory for Agentic Applications](https://arxiv.org/abs/2602.22769)
*Yujie Zhao,Boqin Yuan,Junbo Huang,Haocheng Yuan,Zhongming Yu,Haozhou Xu,Lanxiang Hu,Abhilash Shankarampeta,Zimeng Huang,Wentao Ni,Yuandong Tian,Jishen Zhao*

Main category: cs.AI

TL;DR: AMA-Bench：评估LLM在真实智能体应用中的长时记忆能力，并提出AMA-Agent记忆系统，通过因果图与工具增强检索显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前智能体记忆评估主要关注人机对话场景，而实际应用中智能体记忆是连续的机器生成交互流，需要更贴近真实应用的评估标准

Method: 提出AMA-Bench评估框架，包含真实世界智能体轨迹与专家标注QA，以及可扩展到任意时长的合成轨迹与规则生成QA；并提出AMA-Agent记忆系统，采用因果图和工具增强检索

Result: 现有记忆系统在AMA-Bench上表现不佳，主要缺乏因果性和目标信息，且受限于基于相似性的检索损失；AMA-Agent达到57.22%平均准确率，比最强基线提升11.16%

Conclusion: AMA-Bench填补了智能体记忆评估的空白，AMA-Agent通过因果建模和工具增强检索有效解决了现有记忆系统的局限性，为长时记忆智能体发展提供了重要基准和解决方案

Abstract: Large Language Models (LLMs) are deployed as autonomous agents in increasingly complex applications, where enabling long-horizon memory is critical for achieving strong performance. However, a significant gap exists between practical applications and current evaluation standards for agent memory: existing benchmarks primarily focus on dialogue-centric, human-agent interactions. In reality, agent memory consists of a continuous stream of agent-environment interactions that are primarily composed of machine-generated representations. To bridge this gap, we introduce AMA-Bench (Agent Memory with Any length), which evaluates long-horizon memory for LLMs in real agentic applications. It features two key components: (1) a set of real-world agentic trajectories across representative agentic applications, paired with expert-curated QA, and (2) a set of synthetic agentic trajectories that scale to arbitrary horizons, paired with rule-based QA. Our comprehensive study shows that existing memory systems underperform on AMA-Bench primarily because they lack causality and objective information and are constrained by the lossy nature of similarity-based retrieval employed by many memory systems. To address these limitations, we propose AMA-Agent, an effective memory system featuring a causality graph and tool-augmented retrieval. Our results demonstrate that AMA-Agent achieves 57.22% average accuracy on AMA-Bench, surpassing the strongest memory system baselines by 11.16%.

</details>


### [88] [ClinDet-Bench: Beyond Abstention, Evaluating Judgment Determinability of LLMs in Clinical Decision-Making](https://arxiv.org/abs/2602.22771)
*Yusuke Watanabe,Yohei Kobashi,Takeshi Kojima,Yusuke Iwasawa,Yasushi Okuno,Yutaka Matsuo*

Main category: cs.AI

TL;DR: LLMs在临床不完全信息场景下无法准确判断何时可做决策、何时应弃权，存在过早判断和过度弃权问题，现有基准不足以评估临床安全性。


<details>
  <summary>Details</summary>
Motivation: 临床决策常在信息不完全情况下进行，专家需要判断可用信息是否足够做出判断。过早结论和不必要的弃权都会危及患者安全，需要评估LLMs在这方面的能力。

Method: 开发ClinDet-Bench基准，基于临床评分系统将不完全信息场景分解为可确定和不可确定条件。识别可确定性需要考虑所有关于缺失信息的假设（包括不太可能的），并验证结论是否在所有情况下都成立。

Result: 最近的LLMs无法在不完全信息下识别可确定性，既产生过早判断又过度弃权，尽管它们能正确解释基础评分知识并在完全信息下表现良好。

Conclusion: 现有基准不足以评估LLMs在临床环境中的安全性。ClinDet-Bench为评估可确定性识别提供了框架，促进适当的弃权决策，在医学和其他高风险领域具有潜在适用性。

Abstract: Clinical decisions are often required under incomplete information. Clinical experts must identify whether available information is sufficient for judgment, as both premature conclusion and unnecessary abstention can compromise patient safety. To evaluate this capability of large language models (LLMs), we developed ClinDet-Bench, a benchmark based on clinical scoring systems that decomposes incomplete-information scenarios into determinable and undeterminable conditions. Identifying determinability requires considering all hypotheses about missing information, including unlikely ones, and verifying whether the conclusion holds across them. We find that recent LLMs fail to identify determinability under incomplete information, producing both premature judgments and excessive abstention, despite correctly explaining the underlying scoring knowledge and performing well under complete information. These findings suggest that existing benchmarks are insufficient to evaluate the safety of LLMs in clinical settings. ClinDet-Bench provides a framework for evaluating determinability recognition, leading to appropriate abstention, with potential applicability to medicine and other high-stakes domains, and is publicly available.

</details>


### [89] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个高性能、鲁棒的开源智能体框架，通过智能体图灵活编排、深度推理模式和鲁棒工作流执行，在多个智能体基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得了显著进展，但独立LLM在处理需要与外部工具和环境交互的复杂现实任务时能力开始达到瓶颈。现有智能体框架存在工作流简单、性能不稳定、基准支持有限、过度依赖商业API等问题。

Method: 提出MiroFlow框架，包含三个核心组件：1）智能体图实现灵活编排；2）可选的深度推理模式提升性能；3）鲁棒的工作流执行确保稳定和可复现性能。

Result: 在多个智能体基准测试（GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch、FutureX）中一致达到最先进的性能水平。

Conclusion: MiroFlow可作为深度学习社区易于访问、可复现、可比较的基准框架，为智能体研究提供可靠的基础设施。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [90] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS是一个用于质谱预测的基准框架，支持构建和评估多种深度学习模型架构，提供性能影响因素分析和实际应用指导。


<details>
  <summary>Details</summary>
Motivation: 质谱技术在药物发现和材料科学中至关重要，但实验光谱数据缺乏阻碍了分子识别，需要建立计算预测方法。深度学习模型在预测分子结构光谱方面有潜力，但方法异质性和缺乏明确基准使整体评估具有挑战性。

Method: 创建FlexMS基准框架，支持动态构建多种不同模型架构组合，在预处理公共数据集上使用不同指标评估性能。分析影响性能的因素，包括数据集结构多样性、超参数、预训练效果、元数据消融设置和跨域迁移学习。

Result: FlexMS框架提供了模型性能评估的标准化方法，揭示了影响预测性能的关键因素，包括数据集特性、超参数选择和预训练策略。检索基准模拟了实际识别场景，基于预测光谱对潜在匹配进行评分。

Conclusion: FlexMS为质谱预测提供了实用的基准框架和模型选择指导，通过系统评估不同架构和影响因素，推动了该领域标准化发展，有助于实际应用中的模型选择。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [91] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: SPM-Bench：一个针对扫描探针显微镜的博士级多模态基准测试，通过自动化数据合成管道和Anchor-Gated Sieve技术从arXiv和期刊论文中提取高质量图像-文本对，使用混合云-本地架构实现高效数据处理，并引入SIP-F1评分来评估LLM性能并量化模型"个性"。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在专业科学领域存在明显差距，现有基准测试存在数据污染、复杂度不足和人工成本过高的问题。需要创建一个专门针对扫描探针显微镜（SPM）的高质量、低成本的基准测试。

Method: 1. 开发全自动数据合成管道，确保高权威性和低成本；2. 使用Anchor-Gated Sieve技术从2023-2025年的arXiv和期刊论文中高效提取高价值图像-文本对；3. 采用混合云-本地架构，VLMs仅返回空间坐标"llbox"用于本地高保真裁剪，实现极端令牌节省；4. 引入严格不完美惩罚F1（SIP-F1）评分来评估LLM性能。

Result: 该基准测试能够建立严格的LLM能力层次结构，首次量化模型"个性"（保守型、激进型、赌徒型或智慧型），并通过与模型报告置信度和感知难度的相关性分析，揭示当前AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为自动化科学数据合成提供了一个可推广的范式，能够准确评估LLM在专业科学领域的性能，并为理解AI在复杂物理场景中的推理能力提供了新的方法论。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [92] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 该研究提出了一种诊断对齐框架，通过系统比较AI生成的影像报告与医生验证结果，量化临床AI中专家验证的结构化信号，发现传统二元词汇评估显著低估了临床意义对齐。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床AI中，人机协同验证至关重要，但模型初始推理与专家修正之间的过渡很少被分析为结构化信号。需要一种能够量化校正动态、支持可追溯的人类对齐评估的方法。

Method: 引入诊断对齐框架：将AI生成的影像报告保存为不可变的推理状态，与医生验证结果系统比较。推理流程集成视觉大语言模型、基于BERT的医学实体提取和顺序语言模型推理(SLMI)步骤，在专家评审前强制执行领域一致的细化。使用四级一致性框架评估：精确主要匹配率(PMR)、语义相似性调整率(AMR)、跨类别对齐和综合一致性率(CCR)。

Result: 在21个皮肤病案例评估中：精确一致性达71.4%，语义相似性下保持不变(t=0.60)；结构化跨类别和鉴别诊断重叠分析产生100%综合一致性(95% CI: [83.9%, 100%])；无案例显示完全诊断分歧。表明二元词汇评估显著低估临床意义对齐。

Conclusion: 将专家验证建模为结构化转换，能够实现信号感知的校正动态量化，支持基于影像的临床决策支持系统的可追溯、人类对齐评估。该方法揭示了传统评估方法的局限性，为临床AI验证提供了更精细的分析框架。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [93] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: 提出RepSPD模型，通过黎曼流形上的交叉注意力机制和全局双向对齐策略，改进基于对称正定矩阵的脑电图解码方法，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的脑电图分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，需要更全面的几何深度学习方法来改进脑电图解码

Method: 提出RepSPD模型：1) 在黎曼流形上实现交叉注意力机制，用图导出的功能连接特征调制SPD的几何属性；2) 引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真

Result: 大量实验表明，该框架显著优于现有脑电图表示方法，展现出优越的鲁棒性和泛化能力

Conclusion: RepSPD通过结合黎曼几何和功能连接特征，有效改进了脑电图解码，为神经科学和临床应用提供了更强大的工具

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [94] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 该论文提出CC-BOS框架，利用文言文的简洁性和模糊性自动生成对抗性提示，通过多维度果蝇优化算法在黑盒设置下进行高效的越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的安全风险日益受到关注，现有研究表明LLMs容易受到越狱攻击，且攻击效果在不同语言环境下存在差异。文言文因其简洁性和模糊性可能绕过现有安全约束，暴露LLMs的漏洞。

Method: 提出CC-BOS框架，将提示编码为八个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式和上下文），通过果蝇优化的嗅觉搜索、视觉搜索和柯西变异进行迭代优化，并设计了文言文到英文的翻译模块以提高可读性和评估准确性。

Result: 大量实验表明，CC-BOS框架在越狱攻击中始终优于现有的最先进方法，证明了文言文对抗性提示在黑盒攻击中的有效性。

Conclusion: 文言文因其语言特性能够部分绕过LLMs的安全约束，CC-BOS框架通过自动化生成文言文对抗性提示，有效提升了黑盒越狱攻击的效率和效果，揭示了LLMs在文言文处理方面的安全漏洞。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [95] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的多智能体F1赛车策略优化方法，通过交互模块和自博弈训练生成竞争性策略，能够根据对手行为调整进站时机、轮胎选择和能量分配。


<details>
  <summary>Details</summary>
Motivation: F1比赛中，车队需要根据不断变化的比赛条件和竞争对手的行动来调整比赛策略。传统策略制定方法难以实时应对复杂的多智能体交互环境，因此需要开发能够适应对手行为的智能策略优化系统。

Method: 基于预训练的单智能体策略，引入交互模块来考虑竞争对手的行为。结合交互模块和自博弈训练方案生成竞争性策略，并根据相对性能对智能体进行排名。框架仅使用实际比赛中可用的信息。

Result: 智能体能够根据对手行为自适应调整进站时机、轮胎选择和能量分配，实现了稳健且一致的比赛表现。该方法能够支持比赛策略师在赛前和赛中做出决策。

Conclusion: 提出的强化学习方法成功解决了F1多智能体比赛策略优化问题，通过交互模块和自博弈训练生成了适应对手行为的竞争性策略，为实际比赛策略制定提供了有效支持。

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [96] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 本文提出AILS-AHD方法，利用大语言模型动态生成和优化破坏启发式，在容量限制车辆路径问题上取得显著性能提升，在大型基准测试中创造了8个新的最优解。


<details>
  <summary>Details</summary>
Motivation: CVRP作为组合优化基础问题，其NP-hard特性在大规模实例中带来显著计算挑战，传统方法面临性能瓶颈，需要创新解决方案。

Method: 提出AILS-AHD方法，将进化搜索框架与大语言模型结合，动态生成和优化破坏启发式，并引入基于LLM的加速机制提升计算效率。

Result: 在CVRPLib大规模基准测试中，AILS-AHD在10个实例中的8个创造了新的最优解，性能优于AILS-II和HGS等先进求解器。

Conclusion: LLM驱动的启发式设计在车辆路径优化领域具有巨大潜力，AILS-AHD方法为解决大规模CVRP问题提供了创新且有效的解决方案。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [97] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET系统使用多智能体LLM进行情感去毒化，通过四个智能体分析、调整、监控和指导信息消费，显著降低情感刺激同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容使消费者暴露于过度情感刺激，阻碍冷静决策。需要一种既能减少情感刺激又不限制访问原始文本的信息净化系统。

Method: 提出MALLET多智能体情感去毒化系统，包含四个智能体：情感分析智能体使用6情感BERT分类器量化刺激强度；情感调整智能体使用LLM将文本重写为BALANCED（中性化文本）和COOL（中性化文本+补充文本）两种呈现模式；平衡监控智能体聚合每周信息消费模式并生成个性化建议；个人指导智能体根据消费者敏感度推荐呈现模式。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低（最高19.3%），情感平衡改善，同时保持语义保存；刺激减少与语义保存之间的相关性接近零，证实两者可独立控制；类别分析显示体育、商业、科技类刺激大幅减少（17.8-33.8%），而世界类效果有限，因为事实本身具有高刺激性。

Conclusion: MALLET系统为支持消费者冷静接收信息提供了框架，无需限制访问原始文本，实现了情感刺激减少与语义保存的独立控制。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [98] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: PATRA模型通过模式感知机制提取时间序列的趋势和季节性模式，并设计任务感知平衡奖励来协调不同难度任务的学习，在时间序列问答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列推理方法存在两个局限：1) 将时间序列简单视为文本或图像，无法捕捉回答特定问题所需的趋势和季节性等模式；2) 在混合简单和复杂任务训练时，简单目标主导学习过程，阻碍深度推理能力发展。

Method: 提出PATRA模型，包含：1) 模式感知机制，从时间序列中提取趋势和季节性模式以实现深度对齐；2) 任务感知平衡奖励，协调不同难度任务的学习，激励生成连贯的思维链。

Result: 大量实验表明，PATRA在多样化时间序列问答任务中优于强基线模型，展现出卓越的跨模态理解和推理能力。

Conclusion: PATRA通过模式感知对齐和平衡推理机制，有效解决了现有LLM方法在时间序列推理中的局限性，实现了更深入的模式理解和推理能力。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [99] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个为单细胞基础模型设计的自然语言评估框架，通过虚拟细胞抽象统一评估目标，包含五种自然语言任务，并引入知识增强评估以克服传统指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中LLM评估实践不足：现有基准测试分散在不同任务中，采用多项选择等与现实使用脱节的形式，依赖缺乏可解释性和生物学基础的指标。

Method: 提出SC-ARENA框架，包含：1）虚拟细胞抽象统一评估目标；2）五种自然语言任务（细胞类型注释、描述、生成、扰动预测、科学问答）；3）知识增强评估，整合外部本体、标记数据库和科学文献。

Result: 实验表明：1）在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均，特别是在需要机制或因果理解的任务上；2）知识增强评估确保生物学正确性，提供可解释、证据支持的推理，具有高区分能力。

Conclusion: SC-ARENA为单细胞生物学中的LLM评估提供了统一且可解释的框架，指向开发生物学对齐、可泛化的基础模型。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [100] [Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive](https://arxiv.org/abs/2602.23239)
*Radha Sarma*

Main category: cs.AI

TL;DR: 论文证明基于优化的AI系统（如RLHF训练的LLM）在形式上无法实现规范性治理，因为它们缺乏真正智能体所需的不相容性和否定响应性两个必要条件，导致其失败模式是结构性的而非偶然的。


<details>
  <summary>Details</summary>
Motivation: AI系统在医疗、法律、金融等高风险领域被部署时，人们假设它们可以被规范性治理。本文旨在证明这种假设对于基于优化的系统（特别是通过RLHF训练的大语言模型）在形式上是无效的。

Method: 通过形式化分析，建立真正智能体所需的两个必要且充分的架构条件：1）不相容性 - 将某些边界视为不可协商的约束而非可交易的权重；2）否定响应性 - 当这些边界受到威胁时能够暂停处理的非推理机制。然后证明RLHF系统在构成上与这两个条件不相容。

Result: RLHF系统与规范性治理在形式上不相容，其失败模式（奉承、幻觉、不忠实的推理）不是偶然事故而是结构性表现。此外，部署不当会引发"趋同危机"：人类在指标压力下验证AI输出时，会从真正的智能体退化为标准检查优化器，消除系统中唯一具有规范性问责能力的组件。

Conclusion: 基于优化的AI系统在架构上无法实现规范性治理，这不是可纠正的技术缺陷，而是优化本身的固有形式约束。论文的主要积极贡献是提供了一个与基质无关的架构规范，定义了任何系统（生物、人工或制度）要成为智能体而非复杂工具必须满足的条件。

Abstract: AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains.
  RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations.
  Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.

</details>


### [101] [A Model-Free Universal AI](https://arxiv.org/abs/2602.23242)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 该论文提出了AIQI，这是首个在通用强化学习中被证明具有渐近ε最优性的无模型智能体，通过分布动作值函数的通用归纳实现，突破了传统基于模型方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有通用强化学习中的最优智能体（如AIXI）都是基于模型的，需要显式维护和使用环境模型。这限制了智能体的多样性，且模型方法在实际应用中可能面临复杂性和计算负担的问题。因此需要探索无模型方法是否也能在通用强化学习中实现最优性。

Method: 提出AIQI（Universal AI with Q-Induction）智能体，采用无模型方法，通过对分布动作值函数进行通用归纳，而不是像先前工作那样对策略或环境进行归纳。在"grain of truth"条件下，证明了AIQI的渐近最优性。

Result: 证明了AIQI是强渐近ε最优的，并且是渐近ε贝叶斯最优的。这一结果显著扩展了已知通用智能体的多样性，首次展示了无模型方法在通用强化学习中的理论最优性保证。

Conclusion: AIQI是首个被证明在通用强化学习中具有渐近最优性的无模型智能体，通过分布动作值函数的通用归纳实现了这一突破。这一工作扩展了通用智能体的理论框架，为无模型方法在通用强化学习中的应用提供了理论基础。

Abstract: In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\varepsilon$-optimal and asymptotically $\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.

</details>


### [102] [Mitigating Legibility Tax with Decoupled Prover-Verifier Games](https://arxiv.org/abs/2602.23248)
*Yegon Kim,Juho Lee*

Main category: cs.AI

TL;DR: 提出解耦证明者-验证者游戏框架，通过训练"翻译器"模型将固定求解器的输出转换为可验证形式，避免可读性税问题


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力增强，需要确保其输出能被较弱系统轻松验证。现有证明者-验证者游戏虽能提高可验证性，但会导致准确性下降（可读性税问题）

Method: 将正确性与可验证性条件解耦，训练一个"翻译器"模型，将固定求解器模型的解决方案转换为可验证形式。先训练求解器最大化正确性，再训练翻译器将求解器输出转换为可验证形式同时保留原始答案

Result: 提出解耦的证明者-验证者游戏框架，其均衡对应忠实且可验证的翻译器，解决了可读性税问题

Conclusion: 通过解耦正确性与可验证性训练，可以同时获得高正确性和高可验证性，避免传统方法中的准确性折衷

Abstract: As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a "translator" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.

</details>


### [103] [CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays](https://arxiv.org/abs/2602.23276)
*Hyungyung Lee,Hangyul Yoon,Edward Choi*

Main category: cs.AI

TL;DR: CXReasonAgent是一个结合大语言模型与临床诊断工具的智能体，通过图像衍生的诊断和视觉证据进行基于证据的诊断推理，解决了传统大视觉语言模型在胸部X光诊断中缺乏可靠证据基础和可验证性的问题。


<details>
  <summary>Details</summary>
Motivation: 胸部X光在胸部诊断中具有核心地位，其解读需要多步骤、基于证据的推理。然而，现有的大视觉语言模型（LVLMs）存在以下问题：1）生成的回答虽然看似合理但缺乏可靠的诊断证据基础；2）提供的视觉证据有限，难以验证；3）需要昂贵的重新训练来支持新的诊断任务，限制了在临床环境中的可靠性和适应性。

Method: 提出了CXReasonAgent诊断智能体，将大语言模型（LLM）与临床基础诊断工具相结合，使用图像衍生的诊断和视觉证据进行基于证据的诊断推理。同时引入了CXReasonDial多轮对话基准，包含1,946个对话，涵盖12个诊断任务。

Result: CXReasonAgent能够生成忠实基于证据的响应，相比大视觉语言模型（LVLMs）实现了更可靠和可验证的诊断推理。通过CXReasonDial基准评估验证了该方法的有效性。

Conclusion: 在安全关键的临床环境中，整合临床基础诊断工具至关重要。CXReasonAgent通过结合LLM与诊断工具，提供了更可靠、可验证的诊断推理能力，为临床决策支持系统的发展提供了重要方向。

Abstract: Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.

</details>


### [104] [ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks](https://arxiv.org/abs/2602.23285)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Rikuto Kotoge,Jathurshan Pradeepkumar,Yasuko Matsubara,Jimeng Sun,Yasushi Sakurai,Takashi Matsubara*

Main category: cs.AI

TL;DR: ODEBRAIN：基于神经ODE的脑电动态预测框架，通过整合时空频特征到谱图节点，建模连续潜在动态，显著提升EEG动态预测性能


<details>
  <summary>Details</summary>
Motivation: 传统潜变量方法通常通过循环架构离散化时间建模连续脑动态，这必然导致累积预测误差的复合效应，并且无法捕捉EEG的瞬时非线性特征。需要克服这些挑战来更好地建模神经群体动态。

Method: 提出ODEBRAIN框架：1）将时空频特征整合到谱图节点中；2）使用神经ODE建模连续潜在动态；3）确保潜表示能够捕捉任意时间点上复杂脑状态的随机变化。

Result: 大量实验验证表明，ODEBRAIN在预测EEG动态方面显著优于现有方法，具有增强的鲁棒性和泛化能力。

Conclusion: ODEBRAIN通过神经ODE框架成功克服了传统方法的局限性，能够更好地建模神经群体动态，为神经科学基础研究和临床应用提供了有效工具。

Abstract: Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.

</details>


### [105] [The logic of KM belief update is contained in the logic of AGM belief revision](https://arxiv.org/abs/2602.23302)
*Giacomo Bonanno*

Main category: cs.AI

TL;DR: 该研究将KM信念更新的公理转化为包含三个模态算子的模态逻辑公理，并与AGM信念修正的模态逻辑进行比较，证明AGM信念修正是KM信念更新的特例。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于形式化KM信念更新理论，将其公理转化为模态逻辑框架，并与AGM信念修正理论进行系统比较，以阐明两者之间的关系。

Method: 为KM信念更新的每个公理提供对应的模态逻辑公理，使用三个模态算子：单模态信念算子B、双模态条件算子>和单模态必然算子□。然后将得到的逻辑与从AGM公理转换而来的类似逻辑进行比较。

Result: 证明AGM信念修正逻辑包含KM信念更新逻辑，即KM逻辑的每个公理都是AGM逻辑的定理。对于强版本KM信念更新，两者差异可归结为处理非意外信息的单个公理。

Conclusion: AGM信念修正是KM信念更新的一个特例，两者在模态逻辑框架下具有包含关系，强版本的主要差异仅在于处理非意外信息的公理。

Abstract: For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\mathcal L_{AGM}$ and the former by $\mathcal L_{KM}$ we show that every axiom of $\mathcal L_{KM}$ is a theorem of $\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\mathcal L_{KM}$ and $\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.

</details>


### [106] [Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction](https://arxiv.org/abs/2602.23315)
*Sha Hu*

Main category: cs.AI

TL;DR: 提出基于重采样的推理方法，通过对输入进行不变变换生成多个版本，聚合推理结果以提高准确性


<details>
  <summary>Details</summary>
Motivation: 即使经过优化的AI模型也会因偶然性和认知不确定性而产生推理错误，观察到基于输入不变变换的多次推理中，错误存在部分独立性

Method: 提出"重采样"推理方法：对训练好的AI模型，应用输入的多个变换版本进行推理，然后聚合推理输出得到更准确的结果

Result: 该方法有潜力提高推理准确性，并提供平衡模型大小和性能的策略

Conclusion: 利用认知不确定性导致的推理错误部分独立性，通过重采样和聚合方法可以改善AI模型的推理性能

Abstract: An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a "resampling" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.

</details>


### [107] [Generalized Rapid Action Value Estimation in Memory-Constrained Environments](https://arxiv.org/abs/2602.23318)
*Aloïs Rautureau,Tristan Cazenave,Éric Piette*

Main category: cs.AI

TL;DR: GRAVE2、GRAVER和GRAVER2算法通过两级搜索和节点回收技术，在保持GRAVE算法游戏强度的同时大幅减少了内存使用


<details>
  <summary>Details</summary>
Motivation: GRAVE算法在通用游戏博弈中表现出色，但其需要在每个节点存储额外的胜率/访问统计数据，在内存受限环境中不实用，限制了实际应用

Method: 提出了三种新算法：GRAVE2（通过两级搜索扩展GRAVE）、GRAVER（通过节点回收扩展GRAVE）、GRAVER2（结合两级搜索和节点回收技术）

Result: 这些增强技术能够显著减少存储节点数量，同时匹配GRAVE的游戏强度

Conclusion: 通过两级搜索和节点回收技术，可以在保持GRAVE算法性能的同时解决其内存消耗问题，提高了在内存受限环境中的实用性

Abstract: Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.

</details>
