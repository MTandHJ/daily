<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 19]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.LG](#cs.LG) [Total: 55]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [Reason to Contrast: A Cascaded Multimodal Retrieval Framework](https://arxiv.org/abs/2602.23369)
*Xuanming Cui,Hong-You Chen,Hao Yu,Hao Yuan,Zihao Wang,Shlok Kumar Mishra,Hanchao Yu,Yonghuan Yang,Jun Xiao,Ser-Nam Lim,Jianpeng Cheng,Qi Guo,Xiangjun Fan*

Main category: cs.IR

TL;DR: TTE-v2是一个混合多模态检索框架，通过引入基于额外输入token预算的推理驱动性能扩展，而非依赖模型或嵌入大小，实现了新的性能突破。


<details>
  <summary>Details</summary>
Motivation: 传统多模态检索系统主要依赖双编码器架构，性能与嵌入维度紧密相关。虽然TTE通过引入多模态推理在嵌入前生成额外信息token来改进检索，但仍需探索更有效的性能扩展范式。

Method: 提出TTE-v2混合多模态检索框架，采用级联设计：1）初始多模态检索；2）基于额外推理步骤的重新排序阶段，实现更丰富的查询-候选交互；3）重新排序阶段为硬负样本挖掘和假负样本过滤提供细粒度监督，形成反馈循环增强上游检索器。

Result: 在MMEB-V2基准测试中，TTE-v2-7B达到75.7%的最新最高准确率，TTE-v2-2B匹配或超越使用显著更大外部数据训练的领先7B模型，展示了基于token扩展的替代扩展范式的潜力。

Conclusion: TTE-v2通过推理驱动的性能扩展范式，基于中间推理token缩放实现显著的测试时改进，证明了token级扩展作为多模态检索替代扩展范式的潜力。

Abstract: Traditional multimodal retrieval systems rely primarily on bi-encoder architectures, where performance is closely tied to embedding dimensionality. Recent work, Think-Then-Embed (TTE), shows that incorporating multimodal reasoning to elicit additional informative tokens before embedding can further improve retrieval. In this paper, we extend this paradigm with TTE-v2, a hybrid multimodal retrieval framework that introduces reasoning-driven performance scaling based on additional input token budget rather than model or embedding size. Our approach augments the initial multimodal retrieval with additional reasoning steps for reranking, enabling more expressive query-candidate interactions at test time. The reranking stage further provides fine-grained supervision for hard negative mining and false negative filtering, creating a feedback loop that effectively strengthens the upstream retriever. This cascaded design delivers substantial test-time improvements based on intermediate reasoning token scaling. Experiments on the MMEB-V2 benchmark demonstrate that TTE-v2-7B achieves a new state-of-the-art accuracy of 75.7%, and that TTE-v2-2B matches or surpasses leading 7B models trained with significantly larger external data. Our results highlight the promise of token-wise scaling as an alternative scaling paradigm for multimodal retrieval.

</details>


### [2] [Democratizing GraphRAG: Linear, CPU-Only Graph Retrieval for Multi-Hop QA](https://arxiv.org/abs/2602.23372)
*Qizhi Wang*

Main category: cs.IR

TL;DR: SPRIG是一个CPU-only、线性时间、无需token的GraphRAG系统，使用NER驱动的共现图和个性化PageRank替代昂贵的LLM图构建，在保持Recall@10基本不变的情况下提升28%召回率。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG系统依赖昂贵的LLM图构建和GPU密集型推理，限制了其普及应用。需要开发更高效、成本更低的GraphRAG解决方案。

Method: 提出SPRIG系统：1) 使用轻量级NER驱动的共现图替代LLM图构建；2) 采用个性化PageRank进行检索；3) 实现CPU-only、线性时间、token-free的管道。

Result: SPRIG在保持Recall@10基本不变的情况下，将召回率提升28%。系统分析了CPU友好图检索何时有助于多跳召回，以及何时强词汇混合方法(RRF)已足够。

Conclusion: SPRIG为GraphRAG的民主化提供了现实路径，无需token成本或GPU要求，同时保持了检索性能。

Abstract: GraphRAG systems improve multi-hop retrieval by modeling structure, but many approaches rely on expensive LLM-based graph construction and GPU-heavy inference. We present SPRIG (Seeded Propagation for Retrieval In Graphs), a CPU-only, linear-time, token-free GraphRAG pipeline that replaces LLM graph building with lightweight NER-driven co-occurrence graphs and uses Personalized PageRank (PPR) for 28% with negligible Recall@10 changes. The results characterize when CPU-friendly graph retrieval helps multi-hop recall and when strong lexical hybrids (RRF) are sufficient, outlining a realistic path to democratizing GraphRAG without token costs or GPU requirements.

</details>


### [3] [Higress-RAG: A Holistic Optimization Framework for Enterprise Retrieval-Augmented Generation via Dual Hybrid Retrieval, Adaptive Routing, and CRAG](https://arxiv.org/abs/2602.23374)
*Weixi Lin*

Main category: cs.IR

TL;DR: Higress RAG MCP Server采用"全链路优化"策略解决企业级RAG系统三大挑战：复杂查询检索精度低、生成阶段幻觉率高、实时应用延迟不可接受。


<details>
  <summary>Details</summary>
Motivation: 企业知识管理系统集成LLM时面临RAG范式从概念验证到生产级部署的三大瓶颈：复杂查询检索精度不足、生成阶段幻觉率高、实时应用延迟不可接受，需要系统化解决方案。

Method: 基于Model Context Protocol构建分层架构，采用"全链路优化"策略，包括自适应路由、语义缓存、混合检索和纠正式RAG。关键技术包括Higress-Native Splitter结构化数据摄取、互惠排名融合整合稠密稀疏检索信号、50ms延迟语义缓存动态阈值机制。

Result: 在Higress技术文档和博客的领域特定数据集上验证了架构鲁棒性，通过优化从检索前查询重写到检索后纠正评估的整个检索生命周期，实现了可扩展、抗幻觉的企业AI部署方案。

Conclusion: Higress RAG MCP Server通过全链路优化策略有效解决了企业级RAG系统的核心挑战，为生产级RAG部署提供了可扩展、低延迟、抗幻觉的解决方案。

Abstract: The integration of Large Language Models (LLMs) into enterprise knowledge management systems has been catalyzed by the Retrieval-Augmented Generation (RAG) paradigm, which augments parametric memory with non-parametric external data. However, the transition from proof-of-concept to production-grade RAG systems is hindered by three persistent challenges: low retrieval precision for complex queries, high rates of hallucination in the generation phase, and unacceptable latency for real-time applications. This paper presents a comprehensive analysis of the Higress RAG MCP Server, a novel, enterprise-centric architecture designed to resolve these bottlenecks through a "Full-Link Optimization" strategy. Built upon the Model Context Protocol (MCP), the system introduces a layered architecture that orchestrates a sophisticated pipeline of Adaptive Routing, Semantic Caching, Hybrid Retrieval, and Corrective RAG (CRAG). We detail the technical implementation of key innovations, including the Higress-Native Splitter for structure-aware data ingestion, the application of Reciprocal Rank Fusion (RRF) for merging dense and sparse retrieval signals, and a 50ms-latency Semantic Caching mechanism with dynamic thresholding. Experimental evaluations on domain-specific Higress technical documentation and blogs verify the system's architectural robustness. The results demonstrate that by optimizing the entire retrieval lifecycle - from pre-retrieval query rewriting to post-retrieval corrective evaluation - the Higress RAG system offers a scalable, hallucination-resistant solution for enterprise AI deployment.

</details>


### [4] [Cross-Representation Knowledge Transfer for Improved Sequential Recommendations](https://arxiv.org/abs/2602.23471)
*Artur Gimranov,Viacheslav Yusupov,Elfat Sabitov,Tatyana Matveeva,Anton Lysenko,Ruslan Israfilov,Evgeny Frolov*

Main category: cs.IR

TL;DR: 提出结合Transformer和GNN的新框架，同时编码交互图的结构依赖并跟踪其动态变化，在多个数据集上优于纯序列、纯图及现有混合方法。


<details>
  <summary>Details</summary>
Motivation: Transformer在序列推荐中虽能捕捉序列依赖，但孤立考虑序列元素，隐式处理复杂关系；GNN虽能显式建模高阶交互关系，但难以充分捕捉随时间演化的动态变化。需要结合两者优势解决下一项预测任务。

Method: 提出新框架结合Transformer和GNN，对齐不同表示。同时编码交互图中的结构依赖并跟踪其动态变化，解决下一项预测任务。

Result: 在多个开放数据集上的实验结果表明，所提框架在推荐质量上一致优于纯序列方法、纯图方法以及最近结合两种信号的方法。

Conclusion: 结合Transformer和GNN的框架能有效解决序列推荐中的结构依赖建模和动态变化跟踪问题，显著提升推荐性能。

Abstract: Transformer architectures, capable of capturing sequential dependencies in the history of user interactions, have become the dominant approach in sequential recommender systems. Despite their success, such models consider sequence elements in isolation, implicitly accounting for the complex relationships between them. Graph neural networks, in contrast, explicitly model these relationships through higher order interactions but are often unable to adequately capture their evolution over time, limiting their use for predicting the next interaction. To fill this gap, we present a new framework that combines transformers and graph neural networks and aligns different representations for solving next-item prediction task. Our solution simultaneously encodes structural dependencies in the interaction graph and tracks their dynamic change. Experimental results on a number of open datasets demonstrate that the proposed framework consistently outperforms both pure sequential and graph approaches in terms of recommendation quality, as well as recent methods that combine both types of signals.

</details>


### [5] [Unified Learning-to-Rank for Multi-Channel Retrieval in Large-Scale E-Commerce Search](https://arxiv.org/abs/2602.23530)
*Aditya Gaydhani,Guangyue Xu,Dhanush Kamath,Ankit Singh,Alex Li*

Main category: cs.IR

TL;DR: 该论文提出了一种统一的多通道融合排序模型，将电商搜索中的多通道文档合并问题重新定义为查询相关的学习排序任务，通过联合优化点击、加购和购买等业务指标，显著提升了用户转化率。


<details>
  <summary>Details</summary>
Motivation: 大规模电商搜索需要从海量商品目录中检索多样化的商品，包括畅销品、新品、趋势品和季节性商品。现有系统依赖多个专业检索通道来满足不同目标，但传统的基于排名的融合方法（如RRF和加权交织）使用固定的全局通道权重，无法考虑查询特定的通道效用和跨通道交互，限制了业务KPI（如用户转化）的优化效果。

Method: 将多通道融合问题重新定义为查询相关的学习排序任务，提出统一的排序模型来学习和合并来自多个检索通道的文档。该方法将问题构建为通道感知的学习排序任务，联合优化点击、加购和购买等业务指标，同时融入通道特定的目标。进一步整合了近期用户行为信号来捕捉短期意图变化，这对提升多通道排序中的转化率至关重要。

Result: 在线A/B实验表明，该方法显著优于基于排名的融合方法，用户转化率提升了+2.85%。模型满足生产延迟要求，p95延迟低于50毫秒，并已在Target.com上部署。

Conclusion: 该研究证明了将多通道融合重新定义为查询相关的学习排序问题的有效性，提出的统一模型能够更好地捕捉查询特定的通道效用和跨通道交互，在满足严格延迟约束的同时显著提升了电商搜索的业务指标。

Abstract: Large-scale e-commerce search must surface a broad set of items from a vast catalog, ranging from bestselling products to new, trending, or seasonal items. Modern systems therefore rely on multiple specialized retrieval channels to surface products, each designed to satisfy a specific objective. A key challenge is how to effectively merge documents from these heterogeneous channels into a single ranked list under strict latency constraints while optimizing for business KPIs such as user conversion. Rank-based fusion methods such as Reciprocal Rank Fusion (RRF) and Weighted Interleaving rely on fixed global channel weights and treat channels independently, failing to account for query-specific channel utility and cross-channel interactions. We observe that multi-channel fusion can be reformulated as a query-dependent learning-to-rank problem over heterogeneous candidate sources. In this paper, we propose a unified ranking model that learns to merge and rank documents from multiple retrieval channels. We formulate the problem as a channel-aware learning-to-rank task that jointly optimizes clicks, add-to-carts, and purchases while incorporating channel-specific objectives. We further incorporate recent user behavioral signals to capture short-term intent shifts that are critical for improving conversion in multi-channel ranking. Our online A/B experiments show that the proposed approach outperforms rank-based fusion methods, leading to a +2.85\% improvement in user conversion. The model satisfies production latency requirements, achieving a p95 latency of under 50\,ms, and is deployed on Target.com.

</details>


### [6] [Synthetic Data Powers Product Retrieval for Long-tail Knowledge-Intensive Queries in E-commerce Search](https://arxiv.org/abs/2602.23620)
*Gui Ling,Weiyuan Li,Yue Jiang,Wenjun Peng,Xingxian Liu,Dongshuai Li,Fuyu Lv,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 该论文提出了一种针对长尾、知识密集型查询的电商产品检索数据合成框架，通过将强大的离线查询重写模型能力蒸馏到在线检索系统中，显著提升了长尾查询的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有电商检索系统在处理长尾、知识密集型查询时面临挑战：这些查询语言模式多样、缺乏明确购买意图、需要领域知识推理，且缺乏可靠的行为日志数据，导致检索效果不佳。

Method: 提出高效的数据合成框架，通过LLM的强大语言理解能力训练多候选查询重写模型，使用多奖励信号，并通过强大的离线检索流程捕获其重写能力，生成高质量的查询-产品对合成数据。

Result: 实验表明，仅将合成数据纳入检索模型训练即可带来显著改进。在线A/B测试的人类评估结果显示，用户搜索体验得到明显提升。

Conclusion: 该框架有效解决了长尾、知识密集型查询的检索难题，通过隐式蒸馏强大离线模型能力到在线系统，改善了电商搜索的整体质量和用户体验。

Abstract: Product retrieval is the backbone of e-commerce search: for each user query, it identifies a high-recall candidate set from billions of items, laying the foundation for high-quality ranking and user experience. Despite extensive optimization for mainstream queries, existing systems still struggle with long-tail queries, especially knowledge-intensive ones. These queries exhibit diverse linguistic patterns, often lack explicit purchase intent, and require domain-specific knowledge reasoning for accurate interpretation. They also suffer from a shortage of reliable behavioral logs, which makes such queries a persistent challenge for retrieval optimization. To address these issues, we propose an efficient data synthesis framework tailored to retrieval involving long-tail, knowledge-intensive queries. The key idea is to implicitly distill the capabilities of a powerful offline query-rewriting model into an efficient online retrieval system. Leveraging the strong language understanding of LLMs, we train a multi-candidate query rewriting model with multiple reward signals and capture its rewriting capability in well-curated query-product pairs through a powerful offline retrieval pipeline. This design mitigates distributional shift in rewritten queries, which might otherwise limit incremental recall or introduce irrelevant products. Experiments demonstrate that without any additional tricks, simply incorporating this synthetic data into retrieval model training leads to significant improvements. Online Side-By-Side (SBS) human evaluation results indicate a notable enhancement in user search experience.

</details>


### [7] [Learning to Reflect and Correct: Towards Better Decoding Trajectories for Large-Scale Generative Recommendation](https://arxiv.org/abs/2602.23639)
*Haibo Xing,Hao Deng,Lingyu Mu,Jinxin Hu,Yu Zhang,Xiaoyi Zeng,Jing Zhang*

Main category: cs.IR

TL;DR: GRC框架通过引入结构化反思-修正机制，将生成式推荐从单次解码扩展为生成-反思-修正过程，显著提升推荐质量


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐模型通常采用单次解码，缺乏显式修正机制，导致早期偏差累积并最终降低推荐质量

Method: 提出GRC框架：1) 监督式反思-修正模板，将解码分解为初始草稿生成、多粒度反思和反思引导修正；2) 基于GRPO的强化学习优化整个GRC轨迹；3) 熵引导的反思调度策略动态分配修正资源

Result: 在真实数据集上比6个最先进基线提升高达15.74%，在线A/B测试显示广告收入提升1.79%，仅带来适度延迟开销

Conclusion: GRC是首个结构化反思-修正框架，通过扩展标准解码过程实现生成式推荐的质量提升，在大规模工业推荐中具有重要实践价值

Abstract: Generative Recommendation (GR) has become a promising paradigm for large-scale recommendation systems. However, existing GR models typically perform single-pass decoding without explicit refinement, causing early deviations to accumulate and ultimately degrade recommendation quality. To tackle this problem, we propose GRC, which is, to our knowledge, the first structured reflection-correction framework for GR that extends standard decoding into a Generation-Reflection-Correction (GRC) process. Concretely, GRC introduces a supervised reflection-correction template that decomposes the decoding process into initial draft generation, multi-granular reflection, and reflection-guided correction, thereby enabling structured reflection and correction in the semantic token space. To further explore the enlarged refinement space introduced by the GRC process, we optimize the entire GRC trajectory with GRPO-based reinforcement learning, under a carefully designed reward function with token-level and trajectory-level signals. For efficient online serving, we propose an Entropy-Guided Reflection Scheduling (EGRS) strategy that dynamically allocates more correction budget to high-uncertainty decoding trajectories during beam search. Extensive experiments on real-world datasets show that GRC consistently outperforms six state-of-the-art baselines by up to 15.74%, and online A/B tests demonstrate its substantial practical value in large-scale industrial recommendation, delivering a 1.79% lift in advertising revenue with only modest latency overhead.

</details>


### [8] [Geodesic Semantic Search: Learning Local Riemannian Metrics for Citation Graph Retrieval](https://arxiv.org/abs/2602.23665)
*Brandon Yee,Lucas Wang,Kundana Kommini,Krishna Sharma*

Main category: cs.IR

TL;DR: GSS是一种基于黎曼度量的检索系统，通过学习引用图中节点特定的度量张量，实现几何感知的语义搜索，相比传统欧氏距离方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于嵌入的检索方法使用固定的欧氏距离，无法捕捉复杂语义关系。GSS旨在通过学习节点特定的黎曼度量，在引用图上实现更准确的几何感知语义搜索。

Method: 1. 为每个节点学习低秩度量张量mL_i，构造局部半正定度量mG_i = mL_i mL_i^T + εI；2. 使用多源Dijkstra算法计算测地距离；3. 结合最大边际相关性重排序和路径一致性过滤；4. 采用分层粗到细搜索和k-means池化降低计算成本。

Result: 在包含16.9万篇论文的引用预测基准测试中，GSS相比SPECTER+FAISS基线在Recall@20上获得23%的相对提升。分层搜索将计算成本降低4倍，同时保持97%的检索质量。

Conclusion: GSS通过学习节点特定的黎曼度量实现了有效的几何感知语义搜索，在性能提升的同时提供可解释的引用路径。该方法在理论和实证上都验证了测地距离优于直接相似度的场景。

Abstract: We present Geodesic Semantic Search (GSS), a retrieval system that learns node-specific Riemannian metrics on citation graphs to enable geometry-aware semantic search. Unlike standard embedding-based retrieval that relies on fixed Euclidean distances, \gss{} learns a low-rank metric tensor $\mL_i \in \R^{d \times r}$ at each node, inducing a local positive semi-definite metric $\mG_i = \mL_i \mL_i^\top + \eps \mI$. This parameterization guarantees valid metrics while keeping the model tractable. Retrieval proceeds via multi-source Dijkstra on the learned geodesic distances, followed by Maximal Marginal Relevance reranking and path coherence filtering. On citation prediction benchmarks with 169K papers, \gss{} achieves 23\% relative improvement in Recall@20 over SPECTER+FAISS baselines while providing interpretable citation paths. Our hierarchical coarse-to-fine search with k-means pooling reduces computational cost by 4$\times$ compared to flat geodesic search while maintaining 97\% retrieval quality. We provide theoretical analysis of when geodesic distances outperform direct similarity, characterize the approximation quality of low-rank metrics, and validate predictions empirically. Code and trained models are available at https://github.com/YCRG-Labs/geodesic-search.

</details>


### [9] [FuXi-Linear: Unleashing the Power of Linear Attention in Long-term Time-aware Sequential Recommendation](https://arxiv.org/abs/2602.23671)
*Yufei Ye,Wei Guo,Hao Wang,Luankang Zhang,Heng Chang,Hong Zhu,Yuyang Ye,Yong Liu,Defu Lian,Enhong Chen*

Main category: cs.IR

TL;DR: FuXi-Linear提出了一种线性复杂度的长序列推荐模型，通过独立的时间保留通道和线性位置通道解决现有线性注意力机制在时序信号处理、位置信息不足和架构深度方面的限制，在数千长度序列上实现了更好的推荐质量和显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统主要依赖二次复杂度的注意力机制，限制了处理长用户序列的能力并减慢了推理速度。虽然线性注意力是有前景的替代方案，但现有研究面临三个关键挑战：1) 时序信号常被忽视或通过简单耦合集成，导致时序和语义信号相互干扰且忽略行为周期性；2) 现有线性框架提供的位置信息不足；3) 主要关注短序列和浅层架构。

Method: 提出FuXi-Linear线性复杂度模型，包含两个关键组件：1) 时间保留通道：使用时序数据独立计算周期性注意力权重，防止时序和语义信号之间的串扰；2) 线性位置通道：通过可学习核在线性复杂度内集成位置信息。模型展现出在千长度尺度上的强大幂律缩放特性。

Result: 在数千令牌长度的序列上进行广泛实验，FuXi-Linear在推荐质量上优于最先进模型，同时在预填充阶段实现高达10倍加速，在解码阶段实现高达21倍加速，相比竞争基线。

Conclusion: FuXi-Linear通过独立处理时序信号和增强位置信息，解决了线性注意力在推荐系统中的关键限制，实现了高效的长序列推荐，展现出优异的性能和显著的推理速度提升。

Abstract: Modern recommendation systems primarily rely on attention mechanisms with quadratic complexity, which limits their ability to handle long user sequences and slows down inference. While linear attention is a promising alternative, existing research faces three critical challenges: (1) temporal signals are often overlooked or integrated via naive coupling that causes mutual interference between temporal and semantic signals while neglecting behavioral periodicity; (2) insufficient positional information provided by existing linear frameworks; and (3) a primary focus on short sequences and shallow architectures. To address these issues, we propose FuXi-Linear, a linear-complexity model designed for efficient long-sequence recommendation. Our approach introduces two key components: (1) a Temporal Retention Channel that independently computes periodic attention weights using temporal data, preventing crosstalk between temporal and semantic signals; (2) a Linear Positional Channel that integrates positional information through learnable kernels within linear complexity. Moreover, we demonstrate that FuXi-Linear exhibits a robust power-law scaling property at a thousand-length scale, a characteristic largely unexplored in prior linear recommendation studies. Extensive experiments on sequences of several thousand tokens demonstrate that FuXi-Linear outperforms state-of-the-art models in recommendation quality, while achieving up to 10$\times$ speedup in the prefill stage and up to 21$\times$ speedup in the decode stage compared to competitive baselines. Our code has been released in a public repository https://github.com/USTC-StarTeam/fuxi-linear.

</details>


### [10] [Recommending Search Filters To Improve Conversions At Airbnb](https://arxiv.org/abs/2602.23717)
*Hao Li,Kedar Bellare,Siyu Yang,Sherry Chen,Liwei He,Stephanie Moyerman,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 该论文提出了一种基于机器学习的搜索过滤器推荐系统，旨在提升Airbnb平台的预订转化率，通过推荐中间工具（搜索过滤器）直接针对下游转化目标。


<details>
  <summary>Details</summary>
Motivation: Airbnb作为连接客人和房东的双边在线市场，提供多样化的住宿、体验和服务库存。搜索过滤器在帮助客人导航这种多样性方面起着重要作用，但现有文献中关于搜索过滤器对驱动转化的直接影响研究不足。虽然搜索过滤器旨在促进在线市场的转化，但其直接转化效果尚未得到充分探索。

Method: 提出了一个新颖的机器学习应用框架，通过推荐搜索过滤器来改善预订转化。该框架直接针对下游转化（预订），通过推荐中间工具（搜索过滤器）来实现。构建了Airbnb的过滤器推荐系统，解决了冷启动和严格服务要求等挑战。

Result: 开发的过滤器推荐系统已在Airbnb成功部署，支持多个用户界面，并通过在线A/B测试验证了能够驱动增量预订转化提升。消融研究进一步验证了方法和关键设计选择的有效性。

Conclusion: 通过专注于以转化为导向的过滤器推荐，确保搜索过滤器在Airbnb中实现其最终目的——帮助客人找到并预订理想的住宿。这项工作填补了搜索过滤器对转化直接影响的研究空白。

Abstract: Airbnb, a two-sided online marketplace connecting guests and hosts, offers a diverse and unique inventory of accommodations, experiences, and services. Search filters play an important role in helping guests navigate this variety by refining search results to align with their needs. Yet, while search filters are designed to facilitate conversions in online marketplaces, their direct impact on driving conversions remains underexplored in the existing literature.
  This paper bridges this gap by presenting a novel application of machine learning techniques to recommend search filters aimed at improving booking conversions. We introduce a modeling framework that directly targets lower-funnel conversions (bookings) by recommending intermediate tools, i.e. search filters. Leveraging the framework, we designed and built the filter recommendation system at Airbnb from the ground up, addressing challenges like cold start and stringent serving requirements.
  The filter recommendation system we developed has been successfully deployed at Airbnb, powering multiple user interfaces and driving incremental booking conversion lifts, as validated through online A/B testing. An ablation study further validates the effectiveness of our approach and key design choices. By focusing on conversion-oriented filter recommendations, our work ensures that search filters serve their ultimate purpose at Airbnb - helping guests find and book their ideal accommodations.

</details>


### [11] [UniFAR: A Unified Facet-Aware Retrieval Framework for Scientific Documents](https://arxiv.org/abs/2602.23766)
*Zheng Dou,Zhao Zhang,Deqing Wang,Yikun Ban,Fuzhen Zhuang*

Main category: cs.IR

TL;DR: UniFAR是一个统一的面向科学文档检索的框架，通过自适应多粒度聚合、可学习的方面锚点和联合训练，同时支持文档-文档和问题-文档检索，解决了传统文档中心模型与问题驱动检索之间的系统不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有科学文档检索方法主要基于文档中心表示，但随着LLM和RAG的兴起，检索转向问题驱动模式，导致文档中心模型与问题驱动检索之间存在系统不匹配：输入粒度（长文档vs短问题）、语义焦点（科学论述结构vs具体问题意图）和训练信号（基于引用的相似性vs问题导向的相关性）。

Method: 提出UniFAR统一框架：1）通过自适应多粒度聚合调和粒度差异；2）通过可学习的方面锚点对齐文档结构与问题意图；3）通过联合训练统一文档-文档和问题-文档监督信号。

Result: 实验结果表明，UniFAR在多个检索任务和基础模型上一致优于先前方法，证实了其有效性和通用性。

Conclusion: UniFAR成功解决了文档中心模型与问题驱动检索之间的系统不匹配问题，提供了一个统一的框架来同时支持两种检索模式，在科学文档检索任务中表现出优越性能。

Abstract: Existing scientific document retrieval (SDR) methods primarily rely on document-centric representations learned from inter-document relationships for document-document (doc-doc) retrieval. However, the rise of LLMs and RAG has shifted SDR toward question-driven retrieval, where documents are retrieved in response to natural-language questions (q-doc). This change has led to systematic mismatches between document-centric models and question-driven retrieval, including (1) input granularity (long documents vs. short questions), (2) semantic focus (scientific discourse structure vs. specific question intent), and (3) training signals (citation-based similarity vs. question-oriented relevance). To this end, we propose UniFAR, a Unified Facet-Aware Retrieval framework to jointly support doc-doc and q-doc SDR within a single architecture. UniFAR reconciles granularity differences through adaptive multi-granularity aggregation, aligns document structure with question intent via learnable facet anchors, and unifies doc-doc and q-doc supervision through joint training. Experimental results show that UniFAR consistently outperforms prior methods across multiple retrieval tasks and base models, confirming its effectiveness and generality.

</details>


### [12] [RAD-DPO: Robust Adaptive Denoising Direct Preference Optimization for Generative Retrieval in E-commerce](https://arxiv.org/abs/2602.23964)
*Zhiguo Chen,Guohao Sun,Yiming Qiu,Xingzhi Yao,Mingming Li,Huimu Wang,Yangqi Zhang,Songlin Wang,Sulong Xu*

Main category: cs.IR

TL;DR: 本文提出RAD-DPO方法，解决生成式检索中直接偏好优化对结构化语义ID的三个限制：保护共享层次前缀、减轻标签噪声、缓解多标签查询中的概率挤压效应。


<details>
  <summary>Details</summary>
Motivation: 生成式检索在电商搜索中通过自回归解码语义ID来检索商品，但将其与复杂用户偏好对齐仍具挑战。直接偏好优化应用于结构化语义ID存在三个问题：惩罚共享层次前缀导致梯度冲突、易受隐式反馈中噪声伪负例影响、多标签查询中加剧有效候选间的概率"挤压效应"。

Method: 提出RAD-DPO方法：1) 引入token级梯度分离保护前缀结构；2) 基于相似性的动态奖励加权减轻标签噪声；3) 集成全局SFT损失的多标签全局对比目标，显式扩展正例覆盖。

Result: 在大规模电商平台上进行的广泛离线实验和在线A/B测试表明，该方法在排序质量和训练效率方面均有显著提升。

Conclusion: RAD-DPO有效解决了DPO在生成式检索中对结构化语义ID的三个关键限制，提升了电商搜索中的检索性能。

Abstract: Generative Retrieval (GR) has emerged as a powerful paradigm in e-commerce search, retrieving items via autoregressive decoding of Semantic IDs (SIDs). However, aligning GR with complex user preferences remains challenging. While Direct Preference Optimization (DPO) offers an efficient alignment solution, its direct application to structured SIDs suffers from three limitations: (i) it penalizes shared hierarchical prefixes, causing gradient conflicts; (ii) it is vulnerable to noisy pseudo-negatives from implicit feedback; and (iii) in multi-label queries with multiple relevant items, it exacerbates a probability "squeezing effect" among valid candidates. To address these issues, we propose RAD-DPO, which introduces token-level gradient detachment to protect prefix structures, similarity-based dynamic reward weighting to mitigate label noise, and a multi-label global contrastive objective integrated with global SFT loss to explicitly expand positive coverage. Extensive offline experiments and online A/B testing on a large-scale e-commerce platform demonstrate significant improvements in ranking quality and training efficiency.

</details>


### [13] [Towards Efficient and Generalizable Retrieval: Adaptive Semantic Quantization and Residual Knowledge Transfer](https://arxiv.org/abs/2602.23978)
*Huimu Wang,Xingzhi Yao,Yiming Qiu,Qinghong Zhang,Haotian Wang,Yufan Cui,Songlin Wang,Sulong Xu,Mingming Li*

Main category: cs.IR

TL;DR: SA^2CRQ框架通过序列自适应残差量化动态分配编码长度，为头部项目分配长ID避免碰撞，为尾部项目分配短ID提升泛化，并使用锚定课程残差量化缓解数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 语义ID生成检索在工业应用中面临权衡：头部项目易受ID碰撞影响下游任务，而数据稀疏的尾部项目（包括冷启动项目）泛化能力有限。

Method: 提出SA^2CRQ框架，包含SARQ（基于项目路径熵动态分配编码长度）和ACRQ（利用头部项目学习的冻结语义流形正则化尾部项目表示学习）。

Result: 在大规模工业搜索系统和多个公开数据集上的实验表明，SA^2CRQ相比现有基线取得一致改进，特别是在冷启动检索场景中表现突出。

Conclusion: SA^2CRQ框架有效解决了语义ID生成检索中的头部-尾部权衡问题，通过自适应编码长度分配和课程学习机制，显著提升了检索性能，特别是在冷启动场景中。

Abstract: While semantic ID-based generative retrieval enables efficient end-to-end modeling in industrial applications, these methods face a persistent trade-off: head items are susceptible to ID collisions that negatively impact downstream tasks, whereas data-sparse tail items, including cold-start items, exhibit limited generalization. To address this issue, we propose the Anchored Curriculum with Sequential Adaptive Quantization (SA^2CRQ) framework. The framework introduces Sequential Adaptive Residual Quantization (SARQ) to dynamically allocate code lengths based on item path entropy, assigning longer, discriminative IDs to head items and shorter, generalizable IDs to tail items. To mitigate data sparsity, the Anchored Curriculum Residual Quantization (ACRQ) component utilizes a frozen semantic manifold learned from head items to regularize and accelerate the representation learning of tail items. Experimental results from a large-scale industrial search system and multiple public datasets indicate that SA^2CRQ yields consistent improvements over existing baselines, particularly in cold-start retrieval scenarios.

</details>


### [14] [Robust Aggregation for Federated Sequential Recommendation with Sparse and Poisoned Data](https://arxiv.org/abs/2602.23982)
*Minh Hieu Nguyen*

Main category: cs.IR

TL;DR: 提出针对联邦序列推荐的鲁棒聚合框架，解决稀疏交互序列和恶意客户端更新的双重挑战


<details>
  <summary>Details</summary>
Motivation: 联邦序列推荐中，单个客户端通常只提供短而稀疏的交互序列，限制了用户表示的可靠性；同时联邦优化过程容易受到恶意或损坏客户端更新的攻击，毒化梯度会显著扭曲全局模型

Method: 提出防御感知的聚合机制，识别并降低不可靠客户端更新的权重，同时保留稀疏但良性参与者的信息信号；包含表示级约束以稳定用户和物品嵌入；集成序列感知正则化以保持用户建模的时间连贯性

Result: 未在摘要中明确说明具体实验结果

Conclusion: 该框架专门针对稀疏和对抗条件下的联邦序列推荐设计，通过鲁棒聚合机制解决数据稀疏性和安全性的双重挑战

Abstract: Federated sequential recommendation distributes model training across user devices so that behavioural data remains local, reducing privacy risks. Yet, this setting introduces two intertwined difficulties. On the one hand, individual clients typically contribute only short and highly sparse interaction sequences, limiting the reliability of learned user representations. On the other hand, the federated optimisation process is vulnerable to malicious or corrupted client updates, where poisoned gradients can significantly distort the global model. These challenges are particularly severe in sequential recommendation, where temporal dynamics further complicate signal aggregation. To address this problem, we propose a robust aggregation framework tailored for federated sequential recommendation under sparse and adversarial conditions. Instead of relying on standard averaging, our method introduces a defence-aware aggregation mechanism that identifies and down-weights unreliable client updates while preserving informative signals from sparse but benign participants. The framework incorporates representation-level constraints to stabilise user and item embeddings, preventing poisoned or anomalous contributions from dominating the global parameter space. In addition, we integrate sequence-aware regularisation to maintain temporal coherence in user modelling despite limited local observations.

</details>


### [15] [Colour Contrast on the Web: A WCAG 2.1 Level AA Compliance Audit of Common Crawl's Top 500 Domains](https://arxiv.org/abs/2602.24067)
*Thom Vaughan,Pedro Ortiz Suarez*

Main category: cs.IR

TL;DR: 对500个最常访问网站进行WCAG颜色对比度合规性大规模审计，发现40.9%的颜色配对未达到4.5:1对比度标准，仅20.4%网站完全合规


<details>
  <summary>Details</summary>
Motivation: 研究网页可访问性中的颜色对比度问题，通过大规模自动化审计评估主流网站的WCAG 2.1/2.2 Level AA合规性现状

Method: 使用Common Crawl的WARC存档数据，对500个最常访问网站的240个主页进行静态CSS分析，识别前景/背景颜色配对并评估是否符合4.5:1对比度阈值

Result: 共识别4,327个独特颜色配对，其中1,771个（40.9%）未达到对比度标准；网站平均通过率为62.7%，仅20.4%网站完全合规；不同域名类别间存在显著差异

Conclusion: 颜色对比度仍然是主流网站中普遍存在的可访问性障碍，需要更多关注和改进以确保视觉障碍用户能够平等访问网页内容

Abstract: We present a large-scale automated audit of WCAG 2.1/2.2 Level AA colour contrast compliance across the 500 most frequently crawled registered domains in Common Crawl's CC-MAIN-2026-08 February 2026 crawl archive. Rather than conducting a live crawl, all page content was sourced from Common Crawl's open WARC archives, ensuring reproducibility and eliminating any load on target web servers. Our static CSS analysis of 240 homepages identified 4,327 unique foreground/background colour pairings, of which 1,771 (40.9%) failed to meet the 4.5:1 contrast ratio threshold for normal text. The median per-site pass rate was 62.7%, with 20.4% of sites achieving full compliance across all detected colour pairings. These findings suggest that colour contrast remains a widespread accessibility barrier on the most prominent websites, with significant variation across domain categories.

</details>


### [16] [Recommendation Algorithms: A Comparative Study in Movie Domain](https://arxiv.org/abs/2602.24125)
*Rohit Chivukula,T. Jaya Lakshmi,Hemlata Sharma,C. H. S. N. P. Sairam Rallabandi*

Main category: cs.IR

TL;DR: 该研究将电影推荐视为回归任务，使用Netflix数据集提取新颖特征，结合XGBoost、KNN和矩阵分解算法，发现基于矩阵分解的方法在RMSE指标上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 智能推荐系统能显著提升电商平台收入，电影推荐系统需要更准确的方法来预测用户评分。现有推荐方法众多，但将推荐视为回归任务并提取新颖特征的方法值得探索。

Method: 1. 对Netflix数据集进行探索性数据分析；2. 提取聚合特征、基于矩阵分解的特征、用户和电影相似度特征；3. 使用XGBoost回归算法，并结合Surprise库中的KNN和矩阵分解算法进行推荐。

Result: 基于矩阵分解的算法在均方根误差（RMSE）指标上提供了最佳推荐效果，表明该方法在预测用户评分方面表现最优。

Conclusion: 将推荐视为回归任务并提取多种特征的方法是有效的，矩阵分解算法在电影推荐任务中表现出最佳性能，为推荐系统提供了有前景的技术路径。

Abstract: Intelligent recommendation systems have clearly increased the revenue of well-known e-commerce firms. Users receive product recommendations from recommendation systems. Cinematic recommendations are made to users by a movie recommendation system. There have been numerous approaches to the problem of recommendation in the literature. It is viewed as a regression task in this research. A regression model was built using novel properties extracted from the dataset and used as features in the model. For experimentation, the Netflix challenge dataset has been used. Video streaming service Netflix is a popular choice for many. Customers' prior viewing habits are taken into account when Netflix makes movie recommendations to them. An exploratory data analysis on the Netflix dataset was conducted to gain insights into user rating behaviour and movie characteristics. Various kinds of features, including aggregating, Matrix Factorization (MF) based, and user and movie similarity based, have been extracted in the subsequent stages. In addition to a feature in the XGBoost regression algorithm, the K-Nearest Neighbors and MF algorithms from Python's Surprise library are used for recommendations. Based on Root Mean Square Error (RMSE), MF-based algorithms have provided the best recommendations.

</details>


### [17] [Science Fiction and Fantasy in Wikipedia: Exploring Structural and Semantic Cues](https://arxiv.org/abs/2602.24229)
*Włodzimierz Lewoniewski,Milena Stróżyna,Izabela Czumałowska,Elżbieta Lewańska*

Main category: cs.IR

TL;DR: 研究探讨如何利用维基百科的结构和语义特征识别与科幻和奇幻（SF/F）相关的内容，解决因体裁边界模糊和重叠带来的识别挑战。


<details>
  <summary>Details</summary>
Motivation: 维基百科中科幻和奇幻相关文章的识别具有挑战性，因为体裁边界模糊且经常重叠。虽然维基百科提供了机器可读的结构（如分类、内部链接、Wikidata语句），但这些信号反映了社区惯例，可能存在偏见或不完整。

Method: 研究考察维基百科文章的结构和语义特征，包括分类系统、内部链接（wikilinks）以及对应的Wikidata项目中的声明，以识别与科幻和奇幻相关的内容。

Result: 未在摘要中明确说明具体结果，但研究旨在探索这些特征在识别SF/F内容方面的有效性。

Conclusion: 通过分析维基百科的结构和语义特征，可以更有效地识别科幻和奇幻相关内容，尽管这些信号可能存在社区惯例带来的偏见和不完整性。

Abstract: Identifying which Wikipedia articles are related to science fiction, fantasy, or their hybrids is challenging because genre boundaries are porous and frequently overlap. Wikipedia nonetheless offers machine-readable structure beyond text, including categories, internal links (wikilinks), and statements if corresponding Wikidata items. However, each of these signals reflects community conventions and can be biased or incomplete. This study examines structural and semantic features of Wikipedia articles that can be used to identify content related to science fiction and fantasy (SF/F).

</details>


### [18] [Beyond the Click: A Framework for Inferring Cognitive Traces in Search](https://arxiv.org/abs/2602.24265)
*Saber Zerhoudi,Michael Granitzer*

Main category: cs.IR

TL;DR: 提出从用户行为日志推断认知轨迹的框架，用于构建更人性化的用户模拟器


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟器主要复制用户行为而不理解其思维过程，行为日志记录了用户做什么但没记录他们在想什么或感受什么

Method: 基于信息觅食理论和人类专家判断的多智能体系统，从行为日志推断认知轨迹

Result: 认知轨迹提高了模型在预测会话结果和用户挣扎恢复等任务上的性能，发布了多个公共数据集的注释集合和开源工具

Conclusion: 该工作提供了构建更人性化用户模拟器和从用户导向维度评估检索系统所需的工具和数据

Abstract: User simulators are essential for evaluating search systems, but they primarily copy user actions without understanding the underlying thought process. This gap exists since large-scale interaction logs record what users do, but not what they might be thinking or feeling, such as confusion or satisfaction. To solve this problem, we present a framework to infer cognitive traces from behavior logs. Our method uses a multi-agent system grounded in Information Foraging Theory (IFT) and human expert judgment. These traces improve model performance on tasks like forecasting session outcomes and user struggle recovery. We release a collection of annotations for several public datasets, including AOL and Stack Overflow, and an open-source tool that allows researchers to apply our method to their own data. This work provides the tools and data needed to build more human-like user simulators and to assess retrieval systems on user-oriented dimensions of performance.

</details>


### [19] [Resources for Automated Evaluation of Assistive RAG Systems that Help Readers with News Trustworthiness Assessment](https://arxiv.org/abs/2602.24277)
*Dake Zhang,Mark D. Smucker,Charles L. A. Clarke*

Main category: cs.IR

TL;DR: TREC 2025 DRAGUN赛道开发了支持新闻可信度评估的RAG系统，包含问题生成和报告生成两个任务，并创建了自动化评估工具AutoJudge，与人工评估结果高度相关。


<details>
  <summary>Details</summary>
Motivation: 当前读者难以评估在线新闻的可信度，因为可靠报道与错误信息并存。需要开发辅助系统帮助读者进行新闻可信度评估。

Method: 创建DRAGUN赛道，包含两个任务：1) 问题生成任务：生成10个排名的调查性问题；2) 报告生成任务：基于MS MARCO V2.1 Segmented Corpus生成250字的报告。开发自动化评估流程AutoJudge来评估系统表现。

Result: AutoJudge与TREC人工评估结果高度相关（Task 1的Kendall's τ=0.678，Task 2的τ=0.872）。创建了可重复使用的评估资源和基准。

Conclusion: DRAGUN赛道为新闻可信度评估的RAG系统提供了评估框架和资源，AutoJudge工具能够有效替代人工评估，促进相关研究和系统开发。

Abstract: Many readers today struggle to assess the trustworthiness of online news because reliable reporting coexists with misinformation. The TREC 2025 DRAGUN (Detection, Retrieval, and Augmented Generation for Understanding News) Track provided a venue for researchers to develop and evaluate assistive RAG systems that support readers' news trustworthiness assessment by producing reader-oriented, well-attributed reports. As the organizers of the DRAGUN track, we describe the resources that we have newly developed to allow for the reuse of the track's tasks. The track had two tasks: (Task 1) Question Generation, producing 10 ranked investigative questions; and (Task 2, the main task) Report Generation, producing a 250-word report grounded in the MS MARCO V2.1 Segmented Corpus. As part of the track's evaluation, we had TREC assessors create importance-weighted rubrics of questions with expected short answers for 30 different news articles. These rubrics represent the information that assessors believe is important for readers to assess an article's trustworthiness. The assessors then used their rubrics to manually judge the participating teams' submitted runs. To make these tasks and their rubrics reusable, we have created an automated process to judge runs not part of the original assessing. We show that our AutoJudge ranks existing runs well compared to the TREC human-assessed evaluation (Kendall's $τ= 0.678$ for Task 1 and $τ= 0.872$ for Task 2). These resources enable both the evaluation of RAG systems for assistive news trustworthiness assessment and, with the human evaluation as a benchmark, research on improving automated RAG evaluation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance](https://arxiv.org/abs/2602.23367)
*Shubh Laddha,Lucas Changbencharoen,Win Kuptivej,Surya Shringla,Archana Vaidheeswaran,Yash Bhaskar*

Main category: cs.AI

TL;DR: 该论文提出了首个大规模MCP数据集，包含针对308个MCP服务器中2800个工具的多样化、高质量用户查询，解决了现有数据集缺乏真实用户查询模式的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MCP工具使用数据集缺乏真实、人性化的用户查询，无法反映不同用户如何表达请求，导致评估工具使用和生态系统时存在关键差距，现有基准测试的泛化能力差且可靠性被夸大。

Method: 基于MCP Zero数据集，为308个MCP服务器中的2800个工具生成多样化、高质量的用户查询，每个工具配对多个独特的用户角色，涵盖从精确任务请求到模糊探索性命令的不同用户意图层次。

Result: 创建了首个大规模MCP数据集，包含针对2800个工具的多样化用户查询，反映了真实世界交互模式的复杂性，能够更好地评估MCP服务器的工具使用和生态系统。

Conclusion: 该研究填补了MCP工具使用评估的关键空白，通过引入真实用户查询模式的数据集，为更准确评估MCP服务器的工具使用能力和生态系统提供了重要基础。

Abstract: Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.

</details>


### [21] [Causal Identification from Counterfactual Data: Completeness and Bounding Results](https://arxiv.org/abs/2602.23541)
*Arvind Raghavan,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文提出了CTFIDU+算法，用于从任意Layer 3分布中识别反事实查询，证明了该算法的完备性，并建立了非参数设置下精确因果推理的基本极限。


<details>
  <summary>Details</summary>
Motivation: 先前关于反事实识别完备性的研究仅限于观测或干预分布（Pearl因果层次的第1-2层），因为一般认为无法获得第3层的反事实分布数据。但最近研究发现某些反事实分布可以通过实验方法直接估计（反事实可实现性），这引发了新的问题：在获得部分Layer 3数据的情况下，哪些额外的反事实量变得可识别？

Method: 开发了CTFIDU+算法，用于从任意Layer 3分布集合中识别反事实查询，并证明了该算法对此任务的完备性。基于此，建立了从物理可实现分布中识别反事实的理论极限。

Result: 1) CTFIDU+算法是完备的；2) 确定了非参数设置下精确因果推理的基本极限；3) 对于某些无法识别的关键反事实类型，推导了使用可实现反事实数据的新分析界限；4) 通过仿真验证反事实数据在实践中有助于收紧不可识别量的界限。

Conclusion: 本文解决了在获得部分Layer 3数据的情况下反事实识别的完备性问题，建立了因果推理的理论极限，并为无法精确识别的反事实量提供了实用的界限分析方法。

Abstract: Previous work establishing completeness results for $\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\textit{counterfactual realizabilty}$. This leaves open the question of what $\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.

</details>


### [22] [Planning under Distribution Shifts with Causal POMDPs](https://arxiv.org/abs/2602.23545)
*Matteo Ceriscioli,Karthika Mohan*

Main category: cs.AI

TL;DR: 提出一个基于因果知识的POMDP理论框架，用于处理分布偏移下的规划问题，通过将环境变化表示为对因果POMDP的干预来评估计划并识别环境变化


<details>
  <summary>Details</summary>
Motivation: 现实世界中的规划经常面临分布偏移的挑战，在一个条件下获得的环境模型在状态分布或环境动态变化时可能失效，导致先前学习的策略失败

Method: 使用基于因果知识的部分可观测马尔可夫决策过程（POMDP）理论框架，将环境变化表示为对因果POMDP的干预，维护和更新对潜在状态和底层领域的信念

Result: 证明了价值函数在增强信念空间中保持分段线性凸（PWLC）性质，这保持了基于α向量的POMDP方法的可处理性

Conclusion: 该框架能够在分布偏移下进行规划，通过因果表示评估假设变化下的计划并主动识别环境变化，同时保持规划的可处理性

Abstract: In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $α$-vector-based POMDP methods.

</details>


### [23] [Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem](https://arxiv.org/abs/2602.23579)
*Guillem Rodríguez-Corominas,Maria J. Blesa,Christian Blum*

Main category: cs.AI

TL;DR: 提出RL-CMSA混合方法解决对称单仓库min-max mTSP问题，结合强化学习引导的构造、精确优化和自适应池管理，在随机和TSPLIB实例上表现优于现有混合遗传算法。


<details>
  <summary>Details</summary>
Motivation: 解决多旅行商问题中的min-max变体，目标是平衡多个销售员的工作负载，需要有效方法处理大规模实例和销售员数量增加的情况。

Method: 提出RL-CMSA混合方法：1) 使用基于学习q值的概率聚类构造多样化解；2) 合并路线到紧凑池；3) 求解受限集合覆盖MILP；4) 通过移除、移位和交换操作精化解；5) 根据高质量解更新q值；6) 通过老化和剪枝自适应管理池。

Result: 在随机和TSPLIB实例上，RL-CMSA能一致找到(接近)最优解，在可比时间限制下优于最先进的混合遗传算法，尤其当实例规模和销售员数量增加时表现更佳。

Conclusion: RL-CMSA结合精确优化和强化学习引导的构造，平衡了探索和利用，为对称单仓库min-max mTSP问题提供了有效的解决方案。

Abstract: The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.

</details>


### [24] [SleepLM: Natural-Language Intelligence for Human Sleep](https://arxiv.org/abs/2602.23605)
*Zongzhe Xu,Zitao Shuai,Eideen Mozaffari,Ravi S. Aysola,Rajesh Kumar,Yuzhe Yang*

Main category: cs.AI

TL;DR: SleepLM是一个将自然语言与多模态睡眠生理信号对齐的睡眠语言基础模型，通过大规模睡眠-文本数据集和统一预训练目标，实现了零样本学习、跨模态检索等先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的睡眠分析系统局限于预定义的标签空间（如睡眠阶段或事件），无法描述、查询或泛化到新的睡眠现象，需要建立自然语言与多模态睡眠生理信号之间的桥梁。

Method: 1）引入多级睡眠描述生成流程，构建首个大规模睡眠-文本数据集（超过10万小时数据，来自1万多名个体）；2）提出统一预训练目标，结合对比对齐、描述生成和信号重建，以更好地捕捉生理保真度和跨模态交互。

Result: SleepLM在真实世界的睡眠理解任务中表现出色，在零样本和少样本学习、跨模态检索和睡眠描述生成方面优于现有技术。模型还展现出语言引导的事件定位、针对性洞察生成和零样本泛化到未见任务等有趣能力。

Conclusion: SleepLM通过将自然语言与多模态睡眠生理信号对齐，实现了人类睡眠的对齐、解释和自然语言交互，为睡眠分析开辟了新途径，所有代码和数据将开源。

Abstract: We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.

</details>


### [25] [MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs](https://arxiv.org/abs/2602.23632)
*Lun Zhan,Feng Xiong,Huanyong Liu,Feng Zhang,Yuhui Yin*

Main category: cs.AI

TL;DR: MMKG-RDS是一个基于多模态知识图谱的推理数据合成框架，通过细粒度知识提取、可定制路径采样和多维数据质量评分，有效提升领域模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长尾知识覆盖、有效性验证和可解释性方面存在局限，基于知识图谱的方法在功能性、粒度、可定制性和评估方面仍有不足，需要更灵活的推理数据合成框架。

Method: 提出MMKG-RDS框架，利用多模态知识图谱进行细粒度知识提取，支持可定制的路径采样策略，并采用多维数据质量评分机制来评估合成数据的质量。

Result: 构建了MMKG-RDS-Bench数据集，覆盖5个领域、17种任务类型和14,950个样本。实验显示，在少量合成数据上微调Qwen3模型（0.6B/8B/32B）可使推理准确率提升9.2%。

Conclusion: MMKG-RDS能有效合成高质量训练数据，提升模型推理能力，同时生成具有挑战性的数据（涉及表格和公式），可用于复杂基准测试构建，为领域模型增强提供实用工具。

Abstract: Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS

</details>


### [26] [From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.23701)
*Yawen Wang,Wenjie Wu,Junjie Wang,Qing Wang*

Main category: cs.AI

TL;DR: CHIEF是一个新颖的框架，通过将混乱的多智能体系统轨迹转化为结构化分层因果图，结合分层预言引导回溯和渐进因果筛选策略，显著提升了故障归因的准确性。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统在复杂领域表现出色，但存在固有的脆弱性和不透明的故障机制。现有故障归因方法通常将执行日志视为平面序列，这种线性视角无法解开MAS中复杂的因果联系，导致弱可观测性和模糊的责任边界。

Method: 1) 将混乱轨迹转化为结构化分层因果图；2) 采用分层预言引导回溯，通过合成虚拟预言高效剪枝搜索空间；3) 通过渐进因果筛选策略实现反事实归因，严格区分真实根本原因和传播症状。

Result: 在Who&When基准测试中，CHIEF在智能体级和步骤级准确性上均优于八个强大的最先进基线方法。消融研究进一步证实了每个提出模块的关键作用。

Conclusion: CHIEF通过结构化分层因果分析和高效回溯机制，显著提升了多智能体系统故障归因的准确性和可解释性，为解决MAS的脆弱性和不透明性问题提供了有效方案。

Abstract: LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module.

</details>


### [27] [AI Must Embrace Specialization via Superhuman Adaptable Intelligence](https://arxiv.org/abs/2602.23643)
*Judah Goldfeder,Philippe Wyder,Yann LeCun,Ravid Shwartz Ziv*

Main category: cs.AI

TL;DR: 该论文批判了当前对通用人工智能（AGI）的定义，认为"人类能做的一切"这一标准既不可行也无用，提出了专注于特定领域并追求超人类性能的"超人类适应智能（SAI）"作为替代框架。


<details>
  <summary>Details</summary>
Motivation: 当前关于AGI的讨论存在定义混乱、概念模糊的问题，各方对AGI的理解不一致。作者认为现有的AGI定义（如"能做人类能做的一切"）既不现实也不实用，需要重新思考如何更好地描述和引导AI的未来发展方向。

Method: 通过批判性分析当前AGI定义的缺陷，提出替代概念框架。作者论证了为什么追求"通用性"存在问题，然后引入了"超人类适应智能（SAI）"的概念，并阐述其定义和优势。

Result: 提出了SAI作为替代AGI的概念框架：SAI是能够学习并超越人类在任何重要任务上的表现，并能填补人类能力空白的智能系统。这一概念更清晰、更实用，能够为AI发展提供更好的指导方向。

Conclusion: AI应该专注于专业化而非追求通用性，在特定领域追求超人类性能。SAI提供了一个更清晰、更实用的框架来讨论AI的未来，能够替代模糊的AGI概念，为AI发展提供更有意义的指导。

Abstract: Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.

</details>


### [28] [CIRCLE: A Framework for Evaluating AI from a Real-World Lens](https://arxiv.org/abs/2602.24055)
*Reva Schwartz,Carina Westling,Morgan Briggs,Marzieh Fadaee,Isar Nejadgholi,Matthew Holmes,Fariza Rashid,Maya Carlyle,Afaf Taïk,Kyra Wilson,Peter Douglas,Theodora Skeadas,Gabriella Waters,Rumman Chowdhury,Thiago Lacerda*

Main category: cs.AI

TL;DR: CIRCLE是一个六阶段生命周期框架，旨在弥合模型中心性能指标与AI实际部署效果之间的现实差距，通过将利益相关者关注转化为可测量信号，实现基于实际下游效应的治理。


<details>
  <summary>Details</summary>
Motivation: 现有MLOps框架关注系统稳定性，基准测试衡量抽象能力，但AI决策者缺乏关于AI技术在真实世界用户变异和约束下行为的系统证据。需要弥合模型性能指标与实际部署结果之间的现实差距。

Method: CIRCLE是一个六阶段生命周期框架，将TEVV（测试、评估、验证和确认）中的确认阶段操作化。它通过将堆栈外的利益相关者关注形式化为可测量信号，整合现场测试、红队演练和纵向研究等方法，形成协调的管道。

Result: CIRCLE产生系统化知识：跨站点可比但对本地情境敏感的实证证据。它提供结构化、前瞻性的协议，将情境敏感的定性洞察与可扩展的定量指标联系起来。

Conclusion: CIRCLE框架能够实现基于实际下游效应而非理论能力的治理，为AI技术在真实世界部署中的行为提供系统证据，填补了现有评估方法的空白。

Abstract: This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.

</details>


### [29] [PseudoAct: Leveraging Pseudocode Synthesis for Flexible Planning and Action Control in Large Language Model Agents](https://arxiv.org/abs/2602.23668)
*Yihan,Wen,Xin Chen*

Main category: cs.AI

TL;DR: PseudoAct：通过伪代码合成实现LLM智能体灵活规划与行动控制的新框架，显著提升长时程任务的决策效率和成功率


<details>
  <summary>Details</summary>
Motivation: 传统LLM智能体（如ReAct）依赖反应式决策范式，在复杂长时程任务中容易出现冗余工具使用、推理不稳定和高token消耗等问题，特别是在涉及分支、迭代或多工具协调的任务中表现不佳

Method: 提出PseudoAct框架，利用LLM将任务解决策略表达为代码的能力，合成结构化伪代码计划，将任务分解为子任务并显式编码控制流（包括序列、条件、循环、并行组合等逻辑原语），然后按照全局计划执行行动

Result: 在基准数据集上的实验表明，该方法显著优于现有的反应式智能体方法，在FEVER上实现了20.93%的绝对成功率提升，并在HotpotQA上创造了新的最先进水平

Conclusion: PseudoAct通过伪代码合成实现了显式和时序一致的决策逻辑，减少了冗余行动，防止了无限循环，避免了无信息量的替代探索，实现了长时程决策的一致性和高效性

Abstract: Large language model (LLM) agents typically rely on reactive decision-making paradigms such as ReAct, selecting actions conditioned on growing execution histories. While effective for short tasks, these approaches often lead to redundant tool usage, unstable reasoning, and high token consumption in complex long-horizon tasks involving branching, iteration, or multi-tool coordination. To address these limitations, this paper introduces PseudoAct, a novel framework for flexible planning and action control in LLM agents through pseudocode synthesis. Leveraging the ability of LLMs to express task-solving strategies as code, PseudoAct synthesizes a structured pseudocode plan that decomposes a task into subtasks and explicitly encodes control flow, including sequencing, conditionals, loops, parallel composition, and combinations of these logic primitives. Actions are then executed by following this global plan, making the decision logic explicit and temporally coherent. This design reduces redundant actions, prevents infinite loops, and avoids uninformative alternative exploration, enabling consistent and efficient long-horizon decision-making. Experiments on benchmark datasets show that our method significantly outperforms existing reactive agent approaches, achieving a 20.93% absolute gain in success rate on FEVER and setting a new state-of-the-art on HotpotQA.

</details>


### [30] [ProductResearch: Training E-Commerce Deep Research Agents via Multi-Agent Synthetic Trajectory Distillation](https://arxiv.org/abs/2602.23716)
*Jiangyuan Wang,Kejun Xiao,Huaipeng Zhao,Tao Luo,Xiaoyi Zeng*

Main category: cs.AI

TL;DR: 提出了ProductResearch多智能体框架，通过合成高质量工具使用轨迹来训练电商购物智能体，显著提升了LLM智能体在复杂产品研究任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的电商对话购物智能体缺乏复杂产品研究所需的交互深度和上下文广度，而深度研究范式在迁移到电商领域时存在领域差距，需要新的训练方法。

Method: 采用多智能体框架：用户智能体从行为历史推断购物意图，监督智能体协调研究智能体进行迭代协作，生成合成轨迹并最终形成产品研究报告。通过反思内化过程将多智能体监督交互整合为单角色训练样本。

Result: 基于合成数据微调的紧凑MoE模型在响应全面性、研究深度和用户感知效用方面相比基础模型有显著提升，接近前沿专有深度研究系统的性能。

Conclusion: 多智能体合成轨迹训练是增强基于LLM的购物辅助的有效且可扩展范式，能够训练出处理复杂购物查询的鲁棒智能体。

Abstract: Large Language Model (LLM)-based agents show promise for e-commerce conversational shopping, yet existing implementations lack the interaction depth and contextual breadth required for complex product research. Meanwhile, the Deep Research paradigm, despite advancing information synthesis in web search, suffers from domain gaps when transferred to e-commerce. We propose ProductResearch, a multi-agent framework that synthesizes high-fidelity, long-horizon tool-use trajectories for training robust e-commerce shopping agents. The framework employs a User Agent to infer nuanced shopping intents from behavioral histories, and a Supervisor Agent that orchestrates iterative collaboration with a Research Agent to generate synthetic trajectories culminating in comprehensive, insightful product research reports. These trajectories are rigorously filtered and distilled through a reflective internalization process that consolidates multi-agent supervisory interactions into coherent single-role training examples, enabling effective fine-tuning of LLM agents for complex shopping inquiries. Extensive experiments show that a compact MoE model fine-tuned on our synthetic data achieves substantial improvements over its base model in response comprehensiveness, research depth, and user-perceived utility, approaching the performance of frontier proprietary deep research systems and establishing multi-agent synthetic trajectory training as an effective and scalable paradigm for enhancing LLM-based shopping assistance.

</details>


### [31] [The Auton Agentic AI Framework](https://arxiv.org/abs/2602.23720)
*Sheng Cao,Zhao Chang,Chang Li,Hannan Li,Liyao Fu,Ji Tang*

Main category: cs.AI

TL;DR: Auton Agentic AI Framework：一个标准化自主智能体创建、执行和治理的架构，通过分离认知蓝图和运行时引擎来解决LLM随机输出与后端系统确定性需求之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: AI领域正从生成式AI向智能体AI过渡，但大型语言模型的随机非结构化输出与后端基础设施（数据库、API、云服务）所需的确定性、模式合规输入之间存在根本性架构不匹配。

Method: 提出Auton Agentic AI框架，核心是认知蓝图（声明式、语言无关的智能体身份和能力规范）与运行时引擎（平台特定执行基板）的严格分离。引入增强的部分可观测马尔可夫决策过程、分层记忆整合架构、约束流形形式化、三级自进化框架和运行时优化技术。

Result: 该框架实现了跨语言可移植性、形式化可审计性、通过模型上下文协议的模块化工具集成，并通过并行图执行、推测推理和动态上下文修剪等技术减少了多步骤智能体工作流的端到端延迟。

Conclusion: Auton Agentic AI Framework为自主智能体系统的创建、执行和治理提供了原则性架构，解决了生成式AI向智能体AI过渡中的核心架构不匹配问题，为标准化、可审计、高效的智能体系统开发奠定了基础。

Abstract: The field of Artificial Intelligence is undergoing a transition from Generative AI -- probabilistic generation of text and images -- to Agentic AI, in which autonomous systems execute actions within external environments on behalf of users. This transition exposes a fundamental architectural mismatch: Large Language Models (LLMs) produce stochastic, unstructured outputs, whereas the backend infrastructure they must control -- databases, APIs, cloud services -- requires deterministic, schema-conformant inputs. The present paper describes the Auton Agentic AI Framework, a principled architecture for standardizing the creation, execution, and governance of autonomous agent systems. The framework is organized around a strict separation between the Cognitive Blueprint, a declarative, language-agnostic specification of agent identity and capabilities, and the Runtime Engine, the platform-specific execution substrate that instantiates and runs the agent. This separation enables cross-language portability, formal auditability, and modular tool integration via the Model Context Protocol (MCP). The paper formalizes the agent execution model as an augmented Partially Observable Markov Decision Process (POMDP) with a latent reasoning space, introduces a hierarchical memory consolidation architecture inspired by biological episodic memory systems, defines a constraint manifold formalism for safety enforcement via policy projection rather than post-hoc filtering, presents a three-level self-evolution framework spanning in-context adaptation through reinforcement learning, and describes runtime optimizations -- including parallel graph execution, speculative inference, and dynamic context pruning -- that reduce end-to-end latency for multi-step agent workflows.

</details>


### [32] [Reasoning-Driven Multimodal LLM for Domain Generalization](https://arxiv.org/abs/2602.23777)
*Zhipeng Xu,Zilong Wang,Xinyang Jiang,Dongsheng Li,De Cheng,Nannan Wang*

Main category: cs.AI

TL;DR: 该论文提出RD-MLDG框架，利用多模态大语言模型的推理能力提升领域泛化性能，通过推理链构建和自对齐正则化解决传统方法在视觉特征不变性上的局限。


<details>
  <summary>Details</summary>
Motivation: 当前领域泛化方法主要关注视觉特征不变性，但忽略了多模态大语言模型的推理能力。论文旨在探索通过构建推理链来获得更鲁棒的跨领域预测，解决领域偏移问题。

Method: 提出RD-MLDG框架，包含两个核心组件：1) MTCT（多任务交叉训练），引入直接分类路径来指导推理监督；2) SARR（自对齐推理正则化），通过迭代自标注保持推理链的语义丰富性并缓解推理模式不匹配问题。

Result: 在标准DomainBed数据集（PACS、VLCS、OfficeHome、TerraInc）上的实验表明，RD-MLDG取得了最先进的性能，验证了推理作为鲁棒跨领域泛化补充信号的有效性。

Conclusion: 推理是提升领域泛化性能的有前景的补充信号，RD-MLDG框架通过有效整合推理链监督和自对齐正则化，在多模态大语言模型中实现了更鲁棒的跨领域泛化能力。

Abstract: This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.

</details>


### [33] [EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2602.23802)
*Yiyang Fang,Wenke Huang,Pei Fu,Yihao Yang,Kehua Su,Zhenbo Luo,Jian Luan,Mang Ye*

Main category: cs.AI

TL;DR: 提出EMO-R3框架，通过结构化情感思维和反思式情感奖励增强多模态大语言模型的情感推理能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在情感理解方面存在局限性：监督微调方法泛化能力有限且可解释性差，强化学习方法如GRPO无法与情感认知的内在特性对齐

Method: 提出EMO-R3框架，包含结构化情感思维（引导模型进行结构化、可解释的逐步情感推理）和反思式情感奖励（基于视觉-文本一致性和情感连贯性重新评估推理）

Result: EMO-R3显著提升了MLLMs的可解释性和情感智能，在多个视觉情感理解基准测试中取得了优越性能

Conclusion: EMO-R3框架有效解决了MLLMs在情感推理方面的挑战，通过结构化推理和反思机制增强了模型的情感理解能力

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.

</details>


### [34] [Pessimistic Auxiliary Policy for Offline Reinforcement Learning](https://arxiv.org/abs/2602.23974)
*Fan Zhang,Baoru Huang,Xin Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种悲观辅助策略来缓解离线强化学习中的分布外动作误差累积问题


<details>
  <summary>Details</summary>
Motivation: 离线强化学习从预收集的数据集学习，避免了实时交互的风险和低效，但在学习过程中不可避免地访问分布外动作，这会引入近似误差，导致误差累积和严重的高估问题

Method: 构建了一个悲观辅助策略，通过最大化Q函数的置信下界来采样可靠动作。该策略在已学习策略附近具有相对较高的价值和较低的不确定性，避免了学习过程中采样具有潜在高误差的高价值动作

Result: 悲观辅助策略引入的近似误差较少，有效缓解了误差累积问题。在离线强化学习基准测试上的大量实验表明，使用悲观辅助策略能够有效提升其他离线RL方法的性能

Conclusion: 提出的悲观辅助策略通过采样可靠动作来减少分布外动作带来的近似误差，从而缓解了离线强化学习中的误差累积问题，提高了现有方法的有效性

Abstract: Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.

</details>


### [35] [Portfolio Reinforcement Learning with Scenario-Context Rollout](https://arxiv.org/abs/2602.24037)
*Vanya Priscillia Bendatu,Yao Lu*

Main category: cs.AI

TL;DR: 论文提出了一种宏观条件化场景上下文展开方法，用于在压力事件下生成可信的次日多元收益场景，并通过构建反事实状态来稳定强化学习中的评论家训练，显著提升了投资组合再平衡策略的性能。


<details>
  <summary>Details</summary>
Motivation: 市场机制转换会导致分布偏移，从而降低投资组合再平衡策略的性能。传统方法无法处理历史中未发生的压力事件场景，而基于场景展开的奖励会引入奖励-转移不匹配问题，破坏强化学习评论家训练的稳定性。

Method: 提出宏观条件化场景上下文展开方法，生成压力事件下的次日多元收益场景。分析奖励-转移不匹配问题，构建反事实状态，使用展开隐含的延续来增强评论家智能体的引导目标，实现稳定的学习和偏差-方差权衡。

Result: 在31个不同的美国股票和ETF投资组合宇宙中进行样本外评估，相比经典和基于强化学习的投资组合再平衡基线，该方法将夏普比率提升高达76%，最大回撤降低高达53%。

Conclusion: 通过宏观条件化场景展开和反事实状态构建，有效解决了市场压力事件下的投资组合再平衡问题，显著提升了策略的稳健性和性能表现。

Abstract: Market regime shifts induce distribution shifts that can degrade the performance of portfolio rebalancing policies. We propose macro-conditioned scenario-context rollout (SCR) that generates plausible next-day multivariate return scenarios under stress events. However, doing so faces new challenges, as history will never tell what would have happened differently. As a result, incorporating scenario-based rewards from rollouts introduces a reward--transition mismatch in temporal-difference learning, destabilizing RL critic training.
  We analyze this inconsistency and show it leads to a mixed evaluation target. Guided by this analysis, we construct a counterfactual next state using the rollout-implied continuations and augment the critic agent's bootstrap target. Doing so stabilizes the learning and provides a viable bias-variance tradeoff.
  In out-of-sample evaluations across 31 distinct universes of U.S. equity and ETF portfolios, our method improves Sharpe ratio by up to 76% and reduces maximum drawdown by up to 53% compared with classic and RL-based portfolio rebalancing baselines.

</details>


### [36] [Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction](https://arxiv.org/abs/2602.24080)
*Xiang Li,Jiabao Gao,Sipei Lin,Xuan Zhou,Chi Zhang,Bo Cheng,Jiale Han,Benyou Wang*

Main category: cs.AI

TL;DR: 首个针对语音到语音系统的图灵测试显示，现有系统均未通过测试，与人类对话存在显著差距。瓶颈在于副语言特征、情感表达和对话个性，而非语义理解。


<details>
  <summary>Details</summary>
Motivation: 现代语音到语音系统能否像人类一样对话是一个关键但未解决的问题。为了评估这些系统的人类相似度，研究者进行了首个针对S2S系统的图灵测试。

Method: 收集了2,968个人类判断，评估9个最先进的S2S系统与28名人类参与者的对话。开发了包含18个人类相似度维度的细粒度分类法，并对收集的对话进行众包标注。

Result: 所有被评估的S2S系统均未通过图灵测试。瓶颈主要在于副语言特征、情感表达和对话个性，而非语义理解。现成的AI模型作为图灵测试评判者表现不可靠。

Conclusion: 提出了一个利用细粒度人类相似度评分的可解释模型，能够准确透明地区分人类与机器对话。这项工作为S2S系统建立了首个人类相似度评估框架，超越了二元结果，为对话AI系统的人类化改进铺平了道路。

Abstract: The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.

</details>


### [37] [Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance](https://arxiv.org/abs/2602.24110)
*Yanwei Ren,Haotian Zhang,Likang Xiao,Xikai Zhang,Jiaxing Huang,Jiayan Qiu,Baosheng Yu,Quan Chen,Liu Liu*

Main category: cs.AI

TL;DR: SCOPE框架通过过程奖励模型识别次优轨迹中的首个错误步骤，进行细粒度修正，有效利用部分正确的轨迹，提升探索多样性，在数学推理任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习监督存在局限性，对部分正确但包含少量错误的轨迹惩罚过重，导致模型丢弃有价值的轨迹，降低探索多样性，过早缩小探索空间。

Method: 提出SCOPE框架，利用过程奖励模型精确定位次优轨迹中的首个错误步骤，应用细粒度的步骤级离策略修正，对部分正确的轨迹进行精确优化。

Result: 方法有效拯救部分正确轨迹，将多样性分数提升13.5%，在数学推理任务上达到46.6%的平均准确率，在分布外推理任务上达到53.4%的准确率。

Conclusion: SCOPE通过步骤级修正有效维持了广阔的探索空间，在复杂推理任务上建立了新的最先进结果，并展现出强大的泛化能力。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.

</details>


### [38] [Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume](https://arxiv.org/abs/2602.24195)
*Gregory Kang Ruey Lau,Hieu Dao,Nicole Kan Hui Lin,Bryan Kian Hsiang Low*

Main category: cs.AI

TL;DR: UMPIRE是一个无需训练的多模态大语言模型不确定性量化框架，通过计算响应样本的语义体积来评估不确定性，在各种模态和任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型可能产生看似合理但错误的输出，影响可靠部署。现有不确定性度量方法存在局限性：仅适用于特定模态、依赖外部工具或计算成本高。

Method: 提出UMPIRE框架，无需训练，利用模型内部模态特征，计算采样响应的不连贯调整语义体积，捕捉样本的全局语义多样性和基于内部模型置信度的局部不连贯性。

Result: 在图像、音频、视频-文本基准测试（包括对抗性和分布外设置）中，UMPIRE在错误检测和不确定性校准方面始终优于基线方法，并能泛化到非文本输出任务（如图像和音频生成）。

Conclusion: UMPIRE是一个高效、通用的多模态大语言模型不确定性量化框架，无需外部工具或训练，在各种模态和任务上表现出色，为可靠部署提供了有效的不确定性评估方法。

Abstract: Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.

</details>


### [39] [A Minimal Agent for Automated Theorem Proving](https://arxiv.org/abs/2602.24273)
*Borja Requena Pozo,Austin Letson,Krystian Nowakowski,Izan Beltran Ferreiro,Leopoldo Sarra*

Main category: cs.AI

TL;DR: 提出一个最小化的定理证明基准系统，用于系统比较不同AI定理证明器架构，展示迭代方法相比单次生成的优越性


<details>
  <summary>Details</summary>
Motivation: 需要建立一个标准化的基准系统来系统比较不同AI定理证明器架构，当前缺乏统一的比较框架

Method: 设计实现包含迭代证明精炼、库搜索和上下文管理等核心功能的最小化基准系统，使用不同基准评估各种流行模型和设计选择

Result: 基准系统在性能上可与最先进方法竞争，同时架构显著简化；迭代方法在样本效率和成本效益方面明显优于多次单次生成

Conclusion: 迭代方法在AI定理证明中具有明显优势，开源实现可作为未来研究的参考基准和社区可访问的证明器

Abstract: We propose a minimal agentic baseline that enables systematic comparison across different AI-based theorem prover architectures. This design implements the core features shared among state-of-the-art systems: iterative proof refinement, library search and context management. We evaluate our baseline using qualitatively different benchmarks and compare various popular models and design choices, and demonstrate competitive performance compared to state-of-the-art approaches, while using a significantly simpler architecture. Our results demonstrate consistent advantages of an iterative approach over multiple single-shot generations, especially in terms of sample efficiency and cost effectiveness. The implementation is released open-source as a candidate reference for future research and as an accessible prover for the community.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [40] [SGAgent: Suggestion-Guided LLM-Based Multi-Agent Framework for Repository-Level Software Repair](https://arxiv.org/abs/2602.23647)
*Quanjun Zhang,Chengyu Gao,Yu Han,Ye Shang,Chunrong Fang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: SGAgent是一个基于多智能体的软件修复框架，采用定位-建议-修复范式，通过引入建议阶段来弥补传统定位-修复方法的推理空白，在SWE-Bench上达到51.3%的修复准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体的软件修复方法通常采用定位-修复范式，直接从"bug在哪里"跳到"如何修复"，存在根本性的推理空白。需要一种更系统的方法来加强从定位到修复的过渡。

Method: 提出SGAgent框架，采用定位-建议-修复三阶段范式：1）引入建议阶段，从bug位置逐步检索相关上下文直到完全理解bug，提供可操作的修复建议；2）构建知识图谱增强全局上下文感知和仓库级推理；3）三个专门子智能体（定位器、建议器、修复器）协作实现端到端自动化软件修复。

Result: 在SWE-Bench上，SGAgent使用Claude-3.5达到51.3%修复准确率，81.2%文件级和52.4%函数级定位准确率，平均每个实例成本1.48美元，优于所有使用相同基础模型的基线。在VUL4J和VJBench漏洞修复任务上达到48%准确率，展示了跨任务和编程语言的强泛化能力。

Conclusion: SGAgent通过引入建议阶段和知识图谱增强，有效解决了传统定位-修复范式的推理空白问题，在软件修复任务上取得了最先进的性能，并展示了良好的跨任务泛化能力。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to the emergence of intelligent agents capable of autonomously interacting with environments and invoking external tools. Recently, agent-based software repair approaches have received widespread attention, as repair agents can automatically analyze and localize bugs, generate patches, and achieve state-of-the-art performance on repository-level benchmarks. However, existing approaches usually adopt a localize-then-fix paradigm, jumping directly from "where the bug is" to "how to fix it", leaving a fundamental reasoning gap. To this end, we propose SGAgent, a Suggestion-Guided multi-Agent framework for repository-level software repair, which follows a localize-suggest-fix paradigm. SGAgent introduces a suggestion phase to strengthen the transition from localization to repair. The suggester starts from the buggy locations and incrementally retrieves relevant context until it fully understands the bug, and then provides actionable repair suggestions. Moreover, we construct a Knowledge Graph from the target repository and develop a KG-based toolkit to enhance SGAgent's global contextual awareness and repository-level reasoning. Three specialized sub-agents (i.e., localizer, suggester, and fixer) collaborate to achieve automated end-to-end software repair. Experimental results on SWE-Bench show that SGAgent with Claude-3.5 achieves 51.3% repair accuracy, 81.2% file-level and 52.4% function-level localization accuracy with an average cost of $1.48 per instance, outperforming all baselines using the same base model. Furthermore, SGAgent attains 48% accuracy on VUL4J and VJBench for vulnerability repair, demonstrating strong generalization across tasks and programming languages.

</details>


### [41] [Peeling Off the Cocoon: Unveiling Suppressed Golden Seeds for Mutational Greybox Fuzzing](https://arxiv.org/abs/2602.23736)
*Ruixiang Qian,Chunrong Fang,Zengxu Chen,Youxin Fu,Zhenyu Chen*

Main category: cs.SE

TL;DR: PoCo是一种增强现代基于覆盖率的种子选择技术的方法，通过逐步移除障碍条件语句并进行更深层次的种子选择来改进afl-cmin等技术


<details>
  <summary>Details</summary>
Motivation: 现有的基于覆盖率的种子选择技术（如afl-cmin）在处理包含复杂条件语句的程序时可能无法充分探索所有代码路径，存在优化空间

Method: PoCo采用渐进式方法，逐步识别和移除程序中的障碍条件语句，然后进行更深层次的种子选择，以更全面地探索代码路径

Result: PoCo能够比传统CSS技术更有效地发现新的代码路径，提高代码覆盖率，增强模糊测试的效果

Conclusion: PoCo通过渐进式移除障碍条件语句和深度种子选择，显著改进了现有基于覆盖率的种子选择技术，为模糊测试提供了更强大的工具

Abstract: PoCo is a technique that aims to enhance modern coverage-based seed selection (CSS) techniques (such as afl-cmin) by gradually removing obstacle conditional statements and conducting deeper seed selection.

</details>


### [42] [The Vocabulary of Flaky Tests in the Context of SAP HANA](https://arxiv.org/abs/2602.23957)
*Alexander Berndt,Zoltán Nochta,Thomas Bach*

Main category: cs.SE

TL;DR: 该研究在SAP HANA大型工业项目中评估了基于测试代码中源代码标识符识别不稳定测试的方法，复现了先前研究并扩展了特征提取和分类技术，取得了更高的F1分数，但发现实际应用价值有限。


<details>
  <summary>Details</summary>
Motivation: 不稳定测试（flaky tests）会随机失败而不提供明确的质量信号，影响自动化测试执行。先前研究提出基于测试代码中的源代码标识符来识别不稳定测试，但这些方法尚未在大型工业环境中得到充分评估。本研究旨在在SAP HANA大型工业项目中评估这些方法及其识别不稳定测试根本原因的能力。

Method: 首先复现Pinto等人的先前工作在SAP HANA环境中的应用；其次评估不同的特征提取技术，包括TF-IDF和TF-IDFC-RF；第三评估CodeBERT和XGBoost作为分类模型。为了进行可靠的比较，同时使用了先前研究的数据集和两个SAP HANA数据集。

Result: 复现研究在原始数据集和一个SAP HANA数据集上显示了相似的结果。原始方法在原始数据集上获得0.94的F1分数，在SAP HANA数据集上获得0.92的F1分数；而扩展方法分别达到了0.96和0.99的F1分数。研究发现，在SAP HANA环境中，对外部数据源的依赖是不稳定测试的常见根本原因。

Conclusion: 大型工业项目的词汇在具体术语上略有不同，但术语类别（如远程依赖）与先前实证研究结果相似。然而，即使获得了较高的F1分数，基于源代码标识符识别不稳定测试和黑盒预测在实际应用中的价值有限，因为结果对开发人员来说不够可操作。

Abstract: Background. Automated test execution is an important activity to gather information about the quality of a software project. So-called flaky tests, however, negatively affect this process. Such tests fail seemingly at random without changes to the code and thus do not provide a clear signal. Previous work proposed to identify flaky tests based on the source code identifiers in the test code. So far, these approaches have not been evaluated in a large-scale industrial setting. Aims. We evaluate approaches to identify flaky tests and their root causes based on source code identifiers in the test code in a large-scale industrial project. Method. First, we replicate previous work by Pinto et al. in the context of SAP HANA. Second, we assess different feature extraction techniques, namely TF-IDF and TF-IDFC-RF. Third, we evaluate CodeBERT and XGBoost as classification models. For a sound comparison, we utilize both the data set from previous work and two data sets from SAP HANA. Results. Our replication shows similar results on the original data set and on one of the SAP HANA data sets. While the original approach yielded an F1-Score of 0.94 on the original data set and 0.92 on the SAP HANA data set, our extensions achieve F1-Scores of 0.96 and 0.99, respectively. The reliance on external data sources is a common root cause for test flakiness in the context of SAP HANA. Conclusions. The vocabulary of a large industrial project seems to be slightly different with respect to the exact terms, but the categories for the terms, such as remote dependencies, are similar to previous empirical findings. However, even with rather large F1-Scores, both finding source code identifiers for flakiness and a black box prediction have limited use in practice as the results are not actionable for developers.

</details>


### [43] [Context-Aware Functional Test Generation via Business Logic Extraction and Adaptation](https://arxiv.org/abs/2602.24108)
*Yakun Zhang,Zihan Wang,Xinzhi Peng,Zihao Xie,Xiaodong Wang,Xutao Li,Dan Hao,Lu Zhang,Yunming Ye*

Main category: cs.SE

TL;DR: LogiDroid是一个两阶段方法，通过提取业务逻辑并适配到目标应用来生成移动应用功能测试用例，在两个数据集上分别比现有最佳方法提升48%和55%的成功率。


<details>
  <summary>Details</summary>
Motivation: 移动应用功能测试对验证业务逻辑与用户需求的一致性至关重要，但面临两大挑战：1) 从非结构化需求中获取和复用复杂业务逻辑困难；2) 将业务逻辑适配到多样化GUI环境时存在语义鸿沟，阻碍了针对特定移动应用生成测试用例。

Method: LogiDroid采用两阶段方法：1) 知识检索与融合阶段：构建数据集检索相关案例，为目标功能提取业务逻辑；2) 上下文感知测试生成阶段：联合分析提取的业务逻辑和实时GUI环境，生成包含验证断言的功能测试用例。

Result: 在覆盖28个真实应用和190个功能需求的两个数据集上，LogiDroid在FrUITeR数据集上成功测试了40%的功能需求（比现有最佳方法提升48%以上），在Lin数据集上成功测试了65%（比现有最佳方法提升55%以上）。

Conclusion: LogiDroid通过准确理解应用语义和利用领域专业知识生成完整测试用例，在功能测试生成方面表现出显著有效性，解决了移动应用功能测试中的关键挑战。

Abstract: Functional testing is essential for verifying that the business logic of mobile applications aligns with user requirements, serving as the primary methodology for quality assurance in software development. Despite its importance, functional testing remains heavily dependent on manual effort due to two core challenges. First, acquiring and reusing complex business logic from unstructured requirements remains difficult, which hinders the understanding of specific functionalities. Second, a significant semantic gap exists when adapting business logic to the diverse GUI environments, which hinders the generation of test cases for specific mobile applications. To address the preceding challenges, we propose LogiDroid, a two-stage approach that generates individual functional test cases by extracting business logic and adapting it to target applications. First, in the Knowledge Retrieval and Fusion stage, we construct a dataset to retrieve relevant cases and extract business logic for the target functionality. Second, in the Context-Aware Test Generation stage, LogiDroid jointly analyzes the extracted business logic and the real-time GUI environment to generate functional test cases. This design allows LogiDroid to accurately understand application semantics and use domain expertise to generate complete test cases with verification assertions. We assess the effectiveness of LogiDroid using two widely-used datasets that cover 28 real-world applications and 190 functional requirements. Experimental results show that LogiDroid successfully tested 40% of functional requirements on the FrUITeR dataset (an improvement of over 48% compared to the state-of-the-art approaches) and 65% on the Lin dataset (an improvement of over 55% compared to the state-of-the-art approaches). These results demonstrate the significant effectiveness of LogiDroid in functional test generation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Neuro-Symbolic AI for Analytical Solutions of Differential Equations](https://arxiv.org/abs/2502.01476)
*Orestis Oikonomou,Levi Lingsch,Dana Grund,Siddhartha Mishra,Georgios Kissas*

Main category: cs.LG

TL;DR: SIGS是一个神经符号框架，通过形式语法生成有效构建块，在连续空间中搜索，通过最小化物理残差来组装、评分和优化候选解析解，实现微分方程解析解的自动化发现。


<details>
  <summary>Details</summary>
Motivation: 微分方程的解析解能提供精确、可解释的洞察，但通常难以获得，因为发现它们需要专家直觉或在组合空间中进行穷举搜索。

Method: SIGS使用形式语法生成语法有效的构建块，将这些表达式嵌入连续空间，然后在该空间中搜索，通过最小化基于物理的残差来组装、评分和优化候选解析解。

Result: SIGS是第一个能够解析求解耦合非线性偏微分方程组、在语法规范不完整时发现解、并为缺乏已知解析解的偏微分方程产生准确符号近似的神经符号方法，在标准基准测试中实现了数量级的精度和效率提升。

Conclusion: SIGS通过统一符号推理和数值优化，实现了微分方程解析解的自动化发现，在精度和效率方面显著优于现有符号方法。

Abstract: Analytical solutions to differential equations offer exact, interpretable insight but are rarely available because discovering them requires expert intuition or exhaustive search in combinatorial spaces. We introduce SIGS, a neuro-symbolic framework that automates this process. SIGS uses a formal grammar to generate only syntactically valid building blocks, embeds these expressions into a continuous space, and then searches this space to assemble, score, and refine candidate closed-form solutions by minimizing a physics-based residual. This design unifies symbolic reasoning with numerical optimization; the grammar constrains candidate solution blocks to be proper by construction, while the latent search makes exploration tractable and data-free. SIGS is the first neuro-symbolic method to (i) analytically solve coupled systems of nonlinear PDEs, (ii) discover solutions under grammar misspecification, and (iii) produce accurate symbolic approximations for PDEs lacking known closed-form solutions. Overall, SIGS achieves orders-of-magnitude improvements in accuracy and efficiency over existing symbolic methods on standard benchmarks.

</details>


### [45] [Detoxifying LLMs via Representation Erasure-Based Preference Optimization](https://arxiv.org/abs/2602.23391)
*Nazanin Mohammadi Sepahvand,Eleni Triantafillou,Hugo Larochelle,Doina Precup,Daniel M. Roy,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: REPO是一种新的语言模型去毒方法，通过表示擦除和偏好优化，在token级别消除毒性表示，实现更鲁棒的模型安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有基于DPO、NPO等方法的去毒防御存在局限性，它们只能降低有害输出的可能性，但不够鲁棒，容易受到对抗性提示和微调重新学习攻击的影响。研究表明这些编辑是表面的，有害的"方向"仍然存在于表示中。

Method: 提出表示擦除偏好优化（REPO），将去毒重新定义为token级别的偏好问题。使用新颖的目标函数和偏好数据，强制毒性延续的表示向良性对应物收敛。这种方法进行深度、局部化的编辑，针对毒性编码神经元。

Result: REPO实现了最先进的鲁棒性，能够阻止复杂的威胁，包括重新学习攻击和增强的GCG越狱攻击，而现有的基于表示和输出的方法都失败了。机制分析显示REPO能诱导深度、局部化的编辑，同时保持模型的一般效用。

Conclusion: REPO通过token级别的表示擦除方法，实现了比现有方法更鲁棒的语言模型去毒，能够有效防御各种攻击，同时保持模型性能，为语言模型的安全部署提供了更好的解决方案。

Abstract: Large language models (LLMs) trained on webscale data can produce toxic outputs, raising concerns for safe deployment. Prior defenses, based on applications of DPO, NPO, and similar algorithms, reduce the likelihood of harmful continuations, but not robustly so: they are vulnerable to adversarial prompting and easily undone by fine-tuning-based relearning attacks. Indeed, research has shown that these edits to the model are superficial: linear probing reveals that harmful "directions" remain present in representations. To address this, we propose Representation Erasure-based Preference Optimization (REPO), reformulating detoxification as a token-level preference problem. Using a novel objective with preference data, we force the representations of toxic continuations to converge toward their benign counterparts. Our mechanistic analysis reveals that this granular approach is critical: unlike baselines, REPO induces deep, localized edits to toxicity-encoding neurons while preserving general model utility. Exhaustive evaluations show that REPO achieves state-of-the-art robustness, stopping sophisticated threats-including relearning attacks and enhanced GCG jailbreaks-where existing representation- and output-based methods fail.

</details>


### [46] [U-CAN: Utility-Aware Contrastive Attenuation for Efficient Unlearning in Generative Recommendation](https://arxiv.org/abs/2602.23400)
*Zezheng Wu,Rui Wang,Xinghe Cheng,Yang Shao,Qing Yang,Jiapu Wang,Jingwei Zhang*

Main category: cs.LG

TL;DR: 本文提出U-CAN框架，通过低秩适配器上的精准遗忘技术解决生成式推荐中的隐私问题，避免传统方法导致的灾难性效用损失。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐系统在微调时会无意中将敏感属性编码到模型参数中，引发隐私担忧。现有的机器遗忘技术面临多义性困境，即神经元同时编码敏感数据和一般推理模式，导致传统梯度或剪枝方法造成灾难性的效用损失。

Method: 提出Utility-aware Contrastive AttenuatioN (U-CAN)框架，在低秩适配器上操作。通过对比激活量化风险，专注于对遗忘集高度敏感但对保留集抑制的神经元。引入效用感知校准机制，结合权重大小和保留集激活范数，为对保留性能贡献大的维度分配更高的效用分数。采用自适应软衰减方法，使用可微衰减函数选择性地降低LoRA适配器上的高风险参数。

Result: 在两个公共数据集上的七个指标实验表明，U-CAN实现了强大的隐私遗忘、效用保留和计算效率。

Conclusion: U-CAN框架有效解决了生成式推荐中的隐私遗忘问题，通过精准的参数衰减方法在保护隐私的同时保持了模型性能，避免了传统方法的缺陷。

Abstract: Generative Recommendation (GenRec) typically leverages Large Language Models (LLMs) to redefine personalization as an instruction-driven sequence generation task. However, fine-tuning on user logs inadvertently encodes sensitive attributes into model parameters, raising critical privacy concerns. Existing Machine Unlearning (MU) techniques struggle to navigate this tension due to the Polysemy Dilemma, where neurons superimpose sensitive data with general reasoning patterns, leading to catastrophic utility loss under traditional gradient or pruning methods. To address this, we propose Utility-aware Contrastive AttenuatioN (U-CAN), a precision unlearning framework that operates on low-rank adapters. U-CAN quantifies risk by contrasting activations and focuses on neurons with asymmetric responses that are highly sensitive to the forgetting set but suppressed on the retention set. To safeguard performance, we introduce a utility-aware calibration mechanism that combines weight magnitudes with retention-set activation norms, assigning higher utility scores to dimensions that contribute strongly to retention performance. Unlike binary pruning, which often fragments network structure, U-CAN develop adaptive soft attenuation with a differentiable decay function to selectively down-scale high-risk parameters on LoRA adapters, suppressing sensitive retrieval pathways and preserving the topological connectivity of reasoning circuits. Experiments on two public datasets across seven metrics demonstrate that U-CAN achieves strong privacy forgetting, utility retention, and computational efficiency.

</details>


### [47] [Long Range Frequency Tuning for QML](https://arxiv.org/abs/2602.23409)
*Michael Poppel,Jonas Stein,Sebastian Wölckert,Markus Baumann,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 量子机器学习角度编码模型在可训练频率方法中面临频率可训练性限制，提出基于三元编码的网格初始化方法解决此问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习中可训练频率编码方法理论上效率很高，但实际优化中频率预因子的可训练性有限，当目标频率超出可达范围时优化经常失败，需要解决这一频率可达性限制。

Method: 提出基于三元编码的网格初始化方法，生成密集的整数频率谱，确保目标频率位于局部可达范围内。该方法需要O(log_3(omega_max))个编码门，比固定频率方法指数级减少。

Result: 在三频移高频率合成目标上，三元网格初始化中位R²得分0.9969，而可训练频率基线仅为0.1841。在实际航班乘客数据集上，三元网格初始化中位R²得分0.9671，比可训练频率初始化（0.7876）提升22.8%。

Conclusion: 频率预因子的可训练性限制是量子机器学习角度编码模型的实际瓶颈，三元网格初始化方法通过确保目标频率在局部可达范围内，有效克服了这一限制，显著提升了模型性能。

Abstract: Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876).

</details>


### [48] [EvoX: Meta-Evolution for Automated Discovery](https://arxiv.org/abs/2602.23413)
*Shu Liu,Shubham Agarwal,Monishwaran Maheswaran,Mert Cemri,Zhifei Li,Qiuyang Mang,Ashwin Naren,Ethan Boneh,Audrey Cheng,Melissa Z. Pan,Alexander Du,Kurt Keutzer,Alexandros G. Dimakis,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.LG

TL;DR: EvoX是一种自适应进化方法，通过联合进化候选解和搜索策略，动态调整进化过程，在近200个真实世界优化任务中优于现有AI驱动的进化方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法如AlphaEvolve虽然结合了LLM驱动的优化和进化搜索，但大多依赖固定的搜索策略和预定义参数，这些策略在整个执行过程中保持不变，无法适应不同任务或同一任务中搜索空间随时间变化的情况。

Method: EvoX是一种自适应进化方法，它联合进化候选解和用于生成这些解的搜索策略。系统持续更新如何选择和变化先前解的方式，基于优化进展动态调整，使系统能够在优化过程中在不同搜索策略之间动态切换。

Result: 在近200个真实世界优化任务中，EvoX在大多数任务上优于现有的AI驱动进化方法，包括AlphaEvolve、OpenEvolve、GEPA和ShinkaEvolve。

Conclusion: EvoX通过自适应地优化自身的进化过程，能够动态调整搜索策略，从而在广泛的优化任务中实现更好的性能，解决了现有固定策略方法无法适应任务变化的问题。

Abstract: Recent work such as AlphaEvolve has shown that combining LLM-driven optimization with evolutionary search can effectively improve programs, prompts, and algorithms across domains. In this paradigm, previously evaluated solutions are reused to guide the model toward new candidate solutions. Crucially, the effectiveness of this evolution process depends on the search strategy: how prior solutions are selected and varied to generate new candidates. However, most existing methods rely on fixed search strategies with predefined knobs (e.g., explore-exploit ratios) that remain static throughout execution. While effective in some settings, these approaches often fail to adapt across tasks, or even within the same task as the search space changes over time. We introduce EvoX, an adaptive evolution method that optimizes its own evolution process. EvoX jointly evolves candidate solutions and the search strategies used to generate them, continuously updating how prior solutions are selected and varied based on progress. This enables the system to dynamically shift between different search strategies during the optimization process. Across nearly 200 real-world optimization tasks, EvoX outperforms existing AI-driven evolutionary methods including AlphaEvolve, OpenEvolve, GEPA, and ShinkaEvolve on the majority of tasks.

</details>


### [49] [Human Supervision as an Information Bottleneck: A Unified Theory of Error Floors in Human-Guided Learning](https://arxiv.org/abs/2602.23446)
*Alejandro Rodriguez Dominguez*

Main category: cs.LG

TL;DR: 论文提出"人类有界智能极限"理论，认为仅依赖人类监督的LLM存在固有错误下限，这是监督渠道结构限制而非模型规模问题，可通过非人类辅助信号突破


<details>
  <summary>Details</summary>
Motivation: 大型语言模型主要依赖人类生成的数据和反馈进行训练，但存在持续错误，这些错误源于标注噪声、主观偏好和自然语言的有限表达能力。作者认为这些限制反映了监督渠道的结构特性，而非模型规模或优化问题

Method: 开发统一理论框架，证明当人类监督渠道对潜在评估目标不足时，会作为信息减少渠道，为任何受其主导的学习者引入严格正值的超额风险下限。通过六个互补框架（算子理论、PAC-Bayes、信息论、因果推断、范畴论、RLHF博弈论分析）形式化这一"人类有界智能极限"

Result: 理论解释了为什么单纯扩展无法消除持续的人类对齐错误，并描述了辅助非人类信号（如检索、程序执行、工具）增加有效监督能力并消除下限的条件。在真实偏好数据、合成已知目标任务和外部可验证基准上的实验证实了预测的结构特征

Conclusion: 仅依赖人类监督存在持续错误下限，而足够信息丰富的辅助渠道可以严格减少或消除超额错误。这为理解LLM的局限性提供了理论基础，并指出了通过非人类信号增强监督的路径

Abstract: Large language models are trained primarily on human-generated data and feedback, yet they exhibit persistent errors arising from annotation noise, subjective preferences, and the limited expressive bandwidth of natural language. We argue that these limitations reflect structural properties of the supervision channel rather than model scale or optimization. We develop a unified theory showing that whenever the human supervision channel is not sufficient for a latent evaluation target, it acts as an information-reducing channel that induces a strictly positive excess-risk floor for any learner dominated by it. We formalize this Human-Bounded Intelligence limit and show that across six complementary frameworks (operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic analyses of reinforcement learning from human feedback), non-sufficiency yields strictly positive lower bounds arising from the same structural decomposition into annotation noise, preference distortion, and semantic compression. The theory explains why scaling alone cannot eliminate persistent human-aligned errors and characterizes conditions under which auxiliary non-human signals (e.g., retrieval, program execution, tools) increase effective supervision capacity and collapse the floor by restoring information about the latent target. Experiments on real preference data, synthetic known-target tasks, and externally verifiable benchmarks confirm the predicted structural signatures: human-only supervision exhibits a persistent floor, while sufficiently informative auxiliary channels strictly reduce or eliminate excess error.

</details>


### [50] [Global Interpretability via Automated Preprocessing: A Framework Inspired by Psychiatric Questionnaires](https://arxiv.org/abs/2602.23459)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: REFINE方法通过将非线性处理限制在基线预处理模块中，提取稳定的项目值，然后学习从这些稳定化基线项目到未来症状严重程度的线性映射，在保持可解释性的同时提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 精神病学问卷具有高度情境敏感性，对后续症状严重程度的预测能力较弱。虽然非线性模型可以提高预测准确性，但其有限的可解释性会削弱临床信任。需要一种既能提高准确性又能保持可解释性的方法。

Method: 采用两阶段方法REFINE：1）将非线性能力限制在基线预处理模块中，估计稳定的项目值；2）学习从这些稳定化基线项目到未来严重程度的线性映射。这样将非线性集中在预处理阶段，同时保持预后关系的透明线性。

Result: 在实验中，REFINE优于其他可解释方法，同时在精神病学和非精神病学纵向预测任务中保持了对预后因素的清晰全局归因。

Conclusion: REFINE方法通过分离预处理和预测，将非线性集中在预处理阶段，同时保持线性预测模型，实现了在保持全局可解释性的同时提高预测性能的目标，为临床实践提供了可信的预测工具。

Abstract: Psychiatric questionnaires are highly context sensitive and often only weakly predict subsequent symptom severity, which makes the prognostic relationship difficult to learn. Although flexible nonlinear models can improve predictive accuracy, their limited interpretability can erode clinical trust. In fields such as imaging and omics, investigators commonly address visit- and instrument-specific artifacts by extracting stable signal through preprocessing and then fitting an interpretable linear model. We adopt the same strategy for questionnaire data by decoupling preprocessing from prediction: we restrict nonlinear capacity to a baseline preprocessing module that estimates stable item values, and then learn a linear mapping from these stabilized baseline items to future severity. We refer to this two-stage method as REFINE (Redundancy-Exploiting Follow-up-Informed Nonlinear Enhancement), which concentrates nonlinearity in preprocessing while keeping the prognostic relationship transparently linear and therefore globally interpretable through a coefficient matrix, rather than through post hoc local attributions. In experiments, REFINE outperforms other interpretable approaches while preserving clear global attribution of prognostic factors across psychiatric and non-psychiatric longitudinal prediction tasks.

</details>


### [51] [Uncertainty-aware Language Guidance for Concept Bottleneck Models](https://arxiv.org/abs/2602.23495)
*Yangyi Li,Mengdi Huai*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的概念瓶颈模型方法，通过量化LLM标注概念的不确定性并纳入训练过程，解决现有方法忽视LLM标注不确定性和缺乏有效量化机制的问题。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型需要大量专家知识标注概念，限制了广泛应用。现有利用LLM构建概念瓶颈的方法存在两个关键局限：1) 忽视LLM标注概念的不确定性，缺乏有效的量化机制；2) 未将标注不确定性纳入概念瓶颈模型的学习过程，增加了因LLM幻觉导致错误的风险。

Method: 提出了一种新颖的不确定性感知CBM方法，该方法不仅以有效且无分布的方式严格量化LLM标注概念标签的不确定性，还将量化的概念不确定性纳入CBM训练过程，以考虑LLM标注概念的不同可靠性水平。同时提供了理论分析。

Result: 在真实世界数据集上的广泛实验验证了所提出方法的期望特性。

Conclusion: 该方法通过量化LLM标注的不确定性并将其纳入训练过程，解决了现有方法的关键局限，提高了概念瓶颈模型的可靠性和实用性。

Abstract: Concept Bottleneck Models (CBMs) provide inherent interpretability by first mapping input samples to high-level semantic concepts, followed by a combination of these concepts for the final classification. However, the annotation of human-understandable concepts requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. On the other hand, there are a few works that leverage the knowledge of large language models (LLMs) to construct concept bottlenecks. Nevertheless, they face two essential limitations: First, they overlook the uncertainty associated with the concepts annotated by LLMs and lack a valid mechanism to quantify uncertainty about the annotated concepts, increasing the risk of errors due to hallucinations from LLMs. Additionally, they fail to incorporate the uncertainty associated with these annotations into the learning process for concept bottleneck models. To address these limitations, we propose a novel uncertainty-aware CBM method, which not only rigorously quantifies the uncertainty of LLM-annotated concept labels with valid and distribution-free guarantees, but also incorporates quantified concept uncertainty into the CBM training procedure to account for varying levels of reliability across LLM-annotated concepts. We also provide the theoretical analysis for our proposed method. Extensive experiments on the real-world datasets validate the desired properties of our proposed methods.

</details>


### [52] [FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments](https://arxiv.org/abs/2602.23504)
*Anik Pramanik,Murat Kantarcioglu,Vincent Oria,Shantanu Sharma*

Main category: cs.LG

TL;DR: FedDAG提出了一种新的聚类联邦学习框架，通过整合数据和梯度信息的加权类别相似度度量来改进聚类，并采用双编码器架构实现跨集群特征迁移，在数据异构场景下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统聚类联邦学习方法存在两个主要问题：1）仅依赖数据相似度或梯度相似度进行聚类，导致对客户端相似性的评估不完整；2）限制知识和表示仅在相同集群内共享，阻碍了集群模型从跨集群的多样化客户端群体中受益。

Method: FedDAG采用加权类别相似度度量，整合数据和梯度信息进行更全面的相似性评估。同时采用双编码器架构：主编码器在自身客户端数据上训练，辅助编码器使用互补集群的梯度进行精炼，实现跨集群特征迁移的同时保持集群特定专业化。

Result: 在多种基准测试和数据异构设置下的实验表明，FedDAG在准确率方面持续优于最先进的聚类联邦学习基线方法。

Conclusion: FedDAG通过更全面的相似度度量和跨集群知识共享机制，有效解决了数据异构联邦学习中的性能下降问题，为聚类联邦学习提供了更优的解决方案。

Abstract: Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters. To address these limitations, FedDAG introduces a clustered FL framework, FedDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FedDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization. Experiments on diverse benchmarks and data heterogeneity settings show that FedDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy.

</details>


### [53] [Sample Size Calculations for Developing Clinical Prediction Models: Overview and pmsims R package](https://arxiv.org/abs/2602.23507)
*Diana Shamsutdinova,Felix Zimmer,Oyebayo Ridwan Olaniran,Sarah Markham,Daniel Stahl,Gordon Forbes,Ewan Carr*

Main category: cs.LG

TL;DR: 本文提出了一种新的临床预测模型样本量估算方法，结合学习曲线、高斯过程优化和保证原则，开发了开源的R包pmsims，为复杂数据结构和机器学习模型提供灵活高效的样本量计算方案。


<details>
  <summary>Details</summary>
Motivation: 临床预测模型在医疗决策中应用日益广泛，但确定其开发所需的最小样本量仍是一个关键且未解决的挑战。样本量不足会导致过拟合、泛化能力差和预测偏差。现有方法（启发式规则、闭式公式和基于模拟的方法）在灵活性和准确性方面存在差异，特别是对于复杂数据结构和机器学习模型。

Method: 作者回顾了预测建模中样本量估算的现有方法，提出了区分均值基准则和保证基准则的概念框架。在此基础上，提出了一种新颖的基于模拟的方法，该方法整合了学习曲线、高斯过程优化和保证原则，以识别能够以高概率达到目标性能的样本量。该方法在开源、模型无关的R包pmsims中实现。

Result: 通过案例研究，作者展示了样本量估计在不同方法、性能指标和建模策略之间存在显著差异。与现有工具相比，pmsims提供了灵活、高效和可解释的解决方案，能够适应多样化的模型和用户定义的指标，同时明确考虑了模型性能的变异性。

Conclusion: 该框架和软件通过将灵活性与计算效率相结合，推进了临床预测建模的样本量方法学。未来工作应将这些方法扩展到分层和多模态数据，纳入公平性和稳定性指标，并解决缺失数据和复杂依赖结构等挑战。

Abstract: Background: Clinical prediction models are increasingly used to inform healthcare decisions, but determining the minimum sample size for their development remains a critical and unresolved challenge. Inadequate sample sizes can lead to overfitting, poor generalisability, and biased predictions. Existing approaches, such as heuristic rules, closed-form formulas, and simulation-based methods, vary in flexibility and accuracy, particularly for complex data structures and machine learning models. Methods: We review current methodologies for sample size estimation in prediction modelling and introduce a conceptual framework that distinguishes between mean-based and assurance-based criteria. Building on this, we propose a novel simulation-based approach that integrates learning curves, Gaussian Process optimisation, and assurance principles to identify sample sizes that achieve target performance with high probability. This approach is implemented in pmsims, an open-source, model-agnostic R package. Results: Through case studies, we demonstrate that sample size estimates vary substantially across methods, performance metrics, and modelling strategies. Compared to existing tools, pmsims provides flexible, efficient, and interpretable solutions that accommodate diverse models and user-defined metrics while explicitly accounting for variability in model performance. Conclusions: Our framework and software advance sample size methodology for clinical prediction modelling by combining flexibility with computational efficiency. Future work should extend these methods to hierarchical and multimodal data, incorporate fairness and stability metrics, and address challenges such as missing data and complex dependency structures.

</details>


### [54] [Neural Operators Can Discover Functional Clusters](https://arxiv.org/abs/2602.23528)
*Yicen Li,Jose Antonio Lara Benitez,Ruiyang Hong,Anastasis Kratsios,Paul David McNicholas,Maarten Valentijn de Hoop*

Main category: cs.LG

TL;DR: 神经算子可学习无限维再生核希尔伯特空间中任意有限类别的聚类，即使类别非凸非连通，为函数数据聚类提供理论保证


<details>
  <summary>Details</summary>
Motivation: 神经算子在回归任务中已有深入研究，但在分类及其无监督对应任务——聚类方面缺乏理论理解。本文旨在建立神经算子在无限维函数空间中进行聚类的理论基础，并开发实用的函数数据聚类方法

Method: 1) 理论证明：在温和的核采样假设下，证明基于样本的神经算子可以学习无限维再生核希尔伯特空间中任意有限类别的聚类；2) 实践方法：开发神经算子驱动的函数数据聚类流程，包括固定预训练编码器将离散轨迹提升为连续特征映射，轻量可训练头映射到软分配

Result: 1) 提出通用聚类定理：任何K个闭类都可以在闭集的上Kuratowski拓扑中被神经算子参数化的类以任意精度逼近；2) 在合成ODE基准测试中，提出的实用SNO方法在经典方法失败的机制下成功恢复潜在动态结构

Conclusion: 神经算子不仅适用于回归任务，也能有效处理无限维函数空间中的聚类问题。理论证明提供了神经算子聚类的数学基础，而实际应用展示了该方法在恢复复杂动态系统潜在结构方面的有效性，为函数数据分析开辟了新途径

Abstract: Operator learning is reshaping scientific computing by amortizing inference across infinite families of problems. While neural operators (NOs) are increasingly well understood for regression, far less is known for classification and its unsupervised analogue: clustering. We prove that sample-based neural operators can learn any finite collection of classes in an infinite-dimensional reproducing kernel Hilbert space, even when the classes are neither convex nor connected, under mild kernel sampling assumptions. Our universal clustering theorem shows that any $K$ closed classes can be approximated to arbitrary precision by NO-parameterized classes in the upper Kuratowski topology on closed sets, a notion that can be interpreted as disallowing false-positive misclassifications.
  Building on this, we develop an NO-powered clustering pipeline for functional data and apply it to unlabeled families of ordinary differential equation (ODE) trajectories. Discretized trajectories are lifted by a fixed pre-trained encoder into a continuous feature map and mapped to soft assignments by a lightweight trainable head. Experiments on diverse synthetic ODE benchmarks show that the resulting practical SNO recovers latent dynamical structure in regimes where classical methods fail, providing evidence consistent with our universal clustering theory.

</details>


### [55] [Active Value Querying to Minimize Additive Error in Subadditive Set Function Learning](https://arxiv.org/abs/2602.23529)
*Martin Černý,David Sychrovský,Filip Úradník,Jakub Černý*

Main category: cs.LG

TL;DR: 研究如何通过添加查询来近似未知次可加集函数，以最小化最小和最大补全之间的距离


<details>
  <summary>Details</summary>
Motivation: 次可加集函数在多个领域有重要应用，但完整指定需要指数级数量的值，实践中资源消耗大。当值缺失时，最小和最大补全之间的差距会导致优化不确定性，需要研究如何有效近似这类函数

Method: 1) 深入探索不同类别集函数的最小和最大补全及其距离分析；2) 开发在已知先验下通过离线或在线方式披露额外子集值来最小化距离的方法；3) 在实际场景中进行算法性能的实证演示

Result: 提出了系统的方法来近似次可加集函数，通过有效减少最小和最大补全之间的差距，在理论和实证层面都取得了进展

Conclusion: 该研究为解决次可加集函数近似问题提供了理论框架和实用算法，能够有效处理值缺失情况下的函数近似和优化问题

Abstract: Subadditive set functions play a pivotal role in computational economics (especially in combinatorial auctions), combinatorial optimization or artificial intelligence applications such as interpretable machine learning. However, specifying a set function requires assigning values to an exponentially large number of subsets in general, a task that is often resource-intensive in practice, particularly when the values derive from external sources such as retraining of machine learning models. A~simple omission of certain values introduces ambiguity that becomes even more significant when the incomplete set function has to be further optimized over. Motivated by the well-known result about inapproximability of subadditive functions using deterministic value queries with respect to a multiplicative error, we study a problem of approximating an unknown subadditive (or a subclass of thereof) set function with respect to an additive error -- i. e., we aim to efficiently close the distance between minimal and maximal completions. Our contributions are threefold: (i) a thorough exploration of minimal and maximal completions of different classes of set functions with missing values and an analysis of their resulting distance; (ii) the development of methods to minimize this distance over classes of set functions with a known prior, achieved by disclosing values of additional subsets in both offline and online manner; and (iii) empirical demonstrations of the algorithms' performance in practical scenarios.

</details>


### [56] [Dynamics of Learning under User Choice: Overspecialization and Peer-Model Probing](https://arxiv.org/abs/2602.23565)
*Adhyyan Narang,Sarah Dean,Lillian J Ratliff,Maryam Fazel*

Main category: cs.LG

TL;DR: 该论文研究了多平台机器学习环境中存在的"过度专业化陷阱"问题，发现现有算法可能导致全局性能极差的模型，即使存在低全人口损失的模型。作者提出了一种基于知识蒸馏的探测算法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 在多平台机器学习部署的经济相关场景中，用户会选择最适合自己的平台，现有研究只关注学习者在观察到的数据分布上的"局部"损失。研究发现存在这样的情况：使用现有算法的学习者几乎必然收敛到全局性能极差的模型，即使存在低全人口损失的模型。

Method: 受现代机器学习中知识蒸馏的启发，提出了一种算法，允许学习者"探测"同伴模型的预测，使他们能够了解不选择他们的用户。分析确定了探测成功的条件：当探测源足够信息丰富时（如已知的市场领导者或大多数具有良好全局性能的同行），该过程几乎必然收敛到具有有界全人口风险的平稳点。

Result: 在MovieLens、Census和Amazon Sentiment数据集上的半合成实验验证了研究结果，表明提出的探测算法能够有效避免过度专业化陷阱，收敛到具有良好全局性能的模型。

Conclusion: 在多平台机器学习环境中，过度专业化陷阱是一个严重问题，会导致全局性能恶化。通过探测同伴模型的预测，学习者可以了解不选择他们的用户，从而避免这一陷阱。当探测源足够信息丰富时，该方法能够收敛到具有有界全人口风险的平稳点。

Abstract: In many economically relevant contexts where machine learning is deployed, multiple platforms obtain data from the same pool of users, each of whom selects the platform that best serves them. Prior work in this setting focuses exclusively on the "local" losses of learners on the distribution of data that they observe. We find that there exist instances where learners who use existing algorithms almost surely converge to models with arbitrarily poor global performance, even when models with low full-population loss exist. This happens through a feedback-induced mechanism, which we call the overspecialization trap: as learners optimize for users who already prefer them, they become less attractive to users outside this base, which further restricts the data they observe. Inspired by the recent use of knowledge distillation in modern ML, we propose an algorithm that allows learners to "probe" the predictions of peer models, enabling them to learn about users who do not select them. Our analysis characterizes when probing succeeds: this procedure converges almost surely to a stationary point with bounded full-population risk when probing sources are sufficiently informative, e.g., a known market leader or a majority of peers with good global performance. We verify our findings with semi-synthetic experiments on the MovieLens, Census, and Amazon Sentiment datasets.

</details>


### [57] [SDMixer: Sparse Dual-Mixer for Time Series Forecasting](https://arxiv.org/abs/2602.23581)
*Xiang Ao*

Main category: cs.LG

TL;DR: 提出双流稀疏Mixer预测框架，通过频域和时域分别提取全局趋势和局部动态特征，使用稀疏机制过滤无效信息，提升多元时间序列预测性能


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测在交通、能源、金融等领域应用广泛，但数据常存在多尺度特征、弱相关性和噪声干扰等问题，限制了现有模型的预测性能

Method: 提出双流稀疏Mixer预测框架，在频域和时域分别提取全局趋势和局部动态特征，采用稀疏机制过滤无效信息，增强跨变量依赖建模的准确性

Result: 实验结果表明该方法在多个真实场景数据集上取得了领先性能，验证了其有效性和通用性

Conclusion: 提出的双流稀疏Mixer框架能有效处理多元时间序列中的多尺度特征、弱相关性和噪声问题，提升预测性能，具有实际应用价值

Abstract: Multivariate time series forecasting is widely applied in fields such as transportation, energy, and finance. However, the data commonly suffers from issues of multi-scale characteristics, weak correlations, and noise interference, which limit the predictive performance of existing models. This paper proposes a dual-stream sparse Mixer prediction framework that extracts global trends and local dynamic features from sequences in both the frequency and time domains, respectively. It employs a sparsity mechanism to filter out invalid information, thereby enhancing the accuracy of cross-variable dependency modeling. Experimental results demonstrate that this method achieves leading performance on multiple real-world scenario datasets, validating its effectiveness and generality. The code is available at https://github.com/SDMixer/SDMixer

</details>


### [58] [When Does Multimodal Learning Help in Healthcare? A Benchmark on EHR and Chest X-Ray Fusion](https://arxiv.org/abs/2602.23614)
*Kejing Yin,Haizhou Xu,Wenfang Yao,Chen Liu,Zijie Chen,Yui Haang Cheung,William K. Cheung,Jing Qin*

Main category: cs.LG

TL;DR: 该研究系统评估了电子健康记录（EHR）和胸部X光（CXR）的多模态融合在临床预测中的效果，探讨了融合时机、策略比较、模态缺失鲁棒性和算法公平性四个核心问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在临床决策支持中具有潜力，但多模态学习在实际应用中（特别是在模态缺失和公平性约束下）何时真正有效尚不明确。本研究旨在通过系统基准测试，为开发临床可部署的多模态系统提供实用指导。

Method: 使用MIMIC-IV和MIMIC-CXR标准化队列，系统评估EHR和CXR的多模态融合。研究比较了不同融合策略，测试了现有方法对模态缺失的鲁棒性，并评估了多模态模型的算法公平性。同时发布了可扩展的基准测试工具包。

Result: 多模态融合在模态完整时能提升性能，特别是在需要EHR和CXR互补信息的疾病中。跨模态学习机制能捕获临床相关依赖关系，但EHR的丰富时间结构导致模态不平衡。在现实模态缺失情况下，多模态优势迅速下降，除非模型专门设计处理不完整输入。多模态融合不自动改善公平性，亚组差异主要源于不同人口群体间的敏感性不平等。

Conclusion: 本研究提供了关于多模态学习何时有效、何时失败及原因的实用指导，为开发既有效又可靠的临床可部署多模态系统奠定了基础。开源工具包支持可重复和可扩展的评估。

Abstract: Machine learning holds promise for advancing clinical decision support, yet it remains unclear when multimodal learning truly helps in practice, particularly under modality missingness and fairness constraints. In this work, we conduct a systematic benchmark of multimodal fusion between Electronic Health Records (EHR) and chest X-rays (CXR) on standardized cohorts from MIMIC-IV and MIMIC-CXR, aiming to answer four fundamental questions: when multimodal fusion improves clinical prediction, how different fusion strategies compare, how robust existing methods are to missing modalities, and whether multimodal models achieve algorithmic fairness. Our study reveals several key insights. Multimodal fusion improves performance when modalities are complete, with gains concentrating in diseases that require complementary information from both EHR and CXR. While cross-modal learning mechanisms capture clinically meaningful dependencies beyond simple concatenation, the rich temporal structure of EHR introduces strong modality imbalance that architectural complexity alone cannot overcome. Under realistic missingness, multimodal benefits rapidly degrade unless models are explicitly designed to handle incomplete inputs. Moreover, multimodal fusion does not inherently improve fairness, with subgroup disparities mainly arising from unequal sensitivity across demographic groups. To support reproducible and extensible evaluation, we further release a flexible benchmarking toolkit that enables plug-and-play integration of new models and datasets. Together, this work provides actionable guidance on when multimodal learning helps, when it fails, and why, laying the foundation for developing clinically deployable multimodal systems that are both effective and reliable. The open-source toolkit can be found at https://github.com/jakeykj/CareBench.

</details>


### [59] [On the Convergence of Single-Loop Stochastic Bilevel Optimization with Approximate Implicit Differentiation](https://arxiv.org/abs/2602.23633)
*Yubo Zhou,Luo Luo,Guang Dai,Haishan Ye*

Main category: cs.LG

TL;DR: 本文对单循环随机近似隐式微分算法（SSAID）进行了收敛性分析，证明了其达到ε-平稳点的oracle复杂度为O(κ^7 ε^{-2})，匹配了多循环方法的最优速率，同时保持了单循环的计算效率。


<details>
  <summary>Details</summary>
Motivation: 随机双层优化在元学习和超参数优化中具有重要应用。尽管单循环算法在实践中普遍使用（同时更新上下层变量），但其理论理解，特别是在随机机制下，远不如多循环算法成熟。现有分析往往得到次优收敛速率，或者将关键的下层条件数κ依赖隐藏在一般Lipschitz常数中。

Method: 本文对单循环随机近似隐式微分算法（SSAID）进行了精细化的收敛性分析。该方法采用单循环更新机制，同时优化上下层变量，避免了多循环算法的计算开销。

Result: 证明了SSAID算法达到ε-平稳点的oracle复杂度为O(κ^7 ε^{-2})。这一结果在两个方面具有重要意义：(1) 匹配了最先进多循环方法（如stocBiO）的最优O(ε^{-2})速率，同时保持了单循环更新的计算效率；(2) 首次为基于随机AID的单循环方法提供了显式、细粒度的κ依赖特征。

Conclusion: SSAID不仅是一种启发式方法，而且具有严格的理论基础，其收敛保证与主流多循环框架具有竞争力。这项工作填补了单循环随机双层优化算法理论分析的空白。

Abstract: Stochastic Bilevel Optimization has emerged as a fundamental framework for meta-learning and hyperparameter optimization. Despite the practical prevalence of single-loop algorithms--which update lower and upper variables concurrently--their theoretical understanding, particularly in the stochastic regime, remains significantly underdeveloped compared to their multi-loop counterparts. Existing analyses often yield suboptimal convergence rates or obscure the critical dependence on the lower-level condition number $κ$, frequently burying it within generic Lipschitz constants. In this paper, we bridge this gap by providing a refined convergence analysis of the Single-loop Stochastic Approximate Implicit Differentiation (SSAID) algorithm. We prove that SSAID achieves an $ε$-stationary point with an oracle complexity of $\mathcal{O}(κ^7 ε^{-2})$. Our result is noteworthy in two aspects: (i) it matches the optimal $\mathcal{O}(ε^{-2})$ rate of state-of-the-art multi-loop methods (e.g., stocBiO) while maintaining the computational efficiency of a single-loop update; and (ii) it provides the first explicit, fine-grained characterization of the $κ$-dependence for stochastic AID-based single-loop methods. This work demonstrates that SSAID is not merely a heuristic approach, but admits a rigorous theoretical foundation with convergence guarantees competitive with mainstream multi-loop frameworks.

</details>


### [60] [FlexGuard: Continuous Risk Scoring for Strictness-Adaptive LLM Content Moderation](https://arxiv.org/abs/2602.23636)
*Zhihao Ding,Jinming Li,Ze Lu,Jieming Shi*

Main category: cs.LG

TL;DR: 论文提出FlexGuard方法，通过输出校准的连续风险评分来适应不同严格程度的内容审核需求，解决了现有二元分类审核模型在严格程度变化时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM内容安全审核模型大多采用固定的二元分类方式，假设有害内容的定义是固定的。但实际上，不同平台的执行严格程度（如何定义和强制执行有害内容）各不相同且随时间演变，这使得二元审核模型在需求变化时变得脆弱。

Method: 首先引入FlexBench基准，支持在多种严格程度下进行受控评估。然后提出FlexGuard方法，这是一种基于LLM的审核器，输出反映风险严重程度的校准连续风险评分，并通过阈值设定支持特定严格程度的决策。通过风险对齐优化训练FlexGuard以提高评分-严重程度一致性，并提供实用的阈值选择策略以适应部署时的目标严格程度。

Result: 在FlexBench和公共基准测试上的实验表明，FlexGuard实现了更高的审核准确性，并在不同严格程度下显著提高了鲁棒性。现有审核模型在不同严格程度间存在显著的不一致性：在某一严格程度下表现良好的模型在其他严格程度下性能会大幅下降。

Conclusion: FlexGuard通过连续风险评分和阈值调整机制，为LLM内容审核提供了更灵活、适应性更强的解决方案，能够适应不同平台和随时间变化的审核严格程度要求，提高了实际可用性。

Abstract: Ensuring the safety of LLM-generated content is essential for real-world deployment. Most existing guardrail models formulate moderation as a fixed binary classification task, implicitly assuming a fixed definition of harmfulness. In practice, enforcement strictness - how conservatively harmfulness is defined and enforced - varies across platforms and evolves over time, making binary moderators brittle under shifting requirements. We first introduce FlexBench, a strictness-adaptive LLM moderation benchmark that enables controlled evaluation under multiple strictness regimes. Experiments on FlexBench reveal substantial cross-strictness inconsistency in existing moderators: models that perform well under one regime can degrade substantially under others, limiting their practical usability. To address this, we propose FlexGuard, an LLM-based moderator that outputs a calibrated continuous risk score reflecting risk severity and supports strictness-specific decisions via thresholding. We train FlexGuard via risk-alignment optimization to improve score-severity consistency and provide practical threshold selection strategies to adapt to target strictness at deployment. Experiments on FlexBench and public benchmarks demonstrate that FlexGuard achieves higher moderation accuracy and substantially improved robustness under varying strictness. We release the source code and data to support reproducibility.

</details>


### [61] [FedRot-LoRA: Mitigating Rotational Misalignment in Federated LoRA](https://arxiv.org/abs/2602.23638)
*Haoran Zhang,Dongjun Kim,Seohyeon Cha,Haris Vikalo*

Main category: cs.LG

TL;DR: FedRot-LoRA：通过正交变换对齐客户端更新的联邦LoRA框架，解决因低秩分解旋转不变性导致的聚合误差问题


<details>
  <summary>Details</summary>
Motivation: 联邦LoRA在去中心化数据上微调大语言模型时，因子级平均与数学上正确的本地更新聚合之间存在差异，导致显著的聚合误差和不稳定训练。主要问题是旋转不对齐，源于低秩分解的旋转不变性——语义等价的更新在不同客户端可能表示在不同的潜在子空间中。

Method: 提出FedRot-LoRA框架，在聚合前通过正交变换对齐客户端更新。这种对齐在保持语义更新的同时减少跨客户端子空间不匹配，不增加通信成本或限制模型表达能力。

Result: 在自然语言理解和生成任务上的大量实验表明，FedRot-LoRA在各种异构程度和LoRA秩的设置下，始终优于现有的联邦LoRA基线方法。

Conclusion: FedRot-LoRA通过旋转对齐有效解决了联邦LoRA中的聚合误差问题，提供了更稳定和高效的联邦学习框架，同时保持了通信效率和模型表达能力。

Abstract: Federated LoRA provides a communication-efficient mechanism for fine-tuning large language models on decentralized data. In practice, however, a discrepancy between the factor-wise averaging used to preserve low rank and the mathematically correct aggregation of local updates can cause significant aggregation error and unstable training. We argue that a major source of this problem is rotational misalignment, arising from the rotational invariance of low-rank factorizations -- semantically equivalent updates can be represented in different latent subspaces across clients since $(B_i R_i)(R_i^\top A_i) = B_i A_i$. When such misaligned factors are averaged directly, they interfere destructively and degrade the global update. To address this issue, we propose FedRot-LoRA, a federated LoRA framework that aligns client updates via orthogonal transformations prior to aggregation. This alignment preserves the semantic update while reducing cross-client subspace mismatch, without increasing communication cost or restricting model expressivity. We provide a convergence analysis that examines the aggregation error induced by factor-wise averaging and shows how rotational alignment yields a tighter upper bound on this error. Extensive experiments on natural language understanding and generative tasks demonstrate that FedRot-LoRA consistently outperforms existing federated LoRA baselines across a range of heterogeneity levels and LoRA ranks.

</details>


### [62] [Selective Denoising Diffusion Model for Time Series Anomaly Detection](https://arxiv.org/abs/2602.23662)
*Kohei Obata,Zheng Chen,Yasuko Matsubara,Lingwei Zhu,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 提出AnomalyFilter方法，通过选择性过滤仅去噪异常部分而保留正常部分，改进基于扩散模型的时间序列异常检测性能


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的TSAD方法采用条件策略，从白噪声重建输入实例，但难以准确重建正常部分，导致检测性能不佳

Method: 提出AnomalyFilter方法，在训练阶段掩码高斯噪声，在去噪过程中不向实例添加噪声，构建选择性过滤器仅处理异常部分

Result: 在五个数据集上的实验表明，AnomalyFilter在正常部分实现了显著较低的重建误差，为异常检测有效性提供了实证支持

Conclusion: AnomalyFilter代表了专门为TSAD定制的扩散模型噪声设计的开创性方法，通过两个简单组件的协同作用显著提升了基础扩散模型的性能

Abstract: Time series anomaly detection (TSAD) has been an important area of research for decades, with reconstruction-based methods, mostly based on generative models, gaining popularity and demonstrating success. Diffusion models have recently attracted attention due to their advanced generative capabilities. Existing diffusion-based methods for TSAD rely on a conditional strategy, which reconstructs input instances from white noise with the aid of the conditioner. However, this poses challenges in accurately reconstructing the normal parts, resulting in suboptimal detection performance. In response, we propose a novel diffusion-based method, named AnomalyFilter, which acts as a selective filter that only denoises anomaly parts in the instance while retaining normal parts. To build such a filter, we mask Gaussian noise during the training phase and conduct the denoising process without adding noise to the instances. The synergy of the two simple components greatly enhances the performance of naive diffusion models. Extensive experiments on five datasets demonstrate that AnomalyFilter achieves notably low reconstruction error on normal parts, providing empirical support for its effectiveness in anomaly detection. AnomalyFilter represents a pioneering approach that focuses on the noise design of diffusion models specifically tailored for TSAD.

</details>


### [63] [Disentangled Mode-Specific Representations for Tensor Time Series via Contrastive Learning](https://arxiv.org/abs/2602.23663)
*Kohei Obata,Taichi Murayama,Zheng Chen,Yasuko Matsubara,Yasushi Sakurai*

Main category: cs.LG

TL;DR: MoST是一种专门为多模态张量时间序列设计的表示学习方法，通过张量切片降低复杂度，学习可解耦为各非时间模态的表示，在分类和预测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态张量时间序列在搜索引擎和环境监测等领域普遍存在，学习其表示对多种应用有益，但张量的复杂性阻碍了丰富表示的实现。

Method: MoST使用张量切片方法降低TTS结构复杂度，学习可解耦为各非时间模态的表示。每个表示包含模态特定特征（同一模态内变量间关系）和模态不变特征（不同模态共有）。采用对比学习框架，损失函数包含模态特定和模态不变两部分，有效利用解耦表示作为增强。

Result: 在真实世界数据集上的大量实验表明，MoST在分类和预测准确性方面持续优于最先进的方法。

Conclusion: MoST是一种专门为多模态张量时间序列设计的有效表示学习方法，通过解耦表示学习模态特定和模态不变特征，在多种任务上表现优异。

Abstract: Multi-mode tensor time series (TTS) can be found in many domains, such as search engines and environmental monitoring systems. Learning representations of a TTS benefits various applications, but it is also challenging since the complexities inherent in the tensor hinder the realization of rich representations. In this paper, we propose a novel representation learning method designed specifically for TTS, namely MoST. Specifically, MoST uses a tensor slicing approach to reduce the complexity of the TTS structure and learns representations that can be disentangled into individual non-temporal modes. Each representation captures mode-specific features, which are the relationship between variables within the same mode, and mode-invariant features, which are in common in representations of different modes. We employ a contrastive learning framework to learn parameters; the loss function comprises two parts intended to learn representation in a mode-specific way and mode-invariant way, effectively exploiting disentangled representations as augmentations. Extensive experiments on real-world datasets show that MoST consistently outperforms the state-of-the-art methods in terms of classification and forecasting accuracy. Code is available at https://github.com/KoheiObata/MoST.

</details>


### [64] [Optimizer-Induced Low-Dimensional Drift and Transverse Dynamics in Transformer Training](https://arxiv.org/abs/2602.23696)
*Yongzhong Xu*

Main category: cs.LG

TL;DR: 研究发现小Transformer模型的训练轨迹呈现主导漂移方向与横向残余动力学的几何结构，优化器选择显著影响轨迹的有效维度和结构


<details>
  <summary>Details</summary>
Motivation: 研究训练轨迹的几何结构，了解优化器如何影响参数更新的组织方式，超越仅从损失值观察到的信息

Method: 使用未中心化、行归一化的轨迹PCA分析小Transformer模型的参数更新，比较AdamW和SGD变体在匹配损失水平下的轨迹几何差异

Result: 参数更新组织成主导漂移方向加横向残余动力学；AdamW产生多维漂移结构，SGD产生近似共线参数演化；瞬时梯度与主导方向对齐度低；再加热选择性地扰动横向分量

Conclusion: 优化器选择塑造了学习轨迹的有效维度和结构，这些特征无法仅从损失值中观察到，表明训练动态具有重要的几何结构

Abstract: We study the geometry of training trajectories in small transformer models and find that parameter updates organize into a dominant drift direction with transverse residual dynamics. Using uncentered, row-normalized trajectory PCA, we show that a single direction captures a large fraction of cumulative parameter movement early in training, while remaining components encode oscillatory behavior in auxiliary probe performance. Instantaneous gradients exhibit little alignment with this dominant direction, indicating that it arises from accumulated optimizer updates rather than per-batch gradient structure. Comparing AdamW with SGD variants at matched loss levels reveals substantial differences in trajectory geometry: AdamW develops multi-dimensional drift structure, whereas SGD-family optimizers produce nearly colinear parameter evolution and weaker probe dynamics. Reheating selectively perturbs transverse components with minimal effect on the dominant drift coordinate. These findings suggest that optimizer choice shapes the effective dimensionality and structure of learning trajectories beyond what is apparent from loss values alone.

</details>


### [65] [Bridging Dynamics Gaps via Diffusion Schrödinger Bridge for Cross-Domain Reinforcement Learning](https://arxiv.org/abs/2602.23737)
*Hanping Zhang,Yuhong Guo*

Main category: cs.LG

TL;DR: 提出BDGxRL框架，利用扩散薛定谔桥对齐源域和目标域动态，通过奖励调制机制估计奖励，在源域内完成目标导向的策略学习，无需访问目标环境或奖励信号。


<details>
  <summary>Details</summary>
Motivation: 跨域强化学习面临的主要挑战是缺乏目标域环境交互和奖励监督，这阻碍了直接策略学习。需要一种方法能够在源域内学习可迁移到目标域的策略。

Method: 提出BDGxRL框架：1) 使用扩散薛定谔桥将源域转移与目标域离线演示编码的动态对齐；2) 引入奖励调制机制，基于状态转移估计奖励，应用于DSB对齐样本，确保奖励与目标域动态一致性。

Result: 在MuJoCo跨域基准测试中，BDGxRL优于现有最先进方法，在转移动态变化下表现出强大的适应能力。

Conclusion: BDGxRL能够在无需访问目标环境或奖励的情况下，在源域内完成目标导向的策略学习，有效解决了跨域强化学习中的动态差异问题。

Abstract: Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains. A key challenge lies in the lack of target-domain environment interaction and reward supervision, which prevents direct policy learning. To address this challenge, we propose Bridging Dynamics Gaps for Cross-Domain Reinforcement Learning (BDGxRL), a novel framework that leverages Diffusion Schrödinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. Moreover, we introduce a reward modulation mechanism that estimates rewards based on state transitions, applying to DSB-aligned samples to ensure consistency between rewards and target-domain dynamics. BDGxRL performs target-oriented policy learning entirely within the source domain, without access to the target environment or its rewards. Experiments on MuJoCo cross-domain benchmarks demonstrate that BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts.

</details>


### [66] [OPTIAGENT: A Physics-Driven Agentic Framework for Automated Optical Design](https://arxiv.org/abs/2602.23761)
*Yuyu Geng,Lei Sun,Yao Gao,Xinxin Hu,Zhonghua Yi,Xiaolong Qian,Weijian Hu,Jian Bai,Kaiwei Wang*

Main category: cs.LG

TL;DR: 首次将大语言模型应用于光学设计领域，通过混合训练目标和物理驱动的策略对齐，使非专业用户能够设计功能性透镜系统


<details>
  <summary>Details</summary>
Motivation: 光学设计是高度非凸的优化问题，严重依赖专家经验和领域知识。虽然大语言模型具备丰富的光学知识，但在透镜系统设计方面的能力仍然受限。本研究旨在填补这一空白，让没有正式光学训练的用户也能成功开发功能性透镜系统。

Method: 1. 构建OptiDesignQA数据集，包含经典教科书透镜系统和自动设计算法生成的新配置；2. 通过全系统合成和透镜补全的混合目标将领域知识注入LLM；3. 使用DrGRPO算法和光学词典奖励进行物理驱动的策略对齐；4. 与专业光学优化例程集成进行端到端微调和精度优化。

Result: 实验结果表明，该方法在传统优化基自动设计算法和LLM对比方法中表现出优越性，能够有效设计功能性透镜系统。

Conclusion: 本研究首次成功将大语言模型应用于光学设计领域，通过创新的训练策略和物理对齐方法，显著提升了LLM在光学设计中的能力，为非专业用户参与光学设计提供了可行途径。

Abstract: Optical design is the process of configuring optical elements to precisely manipulate light for high-fidelity imaging. It is inherently a highly non-convex optimization problem that relies heavily on human heuristic expertise and domain-specific knowledge. While Large Language Models (LLMs) possess extensive optical knowledge, their capabilities in leveraging the knowledge in designing lens system remain significantly constrained. This work represents the first attempt to employ LLMs in the field of optical design. We bridge the expertise gap by enabling users without formal optical training to successfully develop functional lens systems. Concretely, we curate a comprehensive dataset, named OptiDesignQA, which encompasses both classical lens systems sourced from standard optical textbooks and novel configurations generated by automated design algorithms for training and evaluation. Furthermore, we inject domain-specific optical expertise into the LLM through a hybrid objective of full-system synthesis and lens completion. To align the model with optical principles, we employ Group Relative Policy Optimization Done Right (DrGRPO) guided by Optical Lexicographic Reward for physics-driven policy alignment. This reward system incorporates structural format rewards, physical feasibility rewards, light-manipulation accuracy, and LLM-based heuristics. Finally, our model integrates with specialized optical optimization routines for end-to-end fine-tuning and precision refinement. We benchmark our proposed method against both traditional optimization-based automated design algorithms and LLM counterparts, and experimental results show the superiority of our method.

</details>


### [67] [MAGE: Multi-scale Autoregressive Generation for Offline Reinforcement Learning](https://arxiv.org/abs/2602.23770)
*Chenxing Lin,Xinhui Gao,Haipeng Zhang,Xinran Li,Haitao Wang,Songzhu Mei,Chenglu Wen,Weiquan Liu,Siqi Shen,Cheng Wang*

Main category: cs.LG

TL;DR: MAGE是一个多尺度自回归生成式离线强化学习方法，通过多尺度轨迹建模解决长视野稀疏奖励任务中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式离线强化学习方法在长视野稀疏奖励任务中表现不佳，传统分层生成方法忽略了轨迹固有的多尺度时间结构，导致性能次优。

Method: MAGE包含条件引导的多尺度自编码器学习分层轨迹表示，以及多尺度Transformer从粗到细的时间尺度自回归生成轨迹表示，并使用条件引导解码器精确控制短期行为。

Result: 在五个离线强化学习基准测试中与十五个基线算法对比，MAGE成功将多尺度轨迹建模与条件引导相结合，在长视野稀疏奖励设置中生成连贯可控的轨迹。

Conclusion: MAGE通过多尺度自回归生成方法有效捕捉轨迹的多分辨率时间依赖关系，解决了长视野稀疏奖励任务中的挑战。

Abstract: Generative models have gained significant traction in offline reinforcement learning (RL) due to their ability to model complex trajectory distributions. However, existing generation-based approaches still struggle with long-horizon tasks characterized by sparse rewards. Some hierarchical generation methods have been developed to mitigate this issue by decomposing the original problem into shorter-horizon subproblems using one policy and generating detailed actions with another. While effective, these methods often overlook the multi-scale temporal structure inherent in trajectories, resulting in suboptimal performance. To overcome these limitations, we propose MAGE, a Multi-scale Autoregressive GEneration-based offline RL method. MAGE incorporates a condition-guided multi-scale autoencoder to learn hierarchical trajectory representations, along with a multi-scale transformer that autoregressively generates trajectory representations from coarse to fine temporal scales. MAGE effectively captures temporal dependencies of trajectories at multiple resolutions. Additionally, a condition-guided decoder is employed to exert precise control over short-term behaviors. Extensive experiments on five offline RL benchmarks against fifteen baseline algorithms show that MAGE successfully integrates multi-scale trajectory modeling with conditional guidance, generating coherent and controllable trajectories in long-horizon sparse-reward settings.

</details>


### [68] [Provable Subspace Identification of Nonlinear Multi-view CCA](https://arxiv.org/abs/2602.23785)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 该论文研究了多视角非线性典型相关分析（CCA）的可识别性问题，证明了在多视角设置下，即使存在非线性映射，CCA仍能恢复共享的潜在子空间，但存在视角间的正交模糊性。


<details>
  <summary>Details</summary>
Motivation: 传统非线性CCA的精确解混被证明是病态问题，作者希望重新构建多视角CCA作为一个基不变子空间识别问题，探索在什么条件下能够识别出共享的潜在结构。

Method: 将多视角CCA重新定义为基不变子空间识别问题，在适当的潜在先验和谱分离条件下，通过理论证明多视角CCA能够恢复成对相关的信号子空间。对于N≥3个视角，目标函数能够分离出所有视角共享的联合相关子空间，同时消除视角私有变异。

Result: 证明了多视角CCA能够恢复成对相关的信号子空间，但存在视角间的正交模糊性。对于三个或更多视角，能够识别出所有视角共享的联合相关子空间。通过谱扰动理论建立了有限样本一致性保证，将经验交叉协方差的集中性转化为明确的子空间误差界限。

Conclusion: 多视角非线性CCA在适当的条件下是可识别的，能够恢复共享的潜在子空间结构，但存在视角间的正交模糊性。理论结果在合成和渲染图像数据集上得到了验证，并确认了假设条件的必要性。

Abstract: We investigate the identifiability of nonlinear Canonical Correlation Analysis (CCA) in a multi-view setup, where each view is generated by an unknown nonlinear map applied to a linear mixture of shared latents and view-private noise. Rather than attempting exact unmixing, a problem proven to be ill-posed, we instead reframe multi-view CCA as a basis-invariant subspace identification problem. We prove that, under suitable latent priors and spectral separation conditions, multi-view CCA recovers the pairwise correlated signal subspaces up to view-wise orthogonal ambiguity. For $N \geq 3$ views, the objective provably isolates the jointly correlated subspaces shared across all views while eliminating view-private variations. We further establish finite-sample consistency guarantees by translating the concentration of empirical cross-covariances into explicit subspace error bounds via spectral perturbation theory. Experiments on synthetic and rendered image datasets validate our theoretical findings and confirm the necessity of the assumed conditions.

</details>


### [69] [UPath: Universal Planner Across Topological Heterogeneity For Grid-Based Pathfinding](https://arxiv.org/abs/2602.23789)
*Aleksandr Ananikian,Daniil Drozdov,Konstantin Yakovlev*

Main category: cs.LG

TL;DR: 该论文提出了一种通用的启发式预测器，能够训练一次即可泛化到各种未见过的任务，显著提升A*算法在网格路径规划中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的启发式方法主要依赖训练和测试网格地图来自相同分布（如城市地图、室内地图等），在分布外任务上表现不佳，这限制了实际应用中需要通用求解器的需求。

Method: 设计了一个通用的启发式预测器模型，通过一次训练就能泛化到完全不同的未见任务上，解决了现有方法对训练和测试数据分布一致性的依赖问题。

Result: 该方法将A*算法的计算量减少了高达2.2倍，同时在完全不同于训练数据的任务上，平均仍能提供与最优解成本相差3%以内的解决方案。

Conclusion: 这是首次通过可学习求解器实现在完全不同于训练数据的任务上达到这一里程碑，为通用路径规划求解器的发展提供了重要突破。

Abstract: The performance of search algorithms for grid-based pathfinding, e.g. A*, critically depends on the heuristic function that is used to focus the search. Recent studies have shown that informed heuristics that take the positions/shapes of the obstacles into account can be approximated with the deep neural networks. Unfortunately, the existing learning-based approaches mostly rely on the assumption that training and test grid maps are drawn from the same distribution (e.g., city maps, indoor maps, etc.) and perform poorly on out-of-distribution tasks. This naturally limits their application in practice when often a universal solver is needed that is capable of efficiently handling any problem instance. In this work, we close this gap by designing an universal heuristic predictor: a model trained once, but capable of generalizing across a full spectrum of unseen tasks. Our extensive empirical evaluation shows that the suggested approach halves the computational effort of A* by up to a factor of 2.2, while still providing solutions within 3% of the optimal cost on average altogether on the tasks that are completely different from the ones used for training $\unicode{x2013}$ a milestone reached for the first time by a learnable solver.

</details>


### [70] [GRAIL: Post-hoc Compensation by Linear Reconstruction for Compressed Networks](https://arxiv.org/abs/2602.23795)
*Wenwu Tang,Dong Wang,Lothar Thiele,Olga Saukh*

Main category: cs.LG

TL;DR: GRAIL是一种无需微调的后处理块补偿方法，通过小规模校准集恢复压缩模型的块级输入输出行为，使用Gram矩阵和岭回归线性重建隐藏表示，可应用于多种压缩选择器。


<details>
  <summary>Details</summary>
Motivation: 结构化深度模型压缩方法虽然硬件友好且能显著降低内存和推理成本，但在激进压缩下会导致精度下降，而后续微调可能因缺少标注数据或训练成本高而不切实际。

Method: 提出GRAIL方法：1）使用小校准集总结隐藏激活的Gram矩阵；2）应用岭回归从压缩后的隐藏表示线性重建原始表示；3）将重建映射吸收到下游投影权重中，同时压缩上游层。

Result: 在ResNets、ViTs和解码器LLMs上，GRAIL在实用压缩范围内持续提升数据无关和数据感知的剪枝或折叠基线的精度或困惑度，开销可控且无需反向传播。

Conclusion: GRAIL是一种简单、无需微调的后处理补偿方法，对压缩选择器无关，仅需少量前向传播无需梯度或标签，能有效恢复压缩模型的性能。

Abstract: Structured deep model compression methods are hardware-friendly and substantially reduce memory and inference costs. However, under aggressive compression, the resulting accuracy degradation often necessitates post-compression finetuning, which can be impractical due to missing labeled data or high training cost. We propose post-hoc blockwise compensation, called GRAIL, a simple zero-finetuning step applied after model compression that restores each block's input-output behavior using a small calibration set. The method summarizes hidden activations via a Gram matrix and applies ridge regression to linearly reconstruct the original hidden representation from the reduced one. The resulting reconstruction map is absorbed into the downstream projection weights, while the upstream layer is compressed. The approach is selector-agnostic (Magnitude, Wanda, Gram-based selection, or folding), data-aware (requiring only a few forward passes without gradients or labels), and recovers classic pruning or folding when the Gram matrix is near identity, indicating weak inter-channel correlations. Across ResNets, ViTs, and decoder-only LLMs, GRAIL consistently improves accuracy or perplexity over data-free and data-aware pruning or folding baselines in practical compression regimes, with manageable overhead and no backpropagation. The code is available at https://github.com/TWWinde/GRAIL.

</details>


### [71] [MPU: Towards Secure and Privacy-Preserving Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2602.23798)
*Tiantong Wang,Xinyu Yan,Tiantong Wu,Yurong Hao,Yong Jiang,Fei Huang,Wei Yang Bryan Lim*

Main category: cs.LG

TL;DR: MPU是一种算法无关的隐私保护多扰动副本遗忘框架，通过服务器端预处理的随机副本生成和后处理的更新聚合，在双重非披露约束下实现大语言模型的机器遗忘。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的机器遗忘面临隐私困境：严格约束禁止共享服务器参数或客户端遗忘集。需要解决这种双重非披露约束，让客户端能在本地私有遗忘集上执行遗忘，同时不访问服务器的确切原始参数。

Method: 提出MPU框架，包含两个服务器端模块：1)预处理：服务器分发多个扰动和重参数化的模型实例；2)后处理：客户端本地执行遗忘后，服务器反转重参数化并通过谐波去噪过程聚合更新以减轻扰动影响。

Result: 在七种遗忘算法上的实验表明，MPU在10%噪声下达到与无噪声基线相当的遗忘性能，大多数算法的平均性能下降远低于1%，在1%噪声下某些算法甚至能超越无噪声基线。

Conclusion: MPU框架有效解决了大语言模型机器遗忘中的双重隐私约束问题，通过扰动副本和聚合机制实现了隐私保护下的高效遗忘，为实际应用提供了可行的解决方案。

Abstract: Machine unlearning for large language models often faces a privacy dilemma in which strict constraints prohibit sharing either the server's parameters or the client's forget set. To address this dual non-disclosure constraint, we propose MPU, an algorithm-agnostic privacy-preserving Multiple Perturbed Copies Unlearning framework that primarily introduces two server-side modules: Pre-Process for randomized copy generation and Post-Process for update aggregation. In Pre-Process, the server distributes multiple perturbed and reparameterized model instances, allowing the client to execute unlearning locally on its private forget set without accessing the server's exact original parameters. After local unlearning, the server performs Post-Process by inverting the reparameterization and aggregating updates with a harmonic denoising procedure to alleviate the impact of perturbation. Experiments with seven unlearning algorithms show that MPU achieves comparable unlearning performance to noise-free baselines, with most algorithms' average degradation well below 1% under 10% noise, and can even outperform the noise-free baseline for some algorithms under 1% noise. Code is available at https://github.com/Tristan-SHU/MPU.

</details>


### [72] [Beyond State-Wise Mirror Descent: Offline Policy Optimization with Parameteric Policies](https://arxiv.org/abs/2602.23811)
*Xiang Li,Nan Jiang,Yuheng Zhang*

Main category: cs.LG

TL;DR: 本文研究了离线强化学习在一般函数逼近下的理论问题，解决了现有算法只能处理有限小动作空间、无法适应独立策略参数化的局限性，将理论保证扩展到大规模或连续动作空间的参数化策略类。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习算法存在两个主要限制：1）计算可行的算法（如PSPI）仅适用于有限小动作空间；2）这些算法依赖状态级镜像下降，需要从价值函数隐式推导策略，无法适应实践中普遍使用的独立策略参数化。

Method: 通过将镜像下降扩展到参数化策略，识别上下文耦合为核心难点，并将镜像下降与自然策略梯度连接起来，从而获得新的分析、理论保证和算法见解，包括离线强化学习与模仿学习的统一。

Result: 成功将离线强化学习的理论保证扩展到参数化策略类，适用于大规模或连续动作空间，并建立了离线强化学习与模仿学习之间的理论联系。

Conclusion: 通过连接镜像下降和自然策略梯度，解决了离线强化学习中参数化策略的理论挑战，为实际应用提供了更广泛的算法框架，并揭示了离线强化学习与模仿学习之间的深刻联系。

Abstract: We investigate the theoretical aspects of offline reinforcement learning (RL) under general function approximation. While prior works (e.g., Xie et al., 2021) have established the theoretical foundations of learning a good policy from offline data via pessimism, existing algorithms that are computationally tractable (often in an oracle-efficient sense), such as PSPI, only apply to finite and small action spaces. Moreover, these algorithms rely on state-wise mirror descent and require actors to be implicitly induced from the critic functions, failing to accommodate standalone policy parameterization which is ubiquitous in practice. In this work, we address these limitations and extend the theoretical guarantees to parameterized policy classes over large or continuous action spaces. When extending mirror descent to parameterized policies, we identify contextual coupling as the core difficulty, and show how connecting mirror descent to natural policy gradient leads to novel analyses, guarantees, and algorithmic insights, including a surprising unification between offline RL and imitation learning.

</details>


### [73] [Learning to maintain safety through expert demonstrations in settings with unknown constraints: A Q-learning perspective](https://arxiv.org/abs/2602.23816)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 提出SafeQIL算法，在未知约束的MDP中从轨迹演示学习策略，平衡奖励最大化与安全性，通过Q值评估状态-动作对的"承诺度"。


<details>
  <summary>Details</summary>
Motivation: 在具有可观测奖励但约束未知、成本不可观测的受限MDP中，从安全执行任务的轨迹演示学习策略，需要在保守性与高奖励轨迹可能性之间取得平衡。

Method: 提出SafeQIL算法，通过Q值量化状态-动作对的"承诺度"，结合任务特定奖励和状态安全性评估，实现安全Q学习的逆强化学习视角。

Result: 在具有挑战性的基准任务上与最先进的逆约束强化学习算法进行比较，展示了SafeQIL算法的优势。

Conclusion: SafeQIL算法能够有效学习最大化有"承诺"轨迹概率的策略，在奖励期望和安全性之间取得良好平衡。

Abstract: Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps. Having these objectives, we aim towards learning a policy that maximizes the probability of the most $promising$ trajectories with respect to the demonstrations. In so doing, we formulate the ``promise" of individual state-action pairs in terms of $Q$ values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. This entails a safe Q-learning perspective of the inverse learning problem under constraints: The devised Safe $Q$ Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits.

</details>


### [74] [Inferring Chronic Treatment Onset from ePrescription Data: A Renewal Process Approach](https://arxiv.org/abs/2602.23824)
*Pavlin G. Poličar,Dalibor Stanimirović,Blaž Zupan*

Main category: cs.LG

TL;DR: 提出基于处方动态的慢性病治疗起始时间推断框架，通过建模处方更新过程检测从偶发性到持续性治疗的转变点


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据常存在左截断问题，导致诊断记录不完整且不可靠，而门诊处方形成的更新轨迹能提供疾病管理的连续信号

Method: 提出概率框架，将处方动态建模为更新过程，通过变化点检测区分偶发性处方（泊松基线）和持续性治疗（威布尔更新模型）两种机制

Result: 在全国240万人的电子处方数据集上验证，相比基于规则的简单触发方法，该方法能提供更合理的时间起始估计，显著减少左截断下的不合理早期检测

Conclusion: 检测性能因疾病而异，与处方密度密切相关，揭示了基于治疗的起始时间推断方法的优势和局限性

Abstract: Longitudinal electronic health record (EHR) data are often left-censored, making diagnosis records incomplete and unreliable for determining disease onset. In contrast, outpatient prescriptions form renewal-based trajectories that provide a continuous signal of disease management. We propose a probabilistic framework to infer chronic treatment onset by modeling prescription dynamics as a renewal process and detecting transitions from sporadic to sustained therapy via change-point detection between a baseline Poisson (sporadic prescribing) regime and a regime-specific Weibull (sustained therapy) renewal model. Using a nationwide ePrescription dataset of 2.4 million individuals, we show that the approach yields more temporally plausible onset estimates than naive rule-based triggering, substantially reducing implausible early detections under strong left censoring. Detection performance varies across diseases and is strongly associated with prescription density, highlighting both the strengths and limits of treatment-based onset inference.

</details>


### [75] [FedNSAM:Consistency of Local and Global Flatness for Federated Learning](https://arxiv.org/abs/2602.23827)
*Junkang Liu,Fanhua Shang,Yuxuan Tian,Hongying Liu,Yuanyuan Liu*

Main category: cs.LG

TL;DR: FedNSAM算法通过引入全局Nesterov动量来协调全局和局部平坦度一致性，解决联邦学习中数据异构性导致局部平坦度与全局平坦度不一致的问题，从而提升全局模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中多步局部更新和数据异构性通常导致更尖锐的全局最小值，这会降低全局模型的性能。现有方法将SAM集成到局部训练中，但在高数据异构性设置下，局部训练的平坦度并不代表全局模型的平坦度，因此需要协调全局和局部平坦度的一致性。

Method: 提出FedNSAM算法，通过引入全局Nesterov动量到局部更新中，使用全局Nesterov动量作为客户端全局扰动的局部估计方向和外推方向，从而加速SAM算法并协调全局和局部平坦度的一致性。

Result: 理论上证明了FedNSAM比FedSAM具有更紧的收敛界；实证上在CNN和Transformer模型上进行了全面实验，验证了FedNSAM的优越性能和效率。

Conclusion: FedNSAM通过协调全局和局部平坦度一致性，有效解决了联邦学习中数据异构性导致的泛化问题，在理论和实验上都表现出优越性能。

Abstract: In federated learning (FL), multi-step local updates and data heterogeneity usually lead to sharper global minima, which degrades the performance of the global model. Popular FL algorithms integrate sharpness-aware minimization (SAM) into local training to address this issue. However, in the high data heterogeneity setting, the flatness in local training does not imply the flatness of the global model. Therefore, minimizing the sharpness of the local loss surfaces on the client data does not enable the effectiveness of SAM in FL to improve the generalization ability of the global model. We define the \textbf{flatness distance} to explain this phenomenon. By rethinking the SAM in FL and theoretically analyzing the \textbf{flatness distance}, we propose a novel \textbf{FedNSAM} algorithm that accelerates the SAM algorithm by introducing global Nesterov momentum into the local update to harmonize the consistency of global and local flatness. \textbf{FedNSAM} uses the global Nesterov momentum as the direction of local estimation of client global perturbations and extrapolation. Theoretically, we prove a tighter convergence bound than FedSAM by Nesterov extrapolation. Empirically, we conduct comprehensive experiments on CNN and Transformer models to verify the superior performance and efficiency of \textbf{FedNSAM}. The code is available at https://github.com/junkangLiu0/FedNSAM.

</details>


### [76] [ULW-SleepNet: An Ultra-Lightweight Network for Multimodal Sleep Stage Scoring](https://arxiv.org/abs/2602.23852)
*Zhaowen Wang,Dongdong Zhou,Qi Xu,Fengyu Cong,Mohammad Al-Sa'd,Jenni Raitoharju*

Main category: cs.LG

TL;DR: ULW-SleepNet是一个超轻量级多模态睡眠分期框架，通过创新的双流可分离卷积块等技术，在保持竞争力的准确率同时大幅减少计算开销，适用于可穿戴和物联网设备的实时睡眠监测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型计算量大且主要针对单通道EEG设计，限制了在多模态PSG数据上的实用性，需要开发更轻量、高效的多模态睡眠分期方法。

Method: 提出ULW-SleepNet框架，采用双流可分离卷积块、深度可分离卷积、通道参数共享和全局平均池化等技术，高效整合多种生理信号信息。

Result: 在Sleep-EDF-20和Sleep-EDF-78数据集上分别达到86.9%和81.4%的准确率，仅需13.3K参数和7.89M FLOPs，相比现有方法参数减少高达98.6%。

Conclusion: ULW-SleepNet在保持竞争力的准确率同时显著降低了计算复杂度，展示了在可穿戴和物联网设备上进行实时睡眠监测的强大潜力。

Abstract: Automatic sleep stage scoring is crucial for the diagnosis and treatment of sleep disorders. Although deep learning models have advanced the field, many existing models are computationally demanding and designed for single-channel electroencephalography (EEG), limiting their practicality for multimodal polysomnography (PSG) data. To overcome this, we propose ULW-SleepNet, an ultra-lightweight multimodal sleep stage scoring framework that efficiently integrates information from multiple physiological signals. ULW-SleepNet incorporates a novel Dual-Stream Separable Convolution (DSSC) Block, depthwise separable convolutions, channel-wise parameter sharing, and global average pooling to reduce computational overhead while maintaining competitive accuracy. Evaluated on the Sleep-EDF-20 and Sleep-EDF-78 datasets, ULW-SleepNet achieves accuracies of 86.9% and 81.4%, respectively, with only 13.3K parameters and 7.89M FLOPs. Compared to state-of-the-art methods, our model reduces parameters by up to 98.6% with only marginal performance loss, demonstrating its strong potential for real-time sleep monitoring on wearable and IoT devices. The source code for this study is publicly available at https://github.com/wzw999/ULW-SLEEPNET.

</details>


### [77] [A Theory of Random Graph Shift in Truncated-Spectrum vRKHS](https://arxiv.org/abs/2602.23880)
*Zhang Wan,Tingting Mu,Samuel Kaski*

Main category: cs.LG

TL;DR: 该论文从随机图生成角度发展了图分类的域适应理论，通过随机图模型分析域偏移，推导出包含域差异、谱几何和振幅三项因子的泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有域适应理论主要处理传统数据分布偏移，但图数据的非欧几里得特性和专用图学习架构使得图分布偏移的细粒度分析变得复杂。需要开发专门的理论框架来分析图分类中的域偏移问题。

Method: 提出基于随机图模型的数据生成过程假设，利用向量值再生核希尔伯特空间（vRKHS）公式化，推导出包含三项因子的泛化界：域差异项、谱几何项（通过截断谱总结）和振幅项（聚合收敛和构造稳定性效应）。

Result: 理论推导出可分解的泛化界，并通过真实数据和模拟实验验证了各项因子的理论见解。该框架为图分类中的域偏移提供了细粒度分析工具。

Conclusion: 该论文建立了图分类域适应的理论框架，通过随机图模型视角和vRKHS公式化，提供了可解释的泛化界分解，为理解图分布偏移的机制和设计更有效的图域适应方法奠定了基础。

Abstract: This paper develops a theory of graph classification under domain shift through a random-graph generative lens, where we consider intra-class graphs sharing the same random graph model (RGM) and the domain shift induced by changes in RGM components. While classic domain adaptation (DA) theories have well-underpinned existing techniques to handle graph distribution shift, the information of graph samples, which are itself structured objects, is less explored. The non-Euclidean nature of graphs and specialized architectures for graph learning further complicate a fine-grained analysis of graph distribution shifts. In this paper, we propose a theory that assumes RGM as the data generative process, exploiting its connection to hypothesis complexity in function space perspective for such fine-grained analysis. Building on a vector-valued reproducing kernel Hilbert space (vRKHS) formulation, we derive a generalization bound whose shift penalty admits a factorization into (i) a domain discrepancy term, (ii) a spectral-geometry term summarized by the accessible truncated spectrum, and (iii) an amplitude term that aggregates convergence and construction-stability effects. We empirically verify the insights on these terms in both real data and simulations.

</details>


### [78] [Hierarchical Concept-based Interpretable Models](https://arxiv.org/abs/2602.23947)
*Oscar Hill,Mateo Espinosa Zarlenga,Mateja Jamnik*

Main category: cs.LG

TL;DR: 提出分层概念嵌入模型（HiCEMs）来建模概念间关系，并引入概念分割方法自动从预训练模型中挖掘细粒度子概念，减少标注负担，提升模型可解释性和干预能力。


<details>
  <summary>Details</summary>
Motivation: 现有概念嵌入模型（CEMs）存在两个主要问题：1）无法表示概念间的层次关系；2）需要不同粒度的概念标注，限制了实际应用。需要一种既能建模概念关系又能减少标注负担的方法。

Method: 提出分层概念嵌入模型（HiCEMs），通过层次结构显式建模概念关系。引入概念分割方法，从预训练CEM的嵌入空间中自动发现细粒度子概念，无需额外标注。使用PseudoKitchens等数据集进行评估。

Result: 1）概念分割方法能够发现训练时未标注的人类可解释子概念，可用于训练高精度的HiCEMs；2）HiCEMs支持不同粒度的测试时概念干预，提高了任务准确性。

Conclusion: HiCEMs通过层次化建模概念关系和自动概念分割，显著提升了概念嵌入模型的可解释性和实用性，减少了标注需求，增强了模型的干预能力。

Abstract: Modern deep neural networks remain challenging to interpret due to the opacity of their latent representations, impeding model understanding, debugging, and debiasing. Concept Embedding Models (CEMs) address this by mapping inputs to human-interpretable concept representations from which tasks can be predicted. Yet, CEMs fail to represent inter-concept relationships and require concept annotations at different granularities during training, limiting their applicability. In this paper, we introduce Hierarchical Concept Embedding Models (HiCEMs), a new family of CEMs that explicitly model concept relationships through hierarchical structures. To enable HiCEMs in real-world settings, we propose Concept Splitting, a method for automatically discovering finer-grained sub-concepts from a pretrained CEM's embedding space without requiring additional annotations. This allows HiCEMs to generate fine-grained explanations from limited concept labels, reducing annotation burdens. Our evaluation across multiple datasets, including a user study and experiments on PseudoKitchens, a newly proposed concept-based dataset of 3D kitchen renders, demonstrates that (1) Concept Splitting discovers human-interpretable sub-concepts absent during training that can be used to train highly accurate HiCEMs, and (2) HiCEMs enable powerful test-time concept interventions at different granularities, leading to improved task accuracy.

</details>


### [79] [MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening](https://arxiv.org/abs/2602.23994)
*Vrushank Ahire,Yogesh Kumar,Anouck Girard,M. A. Ganaie*

Main category: cs.LG

TL;DR: MINT框架通过三阶段跨模态知识转移，将MRI的生物标志物结构转移到语音编码器中，实现无需神经影像的阿尔茨海默病早期筛查。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期筛查中，神经影像（如MRI）成本高且部署受限，而语音分析虽然非侵入但缺乏生物学基础。现有语音分类器独立于神经影像开发，决策边界缺乏生物学依据，在区分认知正常与轻度认知障碍时可靠性有限。

Method: 提出MINT三阶段跨模态框架：1）训练MRI教师模型（1228名受试者）定义紧凑的神经影像嵌入空间；2）通过残差投影头和几何损失将语音表示对齐到冻结的影像流形；3）推理时应用冻结的MRI分类器到对齐的语音嵌入，无需扫描设备。

Result: 在ADNI-4数据集上，对齐的语音达到与纯语音基线相当的性能（AUC 0.720 vs 0.711），同时推理时无需影像。多模态融合优于单独MRI（0.973 vs 0.958）。消融研究确定dropout正则化和自监督预训练是关键设计决策。

Conclusion: 这是首次展示MRI到语音知识转移用于阿尔茨海默病早期筛查，建立了无需推理时神经影像的生物学基础通路，为人群级认知分流提供了新途径。

Abstract: Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference.

</details>


### [80] [Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments](https://arxiv.org/abs/2602.23997)
*Florent Delgrange*

Main category: cs.LG

TL;DR: 该论文提出基础世界模型框架，通过可学习奖励模型、自适应形式验证、在线抽象校准和测试时合成四个组件，使智能体能够合成可验证程序、从少量交互中推导新策略，并在适应新环境时保持正确性。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体方法通常假设固定任务和环境，限制了世界模型支持智能体在条件变化时演化策略的能力。需要一种能够支持高效学习、可靠行动和开放世界适应的基础世界模型框架。

Method: 提出基础世界模型框架，包含四个核心组件：1）从规范中学习奖励模型以支持明确目标的优化；2）在整个学习过程中集成自适应形式验证；3）在线抽象校准以量化模型预测的可靠性；4）由验证器引导的测试时合成和世界模型生成。

Result: 该框架使智能体能够合成可验证程序、从少量交互中推导新策略，并在适应新环境时保持正确性。基础世界模型成为学习、推理和适应的基础，为智能体提供解释和证明其行为的能力。

Conclusion: 基础世界模型框架为下一代自主智能体提供了统一的学习、推理和适应基础，使智能体不仅能够良好行动，还能解释和证明其采用的行为，为开放世界中的可靠智能体发展奠定基础。

Abstract: The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt.

</details>


### [81] [InfoNCE Induces Gaussian Distribution](https://arxiv.org/abs/2602.24012)
*Roy Betser,Eyal Gofer,Meir Yossef Levi,Guy Gilboa*

Main category: cs.LG

TL;DR: 论文证明对比学习中的InfoNCE损失会诱导表示呈现高斯结构，通过理论分析和实验验证了这一发现。


<details>
  <summary>Details</summary>
Motivation: 对比学习已成为现代表示学习的基石，但InfoNCE损失如何影响表示分布的理论理解尚不充分。本研究旨在揭示对比训练中表示的高斯结构特性。

Method: 采用两种互补的理论分析框架：1）在特定对齐和集中假设下，证明高维表示的投影渐近接近多元高斯分布；2）在较弱假设下，通过添加促进低特征范数和高特征熵的正则化项获得类似渐近结果。同时在合成数据和CIFAR-10数据集上进行实验验证。

Result: 理论分析和实验结果表明，InfoNCE目标确实诱导表示呈现高斯分布特性。这一发现在多种编码器架构和规模上保持一致，为观察到的对比表示高斯性提供了原理性解释。

Conclusion: 对比学习中的InfoNCE损失会诱导表示呈现高斯结构，这一高斯模型为分析学习到的表示提供了原理性框架，有望支持对比学习的广泛应用。

Abstract: Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning.

</details>


### [82] [RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models](https://arxiv.org/abs/2602.24040)
*Daniel Yang,Samuel Stante,Florian Redhardt,Lena Libon,Parnian Kassraie,Ido Hakimi,Barna Pásztor,Andreas Krause*

Main category: cs.LG

TL;DR: RewardUQ：一个用于系统评估奖励模型不确定性量化的统一框架，发现模型大小和初始化对性能影响最大，大多数先前工作本可从替代设计选择中受益。


<details>
  <summary>Details</summary>
Motivation: 奖励模型对于将大语言模型与人类偏好对齐至关重要，但现有方法大多依赖点估计而忽略了有限人类反馈带来的认知不确定性。虽然量化这种不确定性可以通过主动学习减少人工标注成本，并在LLM后训练中缓解奖励过优化问题，但不确定性感知的奖励模型缺乏系统比较，理解不足。

Method: 提出了RewardUQ统一框架，系统评估奖励模型的不确定性量化方法。比较常见方法在准确性和校准性标准指标上的表现，并提出结合这两个维度的新排名策略以简化比较。

Result: 实验结果表明，模型大小和初始化对性能影响最为显著，大多数先前工作本可从替代设计选择中受益。为促进新方法的开发和评估，并帮助下游应用部署，发布了开源框架作为Python包。

Conclusion: RewardUQ为奖励模型不确定性量化提供了系统评估框架，揭示了模型大小和初始化的关键作用，为未来研究和应用提供了工具和见解。

Abstract: Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.

</details>


### [83] [pathsig: A GPU-Accelerated Library for Truncated and Projected Path Signatures](https://arxiv.org/abs/2602.24066)
*Tobias Nygaard*

Main category: cs.LG

TL;DR: pathsig是一个PyTorch原生库，用于高效计算路径签名，通过CUDA并行计算实现GPU高吞吐量，相比其他库有10-30倍加速


<details>
  <summary>Details</summary>
Motivation: 现有路径签名库缺乏大规模梯度学习所需的可扩展性，需要更高效的实现来支持现代机器学习应用

Method: 使用PyTorch原生实现，直接在单词基上计算路径签名，通过CUDA内核在前缀封闭单词集上并行更新签名系数

Result: 相比其他库，pathsig在截断签名计算上实现10-30倍加速，在需要反向传播的训练中实现4-10倍加速，同时支持用户指定单词集的投影和各向异性截断

Conclusion: pathsig提供了高效、可扩展的路径签名计算库，支持GPU并行计算和灵活的特征表示，为大规模机器学习应用提供了更好的工具支持

Abstract: Path signatures provide a rich representation of sequential data, with strong theoretical guarantees and good performance in a variety of machine-learning tasks. While signatures have progressed from fixed feature extractors to trainable components of machine-learning models, existing libraries often lack the required scalability for large-scale, gradient-based learning. To address this gap, this paper introduces pathsig, a PyTorch-native library that computes path signatures directly in the word basis. By using CUDA kernels to update signature coefficients in parallel over prefix-closed word sets, pathsig achieves high GPU throughput and near-minimal peak memory. Compared with other libraries, pathsig achieves 10-30x speedups for computation of truncated signatures and up to 4-10x speedups in training that require backpropagation through the signature. Beyond regular truncation, pathsig supports projections of the (infinite-dimensional) signature onto user-specified sets of words and anisotropic truncation motivated by inhomogeneous path regularity, enabling more compact representations that can reduce dimensionality, redundancy, and computational cost.

</details>


### [84] [Leveraging Non-linear Dimension Reduction and Random Walk Co-occurrence for Node Embedding](https://arxiv.org/abs/2602.24069)
*Ryan DeWolfe*

Main category: cs.LG

TL;DR: COVE是一种可解释的高维节点嵌入方法，通过非线性降维技术突破低维限制，在聚类和链接预测任务上表现略优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统节点嵌入方法通常受限于低维表示，作者希望开发一种可解释的高维嵌入方法，同时保持或提升在社区检测和链接预测等任务上的性能。

Method: COVE基于随机游走中节点共现作为相似性指标，与扩散过程密切相关。采用非线性降维技术（UMAP）将高维嵌入降至低维，然后结合HDBSCAN进行聚类分析。

Result: COVE+UMAP+HDBSCAN流水线在社区检测任务上与流行的Louvain算法性能相当，同时在聚类和链接预测任务上略有提升。

Conclusion: 高维节点嵌入结合非线性降维技术是可行的，COVE提供了一种可解释的替代方案，在保持社区检测性能的同时为下游任务提供更丰富的表示。

Abstract: Leveraging non-linear dimension reduction techniques, we remove the low dimension constraint from node embedding and propose COVE, an explainable high dimensional embedding that, when reduced to low dimension with UMAP, slightly increases performance on clustering and link prediction tasks. The embedding is inspired by neural embedding methods that use co-occurrence on a random walk as an indication of similarity, and is closely related to a diffusion process. Extending on recent community detection benchmarks, we find that a COVE UMAP HDBSCAN pipeline performs similarly to the popular Louvain algorithm.

</details>


### [85] [Neural Diffusion Intensity Models for Point Process Data](https://arxiv.org/abs/2602.24083)
*Xinlong Du,Harsha Honnappa,Vinayak Rao*

Main category: cs.LG

TL;DR: 提出Neural Diffusion Intensity Models，一种基于神经SDE的变分框架，用于Cox过程建模，通过扩大滤波理论保证变分族包含真实后验，实现高效推理


<details>
  <summary>Details</summary>
Motivation: Cox过程通过潜在随机强度建模过分散点过程数据，但传统的非参数强度估计和后验推理通常依赖于昂贵的MCMC方法，计算效率低下

Method: 基于扩大滤波理论，证明在点过程观测条件下，潜在强度的扩散结构得以保持并具有显式漂移修正；设计摊销编码器架构，将变长事件序列映射到后验强度路径，通过模拟漂移修正的SDE实现单次前向传播

Result: 在合成和真实世界数据上准确恢复潜在强度动态和后验路径，相比基于MCMC的方法实现数量级的速度提升

Conclusion: 提出的变分框架通过理论保证和高效架构，为Cox过程提供了可扩展且准确的后验推理方法，显著优于传统MCMC方法

Abstract: Cox processes model overdispersed point process data via a latent stochastic intensity, but both nonparametric estimation of the intensity model and posterior inference over intensity paths are typically intractable, relying on expensive MCMC methods. We introduce Neural Diffusion Intensity Models, a variational framework for Cox processes driven by neural SDEs. Our key theoretical result, based on enlargement of filtrations, shows that conditioning on point process observations preserves the diffusion structure of the latent intensity with an explicit drift correction. This guarantees the variational family contains the true posterior, so that ELBO maximization coincides with maximum likelihood estimation under sufficient model capacity. We design an amortized encoder architecture that maps variable-length event sequences to posterior intensity paths by simulating the drift-corrected SDE, replacing repeated MCMC runs with a single forward pass. Experiments on synthetic and real-world data demonstrate accurate recovery of latent intensity dynamics and posterior paths, with orders-of-magnitude speedups over MCMC-based methods.

</details>


### [86] [Learning with a Budget: Identifying the Best Arm with Resource Constraints](https://arxiv.org/abs/2602.24146)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 提出了一种在资源约束下的最佳臂识别算法SH-RR，将资源感知分配集成到经典连续减半框架中，统一了随机和确定性消耗设置的理论分析。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，评估不同替代方案的有效性具有不同的成本或资源消耗。受这种异质性启发，研究在资源约束下的最佳臂识别问题，其中每个臂拉动消耗一种或多种有限资源。

Method: 提出了Successive Halving with Resource Rationing (SH-RR)算法，将资源感知分配集成到经典连续减半框架中，使用新的有效消耗度量来统一随机和确定性消耗设置的理论分析。

Result: SH-RR算法在资源约束下的最佳臂识别问题中表现出色，能够有效处理随机和确定性资源消耗场景，通过新的有效消耗度量提供了统一的理论分析框架。

Conclusion: 该研究为资源约束下的最佳臂识别问题提供了有效的解决方案，SH-RR算法通过集成资源感知分配和新的有效消耗度量，统一了不同消耗设置的理论分析，具有实际应用价值。

Abstract: In many applications, evaluating the effectiveness of different alternatives comes with varying costs or resource usage. Motivated by such heterogeneity, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem, where an agent seeks to identify the best alternative (aka arm) in the presence of resource constraints. Each arm pull consumes one or more types of limited resources. We make two key contributions. First, we propose the Successive Halving with Resource Rationing (SH-RR) algorithm, which integrates resource-aware allocation into the classical successive halving framework on best arm identification. The SH-RR algorithm unifies the theoretical analysis for both the stochastic and deterministic consumption settings, with a new \textit{effective consumption measure

</details>


### [87] [Sandwiching Polynomials for Geometric Concepts with Low Intrinsic Dimension](https://arxiv.org/abs/2602.24178)
*Adam R. Klivans,Konstantinos Stavropoulos,Arsen Vasilyan*

Main category: cs.LG

TL;DR: 本文提出了一种新的低度三明治多项式构造方法，显著改进了多个基础函数类在特定分布下的度界，特别是将k个半空间的函数从指数级改进到多项式级。


<details>
  <summary>Details</summary>
Motivation: 低度三明治多项式近似器在分布偏移学习、可测试学习和污染学习等挑战性学习场景中展现出强大能力，但现有构造方法的度界不够理想，需要改进。

Method: 提出新的构造方法，直接利用目标函数边界的平滑性来构造三明治Lipschitz函数，然后应用高维逼近理论的结果，避免了之前工作中使用的FT-mollification方法。

Result: 对于高斯分布下的k个半空间函数，获得了poly(k)度的三明治多项式，相比之前的2^O(k)度有指数级改进；对于低维多项式阈值函数，获得了双指数级改进。

Conclusion: 新方法相对简单且有效，适用于低维且具有平滑边界的函数类，为三明治多项式构造提供了更优的度界。

Abstract: Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in the context of challenging learning settings such as learning with distribution shift, testable learning, and learning with contamination. A pair of sandwiching polynomials approximate a target function in expectation while also providing pointwise upper and lower bounds on the function's values. In this paper, we give a new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions. In particular, we obtain degree $\mathrm{poly}(k)$ sandwiching polynomials for functions of $k$ halfspaces under the Gaussian distribution, improving exponentially over the prior $2^{O(k)}$ bound. More broadly, our approach applies to function classes that are low-dimensional and have smooth boundary.
  In contrast to prior work, our proof is relatively simple and directly uses the smoothness of the target function's boundary to construct sandwiching Lipschitz functions, which are amenable to results from high-dimensional approximation theory. For low-dimensional polynomial threshold functions (PTFs) with respect to Gaussians, we obtain doubly exponential improvements without applying the FT-mollification method of Kane used in the best previous result.

</details>


### [88] [Multi-Objective Reinforcement Learning for Large-Scale Tote Allocation in Human-Robot Collaborative Fulfillment Centers](https://arxiv.org/abs/2602.24182)
*Sikata Sengupta,Guangyi Liu,Omer Gottesman,Joseph W Durham,Michael Kearns,Aaron Roth,Michael Caldara*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多目标强化学习的方法来优化集装箱式履约中心的整合过程，通过零和博弈中的最佳响应和无悔动态来解决约束RL问题，实现了在满足所有操作约束的同时平衡处理速度、资源使用和空间利用率等多个目标。


<details>
  <summary>Details</summary>
Motivation: 集装箱式履约中心的整合过程需要在处理速度、资源使用和空间利用率等多个相互竞争的目标之间进行权衡，同时还要满足各种现实操作约束。传统方法难以有效处理这种大规模、高维状态空间和动态系统行为的复杂决策问题。

Method: 将问题建模为大规模多目标强化学习任务，利用零和博弈中的最佳响应和无悔动态来解决约束RL问题，实现原则性的极小极大策略学习。还引入了处理误差消除问题的理论框架，避免时间平均解出现振荡行为。

Result: 在真实的仓库模拟中进行策略评估表明，该方法能有效权衡多个目标，并且经验观察到学习到的单一策略能够同时满足所有约束（尽管这在理论上无法保证）。该方法返回的单个迭代的拉格朗日值接近博弈的极小极大值。

Conclusion: 这些结果证明了多目标强化学习在解决大规模工业系统中复杂、高影响力决策问题方面的潜力，为集装箱履约中心的整合优化提供了有效的解决方案。

Abstract: Optimizing the consolidation process in container-based fulfillment centers requires trading off competing objectives such as processing speed, resource usage, and space utilization while adhering to a range of real-world operational constraints. This process involves moving items between containers via a combination of human and robotic workstations to free up space for inbound inventory and increase container utilization. We formulate this problem as a large-scale Multi-Objective Reinforcement Learning (MORL) task with high-dimensional state spaces and dynamic system behavior. Our method builds on recent theoretical advances in solving constrained RL problems via best-response and no-regret dynamics in zero-sum games, enabling principled minimax policy learning. Policy evaluation on realistic warehouse simulations shows that our approach effectively trades off objectives, and we empirically observe that it learns a single policy that simultaneously satisfies all constraints, even if this is not theoretically guaranteed. We further introduce a theoretical framework to handle the problem of error cancellation, where time-averaged solutions display oscillatory behavior. This method returns a single iterate whose Lagrangian value is close to the minimax value of the game. These results demonstrate the promise of MORL in solving complex, high-impact decision-making problems in large-scale industrial systems.

</details>


### [89] [Flow-Based Density Ratio Estimation for Intractable Distributions with Applications in Genomics](https://arxiv.org/abs/2602.24201)
*Egor Antipov,Alessandro Palma,Lorenzo Consoli,Stephan Günnemann,Andrea Dittadi,Fabian J. Theis*

Main category: cs.LG

TL;DR: 提出一种基于条件感知流匹配的密度比估计方法，避免分别计算两个分布的昂贵似然积分，在模拟基准和单细胞基因组学数据分析中表现优异。


<details>
  <summary>Details</summary>
Motivation: 密度比估计是概率建模的核心问题，用于在不同数据生成过程之间进行原则性的似然比较。虽然归一化流等精确似然模型提供了有前景的密度比估计方法，但传统的流基评估需要分别为每个分布计算昂贵的似然积分，计算成本高昂。

Method: 利用条件感知流匹配技术，推导出单一动态公式来跟踪生成轨迹上的密度比。该方法避免了分别计算两个分布似然积分的需求，通过流匹配框架直接建模密度比的变化。

Result: 在闭式比估计的模拟基准测试中表现出竞争性性能。在单细胞基因组学数据分析中，该方法支持多种任务，包括跨实验条件的细胞状态似然比较、治疗效果估计和批次校正评估。

Conclusion: 提出的条件感知流匹配方法为密度比估计提供了高效的计算框架，避免了传统流基方法中分别计算似然积分的计算负担，在理论和实际应用中均表现出良好性能。

Abstract: Estimating density ratios between pairs of intractable data distributions is a core problem in probabilistic modeling, enabling principled comparisons of sample likelihoods under different data-generating processes across conditions and covariates. While exact-likelihood models such as normalizing flows offer a promising approach to density ratio estimation, naive flow-based evaluations are computationally expensive, as they require simulating costly likelihood integrals for each distribution separately. In this work, we leverage condition-aware flow matching to derive a single dynamical formulation for tracking density ratios along generative trajectories. We demonstrate competitive performance on simulated benchmarks for closed-form ratio estimation, and show that our method supports versatile tasks in single-cell genomics data analysis, where likelihood-based comparisons of cellular states across experimental conditions enable treatment effect estimation and batch correction evaluation.

</details>


### [90] [The Stability of Online Algorithms in Performative Prediction](https://arxiv.org/abs/2602.24207)
*Gabriele Farina,Juan Carlos Perdomo*

Main category: cs.LG

TL;DR: 本文展示了在预测性预测框架中，任何无遗憾算法都会收敛到（混合）预测稳定均衡，无需对模型如何影响数据分布做任何限制性假设。


<details>
  <summary>Details</summary>
Motivation: 算法预测在决策中的应用会导致反馈循环：部署的模型主动影响我们观察到的数据分布，而这些数据又用于重新训练模型。这种动态关系由Perdomo等人（2020）在预测性预测工作中形式化。现有研究对模型如何影响分布有严格限制，需要更通用的理论框架。

Method: 使用鞅论论证和允许随机化，避免了所有对模型影响分布的假设，绕过了最近关于寻找稳定模型的硬度结果。将在线优化与预测性预测联系起来，建立了无条件约简。

Result: 证明了任何无遗憾算法在预测性设置中都会收敛到（混合）预测稳定均衡：在这种解中，模型主动塑造数据分布，使得它们自己的预测在后验看起来是最优的。

Conclusion: 该连接揭示了为什么常见算法（如梯度下降）天然具有稳定性和防止失控反馈循环的能力。这项工作有望促进在线优化和预测性预测之间思想的技术转移。

Abstract: The use of algorithmic predictions in decision-making leads to a feedback loop where the models we deploy actively influence the data distributions we see, and later use to retrain on. This dynamic was formalized by Perdomo et al. 2020 in their work on performative prediction. Our main result is an unconditional reduction showing that any no-regret algorithm deployed in performative settings converges to a (mixed) performatively stable equilibrium: a solution in which models actively shape data distributions in ways that their own predictions look optimal in hindsight. Prior to our work, all positive results in this area made strong restrictions on how models influenced distributions. By using a martingale argument and allowing randomization, we avoid any such assumption and sidestep recent hardness results for finding stable models. Lastly, on a more conceptual note, our connection sheds light on why common algorithms, like gradient descent, are naturally stabilizing and prevent runaway feedback loops. We hope our work enables future technical transfer of ideas between online optimization and performativity.

</details>


### [91] [An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks](https://arxiv.org/abs/2602.24209)
*Mohsen Tajgardan,Atena Shiranzaei,Mahdi Rabbani,Reza Khoshkangini,Mahtab Jamali*

Main category: cs.LG

TL;DR: 提出一个高效的联邦学习框架，利用两个不同物联网数据集的共享特征来增强异常检测，同时保持数据集特定特征，并通过可解释AI提高透明度。


<details>
  <summary>Details</summary>
Motivation: 物联网环境中的联邦学习面临数据异构性挑战，设备能力、数据格式和通信约束的差异影响了全局模型性能和隐私保护。在物联网异常检测中，无监督联邦学习虽然避免了集中数据聚合，但特征异构性阻碍了有效实施。

Method: 提出一个高效的无监督联邦学习框架，利用两个不同物联网数据集（一个专注于异常检测，另一个专注于设备识别）的共享特征来增强异常检测，同时保持数据集特定特征。使用可解释AI技术（如SHAP）识别影响本地模型决策的关键特征，提高透明度和可解释性。

Result: 在真实物联网数据集上的实验表明，该方法在异常检测准确率方面显著优于传统的联邦学习方法。

Conclusion: 这项工作强调了利用互补数据集的共享特征来优化无监督联邦学习，在去中心化物联网环境中实现卓越异常检测结果的潜力。

Abstract: Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments.

</details>


### [92] [Adaptive Combinatorial Experimental Design: Pareto Optimality for Decision-Making and Inference](https://arxiv.org/abs/2602.24231)
*Hongrui Xie,Junyu Cao,Kan Xu*

Main category: cs.LG

TL;DR: 该论文首次研究自适应组合实验设计，在组合多臂老虎机中平衡遗憾最小化与统计功效的权衡，提出帕累托最优学习框架和两种算法，证明在两种反馈机制下都能实现帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 在组合多臂老虎机中，最小化遗憾需要重复利用高奖励臂，而准确推断奖励差距需要充分探索次优动作。这种权衡在自适应组合实验设计中尚未得到系统研究，需要建立理论框架来平衡这两个目标。

Method: 通过帕累托最优性概念形式化权衡，考虑两种信息结构：全老虎机反馈和半老虎机反馈。针对这两种情况分别提出MixCombKL和MixCombUCB算法，提供理论保证证明两种算法都是帕累托最优的。

Result: 理论分析表明两种算法都能实现帕累托最优，获得遗憾和臂差距估计误差的有限时间保证。研究还发现更丰富的反馈能显著收紧可达到的帕累托前沿，主要增益来自所提方法下估计精度的提升。

Conclusion: 这些发现为多目标决策中的自适应组合实验建立了原则性框架，首次系统研究了组合多臂老虎机中遗憾最小化与统计功效的权衡问题，为实际应用提供了理论基础。

Abstract: In this paper, we provide the first investigation into adaptive combinatorial experimental design, focusing on the trade-off between regret minimization and statistical power in combinatorial multi-armed bandits (CMAB). While minimizing regret requires repeated exploitation of high-reward arms, accurate inference on reward gaps requires sufficient exploration of suboptimal actions. We formalize this trade-off through the concept of Pareto optimality and establish equivalent conditions for Pareto-efficient learning in CMAB. We consider two relevant cases under different information structures, i.e., full-bandit feedback and semi-bandit feedback, and propose two algorithms MixCombKL and MixCombUCB respectively for these two cases. We provide theoretical guarantees showing that both algorithms are Pareto optimal, achieving finite-time guarantees on both regret and estimation error of arm gaps. Our results further reveal that richer feedback significantly tightens the attainable Pareto frontier, with the primary gains arising from improved estimation accuracy under our proposed methods. Taken together, these findings establish a principled framework for adaptive combinatorial experimentation in multi-objective decision-making.

</details>


### [93] [Time Series Foundation Models as Strong Baselines in Transportation Forecasting: A Large-Scale Benchmark Analysis](https://arxiv.org/abs/2602.24238)
*Javier Pulido,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 评估时间序列基础模型Chronos-2在交通预测任务中的零样本性能，发现其在多个数据集上无需微调即可达到SOTA或竞争性精度，特别是在长预测时域上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习交通预测方法需要针对每个数据集进行特定训练、架构设计和超参数调优，过程繁琐。本研究旨在探索通用时间序列基础模型是否可以作为交通预测的基准方法，减少重复性工作。

Method: 使用最先进的时间序列基础模型Chronos-2，在10个真实世界交通数据集上进行零样本性能评估，涵盖高速公路交通量/流量、城市交通速度、共享单车需求和电动汽车充电站数据。采用一致的评估协议，不仅评估点预测精度，还评估其原生概率输出的预测区间覆盖率和锐度。

Result: 即使没有任何任务特定的微调，Chronos-2在大多数数据集上都能达到最先进或具有竞争力的精度，特别是在长预测时域上经常超越经典统计基线和专门的深度学习架构。其概率输出也能提供有用的不确定性量化。

Conclusion: 时间序列基础模型可以作为交通预测研究的关键基准方法，支持其在交通预测领域的采用，减少对数据集特定训练的需求。

Abstract: Accurate forecasting of transportation dynamics is essential for urban mobility and infrastructure planning. Although recent work has achieved strong performance with deep learning models, these methods typically require dataset-specific training, architecture design and hyper-parameter tuning. This paper evaluates whether general-purpose time-series foundation models can serve as forecasters for transportation tasks by benchmarking the zero-shot performance of the state-of-the-art model, Chronos-2, across ten real-world datasets covering highway traffic volume and flow, urban traffic speed, bike-sharing demand, and electric vehicle charging station data. Under a consistent evaluation protocol, we find that, even without any task-specific fine-tuning, Chronos-2 delivers state-of-the-art or competitive accuracy across most datasets, frequently outperforming classical statistical baselines and specialized deep learning architectures, particularly at longer horizons. Beyond point forecasting, we evaluate its native probabilistic outputs using prediction-interval coverage and sharpness, demonstrating that Chronos-2 also provides useful uncertainty quantification without dataset-specific training. In general, this study supports the adoption of time-series foundation models as a key baseline for transportation forecasting research.

</details>


### [94] [Memory Caching: RNNs with Growing Memory](https://arxiv.org/abs/2602.24281)
*Ali Behrouz,Zeman Li,Yuan Deng,Peilin Zhong,Meisam Razaviyayn,Vahab Mirrokni*

Main category: cs.LG

TL;DR: 本文提出Memory Caching技术，通过缓存RNN隐藏状态检查点来增强循环模型，使其内存容量随序列长度增长，在固定内存的RNN和二次复杂度的Transformer之间提供灵活权衡。


<details>
  <summary>Details</summary>
Motivation: Transformer虽然性能强大但具有二次复杂度，而现有的循环架构在召回密集型任务中表现不佳，主要原因是其固定大小的内存容量限制了性能。

Method: 提出Memory Caching技术，包括四种变体：门控聚合和稀疏选择机制，通过缓存RNN隐藏状态检查点来扩展有效内存容量。

Result: 在语言建模和长上下文理解任务中，MC显著提升了循环模型的性能；在上下文召回任务中，MC变体表现出与Transformer竞争的性能，缩小了与Transformer的差距，并优于现有最先进的循环模型。

Conclusion: Memory Caching是一种简单有效的技术，能够增强循环模型的内存容量，在保持线性复杂度的同时提供接近Transformer的性能，为序列建模提供了新的设计选择。

Abstract: Transformers have been established as the de-facto backbones for most recent advances in sequence modeling, mainly due to their growing memory capacity that scales with the context length. While plausible for retrieval tasks, it causes quadratic complexity and so has motivated recent studies to explore viable subquadratic recurrent alternatives. Despite showing promising preliminary results in diverse domains, such recurrent architectures underperform Transformers in recall-intensive tasks, often attributed to their fixed-size memory. In this paper, we introduce Memory Caching (MC), a simple yet effective technique that enhances recurrent models by caching checkpoints of their memory states (a.k.a. hidden states). Memory Caching allows the effective memory capacity of RNNs to grow with sequence length, offering a flexible trade-off that interpolates between the fixed memory (i.e., $O(L)$ complexity) of RNNs and the growing memory (i.e., $O(L^2)$ complexity) of Transformers. We propose four variants of MC, including gated aggregation and sparse selective mechanisms, and discuss their implications on both linear and deep memory modules. Our experimental results on language modeling, and long-context understanding tasks show that MC enhances the performance of recurrent models, supporting its effectiveness. The results of in-context recall tasks indicate that while Transformers achieve the best accuracy, our MC variants show competitive performance, close the gap with Transformers, and performs better than state-of-the-art recurrent models.

</details>


### [95] [Chunk-wise Attention Transducers for Fast and Accurate Streaming Speech-to-Text](https://arxiv.org/abs/2602.24245)
*Hainan Xu,Vladimir Bataev,Travis M. Bartley,Jagadeesh Balam*

Main category: cs.LG

TL;DR: CHAT模型在RNN-T基础上引入分块注意力机制，在保持流式处理能力的同时提升效率和准确性，特别适用于语音翻译任务。


<details>
  <summary>Details</summary>
Motivation: 传统RNN-T模型在处理语音时存在严格的单调对齐限制，这在语音翻译任务中会影响性能。同时，RNN-T在处理长序列时存在效率和内存消耗问题。

Method: 提出分块注意力转换器（CHAT），将音频分割成固定大小的块，在每个块内使用交叉注意力机制。这种混合方法保持了RNN-T的流式处理能力，同时为局部对齐建模引入了可控的灵活性。

Result: CHAT显著减少了RNN-T需要处理的时间维度，带来显著效率提升：训练峰值内存减少46.2%，训练速度提升1.36倍，推理速度提升1.69倍。在准确性方面，语音识别相对WER降低6.3%，语音翻译BLEU提升18.0%。

Conclusion: CHAT模型为部署更强大的流式语音模型提供了实用解决方案，在不牺牲实时约束的情况下实现了效率和准确性的双重提升，特别在语音翻译任务中表现出色。

Abstract: We propose Chunk-wise Attention Transducer (CHAT), a novel extension to RNN-T models that processes audio in fixed-size chunks while employing cross-attention within each chunk. This hybrid approach maintains RNN-T's streaming capability while introducing controlled flexibility for local alignment modeling. CHAT significantly reduces the temporal dimension that RNN-T must handle, yielding substantial efficiency improvements: up to 46.2% reduction in peak training memory, up to 1.36X faster training, and up to 1.69X faster inference. Alongside these efficiency gains, CHAT achieves consistent accuracy improvements over RNN-T across multiple languages and tasks -- up to 6.3% relative WER reduction for speech recognition and up to 18.0% BLEU improvement for speech translation. The method proves particularly effective for speech translation, where RNN-T's strict monotonic alignment hurts performance. Our results demonstrate that the CHAT model offers a practical solution for deploying more capable streaming speech models without sacrificing real-time constraints.

</details>


### [96] [Taming Momentum: Rethinking Optimizer States Through Low-Rank Approximation](https://arxiv.org/abs/2602.24283)
*Zhengbo Wang,Jian Liang,Ran He,Zilei Wang,Tieniu Tan*

Main category: cs.LG

TL;DR: LoRA-Pre是一种新颖的低秩优化器，通过将动量矩阵分解为紧凑的低秩子空间，显著减少优化器内存开销，在预训练和微调中都取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现代优化器如Adam和Muon依赖一阶和二阶动量，引入显著内存开销，限制了大规模语言模型训练的可扩展性和计算效率。

Method: 将指数移动平均重新定义为在线梯度流的线性回归器训练，并引入LoRA-Pre低秩优化器，将完整动量矩阵分解为在线线性学习器中的紧凑低秩子空间。

Result: 在Llama架构家族（60M到1B参数）的预训练中，LoRA-Pre在所有模型规模上取得最高性能，仅使用基线方法1/8的秩就能获得相当或更好的结果。在微调场景中，相比标准LoRA，在Llama-3.1-8B上提升3.14分，在Llama-2-7B上提升6.17分。

Conclusion: LoRA-Pre通过低秩分解有效减少优化器内存开销，在保持优化性能的同时提高内存效率，在预训练和微调两种范式下都表现出色。

Abstract: Modern optimizers like Adam and Muon are central to training large language models, but their reliance on first- and second-order momenta introduces significant memory overhead, which constrains scalability and computational efficiency. In this work, we reframe the exponential moving average (EMA) used in these momenta as the training of a linear regressor via online gradient flow. Building on this equivalence, we introduce LoRA-Pre, a novel low-rank optimizer designed for efficient pre-training. Specifically, LoRA-Pre reduces the optimizer's memory footprint by decomposing the full momentum matrix into a compact low-rank subspace within the online linear learner, thereby maintaining optimization performance while improving memory efficiency. We empirically validate LoRA-Pre's efficacy by pre-training models from the Llama architecture family, scaling from 60M to 1B parameters. LoRA-Pre achieves the highest performance across all model sizes. Notably, LoRA-Pre demonstrates remarkable rank efficiency, achieving comparable or superior results using only 1/8 the rank of baseline methods. Beyond pre-training, we evaluate LoRA-Pre's effectiveness in fine-tuning scenarios. With the same rank, LoRA-Pre consistently outperforms all efficient fine-tuning baselines. Specifically, compared to standard LoRA, LoRA-Pre achieves substantial improvements of 3.14 points on Llama-3.1-8B and 6.17 points on Llama-2-7B, validating our approach's effectiveness across both pre-training and fine-tuning paradigms. Our code is publicly available at https://github.com/mrflogs/LoRA-Pre.

</details>


### [97] [CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation](https://arxiv.org/abs/2602.24286)
*Weinan Dai,Hanlin Wu,Qiying Yu,Huan-ang Gao,Jiahao Li,Chengquan Jiang,Weiqiang Lou,Yufan Song,Hongli Yu,Jiaze Chen,Wei-Ying Ma,Ya-Qin Zhang,Jingjing Liu,Mingxuan Wang,Xin Liu,Hao Zhou*

Main category: cs.LG

TL;DR: CUDA Agent是一个基于大规模智能体强化学习的系统，通过数据合成、技能增强的开发环境和强化学习算法，显著提升了LLM在CUDA内核优化方面的能力，性能远超现有编译器系统和专有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在通用编程方面表现出色，但在CUDA内核优化方面仍无法与编译器系统（如torch.compile）竞争。现有方法要么依赖无训练的精炼，要么在固定的多轮执行反馈循环中微调模型，都无法从根本上提升模型的内在CUDA优化能力，导致性能提升有限。

Method: CUDA Agent包含三个核心组件：1）可扩展的数据合成流水线；2）技能增强的CUDA开发环境，具有自动验证和分析功能以提供可靠的奖励信号；3）实现稳定训练的强化学习算法技术。该系统通过大规模智能体强化学习来培养CUDA内核的专业知识。

Result: 在KernelBench基准测试中，CUDA Agent在Level-1、Level-2和Level-3三个难度级别上分别比torch.compile快100%、100%和92%。在最难的Level-3设置上，比最强的专有模型（如Claude Opus 4.5和Gemini 3 Pro）性能高出约40%。

Conclusion: CUDA Agent通过创新的智能体强化学习方法，成功解决了LLM在CUDA内核优化方面的局限性，实现了显著的性能提升，为深度学习硬件优化提供了新的有效途径。

Abstract: GPU kernel optimization is fundamental to modern deep learning but remains a highly specialized task requiring deep hardware expertise. Despite strong performance in general programming, large language models (LLMs) remain uncompetitive with compiler-based systems such as torch.compile for CUDA kernel generation. Existing CUDA code generation approaches either rely on training-free refinement or fine-tune models within fixed multi-turn execution-feedback loops, but both paradigms fail to fundamentally improve the model's intrinsic CUDA optimization ability, resulting in limited performance gains. We present CUDA Agent, a large-scale agentic reinforcement learning system that develops CUDA kernel expertise through three components: a scalable data synthesis pipeline, a skill-augmented CUDA development environment with automated verification and profiling to provide reliable reward signals, and reinforcement learning algorithmic techniques enabling stable training. CUDA Agent achieves state-of-the-art results on KernelBench, delivering 100\%, 100\%, and 92\% faster rate over torch.compile on KernelBench Level-1, Level-2, and Level-3 splits, outperforming the strongest proprietary models such as Claude Opus 4.5 and Gemini 3 Pro by about 40\% on the hardest Level-3 setting.

</details>


### [98] [Who Guards the Guardians? The Challenges of Evaluating Identifiability of Learned Representations](https://arxiv.org/abs/2602.24278)
*Shruti Joshi,Théo Saulus,Wieland Brendel,Philippe Brouillard,Dhanya Sridhar,Patrik Reizinger*

Main category: cs.LG

TL;DR: 现有表示学习可识别性评估指标存在假设条件限制，当数据生成过程或编码器几何假设被违反时，这些指标会产生系统性误判


<details>
  <summary>Details</summary>
Motivation: 当前表示学习可识别性评估依赖于标准指标（如MCC、DCI、R²），这些指标假设能够反映理论可识别性保证的等价类恢复程度，但这种假设仅在特定结构条件下成立

Method: 提出一个分类法，将数据生成过程假设与编码器几何假设分离，用于描述现有指标的有效域，并发布一个可重复的压力测试和比较评估套件

Result: 当数据生成过程或编码器几何假设被违反时，现有指标会产生系统性假阳性和假阴性，这种失败既发生在经典可识别性机制内，也发生在最需要可识别性的后验设置中

Conclusion: 表示学习可识别性评估需要更严谨的方法，现有指标的有效性依赖于特定假设条件，需要开发更鲁棒的评估框架来避免系统性误判

Abstract: Identifiability in representation learning is commonly evaluated using standard metrics (e.g., MCC, DCI, R^2) on synthetic benchmarks with known ground-truth factors. These metrics are assumed to reflect recovery up to the equivalence class guaranteed by identifiability theory. We show that this assumption holds only under specific structural conditions: each metric implicitly encodes assumptions about both the data-generating process (DGP) and the encoder. When these assumptions are violated, metrics become misspecified and can produce systematic false positives and false negatives. Such failures occur both within classical identifiability regimes and in post-hoc settings where identifiability is most needed. We introduce a taxonomy separating DGP assumptions from encoder geometry, use it to characterise the validity domains of existing metrics, and release an evaluation suite for reproducible stress testing and comparison.

</details>
